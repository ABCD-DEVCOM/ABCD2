<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title></title>
</head>
<body><div class="page"><p/>
<p>Universitext
</p>
<p>David&nbsp;Borthwick
</p>
<p>Introduction 
to Partial 
Differential 
Equations</p>
<p/>
</div>
<div class="page"><p/>
<p>Universitext</p>
<p/>
</div>
<div class="page"><p/>
<p>More information about this series at http://www.springer.com/series/223
</p>
<p>Universitext
</p>
<p>Series editors
</p>
<p>Sheldon Axler
</p>
<p>San Francisco State University
</p>
<p>Carles Casacuberta
</p>
<p>Universitat de Barcelona
</p>
<p>Angus MacIntyre
</p>
<p>Queen Mary, University of London
</p>
<p>Kenneth Ribet
</p>
<p>University of California, Berkeley
</p>
<p>Claude Sabbah
</p>
<p>&Eacute;cole polytechnique, CNRS, Universit&eacute; Paris-Saclay, Palaiseau
</p>
<p>Endre S&uuml;li
</p>
<p>University of Oxford
</p>
<p>Wojbor A. Woyczyński
</p>
<p>Case Western Reserve University
</p>
<p>Universitext is a series of textbooks that presents material from a wide variety of
</p>
<p>mathematical disciplines at master&rsquo;s level and beyond. The books, often well
</p>
<p>class-tested by their author, may have an informal, personal even experimental
</p>
<p>approach to their subject matter. Some of the most successful and established books
</p>
<p>in the series have evolved through several editions, always following the evolution
</p>
<p>of teaching curricula, into very polished texts.
</p>
<p>Thus as research topics trickle down into graduate-level teaching, first textbooks
</p>
<p>written for new, cutting-edge courses may make their way into Universitext.</p>
<p/>
<div class="annotation"><a href="http://www.springer.com/series/223">http://www.springer.com/series/223</a></div>
</div>
<div class="page"><p/>
<p>David Borthwick
</p>
<p>Introduction to Partial
Differential Equations
</p>
<p>123</p>
<p/>
</div>
<div class="page"><p/>
<p>David Borthwick
Department of Mathematics
and Computer Science
</p>
<p>Emory University
Atlanta, GA
USA
</p>
<p>ISSN 0172-5939 ISSN 2191-6675 (electronic)
Universitext
ISBN 978-3-319-48934-6 ISBN 978-3-319-48936-0 (eBook)
DOI 10.1007/978-3-319-48936-0
</p>
<p>Library of Congress Control Number: 2016955918
</p>
<p>&copy; Springer International Publishing AG 2016, corrected publication 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
</p>
<p>The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a specific statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
</p>
<p>for any errors or omissions that may have been made.
</p>
<p>Printed on acid-free paper
</p>
<p>This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
</p>
<p>The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland</p>
<p/>
</div>
<div class="page"><p/>
<p>Dedicated to my parents</p>
<p/>
</div>
<div class="page"><p/>
<p>Preface
</p>
<p>Partial differential equations (PDE) first appeared over 300 years ago, and the vast
</p>
<p>scope of the theory and applications that have since developed makes it challenging
</p>
<p>to give a reasonable introduction in a single semester. The modern mathematical
</p>
<p>approach to the subject requires considerable background in analysis, including
</p>
<p>topics such as metric space topology, measure theory, and functional analysis.
</p>
<p>This book is intended for an introductory course for students who do not nec-
</p>
<p>essarily have this analysis background. Courses taught at this level traditionally
</p>
<p>focus on some of the more elementary topics, such as Fourier series and simple
</p>
<p>boundary value problems. This approach risks giving students a somewhat narrow
</p>
<p>and outdated view of the subject.
</p>
<p>My goal here is to give a balanced presentation that includes modern methods,
</p>
<p>without requiring prerequisites beyond vector calculus and linear algebra. To allow
</p>
<p>for some of the more advanced methods to be reached within a single semester, the
</p>
<p>treatment is necessarily streamlined in certain ways. Concepts and definitions from
</p>
<p>analysis are introduced only as they will be needed in the text, and the reader is
</p>
<p>asked to accept certain fundamental results without justification. The emphasis is
</p>
<p>not on the rigorous development of analysis in its own right, but rather on the role
</p>
<p>that tools from analysis play in PDE applications.
</p>
<p>The text generally focuses on the most important classical PDE, which are the
</p>
<p>wave, heat, and Laplace equations. Nonlinear equations are discussed to some
</p>
<p>extent, but this coverage is limited. (Even at a very introductory level, the nonlinear
</p>
<p>theory merits a full course to itself.)
</p>
<p>I have tried to stress the interplay between modeling and mathematical analysis
</p>
<p>wherever possible. These connections are vital to the subject, both as a source of
</p>
<p>problems and as an inspiration for the development of methods.
</p>
<p>vii</p>
<p/>
</div>
<div class="page"><p/>
<p>I owe a great debt of gratitude to my colleague Alessandro Veneziani, with
</p>
<p>whom I collaborated on this project originally. The philosophy of the book and
</p>
<p>choice of topics were heavily influenced by our discussions, and I am grateful for
</p>
<p>his support throughout the process. I would also like to thank former student Dallas
</p>
<p>Albritton, for offering comments and suggestions on an early draft. Thanks also to
</p>
<p>the series editors at Springer, for comments that helped improve the writing and
</p>
<p>presentation.
</p>
<p>Atlanta, USA David Borthwick
</p>
<p>September 2016
</p>
<p>viii Preface</p>
<p/>
</div>
<div class="page"><p/>
<p>The original version of the book was revised:
</p>
<p>Belated corrections from author have been
</p>
<p>incorporated. The erratum to the book is
</p>
<p>available at https://doi.org/10.1007/978-3-
</p>
<p>319-48936-0_14
</p>
<p>ix</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents
</p>
<p>1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
</p>
<p>1.1 Partial Differential Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
</p>
<p>1.2 Example: d&rsquo;Alembert&rsquo;s Wave Equation . . . . . . . . . . . . . . . . . . . 2
</p>
<p>1.3 Types of Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
</p>
<p>1.4 Well Posed Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
</p>
<p>1.5 Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
</p>
<p>2 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
</p>
<p>2.1 Real Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
</p>
<p>2.2 Complex Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
</p>
<p>2.3 Domains in Rn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
</p>
<p>2.4 Differentiability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
</p>
<p>2.5 Ordinary Differential Equations . . . . . . . . . . . . . . . . . . . . . . . . . 15
</p>
<p>2.6 Vector Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
</p>
<p>2.7 Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
</p>
<p>3 Conservation Equations and Characteristics . . . . . . . . . . . . . . . . . . . 25
</p>
<p>3.1 Model Problem: Oxygen in the Bloodstream . . . . . . . . . . . . . . . 25
</p>
<p>3.2 Lagrangian Derivative and Characteristics . . . . . . . . . . . . . . . . . 27
</p>
<p>3.3 Higher-Dimensional Equations . . . . . . . . . . . . . . . . . . . . . . . . . . 32
</p>
<p>3.4 Quasilinear Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
</p>
<p>3.5 Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
</p>
<p>4 The Wave Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
</p>
<p>4.1 Model Problem: Vibrating String . . . . . . . . . . . . . . . . . . . . . . . . 45
</p>
<p>4.2 Characteristics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
</p>
<p>4.3 Boundary Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
</p>
<p>4.4 Forcing Terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
</p>
<p>4.5 Model Problem: Acoustic Waves . . . . . . . . . . . . . . . . . . . . . . . . 59
</p>
<p>4.6 Integral Solution Formulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
</p>
<p>4.7 Energy and Uniqueness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
</p>
<p>4.8 Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
</p>
<p>xi</p>
<p/>
</div>
<div class="page"><p/>
<p>5 Separation of Variables. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
</p>
<p>5.1 Model Problem: Overtones . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
</p>
<p>5.2 Helmholtz Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
</p>
<p>5.3 Circular Symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
</p>
<p>5.4 Spherical Symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
</p>
<p>5.5 Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
</p>
<p>6 The Heat Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
</p>
<p>6.1 Model Problem: Heat Flow in a Metal Rod . . . . . . . . . . . . . . . . 97
</p>
<p>6.2 Scale-Invariant Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
</p>
<p>6.3 Integral Solution Formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
</p>
<p>6.4 Inhomogeneous Problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
</p>
<p>6.5 Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
</p>
<p>7 Function Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
</p>
<p>7.1 Inner Products and Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
</p>
<p>7.2 Lebesgue Integration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
</p>
<p>7.3 Lp Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
</p>
<p>7.4 Convergence and Completeness . . . . . . . . . . . . . . . . . . . . . . . . . 119
</p>
<p>7.5 Orthonormal Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
</p>
<p>7.6 Self-adjointness. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
</p>
<p>7.7 Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
</p>
<p>8 Fourier Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
</p>
<p>8.1 Series Solution of the Heat Equation . . . . . . . . . . . . . . . . . . . . . 131
</p>
<p>8.2 Periodic Fourier Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
</p>
<p>8.3 Pointwise Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
</p>
<p>8.4 Uniform Convergence. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
</p>
<p>8.5 Convergence in L2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
</p>
<p>8.6 Regularity and Fourier Coefficients. . . . . . . . . . . . . . . . . . . . . . . 145
</p>
<p>8.7 Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
</p>
<p>9 Maximum Principles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
</p>
<p>9.1 Model Problem: The Laplace Equation. . . . . . . . . . . . . . . . . . . . 155
</p>
<p>9.2 Mean Value Formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
</p>
<p>9.3 Strong Principle for Subharmonic Functions. . . . . . . . . . . . . . . . 165
</p>
<p>9.4 Weak Principle for Elliptic Equations . . . . . . . . . . . . . . . . . . . . . 167
</p>
<p>9.5 Application to the Heat Equation . . . . . . . . . . . . . . . . . . . . . . . . 170
</p>
<p>9.6 Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
</p>
<p>10 Weak Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
</p>
<p>10.1 Test Functions and Weak Derivatives . . . . . . . . . . . . . . . . . . . . . 177
</p>
<p>10.2 Weak Solutions of Continuity Equations . . . . . . . . . . . . . . . . . . 181
</p>
<p>10.3 Sobolev Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
</p>
<p>10.4 Sobolev Regularity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
</p>
<p>xii Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 Weak Formulation of Elliptic Equations . . . . . . . . . . . . . . . . . . . 194
</p>
<p>10.6 Weak Formulation of Evolution Equations . . . . . . . . . . . . . . . . . 196
</p>
<p>10.7 Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
</p>
<p>11 Variational Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
</p>
<p>11.1 Model Problem: The Poisson Equation . . . . . . . . . . . . . . . . . . . . 206
</p>
<p>11.2 Dirichlet&rsquo;s Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
</p>
<p>11.3 Coercivity and Existence of a Minimum. . . . . . . . . . . . . . . . . . . 208
</p>
<p>11.4 Elliptic Regularity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
</p>
<p>11.5 Eigenvalues by Minimization . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
</p>
<p>11.6 Sequential Compactness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
</p>
<p>11.7 Estimation of Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
</p>
<p>11.8 Euler-Lagrange Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
</p>
<p>11.9 Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
</p>
<p>12 Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
</p>
<p>12.1 Model Problem: Coulomb&rsquo;s Law . . . . . . . . . . . . . . . . . . . . . . . . 239
</p>
<p>12.2 The Space of Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
</p>
<p>12.3 Distributional Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
</p>
<p>12.4 Fundamental Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
</p>
<p>12.5 Green&rsquo;s Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252
</p>
<p>12.6 Time-Dependent Fundamental Solutions . . . . . . . . . . . . . . . . . . . 257
</p>
<p>12.7 Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
</p>
<p>13 The Fourier Transform. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
</p>
<p>13.1 Fourier Transform. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
</p>
<p>13.2 Tempered Distributions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
</p>
<p>13.3 The Wave Kernel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
</p>
<p>13.4 The Heat Kernel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
</p>
<p>13.5 Exercises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
</p>
<p>Erratum to: Introduction to Partial Differential Equations . . . . . . . . . . . E1
</p>
<p>Appendix A: Analysis Foundations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
</p>
<p>Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283
</p>
<p>Contents xiii</p>
<p/>
</div>
<div class="page"><p/>
<p>Notations
</p>
<p>:&frac14; Equal by definition
� Equal except on a set of measure zero
� Asymptotic to (ratio approaches 1)
� Comparable to (ratio bounded above and below)
�k k Norm
�k kp L
</p>
<p>p norm
</p>
<p>�; �h i Inner product (L2 by default)
&eth;�; �&THORN; Distributional pairing
An Volume of the unit sphere Sn�1 � Rn
</p>
<p>B
n Open unit ball f xj j\1g � Rn
</p>
<p>B&eth;x0;R&THORN; Open ball f x� x0j j\Rg � R
n
</p>
<p>ck&frac12;�� Fourier coefficient
C Complex numbers
</p>
<p>Cm&eth;Ω&THORN; m-times continuously differentiable functions Ω ! C
Cm&eth;Ω;R&THORN; Real valued Cm functions
</p>
<p>Cm&eth;�Ω&THORN; Cm functions that admit extension across oΩ
Cm
cpt&eth;Ω&THORN; C
</p>
<p>m functions with compact support in Ω
</p>
<p>χA Characteristic (or indicator) function of a set A
</p>
<p>D Unit disk in R2
</p>
<p>Dα Multivariable derivative
</p>
<p>δx Dirac delta function
</p>
<p>oΩ Boundary of Ω
</p>
<p>D0&eth;Ω&THORN; Distributions on Ω
Df &frac12;�� Dirichlet energy
dS Surface integral element
</p>
<p>E Energy of a solution
F Fourier transform
Γ Gamma function
</p>
<p>r Gradient operator
Hm&eth;Ω&THORN; Sobolev space, functions with weak derivatives in L2 to order m
</p>
<p>xv</p>
<p/>
</div>
<div class="page"><p/>
<p>Hm
loc&eth;Ω&THORN; Local Sobolev functions
</p>
<p>H10&eth;Ω&THORN; Closure of C
1
cpt&eth;Ω&THORN; in H
</p>
<p>1&eth;Ω&THORN;
</p>
<p>Ht Heat kernel
</p>
<p>∆ Laplacian operator on Rn
</p>
<p>Lp&eth;Ω&THORN; p-th power integrable functions Ω ! C
</p>
<p>L1
loc&eth;Ω&THORN; Locally integrable functions Ω ! C
</p>
<p>&lsquo;p Discrete Lp space
N Natural numbers {1; 2; 3; . . .}
N0 Non-negative integers {0; 1; 2; 3; . . .}
ν Outward unit normal
</p>
<p>Ω Domain (open, connected set) in Rn
</p>
<p>�Ω Closure of Ω (Ω[ oΩ)
Φ Fundamental solution
</p>
<p>r xj j in Rn
</p>
<p>R Real numbers
</p>
<p>R&frac12;�� Rayleigh quotient
</p>
<p>S
n�1 Unit sphere f xj j &frac14; 1g � Rn
</p>
<p>S&eth;Rn&THORN; Schwartz functions (smooth functions with rapid decay)
</p>
<p>S0&eth;Rn&THORN; Tempered distributions
</p>
<p>Sn&frac12;�� Partial sum of Fourier series
T R=2πZ
Wt Wave kernel
</p>
<p>Z Integers {. . .;�1; 0; 1; . . .}
</p>
<p>xvi Notations</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 1
</p>
<p>Introduction
</p>
<p>1.1 Partial Differential Equations
</p>
<p>Continuous phenomena, such as wave propagation or fluid flow, are generally mod-
</p>
<p>eled with partial differential equations (PDE), which express relationships between
</p>
<p>rates of change with respect to multiple independent variables. In contrast, phenom-
</p>
<p>ena that can be described with a single independent variable, such as the motion
</p>
<p>of a rigid body in classical physics, are modeled by ordinary differential equations
</p>
<p>(ODE).
</p>
<p>A general PDE for a function u has the form
</p>
<p>F
</p>
<p>(
</p>
<p>x, u(x),
&part;u
</p>
<p>&part;x j
(x), . . . ,
</p>
<p>&part;mu
</p>
<p>&part;x j1 . . . &part;x jm
(x)
</p>
<p>)
</p>
<p>= 0. (1.1)
</p>
<p>The order of this equation is m, the order of the highest derivative appearing (which
</p>
<p>is assumed to be finite). A classical solution u admits continuous partial derivatives
</p>
<p>up to order m and satisfies (1.1) at all points x in its domain. In certain situations the
</p>
<p>differentiability requirements can be relaxed, allowing us to define weak solutions
</p>
<p>that do not solve the equation literally.
</p>
<p>A somewhat subtle aspect of the definition (1.1) is the fact that the equation
</p>
<p>is required to be local. This means that functions and derivatives appearing in the
</p>
<p>equation are all evaluated at the same point.
</p>
<p>Although classical physics provided the original impetus for the development
</p>
<p>of PDE theory, PDE models have since played a crucial role in many other fields,
</p>
<p>including engineering, chemistry, biology, ecology, medicine, and finance. Many
</p>
<p>industrial applications of mathematics are based on the numerical analysis of PDE.
</p>
<p>Most PDE are not solvable in the explicit sense that a simple calculus problem can
</p>
<p>be solved. That is, we typically cannot obtain a exact formula for u(x). Therefore
</p>
<p>much of the analysis of PDE is focused on drawing meaningful conclusions from an
</p>
<p>equation without actually writing down a solution.
</p>
<p>&copy; Springer International Publishing AG 2016
</p>
<p>D. Borthwick, Introduction to Partial Differential Equations,
</p>
<p>Universitext, DOI 10.1007/978-3-319-48936-0_1
</p>
<p>1</p>
<p/>
</div>
<div class="page"><p/>
<p>2 1 Introduction
</p>
<p>1.2 Example: d&rsquo;Alembert&rsquo;s Wave Equation
</p>
<p>One of the earliest and most influential PDE models was the wave equation, devel-
</p>
<p>oped by Jean d&rsquo;Alembert in 1746 to describe the motion of a vibrating string. With
</p>
<p>physical constants normalized to 1, the equation reads
</p>
<p>&part;2u
</p>
<p>&part;t2
&minus;
</p>
<p>&part;2u
</p>
<p>&part;x2
= 0, (1.2)
</p>
<p>where u(t, x) denotes the vertical displacement of the string at position x and time
</p>
<p>t . If the string has length ℓ and is attached at both ends, then we also require that
</p>
<p>u(t, 0) = u(t, ℓ) = 0 for all t . We will discuss the formulation of this model in
</p>
<p>Sect. 4.1.
</p>
<p>D&rsquo;Alembert also found a general formula for the solution of (1.2), based on the
</p>
<p>observation that (1.2) is solved by any function of the form f (x &plusmn; t), assuming f is
</p>
<p>twice-differentiable. Given two such functions on R, we can write a general solution
</p>
<p>u(t, x) := f1(x + t)+ f2(x &minus; t). (1.3)
</p>
<p>A similar formula applies in the case of a string with fixed ends. If f is 2ℓ-periodic
</p>
<p>on R, meaning f (x + 2ℓ) = f (x) for all x , then it is easy to check that
</p>
<p>u(t, x) :=
1
</p>
<p>2
[ f (x + t)&minus; f (t &minus; x)] (1.4)
</p>
<p>satisfies u(t, 0) = u(t, ℓ) = 0 for any t .
</p>
<p>One curious feature of this formula is that it appears to give a sensible solution
</p>
<p>even in cases where f is not differentiable. For example, to model a plucked string
</p>
<p>we might take the initial displacement to be a simple piecewise linear function in the
</p>
<p>form of a triangle from the fixed endpoints, as shown in Fig. 1.1.
</p>
<p>If we extend this to an odd, 2ℓ-periodic function on R, then the formula (1.4) yields
</p>
<p>the result illustrated in Fig. 1.2. The initial kink splits into two kinks which travel in
</p>
<p>opposite directions on the string and and appear to rebound from the fixed ends.
</p>
<p>This is not a classical solution because u is not differentiable at the kinks. However,
</p>
<p>u does satisfy the requirements for a weak solution, as we will see in Chap. 10.
</p>
<p>Although a physical string could not exhibit sharp corners without breaking, the
</p>
<p>piecewise linear solutions are nevertheless physically reasonable. Direct observations
</p>
<p>of plucked and bowed strings were first made in the late 19th century by Hermann
</p>
<p>von Helmholtz, who saw patterns of oscillation quite similar to what is shown in
</p>
<p>Fig. 1.2. The appearance of kinks propagating along the string is striking, although
</p>
<p>the corners are not exactly sharp.
</p>
<p>Fig. 1.1 Initial state of a
</p>
<p>plucked string</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_4">http://dx.doi.org/10.1007/978-3-319-48936-0_4</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_10">http://dx.doi.org/10.1007/978-3-319-48936-0_10</a></div>
</div>
<div class="page"><p/>
<p>1.3 Types of Equations 3
</p>
<p>Fig. 1.2 Evolution of the
</p>
<p>plucked string, starting from
</p>
<p>t = 0 at the top
</p>
<p>1.3 Types of Equations
</p>
<p>There is no general theory of PDE that allows us to analyze all equations of the form
</p>
<p>(1.1). To make progress it is necessary to restrict our attention to certain classes of
</p>
<p>equations and develop methods appropriate to those.
</p>
<p>The most fundamental distinction between PDE is the property of linearity. A
</p>
<p>PDE is called linear if it can be written in the form
</p>
<p>Lu = f, (1.5)
</p>
<p>where f is some function independent of u, and L is a differential operator. Many of
</p>
<p>the important classical PDE that we will discuss in this book are linear and of first
</p>
<p>or second order. For such cases L has the general form
</p>
<p>L = &minus;
</p>
<p>n
&sum;
</p>
<p>i, j=1
</p>
<p>ai j
&part;2
</p>
<p>&part;xi&part;x j
+
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>b j
&part;
</p>
<p>&part;x j
+ c, (1.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>4 1 Introduction
</p>
<p>where the coefficients ai j , b j , and c are functions of x. The second-order coefficients
</p>
<p>are assumed to be symmetric, ai j = a j i , because the mixed partials derivatives of a
</p>
<p>twice continuously differentiable function commute.
</p>
<p>Linearity implies that a linear combination of solutions is still a solution, a fact that
</p>
<p>is referred to as the superposition principle. Superposition often lets us decompose
</p>
<p>problems into simpler components, which is the main reason that linear problems
</p>
<p>are much easier to handle than nonlinear. It also makes it possible to work with
</p>
<p>complex-valued solutions, which is sometimes more convenient, because the real
</p>
<p>and imaginary parts of a complex solution will solve the equation independently.
</p>
<p>Most linear PDE are derived as approximations to more realistic, nonlinear mod-
</p>
<p>els. We will focus primarily on the linear case in this book. The main reason for this
</p>
<p>is that nonlinear PDE are inherently more complicated, and for an introduction it
</p>
<p>makes sense to start with the more basic theory. Furthermore, the analysis of nonlin-
</p>
<p>ear problems frequently involves the study of associated linear approximations, so
</p>
<p>that one must understand at least some of the linear theory first.
</p>
<p>Linear equations are further classified by the properties of the terms with the
</p>
<p>highest orders of derivatives, since this determines many qualitative properties of
</p>
<p>solutions. Elliptic equations of second order are associated to an operator L of the
</p>
<p>form (1.6), such that the eigenvalues of the symmetric matrix [ai j ] are strictly positive
</p>
<p>at each point in the domain. The prototype of an elliptic operator is L = &minus;� where
</p>
<p>� denotes the Laplacian,
</p>
<p>� :=
&part;2
</p>
<p>&part;x21
+ &middot; &middot; &middot; +
</p>
<p>&part;2
</p>
<p>&part;x2n
, (1.7)
</p>
<p>named after the mathematician and physicist Pierre-Simon Laplace.
</p>
<p>Equations that include time as an independent variable are called evolution equa-
</p>
<p>tions. The time variable usually plays a very different role from the spatial variables,
</p>
<p>so in such cases we adapt the form (1.6) by separating out the time derivatives
</p>
<p>explicitly.
</p>
<p>The two classic types of second-order evolution equations are hyperbolic and
</p>
<p>parabolic. Hyperbolic equations are exemplified by d&rsquo;Alembert&rsquo;s wave equation
</p>
<p>(1.2). The general form is (1.5) with
</p>
<p>L =
&part;2u
</p>
<p>&part;t2
&minus;
</p>
<p>n
&sum;
</p>
<p>i, j=1
</p>
<p>ai j (x)
&part;2u
</p>
<p>&part;xi x
2
j
</p>
<p>+ (lower order terms), (1.8)
</p>
<p>where once again [ai j ] is assumed to be a strictly positive matrix. Hyperbolic equa-
</p>
<p>tions are used to model oscillatory phenomena.
</p>
<p>Parabolic evolution equations have the form (1.5) with
</p>
<p>L =
&part;u
</p>
<p>&part;t
&minus;
</p>
<p>n
&sum;
</p>
<p>i, j=1
</p>
<p>ai j (x)
&part;2u
</p>
<p>&part;xi x
2
j
</p>
<p>+ (lower order terms), (1.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>1.3 Types of Equations 5
</p>
<p>where [ai j ] is a strictly positive matrix. The heat equation, whose derivation we will
</p>
<p>discuss in detail in Sect. 6.1, is the prototype for this type of equation. Parabolic
</p>
<p>equations are generally used to model phenomena of conduction and diffusion.
</p>
<p>Note that hyperbolic and parabolic equations revert to elliptic equations in the
</p>
<p>spatial variables if the solution is independent of time. Elliptic equations thus serve
</p>
<p>to model the equilibrium states of evolution equations.
</p>
<p>Because of their association with phenomenological properties of a system, the
</p>
<p>terms &ldquo;elliptic&rdquo;, &ldquo;hyperbolic&rdquo;, and &ldquo;parabolic&rdquo; are frequently applied more broadly
</p>
<p>than this simple classification would suggest. A nonlinear equation is typically
</p>
<p>described by the category of its linear approximations, which can change depending
</p>
<p>on the conditions.
</p>
<p>For problems on a bounded domain, the application usually dictates some restric-
</p>
<p>tion on the solutions at the boundary. Two very common types are Dirichlet bound-
</p>
<p>ary conditions, specifying the values of u at the boundary, and Neumann conditions,
</p>
<p>specifying the normal derivatives of u at the boundary. These conditions are named
</p>
<p>for Gustave Lejeune Dirichlet and Carl Neumann, respectively. By default we will
</p>
<p>use these terms in the homogeneous sense, meaning that the boundary values of the
</p>
<p>function or derivative are set equal to zero. For evolution equations, we also impose
</p>
<p>initial conditions, specifying the values of u and possibly its time derivatives at some
</p>
<p>initial time.
</p>
<p>1.4 Well Posed Problems
</p>
<p>The set of functions used to formulate a PDE, which might include coefficients or
</p>
<p>terms in the equation itself as well as boundary and initial conditions, is collectively
</p>
<p>referred to as the input data. The most basic question for any PDE is whether a
</p>
<p>solution exists for a given set of data. However, for most purposes we want to require
</p>
<p>something more. A PDE problem is said to be well posed if, for a given set of data:
</p>
<p>1. A solution exists.
</p>
<p>2. The solution is uniquely determined by the data.
</p>
<p>3. The solution depends continuously on the data.
</p>
<p>These criteria were formulated by Jacques Hadamard in 1902. The first two properties
</p>
<p>hold for ODE under rather general assumptions, but not necessarily for PDE. It is
</p>
<p>easy to find nonlinear equations that admit no solutions, and even in the linear case
</p>
<p>there is no guarantee.
</p>
<p>The third condition, continuous dependence on the input data, is sometimes called
</p>
<p>stability. One practical justification for this requirement is it is not possible to specify
</p>
<p>input data with absolute accuracy. Stability implies that the effects of small variations
</p>
<p>in the data can be controlled.
</p>
<p>For certain PDE, especially the classical linear cases, we have a good under-
</p>
<p>standing of the requirements for well-posedness. For other important problems, for</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_6">http://dx.doi.org/10.1007/978-3-319-48936-0_6</a></div>
</div>
<div class="page"><p/>
<p>6 1 Introduction
</p>
<p>Fig. 1.3 Numerical simulations of blood flow in the aorta. Courtesy of D. Gupta, Emory University
</p>
<p>Hospital, and T. Passerini, M. Piccinelli and A. Veneziani, Emory Mathematics and Computer
</p>
<p>Science
</p>
<p>example in fluid mechanics, well-posedness remains a difficult unsolved conjecture.
</p>
<p>Furthermore, many interesting problems are known not to be well posed. For exam-
</p>
<p>ple, problems in image processing are frequently ill posed, because information is
</p>
<p>lost due to noise or technological limitations.
</p>
<p>1.5 Approaches
</p>
<p>We can organize the methods for handling PDE problems according to three basic
</p>
<p>goals:
</p>
<p>1. Solving: finding explicit formulas for solutions.
</p>
<p>2. Analysis: understanding general properties of solutions.
</p>
<p>3. Approximation: calculating solutions numerically.
</p>
<p>Solving PDE is certainly worth understanding in those special cases where it is
</p>
<p>possible. The solution formulas available for certain classical PDE provide insight
</p>
<p>that is important to the development of the theory.
</p>
<p>The goals of theoretical analysis of PDE are extremely broad. We wish to learn
</p>
<p>as much as we can about the qualitative and quantitative properties of solutions and
</p>
<p>their relationship to the input data.
</p>
<p>Finally, numerical computation is the primary means by which applications of
</p>
<p>PDE are carried out. Computational methods rely on a foundation of theoretical
</p>
<p>analysis, but also bring up new considerations such as efficiency of calculation.
</p>
<p>Example 1.1 Figure 1.3 shows a set of numerical simulations modeling the insertion
</p>
<p>in the aorta of a pipe-like device designed to improve blood flow. The leftmost frame
</p>
<p>shows the aorta before surgery, and the three panes on the right model the insertion</p>
<p/>
</div>
<div class="page"><p/>
<p>1.5 Approaches 7
</p>
<p>at different locations. The PDE model is a complex set of fluid equations called the
</p>
<p>Navier-Stokes equations. These fluid equations are famously difficult to analyze and
</p>
<p>an exact solution is almost never possible. However, the cylinder is one case that can
</p>
<p>be handled explicitly. For the numerical simulations, exact solutions for a cylindrical
</p>
<p>pipe were used to provide boundary data at the point where the pipe meets the aorta.
</p>
<p>Theoretical analysis also plays an important role here, in that the regularity theory
</p>
<p>for the fluid equations is used to predict the accuracy of the simulation. (The complete
</p>
<p>well-posedness analysis of the Navier-Stokes equations remains a famously unsolved
</p>
<p>problem, however.)
</p>
<p>The simulated flows displayed in Fig. 1.3 were computed numerically by a tech-
</p>
<p>nique called the finite element method. This involves discretizing the problem to
</p>
<p>reduce the PDE to a system of linear algebraic equations. Modeling a single heart-
</p>
<p>beat in this simulation require solving a linear system of about 500 million equations.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 2
</p>
<p>Preliminaries
</p>
<p>In this chapter we set the stage for the study of PDE with a review of some core
background material.
</p>
<p>2.1 Real Numbers
</p>
<p>The real number system R is constructed as the &ldquo;completion&rdquo; of the field of rational
numbers. This means that in addition to the algebraic axioms for addition and mul-
tiplication, R satisfies an additional axiom related to the existence of limits. To state
this axiom we use the concept of the supremum (or &ldquo;least upper bound&rdquo;) of subset
A &sub; R. The supremum is a number sup(A) &isin; R such that (1) all elements of A are
less than or equal to sup(A); and (2) no number strictly less than sup(A) has this prop-
erty. The completeness axiom says that every nonempty subset of R that is bounded
above has a supremum. An equivalent statement is that a nonempty subset that is
bounded below has an infimum (&ldquo;greatest lower bound&rdquo;), which is denoted inf(A).
</p>
<p>It is convenient to extend these definitions to unbounded sets bydefining sup(A) :=
&infin; when A is not bounded above, and inf(A) := &minus;&infin; when the set is not bounded
below. We also set sup(&empty;) = &minus;&infin; and inf(&empty;) := +&infin;. With these extensions, sup
and inf are defined for all subsets of R.
</p>
<p>To illustrate the definition, we present a simple result that will prove useful in the
construction of approximating sequences for solutions of PDE.
</p>
<p>Lemma 2.1 For a nonempty set A &sub; R, there exists a sequence of points xk &isin; A
such that
</p>
<p>lim
k&rarr;&infin;
</p>
<p>xk = sup A,
</p>
<p>and similarly for inf A.
</p>
<p>The original version of the book was revised: Belated corrections from author have been incorpo-
rated. The erratum to the book is available at https://doi.org/10.1007/978-3-319-48936-0_14
</p>
<p>&copy; Springer International Publishing AG 2016
D. Borthwick, Introduction to Partial Differential Equations,
Universitext, DOI 10.1007/978-3-319-48936-0_2
</p>
<p>9</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-319-48936-0_14">https://doi.org/10.1007/978-3-319-48936-0_14</a></div>
</div>
<div class="page"><p/>
<p>10 2 Preliminaries
</p>
<p>Proof If A is not bounded above then there exists a sequence of xk &isin; Awith xk &rarr; &infin;.
Therefore the claim holds when sup A = &infin;.
</p>
<p>Now suppose that sup A = α &isin; R. By the definition of the supremum, α &minus; 1/k
is not an upper bound of A for k &isin; N. Therefore, for each k there exists xk &isin; A such
that α &minus; 1/k &lt; xk &le; α. This yields a sequence such that xk &rarr; α. �
</p>
<p>There is an important distinction between supremum and infimum and the related
concepts of maximum and minimum. The latter are required to be elements of the set
and thus may not exist. For example, the interval (0, 1) has sup = 1 and inf = 0, but
has neither max nor min.
</p>
<p>2.2 Complex Numbers
</p>
<p>The complex number system C consists of numbers of the form z = x + iy, where
x, y &isin; R and i2 := &minus;1. The numbers x and y are called the real and imaginary parts
of z. The conjugate of z is
</p>
<p>z̄ := x &minus; iy,
</p>
<p>so that
</p>
<p>Re z :=
z + z̄
2
</p>
<p>, Im z :=
z &minus; z̄
2i
</p>
<p>.
</p>
<p>A nonzero complex number has a multiplicative inverse, given by
</p>
<p>1
</p>
<p>x + iy
=
</p>
<p>x &minus; iy
x2 + y2
</p>
<p>.
</p>
<p>The absolute value on C is the vector absolute value from Euclidean R2,
</p>
<p>|z| :=
&radic;
</p>
<p>x2 + y2.
</p>
<p>This can be written in terms of conjugation,
</p>
<p>|z| =
&radic;
</p>
<p>zz̄,
</p>
<p>which shows in particular that the absolute value is multiplicative,
</p>
<p>|zw| := |z||w|
</p>
<p>for z, w &isin; C.
The basic theory of sequences and series carries over fromR toCwith only minor
</p>
<p>changes. A sequence {zk} in C converges to z if
</p>
<p>lim
k&rarr;&infin;
</p>
<p>|zk &minus; z| = 0,</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Complex Numbers 11
</p>
<p>and a series
&sum;
</p>
<p>ak converges if the sequence of partial sums
&sum;n
</p>
<p>k=1 ak is convergent.
The series converges absolutely if
</p>
<p>&infin;
&sum;
</p>
<p>k=1
|ak | &lt; &infin;.
</p>
<p>It follows from the completeness axiom of R that absolute convergence of a series
in C implies convergence.
</p>
<p>The exponential series,
</p>
<p>ez :=
&infin;
&sum;
</p>
<p>k=0
</p>
<p>zk
</p>
<p>k!
, (2.1)
</p>
<p>converges absolutely for all z &isin; C. The special case where z is purely imaginary
gives an important relation called Euler&rsquo;s formula:
</p>
<p>eiθ =
(
</p>
<p>1&minus;
θ2
</p>
<p>2!
+
</p>
<p>θ4
</p>
<p>4!
&minus; &middot; &middot; &middot;
</p>
<p>)
</p>
<p>+ i
(
</p>
<p>θ
</p>
<p>1!
&minus;
</p>
<p>θ3
</p>
<p>3!
+ &middot; &middot; &middot;
</p>
<p>)
</p>
<p>= cos θ + i sin θ. (2.2)
</p>
<p>Leonhard Euler, arguably the most influential mathematician of the 18th century,
published this identity in 1748. It yields a natural polar-coordinate representation of
complex numbers,
</p>
<p>z = reiθ ,
</p>
<p>where r = |z| and θ is the angle between z and the positive real axis.
The product rule for complex exponentials,
</p>
<p>ezew = ez+w, (2.3)
</p>
<p>follows from the power series definition just as in the real case. In combination with
(2.2) this allows for a very convenient manipulation of trigonometric functions. For
example, setting z = iα and w = iβ in (2.3) and taking the real and imaginary parts
recovers the identities
</p>
<p>cos(α + β) = cosα cosβ &minus; sin α sin β,
sin(α + β) = cosα sin β + sin α cosβ.
</p>
<p>The calculus rules for differentiating and integrating exponentials are derived
from the power series expansion, and thus extend to the complex case. In particular,
</p>
<p>d
</p>
<p>dx
eax = aeax
</p>
<p>for a &isin; C.</p>
<p/>
</div>
<div class="page"><p/>
<p>12 2 Preliminaries
</p>
<p>2.3 Domains in Rn
</p>
<p>For points in Rn we will use the vector notation x = (x1, . . . , xn). The Euclidean
dot product is denoted x &middot; y, and the Euclidean length of a vector is written
</p>
<p>|x| :=
&radic;
</p>
<p>x &middot; x.
</p>
<p>The Euclidean distance is used to define limits: limk&rarr;&infin; xk = w means that
</p>
<p>lim
k&rarr;&infin;
</p>
<p>|xk &minus; w| = 0.
</p>
<p>The ball of radius R &gt; 0 centered at a point x0 &isin; Rn is
</p>
<p>B(x0; R) :=
{
</p>
<p>x &isin; Rn; |x &minus; x0| &lt; R
}
</p>
<p>.
</p>
<p>A small ball centered at x0 is called a neighborhood of x0. If x &isin; A has a neighbor-
hood contained in A then x is called an interior point.
</p>
<p>A subsetU &sub; Rn is open if all of its points are interior. This generalizes the notion
of an open interval in one dimension. The ball B(x0; R) is open, for example, as is
R
</p>
<p>n itself. The empty set is open by default.
A boundary point of A &sub; Rn is a point x &isin; Rn such that every neighborhood of x
</p>
<p>intersects both A and its complement. The distinction between interior and boundary
points is illustrated in Fig. 2.1. Note that boundary points may or may not be included
in the set itself. The boundary of A is denoted
</p>
<p>&part;A := {boundary points of A} .
</p>
<p>For example, the boundary of the ball B(x0; R) is the sphere
</p>
<p>&part;B(x0; R) =
{
</p>
<p>x &isin; Rn; |x &minus; x0| = R
}
</p>
<p>.
</p>
<p>A set is open if and only if it contains no boundary points.
</p>
<p>Fig. 2.1 Open rectangle
in R2
</p>
<p>interior point
</p>
<p>boundary point</p>
<p/>
</div>
<div class="page"><p/>
<p>2.3 Domains in Rn 13
</p>
<p>A subset of Rn is connected if any two points in the set can be joined by a
continuous path within the set. For an open set U this is equivalent to the condition
that U cannot be written as the disjoint union of two nonempty open sets.
</p>
<p>We will refer to a connected open subset Ω &sub; Rn as a domain, and reserve the
notation Ω for this usage. For some problems we assume the domain is bounded,
meaning that Ω &sub; B(0; R) for sufficiently large R.
</p>
<p>The concept of a closed interval can also be generalized to higher dimension. A
subset F &sub; Rn is closed if it contains all of its boundary points, i.e.,
</p>
<p>&part;F &sub; F.
</p>
<p>The union of a subset A &sub; Rn with its boundary is called the closure of A and
denoted
</p>
<p>A := A &cup; &part;A.
</p>
<p>For example,
B(x0; R) :=
</p>
<p>{
</p>
<p>x &isin; Rn; |x &minus; x0| &le; R
}
</p>
<p>.
</p>
<p>It is potentially confusing that an overline is used for set closure and complex con-
jugation, but these notations are standard. Note that closure applies only to sets and
not to numbers or functions.
</p>
<p>A closed set F &isin; Rn contains the limits of all sequences in F that converge in
R
</p>
<p>n . This is because the limit of a sequence contained in a set must either be a point
in the set or a boundary point.
</p>
<p>Closed and open sets are related in the sense that the complement of an open set
is closed, and vice versa. However, the terms are not mutually exclusive, and a set
might not have either property. The interval (a, b] &sub; R is neither open nor closed,
for example. The only subsets of Rn with both properties are Rn itself and &empty;.
</p>
<p>2.4 Differentiability
</p>
<p>The space of continuous, complex-valued functions on a domain Ω &sub; Rn which
admit continuous partial derivatives up to orderm is denoted byCm(Ω). The assump-
tion of continuity for derivatives insures that mixed partials are independent of the
order of differentiation. A smooth function has continuous derivatives to all orders;
the corresponding space is written C&infin;(Ω).
</p>
<p>We use the notation Cm(Ω;R) to specify real-valued functions, and similarly
Cm(Ω;Rn) denotes the space of vector-valued functions. It is common to use Cm as
an adjective, short for &ldquo;m-times continuously differentiable&rdquo;.
</p>
<p>The definition of Cm(Ω) makes no conditions on the behavior of functions as
the boundary is approached. To impose such restrictions, we use the notation Cm(Ω)
to denote the space of functions that admit Cm extensions across the boundary. For
example, the function
</p>
<p>&radic;
x &isin; C&infin;(0, 1) is an element of C0[0, 1], but not C1[0, 1].</p>
<p/>
</div>
<div class="page"><p/>
<p>14 2 Preliminaries
</p>
<p>The support of f &isin; C0(Ω) is defined as
</p>
<p>supp f := {x &isin; Ω; f (x) 
= 0}. (2.4)
</p>
<p>Note that the definition includes a closure. This means that the support does not
exclude points where the function merely &ldquo;passes through&rdquo; zero. For example, the
support of sin(x) is R rather than R\πZ.
</p>
<p>A closed and bounded subset of Rn is said to be compact. We denote by Cmcpt(Ω)
the space of functions on Ω that have compact support, meaning that supp f is a
compact subset of Ω . Since Ω is open and the support is closed, this requires in
particular that supp f be a strict subset of Ω . For example, 1 &minus; x2 vanishes at the
boundary of (&minus;1, 1), but does not have compact support in this domain because its
support is [&minus;1, 1].
</p>
<p>Example 2.2 To demonstrate the existence of compactly supported smooth func-
tions, consider
</p>
<p>h(x) =
{
</p>
<p>e&minus;1/(1&minus;x
2), |x | &lt; 1,
</p>
<p>0, |x | &ge; 1,
</p>
<p>which has support [&minus;1, 1]. As illustrated in Fig. 2.2, the function becomes extremely
flat as x &rarr; &plusmn;1.
</p>
<p>To show that h is in fact smooth, we note that
</p>
<p>h(m)(x) =
{
</p>
<p>qm (x)
</p>
<p>(1&minus;x2)m e
&minus;1/(1&minus;x2), |x | &lt; 1,
</p>
<p>0, |x | &ge; 1,
</p>
<p>where qm denotes a polynomial of degree m. As x &rarr; &plusmn;1, the term (1&minus; x2)&minus;m blows
up while the exponential term tends rapidly to zero. Using l&rsquo;H&ocirc;pital&rsquo;s rule one can
check that the exponential dominates this limit, so that all derivatives of h vanish as
x &rarr; &plusmn;1 from |x | &le; 1. This shows that h &isin; C&infin;cpt(R).
</p>
<p>The function h can be integrated to produce a smooth function that is constant for
|x | &ge; 1. By translating and rescaling, this construction gives, for a &lt; b, a function
ϕ &isin; C&infin;(R) satisfying
</p>
<p>ϕ(x) =
{
</p>
<p>0, x &le; a,
1, x &ge; b.
</p>
<p>Fig. 2.2 Compactly
supported smooth function</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Differentiability 15
</p>
<p>These &ldquo;smooth step functions&rdquo; are useful as building blocks for pasting together
smooth functions on different domains. &diams;
</p>
<p>Certain problems require regularity of the boundary of Ω . For example, many
theorems in vector calculus require the existence of a normal vector on the boundary,
which does not exist for a general domain. A standard hypothesis for such theorems
is that &part;Ω is piecewiseC1. This means that &part;Ω consists of a finite number of compo-
nentswhich admit regular coordinate parametrization. A coordinate parametrization
is a map σ &isin; C1(U;Rn) where U is a domain in Rn&minus;1. To say the parametrization is
regular means that the tangent vectors defined by
</p>
<p>&part;σ
</p>
<p>&part;w j
:=
</p>
<p>(
</p>
<p>&part;σ1
</p>
<p>&part;w j
, . . . ,
</p>
<p>&part;σn
</p>
<p>&part;w j
</p>
<p>)
</p>
<p>,
</p>
<p>j = 1, . . . , n &minus; 1, are linearly independent at each point of &part;Ω .
The piecewise C1 boundary assumption guarantees that the points of each bound-
</p>
<p>ary component have well-defined tangent spaces and normal directions. As an exam-
ple, the unit cube inR3 has piecewiseC1 boundary, consisting of 6 planar components
with normal directions parallel to the coordinate axes.
</p>
<p>In this textwewill focus on relatively simple domainswith straightforward bound-
ary parametrizations.
</p>
<p>2.5 Ordinary Differential Equations
</p>
<p>Our development of PDE theory will not rely on any advanced techniques for the
solution of ODE, but it will be useful to recall some basic material.
</p>
<p>First-order ODE can often be solved directly by methods from calculus. The
easiest cases are equations of the form
</p>
<p>dy
</p>
<p>dt
= g(y)h(t),
</p>
<p>where the variables can be separated to yield an integral formula
</p>
<p>&int;
</p>
<p>dy
</p>
<p>g(y)
=
</p>
<p>&int;
</p>
<p>dt
</p>
<p>h(t)
.
</p>
<p>Integrating both sides yields a family of solutions with one undetermined constant.</p>
<p/>
</div>
<div class="page"><p/>
<p>16 2 Preliminaries
</p>
<p>Example 2.3 Consider the equation
</p>
<p>dy
</p>
<p>dt
= ay, y(0) = y0.
</p>
<p>for a 
= 0. This is called the growth or decay equation, depending on the sign of a.
Separating the variables gives
</p>
<p>&int;
</p>
<p>dy
</p>
<p>y
=
</p>
<p>&int;
</p>
<p>a dt,
</p>
<p>which integrates to ln y = at +C . Solving for y and using the initial condition gives
</p>
<p>y(t) = y0eat .
</p>
<p>&diams;
</p>
<p>Higher-order ODE are generally analyzed by reducing to a system of first-order
equations. To reduce the nth-order equation
</p>
<p>y
(n)(t) = F
</p>
<p>(
</p>
<p>t, y, y&prime;, . . . , y(n&minus;1)
)
</p>
<p>, (2.5)
</p>
<p>we define the vector-valued function w = (y, y&prime;, . . . , y(n&minus;1)). This satisfies the first-
order system
</p>
<p>d
</p>
<p>dt
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>w1
...
</p>
<p>wn&minus;1
wn
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>=
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>w2
...
</p>
<p>wn
F(t,w)
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>First-order systems can be solved generally by the strategy of Picard iteration,
named for mathematician &Eacute;mile Picard. The first step is to write the vector equation,
</p>
<p>dw
</p>
<p>dt
= F(t,w), w(t0) = w0, (2.6)
</p>
<p>in an equivalent form as a recursive integral equation,
</p>
<p>w(t) = w0 +
&int; t
</p>
<p>t0
</p>
<p>F(s,w(s)) ds.
</p>
<p>For the construction, we set u0(t) := w0 and define a sequence of functions by
</p>
<p>uk(t) = w0 +
&int; t
</p>
<p>t0
</p>
<p>F(s, uk&minus;1(s)) ds (2.7)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.5 Ordinary Differential Equations 17
</p>
<p>for k = 1, 2, . . . . It can be shown that the limit of this sequence exists and solves
(2.6) under some general assumptions on F, which leads to a proof of the following
result.
</p>
<p>Theorem 2.4 (Picard iteration) Suppose that F is a continuous function on I &times;Ω
where I is an open interval containing t0 andΩ is a domain in R
</p>
<p>n containingw0, and
</p>
<p>that F is continuously differentiable with respect to w. Then (2.6) admits a unique
solution on some interval (t0 &minus; ε, t0 + ε) with ε &gt; 0.
</p>
<p>Applying Theorem 2.4 to (2.5) shows that an nth order ODE satisfying the reg-
ularity assumptions has a unique local solution specified by the initial values of the
function and its first n &minus; 1 derivatives.
</p>
<p>TheC1 hypothesis on F is stronger than necessary, but this version will suffice for
our purposes. The point we would like to stress here is the relative ease with which
ODE can be analyzed under very general conditions. This is very different from the
PDE theory, where no such general results are possible.
</p>
<p>Example 2.5 The harmonic ODE is the equation
</p>
<p>d2y
</p>
<p>dt2
= &minus;κ2y,
</p>
<p>for κ &gt; 0. In view of the solution to the growth/decay equation in Example 2.3, it is
reasonable to start with an exponential solution as a guess. Substituting eαt into the
equation yields α2 = &minus;κ2. From α = &plusmn;iκ we obtain the general solution,
</p>
<p>y(t) = c1eiκt + c2e&minus;iκt .
</p>
<p>To see how this relates to the Picard iteration method described above, consider
the corresponding system (2.6) for w = (y, y&prime;):
</p>
<p>dw
</p>
<p>dt
=
</p>
<p>(
</p>
<p>0 1
&minus;κ2 0
</p>
<p>)
</p>
<p>w.
</p>
<p>With w0 = (a, b), the recursive formula (2.7) yields the sequence of functions
</p>
<p>uk(t) =
k
</p>
<p>&sum;
</p>
<p>j=0
</p>
<p>t j
</p>
<p>j !
</p>
<p>(
</p>
<p>0 1
&minus;κ2 0
</p>
<p>)j (
</p>
<p>a
</p>
<p>b
</p>
<p>)
</p>
<p>for k &isin; N. In the limit k &rarr; &infin; this gives
</p>
<p>w(t) =
[
</p>
<p>1&minus;
(κt)2
</p>
<p>2!
+
</p>
<p>(κt)4
</p>
<p>4!
&minus; &middot; &middot; &middot;
</p>
<p>](
</p>
<p>a
</p>
<p>b
</p>
<p>)
</p>
<p>+
1
</p>
<p>κ
</p>
<p>[
</p>
<p>κt &minus;
(κt)3
</p>
<p>3!
+
</p>
<p>(κt)5
</p>
<p>5!
&minus; &middot; &middot; &middot;
</p>
<p>](
</p>
<p>b
</p>
<p>&minus;κ2a
</p>
<p>)
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>18 2 Preliminaries
</p>
<p>Reading off y as the first component of w gives the familiar trigonometric solution,
</p>
<p>y(t) = a cos(κt)+
b
</p>
<p>κ
sin(κt).
</p>
<p>The trigonometric and complex exponential solutions are related by Euler&rsquo;s formula
(2.2). &diams;
</p>
<p>2.6 Vector Calculus
</p>
<p>The classical theorems of vector calculus were motivated by PDE problems arising
in physics. For our purposes the most important of these results is the divergence
theorem. We assume that the reader is familiar with the divergence theorem in the
context of R2 or R3. In this section we will cover the basic definitions needed to state
the result in Rn and develop its corollaries.
</p>
<p>As noted in Sect. 2.3, we always take a domain Ω &sub; Rn to be connected and
open. The gradient of f &isin; C1(Ω) is the vector-valued function
</p>
<p>&nabla;f :=
(
</p>
<p>&part; f
</p>
<p>&part;x1
, . . . ,
</p>
<p>&part; f
</p>
<p>&part;xn
</p>
<p>)
</p>
<p>.
</p>
<p>For a vector-valued function v &isin; C1(Ω;Rn) with components (v1, . . . , vn), the
divergence is
</p>
<p>&nabla; &middot; v :=
&part;v1
</p>
<p>&part;x1
+ &middot; &middot; &middot; +
</p>
<p>&part;vn
</p>
<p>&part;xn
.
</p>
<p>The Laplacian operator introduced in (1.7) is the divergence of the gradient
</p>
<p>�u := &nabla; &middot; (&nabla;u).
</p>
<p>For this reason � is sometimes written &nabla;2.
IfΩ is bounded then the Riemannian integral of f &isin; C0(Ω) exists and is denoted
</p>
<p>by
&int;
</p>
<p>Ω
</p>
<p>f (x) dn x,
</p>
<p>where dn x is a shorthand for dx1 &middot; &middot; &middot; dxn . The integral can be extended to unbounded
domains if the appropriate limits exist. We will discuss a further generalization of
the Riemann definition in Chap.7.
</p>
<p>One issue we will come across frequently is differentiation under the integral.
If Ω &sub; Rn is a bounded domain and u and &part;u/&part;t are continuous functions on
(a, b)&times;Ω, then the Leibniz integral rule says that</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_1">http://dx.doi.org/10.1007/978-3-319-48936-0_1</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
</div>
<div class="page"><p/>
<p>2.6 Vector Calculus 19
</p>
<p>d
</p>
<p>dt
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>u(t, x) dn x =
&int;
</p>
<p>Ω
</p>
<p>&part;u
</p>
<p>&part;t
(t, x) dn x.
</p>
<p>Differentiation under the integral may still work when the integrals are improper, but
this requires greater care.
</p>
<p>To set up boundary integrals for a domain with piecewise C1 boundary, we need
to define the surface integral over a regular coordinate patch σ : U &sub; Rn&minus;1 &rarr; &part;Ω .
Let ν : U &rarr; Rn denote the unit normal vector pointing outwards from the domain.
The surface integral for such a patch is defined by
</p>
<p>&int;
</p>
<p>σ (U )
</p>
<p>f d S :=
&int;
</p>
<p>U
</p>
<p>f (σ (w))
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>det
</p>
<p>[
</p>
<p>&part;σ
</p>
<p>&part;w1
, . . . ,
</p>
<p>&part;σ
</p>
<p>&part;wn&minus;1
, ν
</p>
<p>]∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>dn&minus;1w, (2.8)
</p>
<p>where det[. . . ]denotes the determinant of amatrix of columnvectors. The full surface
integral over &part;Ω is defined by summing over the boundary coordinate patches. For
simplicity, we notate this as a single integral,
</p>
<p>&int;
</p>
<p>&part;Ω
</p>
<p>f d S.
</p>
<p>In R2, a boundary parametrization will be a curve σ (t) and (2.8) reduces to the
arclength integral
</p>
<p>&int;
</p>
<p>σ (U )
</p>
<p>f d S :=
&int;
</p>
<p>U
</p>
<p>f (σ (t))
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>dσ
</p>
<p>dt
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>dt.
</p>
<p>In R3, the unit normal for a surface patch can be computed from the cross product
of the tangent vectors. This leads to the surface integral formula
</p>
<p>&int;
</p>
<p>σ (U )
</p>
<p>f d S :=
&int;
</p>
<p>U
</p>
<p>f (σ (w))
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&part;σ
</p>
<p>&part;w1
&times;
</p>
<p>&part;σ
</p>
<p>&part;w2
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>d2w.
</p>
<p>Even in low dimensions surface integrals can be rather complicated.Wewill make
explicit use of these formulas only in relatively simple cases, such as rectangular
regions and spheres.
</p>
<p>We can use (2.8) to decompose integrals into radial and spherical components.
This is particularly useful when the domain is a ball. Let r := |x| be the radial
coordinate, and define the unit sphere
</p>
<p>S
n&minus;1 := {r = 1} &sub; Rn.
</p>
<p>A point x 
= 0 can be written uniquely as rω for ω &isin; Sn&minus;1 and r &gt; 0. Let ω(y) be a
parametrization of Sn&minus;1 by coordinates y &isin; U &sub; Rn&minus;1. For the change of variables
(r,y) �&rarr; x = rω(y), the Jacobian formula gives</p>
<p/>
</div>
<div class="page"><p/>
<p>20 2 Preliminaries
</p>
<p>dn x =
∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>det
</p>
<p>[
</p>
<p>&part;x
</p>
<p>&part;y1
, . . . ,
</p>
<p>&part;x
</p>
<p>&part;yn&minus;1
,
&part;x
</p>
<p>&part;r
</p>
<p>]
∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>dr dy1 &middot; &middot; &middot; dyn&minus;1 (2.9)
</p>
<p>=
∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>det
</p>
<p>[
</p>
<p>&part;ω
</p>
<p>&part;y1
, . . . ,
</p>
<p>&part;ω
</p>
<p>&part;yn&minus;1
,ω
</p>
<p>]
∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>rn&minus;1 dr dy1 &middot; &middot; &middot; dyn&minus;1.
</p>
<p>On the unit sphere, the outward unit normal ν is equal to ω. Thus (2.9) reduces to
</p>
<p>dn x = rn&minus;1 dr d S(y).
</p>
<p>For an integral over the ball this yields the radial integral formula,
</p>
<p>&int;
</p>
<p>B(0;R)
f (x) dn x =
</p>
<p>&int;
</p>
<p>Sn&minus;1
</p>
<p>&int; R
</p>
<p>0
f (rω(y)) rn&minus;1dr d S(y). (2.10)
</p>
<p>With these definitions in place, we turn to the divergence theorem, which relates
the flux of a vector field through a closed surface to the divergence of the field in the
interior. This result is generally attributed to Carl Friedrich Gauss, who published a
version in 1813 in conjunction with his work on electrostatics.
</p>
<p>Theorem 2.6 (Divergence theorem) Suppose Ω &sub; Rn is a bounded domain with
piecewise C1 boundary. For a vector field F &isin; C1(Ω;Rn),
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>&nabla; &middot; F dn x =
&int;
</p>
<p>&part;Ω
</p>
<p>F &middot; ν d S,
</p>
<p>where ν is the outward unit normal to &part;Ω .
</p>
<p>A full proof can be found in advanced calculus texts. To illustrate the idea, we
will show how the argument works for a spherical domain in R3.
</p>
<p>Example 2.7 Let B3 = {r &lt; 1} &sub; R3. Because a vector field can be decomposed
into components, it suffices to consider a field parallel to one of the coordinate axes,
say F = (0, 0, f ). The divergence is then
</p>
<p>&nabla; &middot; F =
&part; f
</p>
<p>&part;x3
.
</p>
<p>In cylindrical coordinates, x = (ρ cos θ, ρ sin θ, z), the volume element is
</p>
<p>d3x = ρ dρ dφ dz,
</p>
<p>so the left side of the divergence formula becomes</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 Vector Calculus 21
</p>
<p>&int;
</p>
<p>B3
</p>
<p>&nabla; &middot; F d3x =
&int; 2π
</p>
<p>0
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>&int;
</p>
<p>&radic;
1&minus;ρ2
</p>
<p>&minus;
&radic;
</p>
<p>1&minus;ρ2
</p>
<p>&part; f
</p>
<p>&part;z
ρ dz dρ dθ
</p>
<p>=
&int; 2π
</p>
<p>0
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>[
</p>
<p>f
(
</p>
<p>ρ cos θ, ρ sin θ,
&radic;
</p>
<p>1&minus; ρ2
)
</p>
<p>(2.11)
</p>
<p>&minus; f
(
</p>
<p>ρ cos θ, ρ sin θ,&minus;
&radic;
</p>
<p>1&minus; ρ2
)]
</p>
<p>ρ dρ dθ.
</p>
<p>Note that z = &plusmn;
&radic;
</p>
<p>1&minus; ρ2 gives the restriction to the upper and lower hemispheres,
respectively.
</p>
<p>We denote the two hemispheres S2&plusmn; &sub; S2 and parametrize them as
</p>
<p>ω&plusmn;(ρ, θ) =
(
</p>
<p>ρ cos θ, ρ sin θ,&plusmn;
&radic;
</p>
<p>1&minus; ρ2
)
</p>
<p>.
</p>
<p>The corresponding surface area elements are given by
</p>
<p>d S =
∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&part;ω&plusmn;
</p>
<p>&part;ρ
&times;
</p>
<p>&part;ω&plusmn;
</p>
<p>&part;θ
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>dρ dθ
</p>
<p>=
ρ
</p>
<p>&radic;
</p>
<p>1&minus; ρ2
dρ dθ.
</p>
<p>Thus,
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>&int; 1
</p>
<p>0
f
(
</p>
<p>ρ cos θ, ρ sin θ,&plusmn;
&radic;
</p>
<p>1&minus; ρ2
)
</p>
<p>ρ dρ dθ =
&int;
</p>
<p>S
2
&plusmn;
</p>
<p>f
&radic;
</p>
<p>1&minus; ρ2 d S.
</p>
<p>On S2&plusmn; we have F &middot; ν = &plusmn; f
&radic;
</p>
<p>1&minus; ρ2, so that
&int;
</p>
<p>S
2
&plusmn;
</p>
<p>f
&radic;
</p>
<p>1&minus; ρ2 d S = &plusmn;
&int;
</p>
<p>S
2
&plusmn;
</p>
<p>F &middot; ν d S.
</p>
<p>Applying this to (2.11) reduces the equation to
</p>
<p>&int;
</p>
<p>B3
</p>
<p>&nabla; &middot; F d3x =
&int;
</p>
<p>S2
</p>
<p>F &middot; ν d S,
</p>
<p>verifying the divergence theorem in this special case. &diams;
</p>
<p>Theorem 2.6 can be used to evaluate integrals of the Laplacian of a function by
substituting F = &nabla;u for the vector field. Inside the volume integral this yields the
integrand
</p>
<p>&nabla; &middot; F = �u.
</p>
<p>On the surface side, the integrand becomes the directional derivative with respect to
the outward unit normal, which is denoted</p>
<p/>
</div>
<div class="page"><p/>
<p>22 2 Preliminaries
</p>
<p>&part;u
</p>
<p>&part;ν
:= ν &middot; &nabla;u
</p>
<p>∣
</p>
<p>∣
</p>
<p>&part;Ω
.
</p>
<p>Corollary 2.8 If Ω &sub; Rn is a bounded domain with piecewise C1 boundary, and
u &isin; C2(Ω), then
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>�u dn x =
&int;
</p>
<p>&part;Ω
</p>
<p>&part;u
</p>
<p>&part;ν
d S.
</p>
<p>The application we will encounter most frequently is to the ball B(0; R) &isin; Rn .
The outward unit normal is parallel to the position vector, so that
</p>
<p>ν =
x
</p>
<p>R
.
</p>
<p>It follows from the chain rule that
</p>
<p>&part;u
</p>
<p>&part;ν
=
</p>
<p>&part;u
</p>
<p>&part;r
. (2.12)
</p>
<p>Example 2.9 Consider a radial function g(r) where r := |x| for x &isin; Rn . For the
ball B(0; a), the radial integral formula (2.10) gives
</p>
<p>&int;
</p>
<p>B(0;a)
�g dn x = An
</p>
<p>&int; a
</p>
<p>0
�g(r)rn&minus;1 dr,
</p>
<p>where
An := vol(Sn&minus;1). (2.13)
</p>
<p>By (2.12),
</p>
<p>&int;
</p>
<p>&part;B(0;a)
</p>
<p>&part;g
</p>
<p>&part;ν
d S =
</p>
<p>&int;
</p>
<p>&part;B(0;a)
</p>
<p>&part;g
</p>
<p>&part;r
(a) d S
</p>
<p>= Anan&minus;1
&part;g
</p>
<p>&part;r
(a).
</p>
<p>The formula from Corollary 2.8 reduces in this case to
</p>
<p>&int; a
</p>
<p>0
�g(r)rn&minus;1 dr = an&minus;1
</p>
<p>&part;g
</p>
<p>&part;r
(a). (2.14)
</p>
<p>Differentiating (2.14) with respect to a gives, by the fundamental theorem of
calculus,
</p>
<p>an&minus;1�g(a) =
&part;
</p>
<p>&part;a
</p>
<p>[
</p>
<p>an&minus;1
&part;g
</p>
<p>&part;r
(a)
</p>
<p>]
</p>
<p>.
</p>
<p>This holds for all a &gt; 0, so evidently the Laplacian of a radial function is given by</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 Vector Calculus 23
</p>
<p>�g = r1&minus;n
&part;
</p>
<p>&part;r
</p>
<p>[
</p>
<p>rn&minus;1
&part;
</p>
<p>&part;r
</p>
<p>]
</p>
<p>g. (2.15)
</p>
<p>In principle one could derive this formula directly from the chain rule, but the direct
computation is difficult in high dimensions. &diams;
</p>
<p>There are two other direct corollaries of the divergence theorem that will be used
frequently. These are named for the mathematical physicist George Green, who used
them to develop solution formulas for some classical PDE.
</p>
<p>The first result is a generalization of Corollary 2.8, obtained from Theorem 2.6
by the substitution F = v&nabla;u for a pair of functions u, v. The product rule for
differentiation gives
</p>
<p>&nabla; &middot; (v&nabla;u) = &nabla;v &middot; &nabla;u + v�u, (2.16)
</p>
<p>which can easily be checked by writing out the components of the gradient.
</p>
<p>Theorem 2.10 (Green&rsquo;s first identity) If Ω &sub; Rn is a bounded domain with piece-
wise C1 boundary, then for u &isin; C2(Ω), and v &isin; C1(Ω),
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>[&nabla;v &middot; &nabla;u + v�u] dn x =
&int;
</p>
<p>&part;Ω
</p>
<p>v
&part;u
</p>
<p>&part;ν
d S.
</p>
<p>The second identity follows from the first by interchanging u with v and then
subtracting the result.
</p>
<p>Theorem 2.11 (Green&rsquo;s second identity) If Ω &sub; Rn is a bounded domain with
piecewise C1 boundary, then for u, v &isin; C2(Ω),
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>[v�u &minus; u�v] dn x =
&int;
</p>
<p>&part;Ω
</p>
<p>(
</p>
<p>v
&part;u
</p>
<p>&part;ν
&minus; u
</p>
<p>&part;v
</p>
<p>&part;ν
</p>
<p>)
</p>
<p>d S.
</p>
<p>2.7 Exercises
</p>
<p>2.1 For r := |x| in Rn , and α &isin; R, compute &nabla;(rα) and �(rα).
</p>
<p>2.2 Polar coordinates (r, θ) in R2 are related to Cartesian coordinates (x1, x2) by
</p>
<p>x1 = r cos θ, x2 = r sin θ.
</p>
<p>(a) Use the chain rule to compute &part;
&part;r
</p>
<p>and &part;
&part;θ
</p>
<p>in terms of &part;
&part;x1
</p>
<p>and &part;
&part;x2
</p>
<p>.
(b) Find the expression for� in the (r, θ) coordinates. (The radial part should agree
</p>
<p>with (2.15).)
</p>
<p>2.3 In Rn let Ω be the unit cube (0, 1)n . Define
</p>
<p>w(x) = f (x)e j ,</p>
<p/>
</div>
<div class="page"><p/>
<p>24 2 Preliminaries
</p>
<p>where f &isin; C&infin;(Ω) and e j is the j th coordinate vector, e j := (0, . . . , 1, . . . , 0).
Compute both sides of the formula from Theorem 2.6 in this case, and show that
the result reduces to an application of the fundamental theorem of calculus in the x j
variable.
</p>
<p>2.4 For f &isin; C0(Rn) set
h(t) :=
</p>
<p>&int;
</p>
<p>B(0;t)
f (x) dn x.
</p>
<p>for t &ge; 0. Use the radial decomposition formula (2.10) to show that
</p>
<p>dh
</p>
<p>dt
=
</p>
<p>&int;
</p>
<p>&part;B(0;t)
f (w) d S(w).
</p>
<p>2.5 The gamma function is defined for z &gt; 0 by
</p>
<p>Ŵ(z) :=
&int; &infin;
</p>
<p>0
t z&minus;1e&minus;t dt. (2.17)
</p>
<p>Note that Ŵ(1) = 1 and integration by parts gives the recursion relation Ŵ(z + 1) =
zŴ(z). In this problem we will show that the volume of the unit sphere in Rn is given
by
</p>
<p>An =
2π
</p>
<p>n
2
</p>
<p>Ŵ( n2 )
. (2.18)
</p>
<p>(a) Use the radial formula (2.10) and the substitution u := r2 to compute that
&int;
</p>
<p>Rn
</p>
<p>e&minus;r
2
</p>
<p>dn x =
1
</p>
<p>2
AnŴ(
</p>
<p>n
2 ). (2.19)
</p>
<p>(b) Observe that we can rewrite
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>e&minus;r
2
</p>
<p>dn x =
&int;
</p>
<p>Rn
</p>
<p>e&minus;(x
2
1+&middot;&middot;&middot;+x2n ) dn x
</p>
<p>=
[
</p>
<p>2
&int; &infin;
</p>
<p>0
e&minus;x
</p>
<p>2
dx
</p>
<p>]n
</p>
<p>.
</p>
<p>Substitute t = x2 to evaluate the one-dimensional integral in terms of Ŵ( 12 ).
(c) Compare (a) to (b) to obtain a formula for An .
(d) Use (c) and the fact that A2 = 2π to compute Ŵ( 12 ) and reduce the formula to
</p>
<p>(2.18).</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 3
</p>
<p>Conservation Equations and Characteristics
</p>
<p>A conservation law for a physical system states that a certain quantity (e.g., mass,
</p>
<p>energy, or momentum) is independent of time. For continuous systems such as fluids
</p>
<p>or gases, these global quantities can be defined as integrals of density functions. The
</p>
<p>conservation law then translates into a local form, as a PDE for the density function.
</p>
<p>In this section we will study some first-order PDE that arise from conservation
</p>
<p>laws. We introduce a classic technique, called the method of characteristics, for
</p>
<p>analyzing these equations.
</p>
<p>3.1 Model Problem: Oxygen in the Bloodstream
</p>
<p>To derive the conservation equation, we consider a simple model for the concentration
</p>
<p>of oxygen carried by the bloodstream. For this discussion we ignore any external
</p>
<p>effects that might break the conservation of mass, such as absorption of oxygen into
</p>
<p>the walls of a blood vessel. (Some examples of external effects will be considered in
</p>
<p>the exercises.)
</p>
<p>Let us model an artery as a straight tube, as pictured in Fig. 3.1. We assume that the
</p>
<p>concentration is constant on cross-sections of the tube, so that the problem reduces
</p>
<p>to one spatial dimension. For the moment, suppose that the artery extends along the
</p>
<p>real line and is parametrized by x &isin; R.
Let u(t, x) denote the oxygen concentration, expressed in units of mass per unit
</p>
<p>length. Within a fixed interval [a, b], as highlighted in Fig. 3.1, the total mass at time
t is given by an integral,
</p>
<p>m(t) :=
&int; b
</p>
<p>a
</p>
<p>u(t, x) dx . (3.1)
</p>
<p>The original version of the book was revised: Belated corrections from author have been incorpo-
</p>
<p>rated. The erratum to the book is available at https://doi.org/10.1007/978-3-319-48936-0_14
</p>
<p>&copy; Springer International Publishing AG 2016
</p>
<p>D. Borthwick, Introduction to Partial Differential Equations,
</p>
<p>Universitext, DOI 10.1007/978-3-319-48936-0_3
</p>
<p>25</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-319-48936-0_14">https://doi.org/10.1007/978-3-319-48936-0_14</a></div>
</div>
<div class="page"><p/>
<p>26 3 Conservation Equations and Characteristics
</p>
<p>a b
x
</p>
<p>Fig. 3.1 One-dimensional model of an artery
</p>
<p>The instantaneous flow rate at a given point x is called the flux q(t, x), expressed
</p>
<p>as mass per unit time. The general relationship between flux and concentration is
</p>
<p>flux = (concentration)&times; (velocity).
</p>
<p>For the bloodstream model we can reasonably assume that velocity is independent
</p>
<p>of the oxygen concentration (because oxygen accounts for a relatively small portion
</p>
<p>of the total density). This assumption implies that q has a linear dependence on u. In
</p>
<p>other models the velocity might depend on the concentration, making q a nonlinear
</p>
<p>function of u.
</p>
<p>Conservation of mass implies that the total amount of oxygen within the segment
</p>
<p>changes only as oxygen flows across the boundary points at x = a and x = b. Since
the flow across these points is given by the flux, the corresponding equation is
</p>
<p>dm
</p>
<p>dt
(t) = q(t, a)&minus; q(t, b). (3.2)
</p>
<p>If q is continuously differentiable with respect to position, then the fundamental
</p>
<p>theorem of calculus allows us to write the right-hand side of (3.2) as an integral,
</p>
<p>q(t, a)&minus; q(t, b) = &minus;
&int; b
</p>
<p>a
</p>
<p>&part;q
</p>
<p>&part;x
dx .
</p>
<p>We can also differentiate the integral in (3.1) to obtain
</p>
<p>dm
</p>
<p>dt
=
</p>
<p>&int; b
</p>
<p>a
</p>
<p>&part;u
</p>
<p>&part;t
dx,
</p>
<p>provided that u(t, x) is continuously differentiable with respect to time. These cal-
</p>
<p>culations transform (3.2) into the integral equation
</p>
<p>&int; b
</p>
<p>a
</p>
<p>(
</p>
<p>&part;u
</p>
<p>&part;t
+
</p>
<p>&part;q
</p>
<p>&part;x
</p>
<p>)
</p>
<p>dx = 0. (3.3)
</p>
<p>Since the segment was arbitrary, (3.3) should hold for all values of a, b. This is
</p>
<p>only possible if the integrand is identically zero, which gives the local form of the
</p>
<p>law conservation of mass:
&part;u
</p>
<p>&part;t
+
</p>
<p>&part;q
</p>
<p>&part;x
= 0. (3.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.1 Model Problem: Oxygen in the Bloodstream 27
</p>
<p>This relationship between concentration and flux is called the continuity equation
</p>
<p>(or transport equation). The continuity equation applies generally to the physical
</p>
<p>process of advection, which refers to the motion of particles in a bulk fluid flow.
</p>
<p>To adapt (3.4) to a particular model, we need to specify the relationship between
</p>
<p>q and u. As we remarked above, for the bloodstream model it is reasonable to assume
</p>
<p>a linear relationship,
</p>
<p>q = vu, (3.5)
</p>
<p>where the velocity v(t, x) is part of the input data for the equation. Under this
</p>
<p>assumption (3.4) reduces to
</p>
<p>&part;u
</p>
<p>&part;t
+ v
</p>
<p>&part;u
</p>
<p>&part;x
+ u
</p>
<p>&part;v
</p>
<p>&part;x
= 0, (3.6)
</p>
<p>which is called the linear conservation equation.
</p>
<p>3.2 Lagrangian Derivative and Characteristics
</p>
<p>In this section we will discuss the strategy for solving a first-order PDE such as (3.6).
</p>
<p>The basic idea is to adopt the perspective of an observer traveling with velocity v.
</p>
<p>This is like taking measurements in a river from a raft drawn by the current. Once
</p>
<p>we fix a starting point for the observer, the observed concentration depends only on
</p>
<p>the time variable, thus reducing the equation to an ODE.
</p>
<p>This principle applies to any first-order PDE of the form
</p>
<p>&part;u
</p>
<p>&part;t
+ v
</p>
<p>&part;u
</p>
<p>&part;x
+ w = 0, (3.7)
</p>
<p>where v = v(t, x) is independent of u. The zeroth-order term w could be a general
function w(t, x, u). A trajectory t �&rarr; x(t) is called a characteristic for the equation
(3.7) if
</p>
<p>dx
</p>
<p>dt
(t) = v(t, x(t)). (3.8)
</p>
<p>For v and &part;v/&part;x continuous, Theorem 2.4 shows that a unique solution exists in the
</p>
<p>neighborhood of each starting point (t0, x0).
</p>
<p>Example 3.1 Suppose v(t, x) = at + b, with a and b constant. Integration over t
gives
</p>
<p>x(t) =
a
</p>
<p>2
t2 + bt + x0.
</p>
<p>The characteristics are a family of curves indexed by the parameter x0, as illustrated
</p>
<p>in Fig. 3.2. &diams;</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>28 3 Conservation Equations and Characteristics
</p>
<p>Fig. 3.2 Sample
</p>
<p>characteristics for the
</p>
<p>velocity v(t, x) = 1 + 2t
</p>
<p>t
</p>
<p>x
</p>
<p>From the point of view of an observer carried by the flow, the measured concen-
</p>
<p>tration is u(t, x(t)). The observed rate of change is the derivative of this quantity,
</p>
<p>Du
</p>
<p>Dt
(t) :=
</p>
<p>d
</p>
<p>dt
u(t, x(t)), (3.9)
</p>
<p>called the Lagrangian derivative (or material derivative). This concept was devel-
</p>
<p>oped by the 18th century mathematician and physicist Joseph-Louis Lagrange. Note
</p>
<p>that Du/Dt depends also on the initial value (t0, x0) that determines the character-
</p>
<p>istic. For convenience we suppress the initial point from the notation.
</p>
<p>Theorem 3.2 On each characteristic, (3.7) reduces to the ODE
</p>
<p>Du
</p>
<p>Dt
+ w̃ = 0, (3.10)
</p>
<p>where w̃ is the restriction of w to the characteristic,
</p>
<p>w̃(t) := w
(
</p>
<p>t, x(t), u(t, x(t)
)
</p>
<p>.
</p>
<p>In particular, if w = 0 then u is constant on each characteristic.
</p>
<p>Proof Applying the chain rule in (3.9) gives
</p>
<p>Du
</p>
<p>Dt
=
</p>
<p>&part;u
</p>
<p>&part;t
+
</p>
<p>&part;u
</p>
<p>&part;x
</p>
<p>dx
</p>
<p>dt
,
</p>
<p>with the understanding that the partial derivatives on the right are evaluated at the
</p>
<p>point (t, x(t)). Because x(t) solves (3.8), this reduces to
</p>
<p>Du
</p>
<p>Dt
=
</p>
<p>&part;u
</p>
<p>&part;t
+ v
</p>
<p>&part;u
</p>
<p>&part;x
, (3.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Lagrangian Derivative and Characteristics 29
</p>
<p>If we restrict the variables in (3.7) to (t, x(t)), then the first two terms match the
</p>
<p>right-hand side of (3.11), reducing the equation to (3.10).
</p>
<p>If w = 0, then (3.10) becomes
</p>
<p>Du
</p>
<p>Dt
= 0.
</p>
<p>This is equivalent to the statement that u(t, x(t)) is independent of t . �
</p>
<p>With Theorem 3.2 we can effectively reduce the PDE (3.7) to a pair of ODE,
</p>
<p>namely the characteristic equation (3.8) and the Lagrangian derivative equation
</p>
<p>(3.10). In many cases, solving these ODE will lead to an explicit formula for u(t, x).
</p>
<p>This approach is referred to as the method of characteristics.
</p>
<p>Example 3.3 For constants a, b &isin; R, assume that u(t, x) satisfies
</p>
<p>&part;u
</p>
<p>&part;t
+ (at + b)
</p>
<p>&part;u
</p>
<p>&part;x
= 0,
</p>
<p>with the initial condition
</p>
<p>u(0, x) = g(x),
</p>
<p>for some function g &isin; C1(R). The characteristics for this velocity, v(t, x) = at + b,
were computed in Example 3.1.
</p>
<p>According to Theorem 3.2, u is constant along characteristics, implying that
</p>
<p>u
(
</p>
<p>t,
a
</p>
<p>2
t2 + bt + x0
</p>
<p>)
</p>
<p>= u(0, x0) = g(x0), (3.12)
</p>
<p>for all t &isin; R. This is not yet a formula for u(t, x), but we can derive the solution
formula by identifying
</p>
<p>x =
a
</p>
<p>2
t2 + bt + x0.
</p>
<p>Solving for x0 in terms of x and substituting this into (3.12) gives
</p>
<p>u(t, x) = g
(
</p>
<p>x &minus;
a
</p>
<p>2
t2 &minus; bt
</p>
<p>)
</p>
<p>.
&diams;</p>
<p/>
</div>
<div class="page"><p/>
<p>30 3 Conservation Equations and Characteristics
</p>
<p>Example 3.4 For steady flow through a pipe of changing diameter, the velocity would
</p>
<p>vary with position rather than time. Let v(t, x) = a + bx for x &ge; 0, with a, b &gt; 0.
The resulting characteristic equation (3.8) is
</p>
<p>dx
</p>
<p>dt
= a + bx .
</p>
<p>This can be solved by the standard ODE technique of separating the t and x variables
</p>
<p>to different sides of the equation:
</p>
<p>dx
</p>
<p>a + bx
= dt.
</p>
<p>Integration of both sides gives the general solution
</p>
<p>1
</p>
<p>b
ln(a + bx) = t + C,
</p>
<p>with C a constant of integration. (Note that a+bx &gt; 0 by our assumptions.) Solving
for x gives
</p>
<p>x(t) =
1
</p>
<p>b
</p>
<p>[
</p>
<p>eb(t+C) &minus; a
]
</p>
<p>.
</p>
<p>Given the assumption x &ge; 0, it is natural to index the characteristics by the start
time t0 such that x(t0) = 0. With this convention, the family of solutions is
</p>
<p>x(t) =
a
</p>
<p>b
</p>
<p>[
</p>
<p>eb(t&minus;t0) &minus; 1
]
</p>
<p>. (3.13)
</p>
<p>These characteristic curves are illustrated in Fig. 3.3.
</p>
<p>With v = a + bx the linear conservation equation (3.6) becomes
</p>
<p>&part;u
</p>
<p>&part;t
+ (a + bx)
</p>
<p>&part;u
</p>
<p>&part;x
+ bu = 0.
</p>
<p>Let us find the solution under the boundary condition
</p>
<p>u(t, 0) = f (t). (3.14)
</p>
<p>Since &part;v/&part;x = b, (3.10) gives
</p>
<p>Du
</p>
<p>Dt
+ bu = 0.
</p>
<p>This is a decay equation, with the family of exponential solutions
</p>
<p>u(t, x(t)) = Ae&minus;bt .</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Lagrangian Derivative and Characteristics 31
</p>
<p>Fig. 3.3 Characteristic lines
</p>
<p>for the position-dependent
</p>
<p>velocity function of
</p>
<p>Example 3.4
</p>
<p>x
</p>
<p>t
</p>
<p>To fix A, we substitute the starting point (t0, 0) into the equation and obtain
</p>
<p>u(t, x(t)) = f (t0)e&minus;b(t&minus;t0). (3.15)
</p>
<p>Putting together (3.13) and (3.15) and applying the boundary condition (3.14)
</p>
<p>gives
</p>
<p>u
(
</p>
<p>t,
a
</p>
<p>b
</p>
<p>[
</p>
<p>eb(t&minus;t0) &minus; 1
]
</p>
<p>)
</p>
<p>= f (t0)e&minus;b(t&minus;t0). (3.16)
</p>
<p>To express this as a function of (t, x), we set
</p>
<p>x =
a
</p>
<p>b
</p>
<p>[
</p>
<p>eb(t&minus;t0) &minus; 1
]
</p>
<p>,
</p>
<p>and solve for t0 to obtain
</p>
<p>t0 = t +
1
</p>
<p>b
ln
</p>
<p>(
</p>
<p>a
</p>
<p>a + bx
</p>
<p>)
</p>
<p>.
</p>
<p>Substituting this expression into (3.16) gives the final form of the solution:
</p>
<p>u(t, x) =
(
</p>
<p>a
</p>
<p>a + bx
</p>
<p>)
</p>
<p>f
</p>
<p>(
</p>
<p>t +
1
</p>
<p>b
ln
</p>
<p>(
</p>
<p>a
</p>
<p>a + bx
</p>
<p>))
</p>
<p>.
</p>
<p>A sample solution is illustrated in Fig. 3.4 for a = 1, b = 1
2
. For this example
</p>
<p>the boundary condition f (t) was taken to have support between t = &minus;1 and t = 1,
with a maximum at t = 0. The plots of u(t, x) on the right show concentrations at
a succession of times. Mass conservation is reflected in the fact that the total area
</p>
<p>under each of these curves is independent of t . &diams;</p>
<p/>
</div>
<div class="page"><p/>
<p>32 3 Conservation Equations and Characteristics
</p>
<p>Fig. 3.4 Behavior of solutions for Example 3.4. In the contour plot on the left, darker regions
</p>
<p>correspond to higher concentration. The change in colors corresponds to exponential decay along
</p>
<p>the characteristics illustrated in Fig. 3.3
</p>
<p>3.3 Higher-Dimensional Equations
</p>
<p>For flow problems in more than one spatial dimension, we can develop a continuity
</p>
<p>equation analogous to (3.4) by the same reasoning as in Sect. 3.1. Suppose u(t, x)
</p>
<p>represents a concentration defined for t &isin; R and x &isin; Rn . Let R &sub; Rn be a bounded
region with C1 boundary. The total mass within this region is given by the volume
</p>
<p>integral
</p>
<p>m(t) :=
&int;
</p>
<p>R
</p>
<p>u(t, x) dn x.
</p>
<p>The flow of u is represented by a vector-valued flux density q(t, x). The
</p>
<p>interpretation of the flux density is that the rate at which mass passes through an
</p>
<p>(n &minus; 1)-dimensional surface is given by the surface integral of q over this surface.
In particular, the rate at which mass exits R through the boundary is the quantity
</p>
<p>&int;
</p>
<p>&part;R
</p>
<p>ν &middot; q d S,
</p>
<p>where ν is the outward unit normal vector defined on &part;R.
</p>
<p>Conservation of mass dictates that the mass within R can change only as mass
</p>
<p>enters or leaves through the boundary. In other words,
</p>
<p>dm
</p>
<p>dt
= &minus;
</p>
<p>&int;
</p>
<p>&part;R
</p>
<p>ν &middot; q d S. (3.17)
</p>
<p>Assuming that q is C1 with respect to x, the Divergence Theorem (Theorem 2.6)
</p>
<p>allows us to rewrite the flux integral as
</p>
<p>&int;
</p>
<p>&part;R
</p>
<p>q &middot; ν d S =
&int;
</p>
<p>R
</p>
<p>&nabla; &middot; q dn x. (3.18)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>3.3 Higher-Dimensional Equations 33
</p>
<p>Note that since q depends on both t and x, the notation &nabla; &middot; q is slightly ambiguous.
We follow the standard convention that vector calculus operators such as &nabla; and Δ
act only on spatial variables.
</p>
<p>If u is C1 with respect to t , then we can also differentiate the integral for m to
</p>
<p>obtain
dm
</p>
<p>dt
=
</p>
<p>&int;
</p>
<p>R
</p>
<p>&part;u
</p>
<p>&part;t
dn x.
</p>
<p>Combining this with (3.17) and (3.18) gives
</p>
<p>&int;
</p>
<p>R
</p>
<p>(
</p>
<p>&part;u
</p>
<p>&part;t
+ &nabla; &middot; q
</p>
<p>)
</p>
<p>dn x = 0. (3.19)
</p>
<p>As in the one-dimensional case, we now observe that since (3.19) holds for an arbi-
</p>
<p>trary region R, the integrand must vanish. This is the higher-dimensional continuity
</p>
<p>equation:
&part;u
</p>
<p>&part;t
+&nabla; &middot; q = 0. (3.20)
</p>
<p>Suppose we make the linear assumption that q = vu for a velocity field v which
is independent of u. The product rule for the divergence of a vector field is
</p>
<p>&nabla; &middot; (vu) = (&nabla; &middot; v)u + v &middot; &nabla;u.
</p>
<p>Substituting this into (3.20) gives the higher-dimensional form of the linear conser-
</p>
<p>vation equation
&part;u
</p>
<p>&part;t
+ v &middot; &nabla;u + (&nabla; &middot; v)u = 0. (3.21)
</p>
<p>In the special case where &nabla; &middot; v = 0 the velocity field is called solenoidal (or
divergence-free). This situation arises frequently in applications, because incom-
</p>
<p>pressible fluids like blood or water have solenoidal velocity fields.
</p>
<p>The method of characteristics from Sect. 3.2 can be adapted directly to (3.21).
</p>
<p>Consider a somewhat more general first-order PDE in the form
</p>
<p>&part;u
</p>
<p>&part;t
+ v &middot; &nabla;u + w = 0, (3.22)
</p>
<p>with v = v(t, x) and w = w(t, x, u). The characteristics associated to this equation
are by definition the solutions of
</p>
<p>dx
</p>
<p>dt
(t) = v(t, x(t)). (3.23)</p>
<p/>
</div>
<div class="page"><p/>
<p>34 3 Conservation Equations and Characteristics
</p>
<p>Theorem 2.4 guarantees that characteristics exist in the neighborhood of each start-
</p>
<p>ing point (t0, x0) provided v(t, x) and its partial derivatives with respect to x are
</p>
<p>continuous.
</p>
<p>The Lagrangian derivative of u along x(t) is defined as before by
</p>
<p>Du
</p>
<p>Dt
(t) :=
</p>
<p>d
</p>
<p>dt
u(t, x(t)).
</p>
<p>The higher-dimensional version of Theorem 3.2 is the following:
</p>
<p>Theorem 3.5 On each characteristic curve, the PDE (3.22) reduces to the ODE
</p>
<p>Du
</p>
<p>Dt
+ w̃ = 0, (3.24)
</p>
<p>where w̃ denotes the restriction of w to the characteristic. In particular, if w = 0
then u is constant on each characteristic.
</p>
<p>Proof By the chain rule,
</p>
<p>Du
</p>
<p>Dt
(t) =
</p>
<p>&part;u
</p>
<p>&part;t
(t, x(t))+ &nabla;u(t, x(t)) &middot;
</p>
<p>dx
</p>
<p>dt
(t).
</p>
<p>Since x(t) satisfies (3.23), this gives
</p>
<p>Du
</p>
<p>Dt
=
</p>
<p>&part;u
</p>
<p>&part;t
+ v &middot; &nabla;u.
</p>
<p>Substituting this into (3.22) reduces the equation to (3.24).
</p>
<p>If w = 0 the equation becomes
</p>
<p>Du
</p>
<p>Dt
= 0,
</p>
<p>which means precisely that u is constant along the characteristic curves. �
</p>
<p>Example 3.6 Consider a two-dimensional channel modeled as Ω = R &times; [&minus;1, 1]
with coordinates x = (x1, x2). The velocity field
</p>
<p>v(t, x) := (1 &minus; x22 , 0). (3.25)
</p>
<p>is solenoidal and vanishes on the boundary {x2 = &plusmn;1}. The characteristic line orig-
inating from (a, b) &isin; Ω at t = 0 is
</p>
<p>x(t) =
(
</p>
<p>a + (1 &minus; b2)t, b
)
</p>
<p>.
</p>
<p>Let us consider the conservation equation (3.21) for (t, x) &isin; R&times;Ω , with v given
by (3.25), subject to the initial condition</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>3.3 Higher-Dimensional Equations 35
</p>
<p>x
</p>
<p>y
</p>
<p>t = 0 t = 3 t = 6
</p>
<p>Fig. 3.5 Evolution of a circle according to the two-dimensional flow in Example 3.6
</p>
<p>u(0, x) = g(x),
</p>
<p>for g &isin; C1(Ω). Since v is solenoidal, Theorem 3.5 implies that u is constant on
characteristics. This gives the relation
</p>
<p>u
(
</p>
<p>t, a + (1 &minus; b2)t, b
)
</p>
<p>:= g(a, b).
</p>
<p>Rewriting this as a function of (t, x, y) gives
</p>
<p>u(t, x, y) = g
(
</p>
<p>x &minus; (1 &minus; b2)t, y
)
</p>
<p>.
</p>
<p>Figure 3.5 illustrates the evolution of a circular &ldquo;ink spot&rdquo; distribution under this
</p>
<p>flow. Conservation of mass is reflected in the fact that the area of the spot is inde-
</p>
<p>pendent of t . &diams;
</p>
<p>For applications of Theorem 3.5 on a bounded domain Ω &isin; Rn , the specification
of boundary conditions can be quite a complicated problem, especially if the velocity
</p>
<p>is time-dependent. (We avoided this problem in Example 3.6 by taking v tangent to
</p>
<p>&part;Ω .) We will illustrate this issue in the exercises.
</p>
<p>3.4 Quasilinear Equations
</p>
<p>The method of characteristics remains an important tool for analysis of first-order
</p>
<p>PDE even in the nonlinear case. In this section we will illustrate the application of
</p>
<p>this method to the continuity equation (3.20) in the case of a flux term q that depends
</p>
<p>on the concentration u.
</p>
<p>To simplify the analysis, we assume that q = q(u), with no explicit dependence
on t and x . By the chain rule, (3.20) then reduces to the form
</p>
<p>&part;u
</p>
<p>&part;t
+ a(u) &middot; &nabla;u = 0, (3.26)</p>
<p/>
</div>
<div class="page"><p/>
<p>36 3 Conservation Equations and Characteristics
</p>
<p>where a(u) := dq/du. This type of PDE is called quasilinear, which means that the
equation is linear in the highest-order derivatives (which are merely first order in this
</p>
<p>case).
</p>
<p>A comparison of (3.26) to the linear conservation equation (3.21) shows that a(u)
</p>
<p>is now playing the role of velocity. This suggests a definition for the characteristics,
</p>
<p>but we must keep in mind that a(u) depends on t and x implicitly through u.
</p>
<p>Theorem 3.7 Suppose that u &isin; C1([0, T ] &times; Ω) is a solution of (3.26) for some
region Ω &sub; Rn , with a &isin; C1(R;Rn). Then for each x0 &isin; Ω , u is constant along the
characteristic line defined by
</p>
<p>x(t) = x0 + a(u(0, x0))t.
</p>
<p>Proof Suppose that a solution u exists. Let x(t) be the solution to the ODE
</p>
<p>dx
</p>
<p>dt
(t) = a(u(t, x(t))), x(0) = x0,
</p>
<p>for t &isin; [0, T ]. Existence of such a characteristic is guaranteed by Theorem 2.4, at
least for t near 0, because the composition a ◦ u is C1 as a function of (t, x) by the
assumptions on a and u.
</p>
<p>To establish the claim that u(t, x(t)) is independent of t , we use the chain rule to
</p>
<p>differentiate
</p>
<p>d
</p>
<p>dt
u(t, x(t)) =
</p>
<p>&part;u
</p>
<p>&part;t
(t, x(t))+&nabla;u(t, x(t)) &middot;
</p>
<p>dx
</p>
<p>dt
(t)
</p>
<p>=
&part;u
</p>
<p>&part;t
(t, x(t))+ a(u(t, x(t))) &middot; &nabla;u(t, x(t)).
</p>
<p>The right-hand side vanishes by (3.26), so that
</p>
<p>d
</p>
<p>dt
u(t, x(t)) = 0.
</p>
<p>This implies that
</p>
<p>u(t, x(t)) = u(0, x0),
</p>
<p>which means that a(u(t, x(t))) is also constant. The characteristic equation reduces
</p>
<p>to
dx
</p>
<p>dt
(t) = a(u(0, x0)),
</p>
<p>and we can integrate over t to compute x(t). �
</p>
<p>In contrast to the characteristic equation (3.8) in the linear case, the equation for
</p>
<p>x(t) here depends on the initial condition u(0, x0). Furthermore, it is important to
</p>
<p>keep in mind that Theorem 3.7 does not imply that a solution to (3.26) exists; this</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>3.4 Quasilinear Equations 37
</p>
<p>is assumed as a hypothesis. As we will see below, it is possible that the conclusion
</p>
<p>of the theorem will lead to a contradiction, in the form of multiple values for the
</p>
<p>solution at the same point. The implication in such a case is that a classical solution
</p>
<p>does not exist.
</p>
<p>To illustrate the application of Theorem 3.7, let us consider a simple model for
</p>
<p>traffic on a single-lane road of infinite length, parametrized by x &isin; R. Let u(t, x)
denote the linear density of cars at a given point and time. Cars are discrete objects,
</p>
<p>of course, but for modeling purposes we can assume that u is a C1 function that
</p>
<p>describes the density in an aggregate sense.
</p>
<p>In traffic flow, the density of cars affects the flow velocity, with traffic slowing
</p>
<p>down and possibly stopping as the density increases. A standard way to model this
</p>
<p>effect is to set a maximum value for the velocity vm (presumably the speed limit).
</p>
<p>The velocity is assumed to take its maximum value at u = 0 and decrease linearly
as u increases, up to some maximum value um for which v = 0. In other words, for
this model u &isin; [0, um] and
</p>
<p>v(u) := vm
(
</p>
<p>1 &minus;
u
</p>
<p>um
</p>
<p>)
</p>
<p>.
</p>
<p>Since v &ge; 0, the model always assumes that traffic moves to the right.
To eliminate the constants and focus on the equation itself, let us set vm = 1 and
</p>
<p>um = 1, reducing the velocity equation to
</p>
<p>v(u) = 1 &minus; u
</p>
<p>for u &isin; [0, 1]. The corresponding flux is
</p>
<p>q(u) = u &minus; u2.
</p>
<p>Substituting these assumptions into (3.26), we obtain a quasilinear equation called
</p>
<p>the traffic equation:
&part;u
</p>
<p>&part;t
+ (1 &minus; 2u)
</p>
<p>&part;u
</p>
<p>&part;x
= 0. (3.27)
</p>
<p>Suppose we impose a general initial condition of the form
</p>
<p>u(0, x) = h(x),
</p>
<p>for some h : R &rarr; [0, 1]. Assuming a solution exists, Theorem 3.7 gives the family
of characteristics
</p>
<p>x(t) = x0 + (1 &minus; 2h(x0))t. (3.28)
</p>
<p>Therefore, the solution u must satisfy
</p>
<p>u
(
</p>
<p>t, x0 + (1 &minus; 2h(x0))t
)
</p>
<p>= h(x0). (3.29)</p>
<p/>
</div>
<div class="page"><p/>
<p>38 3 Conservation Equations and Characteristics
</p>
<p>Fig. 3.6 Initial traffic
</p>
<p>density modeling a line of
</p>
<p>cars stopped at a traffic light
</p>
<p>x
</p>
<p>h(x)
</p>
<p>As we will demonstrate in the examples below, (3.29) leads to a solution formula for
</p>
<p>some choices of h, while for others it leads to a contradiction.
</p>
<p>Example 3.8 Figure 3.6 shows a plot of the initial condition
</p>
<p>h(x) =
1
</p>
<p>2
&minus;
</p>
<p>1
</p>
<p>π
arctan(20x),
</p>
<p>which could represent a line of cars stopped at a traffic light at the point x = 0. The
corresponding characteristic lines as given by (3.28) are plotted in Fig. 3.7.
</p>
<p>To derive a formula for u(t, x) from (3.29), we need to invert the equation
</p>
<p>x = x0 + (1 &minus; 2h(x0))t,
</p>
<p>to express x0 as a function of t and x . For the function h given above it is not possible
</p>
<p>to do this explicitly. However, there is a unique solution for each (t, x), which can
</p>
<p>easily be calculated numerically. The resulting solutions are shown in Fig. 3.8.
</p>
<p>�
</p>
<p>Example 3.9 In order to solve the traffic equation explicitly, let us simplify the initial
</p>
<p>condition to the piecewise linear function
</p>
<p>h(x) =
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>1, x &le; 0,
1 &minus; x, 0 &lt; x &lt; 1,
0, x &ge; 1.
</p>
<p>This is not C1, but the resulting solution could be interpreted as a weak solution in
</p>
<p>the sense described in Sect. 1.2. We will discuss the precise definition in Chap. 10.
</p>
<p>By the formula from Theorem 3.7, the characteristic lines are
</p>
<p>x(t) =
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>x0 &minus; t, x0 &le; 0,
x0 + (2x0 &minus; 1)t, 0 &lt; x0 &lt; 1,
x0 + t, x0 &ge; 1.
</p>
<p>(3.30)
</p>
<p>Solving these equations for x0 gives</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_1">http://dx.doi.org/10.1007/978-3-319-48936-0_1</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_10">http://dx.doi.org/10.1007/978-3-319-48936-0_10</a></div>
</div>
<div class="page"><p/>
<p>3.4 Quasilinear Equations 39
</p>
<p>x
</p>
<p>t
</p>
<p>Fig. 3.7 Characteristic lines for the initial density shown in Fig. 3.6
</p>
<p>x
</p>
<p>u(t, x)
</p>
<p>t = 0
</p>
<p>t = 2
</p>
<p>Fig. 3.8 Solutions for the traffic light problem
</p>
<p>x0 =
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>x + t, x &le; &minus;t,
x+t
1+2t , &minus;t &lt; x &lt; 1 + t,
x &minus; t. x &ge; 1 + t.
</p>
<p>Therefore, by the solution formula (3.29), the solution is
</p>
<p>u(t, x) =
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>1, x &le; &minus;t,
1 &minus; x+t
</p>
<p>1+2t , &minus;t &lt; x &lt; 1 + t,
0. x &ge; 1 + t.
</p>
<p>(3.31)
</p>
<p>This is a continuous function, but differentiability fails on the lines x = &minus;t and
x = 1 + t . Away from these lines it is easy to check that u solves (3.27).
</p>
<p>Despite the lack of smoothness, this solution is quite reasonable. To illustrate
</p>
<p>this, let us trace the motion of a particular car starting from the position x0 &le; 0. The
velocity of the car is given by the flow rate v(u) = 1 &minus; u. The initial density at x0 is
u = 1, so the car is stationary for a time. According to (3.31), at t = &minus;x0 the value
of (t, x) enters the region where &minus;t &lt; x &lt; 1+ t and so at this time the density starts
to decrease and the car starts to move. For (t, x) in this range, (3.31) gives</p>
<p/>
</div>
<div class="page"><p/>
<p>40 3 Conservation Equations and Characteristics
</p>
<p>Fig. 3.9 Trajectories of
</p>
<p>individual cars according to
</p>
<p>the model of Example 3.9
</p>
<p>x
</p>
<p>t
</p>
<p>v(t, x) = 1 &minus; u(t, x) =
x + t
1 + 2t
</p>
<p>. (3.32)
</p>
<p>Let s(t) denote the position of the car at time t . For t &ge; &minus;x0 the velocity formula
(3.32) gives the equation
</p>
<p>ds
</p>
<p>dt
=
</p>
<p>s + t
1 + 2t
</p>
<p>, (3.33)
</p>
<p>The initial condition at t = &minus;x0 is the original starting point s(&minus;x0) = x0. The
standard ODE method of integrating factors can be used to solve (3.33), yielding
</p>
<p>s(t) =
{
</p>
<p>x0, 0 &le; t &le; &minus;x0,
1 + t &minus;
</p>
<p>&radic;
(1 &minus; 2x0)(1 + 2t), t &ge; &minus;x0.
</p>
<p>These trajectories are illustrated in Fig. 3.9. As we might expect, the cars further back
</p>
<p>in the line wait longer before moving, but each car eventually moves forward and
</p>
<p>gradually accelerates. &diams;
</p>
<p>Example 3.10 Consider the initial condition
</p>
<p>h(x) =
1
</p>
<p>2
+
</p>
<p>1
</p>
<p>π
arctan(20x),
</p>
<p>as shown in Fig. 3.10. This is the reverse of the initial condition of Example 3.8.
</p>
<p>The characteristics specified in Theorem 3.7 now cross each other, as illustrated in
</p>
<p>Fig. 3.11. The existence of crossings implies that a classical solution with this initial
</p>
<p>condition cannot exist beyond the time of the first crossing.
</p>
<p>If we were to trace the trajectories of individual cars, as we did in Example 3.9,
</p>
<p>we would see that these also intersect each other at the points where characteristics
</p>
<p>cross. In effect, the model predicts the formation of a traffic jam. &diams;</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Quasilinear Equations 41
</p>
<p>Fig. 3.10 Initial traffic
</p>
<p>density with a
</p>
<p>near-maximum density of
</p>
<p>cars to the right
</p>
<p>x
</p>
<p>h(x)
</p>
<p>Fig. 3.11 Conflicting
</p>
<p>characteristic lines for the
</p>
<p>initial density shown in
</p>
<p>Fig. 3.10
</p>
<p>x
</p>
<p>t
</p>
<p>A crossing of characteristics as observed in Example 3.10 is called a shock. After
</p>
<p>the shock, the solution is forced to have discontinuities. The proper interpretation of
</p>
<p>this situation requires weak solutions, for which discontinuities are allowed. We will
</p>
<p>return to this issue in Chap. 10.
</p>
<p>3.5 Exercises
</p>
<p>3.1 Consider the conservation equation with a constant velocity c &gt; 0,
</p>
<p>&part;u
</p>
<p>&part;t
+ c
</p>
<p>&part;u
</p>
<p>&part;x
= 0,
</p>
<p>on the quadrant t &ge; 0, x &ge; 0. Suppose the boundary and initial conditions are
{
</p>
<p>u(0, x) = g(x), x &ge; 0,
u(t, 0) = h(t), t &ge; 0,
</p>
<p>for g, h &isin; C1[0,&infin;).</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_10">http://dx.doi.org/10.1007/978-3-319-48936-0_10</a></div>
</div>
<div class="page"><p/>
<p>42 3 Conservation Equations and Characteristics
</p>
<p>(a) Find a formula for the solution u(t, x) in terms of g and h.
</p>
<p>(b) Find a matching condition for g and h that will ensure that u(t, x) is a C1
</p>
<p>function.
</p>
<p>3.2 In the continuity equation (3.4), external factors that break the conservation of
</p>
<p>mass are accounted for by adding terms to the right-hand side.
</p>
<p>(a) A forcing term f (t, x) is independent of the existing concentration. (In the
</p>
<p>bloodstream model of Sect. 3.1, this could represent intravenous injection, for
</p>
<p>example.) Assume that c is constant, f &isin; C1(R2), and g &isin; C1(R). Solve the
equation
</p>
<p>&part;u
</p>
<p>&part;t
+ c
</p>
<p>&part;u
</p>
<p>&part;x
= f, u(0, x) = g(x),
</p>
<p>to find an explicit formula for u(t, x) in terms of f and g.
</p>
<p>(b) A reaction term depends on the concentration u. The simplest case is a linear
</p>
<p>term γ u where the coefficient is some function γ (t, x). (This could represent
</p>
<p>absorption of oxygen into the walls of the artery, for example.) Assume that c is
</p>
<p>constant, γ &isin; C1(R2), and g &isin; C1(R). Solve the equation
</p>
<p>&part;u
</p>
<p>&part;t
+ c
</p>
<p>&part;u
</p>
<p>&part;x
= γ u, u(0, x) = g(x),
</p>
<p>to find an explicit formula for u(t, x) in terms of γ and g.
</p>
<p>3.3 Assume that u satisfies the linear conservation equation
</p>
<p>&part;u
</p>
<p>&part;t
+ 2t
</p>
<p>&part;u
</p>
<p>&part;x
= 0,
</p>
<p>for t &isin; R and x &isin; [0, 1]. Suppose the boundary conditions are given by
</p>
<p>u(t, 0) = h0(t), u(t, 1) = h1(t).
</p>
<p>Find a relation between h0 and h1. (This shows that we can only impose a boundary
</p>
<p>condition at one side of the interval [0, 1].)
</p>
<p>3.4 If the spatial domain in the linear conservation equation (3.21) is a bounded
</p>
<p>region Ω &sub; Rn , then for a given velocity field v, the inflow boundary &part;Ωin &isin; &part;Ω
is defined as the set of boundary points where v points into Ω . Fixing boundary
</p>
<p>conditions on the inflow boundary will generally determine the solution in the interior.
</p>
<p>Suppose Ω = (&minus;1, 1) &times; (&minus;1, 1) &isin; R2 with coordinates (x1, x2). For the velocity
fields below, determine the characteristics and specify the inflow boundary. Draw a
</p>
<p>sketch of Ω for each case, indicating these features.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 Exercises 43
</p>
<p>(a) v(x1, x2) = (x2, 1).
</p>
<p>(b) v(x1, x2) = (1,&minus;x2).
</p>
<p>3.5 Suppose that a section of of a river is modeled as a rectangle Ω = (0, ℓ) &times;
(0, 1) &sub; R2, parametrized by (x1, x2). Assume the flow is parallel to the x1-axis,
with velocity
</p>
<p>v(x1, x2) = ( f (x2), 0),
</p>
<p>for some positive function f on (0, 1). Assume also that the concentration on the
</p>
<p>left boundary {x1 = 0} is given by
</p>
<p>u(t, 0, x2) = h(t, x2).
</p>
<p>Find a formula for u(t, x1, x2) in terms of the functions h and f .
</p>
<p>3.6 Burgers&rsquo; equation is a simple quasilinear equation that appears in models of gas
</p>
<p>dynamics,
&part;u
</p>
<p>&part;t
+ u
</p>
<p>&part;u
</p>
<p>&part;x
= 0.
</p>
<p>(a) Use the method of characteristics as described in Sect. 3.4 to find a formula for
</p>
<p>the solution u(t, x) given the initial condition
</p>
<p>u(0, x) =
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>0, x &le; 0,
x
a
, 0 &lt; x &lt; a,
</p>
<p>1, x &ge; a.
</p>
<p>(b) Suppose a &gt; b and
</p>
<p>u(0, x) =
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>a, x &le; 0,
a(1 &minus; x)+ bx, 0 &lt; x &lt; 1,
b, x &ge; 1.
</p>
<p>Show that all of the characteristics originating from x0 &isin; [0, 1] meet at the same
point (thus creating a shock).
</p>
<p>3.7 In the mid-19th century, William Hamilton and Carl Jacobi developed a formu-
</p>
<p>lation of classical mechanics based on ideas from geometric optics. In this approach
</p>
<p>the dynamics of a free particle in R are described by a generating function u(t, x)
</p>
<p>satisfying the Hamilton-Jacobi equation:
</p>
<p>&part;u
</p>
<p>&part;t
+
</p>
<p>1
</p>
<p>2
</p>
<p>(
</p>
<p>&part;u
</p>
<p>&part;x
</p>
<p>)2
</p>
<p>= 0. (3.34)</p>
<p/>
</div>
<div class="page"><p/>
<p>44 3 Conservation Equations and Characteristics
</p>
<p>Assume that u &isin; C1([0,&infin;) &times; Rn) is a solution of (3.34). By analogy with Theo-
rem 3.7, a characteristic of (3.34) is defined as a solution of
</p>
<p>dx
</p>
<p>dt
(t) =
</p>
<p>&part;u
</p>
<p>&part;x
(t, x(t)), x(0) = x0. (3.35)
</p>
<p>(a) Assuming that x(t) solves (3.35), use the chain rule to compute d2x/dt2.
</p>
<p>(b) Differentiate (3.34) with respect to x and then restrict the result to (t, x(t)),
</p>
<p>where x(t) solves (3.35). Conclude from (a) that to
</p>
<p>d2x
</p>
<p>dt2
= 0.
</p>
<p>Hence, for some constant v0 (which depends on the characteristic),
</p>
<p>x(t) = x0 + v0t.
</p>
<p>(c) Show that the Lagrangian derivative of u along x(t) satisfies
</p>
<p>Du
</p>
<p>Dt
=
</p>
<p>1
</p>
<p>2
v20,
</p>
<p>implying that
</p>
<p>u(t, x0 + v0t) = u(0, x0)+
1
</p>
<p>2
v20 t.
</p>
<p>(d) Use this approach to find the solution u(t, x) under the initial condition
</p>
<p>u(0, x) = x2.
</p>
<p>(For the characteristic starting at (0, x0), note that you can compute v0 by eval-
</p>
<p>uating (3.35) at t = 0.)</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 4
</p>
<p>The Wave Equation
</p>
<p>As we noted in Sect. 1.2, d&rsquo;Alembert&rsquo;s derivation of the wave equation in the 18th
</p>
<p>century was an early milestone in the development of PDE theory. In this chapter
</p>
<p>we will develop this equation as a model for the vibrating string problem, and derive
</p>
<p>d&rsquo;Alembert&rsquo;s explicit solution in one dimension using the method of characteristics
</p>
<p>introduced in Chap.3.
</p>
<p>In higher dimensions the wave equation is used to model electromagnetic or
</p>
<p>acoustic waves. We will discussion the derivation of the acoustic model later in
</p>
<p>Sect. 4.5. A clever reduction trick allows the solution formula for Rn to be deduced
</p>
<p>from the one-dimensional case. The resulting integral formula yields insight into the
</p>
<p>propagation of waves in different dimensions.
</p>
<p>The chapter concludes with a discussion of the energy of a solution, based on the
</p>
<p>physical principles of kinetic and potential energy.
</p>
<p>4.1 Model Problem: Vibrating String
</p>
<p>Consider a flexible string that is stretched tight between two points, like the strings
</p>
<p>on a violin or guitar. The stretching of the string creates a tension force T that pulls
</p>
<p>in both directions at each point along its length. For simplicity, let us assume that
</p>
<p>any other forces acting on the string, including gravity, are negligible compared to
</p>
<p>the tension. The linear density of mass ρ is taken to be constant along the string.
</p>
<p>For a violin string it is also reasonable to assume that the displacement of the
</p>
<p>string is extremely small relative to its length. This assumption justifies taking T to
</p>
<p>be a fixed constant, ignoring the additional stretching that occurs when the string
</p>
<p>is displaced. It also allows us to treat horizontal and vertical components of the
</p>
<p>displacement independently, so we can restrict our attention to the vertical.
</p>
<p>The original version of the book was revised: Belated corrections from author have been incorpo-
</p>
<p>rated. The erratum to the book is available at https://doi.org/10.1007/978-3-319-48936-0_14
</p>
<p>&copy; Springer International Publishing AG 2016
</p>
<p>D. Borthwick, Introduction to Partial Differential Equations,
</p>
<p>Universitext, DOI 10.1007/978-3-319-48936-0_4
</p>
<p>45</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_1">http://dx.doi.org/10.1007/978-3-319-48936-0_1</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_3">http://dx.doi.org/10.1007/978-3-319-48936-0_3</a></div>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-319-48936-0_14">https://doi.org/10.1007/978-3-319-48936-0_14</a></div>
</div>
<div class="page"><p/>
<p>46 4 The Wave Equation
</p>
<p>Let the string be parametrized by x &isin; [0, ℓ]. The vertical displacement as a
function of time is denoted by u(t, x). To develop an equation for u, we first discretize
</p>
<p>the model by subdividing the total length ℓ into segments of length Δx = ℓ/n for
some large n. Each segment has a mass ρΔx and is subject to the tension forces
</p>
<p>pulling in the direction of its neighbors on either side.
</p>
<p>For j = 0, . . . n, let x j := jΔx be the position of the j th segment along the string.
The segments j = 0 and j = n represent the fixed endpoints, with j = 1, . . . , n&minus; 1
in the interior. Let u(t, x j ) denote the vertical displacement of the j th segment as a
</p>
<p>function of time. Figure4.1 illustrates this discretization (with displacements greatly
</p>
<p>exaggerated).
</p>
<p>To develop an equation for the string, we apply Newton&rsquo;s laws of motion to the
</p>
<p>segments of the discretization, as if they were single particles. The j th particle is
</p>
<p>being pulled by its neighbors with a force T on each side. Unless the string is straight,
</p>
<p>these forces are not quite aligned.
</p>
<p>In termsof the angles labeled inFig. 4.2, the net vertical force on a single segment is
</p>
<p>ΔF(t, x j ) = T sinα j + T sin β j .
</p>
<p>We have assumed that the relative displacements are extremely small, so the angles
</p>
<p>α j , β j will be very small also. To leading order, we can replace the sines by tangents,
</p>
<p>which are linear in u,
</p>
<p>sinα j &asymp;
u(t, x j&minus;1)&minus; u(t, x j )
</p>
<p>Δx
, sin β j &asymp;
</p>
<p>u(t, x j+1)&minus; u(t, x j )
Δx
</p>
<p>.
</p>
<p>With this linear approximation, the net vertical force at the point x j becomes
</p>
<p>ΔF(t, x j ) =
T
</p>
<p>Δx
</p>
<p>[
</p>
<p>u(t, x j+1)+ u(t, x j&minus;1)&minus; 2u(t, x j )
]
</p>
<p>. (4.1)
</p>
<p>Fig. 4.1 Discrete model for the displacement of the string
</p>
<p>Fig. 4.2 Discrete model for the displacement of the string</p>
<p/>
</div>
<div class="page"><p/>
<p>4.1 Model Problem: Vibrating String 47
</p>
<p>The equation of motion for the j th segment now comes from Newton&rsquo;s law: mass
</p>
<p>times acceleration equals force. At the point x j this translates to
</p>
<p>ρΔx
&part;2u
</p>
<p>&part;t2
(t, x j ) = ΔF(t, x j ). (4.2)
</p>
<p>Using (4.1) on the right then gives
</p>
<p>&part;2u
</p>
<p>&part;t2
(t, x j ) =
</p>
<p>T
</p>
<p>ρ
</p>
<p>u(t, x j+1)+ u(t, x j&minus;1)&minus; 2u(t, x j )
(Δx)2
</p>
<p>. (4.3)
</p>
<p>The final step is to take the continuum limit n &rarr; &infin; and Δx &rarr; 0. Assuming
that u is twice continuously differentiable as a function of x , we can deduce from
</p>
<p>the quadratic Taylor approximation of u(t, x) that
</p>
<p>lim
Δx&rarr;0
</p>
<p>u(t, x +Δx)+ u(t, x &minus;Δx)&minus; 2u(t, x)
(Δx)2
</p>
<p>=
&part;2u
</p>
<p>&part;x2
(t, x).
</p>
<p>Hence, taking Δx &rarr; 0 in (4.3) gives
</p>
<p>&part;2u
</p>
<p>&part;t2
&minus;
</p>
<p>T
</p>
<p>ρ
</p>
<p>&part;2u
</p>
<p>&part;x2
= 0. (4.4)
</p>
<p>This is the one-dimensional wave equation. The fixed ends of the string correspond
</p>
<p>to Dirichlet boundary conditions,
</p>
<p>u(t, 0) = u(t, ℓ) = 0.
</p>
<p>4.2 Characteristics
</p>
<p>For convenience, set c2 := T/ρ in (4.4), assuming c &gt; 0, and rewrite the equation
as
</p>
<p>&part;2u
</p>
<p>&part;t2
&minus; c2
</p>
<p>&part;2u
</p>
<p>&part;x2
= 0. (4.5)
</p>
<p>The constant c is called the propagation speed, for reasons that will become apparent
</p>
<p>as we analyze the equation.
</p>
<p>Let the physical domain be x &isin; R for the moment; we will discuss boundary
conditions later. The key to applying the method of characteristics to (4.5) is that the
</p>
<p>differential operator appearing in the equation factors as a product of two first-order
</p>
<p>operators, i.e.,
</p>
<p>&part;2
</p>
<p>&part;t2
&minus; c2
</p>
<p>&part;2
</p>
<p>&part;x2
=
</p>
<p>(
</p>
<p>&part;
</p>
<p>&part;t
+ c
</p>
<p>&part;
</p>
<p>&part;x
</p>
<p>) (
</p>
<p>&part;
</p>
<p>&part;t
&minus; c
</p>
<p>&part;
</p>
<p>&part;x
</p>
<p>)
</p>
<p>. (4.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>48 4 The Wave Equation
</p>
<p>Individually, these operators have characteristic lines t �&rarr; x0 &plusmn; ct . Both sets of
characteristics will play an important role here.
</p>
<p>Theorem 4.1 Under the initial conditions
</p>
<p>u(0, x) = g(x),
&part;u
</p>
<p>&part;t
(0, x) = h(x), (4.7)
</p>
<p>for g &isin; C2(R) and h &isin; C1(R), the wave equation (4.5) admits a unique solution
</p>
<p>u(t, x) =
1
</p>
<p>2
</p>
<p>[
</p>
<p>g(x + ct)+ g(x &minus; ct)
]
</p>
<p>+
1
</p>
<p>2c
</p>
<p>&int; x+ct
</p>
<p>x&minus;ct
h(τ ) dτ . (4.8)
</p>
<p>Proof Consider the auxiliary function w(t, x) defined by
</p>
<p>w :=
&part;u
</p>
<p>&part;t
&minus; c
</p>
<p>&part;u
</p>
<p>&part;x
. (4.9)
</p>
<p>By (4.6), w satisfies the linear conservation equation
</p>
<p>&part;w
</p>
<p>&part;t
+ c
</p>
<p>&part;w
</p>
<p>&part;x
= 0.
</p>
<p>The characteristics for this equation are given by x+(t) = x0 + ct . By Theorem 3.2
the unique solution with an initial condition w(0, x) = w0(x) is
</p>
<p>w(t, x) = w0(x &minus; ct). (4.10)
</p>
<p>We will relate w0 back to the initial conditions g and h in a moment.
</p>
<p>Withw given by (4.10), the definition (4.9) can be regarded as a linear conservation
</p>
<p>equation for u,
&part;u
</p>
<p>&part;t
&minus; c
</p>
<p>&part;u
</p>
<p>&part;x
= w, (4.11)
</p>
<p>where w acts as a forcing term as described in Exercise 3.2. The characteristics of
</p>
<p>(4.11) are x&minus;(t) = x0 &minus; ct . By Theorem 3.2, we can thus reduce the equation to the
form
</p>
<p>d
</p>
<p>dt
u(t, x0 &minus; ct) = w(t, x0 &minus; ct). (4.12)
</p>
<p>The unique solution to (4.12) under the initial condition u(0, x) = g(x) is given
by direct integration with respect to time:
</p>
<p>u(t, x0 &minus; ct) = g(x0)+
&int; t
</p>
<p>0
</p>
<p>w(s, x0 &minus; cs) ds.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_3">http://dx.doi.org/10.1007/978-3-319-48936-0_3</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_3">http://dx.doi.org/10.1007/978-3-319-48936-0_3</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_3">http://dx.doi.org/10.1007/978-3-319-48936-0_3</a></div>
</div>
<div class="page"><p/>
<p>4.2 Characteristics 49
</p>
<p>Setting x = x0 &minus; ct then gives
</p>
<p>u(t, x) = g(x + ct)+
&int; t
</p>
<p>0
</p>
<p>w(s, x &minus; c(s &minus; t)) ds.
</p>
<p>Using the formula (4.10) for the solution w, we obtain
</p>
<p>u(t, x) = g(x + ct)+
&int; t
</p>
<p>0
</p>
<p>w0(x &minus; 2cs + ct) ds.
</p>
<p>As a final step, the substitution τ := x + ct &minus; 2cs gives
</p>
<p>u(t, x) = g(x + ct)+
1
</p>
<p>2c
</p>
<p>&int; x+ct
</p>
<p>x&minus;ct
w0(τ ) dτ . (4.13)
</p>
<p>The function w0 can be computed from the initial conditions (4.7),
</p>
<p>w0(x) :=
&part;u
</p>
<p>&part;t
(0, x)&minus; c
</p>
<p>&part;u
</p>
<p>&part;x
(0, x)
</p>
<p>= h(x)&minus; c
&part;g
</p>
<p>&part;x
(x).
</p>
<p>The w0 contribution to (4.13) is then given by
</p>
<p>1
</p>
<p>2c
</p>
<p>&int; x+ct
</p>
<p>x&minus;ct
w0(τ ) dτ =
</p>
<p>1
</p>
<p>2c
</p>
<p>&int; x+ct
</p>
<p>x&minus;ct
h(τ ) dτ &minus;
</p>
<p>1
</p>
<p>2
</p>
<p>&int; x+ct
</p>
<p>x&minus;ct
</p>
<p>&part;g
</p>
<p>&part;x
(τ ) dτ
</p>
<p>=
1
</p>
<p>2c
</p>
<p>&int; x+ct
</p>
<p>x&minus;ct
h(τ ) dτ &minus;
</p>
<p>1
</p>
<p>2
</p>
<p>[
</p>
<p>g(x + ct)&minus; g(x &minus; ct)
]
</p>
<p>.
</p>
<p>Substituting back into (4.13) now gives the formula (4.8). �
</p>
<p>To highlight the role played by the characteristic lines in the solution of Theo-
</p>
<p>rem 4.1, consider the functions
</p>
<p>u&plusmn;(x) :=
1
</p>
<p>2
g(x)∓
</p>
<p>1
</p>
<p>2c
</p>
<p>&int; x
</p>
<p>0
</p>
<p>h(τ ) dτ .
</p>
<p>In terms of u&plusmn;, the solution (4.8) simplifies to
</p>
<p>u(x, t) = u+(x &minus; ct)+ u&minus;(x + ct), (4.14)
</p>
<p>matching the form of the solution stated in (1.3). The subscripts in u&plusmn; indicate the
propagation direction, i.e., u+ propagates to the right and u&minus; to the left. In either
direction the speed of propagation is the parameter c.
</p>
<p>Example 4.2 Consider the wave equation (4.5) with the initial conditions h(x) = 0
and</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_1">http://dx.doi.org/10.1007/978-3-319-48936-0_1</a></div>
</div>
<div class="page"><p/>
<p>50 4 The Wave Equation
</p>
<p>Fig. 4.3 Evolution of a
</p>
<p>solution to the wave equation
</p>
<p>g(x) =
{
</p>
<p>(1&minus; x2)2, |x | &le; 1,
0, |x | &gt; 1.
</p>
<p>By (4.8) the solution is the superposition of two localized bumps, which propagate
</p>
<p>in opposite directions as illustrated in Fig. 4.3. &diams;
</p>
<p>In Example 4.2 the initial condition was supported in [&minus;1, 1], and we can see
in Fig. 4.3 that the resulting solution has support in a V-shaped region. This region
</p>
<p>could be identified as the span of the characteristic lines emerging from the initial
</p>
<p>support interval.
</p>
<p>This restriction of the support of a solution is closely related toHuygens&rsquo; principle,
</p>
<p>an empirical law for propagation of light waves published by Christiaan Huygens
</p>
<p>in 1678. The one-dimensional wave equation exhibits a special, strict form of this
</p>
<p>principle:
</p>
<p>Theorem 4.3 (Huygens&rsquo; principle in dimension one) Suppose u solves the wave
</p>
<p>equation (4.5) for t &ge; 0, x &isin; R, with initial data given by (4.7). If the functions g, h
are supported in a bounded interval [a, b], then
</p>
<p>supp u &sub;
{
</p>
<p>(t, x) &isin; R+ &times; R; x &isin; [a &minus; ct, b + ct]
}
</p>
<p>.
</p>
<p>Proof Consider the components of the solution (4.8). The g term will vanish unless
</p>
<p>x &plusmn; ct &isin; [a, b]. The support of this term is thus restricted to x &isin; [a &minus; ct, b&minus; ct] or
x &isin; [a+ct, b+ct]. As for the h term, the integral over τ will vanish unless the interval
[x &minus; ct, x + ct] intersects [a, b], which occurs only when x &isin; [a &minus; ct, b + ct]. �
</p>
<p>The restriction of support described in Theorem 4.3 is illustrated in Fig. 4.4. The
</p>
<p>term g contributes only in the regions shown in blue, but the h term may con-
</p>
<p>tribute throughout the full support region. However, the solution is constant (equal to
&int; b
</p>
<p>a
h(τ )dτ when [a, b] is contained in [x &minus; ct, x + ct]. This constant region shown
</p>
<p>in purple in Fig. 4.4.
</p>
<p>Example 4.4 Suppose the initial data from Example 4.2 are altered to include a
</p>
<p>singularity at x = 0. For example,</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Characteristics 51
</p>
<p>Fig. 4.4 Support of a wave
</p>
<p>solution with initial data in a
</p>
<p>bounded interval
</p>
<p>Fig. 4.5 Propagation of
</p>
<p>singularities of the wave
</p>
<p>equation along characteristic
</p>
<p>lines
</p>
<p>g(x) =
{
</p>
<p>(1&minus; |x |)2, |x | &le; 1,
0, |x | &ge; 1.
</p>
<p>Then (4.8) still gives a formula for the solution even though g is not differentiable.
</p>
<p>(This is a weak solution in the sense we will describe in Chap.10). A set of solutions
</p>
<p>at different points in time is plotted in Fig. 4.5. Observe that the original singularity
</p>
<p>splits into two singularities, which propagate outward along the two characteristic
</p>
<p>lines emanating from x = 0. &diams;
</p>
<p>4.3 Boundary Problems
</p>
<p>In the string model of Sect. 4.1 the domain of the wave equation (4.5) was restricted
</p>
<p>to x &isin; [0, ℓ], with Dirichlet boundary conditions
</p>
<p>u(t, 0) = u(t, ℓ) = 0, for all t &ge; 0. (4.15)
</p>
<p>Suppose the initial data are given for x &isin; [0, ℓ] by
</p>
<p>u(0, x) = g(x),
&part;u
</p>
<p>&part;t
(0, x) = h(x), (4.16)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_10">http://dx.doi.org/10.1007/978-3-319-48936-0_10</a></div>
</div>
<div class="page"><p/>
<p>52 4 The Wave Equation
</p>
<p>with g &isin; C2[0, ℓ], h &isin; C1[0, ℓ]. Both g and h are assumed to vanish at the endpoints
of [0, ℓ].
</p>
<p>The solution of the wave equation on R provided in Theorem 4.1 can be adapted
</p>
<p>to the boundary conditions (4.15). The idea is to extend g, h to R in such a way that
</p>
<p>the formula (4.8) gives a solution satisfying the boundary conditions for all t .
</p>
<p>Theorem 4.5 The wave equation (4.5) on [0, ℓ], with Dirichlet boundary conditions
and satisfying the initial conditions (4.16), admits a solution of the form (4.8), only
</p>
<p>if the initial data extensions to R as odd, 2ℓ-periodic functions, with g &isin; C2(R) and
h &isin; C1(R).
Proof By linearity we can consider the g and h terms independently. Assume that
</p>
<p>the g term,
1
</p>
<p>2
</p>
<p>[
</p>
<p>g(x + ct)+ g(x &minus; ct)
]
</p>
<p>, (4.17)
</p>
<p>is defined for all t and x and satisfies the boundary conditions on [0, ℓ] for all values
of t . At x = 0 the condition u(t, 0) = 0 will be satisfied if and only if
</p>
<p>g(ct)+ g(&minus;ct) = 0, for all t &ge; 0.
</p>
<p>In other words, u(t, 0) = 0 if and only if g is odd. At x = ℓ the condition is
</p>
<p>g(ℓ+ ct)+ g(ℓ&minus; ct) = 0, for all t &ge; 0.
</p>
<p>This is equivalent to the condition that g is odd with respect to reflection at the point
</p>
<p>x = ℓ.
The composition of the reflections about 0 and ℓ gives translation by 2ℓ. Hence
</p>
<p>the expression (4.17) satisfies the boundary conditions if and only if g is odd and
</p>
<p>2ℓ-periodic.
</p>
<p>A similar argument works for the h term,
</p>
<p>u(t, x) =
1
</p>
<p>2c
</p>
<p>&int; x+ct
</p>
<p>x&minus;ct
h(τ ) dτ . (4.18)
</p>
<p>The requirement at x = 0 is
&int; ct
</p>
<p>&minus;ct
h(τ ) dτ = 0, for all t &ge; 0. (4.19)
</p>
<p>Differentiation with respect to t , using the fundamental theorem of calculus, shows
</p>
<p>that (4.19) is satisfied if and only if h is odd with respect to reflection at 0. Similarly,
</p>
<p>the condition
&int; ℓ+ct
</p>
<p>ℓ&minus;ct
h(τ ) dτ = 0, for all t &ge; 0
</p>
<p>requires odd symmetry with respect to reflection at x = ℓ. �</p>
<p/>
</div>
<div class="page"><p/>
<p>4.3 Boundary Problems 53
</p>
<p>Example 4.6 Consider the vibrating string problem with c = 1 and ℓ = 1. Suppose
that the solution initially has the form (4.14) with u+ = 0 and the left-propagating
solution given by the function u&minus; shown in Fig. 4.6. For small t &gt; 0 the solution is
</p>
<p>u(t, x) = u&minus;(x + t), (4.20)
</p>
<p>but eventually the bump hits the boundary at x = 0, and we would like to understand
what happens then.
</p>
<p>To apply Theorem 4.5, we must first solve for g and h in terms of u+. By (4.20)
we set g(x) = u+(x) and
</p>
<p>h(x) =
&part;
</p>
<p>&part;t
u&minus;(x + t)
</p>
<p>∣
</p>
<p>∣
</p>
<p>t=0 =
du&minus;
</p>
<p>dx
(x).
</p>
<p>The resulting functions g and h, extended to odd functions onR, are shown in Fig. 4.7.
</p>
<p>According to Theorem 4.5 we can compute the solution from (4.8) using these
</p>
<p>odd periodic extensions of g and h. The results are shown in Fig. 4.8. The bump
</p>
<p>temporarily disappears at t = 0.3 and then reemerges as an inverted bump traveling
in the opposite direction. &diams;
</p>
<p>4.4 Forcing Terms
</p>
<p>The derivation of the string model in Sect. 4.1 assumed that no external forces act
</p>
<p>on the string. Additional forces could be incorporated by adding extra terms to the
</p>
<p>expression (4.1) for the force on a segment. In the continuum limit this yields a
</p>
<p>Fig. 4.6 The initial waveform u+
</p>
<p>Fig. 4.7 The odd extensions of the initial conditions g and h</p>
<p/>
</div>
<div class="page"><p/>
<p>54 4 The Wave Equation
</p>
<p>Fig. 4.8 Reflection of a propagating bump at the endpoint of the string
</p>
<p>forcing term on the right-hand side of (4.5):
</p>
<p>&part;2u
</p>
<p>&part;t2
&minus; c2
</p>
<p>&part;2u
</p>
<p>&part;x2
= f, (4.21)
</p>
<p>where f = f (t, x). The forcing term could be used to model plucking or bowing of
the string, for example.
</p>
<p>In this section we introduce a technique, calledDuhamel&rsquo;s method, that allows us
</p>
<p>to adapt solution methods for evolution equations to include a forcing term. The idea,
</p>
<p>which is closely related to a standard ODE technique called variation of parameters,
</p>
<p>is to reformulate the forcing term as an initial condition. This technique is named
</p>
<p>for the 19th century French mathematician and physicist Jean-Marie Duhamel, who
</p>
<p>developed the idea in a study of the heat equation.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Forcing Terms 55
</p>
<p>Fig. 4.9 Domain of
</p>
<p>dependence for the point
</p>
<p>(t, x)
</p>
<p>To focus our attention on the driving term, let us consider (4.21) on the domain
</p>
<p>x &isin; R with the initial conditions set to zero. For a given c, define the domain of
dependence of a point (t, x) with t &gt; 0 and x &isin; R by
</p>
<p>Dt,x :=
{
</p>
<p>(s, x &prime;) &isin; R+ &times; R : x &minus; c(t &minus; s) &le; x &prime; &le; x + c(t &minus; s)
}
</p>
<p>.
</p>
<p>This is a triangular region, as pictured in Fig. 4.9. The terminology refers to the fact
</p>
<p>that the solution u(t, x) is influenced only by the values of f within Dt,x , as the next
</p>
<p>result shows.
</p>
<p>Theorem 4.7 For f &isin; C1(R), the unique solution of (4.21) satisfying the initial
conditions
</p>
<p>u(0, x) = 0,
&part;u
</p>
<p>&part;t
(0, x) = 0,
</p>
<p>is given by
</p>
<p>u(t, x) =
1
</p>
<p>2c
</p>
<p>&int;
</p>
<p>Dt,x
</p>
<p>f (s, x &prime;) dx &prime; ds. (4.22)
</p>
<p>Proof For each s &ge; 0, let ηs(t, x) be the solution of the homogeneous wave equation
(4.5) for t &ge; s, subject to the initial conditions
</p>
<p>ηs(t, x)
∣
</p>
<p>∣
</p>
<p>t=s = 0,
&part;ηs
</p>
<p>&part;t
(t, x)
</p>
<p>∣
</p>
<p>∣
</p>
<p>t=s = f (s, x). (4.23)
</p>
<p>This function can be written explicitly by shifting t to t &minus; s in (4.8),
</p>
<p>ηs(t, x) =
1
</p>
<p>2c
</p>
<p>&int; x+c(t&minus;s)
</p>
<p>x&minus;c(t&minus;s)
f (s, x &prime;) dx &prime;. (4.24)
</p>
<p>We claim that the solution of (4.21) is given by the integral
</p>
<p>u(t, x) :=
&int; t
</p>
<p>0
</p>
<p>ηs(t, x) ds. (4.25)</p>
<p/>
</div>
<div class="page"><p/>
<p>56 4 The Wave Equation
</p>
<p>Note that the integration variable here is s rather than t .
</p>
<p>We will first check that this definition of u satisfies the initial conditions. For
</p>
<p>t = 0 the integral in (4.25) clearly vanishes, so that u(0, x) = 0 is satisfied. By the
fundamental theorem of calculus, differentiating (4.25) with respect to t gives
</p>
<p>&part;u
</p>
<p>&part;t
(t, x) = ηs(t, x)
</p>
<p>∣
</p>
<p>∣
</p>
<p>s=t +
&int; t
</p>
<p>0
</p>
<p>&part;ηs
</p>
<p>&part;t
(t, x) ds.
</p>
<p>The first term vanishes for all t by the initial condition (4.23), leaving
</p>
<p>&part;u
</p>
<p>&part;t
(t, x) =
</p>
<p>&int; t
</p>
<p>0
</p>
<p>&part;ηs
</p>
<p>&part;t
(t, x) ds, (4.26)
</p>
<p>for all t &ge; 0. Setting t = 0 gives
</p>
<p>&part;u
</p>
<p>&part;t
(0, x) = 0.
</p>
<p>Now let us check that the u defined in (4.25) solves (4.21). Differentiating (4.26)
</p>
<p>once more gives
</p>
<p>&part;2u
</p>
<p>&part;t2
(t, x) =
</p>
<p>&part;ηs
</p>
<p>&part;t
(t, x)
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>s=t
+
</p>
<p>&int; t
</p>
<p>0
</p>
<p>&part;2ηs
</p>
<p>&part;t2
(t, x) ds. (4.27)
</p>
<p>By (4.23) the first term on the right is equal to f (t, x). To simplify the second term,
</p>
<p>we use the fact that ηs solves (4.5) and the definition of u to compute
</p>
<p>&int; t
</p>
<p>0
</p>
<p>&part;2ηs
</p>
<p>&part;t2
(t, x) ds = c2
</p>
<p>&int; t
</p>
<p>0
</p>
<p>&part;2ηs
</p>
<p>&part;x2
(t, x) ds
</p>
<p>= c2
&part;2u
</p>
<p>&part;x2
.
</p>
<p>Therefore, (4.27) reduces to
</p>
<p>&part;2u
</p>
<p>&part;t2
= f + c2
</p>
<p>&part;2u
</p>
<p>&part;x2
,
</p>
<p>proving that u solves (4.21).
</p>
<p>Combining (4.24) and (4.25), we can write the formula for u as
</p>
<p>u(t, x) =
1
</p>
<p>2c
</p>
<p>&int; t
</p>
<p>0
</p>
<p>(&int; x+c(t&minus;s)
</p>
<p>x&minus;c(t&minus;s)
f (s, x &prime;) dx &prime;
</p>
<p>)
</p>
<p>ds,
</p>
<p>which is equivalent to (4.22).</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Forcing Terms 57
</p>
<p>Fig. 4.10 Range of
</p>
<p>influence of the point (t0, x0)
</p>
<p>To prove uniqueness, suppose u1 and u2 are solutions of (4.21). Then u2 &minus; u1 is
a solution of (4.5). Since u2 &minus; u1 also has vanishing initial conditions, Theorem 4.1
implies that u2 &minus; u1 = 0. Hence the solution is unique. �
</p>
<p>By the superposition principle, Theorem 4.7 is easily extended to the case of
</p>
<p>nonzero initial conditions, by setting u = v + w where v is a solution of the form
(4.22) and w is a solution of the form (4.8).
</p>
<p>The concept of domain of dependence still applies when the initial conditions are
</p>
<p>nonzero. In the solution formula (4.8), u(t, x) depends only on the values of g and
</p>
<p>h at the base of the triangle, Dt,x &cap; {t = 0}. Thus it is still the case that the solution
u(t, x) depends only on the data within Dt,x .
</p>
<p>The existence of the domain of dependence is a limitation imposed by the prop-
</p>
<p>agation speed c. For systems governed by the wave equations (4.5) or (4.21), no
</p>
<p>information can travel at a speed faster than c.
</p>
<p>The region of the space-time plane in which solutions can be affected by the data
</p>
<p>at a particular point (t0, x0) is called the range of influence of this point. By the
</p>
<p>definition of the domain of dependence, the range of influence consists of the points
</p>
<p>(t, x) such that (t0, x0) &isin; Dt,x . This region is a triangle with vertex (t0, x0) and sides
given by the characteristics (t, x0 &plusmn; c(t &minus; t0)), as shown in Fig. 4.10.
</p>
<p>Duhamel&rsquo;s method applies also to the case of a vibrating string with fixed ends.
</p>
<p>Assuming that f (t, x) satisfies the boundary conditions at x = 0 and x = ℓ, we
extend f to an odd 2ℓ-periodic function on R, just as in Theorem 4.5. This exten-
</p>
<p>sion guarantees that the intermediate solution ηs defined by (4.24) will satisfy the
</p>
<p>boundary conditions also. And then so will the solution u(t, x) defined by (4.25).
</p>
<p>Example 4.8 Consider a string of length ℓ with propagation speed c = 1. Suppose
the forcing term is given by
</p>
<p>f (t, x) = cos(ωt) sin(ω0x), (4.28)
</p>
<p>where ω0 := π/ℓ and ω &gt; 0 is the driving frequency. Since sin(ω0x) is odd and
2π-periodic, the extension required by Theorem 4.5 is automatic. As in Theorem 4.7,
</p>
<p>let us set the initial conditions g = h = 0 to focus on the forcing term.
Substituting (4.28) into (4.22) gives</p>
<p/>
</div>
<div class="page"><p/>
<p>58 4 The Wave Equation
</p>
<p>u(t, x) =
1
</p>
<p>2
</p>
<p>&int; t
</p>
<p>0
</p>
<p>&int; x+t&minus;s
</p>
<p>x&minus;t+s
cos(ωs) sin(ω0x
</p>
<p>&prime;) dx &prime; ds
</p>
<p>=
1
</p>
<p>2ω0
</p>
<p>&int; t
</p>
<p>0
</p>
<p>[cos(ω0(x &minus; t + s))&minus; cos(ω0(x + t &minus; s))] cos(ωs) ds
</p>
<p>A trigonometric identity reduces this to
</p>
<p>u(t, x) =
sin(ω0x)
</p>
<p>ω0
</p>
<p>&int; t
</p>
<p>0
</p>
<p>sin(ω0(t &minus; s)) cos(ωs) ds. (4.29)
</p>
<p>For ω &#13;= ω0 we obtain
</p>
<p>u(t, x) =
sin(ω0x)
</p>
<p>ω20 &minus; ω2
[cos(ωt)&minus; cos(ω0t)] .
</p>
<p>Note that the x dependence of the solution matches that of the forcing term. The
</p>
<p>interesting part of this solution is the oscillation, which includes both frequencies ω
</p>
<p>and ω0. Figure4.11 illustrates the behavior of the amplitude as a function of time, in
</p>
<p>a case where ω ≪ ω0. The large-scale oscillation has a period 1/ω, corresponding to
the low driving frequency. The solution also exhibits fast oscillations at the frequency
</p>
<p>ω0 which depends only on ℓ.
</p>
<p>For ω = ω0 the formula (4.29) gives the solution
</p>
<p>u(t, x) =
t
</p>
<p>2ω0
sin(ω0x) sin(ω0t).
</p>
<p>The resulting amplitude grows linearly, as shown in Fig. 4.12. &diams;
</p>
<p>The physical phenomenon illustrated by Example 4.8 is called resonance. If the
</p>
<p>string is driven at its natural frequency ω0 then it will continually absorb energy from
</p>
<p>the driving force. Of course, there is a limit to how much energy a physical string
</p>
<p>could absorb before it breaks. Once the displacement amplitude becomes sufficiently
</p>
<p>large, the linear wave equation (4.5) no longer serves as an appropriate model.
</p>
<p>Fig. 4.11 Oscillation pattern
</p>
<p>with a driving frequency
</p>
<p>ω = ω0/10</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Model Problem: Acoustic Waves 59
</p>
<p>Fig. 4.12 Growth of the
</p>
<p>amplitude at the resonance
</p>
<p>frequency ω = ω0
</p>
<p>4.5 Model Problem: Acoustic Waves
</p>
<p>The vibration of a drumhead can be modeled on a bounded domain Ω &sub; R2, with
a function u(t, x) representing the vertical displacement of the membrane at time t
</p>
<p>and position x &isin; Ω . With arguments similar to those in Sect. 4.1, one can derive the
equation
</p>
<p>&part;2u
</p>
<p>&part;t2
&minus; c2�u = 0, (4.30)
</p>
<p>where � is the Laplacian operator (1.7). The wave equation (4.30) appears in many
</p>
<p>other contexts as well, including the propagation of light and all other forms of
</p>
<p>electromagnetic radiation. In all these cases the constant c represents the speed of
</p>
<p>propagation.
</p>
<p>In this section we will derive the three-dimensional wave equation as a model
</p>
<p>for acoustic waves traveling through the air. Acoustic waves consist of fluctuations
</p>
<p>of pressure which propagate through a gas. To analyze them, we must consider the
</p>
<p>relationships between the pressure P , the velocity field v, and the density ρ. For a
</p>
<p>gas in motion these are all functions of both time and position.
</p>
<p>Because acoustic waves involve minute pressure fluctuations with very little heat
</p>
<p>transfer, the relationship between pressure and density is given by the adiabatic gas
</p>
<p>law
</p>
<p>P = Cργ, (4.31)
</p>
<p>where C and γ are physical constants. We will fix background atmospheric values
</p>
<p>of the pressure P0 and density ρ0 and focus on the deviations
</p>
<p>u := P &minus; P0, σ := ρ &minus; ρ0.
</p>
<p>Applying (4.31) to P/P0 gives the equation
</p>
<p>1+
u
</p>
<p>P0
=
</p>
<p>(
</p>
<p>1+
σ
</p>
<p>ρ0
</p>
<p>)γ
</p>
<p>.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_1">http://dx.doi.org/10.1007/978-3-319-48936-0_1</a></div>
</div>
<div class="page"><p/>
<p>60 4 The Wave Equation
</p>
<p>Since σ/ρ0 is assumed to be very small, we can linearize by taking a first-order
</p>
<p>Taylor approximation on the right side. This yields
</p>
<p>u =
γP0
</p>
<p>ρ0
σ. (4.32)
</p>
<p>The dynamics of the gas are modeled with two conservation laws. The first is
</p>
<p>conservation of mass (3.20), which yields
</p>
<p>&part;ρ
</p>
<p>&part;t
+&nabla; &middot; (ρv) = 0.
</p>
<p>Since σ and v are both assumed to be very small, for the leading approximation we
</p>
<p>can replace ρ by ρ0 to obtain
</p>
<p>&part;σ
</p>
<p>&part;t
+ ρ0&nabla; &middot; v = 0. (4.33)
</p>
<p>The second dynamical law is conservation of momentum. This is encapsulated in
</p>
<p>a fluid equation derived by Euler in 1757, called Euler&rsquo;s force equation:
</p>
<p>&minus;&nabla;P = ρ
(
</p>
<p>&part;
</p>
<p>&part;t
+ v &middot; &nabla;
</p>
<p>)
</p>
<p>v.
</p>
<p>Euler&rsquo;s equation is an aggregate form of Newton&rsquo;s second law (force equals mass
</p>
<p>times acceleration). Note that the &ldquo;acceleration&rdquo; term on the right is the Lagrangian
</p>
<p>derivative of the velocity field v. As above, we substitute P = P0+u and ρ = ρ0+σ
and keep only the first order terms to derive the linearization
</p>
<p>&minus;&nabla;u = ρ0
&part;v
</p>
<p>&part;t
. (4.34)
</p>
<p>The final step is to eliminate the velocity field from the equation. Substituting
</p>
<p>(4.32) into (4.33) and differentiating with respect to time gives
</p>
<p>&part;2u
</p>
<p>&part;t2
= &minus;γP0
</p>
<p>&part;
</p>
<p>&part;t
(&nabla; &middot; v). (4.35)
</p>
<p>On the other hand, by (4.34),
</p>
<p>&part;
</p>
<p>&part;t
(&nabla; &middot; v) = &nabla; &middot;
</p>
<p>&part;v
</p>
<p>&part;t
</p>
<p>= &nabla; &middot;
(
</p>
<p>&minus;
&nabla;u
ρ0
</p>
<p>)
</p>
<p>= &minus;
1
</p>
<p>ρ0
�u.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_3">http://dx.doi.org/10.1007/978-3-319-48936-0_3</a></div>
</div>
<div class="page"><p/>
<p>4.5 Model Problem: Acoustic Waves 61
</p>
<p>Substituting this in (4.35) yields the acoustic wave equation,
</p>
<p>&part;2u
</p>
<p>&part;t2
&minus;
</p>
<p>γP0
</p>
<p>ρ0
�u = 0.
</p>
<p>As with our previous derivations, many approximations are required to produce a
</p>
<p>linear equation. These simplifications are well justified for sound waves at ordinary
</p>
<p>volume levels, but more dramatic pressure fluctuations would require a nonlinear
</p>
<p>equation.
</p>
<p>4.6 Integral Solution Formulas
</p>
<p>Let us consider the wave equation (4.30) on R3 with c = 1,
</p>
<p>&part;2u
</p>
<p>&part;t2
&minus;�u = 0.
</p>
<p>This problem can be reduced to the one-dimensional case by a clever averaging trick.
</p>
<p>For f &isin; C0(R3), define
</p>
<p>f̃ (x; ρ) :=
1
</p>
<p>4πρ
</p>
<p>&int;
</p>
<p>&part;B(x;ρ)
f (w) dS(w), (4.36)
</p>
<p>where x &isin; R3 and ρ &gt; 0. The surface area of &part;B(x; ρ) is 4πρ2, so f̃ is ρ times the
spherical average of f . By continuity the spherical average approaches the value of
</p>
<p>the function at the center point as ρ &rarr; 0, so that
</p>
<p>lim
ρ&rarr;0
</p>
<p>f̃ (x; ρ)
ρ
</p>
<p>= f (x). (4.37)
</p>
<p>The dimensional reduction of the wave equation is based on the following formula
</p>
<p>of Jean-Gaston Darboux.
</p>
<p>Lemma 4.9 (Darboux&rsquo;s formula) For f &isin; C2(R3),
</p>
<p>&part;2
</p>
<p>&part;ρ2
f̃ (x; ρ) = �x f̃ (x; ρ).
</p>
<p>Proof To compute the radial derivative of the spherical average, it is helpful to change
</p>
<p>coordinates by settingw = x+ρ y, so that the domain of y is the unit sphereS2 &sub; R3,
</p>
<p>1
</p>
<p>4πρ2
</p>
<p>&int;
</p>
<p>&part;B(x;ρ)
f (w) dS(w) =
</p>
<p>1
</p>
<p>4π
</p>
<p>&int;
</p>
<p>S2
</p>
<p>f (x + ρ y) dS( y).</p>
<p/>
</div>
<div class="page"><p/>
<p>62 4 The Wave Equation
</p>
<p>Differentiation under the integral gives,
</p>
<p>&part;
</p>
<p>&part;ρ
</p>
<p>&int;
</p>
<p>S2
</p>
<p>f (x + ρ y) dS( y)
1
</p>
<p>4π
=
</p>
<p>&int;
</p>
<p>S2
</p>
<p>&nabla; f (x + ρ y) &middot; y dS( y).
</p>
<p>In the original coordinates this implies
</p>
<p>&part;
</p>
<p>&part;ρ
</p>
<p>[
</p>
<p>1
</p>
<p>4πρ2
</p>
<p>&int;
</p>
<p>&part;B(x;ρ)
f (w) dS(w)
</p>
<p>]
</p>
<p>=
1
</p>
<p>4πρ2
</p>
<p>&int;
</p>
<p>&part;B(x;ρ)
&nabla; f (w) &middot;
</p>
<p>(
</p>
<p>w &minus; x
ρ
</p>
<p>)
</p>
<p>dS(w).
</p>
<p>(4.38)
</p>
<p>Since (w &minus; x)/ρ is the outward unit normal to &part;B(x; ρ),
</p>
<p>&nabla; f (w) &middot;
(
</p>
<p>w &minus; x
ρ
</p>
<p>)
</p>
<p>=
&part; f
</p>
<p>&part;ν
(w).
</p>
<p>Furthermore, by Corollary 2.8,
</p>
<p>&int;
</p>
<p>&part;B(x;ρ)
</p>
<p>&part; f
</p>
<p>&part;ν
(w) dS(w) =
</p>
<p>&int;
</p>
<p>B(x;ρ)
� f (w) d3w.
</p>
<p>Applying this to the right-hand side of (4.38) gives
</p>
<p>&part;
</p>
<p>&part;ρ
</p>
<p>[
</p>
<p>1
</p>
<p>4πρ2
</p>
<p>&int;
</p>
<p>&part;B(x;ρ)
f (w) dS(w)
</p>
<p>]
</p>
<p>=
1
</p>
<p>4πρ2
</p>
<p>&int;
</p>
<p>B(x;ρ)
� f (w) d3w. (4.39)
</p>
<p>Substituting the definition of f̃ in (4.39) yields
</p>
<p>&part;
</p>
<p>&part;ρ
f̃ (x; ρ) =
</p>
<p>1
</p>
<p>4πρ2
</p>
<p>&int;
</p>
<p>&part;B(x;ρ)
f (w) dS(w)+
</p>
<p>1
</p>
<p>4πρ
</p>
<p>&int;
</p>
<p>B(x;ρ)
� f (w) d3w
</p>
<p>A further differentiation using (4.39) and the radial derivative formula from Exer-
</p>
<p>cise 2.4 then gives
</p>
<p>&part;2
</p>
<p>&part;ρ2
f̃ (x; ρ) =
</p>
<p>1
</p>
<p>4πρ
</p>
<p>&part;
</p>
<p>&part;ρ
</p>
<p>&int;
</p>
<p>B(x;ρ)
� f (w) d3w
</p>
<p>=
1
</p>
<p>4πρ
</p>
<p>&int;
</p>
<p>&part;B(x;ρ)
� f (w) dS(w).
</p>
<p>(4.40)
</p>
<p>On the other hand</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>4.6 Integral Solution Formulas 63
</p>
<p>�x f̃ (x; ρ) = �x
[
</p>
<p>1
</p>
<p>4πρ
</p>
<p>&int;
</p>
<p>S2
</p>
<p>f (x + ρ y) dS( y)
]
</p>
<p>=
1
</p>
<p>4πρ
</p>
<p>&int;
</p>
<p>S2
</p>
<p>� f (x + ρ y) dS( y)
</p>
<p>=
1
</p>
<p>4πρ
</p>
<p>&int;
</p>
<p>&part;B(x;ρ)
� f (w) dS(w).
</p>
<p>(4.41)
</p>
<p>The claim thus follows from (4.40). �
</p>
<p>Lemma 4.9 allows us to relate the three-dimensional wave equation in variables
</p>
<p>(t, x) to a one-dimensional equation in variables (t, ρ). The result is a solution
</p>
<p>formula first derived in 1883 by the physicist Gustav Kirchhoff.
</p>
<p>Theorem 4.10 (Kirchhoff&rsquo;s integral formula) For u &isin; C2([0,&infin;) &times; R3), suppose
that
</p>
<p>&part;2u
</p>
<p>&part;t2
&minus;�u = 0
</p>
<p>under the initial conditions
</p>
<p>u|t=0 = g,
&part;u
</p>
<p>&part;t
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>t=0
= h.
</p>
<p>Then
</p>
<p>u(t, x) =
&part;
</p>
<p>&part;t
g̃(x; t)+ h̃(x; t),
</p>
<p>with g̃ and h̃ defined as in (4.36).
</p>
<p>Proof Define
</p>
<p>ũ(t, x; ρ) :=
1
</p>
<p>4πρ
</p>
<p>&int;
</p>
<p>&part;B(x;ρ)
u(t,w) dS(w).
</p>
<p>Since u satisfies the wave equation, differentiating under the integral gives
</p>
<p>&part;2
</p>
<p>&part;t2
ũ(t, x; ρ) =
</p>
<p>1
</p>
<p>4πρ
</p>
<p>&int;
</p>
<p>&part;B(x;ρ)
�u(t,w) dS(w).
</p>
<p>By the calculation (4.41) this is equivalent to
</p>
<p>&part;2
</p>
<p>&part;t2
ũ(t, x; ρ) = �x ũ(t, x; ρ).
</p>
<p>Lemma 4.9 then shows that
</p>
<p>(
</p>
<p>&part;2
</p>
<p>&part;t2
&minus;
</p>
<p>&part;2
</p>
<p>&part;ρ2
</p>
<p>)
</p>
<p>ũ(t, x; ρ) = 0. (4.42)</p>
<p/>
</div>
<div class="page"><p/>
<p>64 4 The Wave Equation
</p>
<p>The initial conditions for ũ follow from the initial conditions for u,
</p>
<p>ũ(0, x; ρ) = g̃(x; ρ),
&part;
</p>
<p>&part;t
ũ(0, x; ρ) = h̃(x; ρ).
</p>
<p>By (4.37) we also have a boundary condition at ρ = 0,
</p>
<p>ũ(t, x; 0) = 0.
</p>
<p>Using Theorem 4.1 and the reflection argument from Theorem 4.5, we conclude
</p>
<p>that the unique solution of (4.42) under these conditions is given by extending g̃(x; ρ)
and h̃(x; ρ) to ρ &isin; R with odd symmetry and then using the d&rsquo;Alembert formula,
</p>
<p>ũ(t, x; ρ) =
1
</p>
<p>2
</p>
<p>[
</p>
<p>g̃(x; ρ + t)+ g̃(x; ρ &minus; t)
]
</p>
<p>+
1
</p>
<p>2
</p>
<p>&int; ρ+t
</p>
<p>ρ&minus;t
h̃(x; τ ) dτ . (4.43)
</p>
<p>By (4.37), we can recover u from this formula by setting
</p>
<p>u(t, x) = lim
ρ&rarr;0
</p>
<p>ũ(t, x; ρ)
ρ
</p>
<p>. (4.44)
</p>
<p>To evaluate this limit, first note that for 0 &le; ρ &le; t the odd symmetry of g̃ and h̃ with
respect to ρ can be used to rewrite (4.43) as
</p>
<p>ũ(t, x; ρ) =
1
</p>
<p>2
</p>
<p>[
</p>
<p>g̃(x; t + ρ)&minus; g̃(x; t &minus; ρ)
]
</p>
<p>+
1
</p>
<p>2
</p>
<p>&int; t+ρ
</p>
<p>t&minus;ρ
h̃(x; τ ) dτ .
</p>
<p>The computations are now straightforward:
</p>
<p>lim
ρ&rarr;0
</p>
<p>1
</p>
<p>2ρ
</p>
<p>[
</p>
<p>g̃(x; t + ρ)&minus; g̃(x; t &minus; ρ)
]
</p>
<p>=
&part;
</p>
<p>&part;t
g̃(x; t),
</p>
<p>and
</p>
<p>lim
ρ&rarr;0
</p>
<p>1
</p>
<p>2ρ
</p>
<p>&int; t+ρ
</p>
<p>t&minus;ρ
h̃(x; τ ) dτ = h̃(x; t).
</p>
<p>The claimed solution formula thus follows from (4.44). �
</p>
<p>One interesting consequence of the Kirchhoff formula is the fact that three-
</p>
<p>dimensional wave propagation exhibits a strict form of the Huygens&rsquo; principle. The-
</p>
<p>orem 4.10 shows that the range of influence of the point (t0, x0) is the forward light
</p>
<p>cone,
</p>
<p>Γ+(t0, x0) := {(t, x); t &gt; t0, |x &minus; x0| = t &minus; t0} .
</p>
<p>This matches the result of Theorem 4.3 for the one-dimensional wave equation. The
</p>
<p>strict Huygens phenomenon is readily observable for acoustic waves, in the fact that</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 Integral Solution Formulas 65
</p>
<p>Fig. 4.13 The plot on the right shows the observed waveform at a distance 3 from the origin, caused
</p>
<p>by the initial radial impulse shown on the left
</p>
<p>a sudden sound like a clap propagates as a sharp wavefront that is heard as a single
</p>
<p>discrete event, without aftereffects unless there are reflective surfaces to cause an
</p>
<p>echo. Figure4.13 illustrates this effect; an observer located away from the origin
</p>
<p>experiences a waveform of duration equal to the diameter of the initial impulse. The
</p>
<p>strict Huygens&rsquo; principle holds in every odd dimension greater than 1, but fails in
</p>
<p>even dimensions, as we will illustrate below.
</p>
<p>The spherical averaging trick used for Theorem 4.10 also works in higher odd
</p>
<p>dimensions, although the solution formulas become more complicated. For even
</p>
<p>dimensions, solution formulas can be derived from the odd-dimensional case by a
</p>
<p>technique called the method of descent.
</p>
<p>Wewill work this out for the two-dimensional case. Suppose u &isin; C2([0,&infin;)&times;R2)
solves the wave equation with initial conditions
</p>
<p>u|t=0 = g,
&part;u
</p>
<p>&part;t
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>t=0
= h,
</p>
<p>with g, h functions onR2. If we extend g and h toR3 as functions that are independent
</p>
<p>of x3, then Kirchhoff&rsquo;s formula gives a solution to the three dimensional problem.
</p>
<p>Since this solution is also independent of x3, it &ldquo;descends&rdquo; to a solution in R
2. The
</p>
<p>resulting formula was first worked out by Sim&eacute;on Poisson in the early 19th century
</p>
<p>(well before Kirchhoff&rsquo;s three-dimensional formula).
</p>
<p>Corollary 4.11 (Poisson&rsquo;s integral formula)For u &isin; C2([0,&infin;)&times;R2), suppose that
</p>
<p>&part;2u
</p>
<p>&part;t2
&minus;�u = 0
</p>
<p>under the initial conditions
</p>
<p>u|t=0 = g,
&part;u
</p>
<p>&part;t
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>t=0
= h.
</p>
<p>Then</p>
<p/>
</div>
<div class="page"><p/>
<p>66 4 The Wave Equation
</p>
<p>u(t, x) =
&part;
</p>
<p>&part;t
</p>
<p>(
</p>
<p>t
</p>
<p>2π
</p>
<p>&int;
</p>
<p>D
</p>
<p>g(x &minus; t y)
&radic;
</p>
<p>1&minus; | y|2
d2 y
</p>
<p>)
</p>
<p>+
t
</p>
<p>2π
</p>
<p>&int;
</p>
<p>D
</p>
<p>h(x &minus; t y)
&radic;
</p>
<p>1&minus; | y|2
d2 y.
</p>
<p>Proof Following the procedure described above, we extend g and h to functions on
</p>
<p>R
3 independent of x3. In this case the integral (4.36) becomes
</p>
<p>g̃(x; ρ) =
ρ
</p>
<p>4π
</p>
<p>&int;
</p>
<p>S2
</p>
<p>g(x1 + ρy1, x2 + ρy2) dS( y) (4.45)
</p>
<p>for x &isin; R2. By symmetry we can restrict our attention to the upper hemisphere,
parametrized in polar coordinates by
</p>
<p>y =
(
</p>
<p>r cos θ, r sin θ,
&radic;
</p>
<p>1&minus; r2
)
</p>
<p>.
</p>
<p>The surface area element is
</p>
<p>dS =
r
</p>
<p>&radic;
1&minus; r2
</p>
<p>dr dθ,
</p>
<p>so that (4.45) becomes
</p>
<p>g̃(x; ρ) =
ρ
</p>
<p>2π
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>&int; r
</p>
<p>0
</p>
<p>g(x1 + ρr cos θ, x2 + ρr sin θ)&radic;
1&minus; r2
</p>
<p>r dr dθ
</p>
<p>=
ρ
</p>
<p>2π
</p>
<p>&int;
</p>
<p>D
</p>
<p>g(x + ρ y)
&radic;
</p>
<p>1&minus; | y|2
d2 y.
</p>
<p>The claimed two-dimensional solution follows by substituting this formula for g̃ and
</p>
<p>the corresponding result for h̃ in the Kirchhoff formula from Theorem 4.10. �
</p>
<p>Corollary 4.11 shows that the rangeof influence of (t0, x0) for the two-dimensional
</p>
<p>wave equation is the solid region bounded by the forward light cone Γ+(t0, x0), not
just the surface. Thus, in R2 the wave caused by a sudden disturbance has a lingering
</p>
<p>&ldquo;tail&rdquo; after the initial wavefront has passed, as illustrated in Fig. 4.14.
</p>
<p>Fig. 4.14 Two-dimensional
</p>
<p>waveform observed at a
</p>
<p>distance 3 from the origin,
</p>
<p>corresponding to the radial
</p>
<p>impulse shown on the left in
</p>
<p>Fig. 4.13</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 Integral Solution Formulas 67
</p>
<p>In all dimensions, solutions of the wave equation exhibit a phenomenon known
</p>
<p>as finite propagation speed. If the constant c is reinstated as in (4.30), then the range
</p>
<p>of influence is restricted to spacetime points that are reachable at a speed less than
</p>
<p>or equal to c.
</p>
<p>4.7 Energy and Uniqueness
</p>
<p>In Theorem 4.1, uniqueness of solutions was a consequence of the method of char-
</p>
<p>acteristics. In this section we will present an alternative approach, which allows us
</p>
<p>to deduce uniqueness directly from the equation without requiring any knowledge
</p>
<p>of the solution other than differentiability. This argument is based on the concept of
</p>
<p>energy of a solution, which proves to be a powerful tool for analyzing many different
</p>
<p>types of PDE.
</p>
<p>To motivate the definition, let us specialize again to the case of a string of length
</p>
<p>ℓ with fixed ends. Assume that u &isin; C2([0,&infin;] &times; [0, ℓ]) satisfies the string wave
equation (4.4) with Dirichlet boundary conditions. In the discrete model of the string
</p>
<p>used for the derivation of the equation, the segment of length Δx located at x j had
</p>
<p>mass ρΔx and velocity &part;u
&part;t
(x j ). By the standard expression for the kinetic energy of a
</p>
<p>moving particle, 1
2
(mass)&times;(velocity)2, the kinetic energy of this segment is therefore
</p>
<p>1
</p>
<p>2
ρΔx
</p>
<p>[
</p>
<p>&part;u
</p>
<p>&part;t
(x j )
</p>
<p>]2
</p>
<p>.
</p>
<p>Summing over the segments and passing the continuum limit gives a formula for the
</p>
<p>total kinetic energy of the string:
</p>
<p>EK :=
ρ
</p>
<p>2
</p>
<p>&int; ℓ
</p>
<p>0
</p>
<p>(
</p>
<p>&part;u
</p>
<p>&part;t
</p>
<p>)2
</p>
<p>dx .
</p>
<p>The potential energy of the solution can be calculated as the energy required to
</p>
<p>move the string fromzerodisplacement into the configurationdescribedbyu(t, &middot;). Let
us represent this process by scaling the displacement to su(t, &middot;) for s &isin; [0, 1]. By (4.1)
the opposing force generated by the tension also scales proportionally to s. The work
</p>
<p>required to shift the segment at x j from s to s+Δs is therefore sΔF(t, x j )u(t, x j )Δs.
The potential energy associated with this segment is
</p>
<p>ΔEP(t, x j ) := &minus;
&int; 1
</p>
<p>0
</p>
<p>sΔF(t, x j )u(t, x j ) ds
</p>
<p>= &minus;
1
</p>
<p>2
u(t, x j )ΔF(t, x j )
</p>
<p>&asymp; &minus;
T
</p>
<p>2
u(t, x j )
</p>
<p>&part;2u
</p>
<p>&part;x2
(t, x j )Δx,</p>
<p/>
</div>
<div class="page"><p/>
<p>68 4 The Wave Equation
</p>
<p>with a minus sign because the displacement and force are in opposing directions.
</p>
<p>Summing over the segments and taking the continuum limit gives the total poten-
</p>
<p>tial energy,
</p>
<p>EP(t) := &minus;
T
</p>
<p>2
</p>
<p>&int; ℓ
</p>
<p>0
</p>
<p>u
&part;2u
</p>
<p>&part;x2
dx .
</p>
<p>For comparison to the kinetic term, it is convenient to integrate by parts and rewrite
</p>
<p>this in the form
</p>
<p>EP(t) =
T
</p>
<p>2
</p>
<p>&int; ℓ
</p>
<p>0
</p>
<p>(
</p>
<p>&part;u
</p>
<p>&part;x
</p>
<p>)2
</p>
<p>dx .
</p>
<p>The total energy of the one-dimensional string at time t is given by
</p>
<p>E = EK + EP .
</p>
<p>For the higher-dimensional wave equation (4.30) on a domain Ω &sub; Rn the cor-
responding definition is
</p>
<p>E[u](t) :=
1
</p>
<p>2
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>[
</p>
<p>(
</p>
<p>&part;u
</p>
<p>&part;t
</p>
<p>)2
</p>
<p>+ c2|&nabla;u|2
]
</p>
<p>dnx (4.46)
</p>
<p>This is well-defined for u &isin; C2([0,&infin;)&times;Ω), provided Ω is bounded.
</p>
<p>Theorem 4.12 SupposeΩ &sub; Rn is a bounded domain with piecewise C1 boundary.
If u &isin; C2([0,&infin;) &times;Ω) is a solution of (4.30) with u|&part;Ω = 0, then the energy E[u]
defined by (4.46) is independent of t .
</p>
<p>Proof The assumptions on u justify differentiating under the integral, so that
</p>
<p>d
</p>
<p>dt
E[u] =
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>[
</p>
<p>&part;u
</p>
<p>&part;t
</p>
<p>&part;2u
</p>
<p>&part;t2
+ c2&nabla;
</p>
<p>(
</p>
<p>&part;u
</p>
<p>&part;t
</p>
<p>)
</p>
<p>&middot; &nabla;u
]
</p>
<p>dnx.
</p>
<p>Under the condition u|&part;Ω = 0, Green&rsquo;s first identity (Theorem 2.10) applies to the
second term to give
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>&nabla;
(
</p>
<p>&part;u
</p>
<p>&part;t
</p>
<p>)
</p>
<p>&middot; &nabla;u dnx = &minus;
&int;
</p>
<p>Ω
</p>
<p>&part;u
</p>
<p>&part;t
�u dnx. (4.47)
</p>
<p>Thus
d
</p>
<p>dt
E[u] =
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>[
</p>
<p>&part;u
</p>
<p>&part;t
</p>
<p>(
</p>
<p>&part;2u
</p>
<p>&part;t2
&minus; c2�u
</p>
<p>)]
</p>
<p>dnx,
</p>
<p>and (4.30) implies that E is constant. �
</p>
<p>Corollary 4.13 SupposeΩ &sub; Rn is a bounded domainwith piecewiseC1 boundary.
A solution u &isin; C2(R+ &times;Ω) of the equation</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>4.7 Energy and Uniqueness 69
</p>
<p>&part;2u
</p>
<p>&part;t2
&minus; c2�u = f, u|&part;Ω = 0,
</p>
<p>u|t=0 = g,
&part;u
</p>
<p>&part;t
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>t=0
= h,
</p>
<p>is uniquely determined by the functions f, g, h.
</p>
<p>Proof If u1 and u2 are solutions of the equation with the same initial conditions, then
</p>
<p>w := u1 &minus; u2 satisfies (4.30) with the initial conditions
</p>
<p>w(0, x) = 0,
&part;w
</p>
<p>&part;t
(0, x) = 0.
</p>
<p>At time t = 0 this gives E[w] = 0, and Theorem 4.12 then implies that E[w] = 0
for all t . Since the terms in the integrand of E[w] are non-negative, they must each
vanish. This shows that w is constant, and hence w = 0 by the initial conditions.
Therefore u1 = u2. �
</p>
<p>4.8 Exercises
</p>
<p>4.1 Suppose u(t, x) is a solution of the wave equation (4.5) for x &isin; R. Let P be a
parallelogram in the (t, x) plane whose sides are characteristic lines. Show that the
</p>
<p>value of u at each vertex of P is determined by the values at the other three vertices.
</p>
<p>4.2 The wave equation (4.5) is an appropriate model for the longitudinal vibrations
</p>
<p>of a spring. In this application u(t, x) represents displacement parallel to the spring.
</p>
<p>Suppose that spring has length ℓ and is free at the ends. This corresponds to the
</p>
<p>Neumann boundary conditions
</p>
<p>&part;u
</p>
<p>&part;x
(t, 0) =
</p>
<p>&part;u
</p>
<p>&part;x
(t, ℓ) = 0, for all t &ge; 0.
</p>
<p>Assume the initial conditions are g and h as in (4.16), which also satisfy Neumann
</p>
<p>boundary conditions on [0, ℓ]. Determine the appropriate extensions of g and h
from [0, ℓ] to R so that the solution u(t, x) given by (4.8) will satisfy the Neumann
boundary problem for all t .
</p>
<p>4.3 In the derivation in Sect. 4.1, suppose we include the effect of gravity by adding
</p>
<p>a term &minus;ρgΔx to the discrete equation of motion (4.2), where g &gt; 0 is the constant
of gravitational acceleration. The wave equation is then modified to
</p>
<p>&part;2u
</p>
<p>&part;t2
&minus; c2
</p>
<p>&part;2u
</p>
<p>&part;x2
= &minus;g. (4.48)</p>
<p/>
</div>
<div class="page"><p/>
<p>70 4 The Wave Equation
</p>
<p>Assume that x &isin; [0, ℓ], with u satisfying Dirichlet boundary conditions at the end-
points.
</p>
<p>(a) Find an equilibrium solution u0(x) for (4.48), that satisfies the boundary condi-
</p>
<p>tions but does not depend on time.
</p>
<p>(b) Show that ifu1 is a solutionof the originalwave equation (4.5), alsowithDirichlet
</p>
<p>boundary conditions, then u = u0 + u1 solves (4.48).
</p>
<p>(c) Given the initial conditions u(x, 0) = 0, &part;u
&part;t
(x, 0) = 0, find the corresponding
</p>
<p>initial conditions for u1. Then apply Theorem 4.5 to find u1 and hence solve
</p>
<p>for u.
</p>
<p>4.4 In Example 4.8, let the forcing term be
</p>
<p>f (t, x) = cos(ωt) sin(ωkx),
</p>
<p>with ω &gt; 0 and
</p>
<p>ωk :=
kπ
</p>
<p>ℓ
.
</p>
<p>Find the solution u(t, x) given initial conditions g = h = 0. Include both cases
ω &#13;= ωk and ω = ωk .
</p>
<p>4.5 The telegraph equation is a variant of the wave equation that describes the
</p>
<p>propagation of electrical signals in a one-dimensional cable:
</p>
<p>&part;2u
</p>
<p>&part;t2
+ a
</p>
<p>&part;u
</p>
<p>&part;t
+ bu &minus; c2
</p>
<p>&part;2u
</p>
<p>&part;x2
= 0,
</p>
<p>where u(t, x) is the line voltage, c is the propagation speed, and a, b &gt; 0 are
</p>
<p>determined by electrical properties of the cable (resistance, inductance, etc.). Show
</p>
<p>that the substitution
</p>
<p>u(t, x) = e&minus;at/2w(t, x)
</p>
<p>reduces the telegraph equation to an ordinary wave equation for w, provided a and
</p>
<p>b satisfy a certain condition. Find the general solution in this case. (This result has
</p>
<p>important practical applications, in that the electrical properties of long cables can
</p>
<p>be &ldquo;tuned&rdquo; to eliminate distortion.)
</p>
<p>4.6 An alternative approach to the one-dimensional wave equation is to recast the
</p>
<p>PDE as a pair of ODE. Consider the wave equation with forcing term,
</p>
<p>&part;2u
</p>
<p>&part;t2
&minus; c2
</p>
<p>&part;2u
</p>
<p>&part;x2
= f.
</p>
<p>(a) Define a vector-valued function v = (v1, v2) with components</p>
<p/>
</div>
<div class="page"><p/>
<p>4.8 Exercises 71
</p>
<p>v1 :=
&part;u
</p>
<p>&part;t
, v2 :=
</p>
<p>&part;u
</p>
<p>&part;x
.
</p>
<p>Show that v satisfies a vector equation
</p>
<p>&part;v
</p>
<p>&part;t
&minus; A &middot;
</p>
<p>&part;v
</p>
<p>&part;x
= b. (4.49)
</p>
<p>where b := ( f, 0) and A is the matrix
</p>
<p>A :=
(
</p>
<p>0 c2
</p>
<p>1 0
</p>
<p>)
</p>
<p>.
</p>
<p>(b) The vector equation (4.49) can be solved by diagonalizing A. Check that if we
</p>
<p>set
</p>
<p>T :=
(
</p>
<p>1 c
</p>
<p>1 &minus;c
</p>
<p>)
</p>
<p>,
</p>
<p>then
</p>
<p>T AT&minus;1 =
(
</p>
<p>c 0
</p>
<p>0 &minus;c
</p>
<p>)
</p>
<p>.
</p>
<p>Then show under that the substitution
</p>
<p>w := T v,
</p>
<p>(4.49) reduces to a pair of linear conservation equations for the components of
</p>
<p>w:
{
</p>
<p>&part;w1
&part;t
</p>
<p>&minus; c &part;w1
&part;x
</p>
<p>= f,
&part;w2
&part;t
</p>
<p>+ c &part;w2
&part;x
</p>
<p>= f.
(4.50)
</p>
<p>(c) Translate the initial conditions
</p>
<p>u(0, x) = g(x),
&part;u
</p>
<p>&part;t
(0, x) = h(x),
</p>
<p>into initial conditions for w1 and w2, and then solve (4.50) using the method of
</p>
<p>characteristics.
</p>
<p>(d) Combine the solutions forw1 andw2 to compute v1 = &part;u/&part;t , and then integrate
to solve for u. Your answer should be a combination of of the d&rsquo;Alembert formula
</p>
<p>(4.8) and the Duhamel formula (4.22).
</p>
<p>4.7 The evolution of a quantum-mechanical wave function u(t, x) is governed by
</p>
<p>the Schr&ouml;dinger equation:</p>
<p/>
</div>
<div class="page"><p/>
<p>72 4 The Wave Equation
</p>
<p>&part;u
</p>
<p>&part;t
&minus; i�u = 0 (4.51)
</p>
<p>(ignoring the physical constants). Suppose that u(t, x) is a solution of (4.51) for
</p>
<p>t &isin; [0,&infin;) and x &isin; Rn , with initial condition
</p>
<p>u(0, x) = g(x).
</p>
<p>Assume that
&int;
</p>
<p>Rn
</p>
<p>|g|2 dnx &lt; &infin;.
</p>
<p>(a) Show that for all t &ge; 0,
&int;
</p>
<p>Rn
</p>
<p>|u(t, x)|2 dnx =
&int;
</p>
<p>Rn
</p>
<p>|g|2 dnx.
</p>
<p>(In quantummechanics |u|2 is interpreted as a probability density, so this identity
is conservation of total probability.)
</p>
<p>(b) Show that a solution of Schr&ouml;dinger&rsquo;s equation is uniquely determined by the
</p>
<p>initial condition g.
</p>
<p>4.8 In Rn consider the wave equation
</p>
<p>&part;2u
</p>
<p>&part;t2
&minus; c2�u = 0. (4.52)
</p>
<p>The plane wave solutions have the form
</p>
<p>u(t, x) = ei(k&middot;x&minus;ωt), (4.53)
</p>
<p>where ω &isin; R and k &isin; Rn are constants.
</p>
<p>(a) Find the condition on ω = ω(k) for which u solves (4.52).
</p>
<p>(b) For fixed t, θ &isin; R, show that
{
</p>
<p>x &isin; Rn; u(t, x) = eiθ
}
</p>
<p>is a set of planes per-
</p>
<p>pendicular to k. Show that these planes propagate, as t increases, in a direction
</p>
<p>parallel to k with speed given by c. (Hence the term &ldquo;plane&rdquo; wave.)
</p>
<p>4.9 The Klein-Gordon equation in Rn is a variant of the wave equation that appears
</p>
<p>in relativistic quantum mechanics,
</p>
<p>&part;2u
</p>
<p>&part;t2
&minus;�u + m2u = 0, (4.54)
</p>
<p>where m is the mass of a particle.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.8 Exercises 73
</p>
<p>(a) Find a formula for ω = ω(k,m) under which this equation admits plane wave
solutions of the form (4.53).
</p>
<p>(b) Show that we can define a conserved energy E for this equation by adding a term
</p>
<p>proportional to u2 to the integrand in (4.46).</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 5
</p>
<p>Separation of Variables
</p>
<p>Some PDE can be split into pieces that involve distinct variables. For example, the
equation
</p>
<p>&part;u
</p>
<p>&part;t
&minus; a(t)b(x)�u = 0
</p>
<p>could be written as
1
</p>
<p>a(t)
</p>
<p>&part;u
</p>
<p>&part;t
= b(x)�u,
</p>
<p>provided a(t) �= 0. This puts all of the t derivatives and t-dependent coefficients on
the left and all of the terms involving x on the right.
</p>
<p>Splitting an equation this way is called separation of variables. For PDE that
admit separation, it is natural to look for product solutions whose factors depend on
the separate variables, e.g., u(t, x) = v(t)φ(x). The full PDE then reduces to a pair
of equations for the factors. In some cases, one or both of the reduced equations is
an ODE that can be solved explicitly.
</p>
<p>This idea is most commonly applied to evolution equations such as the heat or
wave equations. The classical versions of these PDE have constant coefficients, and
separation of variables can thus be used to split the time variable from the spatial
variables. This reduces the evolution equation to a simple temporal ODE and a spatial
PDE problem.
</p>
<p>Separation among the spatial variables is sometimes possible as well, but this
requires symmetry in the equation that is also shared by the domain. For example,
we can separate variables for the Laplacian on rectangular or circular domains in R2.
But if the domain is irregular or the differential operator has variable coefficients,
then separation is generally not possible.
</p>
<p>Despite these limitations, separation of variables plays a significant role the devel-
opment of PDE theory. Explicit solutions can still yield valuable information even if
they are very special cases.
</p>
<p>&copy; Springer International Publishing AG 2016
D. Borthwick, Introduction to Partial Differential Equations,
Universitext, DOI 10.1007/978-3-319-48936-0_5
</p>
<p>75</p>
<p/>
</div>
<div class="page"><p/>
<p>76 5 Separation of Variables
</p>
<p>In
te
</p>
<p>n
si
</p>
<p>ty
(d
</p>
<p>B
)
</p>
<p>Frequency (Hz)
</p>
<p>Fig. 5.1 Frequency decomposition for the sound of a violin string
</p>
<p>5.1 Model Problem: Overtones
</p>
<p>In 1636 the mathematician Marin Mersenne published his observation that a vibrat-
ing string produces multiple pitches simultaneously. The most audible pitch corre-
sponds to the lowest frequency of vibration, called the fundamental tone of the string.
Mersenne also detected higher pitches, at integer multiples of the fundamental fre-
quency. (The relationship between frequency and pitch is logarithmic; doubling the
frequency raises the pitch by one octave.)
</p>
<p>The higher multiples of the fundamental frequency are called overtones of the
string. Figure 5.1 shows the frequency decomposition for a sound sample of a bowed
violin string, with a fundamental frequency of 440 Hz. The overtones appear as peaks
in the intensity plot at multiples of 440.
</p>
<p>At the time of Mersenne&rsquo;s observations, there was no theoretical model for string
vibration that would explain the overtones. The wave equation that d&rsquo;Alembert sub-
sequently developed (a century later) gave the first theoretical justification. However,
this connection is not apparent in the explicit solution formula developed in Sect. 4.3.
To understand how the overtones are predicted by the wave equation, we need to
organize the solutions in terms of frequency.
</p>
<p>5.2 Helmholtz Equation
</p>
<p>The classical evolution equations on Rn have the form
</p>
<p>Ptu &minus;�u = 0, (5.1)
</p>
<p>where Pt is a first- or second-order differential operator involving only the time
variable. Examples include the wave equation (Pt = &part;2/&part;t2), heat equation (Pt =
&part;/&part;t), and Schr&ouml;dinger equation (Pt = &minus;i&part;/&part;t).</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_4">http://dx.doi.org/10.1007/978-3-319-48936-0_4</a></div>
</div>
<div class="page"><p/>
<p>5.2 Helmholtz Equation 77
</p>
<p>Lemma 5.1 If u is a classical solution of (5.1) of the form
</p>
<p>u(t, x) = v(t)φ(x),
</p>
<p>for t &isin; R and x &isin; Ω &sub; Rn , then in any region where u is nonzero there is a constant
κ such that the components solve the equations
</p>
<p>Ptv = κv, �φ = κφ. (5.2)
</p>
<p>Proof Substituting u = vφ into (5.1) gives
</p>
<p>φPtv &minus; v�φ = 0.
</p>
<p>Assuming that u is nonzero, we can divide by u to obtain
</p>
<p>1
</p>
<p>v
Ptv =
</p>
<p>1
</p>
<p>φ
�φ.
</p>
<p>The left hand-side is independent of x and the right is independent of t . We conclude
that both sides must be equal to some constant κ. �
</p>
<p>The two differential equations in (5.2) are analogous to eigenvalue equations from
linear algebra, with the role of the linear operator or matrix taken by the differential
operators Pt or �.
</p>
<p>Let us first focus on the spatial problem, which is usually written in the form
</p>
<p>&minus;�φ = λφ. (5.3)
</p>
<p>This is called the Helmholtz equation, after the 19th century physicist Hermann von
Helmholtz. The minus sign is included so that λ &ge; 0 for the most common types of
boundary conditions. Adapting the linear algebra terminology, we refer to the number
λ in (5.3) as an eigenvalue and the corresponding solution φ as an eigenfunction.
The Helmholtz equation is sometimes called the Laplacian eigenvalue equation.
</p>
<p>We will present a general analysis of the Helmholtz problem on any bounded
domain in Rn in Chap. 11, and later in this chapter we will consider some two- or three-
dimensional cases for which further spatial separation is possible. For the remainder
of this section we restrict our attention to problems in one spatial dimensional, for
which (5.3) is an ODE.
</p>
<p>Theorem 5.2 For φ &isin; C2[0, ℓ] the equation
</p>
<p>&minus;
d2φ
</p>
<p>dx2
= λφ, φ(0) = φ(ℓ) = 0, (5.4)
</p>
<p>has nonzero solutions only if
</p>
<p>λn :=
π2n2
</p>
<p>ℓ2</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_11">http://dx.doi.org/10.1007/978-3-319-48936-0_11</a></div>
</div>
<div class="page"><p/>
<p>78 5 Separation of Variables
</p>
<p>for n &isin; N. Up to a constant multiple, the corresponding solutions are
</p>
<p>φn(x) := sin(
&radic;
</p>
<p>λnx). (5.5)
</p>
<p>Proof Note that (5.4) implies
</p>
<p>λ
</p>
<p>&int; ℓ
</p>
<p>0
|φ|2 dx = &minus;
</p>
<p>&int; ℓ
</p>
<p>0
</p>
<p>d2φ
</p>
<p>dx2
φ dx .
</p>
<p>Using the Dirichlet boundary conditions we can integrate by parts on the right without
any boundary term, yielding
</p>
<p>λ
</p>
<p>&int; ℓ
</p>
<p>0
|φ|2 dx =
</p>
<p>&int; ℓ
</p>
<p>0
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>dφ
</p>
<p>dx
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>2
</p>
<p>dx . (5.6)
</p>
<p>Assuming that φ is not identically zero, this shows that λ &ge; 0. Furthermore, λ = 0
implies dφ/dx = 0, which gives a constant solution. The only constant solution is
the trivial case φ = 0, because of the boundary conditions.
</p>
<p>It therefore suffices to consider the caseλ &gt; 0, for which the ODE in (5.4) reduces
to the harmonic oscillator equation, with independent solutions given by sin(
</p>
<p>&radic;
λx)
</p>
<p>and cos(
&radic;
λx). Only sine satisfies the condition φ(0) = 0, so the possible solutions
</p>
<p>have the form
φ(x) = sin(
</p>
<p>&radic;
λx).
</p>
<p>To satisfy the condition φ(ℓ) = 0 we must have
</p>
<p>sin(
&radic;
λℓ) = 0.
</p>
<p>For a nonzero solution this imposes the restriction that
&radic;
λℓ &isin; πN, which gives the
</p>
<p>claimed set of solutions. �
</p>
<p>Some of the eigenfunctions obtained in Theorem 5.2 are illustrated in Fig. 5.2. For
the sake of application to our original string model, let us reinstate the propagation
speed c :=
</p>
<p>&radic;
T/ρ and write the string equation as
</p>
<p>&part;2u
</p>
<p>&part;t2
&minus; c2�u = 0, u(t, 0) = u(0, ℓ) = 0. (5.7)
</p>
<p>Fig. 5.2 The first four
eigenfunctions for a
vibrating string with fixed
ends
</p>
<p>ℓ</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 Helmholtz Equation 79
</p>
<p>With the spatial solution given by the eigenfunction associated to λn , the correspond-
ing temporal eigenvalue equation is also a harmonic oscillator ODE,
</p>
<p>&minus;
d2v
</p>
<p>dt2
= c2λnv.
</p>
<p>The solutions could be written in terms of sines and cosines, but for the temporal
component it is usually more convenient to use the complex exponential form. The
general complex-valued solution is
</p>
<p>vn(t) = aneiωn t + bne&minus;iωn t ,
</p>
<p>with an, bn &isin; C and
ωn := c
</p>
<p>&radic;
</p>
<p>λn =
cπn
</p>
<p>ℓ
. (5.8)
</p>
<p>For real-valued solutions, the coefficients are restricted by bn = an .
Combining the temporal and spatial components gives a set of solutions for the
</p>
<p>vibrating string problem:
</p>
<p>un(t, x) =
[
</p>
<p>ane
iωn t + bne&minus;iωn t
</p>
<p>]
</p>
<p>sin(
&radic;
</p>
<p>λnx), (5.9)
</p>
<p>for n &isin; N.
The functions (5.9) are referred to as &ldquo;pure-tone&rdquo; solutions, because they model
</p>
<p>oscillation at a single frequency ωn . In the case of visible light waves, the frequency
corresponds directly to color. For this reason the set of frequencies {ωn} is called
the spectrum. By association, the term spectrum is also used for sets of eigenvalues
appearing in more general problems. For example, the set {λn} of eigenvalues for
which the Helmholtz problem has a nontrivial solution is called the spectrum of the
Laplacian, even though λn is proportional to the square of the frequency ωn .
</p>
<p>From (5.8) we can deduce the fundamental tone of the string, as predicted by
d&rsquo;Alembert&rsquo;s wave equation model. To convert frequency to the standard unit of Hz
(cycles per second), we divide ω1 by 2π to obtain the formula
</p>
<p>ω1
</p>
<p>2π
=
</p>
<p>1
</p>
<p>2ℓ
</p>
<p>&radic;
</p>
<p>T
</p>
<p>ρ
. (5.10)
</p>
<p>This is known as Mersenne&rsquo;s law, published in 1637.
The wave equation model also predicts the higher frequencies ωn = nω1, cor-
</p>
<p>responding to the sequence of overtones noted illustrated in Fig. 5.1. The fact that
each overtone is associated with a particular spatial eigenfunction is significant. The
waveforms for higher overtones have nodes, meaning points where the string is sta-
tionary. As we can see in Fig. 5.2, the nodes associated to the frequency ωn subdivide
the string into n equal segments. Touching the string lightly at one of these nodes</p>
<p/>
</div>
<div class="page"><p/>
<p>80 5 Separation of Variables
</p>
<p>will knock out the lower frequencies, a practice string players refer to as playing a
&ldquo;harmonic&rdquo;.
</p>
<p>As this discussion illustrates, the &ldquo;spectral analysis&rdquo; of the wave equation is more
directly connected to experimental observation than the explicit solution formula
(4.8). The displacement of a vibrating string is technically difficult to observe directly
because the motion is both rapid and of small amplitude. Such observations were
first achieved by Hermann von Helmholtz in the mid-19th century.
</p>
<p>Example 5.3 The one-dimensional wave equation can be used to model for the fluc-
tuations of air pressure inside a clarinet. The interior of a clarinet is essentially a
cylindrical column, and for simplicity we can assume that the pressure is constant
on cross-sections of the cylinder, so that the variations in pressure are described by
a function u(t, x) with x &isin; [0, ℓ], where ℓ is the length of the instrument. Pressure
fluctuations are measured relative to the fixed atmospheric background, with u = 0
for atmospheric pressure.
</p>
<p>The maximum pressure fluctuation occurs at the mouthpiece at x = 0, where a
reed vibrates as the player blows air into the instrument. Since a local maximum
of the pressure corresponds to a critical point of u(t, &middot;), the appropriate boundary
condition is
</p>
<p>&part;u
</p>
<p>&part;x
(t, 0) = 0. (5.11)
</p>
<p>At the opposite end the air column is open to the atmosphere, so the pressure does
not fluctuate,
</p>
<p>u(t, ℓ) = 0. (5.12)
</p>
<p>The evolution of u as a function of t is governed by the wave equation (4.5), with
c equal to the speed of sound. The corresponding Helmholtz problem is
</p>
<p>&minus;
d2φ
</p>
<p>dx2
= λφ, φ&prime;(0) = 0, φ(ℓ) = 0. (5.13)
</p>
<p>The boundary condition at x = 0 implies that
</p>
<p>φ(x) = cos(
&radic;
λx),
</p>
<p>and the condition at x = ℓ then requires
</p>
<p>cos(
&radic;
λℓ) = 0.
</p>
<p>This means that the eigenvalues are given by
</p>
<p>λn :=
π2
</p>
<p>ℓ2
(n &minus; 12 )
</p>
<p>2,
</p>
<p>for n &isin; N. Some of the resulting eigenfunctions are shown in Fig. 5.3.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_4">http://dx.doi.org/10.1007/978-3-319-48936-0_4</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_4">http://dx.doi.org/10.1007/978-3-319-48936-0_4</a></div>
</div>
<div class="page"><p/>
<p>5.2 Helmholtz Equation 81
</p>
<p>ℓ
</p>
<p>Fig. 5.3 The first four eigenfunctions for pressure fluctuations in a clarinet
</p>
<p>d
B
</p>
<p>Hz
</p>
<p>Fig. 5.4 Clarinet frequency decomposition
</p>
<p>The corresponding oscillation frequencies are given by
</p>
<p>ωn =
cπ
</p>
<p>ℓ
(n &minus; 12 ).
</p>
<p>In contrast to the string, the model predicts that the clarinet&rsquo;s spectrum will contain
only odd multiples of the fundamental frequency ω1. Figure 5.4 shows the frequency
decomposition for a clarinet sound sample. The prediction holds true for the first few
modes, but the simple model appears to break down at higher frequencies. &diams;
</p>
<p>5.3 Circular Symmetry
</p>
<p>In dimension greater than one, spatial separation of variables is essentially the only
way to compute explicit solutions of the Helmholtz equation (5.3), and this only
works for very special cases. The most straightforward example is a rectangular
domain in Rn , which we will discuss in the exercises.
</p>
<p>In this section we consider the simplest non-rectangular case, based on polar
coordinates (r, θ) in R2. Separation in polar coordinates allows us to compute eigen-
functions and eigenvalues on a disk in R2, for example.
</p>
<p>With x = (x1, x2) in R2, polar coordinates are defined by
</p>
<p>(x1, x2) = (r cos θ, r sin θ).</p>
<p/>
</div>
<div class="page"><p/>
<p>82 5 Separation of Variables
</p>
<p>The polar form of the Laplacian is computed by writing
</p>
<p>� =
&part;2
</p>
<p>&part;x21
+
</p>
<p>&part;2
</p>
<p>&part;x22
</p>
<p>and then converting the partials with respect to x1 and x2 into r and θ derivatives
using the chain rule. The result is
</p>
<p>� =
1
</p>
<p>r
</p>
<p>&part;
</p>
<p>&part;r
</p>
<p>(
</p>
<p>r
&part;
</p>
<p>&part;r
</p>
<p>)
</p>
<p>+
1
</p>
<p>r2
</p>
<p>&part;2
</p>
<p>&part;θ2
. (5.14)
</p>
<p>Note that there are no mixed partials involving both r and θ, and that the coefficients
do not depend on θ. This allows separation of r and θ, provided the domain is defined
by specifying ranges of r and θ.
</p>
<p>To solve the radial eigenvalue equation, we will use Bessel functions, named for
the astronomer Friedrich Bessel. Bessel&rsquo;s equation is the ODE:
</p>
<p>z2 f &prime;&prime;(z)+ z f &prime;(z)+ (z2 &minus; k2) f (z) = 0, (5.15)
</p>
<p>with k &isin; C in general. For our application k will be an integer. The standard pair of
linearly independent solutions is given by the Bessel functions Jk(z) and Yk(z).
</p>
<p>The Bessel J-functions, a few of which are pictured in Fig. 5.5, satisfy
</p>
<p>J&minus;k(z) = (&minus;1)k Jk(z), (5.16)
</p>
<p>for all k &isin; Z. Bessel represented these solutions as integrals:
</p>
<p>Jk(z) :=
1
</p>
<p>π
</p>
<p>&int; π
</p>
<p>0
cos
</p>
<p>(
</p>
<p>z sin θ &minus; kθ
)
</p>
<p>dθ.
</p>
<p>One can also write Jk as a power series k &isin; N0,
</p>
<p>Fig. 5.5 The first four
Bessel J-functions
</p>
<p>J0
</p>
<p>J1
J2
</p>
<p>J3</p>
<p/>
</div>
<div class="page"><p/>
<p>5.3 Circular Symmetry 83
</p>
<p>Jk(z) =
( z
</p>
<p>2
</p>
<p>)k
&infin;
&sum;
</p>
<p>l=0
</p>
<p>1
</p>
<p>l!(k + l)!
</p>
<p>(
</p>
<p>&minus;
z2
</p>
<p>4
</p>
<p>)l
</p>
<p>. (5.17)
</p>
<p>Together with (5.16), this shows that Jk(z) &sim; ckz|k| as z &rarr; 0 for any k &isin; Z. In
contrast, the Bessel Y-function satisfies Yk(z) &sim; ckz&minus;|k| as z &rarr; 0.
</p>
<p>A change of sign in (5.15) gives the equation
</p>
<p>z2 f &prime;&prime;(z)+ z f &prime;(z)+ (z2 + k2) f (z) = 0. (5.18)
</p>
<p>Its standard solutions are the modified Bessel functions Ik(z) and Kk(z). As z &rarr; 0
these satisfy the asymptotics Ik(z) &sim; ckz|k|, as illustrated in Fig. 5.6, and Kk(z) &sim;
ckz
</p>
<p>&minus;|k|.
</p>
<p>Lemma 5.4 Suppose φ &isin; C2(R2) is a solution of
</p>
<p>&minus;�φ = λφ,
</p>
<p>that factors as a product h(r)w(θ). Then, up to a multiplicative constant, φ has the
</p>
<p>form
</p>
<p>φλ,k(r, θ) := hk(r)eikθ, (5.19)
</p>
<p>for some k &isin; Z, with
</p>
<p>hk(r) :=
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>r |k|, λ = 0,
Jk(
</p>
<p>&radic;
λr), λ &gt; 0,
</p>
<p>Ik(
&radic;
&minus;λr), λ &lt; 0.
</p>
<p>Proof Under the assumption φ = hw, the Helmholtz equation reduces by (5.14) to
</p>
<p>w
</p>
<p>r
</p>
<p>&part;
</p>
<p>&part;r
</p>
<p>(
</p>
<p>r
&part;h
</p>
<p>&part;r
</p>
<p>)
</p>
<p>+
h
</p>
<p>r2
</p>
<p>&part;2w
</p>
<p>&part;θ2
+ λhw = 0.
</p>
<p>Fig. 5.6 The first four
modified Bessel I-functions
</p>
<p>I0
</p>
<p>I1
</p>
<p>I2
</p>
<p>I3</p>
<p/>
</div>
<div class="page"><p/>
<p>84 5 Separation of Variables
</p>
<p>With some rearranging, we can separate the r and θ variables,
</p>
<p>1
</p>
<p>h
</p>
<p>(
</p>
<p>r
&part;
</p>
<p>&part;r
</p>
<p>)2
</p>
<p>h + λ2r2 = &minus;
1
</p>
<p>w
</p>
<p>&part;2w
</p>
<p>&part;θ2
, (5.20)
</p>
<p>provided h and w are nonzero.
As in Lemma 5.1, we conclude that both sides must be equal to some constant κ.
</p>
<p>The θ equation is
</p>
<p>&minus;
&part;2w
</p>
<p>&part;θ2
= κw. (5.21)
</p>
<p>The function w(θ) is assumed to be 2π-periodic. By the arguments used in Theo-
rem 5.2, a nontrivial solution is possible only if κ = k2 where k is an integer. A full
set of 2π-periodic solutions of (5.21) is given by
</p>
<p>wk(θ) := eikθ, k &isin; Z.
</p>
<p>Before examining the radial equation, let us note that the assumption that φ is
C2 imposes a boundary condition at r = 0. To see this, first note that the function
r =
</p>
<p>&radic;
</p>
<p>x21 + x22 is continuous at (0, 0) but not differentiable. For r &gt; 0,
</p>
<p>&part;r
</p>
<p>&part;x j
=
</p>
<p>x j
</p>
<p>r
,
</p>
<p>which does not have a limit as r &rarr; 0. On the other hand, the functions
</p>
<p>re&plusmn;iθ = x1 &plusmn; i x2
</p>
<p>are C&infin;. Similarly, for k &isin; Z we have
</p>
<p>r |k|eikθ =
</p>
<p>{
</p>
<p>(x1 + i x2)k, k &isin; N0,
(x1 &minus; i x2)&minus;k, &minus;k &isin; N.
</p>
<p>(5.22)
</p>
<p>These functions are polynomial and hence C&infin;. We will see below that the solutions
of the radial equation corresponding to κ = k2 satisfy h(r) &sim; ar&plusmn;k as r &rarr; 0, for
some constant a. The differentiability of φ at the origin will require the asymptotic
condition
</p>
<p>hk(r) &sim; ar |k| (5.23)
</p>
<p>as r &rarr; 0.
For wk(θ) = eikθ, the radial component of (5.20) is
</p>
<p>(
</p>
<p>r
&part;
</p>
<p>&part;r
</p>
<p>)2
</p>
<p>hk + (λr2 &minus; k2)hk = 0. (5.24)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.3 Circular Symmetry 85
</p>
<p>The case λ = 0 is relatively straightforward to analyze. In this case (5.24) is homoge-
neous in the r variable (meaning invariant under scaling). Such equations are solved
by monomials of the form hk(r) = rα with α &isin; R. If we substitute this guess into
(5.24) with λ = 0, the equation reduces to
</p>
<p>α2 &minus; k2 = 0,
</p>
<p>with solutions α = &plusmn;k. Since a second order ODE has exactly two independent
solutions, the functions r&plusmn;k give a full set of solutions for k �= 0. For k = 0 the two
possibilities are 1 and ln r . By the condition (5.23), the solutions ln r and r&minus;|k| must
be ruled out. The only possible solutions for λ = 0 are thus
</p>
<p>hk(r) = r |k|.
</p>
<p>Note that the resulting solutions,
</p>
<p>φ0,k(r, θ) := r |k|eikθ,
</p>
<p>are precisely the polynomials (5.22).
For λ &gt; 0 (5.24) can be reduced to the Bessel form (5.15) by the change of
</p>
<p>variables z =
&radic;
λr . The possible solutions Yk(
</p>
<p>&radic;
λr) are ruled out because they
</p>
<p>diverge at r = 0. On the other hand, the power series (5.17) shows that the func-
tion hk(r) = Jk(
</p>
<p>&radic;
λr) satisfies the condition (5.23). Thus for λ &gt; 0 the possible
</p>
<p>eigenfunction with k &isin; Z is
</p>
<p>φλ,k(r, θ) := Jk(
&radic;
λr)eikθ.
</p>
<p>We should check that this function is at least C2 at the origin. In fact, it follows from
the power series expansion (5.17) that φλ,k is C&infin; on R2.
</p>
<p>Similar considerations apply for λ &lt; 0, except that this time the substitution
z =
</p>
<p>&radic;
&minus;λr reduces (5.24) to (5.18). The condition (5.23) is satisfied only for the
</p>
<p>solution Ik(
&radic;
&minus;λr). �
</p>
<p>Example 5.5 The linear model for the vibration of a drumhead is the wave equation
(4.30). For a circular drum we can take the spatial domain to be the unit disk D :=
{r &lt; 1} &sub; R2. Lemma 5.1 reduces the problem of determining the frequencies of
the drum to the Helmholtz equation,
</p>
<p>&minus;�φ = λφ, φ|&part;D = 0. (5.25)
</p>
<p>The possible product solutions are given by Lemma 5.4, subject to the boundary
condition hk(1) = 0. This rules out λ &le; 0, because in that case hk(r) has no zeros
for r &gt; 0.
</p>
<p>For λ &gt; 0, we have hk(r) = Jk(
&radic;
λr), and the boundary condition takes the form
</p>
<p>Jk(
&radic;
λ) = 0.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_4">http://dx.doi.org/10.1007/978-3-319-48936-0_4</a></div>
</div>
<div class="page"><p/>
<p>86 5 Separation of Variables
</p>
<p>Table 5.1 Zeros of the Bessel function Jk . For each k, the spacing between zeros approaches π as
m &rarr; &infin;
k jk,1 jk,2 jk,3 jk,4
</p>
<p>0 2.405 5.520 8.654 11.792
</p>
<p>1 3.832 7.016 10.174 13.324
</p>
<p>2 5.136 8.417 11.620 14.796
</p>
<p>3 6.380 9.761 13.015 16.223
</p>
<p>4 7.588 11.065 14.373 17.616
</p>
<p>(This is analogous to the condition sin(
&radic;
λℓ) = 0 from the one-dimensional string
</p>
<p>problem.) Although Jk is not a periodic function, it does have an infinite sequence
of positive zeros with roughly evenly spacing. It is customary to write these zeros in
increasing order as
</p>
<p>0 &lt; jk,1 &lt; jk,2 &lt; . . . .
</p>
<p>By the symmetry (5.16),
j&minus;k,m = jk,m .
</p>
<p>Table 5.1 lists some of these zeros.
Restricting
</p>
<p>&radic;
λ to the set of Bessel zeros gives the set of eigenvalues
</p>
<p>λk,m = j2k,m,
</p>
<p>indexed by k &isin; Z,m &isin; N. The corresponding eigenfunctions are
</p>
<p>φk,m(r, θ) := Jk( jk,mr)eikθ. (5.26)
</p>
<p>The first set of these are illustrated in Fig. 5.7.
The collection of functions (5.26) yields a complete list of eigenfunctions and
</p>
<p>eigenvalues for D, although that is not something we can prove here. &diams;
</p>
<p>The eigenvalues calculated in Example 5.5 correspond to vibrational frequencies
</p>
<p>ωk,m := cjk,m,
</p>
<p>for k &isin; Z and m &isin; N. The propagation speed c depends on physical properties
such as tension and density. The relative size of the frequencies helps to explain
the lack of definite pitch in the sound of a drum. The ratios of overtones above the
fundamental ω0,1 are shown in Table 5.2. In contrast to the vibrating string case,
where the corresponding ratios were integers 1, 2, 3, . . ., or the clarinet model of
Example 5.3 with ratios 1, 3, 5, . . ., the frequencies of the drum are closely spaced
with no evident pattern.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 Spherical Symmetry 87
</p>
<p>m = 1 m = 2 m = 3
</p>
<p>k = 2
</p>
<p>k = 1
</p>
<p>k = 0
</p>
<p>Fig. 5.7 Contour plots of the spatial component of the eigenfunctions of D
</p>
<p>Table 5.2 Frequency ratios
for a circular drumhead
</p>
<p>k m ωk,m /ω0,1
</p>
<p>0 1 1
</p>
<p>1 1 1.593
</p>
<p>2 1 2.136
</p>
<p>0 2 2.295
</p>
<p>3 1 2.653
</p>
<p>1 2 2.917
</p>
<p>4 1 3.155
</p>
<p>5.4 Spherical Symmetry
</p>
<p>Another special case that allows separation of spatial variables is spherical symmetry
in R3. Spherical coordinates (r,ϕ, θ) are defined through the relation
</p>
<p>(x1, x2, x3) = (r sinϕ cos θ, r sinϕ sin θ, r cosϕ).</p>
<p/>
</div>
<div class="page"><p/>
<p>88 5 Separation of Variables
</p>
<p>Note that θ is the azimuthal angle here and ϕ the polar angle, consistent with the
notation from Sect. 5.3. (This convention is standard in mathematics; in physics the
roles are often reversed.)
</p>
<p>As in the circular case, we can use the chain rule to translate the three-dimensional
Laplacian into spherical variables:
</p>
<p>� =
1
</p>
<p>r2
</p>
<p>&part;
</p>
<p>&part;r
</p>
<p>(
</p>
<p>r2
&part;
</p>
<p>&part;r
</p>
<p>)
</p>
<p>+
1
</p>
<p>r2 sinϕ
</p>
<p>&part;
</p>
<p>&part;ϕ
</p>
<p>(
</p>
<p>sinϕ
&part;
</p>
<p>&part;ϕ
</p>
<p>)
</p>
<p>+
1
</p>
<p>r2 sin2 ϕ
</p>
<p>&part;2
</p>
<p>&part;θ2
. (5.27)
</p>
<p>It is not immediately clear that this operator admits separation, because the coef-
ficients depend on both r and ϕ. Note, however, that we can factor r&minus;2 out of the
angular derivative terms, to write (5.27) as
</p>
<p>� =
1
</p>
<p>r2
</p>
<p>&part;
</p>
<p>&part;r
</p>
<p>(
</p>
<p>r2
&part;
</p>
<p>&part;r
</p>
<p>)
</p>
<p>+
1
</p>
<p>r2
�S2 , (5.28)
</p>
<p>where
</p>
<p>�S2 :=
1
</p>
<p>sinϕ
</p>
<p>&part;
</p>
<p>&part;ϕ
</p>
<p>(
</p>
<p>sinϕ
&part;
</p>
<p>&part;ϕ
</p>
<p>)
</p>
<p>+
1
</p>
<p>sin2 ϕ
</p>
<p>&part;2
</p>
<p>&part;θ2
. (5.29)
</p>
<p>Here S2 stands for the unit sphere {r = 1} &sub; R3, and �S2 is called the spherical
Laplacian.
</p>
<p>The expression (5.29) may look awkward at first glance, but �S2 is a very natural
operator geometrically. From the fact that � is invariant under rotations of R3 about
the origin, we can deduce that �S2 is also invariant under rotations of the sphere. It is
possible to show that �S2 is the only second-order operator with this property, up to
a multiplicative constant. The operator �S2 is thus as symmetric as possible, and the
reason that (5.29) looks so complicated is that the standard coordinate system (θ,ϕ)
does not reflect the full symmetry of the sphere.
</p>
<p>We will discuss the radial component of (5.28) in an example below. For now let
us focus on the Helmholtz problem on the sphere, which allows further separation
of the θ and ϕ variables.
</p>
<p>The classical ODE that arises from separation of the angle variables is the asso-
ciated Legendre equation:
</p>
<p>(1 &minus; z2) f &prime;&prime;(z)&minus; 2z f &prime;(z)+
(
</p>
<p>ν(ν + 1)&minus;
&micro;2
</p>
<p>1 &minus; z2
</p>
<p>)
</p>
<p>f (z) = 0, (5.30)
</p>
<p>with parameters &micro;, ν &isin; C. A pair of linearly independent solutions is given by the
Legendre functions P&micro;ν (z) and Q
</p>
<p>&micro;
ν (z).
</p>
<p>In the special case where ν is replaced by l &isin; N0 and &micro; by a number m &isin;
{&minus;l, . . . , l}, respectively, the Legendre P-functions are given by a relatively simple
formula:
</p>
<p>Pml (z) =
(&minus;1)m
</p>
<p>2ll!
(1 &minus; z2)m/2
</p>
<p>d l+m
</p>
<p>dzl+m
(z2 &minus; 1)l . (5.31)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 Spherical Symmetry 89
</p>
<p>Associated to this set of Legendre functions are functions of the angle variables
called spherical harmonics. These are defined by
</p>
<p>Yml (ϕ, θ) := cl,me
imθPml (cosϕ), (5.32)
</p>
<p>where cl,m is a normalization constant whose value is not important for us.
From (5.31), using z = cosϕ and 1 &minus; z2 = sin2 ϕ, we can see that Yml is a
</p>
<p>polynomial of degree l in sinϕ and cosϕ. This makes it relatively straightforward
to check that each Yml (ϕ, θ) is a smooth function on S
</p>
<p>2.
</p>
<p>Lemma 5.6 Suppose u &isin; C2(S2) is a solution of the equation
</p>
<p>&minus;�S2u = λu (5.33)
</p>
<p>that factors as u(ϕ, θ) = v(ϕ)w(θ). Then up to a multiplicative constant, u is equal
to a spherical harmonic Yml for l &isin; N0 and m &isin; {&minus;l, . . . , l}. The corresponding
eigenvalues depend only on l,
</p>
<p>λl = l(l + 1),
</p>
<p>and each has multiplicity 2l + 1.
</p>
<p>Proof By (5.29), the substitution u = vw leads to the separated equation
</p>
<p>sinϕ
</p>
<p>v
</p>
<p>&part;
</p>
<p>&part;ϕ
</p>
<p>(
</p>
<p>sinϕ
&part;v
</p>
<p>&part;ϕ
</p>
<p>)
</p>
<p>+ λ = &minus;
1
</p>
<p>w
</p>
<p>&part;2w
</p>
<p>&part;θ2
.
</p>
<p>The continuity of u requires that w be 2π-periodic. Hence, for the θ equation
</p>
<p>&minus;
&part;2w
</p>
<p>&part;θ2
= κw,
</p>
<p>the full set of solutions is represented by w(θ) = eimθ with κ = m2 for m &isin; Z.
With u(θ,ϕ) = vm(ϕ)eimθ, the eigenvalue equation (5.33) reduces to
</p>
<p>1
</p>
<p>sinϕ
</p>
<p>d
</p>
<p>dϕ
</p>
<p>(
</p>
<p>sinϕ
dvm
</p>
<p>dϕ
</p>
<p>)
</p>
<p>+
(
</p>
<p>λ&minus;
m2
</p>
<p>sin2 ϕ
</p>
<p>)
</p>
<p>vm = 0.
</p>
<p>Under the substitutions z = cosϕ and vm(ϕ) = f (cosϕ), this becomes
</p>
<p>(1 &minus; z2) f &prime;&prime; &minus; 2z f &prime; +
(
</p>
<p>λ&minus;
m2
</p>
<p>1 &minus; z2
</p>
<p>)
</p>
<p>f = 0,
</p>
<p>which is recognizable as the Legendre equation (5.30) with parameters m = &micro; and
λ = ν(ν + 1).
</p>
<p>Although S2 does not have a boundary, use of the coordinate ϕ &isin; [0,π] creates
artificial boundaries at the endpoints, i.e., at the poles of the sphere. This is analogous</p>
<p/>
</div>
<div class="page"><p/>
<p>90 5 Separation of Variables
</p>
<p>to the boundary at r = 0 in Lemma 5.4. We need to find solutions which will be
smooth at the poles.
</p>
<p>It turns out that for m &isin; Z, the function Qmν (z) diverges as z &rarr; 1 for any ν &isin; C.
Similarly, the functions Pmν (z) diverge as z &rarr; &minus;1 except for the special cases
Pml (z) given by (5.31). In other words, up to a multiplicative constant vm(ϕ) must
be equal to Pml (cosϕ) for some l &isin; N0 with l &ge; |m|. The corresponding solution u
is proportional to the spherical harmonic Yml .
</p>
<p>By the identification ν = l, the eigenvalue is given by λ = l(l + 1). The corre-
sponding multiplicity is the number of possible choices of m &isin; {&minus;l, . . . , l}, namely
2l + 1. �
</p>
<p>The spherical harmonics appearing in Lemma 5.6 give a complete set of eigen-
functions for �S2 , in the sense that the only possible eigenvalues are l(l + 1) for
l &isin; N0 and an eigenfunction with eigenvalue l(l + 1) is a linear combination of the
Yml form &isin; {&minus;l, . . . , l}. To prove this requires more advanced methods than we have
available here.
</p>
<p>Example 5.7 In 1925, Erwin Schr&ouml;dinger developed a quantum model for the hydro-
gen atom in which the electron energy levels are given by the eigenvalues of the
equation
</p>
<p>(
</p>
<p>&minus;�&minus;
1
</p>
<p>r
</p>
<p>)
</p>
<p>φ = λφ (5.34)
</p>
<p>on R3. (We have omitted the physical constants.) The eigenfunctions φ are assumed
to be bounded near r = 0 and decaying to zero as r &rarr; &infin;.
</p>
<p>Since the term 1/r is radial, separation of the radial and angular variables is
possible in (5.34). By Lemma 5.6, the angular components are given by spherical
harmonics. A corresponding full solution has the form
</p>
<p>φ(r,ϕ, θ) = h(r)Yml (ϕ, θ). (5.35)
</p>
<p>Substituting this into (5.34) and using the spherical form of the Laplacian (5.28)
gives the radial equation
</p>
<p>[
</p>
<p>&minus;
1
</p>
<p>r2
</p>
<p>d
</p>
<p>dr
</p>
<p>(
</p>
<p>r2
d
</p>
<p>dr
</p>
<p>)
</p>
<p>+
l(l + 1)
</p>
<p>r2
&minus;
</p>
<p>1
</p>
<p>r
</p>
<p>]
</p>
<p>h(r) = λh(r). (5.36)
</p>
<p>One strategy used to analyze an ODE such as (5.36) is to first consider the asymptotic
behavior of solutions as r &rarr; 0 or &infin;.
</p>
<p>Suppose we assume h(r) &sim; rα as r &rarr; 0. Plugging this into (5.36) and comparing
the two sides gives a leading term
</p>
<p>&minus;α(α+ 1)rα&minus;2 + l(l + 1)rα&minus;2
</p>
<p>on the left side, with all other terms of order rα&minus;1 or less. This shows that h(r) &sim; rα
as r &rarr; 0 is possible only if</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 Spherical Symmetry 91
</p>
<p>α(α+ 1) = l(l + 1).
</p>
<p>The two solutions are α = l or α = &minus;l &minus; 1. Taking α &lt; 0 would cause h(r) to
diverge as r &rarr; 0. Therefore to obtain a solution bounded at the origin, we will
assume that
</p>
<p>h(r) &sim; r l
</p>
<p>as r &rarr; 0.
As r &rarr; &infin;, if we consider the terms in (5.36) with coefficients of order r0 and
</p>
<p>drop the rest, the equation becomes
</p>
<p>&minus; h&prime;&prime;(r) &sim; λh(r). (5.37)
</p>
<p>If λ &ge; 0 then this shows that h(r) could not possibly decay at infinity. Hence we
assume that λ &lt; 0 and set
</p>
<p>σ2 := &minus;λ,
</p>
<p>with σ &isin; R. The asymptotic equation (5.37) implies the behavior
</p>
<p>h(r) &sim; ce&minus;σr
</p>
<p>as r &rarr; &infin;.
Determining these asymptotics allows us to make an educated guess for the form
</p>
<p>of the solution. For an as yet undetermined function q(r), we set
</p>
<p>h(r) = q(r)r le&minus;σr , (5.38)
</p>
<p>with the conditions that q(0) = 1 and q(r) has subexponential growth as r &rarr; &infin;.
The goal of setting up the solution this way is that the equation for q(r) will simplify.
Substituting (5.38) into (5.36) leads to the equation
</p>
<p>rq &prime;&prime; + 2(1 + l &minus; rσ)q &prime; + (1 &minus; 2σ(l + 1))q = 0. (5.39)
</p>
<p>To find solutions, we suppose q(r) is given by a power series
</p>
<p>q(r) =
&infin;
&sum;
</p>
<p>k=0
</p>
<p>akr
k,
</p>
<p>with a0 = 1. Plugging this into (5.39) gives
</p>
<p>0 =
&infin;
&sum;
</p>
<p>k=0
</p>
<p>[
</p>
<p>k(k &minus; 1)akr k&minus;1 + 2(1 + l &minus; rσ)kakr k&minus;1 + (1 &minus; 2σ(l + 1))akr k
]
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>92 5 Separation of Variables
</p>
<p>Equating the coefficient of r k to zero then gives a recursive relation
</p>
<p>ak+1 =
2σ(k + l + 1)&minus; 1
(k + 1)(k + 2l + 2)
</p>
<p>ak . (5.40)
</p>
<p>If we assume that the numerator of (5.40) never vanishes, then the recursion
relation implies that
</p>
<p>ak &sim;
(2σ)k
</p>
<p>k!
</p>
<p>as k &rarr; &infin;. This would give q(r) &sim; ce2σr as r &rarr; &infin;, making h(r) also grow
exponentially as r &rarr; &infin;.
</p>
<p>The only way to avoid this exponential growth is for the sequence ofak to terminate
at some point, so that q is a polynomial. The numerator on the right side of (5.40)
will eventually vanish if and only if
</p>
<p>σ =
1
</p>
<p>2n
,
</p>
<p>for some integer n &ge; l + 1. Under this assumption the sequence ak terminates at
k = n &minus; l &minus; 1. Since λ = &minus;σ2, this restriction on σ gives the set of eigenvalues
</p>
<p>λn := &minus;
1
</p>
<p>4n2
, n &isin; N.
</p>
<p>This is in fact the complete set of eigenvalues for this problem, given the conditions
we have imposed at r = 0 and r &rarr; &infin;. With this eigenvalue calculation, Schr&ouml;dinger
was able to give the first theoretical explanation of the emission spectrum of hydrogen
gas (i.e., the set of wavelengths observed when the gas is excited electrically). The
origin of these emission lines had been a mystery since their discovery by Anders
Jonas &Aring;ngstr&ouml;m in the mid-19th century.
</p>
<p>Each value of n corresponds to a family of eigenfunctions given by
</p>
<p>φn,l,m(r,ϕ, θ) = r lqn,l(r)e&minus;
r
</p>
<p>2n Yml (ϕ, θ),
</p>
<p>for l &isin; {0, . . . , n &minus; 1}, m &isin; {&minus;l, . . . , l}. Here qn,l(r) denotes the polynomial of
degree n &minus; l &minus; 1 with coefficients specified by (5.40). To compute the multiplicity
of λn , we count n&minus; 1 choices for l and then 2l + 1 choices of m for each l. The total
multiplicity is
</p>
<p>n&minus;1
&sum;
</p>
<p>l=0
</p>
<p>(2l + 1) = n2.
</p>
<p>&diams;</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Exercises 93
</p>
<p>5.5 Exercises
</p>
<p>5.1 On the half-strip Ω = (0, 1)&times; (0,&infin;) &sub; R2, find the solutions of
</p>
<p>�u = 0
</p>
<p>that factor as a product u(x1, x2) = g(x1)h(x2), under the boundary conditions
</p>
<p>u(0, x2) = u(1, x2) = u(x1, 0) = 0.
</p>
<p>5.2 The linear model for vibrations of a rectangular drumhead is the wave equa-
tion (4.30) with Dirichlet boundary conditions on a rectangle R := [0, ℓ1]&times;[0, ℓ2] &sub;
R
</p>
<p>2. Separation of variables leads to the corresponding Helmholtz problem
</p>
<p>&minus;�φ = λφ, φ|&part;R = 0.
</p>
<p>Find the eigenfunctions of product type, φ(x1, x2) = φ1(x1)φ2(x2), and the associ-
ated frequencies of vibration. For ℓ1 = ℓ2, compare the ratios of these frequencies
to Table 5.2. Would a square drum do a better job of producing a definite pitch?
</p>
<p>5.3 The one-dimensional heat equation for the temperature u(t, x) of a metal bar
of length ℓ is
</p>
<p>&part;u
</p>
<p>&part;t
&minus;
</p>
<p>&part;2u
</p>
<p>&part;x2
= 0,
</p>
<p>for t &ge; 0 and x &isin; (0, ℓ). (We will derive this in Sect. 6.1.) If the ends of the bar are
insulated, then u should satisfy Neumann boundary conditions
</p>
<p>&part;u
</p>
<p>&part;x
(t, 0) =
</p>
<p>&part;u
</p>
<p>&part;x
(t, ℓ) = 0.
</p>
<p>Find the product solutions u(t, x) = v(t)φ(x).
</p>
<p>5.4 The damped wave equation on Ω &sub; Rn is
</p>
<p>&part;2u
</p>
<p>&part;t2
+ γ
</p>
<p>&part;u
</p>
<p>&part;t
&minus;�u = 0, (5.41)
</p>
<p>where u &isin; C2([0,&infin;)&times;Ω) and γ &ge; 0 is a constant called the coefficient of friction.
Suppose thatφ &isin; C2(Ω) satisfies the Helmholtz equation (5.3) onΩ with eigenvalue
λ &gt; 0, for some appropriate choice of boundary conditions. Show that that (5.41)
has solutions of the form
</p>
<p>u(t, x) = φ(x)eiωt ,</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_4">http://dx.doi.org/10.1007/978-3-319-48936-0_4</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_6">http://dx.doi.org/10.1007/978-3-319-48936-0_6</a></div>
</div>
<div class="page"><p/>
<p>94 5 Separation of Variables
</p>
<p>and find the set of possible values of ω. In particular, show that Im ω &gt; 0 if γ &gt; 0,
which implies that the solutions decay exponentially in time. Does this decay rate
depend on the oscillation frequency?
</p>
<p>5.5 Consider this example of a nonlinear diffusion equation:
</p>
<p>&part;u
</p>
<p>&part;t
&minus;�(u2) = 0,
</p>
<p>for t &ge; 0, x &isin; Rn .
</p>
<p>(a) Assuming a product solution of the form u(t, x) = v(t)φ(x), separate variables
and find the equations for v(t) and φ(x).
</p>
<p>(b) Show that φ(x) = |x|2 solves the spatial equation, and find the corresponding
function v(t) given the initial condition v(0) = a &gt; 0. (Observe that the solution
&ldquo;blows up&rdquo; at a finite time that depends on a.)
</p>
<p>5.6 In polar coordinates for R2, define the domain
</p>
<p>Ω =
{
</p>
<p>(r, θ); 0 &lt; r &lt; 1, 0 &lt; θ &lt; π/3
}
</p>
<p>,
</p>
<p>which is a sector within the unit disk. Find the eigenvalues of � on Ω with Dirichlet
boundary conditions.
</p>
<p>5.7 The quantum energy levels of a harmonic oscillator in Rn are the eigenvalues
of the equation
</p>
<p>(
</p>
<p>&minus;�+ |x|2
)
</p>
<p>φ = λφ, (5.42)
</p>
<p>under the condition that φ &isin; C2(Rn) and φ &rarr; 0 at infinity.
</p>
<p>(a) First consider the case n = 1:
(
</p>
<p>&minus;
&part;2
</p>
<p>&part;x2
+ x2
</p>
<p>)
</p>
<p>φ = κφ, (5.43)
</p>
<p>Substitute
φ(x) = q(x)e&minus;x2/2
</p>
<p>into (5.43) and find the corresponding ODE for q.
(b) Assume that the function q from (a) is given by a power series in x ,
</p>
<p>q(x) =
&infin;
&sum;
</p>
<p>k=0
</p>
<p>akx
k,
</p>
<p>and find a recursive equation for ak+2 in terms of ak .
(c) Find the values of κ for which the power series for q from (b) truncates to a
</p>
<p>polynomial. (The resulting functions q are called Hermite polynomials.)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Exercises 95
</p>
<p>(d) Returning to the original problem, by reducing (5.42) to n copies of the case
(5.43), deduce the set of eigenvalues λ.
</p>
<p>5.8 Let B3 &sub; R3 be the unit ball {r &lt; 1}. Consider the Helmholtz problem
</p>
<p>&minus;�φ = λφ,
</p>
<p>with Dirichlet boundary conditions at r = 1.
</p>
<p>(a) Assume that
φ(r,ϕ, θ) = hl(r)Yml (ϕ, θ),
</p>
<p>where Yml is the spherical harmonic introduced in Sect. 5.4. Find the radial equa-
tion for hl(r).
</p>
<p>(b) For l = 0 show that the radial equation is solved by
</p>
<p>h0(r) =
sin(
</p>
<p>&radic;
λr)
</p>
<p>r
.
</p>
<p>What set of eigenvalues λ does this give?
(c) Show that the substitution,
</p>
<p>hl(r) = r&minus;
1
2 fl(
</p>
<p>&radic;
λr),
</p>
<p>reduces the equation from (a) to a Bessel equation (5.15) for fl(z), with a frac-
tional value of k. Use this to write the solution hl(r) in terms of Jk .
</p>
<p>(d) Express the eigenvalues λ in terms of Bessel zeros with fractional values of k.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 6
</p>
<p>The Heat Equation
</p>
<p>In physics, the term heat is used to describe the transfer of internal energy within a
</p>
<p>system of particles. When this transfer results from collective motion of particles in
</p>
<p>a gas or fluid, the process is called convection. The continuity equation developed in
</p>
<p>Sect. 3.1 describes convection by fluid flow, which is the special case called advection.
</p>
<p>Another form of convection is conduction, where the heat transfer caused by random
</p>
<p>collisions of individual particles.
</p>
<p>The basic mathematical model for heat conduction is a PDE called the heat equa-
</p>
<p>tion, developed by Joseph Fourier in the early 19th century. In this chapter we will
</p>
<p>discuss the derivation and develop some basic properties of this equation, our first
</p>
<p>example of a PDE of parabolic type.
</p>
<p>6.1 Model Problem: Heat Flow in a Metal Rod
</p>
<p>A metal rod that is sufficiently thin can be treated as one-dimensional system. Let
</p>
<p>u(t, x) denote the temperature of the rod at time t and position x , with x &isin; R for
now.
</p>
<p>There are two physical principles that govern the flow of heat in the rod. The
</p>
<p>first is the relationship between thermal (internal) energy and temperature. Thermal
</p>
<p>energy is proportional to a product of density and temperature, by a constant c called
</p>
<p>the specific heat of the material. Thus, the total thermal energy in a segment [a, b]
is given by
</p>
<p>U = c
&int; b
</p>
<p>a
</p>
<p>ρu dx . (6.1)
</p>
<p>We will assume that the density ρ is constant, although it could be variable in some
</p>
<p>applications.
</p>
<p>The original version of the book was revised: Belated corrections from author have been incorpo-
</p>
<p>rated. The erratum to the book is available at https://doi.org/10.1007/978-3-319-48936-0_14
</p>
<p>&copy; Springer International Publishing AG 2016
</p>
<p>D. Borthwick, Introduction to Partial Differential Equations,
</p>
<p>Universitext, DOI 10.1007/978-3-319-48936-0_6
</p>
<p>97</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_3">http://dx.doi.org/10.1007/978-3-319-48936-0_3</a></div>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-319-48936-0_14">https://doi.org/10.1007/978-3-319-48936-0_14</a></div>
</div>
<div class="page"><p/>
<p>98 6 The Heat Equation
</p>
<p>The second principle is Fourier&rsquo;s law of heat conduction, which describes how heat
</p>
<p>flows from hotter regions to colder regions. In its one-dimensional form, Fourier&rsquo;s
</p>
<p>law says that the flux of thermal energy across a given point is given by
</p>
<p>q = &minus;k
&part;u
</p>
<p>&part;x
, (6.2)
</p>
<p>where the constant k &gt; 0 is the thermal conductivity of the material.
</p>
<p>Assuming that the rod is thermally isolated, conservation of energy dictates the
</p>
<p>rate of change of the thermal energy within the segment is equal to the flux across
</p>
<p>its boundaries, i.e.,
dU
</p>
<p>dt
(t) = q(t, a)&minus; q(t, b). (6.3)
</p>
<p>As in our derivation of the local equation for conservation of mass, the combination
</p>
<p>of (6.1) and (6.3) yields an integral equation
</p>
<p>&int; b
</p>
<p>a
</p>
<p>(
</p>
<p>cρ
&part;u
</p>
<p>&part;t
+
</p>
<p>&part;q
</p>
<p>&part;x
</p>
<p>)
</p>
<p>dx = 0.
</p>
<p>Since a and b were arbitrary, this implies a local conservation law,
</p>
<p>cρ
&part;u
</p>
<p>&part;t
+
</p>
<p>&part;q
</p>
<p>&part;x
= 0.
</p>
<p>Using the formula for q from Fourier&rsquo;s law (6.2), we obtain the one-dimensional
</p>
<p>heat equation:
&part;u
</p>
<p>&part;t
&minus;
</p>
<p>k
</p>
<p>cρ
</p>
<p>&part;2u
</p>
<p>&part;x2
= 0. (6.4)
</p>
<p>For a rod of finite length, the solution u will satisfy boundary conditions that
</p>
<p>depend on how the rod interacts with its environment. If the rod is parametrized by
</p>
<p>x &isin; [0, ℓ] and we assume that each end is held at a fixed temperature, then this fixes
the values at the endpoints,
</p>
<p>u(t, 0) = T0, u(t, ℓ) = T1 (6.5)
</p>
<p>for all t . These are inhomogeneous Dirichlet boundary conditions. In one dimension
</p>
<p>the inhomogeneous problem can be reduced very simply to the homogeneous case
</p>
<p>by noting that
</p>
<p>u0(x) := T0
(
</p>
<p>1 &minus;
x
</p>
<p>ℓ
</p>
<p>)
</p>
<p>+ T1
x
</p>
<p>ℓ
</p>
<p>gives an equilibrium solution to the heat equation satisfying the boundary condi-
</p>
<p>tions (6.5). By the superposition principle, u &minus; u0 satisfies the heat equation with
homogeneous Dirichlet conditions.
</p>
<p>Another possible boundary assumption is that the ends are insulated, so that no
</p>
<p>thermal energy flows in or out. This means that q vanishes at the boundary, yielding</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 Model Problem: Heat Flow in a Metal Rod 99
</p>
<p>the Neumann boundary conditions
</p>
<p>&part;u
</p>
<p>&part;x
(t, 0) =
</p>
<p>&part;u
</p>
<p>&part;x
(t, ℓ) = 0.
</p>
<p>Example 6.1 On the bounded interval [0, π ], we can find product solutions to the
heat equation using Lemma 5.1. For the Dirichlet boundary conditions u(0) =
u(π) = 0, Theorem 5.2 gives the set of sine eigenfunctions (5.5). The corresponding
heat equation solutions are
</p>
<p>u(t, x) = e&minus;n2t sin (nx)
</p>
<p>for n &isin; N. Note that all of these solutions decay exponentially to 0 as t &rarr; &infin;.
For insulated ends we switch to Neumann conditions and obtain the cosine modes.
</p>
<p>The resulting set of solutions is
</p>
<p>u(t, x) = e&minus;n2t cos (nx)
</p>
<p>for n &isin; N0. In this case the n = 0 mode yields a constant solution.
In Chap. 8 we will discuss the construction of series solutions from these trigono-
</p>
<p>metric families. &diams;
</p>
<p>The higher dimensional form of the heat equation can be derived by an argument
</p>
<p>similar to that given above. In Rn , the thermal flux q is vector valued, and Fourier&rsquo;s
</p>
<p>law becomes the gradient formula
</p>
<p>q = &minus;k&nabla;u.
</p>
<p>Local conservation of energy is expressed by the continuity equation (3.20),
</p>
<p>cρ
&part;u
</p>
<p>&part;t
+ &nabla; &middot; q = 0.
</p>
<p>In combination, these yield the n-dimensional heat equation,
</p>
<p>&part;u
</p>
<p>&part;t
&minus;
</p>
<p>k
</p>
<p>cρ
�u = 0. (6.6)
</p>
<p>The importance of the heat equation as a model extends well beyond its original
</p>
<p>thermodynamic context. One of the most prominent examples of this is Albert Ein-
</p>
<p>stein&rsquo;s probabilistic derivation of the heat equation as a model for Brownian motion
</p>
<p>in 1905, in one of the set of papers for which he was later awarded the Nobel prize.
</p>
<p>Brownian motion is named for the botanist Robert Brown, who observed in 1827 that
</p>
<p>minute particles ejected by pollen grains drifted erratically when suspended in water,
</p>
<p>with a jittery motion for which no explanation was available at the time. Einstein
</p>
<p>theorized that the motion was caused by collisions with a large number of molecules
</p>
<p>whose velocities were distributed randomly. The existence of atoms and molecules</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_5">http://dx.doi.org/10.1007/978-3-319-48936-0_5</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_5">http://dx.doi.org/10.1007/978-3-319-48936-0_5</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_5">http://dx.doi.org/10.1007/978-3-319-48936-0_5</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_8">http://dx.doi.org/10.1007/978-3-319-48936-0_8</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_3">http://dx.doi.org/10.1007/978-3-319-48936-0_3</a></div>
</div>
<div class="page"><p/>
<p>100 6 The Heat Equation
</p>
<p>was still unconfirmed in 1905, and Einstein&rsquo;s model provided crucial supporting
</p>
<p>evidence.
</p>
<p>To summarize Einstein&rsquo;s argument, suppose that a total of n particles are dis-
</p>
<p>tributed on the real line. In an interval of time τ , the position of each particle is
</p>
<p>assumed to change by a random amount according to a distribution function φ. To
</p>
<p>be more precise, the number of particles experiencing a displacement between σ and
</p>
<p>σ + dσ is
dn = nφ(σ) dσ.
</p>
<p>The total number of particles is conserved, which imposes the condition
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
φ(σ) dσ = 1. (6.7)
</p>
<p>The distribution of displacements is assumed to be symmetric, φ(σ) = φ(&minus;σ),
meaning that particles are equally likely to move left or right.
</p>
<p>Suppose the distribution of particles at time t is given by a density functionρ(t, x).
</p>
<p>Under the displacement hypothesis, the values of this density at times t and t + τ
are related by
</p>
<p>ρ(t + τ, x) =
&int; &infin;
</p>
<p>&minus;&infin;
ρ(t, x &minus; σ)φ(σ ) dσ. (6.8)
</p>
<p>To find an equation for ρ, Einstein takes the Taylor expansions of the density on both
</p>
<p>sides of (6.8), obtaining
</p>
<p>ρ(t + τ, x) = ρ(t, x)+
&part;ρ
</p>
<p>&part;t
(t, x)τ + . . . (6.9)
</p>
<p>on the left, and
</p>
<p>ρ(t, x &minus; σ) = ρ(t, x)&minus;
&part;ρ
</p>
<p>&part;x
(t, x)σ +
</p>
<p>1
</p>
<p>2
</p>
<p>&part;2ρ
</p>
<p>&part;x2
(t, x)σ 2 + . . .
</p>
<p>inside the integral. Integrating the latter expansion against φ gives, by (6.7) and the
</p>
<p>assumption that φ is even (which knocks out the linear term),
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
ρ(t, x &minus; σ)φ(σ ) dσ = ρ(t, x)+
</p>
<p>1
</p>
<p>2
</p>
<p>&part;2ρ
</p>
<p>&part;x2
(t, x)
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
σ 2φ(σ) dσ + . . . .
</p>
<p>Substituting this formula together with (6.9) into (6.8) and keeping the leading terms
</p>
<p>gives
</p>
<p>&part;ρ
</p>
<p>&part;t
(t, x)τ =
</p>
<p>1
</p>
<p>2
</p>
<p>&part;2ρ
</p>
<p>&part;x2
(t, x)
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
σ 2φ(σ) dσ.
</p>
<p>Einstein then assumes that the value</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 Model Problem: Heat Flow in a Metal Rod 101
</p>
<p>D =
1
</p>
<p>2τ
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
σ 2φ(σ) dσ (6.10)
</p>
<p>is a fixed constant, so that the equation for ρ becomes
</p>
<p>&part;ρ
</p>
<p>&part;t
&minus; D
</p>
<p>&part;2ρ
</p>
<p>&part;x2
= 0,
</p>
<p>i.e., the heat equation. Remarkably, the function φ representing the random distri-
</p>
<p>bution of displacements plays no role in the final equation, except in the value of
</p>
<p>the constant D. This fact is related to a fundamental result in probability called the
</p>
<p>central limit theorem.
</p>
<p>Diffusion models involving random motions of particles are prevalent in physics,
</p>
<p>biology, and chemistry. The same statistical principles appear in other applications
</p>
<p>as well, for example in models of the spread of infection in medicine, or in the study
</p>
<p>of fluctuating financial markets. The heat equation plays a fundamental role in all of
</p>
<p>these applications.
</p>
<p>6.2 Scale-Invariant Solution
</p>
<p>Let us consider the heat equation on R, with physical constants normalized to 1,
</p>
<p>&part;u
</p>
<p>&part;t
&minus;
</p>
<p>&part;2u
</p>
<p>&part;x2
= 0. (6.11)
</p>
<p>Note that the equation is invariant under the rescaling (t, x) �&rarr; (λ2 y, λx), with λ
a nonzero constant. This suggests a change of variables to the scale-invariant ratio
</p>
<p>y := x/
&radic;
</p>
<p>t might simplify the equation.
</p>
<p>Let us try to find a solution of the form u(t, x) = q(y) for t &gt; 0. By the chain
rule,
</p>
<p>&part;u
</p>
<p>&part;t
= &minus;
</p>
<p>y
</p>
<p>2t
q &prime;,
</p>
<p>&part;2u
</p>
<p>&part;x2
=
</p>
<p>1
</p>
<p>t
q &prime;&prime;.
</p>
<p>Thus, as an equation for q, (6.11) reduces to an ODE,
</p>
<p>q &prime;&prime; = &minus;
y
</p>
<p>2
q &prime;.
</p>
<p>This can be solved for q &prime; by separation of variables for ODE, as described in Sect. 2.5,
</p>
<p>q &prime;(y) = q &prime;(0)e&minus;y2/4.
</p>
<p>A further integration yields</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>102 6 The Heat Equation
</p>
<p>q(y) = q &prime;(0)
&int; y
</p>
<p>0
</p>
<p>e&minus;y
&prime;2/4 dy&prime; + q(0).
</p>
<p>In the original coordinates this translates to
</p>
<p>u(t, x) = C1
&int; x&radic;
</p>
<p>t
</p>
<p>0
</p>
<p>e&minus;y
2/4 dy + C2.
</p>
<p>It is easy to confirm that this solves (6.11) for t &gt; 0. To see what happens as t &rarr; 0,
note that
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>e&minus;y
2/4 dy =
</p>
<p>&radic;
π,
</p>
<p>by the computations from Exercise 2.5. Thus
</p>
<p>lim
t&rarr;0
</p>
<p>u(t, x) =
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>C1
&radic;
π + C2, x &gt; 0,
</p>
<p>0, x = 0,
&minus;C1
</p>
<p>&radic;
π + C2, x &lt; 0.
</p>
<p>(6.12)
</p>
<p>In view of this limiting behavior, let us consider the particular solution U defined by
</p>
<p>setting C1 = 1&radic;4π , C2 =
1
2
,
</p>
<p>U (t, x) :=
1
</p>
<p>&radic;
4π
</p>
<p>&int; x&radic;
t
</p>
<p>0
</p>
<p>e&minus;y
2/4 dy +
</p>
<p>1
</p>
<p>2
. (6.13)
</p>
<p>This solution is plotted for some small values of t in Fig. 6.1. By (6.12), limt&rarr;0
U (t, x) = Θ(x), the Heaviside step function defined by
</p>
<p>Θ(x) :=
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>1, x &gt; 0,
1
2
, x = 0,
</p>
<p>0, x &lt; 0.
</p>
<p>The fact that U (t, x) has such a simple limit as t &rarr; 0 can be used to derive a more
general integral formula. Suppose we want to solve (6.11) for the initial condition
</p>
<p>u(t, x) = ϕ(x)
</p>
<p>Fig. 6.1 The heat solution
</p>
<p>U (t, x) for times from t = 0
to 3</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>6.2 Scale-Invariant Solution 103
</p>
<p>with ϕ &isin; C&infin;cpt(R). The key observation is that ϕ can be reproduced by integrating its
derivative against the Heaviside function,
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
ϕ&prime;(z)Θ(x &minus; z) dz =
</p>
<p>&int; x
</p>
<p>&minus;&infin;
ϕ(z) dx &prime;
</p>
<p>= ϕ(x).
</p>
<p>This suggests that we could solve the heat equation with initial condition ϕ by setting
</p>
<p>u(t, x) :=
&int; &infin;
</p>
<p>&minus;&infin;
ϕ&prime;(z)U (t, x &minus; z) dz.
</p>
<p>For t &gt; 0 the function U (t, x &minus; z) is continuously differentiable, so we can integrate
by parts to rewrite this as
</p>
<p>u(t, x) = &minus;
&int; &infin;
</p>
<p>&minus;&infin;
ϕ(z)
</p>
<p>&part;U
</p>
<p>&part;z
(t, x &minus; z) dz
</p>
<p>=
1
</p>
<p>&radic;
4π t
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
ϕ(z)e&minus;(x&minus;z)
</p>
<p>2/4t dz.
</p>
<p>(6.14)
</p>
<p>In terms of the function
</p>
<p>Ht (x) :=
1
</p>
<p>&radic;
4π t
</p>
<p>e&minus;x
2/4t ,
</p>
<p>the solution is
</p>
<p>u(t, x) =
&int; &infin;
</p>
<p>&minus;&infin;
Ht (x &minus; z)ϕ(z) dz.
</p>
<p>This integral is called the convolution of Ht with ϕ and denoted u = Ht &lowast; ϕ.
In the next section, we will generalize this convolution formula to higher dimen-
</p>
<p>sion, and check that it does yield a solution of the heat equation satisfying the desired
</p>
<p>initial condition.
</p>
<p>6.3 Integral Solution Formula
</p>
<p>Consider the heat equation on Rn ,
</p>
<p>&part;u
</p>
<p>&part;t
&minus;�u = 0 (6.15)
</p>
<p>for t &gt; 0, with initial condition
</p>
<p>u(0, x) = g(x)</p>
<p/>
</div>
<div class="page"><p/>
<p>104 6 The Heat Equation
</p>
<p>for x &isin; Rn .
Inspired by the calculations in Sect. 6.2, we define
</p>
<p>Ht (x) := (4π t)&minus;
n
2 e|x|
</p>
<p>2/4t (6.16)
</p>
<p>for t &gt; 0. The normalizing factor (4π t)&minus;
n
2 is chosen so that
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>Ht (x) d
n
x = 1. (6.17)
</p>
<p>(See Exercise 2.5 for the computation of integrals of this type.)
</p>
<p>Direct differentiation shows that for t &gt; 0,
</p>
<p>&part;
</p>
<p>&part;t
Ht &minus;�Ht = 0, (6.18)
</p>
<p>so that Ht (x) is a solution of the heat equation. However, the limit of Ht (x) as t &rarr; 0
is 0 for x 
= 0 and &infin; for x = 0, which does not seem to make sense as a distribution
of temperatures. (We will return to discuss the interpretation of this initial condition
</p>
<p>in Chap. 12.)
</p>
<p>With (6.14) as motivation, our goal in this section is to show that the convolution
</p>
<p>u = Ht &lowast; g satisfies the heat equation on Rn for a continuous and bounded initial
condition g. A function that acts on other functions by convolution is an integral
</p>
<p>kernel, and Ht is specifically called the heat kernel on R
n . It is also called the
</p>
<p>fundamental solution of the heat equation, for reasons we will explain in Chap. 12.
</p>
<p>Theorem 6.2 For a bounded function g &isin; C0(Rn), the heat equation
(
</p>
<p>&part;
</p>
<p>&part;t
&minus;�
</p>
<p>)
</p>
<p>u = 0, u|t=0 = g, (6.19)
</p>
<p>admits a classical solution given by
</p>
<p>u(t, x) = Ht &lowast; g(x). (6.20)
</p>
<p>Proof Explicitly, the formula (6.20) says that
</p>
<p>u(t, x) = (4π t)&minus;
n
2
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>e&minus;|x&minus; y|
2/4t g( y) dn y. (6.21)
</p>
<p>Because the domain is infinite here, we should treat differentiation under the integral
</p>
<p>with some care. To justify this, the key point is that the partial derivatives of Ht can be
</p>
<p>estimated by expressions of the form c1(t, x)e
&minus;c2(t,x)| y|2 , where the dependence of the</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_12">http://dx.doi.org/10.1007/978-3-319-48936-0_12</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_12">http://dx.doi.org/10.1007/978-3-319-48936-0_12</a></div>
</div>
<div class="page"><p/>
<p>6.3 Integral Solution Formula 105
</p>
<p>constants c1 and c2 on t and x is continuous. We will not go into the technical details,
</p>
<p>but this makes it relatively straightforward to check that differentiating under the
</p>
<p>integral works in this case. In particular, the fact that Ht (x) solves the heat equation
</p>
<p>implies (6.19).
</p>
<p>To check the initial condition, fix some x &isin; Rn . A change of variables to w =
( y &minus; x)/
</p>
<p>&radic;
t in (6.21) gives
</p>
<p>u(t, x) = (4π)&minus;
n
2
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>e&minus;|w|
2
</p>
<p>g
(
</p>
<p>x + t
1
2 w
</p>
<p>)
</p>
<p>dnw
</p>
<p>=
&int;
</p>
<p>Rn
</p>
<p>H1(w)g
(
</p>
<p>x + t
1
2 w
</p>
<p>)
</p>
<p>dnw
</p>
<p>By (6.17) we can also write
</p>
<p>g(x) =
&int;
</p>
<p>Rn
</p>
<p>H1(w)g(x) d
n
w.
</p>
<p>Thus the difference we are trying to estimate is
</p>
<p>u(t, x)&minus; g(x) =
&int;
</p>
<p>Rn
</p>
<p>H1(w)
[
</p>
<p>g
(
</p>
<p>x + t
1
2 w
</p>
<p>)
</p>
<p>&minus; g(x)
]
</p>
<p>dnw. (6.22)
</p>
<p>Given ε &gt; 0, we can use the exponential decay of H1(w) as |w| &rarr; &infin; to choose
R so that
</p>
<p>&int;
</p>
<p>|w|&ge;R
H1(w) d
</p>
<p>n
w &lt; ε.
</p>
<p>Since g is bounded by assumption, there exists a constant M so that |g| &le; M . The
&ldquo;large-w&rdquo; piece of (6.22) can thus be estimated by
</p>
<p>&int;
</p>
<p>|w|&ge;R
H1(w)
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
g
(
</p>
<p>x + t
1
2 w
</p>
<p>)
</p>
<p>&minus; g(x)
∣
</p>
<p>∣
</p>
<p>∣
dnw &le; 2Mε. (6.23)
</p>
<p>On the other hand, by the continuity of g we can choose δ &gt; 0 so that for y such
</p>
<p>that for |x &minus; y| &lt; δ, we have
</p>
<p>|g (x &minus; y)&minus; g(x)| &le; ε.
</p>
<p>Thus for |w| &lt; R and t &lt; δ2/R2,
∣
</p>
<p>∣
</p>
<p>∣
g
(
</p>
<p>x + t
1
2 w
</p>
<p>)
</p>
<p>&minus; g(x)
∣
</p>
<p>∣
</p>
<p>∣
&le; ε.
</p>
<p>It follows that for t &lt; δ2/R2,</p>
<p/>
</div>
<div class="page"><p/>
<p>106 6 The Heat Equation
</p>
<p>&int;
</p>
<p>|w|&lt;R
H1(w)
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
g
(
</p>
<p>x + t
1
2 w
</p>
<p>)
</p>
<p>&minus; g(x)
∣
</p>
<p>∣
</p>
<p>∣
dnw &le; ε
</p>
<p>&int;
</p>
<p>|w|&lt;R
H1(w) d
</p>
<p>n
w
</p>
<p>&le; ε
&int;
</p>
<p>Rn
</p>
<p>H1(w) d
n
w
</p>
<p>= ε.
</p>
<p>(6.24)
</p>
<p>Combining the estimates (6.23) and (6.24) gives
</p>
<p>|u(t, x)&minus; g(x)| &le; (2M + 1)ε
</p>
<p>for 0 &lt; t &lt; δ2/R2. Since ε was arbitrary, this shows that
</p>
<p>lim
t&rarr;0
</p>
<p>u(t, x) = g(x). �
</p>
<p>Without extra restrictions on u, the solution of (6.19) is not necessarily unique.
</p>
<p>However, if we start from a bounded initial condition g, then it is physically reason-
</p>
<p>able to assume that u is bounded over finite time intervals.
</p>
<p>Theorem 6.3 Under the assumption that u(t, &middot;) is bounded on [0, T ] &times; Rn for each
T &gt; 0, the solution of the heat equation (6.19) is unique.
</p>
<p>We will develop tools to prove this result (maximum principles) in Chap. 9. The
</p>
<p>statement can be improved by weakening the boundedness hypothesis to an assump-
</p>
<p>tion of exponential growth. The counterexamples to uniqueness exhibit superexpo-
</p>
<p>nential growth and are not considered valid as physical solutions.
</p>
<p>In combination, Theorems 6.2 and 6.3, show that a bounded solution of the heat
</p>
<p>equation on Rn with continuous initial data satisfies (6.21). The function Ht (x) is C
&infin;
</p>
<p>in both variables for t &gt; 0. As we noted in the proof of Theorem 6.2, differentiation
</p>
<p>under the integral is justified in (6.21), so this regularity can be extended to general
</p>
<p>solutions.
</p>
<p>Theorem 6.4 Suppose that u is a bounded solution of the heat equation (6.19) for
</p>
<p>a bounded initial condition g &isin; C0(Rn). Then u &isin; C&infin;((0,&infin;)&times; Rn).
</p>
<p>Similar regularity results hold for the heat equation in other contexts, for example
</p>
<p>on a bounded domain. We will discuss some of these cases later in Sect. 8.6. This
</p>
<p>behavior, i.e., smoothness of solutions that does not depend on the regularity of the
</p>
<p>initial data, is characteristic of parabolic equations.
</p>
<p>Another interesting feature of the heat kernel is the fact that Ht (x) is strictly
</p>
<p>positive for all t &gt; 0 and x &isin; Rn . This means that if g is nonnegative and not iden-
tically zero, then u is nonzero at all points x &isin; Rn for t &gt; 0. Compare this to the
Huygens principle that we observed in Chap. 4, which says that for solutions of the
</p>
<p>wave equation the range of influence of a point is limited by the (finite) propagation
</p>
<p>speed. The heat equation exhibits infinite propagation speed.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_9">http://dx.doi.org/10.1007/978-3-319-48936-0_9</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_8">http://dx.doi.org/10.1007/978-3-319-48936-0_8</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_4">http://dx.doi.org/10.1007/978-3-319-48936-0_4</a></div>
</div>
<div class="page"><p/>
<p>6.3 Integral Solution Formula 107
</p>
<p>We can see the origin of the infinite propagation speed in Einstein&rsquo;s diffusion
</p>
<p>model from Sect. 6.1. In (6.10) the value D, which is assumed to be constant, is the
</p>
<p>average squared displacement per unit time. The fact that D is fixed implies that in
</p>
<p>the continuum limit τ &rarr; 0, the average absolute displacement per unit time diverges.
Hence the infinite propagation speed is built in to the construction of the model. It
</p>
<p>reflects the fact that models of diffusion are inherently statistical, and not expected
</p>
<p>to be accurate on a microscopic scale.
</p>
<p>6.4 Inhomogeneous Problem
</p>
<p>Duhamel&rsquo;s method, which was used to construct solutions of the inhomogeneous
</p>
<p>wave equation in Sect. 4.4, was originally developed in the context of the heat equa-
</p>
<p>tion. There are slight differences in the setup, but the basic idea of translating a
</p>
<p>forcing term into an initial condition applies in both settings.
</p>
<p>Consider the equation on Rn ,
</p>
<p>&part;u
</p>
<p>&part;t
&minus;�u = f (6.25)
</p>
<p>for t &gt; 0, with initial condition u(0, x) = 0. For s &ge; 0, let ηs(t, x) be the solution
of the homogeneous heat equation (6.19) for t &ge; s, subject to the initial condition
</p>
<p>ηs(t, x)
∣
</p>
<p>∣
</p>
<p>t=s = f (s, x). (6.26)
</p>
<p>We claim that the solution is then given by the integral
</p>
<p>u(t, x) :=
&int; t
</p>
<p>0
</p>
<p>ηs(t, x) ds.
</p>
<p>Using the formula for ηs provided by Theorem 6.2, the proposed solution can be
</p>
<p>written
</p>
<p>u(t, x) =
&int; t
</p>
<p>0
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>Ht&minus;s(x &minus; y) f (s, y) dn y ds. (6.27)
</p>
<p>To justify this formula, we must investigate carefully what happens near the point
</p>
<p>t = s.
</p>
<p>Theorem 6.5 Assuming that f &isin; C2([0,&infin;)&times; Rn) and is compactly supported, the
formula (6.27) yields a classical solution to the inhomogeneous heat equation (6.25).
</p>
<p>Proof We can see that u is at least C2 by changing variables in the integral formula
</p>
<p>to obtain
</p>
<p>u(t, x) =
&int; t
</p>
<p>0
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>Hs( y) f (t &minus; s, x &minus; y) dn y ds.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_4">http://dx.doi.org/10.1007/978-3-319-48936-0_4</a></div>
</div>
<div class="page"><p/>
<p>108 6 The Heat Equation
</p>
<p>Since Hs( y) is smooth near s = t and f is compactly supported, differentiation under
the integral is justified. This gives
</p>
<p>&part;u
</p>
<p>&part;t
(t, x) =
</p>
<p>&int; t
</p>
<p>0
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>Hs( y)
&part; f
</p>
<p>&part;t
(t &minus; s, x &minus; y) dn y ds
</p>
<p>+
&int;
</p>
<p>Rn
</p>
<p>Ht ( y)
&part; f
</p>
<p>&part;t
(0, x &minus; y) dn y,
</p>
<p>(6.28)
</p>
<p>and
</p>
<p>�u(t, x) =
&int; t
</p>
<p>0
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>Hs( y)�x f (t &minus; s, x &minus; y) dn y ds. (6.29)
</p>
<p>Our goal is to integrate by parts in these formulas, to exploit the fact that Hs solves
</p>
<p>the heat equation. Here we must be careful, because of the singular behavior of Hs
at s = 0.
</p>
<p>To deal with this singularity, we split the integral at s = ε. For the first integral in
(6.28), switching the t derivative to an s derivative and integrating by parts gives
</p>
<p>&int; t
</p>
<p>ε
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>Hs( y)
&part; f
</p>
<p>&part;t
(t &minus; s, x &minus; y) dn y ds
</p>
<p>= &minus;
&int; t
</p>
<p>ε
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>Hs( y)
&part; f
</p>
<p>&part;s
(t &minus; s, x &minus; y) dn y ds
</p>
<p>=
&int; t
</p>
<p>ε
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>&part;Hs
</p>
<p>&part;s
( y) f (t &minus; s, x &minus; y) dn y ds
</p>
<p>&minus;
&int;
</p>
<p>Rn
</p>
<p>Ht ( y) f (0, x &minus; y) dn y +
&int;
</p>
<p>Rn
</p>
<p>Hε( y) f (t &minus; ε, x &minus; y) dn y
</p>
<p>The corresponding result for (6.29) has no boundary terms because of the compact
</p>
<p>support of f ,
&int; t
</p>
<p>ε
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>Hs( y)�x f (t &minus; s, x &minus; y) dn y ds
</p>
<p>=
&int; t
</p>
<p>ε
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>� y Hs( y) f (t &minus; s, x &minus; y) dn y ds
</p>
<p>Applying these integrations by parts to (6.28) and (6.29), and using the fact that
</p>
<p>( &part;
&part;s
</p>
<p>&minus;�)Hs = 0, we obtain
</p>
<p>(
</p>
<p>&part;
</p>
<p>&part;t
&minus;�
</p>
<p>)
</p>
<p>u(t, x) =
&int;
</p>
<p>Rn
</p>
<p>Hε( y) f (t &minus; ε, x &minus; y) dn y
</p>
<p>+
&int; ε
</p>
<p>0
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>Hs( y)
</p>
<p>(
</p>
<p>&part;
</p>
<p>&part;t
&minus;�x
</p>
<p>)
</p>
<p>f (t &minus; s, x &minus; y) dn y ds.
(6.30)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Inhomogeneous Problem 109
</p>
<p>Since Hs &gt; 0, the second term can be estimated by
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&int; ε
</p>
<p>0
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>Hs( y)
</p>
<p>(
</p>
<p>&part;
</p>
<p>&part;t
&minus;�x
</p>
<p>)
</p>
<p>f (t &minus; s, x &minus; y) dn y ds
∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&le; C
&int; ε
</p>
<p>0
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>Hs( y) d
n
</p>
<p>y ds,
</p>
<p>where C is the maximum value of
∣
</p>
<p>∣( &part;
&part;t
&minus;�) f
</p>
<p>∣
</p>
<p>∣ (which exists by the assumption
</p>
<p>of compact support). By (6.17), the integral of Hs( y) over y &isin; Rn evaluates to 1,
yielding the estimate
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&int; ε
</p>
<p>0
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>Hs( y)
</p>
<p>(
</p>
<p>&part;
</p>
<p>&part;t
&minus;�x
</p>
<p>)
</p>
<p>f (t &minus; s, x &minus; y) dn y ds
∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&le; Cε.
</p>
<p>We can therefore take ε &rarr; 0 in (6.30) to obtain
(
</p>
<p>&part;
</p>
<p>&part;t
&minus;�
</p>
<p>)
</p>
<p>u(t, x) = lim
ε&rarr;0
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>Hε( y) f (t &minus; ε, x &minus; y) dn y.
</p>
<p>The remaining limit is very close to the limit computed in the proof of Theorem 6.2,
</p>
<p>except that t is replaced by t &minus; ε. A simple modification of that argument shows that
</p>
<p>lim
ε&rarr;0
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>Hε( y) f (t &minus; ε, x &minus; y) dn y = f (t, x).
</p>
<p>This completes the proof that u satisfies (6.25). �
</p>
<p>6.5 Exercises
</p>
<p>6.1 Biological processes and chemical reactions are frequently described by
</p>
<p>reaction-diffusion equations, consisting of the heat equation modified by a reaction
</p>
<p>term. Consider the simplest such equation,
</p>
<p>&part;u
</p>
<p>&part;t
+ γ u &minus;�u = 0.
</p>
<p>on Rn with initial condition u(0, x) = f (x). Assuming f is continuous and bounded,
find a formula for the solution. Hint: use a substitution of the form u &rarr; e&minus;at u to
reduce this to the ordinary heat equation.
</p>
<p>6.2 For t &ge; 0, x &ge; 0, suppose that u(t, x) satisfies the one-dimensional heat equa-
tion (6.11) with the initial condition u(0, x) = 0 for x &ge; 0 and the boundary condition
</p>
<p>u(t, 0) = A cos(ωt)</p>
<p/>
</div>
<div class="page"><p/>
<p>110 6 The Heat Equation
</p>
<p>for t &ge; 0. Under the additional requirement that u(t, &middot;) is bounded, find a solution
u(t, x). Hint: use separation of variables and assume that the temporal components
</p>
<p>have the form e&plusmn;iωt .
</p>
<p>6.3 Let Ω &sub; Rn be a bounded domain with piecewise C1 boundary. Suppose that
u(t, x) satisfies the heat equation
</p>
<p>&part;u
</p>
<p>&part;t
&minus;�u = 0,
</p>
<p>on (0,&infin;)&times;Ω . Following the discussion from Sect. 6.1, we define the total thermal
energy at time t by
</p>
<p>U[t] =
&int;
</p>
<p>Ω
</p>
<p>u(t, x) dn x.
</p>
<p>(a) Assume that u satisfies Neumann boundary conditions,
</p>
<p>&part;u
</p>
<p>&part;ν
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&part;Ω
= 0,
</p>
<p>(the insulated case). Show that U is constant.
</p>
<p>(b) Assume that u is positive in the interior of Ω and equals 0 on the boundary.
</p>
<p>Show that U(t) is decreasing in this case.
</p>
<p>6.4 Let Ω &sub; Rn be a bounded domain with piecewise C1 boundary. Suppose that
u(t, x) is real-valued and satisfies the heat equation
</p>
<p>&part;u
</p>
<p>&part;t
&minus;�u = 0,
</p>
<p>on (0,&infin;)&times;Ω . Define
η(t) :=
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>u(t, x)2 dn x. (6.31)
</p>
<p>(a) Assume that u satisfies the Dirichlet boundary conditions:
</p>
<p>u(t, x)|x&isin;&part;Ω = 0
</p>
<p>for t &ge; 0. Show that η decreases as a function of t .
(b) Use (a) to show that a solution u satisfying boundary and initial conditions
</p>
<p>u|t=0 = g, u|x&isin;&part;Ω = h,
</p>
<p>for some continuous functions g on Ω and h on &part;Ω , is uniquely determined by
</p>
<p>g and h.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 7
</p>
<p>Function Spaces
</p>
<p>In the preceding chapters we have seen that separation of variables can generate
</p>
<p>families of product solutions for certain PDE. For example, we found families of
</p>
<p>trigonometric solutions of the wave equation in Sect. 5.2 and the heat equation in
</p>
<p>Sect. 6.1. By the superposition principle, finite linear combinations of these functions
</p>
<p>give more general solutions.
</p>
<p>It is natural to hope that we could push this construction farther and obtain solu-
</p>
<p>tions by infinite series. Solutions of PDE by trigonometric series were studied exten-
</p>
<p>sively in the 18th century by d&rsquo;Alembert, Euler, Bernoulli, and others. However,
</p>
<p>notions of convergence were not well developed at that time, and many fundamental
</p>
<p>questions were left open.
</p>
<p>In this chapter we will introduce some basic concepts of functional analysis, which
</p>
<p>will give us the tools to address some of these fundamental issues.
</p>
<p>7.1 Inner Products and Norms
</p>
<p>We assume that the reader has had a basic course in linear algebra and is familiar
</p>
<p>with the notion of a vector space, i.e., a set equipped with the operations of addition
</p>
<p>and scalar multiplication. The basic finite-dimensional example is the vector space
</p>
<p>R
n . This space comes equipped with a natural inner product given by the dot product
</p>
<p>v &middot; w for v,w &isin; Rn . The Euclidean length of a vector v &isin; Rn is ‖v‖ :=
&radic;
</p>
<p>v &middot; v.
In this section we will review the corresponding definitions for general real or
</p>
<p>complex vector spaces, which include function spaces. One important set of exam-
</p>
<p>ples are the spaces Cm(Ω) introduced in Sect. 2.4, consisting of m-times continuously
</p>
<p>differentiable complex-valued functions on a domain Ω &sub; Rn . Because differentia-
bility and continuity of functions are preserved under linear combination and scalar
</p>
<p>multiplication, Cm(Ω) is naturally a complex vector space.
</p>
<p>An inner product on a complex vector space V is a function of two variables,
</p>
<p>u, v &isin; V �&rarr; 〈u, v〉 &isin; C,
&copy; Springer International Publishing AG 2016
</p>
<p>D. Borthwick, Introduction to Partial Differential Equations,
</p>
<p>Universitext, DOI 10.1007/978-3-319-48936-0_7
</p>
<p>111</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_5">http://dx.doi.org/10.1007/978-3-319-48936-0_5</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_6">http://dx.doi.org/10.1007/978-3-319-48936-0_6</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>112 7 Function Spaces
</p>
<p>satisfying the following properties:
</p>
<p>(I1) Positive definiteness: 〈v, v〉 &ge; 0 for v &isin; V , with equality only if v = 0.
(I2) Symmetry: for v,w &isin; V ,
</p>
<p>〈v,w〉 = 〈w, v〉.
</p>
<p>(I3) Linearity in the first variable: for c1, c2 &isin; C and v1, v2, w &isin; V ,
</p>
<p>〈c1v1 + c2v2, w〉 = c1 〈v1, w〉 + c2 〈v2, w〉 .
</p>
<p>Together, (I2) and (I3) imply conjugate linearity in the second variable,
</p>
<p>〈w, c1v1 + c2v2〉 = c1 〈w, v1〉 + c2 〈w, v2〉 .
</p>
<p>The combination of linearity and conjugate linearity in the respective variables is
</p>
<p>called sesquilinearity. In the real case, the complex conjugation can be omitted,
</p>
<p>reducing sesquilinearity to bilinearity.
</p>
<p>An inner product space is a real or complex vector space V equipped with an
</p>
<p>inner product 〈&middot;, &middot;〉. The Euclidean inner product on Cn is defined by including a
conjugation in the dot product,
</p>
<p>〈v,w〉 := v &middot; w. (7.1)
</p>
<p>One way to define an inner product on function spaces is by integration. For example,
</p>
<p>on C0[0, 1] we could take
</p>
<p>〈 f, g〉 :=
&int; 1
</p>
<p>0
</p>
<p>f g dx .
</p>
<p>Certain geometric notions are carried over from Euclidean geometry to inner product
</p>
<p>spaces. For example, vectors u, v in an inner product space V are called orthogonal if
</p>
<p>〈u, v〉 = 0.
</p>
<p>The analog of length for vectors in V is called a norm. A norm is a function
</p>
<p>‖&middot;‖ : V &rarr; R satisfying the following properties: for all u, v &isin; V and scalar λ,
</p>
<p>(N1) Positive definiteness: ‖u‖ &ge; 0 with equality only if u = 0.
(N2) Homogeneity: ‖λu‖ = |λ| ‖u‖.
(N3) Triangle inequality: ‖u + v‖ &le; ‖u‖ + ‖v‖.
</p>
<p>For an inner product space, the definition of the Euclidean length in terms of the dot
</p>
<p>product suggests that the function
</p>
<p>‖u‖ :=
&radic;
</p>
<p>〈u, u〉 (7.2)
</p>
<p>should yield a norm.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 Inner Products and Norms 113
</p>
<p>Positive definiteness of (7.2) clearly follows from positive definiteness of the
</p>
<p>inner product, and homogeneity follows from sesquilinearity. To see that (7.2) also
</p>
<p>satisfies the triangle inequality, we present a relation first derived in the Euclidean
</p>
<p>case by the great 19th century analyst Augustin-Louis Cauchy; Hermann Schwarz
</p>
<p>later generalized the result to inner product spaces.
</p>
<p>Theorem 7.1 (Cauchy-Schwarz inequality) For an inner product space V with ‖&middot;‖
defined by (7.2),
</p>
<p>|〈v,w〉| &le; ‖v‖ ‖w‖
</p>
<p>for all v,w &isin; V .
</p>
<p>Proof For v,w &isin; V and t &isin; R, consider the function
</p>
<p>q(t) :=
∥
</p>
<p>∥v + t 〈v,w〉w
∥
</p>
<p>∥
</p>
<p>2
,
</p>
<p>The claimed inequality is trivial if w = 0, so assume w �= 0. By (I2), (I3), and (7.2),
</p>
<p>q(t) =
&lang;
</p>
<p>v + t 〈v,w〉w, v + t 〈v,w〉w
&rang;
</p>
<p>= ‖v‖2 + 2t |〈v,w〉|2 + t2 |〈v,w〉|2 ‖w‖2 .
</p>
<p>The minimum of this quadratic polynomial occurs at t0 = &minus;‖w‖&minus;2. Since q &ge; 0,
</p>
<p>0 &le; q(t0) = ‖v‖2 &minus;
|〈v,w〉|2
</p>
<p>‖w‖2
,
</p>
<p>which gives the claimed inequality. �
</p>
<p>The triangle inequality for (7.2) follows from the Cauchy-Schwarz inequality by
</p>
<p>‖u + v‖2 = 〈u + v, u + v〉
= ‖u‖2 + 2 Re 〈u, v〉 + ‖v‖2
</p>
<p>&le; ‖u‖2 + 2 |〈u, v〉| + ‖v‖2
</p>
<p>&le; ‖u‖2 + 2 ‖u‖ ‖v‖ + ‖v‖2
</p>
<p>= (‖u‖ + ‖v‖)2 .
</p>
<p>Thus (7.2) defines a norm associated to the inner product. This definition of the norm
</p>
<p>is used by default on an inner product space.</p>
<p/>
</div>
<div class="page"><p/>
<p>114 7 Function Spaces
</p>
<p>It is possible to have a norm that is not associated to an inner product. For example,
</p>
<p>this is the case for the sup norm, defined for f &isin; C0(Ω), with Ω &sub; Rn bounded, by
</p>
<p>sup
x&isin;Ω
</p>
<p>| f (x)| := sup
{
</p>
<p>| f (x)| ; x &isin; Ω
}
</p>
<p>. (7.3)
</p>
<p>We will explain how to tell that a norm does not come from an inner product in the
</p>
<p>exercises.
</p>
<p>7.2 Lebesgue Integration
</p>
<p>In the early 20th century, Henri Lebesgue developed an extension of the classic
</p>
<p>definition of the integral introduced by Bernhard Riemann in 1854. (Riemann&rsquo;s is
</p>
<p>the version commonly taught in calculus courses.) Lebesgue&rsquo;s definition agrees with
</p>
<p>the Riemann integral when the latter exists, but extends to a broader class of integrable
</p>
<p>functions.
</p>
<p>A full course would be needed to develop this integration theory properly. In this
</p>
<p>section, we present only a brief sketch of the Lebesgue theory, with the focus on the
</p>
<p>features most relevant for applications to PDE.
</p>
<p>The Lebesgue integral is based on a generalized notion of volume for subsets of
</p>
<p>R
n , which can be defined in terms of approximation by rectangles. For a rectangular
</p>
<p>subset in R &sub; Rn , let vol(R) denote the usual notion of volume, the product of
the lengths of the sides. (It is conventional to use &ldquo;volume&rdquo; as a general term when
</p>
<p>the dimension is arbitrary.) The volume of a subset A &sub; Rn can be overestimated
by covering the set with rectangles, as illustrated in Fig. 7.1. The (n-dimensional)
</p>
<p>measure of A is defined by taking the infimum of these overestimates,
</p>
<p>mn(A) := inf
{ &infin;
&sum;
</p>
<p>j=1
vol(R j ); A &sub;
</p>
<p>&infin;
⋃
</p>
<p>j=1
R j
</p>
<p>}
</p>
<p>. (7.4)
</p>
<p>For a bounded region with C1 boundary, the definition (7.4) reproduces the notion of
</p>
<p>volume used in multivariable calculus. Note that the concept of measure is dependent
</p>
<p>on the dimension. The measure of a line segment in R1 is the length, but a line segment
</p>
<p>has measure zero in Rn for n &ge; 2.
There is a major technicality in the application of (7.4). In order to make the
</p>
<p>definition of measure consistent with respect to basic set operations, we cannot apply
</p>
<p>it to all possible subsets of Rn . Instead, the definition is restricted to a special class
</p>
<p>of measurable sets. Lebesgue gave a criterion for measurability that rules out certain
</p>
<p>exotic sets for which volume is ill-defined. Fortunately, these sets are so exotic that
</p>
<p>we are unlikely to encounter them in normal usage. All open and closed sets in Rn
</p>
<p>are included in the measurable category, as are any sets constructed from them by
</p>
<p>basic set operations of union and intersection.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Lebesgue Integration 115
</p>
<p>Fig. 7.1 Covering a set with
</p>
<p>rectangles
</p>
<p>The characteristic function of a set A &sub; Rn is defined by
</p>
<p>χA(x) :=
{
</p>
<p>1, x &isin; A,
0, otherwise.
</p>
<p>(7.5)
</p>
<p>The measure can be used to define the integral of a characteristic function,
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>χA d
n x := mn(A),
</p>
<p>provided A is a measurable set. The integral of a general function is then built
</p>
<p>from approximations by linear combinations of characteristic functions. In order to
</p>
<p>construct these approximations, we need to use a restricted class of functions. A
</p>
<p>function f : Ω &rarr; C is called measurable if the preimage f &minus;1(R) is a measurable
subset of Ω for every rectangle R &sub; C. Every Riemann-integrable function is
measurable in the Lebesgue sense, so the measurable class includes all functions
</p>
<p>encountered in a traditional calculus class. Henceforth, whenever we write f : Ω &rarr;
C or R, we will assume implicitly that f is measurable.
</p>
<p>With this basic picture in mind, we will ask the reader to accept certain important
</p>
<p>consequences of the Lebesgue definition without further justification. In examples
</p>
<p>and exercises we will confine our attention to functions for which ordinary
</p>
<p>Riemannian integrals exist.
</p>
<p>It is standard practice when working with function spaces related to integration
</p>
<p>to make an equivalence:
</p>
<p>f &equiv; g &lArr;&rArr; f = g except on a set of measure zero. (7.6)
</p>
<p>For example, in R the characteristic functions of the intervals (a, b) and [a, b] are
equivalent. In measure theory, a property is said to hold almost everywhere if it fails
</p>
<p>only on a set of measure zero. The equivalence (7.6) amounts to identifying functions
</p>
<p>that agree almost everywhere.</p>
<p/>
</div>
<div class="page"><p/>
<p>116 7 Function Spaces
</p>
<p>If functions f and g satisfy
</p>
<p>&int;
</p>
<p>| f &minus; g| dn x = 0,
</p>
<p>then there is no way to distinguish them in terms of integration. The definition (7.6)
</p>
<p>is motivated by the following:
</p>
<p>Lemma 7.2 For measurable functions f, g : Ω &rarr; C with Ω &sub; Rn ,
&int;
</p>
<p>Ω
</p>
<p>| f &minus; g| dn x = 0
</p>
<p>if and only if f &equiv; g.
</p>
<p>7.3 L p Spaces
</p>
<p>A function f : Ω &rarr; C is defined to be integrable if its integral converges absolutely,
i.e.,
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>| f | dn x &lt; &infin;.
</p>
<p>For p &ge; 1, we define the space of &ldquo;p-integrable&rdquo; functions by
</p>
<p>L p(Ω) :=
{
</p>
<p>f : Ω &rarr; C;
&int;
</p>
<p>Ω
</p>
<p>| f |p dn x &lt; &infin;
}
</p>
<p>, (7.7)
</p>
<p>with the understanding that functions in L p are identified according to the equivalence
</p>
<p>(7.6). The space L p(Ω) is clearly closed under scalar multiplication. Closure under
</p>
<p>addition is a consequence of the convexity of the function x �&rarr; |x |p for p &ge; 1,
which implies the inequality
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>f + g
2
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>p
</p>
<p>&le;
| f |p + |g|p
</p>
<p>2
.
</p>
<p>Hence L p(Ω) is a complex vector space for p &ge; 1.
The L p norm is defined by
</p>
<p>‖ f ‖p :=
(&int;
</p>
<p>Ω
</p>
<p>| f |p dn x
)1
</p>
<p>p
</p>
<p>.
</p>
<p>To check that this is really a norm, we first note that Lemma 7.2 implies positive
</p>
<p>definiteness (N1) because of the equivalence relation (7.6). Homogeneity (N2) is
</p>
<p>satisfied because of cancellation between the powers p and 1/p.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 L p Spaces 117
</p>
<p>Fig. 7.2 Step function
</p>
<p>The triangle inequality (N3) is clear for p = 1 because | f + g| &le; | f | + |g|. And
for p = 2 it follows from Cauchy-Schwarz inequality, because ‖&middot;‖2 is associated to
the inner product
</p>
<p>〈 f, g〉 :=
&int;
</p>
<p>Ω
</p>
<p>f g dn x. (7.8)
</p>
<p>In general the L p triangle inequality,
</p>
<p>‖ f + g‖p &le; ‖ f ‖p + ‖g‖p ,
</p>
<p>is called the Minkowski inequality and holds for p &ge; 1. We omit the proof because
we are mainly concerned with the cases L1 and L2.
</p>
<p>Example 7.3 To illustrate the distinction between the L p norms, consider the func-
</p>
<p>tion
</p>
<p>h := aχ[0,l],
</p>
<p>for a, l &gt; 0, as shown in Fig. 7.2.
</p>
<p>For general p &ge; 1,
</p>
<p>‖h‖p =
[&int; l
</p>
<p>0
</p>
<p>a pdx
</p>
<p>]1/p
</p>
<p>= al1/p.
(7.9)
</p>
<p>If we think of h as a density function, then the L1 norm gives the total mass ‖h‖1 = al.
The sensitivity of ‖&middot;‖p to the spread of the function decreases as p increases, as
illustrated by the fact that
</p>
<p>lim
p&rarr;&infin;
</p>
<p>‖h‖p = a,
</p>
<p>For large p, the L p norms increasingly become measures of local concentration
</p>
<p>rather than mass. &diams;
</p>
<p>Example 7.3 suggests the possibility of defining a space L&infin; that is a limiting case
of the L p spaces, with a norm that generalizes the sup norm (7.3). The sup norm itself
</p>
<p>does not respect the equivalence (7.6), so we must modify the definition to define a
</p>
<p>norm consistent with the other L p spaces.</p>
<p/>
</div>
<div class="page"><p/>
<p>118 7 Function Spaces
</p>
<p>For a function h : Ω &rarr; R, the essential supremum is
</p>
<p>ess-sup(h) := inf
{
</p>
<p>a &isin; R; {h &gt; a} has measure zero
}
</p>
<p>. (7.10)
</p>
<p>Note that {h &gt; a} has measure zero precisely when h is equivalent to a function
bounded by a. The value ess-sup(h) is thus the least upper bound among all functions
</p>
<p>equivalent to h. For continuous functions the essential supremum reduces to the
</p>
<p>supremum.
</p>
<p>For f : Ω &rarr; C, we define
</p>
<p>‖ f ‖&infin; := ess-sup | f | . (7.11)
</p>
<p>The normed vector space L&infin;(Ω) consists of functions which are &ldquo;essentially
bounded&rdquo;,
</p>
<p>L&infin;(Ω) :=
{
</p>
<p>f : Ω &rarr; C; ‖ f ‖&infin; &lt; &infin;
}
</p>
<p>, (7.12)
</p>
<p>subject to the equivalence (7.6).
</p>
<p>Collectively, the L p spaces play a vital role in the analysis of PDE. The different
</p>
<p>norms can be thought of as a collection of measuring tools. Although the full toolkit
</p>
<p>is needed for many applications, for this book we will rely on the cases p = 1, 2,
or &infin;.
Example 7.4 The Schr&ouml;dinger equation in Rn describes the evolution of a quantum-
</p>
<p>mechanical wave function ψ(t, x):
</p>
<p>&minus;i
&part;ψ
</p>
<p>&part;t
= �ψ.
</p>
<p>In Exercise 4.7 we saw that solutions have constant spatial L2 norms,
</p>
<p>‖ψ(t, &middot;)‖2 = ‖ψ(0, &middot;)‖2 ,
</p>
<p>which corresponds to the conservation of total probability. On the other hand, solu-
</p>
<p>tions also satisfy a dispersive estimate
</p>
<p>‖ψ(t, &middot;)‖&infin; &le; Ct&minus;n/2 ‖ψ(0, &middot;)‖1 ,
</p>
<p>for all t &gt; 0, with C a dimensional constant. The norm on the left measures the peak
</p>
<p>amplitude of the wave. By the estimate on the right, this amplitude is bounded in
</p>
<p>terms of the mass and decays as a function of time. In general, dispersive estimates
</p>
<p>describe the spreading of solutions as a function of time. &diams;
</p>
<p>It is conventional to represent elements of L p as ordinary functions, even though
</p>
<p>each element is actually an equivalence class of functions identified under (7.6).
</p>
<p>This usually causes no trouble because equivalent functions give the same results in
</p>
<p>integrals.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_4">http://dx.doi.org/10.1007/978-3-319-48936-0_4</a></div>
</div>
<div class="page"><p/>
<p>7.3 L p Spaces 119
</p>
<p>One point that requires clarification, however, is the issue of continuity or differ-
</p>
<p>entiability of functions in L p. Under (7.6), a Cm function is equivalent to a class of
</p>
<p>functions which are not even continuous. To account for this technicality, we adopt
</p>
<p>the convention that if a function in L p is equivalent to a continuous function, then the
</p>
<p>continuous representative is used by default. This is unambiguous because the con-
</p>
<p>tinuous representative is unique when it exists. Under this convention, the statement
</p>
<p>that f &isin; L p is a Cm function really means that f admits a continuous representative
which is Cm .
</p>
<p>7.4 Convergence and Completeness
</p>
<p>In a normed vector space V , convergence of a sequence vn &rarr; v means
</p>
<p>lim
n&rarr;&infin;
</p>
<p>‖vn &minus; v‖ = 0. (7.13)
</p>
<p>We might also write this as
</p>
<p>v = lim vn,
</p>
<p>provided the choice of norm is clear.
</p>
<p>It frequently proves useful to approximate L p functions by smooth functions. For
</p>
<p>p &ge; 1 there is a natural inclusion
</p>
<p>C&infin;cpt(Ω) &sub; L p(Ω),
</p>
<p>because continuous functions on a compact set are bounded. The Lebesgue theory
</p>
<p>gives the following:
</p>
<p>Theorem 7.5 Assume 1 &le; p &lt; &infin;. For a function f &isin; L p(Ω) there exists an
approximating sequence {ψk} &sub; C&infin;cpt(Ω), such that
</p>
<p>lim
k&rarr;&infin;
</p>
<p>‖ψk &minus; f ‖p = 0.
</p>
<p>A subset W of a normed vector space V is called dense if every v &isin; V can be
obtained as a limit of a sequence in W . Theorem 7.5 thus states that C&infin;cpt(Ω) is dense
in L p(Ω) for p &isin; [1,&infin;).
</p>
<p>In PDE applications, a common method of proving the existence of a solution is to
</p>
<p>construct a sequence of approximate solutions, and then establish convergence of this
</p>
<p>sequence with respect to an appropriate norm. We cannot simply use the definition
</p>
<p>(7.13) to check convergence in this situation, because the limiting function may not
</p>
<p>exist. It is therefore crucial to be able to deduce convergence using only the sequence
</p>
<p>itself.</p>
<p/>
</div>
<div class="page"><p/>
<p>120 7 Function Spaces
</p>
<p>Fig. 7.3 A Cauchy sequence
</p>
<p>with respect to ‖&middot;‖1
</p>
<p>The most useful tool for this purpose is a slightly weaker form of convergence. A
</p>
<p>sequence {vk} &sub; V is said to be Cauchy if the difference between elements converges
to zero: given ε &gt; 0 there exists an N such that k,m &ge; N implies
</p>
<p>‖vk &minus; vm‖ &lt; ε.
</p>
<p>This Cauchy condition is sometimes written as a double limit,
</p>
<p>lim
k,m&rarr;&infin;
</p>
<p>‖vk &minus; vm‖ = 0.
</p>
<p>Every convergent sequence is Cauchy. This is because the triangle inequality
</p>
<p>implies
</p>
<p>‖vk &minus; vm‖ = ‖vk &minus; v + v &minus; vm‖
&le; ‖vk &minus; v‖ + ‖v &minus; vm‖ .
</p>
<p>If the sequence converges then the terms on the right are arbitrarily small for k and
</p>
<p>m sufficiently large.
</p>
<p>In Rn , it follows from the completeness axiom for real numbers that all Cauchy
</p>
<p>sequences are convergent. (See Theorem A.3.) This property does not necessarily
</p>
<p>hold in a general normed vector space, as the following demonstrates.
</p>
<p>Example 7.6 Consider the space C0[&minus;1, 1] equipped with the L1 norm ‖&middot;‖1. For
n &isin; N define the functions
</p>
<p>fn(x) =
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>&minus;1, x &lt; &minus; 1
n
,
</p>
<p>nx &minus; 1
n
&le; x &le; 1
</p>
<p>n
,
</p>
<p>1, x &gt; 1
n
,
</p>
<p>as illustrated in Fig. 7.3.
</p>
<p>We can see that the sequence { fn} is Cauchy by computing
</p>
<p>‖ fk &minus; fm‖1 =
&int; 1
</p>
<p>&minus;1
| fk &minus; fm | dx
</p>
<p>=
∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>1
</p>
<p>k
&minus;
</p>
<p>1
</p>
<p>m
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Convergence and Completeness 121
</p>
<p>However, for f &isin; C0[&minus;1, 1],
</p>
<p>lim
k&rarr;&infin;
</p>
<p>‖ fk &minus; f ‖1 =
&int; 0
</p>
<p>&minus;1
| f + 1| dx +
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>| f &minus; 1| dx .
</p>
<p>This limit equals 0 only if f (x) = &minus;1 for x &lt; 0 and f (x) = 1 for x &gt; 0. That
is not possible for f continuous. Therefore the sequence { fn} does not converge in
C0[&minus;1, 1]. &diams;
</p>
<p>A normed vector space V is complete if all Cauchy sequences in V converge
</p>
<p>within V . Theorem A.3 implies that Euclidean Rn is complete in this sense. For L p
</p>
<p>spaces the Lebesgue integration theory gives the following result.
</p>
<p>Theorem 7.7 For a domain Ω &sub; Rn , the normed vector space L p(Ω) is complete
for each p &isin; [1,&infin;].
</p>
<p>In functional analysis, a complete normed vector space is called a Banach space
</p>
<p>and a complete inner product space is called a Hilbert space. Thus Theorem 7.7 could
</p>
<p>be paraphrased as the statement that L p(Ω) is a Banach space. The inner product
</p>
<p>space L2(Ω) is a Hilbert space.
</p>
<p>A subspace W &sub; V is closed if it contains the limit of every sequence in W that
converges in V .
</p>
<p>Lemma 7.8 If V is a complete normed vector space and W &sub; V is a closed sub-
space, then W is complete with respect to the norm of V .
</p>
<p>Proof Suppose {wk} &sub; W is a Cauchy sequence. The sequence is also Cauchy in
V , and so converges to some v &isin; V by the completeness of V . Since W is closed,
v &isin; W . �
</p>
<p>The L p function spaces have discrete counterparts, denoted by ℓp , whose elements
</p>
<p>are sequences. To a sequence (a1, a2, . . . ) of complex numbers we associate the
</p>
<p>function a : N &rarr; C defined by j �&rarr; a j . The ℓp norm of this function is
</p>
<p>‖a‖ℓp :=
[ &infin;
&sum;
</p>
<p>j=1
</p>
<p>∣
</p>
<p>∣a j
∣
</p>
<p>∣
</p>
<p>p
</p>
<p>]
1
p
</p>
<p>,
</p>
<p>The corresponding vector spaces are
</p>
<p>ℓp(N) :=
{
</p>
<p>a : N &rarr; C; ‖a‖ℓp &lt; &infin;
}
</p>
<p>, (7.14)
</p>
<p>for p &ge; 1. It is possible to prove directly that ℓp(N) is complete, but this can also be
deduced easily from Lemma 7.8. We interpret ℓp(N) as a closed subspace of L p(R)
</p>
<p>consisting of functions which are constant on each interval [ j, j + 1) for j &isin; N
and zero on (&minus;&infin;, 0). On this subspace the L p norm reduces to the ℓp norm, so that
Lemma 7.8 implies that ℓp(N) is complete. In particular, ℓ2(N) is a Hilbert space
</p>
<p>with the inner product</p>
<p/>
</div>
<div class="page"><p/>
<p>122 7 Function Spaces
</p>
<p>〈a, b〉ℓ2 :=
&infin;
&sum;
</p>
<p>j=1
a j b j .
</p>
<p>7.5 Orthonormal Bases
</p>
<p>Let H be an infinite-dimensional complex Hilbert space. A sequence of vectors
</p>
<p>{e1, e2, . . . } &sub; H is orthonormal if
</p>
<p>&lang;
</p>
<p>e j , ek
&rang;
</p>
<p>=
{
</p>
<p>1, j = k,
0, j �= k,
</p>
<p>(7.15)
</p>
<p>for all j, k &isin; N. An orthonormal basis for H is an orthonormal sequence such that
each v &isin; H admits a unique representation as a convergent series,
</p>
<p>v =
&infin;
&sum;
</p>
<p>j=1
c j e j , (7.16)
</p>
<p>with c j &isin; C.
As we will see in Sect. 7.6, the sets of eigenfunctions of certain differential opera-
</p>
<p>tors naturally form orthonormal sequences with respect to the L2 inner product. For
</p>
<p>example the sine eigenfunctions appearing in Theorem 5.2 have this property. If a
</p>
<p>sequence of eigenfunctions forms a basis, then we can expand general functions in
</p>
<p>terms of eigenfunctions.
</p>
<p>Suppose we are given an orthonormal sequence
{
</p>
<p>e j
}
</p>
<p>&sub; H , and we would like to
show that this forms a basis. To represent an element v &isin; H in the form (7.16), we
must decide how to choose the coefficients c j . This works in much the same way as
</p>
<p>it does in finite dimensions. By the orthonormality property (7.15), we can compute
</p>
<p>that
&lang; n
&sum;
</p>
<p>j=1
c j e j , ek
</p>
<p>&rang;
</p>
<p>= ck (7.17)
</p>
<p>for all n &ge; k. Assuming that
&sum;
</p>
<p>c j e j converges to v in H , we can take the limit
</p>
<p>n &rarr; &infin; in (7.17) to compute
〈v, ek〉 = ck . (7.18)
</p>
<p>Based on this calculation, we assign coefficients to v by setting
</p>
<p>c j [v] :=
&lang;
</p>
<p>v, e j
&rang;
</p>
<p>. (7.19)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_5">http://dx.doi.org/10.1007/978-3-319-48936-0_5</a></div>
</div>
<div class="page"><p/>
<p>7.5 Orthonormal Bases 123
</p>
<p>The corresponding partial sums for n &isin; N are denoted by
</p>
<p>Sn[v] :=
n
</p>
<p>&sum;
</p>
<p>j=1
c j [v]e j . (7.20)
</p>
<p>The condition that {e j } is a basis is equivalent to the convergence of Sn[v] &rarr; v in
H for every v &isin; H .
</p>
<p>Theorem 7.9 (Bessel&rsquo;s inequality) Assume that {e j } is an orthonormal sequence in
an infinite-dimensional Hilbert space H. For v &isin; H, the series
</p>
<p>&sum;
∣
</p>
<p>∣c j [v]
∣
</p>
<p>∣
</p>
<p>2
converges
</p>
<p>and the limit satisfies
&infin;
&sum;
</p>
<p>j=1
</p>
<p>∣
</p>
<p>∣c j [v]
∣
</p>
<p>∣
</p>
<p>2 &le; ‖v‖2 .
</p>
<p>Equality holds if and only if Sn[v] &rarr; v in H.
</p>
<p>Proof Using the sesquilinearity (I3) of the inner product, we can expand
</p>
<p>∥
</p>
<p>∥v &minus; Sn[v]
∥
</p>
<p>∥
</p>
<p>2 := 〈v &minus; Sn[v], v &minus; Sn[v]〉
= 〈v, v〉 &minus; 〈Sn[v], v〉 &minus; 〈v, Sn[v]〉 + 〈Sn[v], Sn[v]〉 ,
</p>
<p>for n &isin; N. By the definition (7.20) of Sn[v] and the orthonormality condition (7.15),
</p>
<p>〈Sn[v], v〉 = 〈v, Sn[v]〉 = 〈Sn[v], Sn[v]〉 =
n
</p>
<p>&sum;
</p>
<p>j=1
</p>
<p>∣
</p>
<p>∣c j [v]
∣
</p>
<p>∣
</p>
<p>2
.
</p>
<p>We thus conclude that
</p>
<p>∥
</p>
<p>∥v &minus; Sn[v]
∥
</p>
<p>∥
</p>
<p>2 = ‖v‖2 &minus;
n
</p>
<p>&sum;
</p>
<p>j=1
</p>
<p>∣
</p>
<p>∣c j [v]
∣
</p>
<p>∣
</p>
<p>2
. (7.21)
</p>
<p>Since the left-hand side is positive, the identity (7.21) shows that
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>∣
</p>
<p>∣c j [v]
∣
</p>
<p>∣
</p>
<p>2 &le; ‖v‖2 ,
</p>
<p>for all n &isin; N. The partial sums of the series
&sum;
</p>
<p>∣
</p>
<p>∣c j [v]
∣
</p>
<p>∣
</p>
<p>2
are thus bounded and the
</p>
<p>terms are all positive. Hence the series converges by the monotone sequence theorem,
</p>
<p>to a limit satisfying the claimed bound,</p>
<p/>
</div>
<div class="page"><p/>
<p>124 7 Function Spaces
</p>
<p>&infin;
&sum;
</p>
<p>j=1
</p>
<p>∣
</p>
<p>∣c j [v]
∣
</p>
<p>∣
</p>
<p>2 &le; ‖v‖2 .
</p>
<p>To complete the proof, note that Sn[v] &rarr; v in H means that the limit as n &rarr; &infin;
of the left-hand side of (7.21) is zero. Hence Sn[v] &rarr; v if and only if
</p>
<p>lim
n&rarr;&infin;
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>∣
</p>
<p>∣c j [v]
∣
</p>
<p>∣
</p>
<p>2 = ‖v‖2 . �
</p>
<p>The combination of completeness and Bessel&rsquo;s inequality leads to an alternative
</p>
<p>characterization of a basis that is easier to apply.
</p>
<p>Theorem 7.10 Suppose H is an infinite-dimensional Hilbert space. An orthonor-
</p>
<p>mal sequence in H forms a basis if and only if 0 is the only element of H that is
</p>
<p>orthogonal to all vectors in the sequence.
</p>
<p>Proof Assume first that {e j } forms a basis, so that every v &isin; H can be written as a
convergent sum
</p>
<p>&sum;
</p>
<p>c j [v]e j . If v is orthogonal to all of the vectors e j , then c j [v] = 0
for all j by (7.19). Hence v = 0.
</p>
<p>To establish the converse statement, let {e j } be an orthonormal sequence. For
v &isin; H , Bessel&rsquo;s inequality implies
</p>
<p>&infin;
&sum;
</p>
<p>j=1
</p>
<p>∣
</p>
<p>∣c j [v]
∣
</p>
<p>∣
</p>
<p>2 &le; ‖v‖2 &lt; &infin;. (7.22)
</p>
<p>For n &le; m,
∥
</p>
<p>∥Sm[v] &minus; Sn[v]
∥
</p>
<p>∥
</p>
<p>2 =
∥
</p>
<p>∥
</p>
<p>∥
</p>
<p>∥
</p>
<p>m
&sum;
</p>
<p>j=n+1
c j [v]e j
</p>
<p>∥
</p>
<p>∥
</p>
<p>∥
</p>
<p>∥
</p>
<p>2
</p>
<p>=
m
&sum;
</p>
<p>j=n+1
</p>
<p>∣
</p>
<p>∣c j [v]
∣
</p>
<p>∣
</p>
<p>2
.
</p>
<p>Hence (7.22) implies that
</p>
<p>lim
m,n&rarr;&infin;
</p>
<p>∥
</p>
<p>∥Sm[v] &minus; Sn[v]
∥
</p>
<p>∥
</p>
<p>2 = 0,
</p>
<p>meaning that the sequence {Sn[v]} is Cauchy in H . By completeness of H this implies
that Sn[v] &rarr; ṽ for some ṽ &isin; H .</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5 Orthonormal Bases 125
</p>
<p>Now assume that 0 is the only vector orthogonal to e j for all j . For n &ge; j we
have
</p>
<p>&lang;
</p>
<p>v &minus; Sn[v], e j
&rang;
</p>
<p>=
&lang;
</p>
<p>v, e j
&rang;
</p>
<p>&minus;
&lang;
</p>
<p>Sn[v], e j
&rang;
</p>
<p>= c j [v] &minus; c j [v]
= 0.
</p>
<p>Taking the limit as n &rarr; &infin; with j fixed gives
&lang;
</p>
<p>v &minus; ṽ, e j
&rang;
</p>
<p>= 0.
</p>
<p>Thus v&minus;ṽ is orthogonal to every e j , implying that v = ṽ. This proves that Sn[v] &rarr; v
in H for each v &isin; H , and thus {e j } is a basis. �
</p>
<p>7.6 Self-adjointness
</p>
<p>The process of forming a basis from eigenvectors of an operator should be familiar
</p>
<p>from linear algebra; for a finite-dimensional matrix this is called diagonalization.
</p>
<p>Let us briefly recall the basic facts for the finite-dimensional case. A complex n &times; n
matrix A is self-adjoint (also called Hermitian) if the matrix is equal to its conjugate
</p>
<p>transpose. In terms of the Euclidean inner product (7.1) this means precisely that
</p>
<p>〈Au, v〉 = 〈u, Av〉 (7.23)
</p>
<p>for all u, v &isin; Cn . (In the real case self-adjoint is the same as symmetric.)
The spectral theorem in linear algebra says that for a self-adjoint matrix A there
</p>
<p>exists an orthonormal basis for Cn consisting of eigenvectors for A, with real eigen-
</p>
<p>values. Functional analysis allows a powerful extension of this result, that applies
</p>
<p>in particular to certain differential operators acting on L2 spaces. The full spectral
</p>
<p>theorem for Hilbert spaces is too technical for us to state here, but we will prove a
</p>
<p>version of this for the Laplacian on bounded domains later in Sect. 11.5.
</p>
<p>Self-adjointness remains important as a hypothesis for the more general spectral
</p>
<p>theorem, but even this condition becomes rather technical in the Hilbert space set-
</p>
<p>ting. The issues arise from the fact that differentiable operators cannot act on the
</p>
<p>whole space L2(Ω) because L2 functions need not be differentiable. We will avoid
</p>
<p>these complexities, by focusing on the Laplacian and restricting our attention to C2
</p>
<p>functions.
</p>
<p>Lemma 7.11 Suppose that Ω &isin; Rn is a bounded domain with C1 boundary. If
u, v &isin; C2(Ω) both satisfy either Dirichlet or Neumann boundary conditions on &part;Ω ,
then
</p>
<p>〈�u, v〉 = 〈u,�v〉 . (7.24)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_11">http://dx.doi.org/10.1007/978-3-319-48936-0_11</a></div>
</div>
<div class="page"><p/>
<p>126 7 Function Spaces
</p>
<p>Proof By Green&rsquo;s first identity (Theorem 2.10),
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>[
</p>
<p>u�v &minus; v�u
]
</p>
<p>dn x =
&int;
</p>
<p>&part;Ω
</p>
<p>[
</p>
<p>u
&part;v
</p>
<p>&part;ν
&minus; v
</p>
<p>&part;u
</p>
<p>&part;ν
</p>
<p>]
</p>
<p>d S. (7.25)
</p>
<p>The Dirichlet conditions require that
</p>
<p>u|&part;Ω = v|&part;Ω = 0,
</p>
<p>implying the vanishing of the right-hand side of (7.25). Similarly, the Neumann
</p>
<p>conditions
&part;u
</p>
<p>&part;ν
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&part;Ω
=
</p>
<p>&part;v
</p>
<p>&part;ν
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&part;Ω
= 0 (7.26)
</p>
<p>also imply that the integrand on the right vanishes. �
</p>
<p>Boundary conditions for which (7.24) holds are called self-adjoint boundary con-
</p>
<p>ditions (for the Laplacian). Formally, (7.24) resembles the matrix condition (7.23),
</p>
<p>but of course there is no analog of boundary conditions in the matrix case. The proper
</p>
<p>definition of self-adjointness in functional analysis involves a more precise speci-
</p>
<p>fication of the domain on which � acts and (7.24) holds. Even without going into
</p>
<p>these details, we can still draw some meaningful conclusions from Lemma 7.11.
</p>
<p>Lemma 7.12 Suppose
{
</p>
<p>λ j
}
</p>
<p>is a sequence of eigenvalues of &minus;� on a bounded
domain Ω &sub; Rn , with eigenvectors in C2(Ω) subject to a self-adjoint boundary
condition. Then λ j &isin; R and, after possible rearrangement, the eigenvectors form an
orthonormal sequence in L2(Ω).
</p>
<p>Furthermore, λ j &gt; 0 for Dirichlet conditions, and λ j &ge; 0 for Neumann.
</p>
<p>Proof Suppose we have a sequence
{
</p>
<p>φ j
}
</p>
<p>&sub; C2(Ω) satisfying
</p>
<p>&minus;�φ j = λ jφ j .
</p>
<p>The condition (7.24) implies that for j, k &isin; Z,
&lang;
</p>
<p>�φ j ,φk
&rang;
</p>
<p>=
&lang;
</p>
<p>φ j ,�φk
&rang;
</p>
<p>.
</p>
<p>By the eigenvalue property this reduces to
</p>
<p>(
</p>
<p>λ j &minus; λk
)
</p>
<p>&lang;
</p>
<p>φ j ,φk
&rang;
</p>
<p>= 0. (7.27)
</p>
<p>For j = k the inner product equals
∥
</p>
<p>∥φ j
∥
</p>
<p>∥
</p>
<p>2
</p>
<p>2
&gt; 0, implying that λ j &isin; R for all j .
</p>
<p>We can thus drop the conjugation in (7.27). If λ j �= λk , then this now implies that
&lang;
</p>
<p>φ j ,φk
&rang;
</p>
<p>= 0.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>7.6 Self-adjointness 127
</p>
<p>If some of the λ j &rsquo;s are equal, then every linear combination of the corresponding
</p>
<p>of eigenfunctions will still be an eigenfunction for the same value of λ j . Hence we
</p>
<p>can rearrange the eigenfunctions sharing a common eigenvalue into an orthogonal
</p>
<p>set using the Gram-Schmidt procedure from linear algebra.
</p>
<p>By multiplying the eigenfunctions by constants we can normalize so that
∥
</p>
<p>∥φ j
∥
</p>
<p>∥
</p>
<p>2
=
</p>
<p>1. The divergence theorem (Theorem 2.6) then implies
</p>
<p>λ j =
&lang;
</p>
<p>&minus;�φ j ,φ j
&rang;
</p>
<p>=
&int;
</p>
<p>Ω
</p>
<p>∣
</p>
<p>∣&nabla;φ j
∣
</p>
<p>∣
</p>
<p>2
dn x &minus;
</p>
<p>&int;
</p>
<p>&part;Ω
</p>
<p>φ j
&part;φ j
</p>
<p>&part;ν
d S.
</p>
<p>Either Dirichlet or Neumann conditions will cause the second term to vanish, imply-
</p>
<p>ing that λ j &ge; 0. If λ j = 0 then the equation also shows that &nabla;φ j &equiv; 0, implying that
φ j is constant. In the Dirichlet case the only constant solution is trivial, φ j &equiv; 0, but
for Neumann conditions a nonzero constant is possible. �
</p>
<p>Example 7.13 In Example 5.5 we found a set of eigenfunctions for a circular drum-
</p>
<p>head modeled by the unit disk, with Dirichlet boundary conditions. The eigenfunc-
</p>
<p>tions were given in polar coordinates by
</p>
<p>φk,m(r, θ) := eikθ Jk( jk,mr), k &isin; Z,m &isin; N,
</p>
<p>where jk,m is the mth positive zero of the Bessel function Jk . The eigenvalues of &minus;�
in this case are the values jk,m . Since the only possible matches among the Bessel
</p>
<p>zeros are jk,m = j&minus;k,m , these are the only potential non-orthogonal pairs.
Let us examine the orthogonality condition more explicitly. In polar coordinates,
</p>
<p>the L2 inner product of two eigenfunctions is given by
</p>
<p>&lang;
</p>
<p>φk,m,φk &prime;,m &prime;
&rang;
</p>
<p>L2
=
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>φk,m(r, θ)φk &prime;,m &prime;(r, θ) r dθ dr
</p>
<p>=
&int; 1
</p>
<p>0
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>ei(k&minus;k
&prime;)θ Jk( jk,mr)Jk &prime;( jk &prime;,m &prime;r) r dθ dr.
</p>
<p>Note that the eigenfunctions are clearly orthogonal when k �= k &prime;, because the θ
integral vanishes in this case. If we set k = k &prime;, then the θ integral is trivial and the
inner product becomes
</p>
<p>&lang;
</p>
<p>φk,m,φk,m &prime;
&rang;
</p>
<p>L2
= 2π
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>r Jk( jk,mr)Jk( jk,m &prime;r) dr.
</p>
<p>By Lemma 7.12 this integral vanishes for m �= m &prime;. The cancellations occur because
of the oscillations, just as for sine functions, as Fig. 7.4 illustrates.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_5">http://dx.doi.org/10.1007/978-3-319-48936-0_5</a></div>
</div>
<div class="page"><p/>
<p>128 7 Function Spaces
</p>
<p>Fig. 7.4 Radial components
</p>
<p>J1( j1,mr) of orthogonal
</p>
<p>eigenfunctions on the disk
</p>
<p>= 1 2 3 4
</p>
<p>7.7 Exercises
</p>
<p>7.1 A norm ‖&middot;‖ on a vector space V satisfies the parallelogram law if
</p>
<p>‖v + w‖2 + ‖v &minus; w‖2 = 2 ‖v‖2 + 2 ‖w‖2 ,
</p>
<p>for all v,w &isin; V .
</p>
<p>(a) Show that a norm defined by an inner product as in (7.2) satisfies the parallelo-
</p>
<p>gram law.
</p>
<p>(b) In L p(R), define the functions
</p>
<p>f (x) = χ[0,2], g(x) = χ[0,1] &minus; χ[1,2].
</p>
<p>Use these to show that the parallelogram law fails for ‖&middot;‖p if p �= 2.
</p>
<p>(c) Find an example to show that the parallelogram law fails for the sup norm (7.3).
</p>
<p>7.2 Consider the sequence of functions on R defined by
</p>
<p>fn(x) =
{
</p>
<p>ne&minus;n
2x , x &ge; 0,
</p>
<p>0, x &lt; 0.
</p>
<p>Show that fn &rarr; 0 in L1(R) but not in L2(R).
</p>
<p>7.3 Consider the sequence of functions on R defined by
</p>
<p>gn(x) = n&minus;1χ[0,n].
</p>
<p>Show that gn &rarr; 0 in L2(R) but not in L1(R).</p>
<p/>
</div>
<div class="page"><p/>
<p>7.7 Exercises 129
</p>
<p>7.4 Consider the sequence fn(x) = xn for x &isin; (0, 1). Show that fn &rarr; 0 in L p(0, 1)
for each p &isin; [1,&infin;), but not for p = &infin;.
</p>
<p>7.5 Assume that Ω &sub; Rn is a bounded domain. Show that there is a constant C &gt; 0
such that for f &isin; L2(Ω),
</p>
<p>‖ f ‖1 &le; C ‖ f ‖2 .
</p>
<p>This implies in particular that L2(Ω) &sub; L1(Ω). Find an example to show that this
result does not hold for Ω unbounded.
</p>
<p>7.6 As an application of the Cauchy-Schwarz inequality, we can use the quantity
</p>
<p>η defined in Exercise 6.4 to show that solutions of the heat equation with fixed
</p>
<p>boundary values are uniquely determined by the values at time t = T &gt; 0. Under
the hypotheses from that exercise, suppose that u solves the heat equation with
</p>
<p>u|t=T = 0, u|x&isin;&part;Ω = 0.
</p>
<p>The goal is to show that these assumptions imply u = 0 for all t .
</p>
<p>(a) Use the Cauchy-Schwarz inequality to deduce that
</p>
<p>η&prime;(t)2 &le; 4η(t)
&int;
</p>
<p>Ω
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&part;u
</p>
<p>&part;t
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>2
</p>
<p>dn x.
</p>
<p>where η is defined as in (6.31).
</p>
<p>(b) Show that
</p>
<p>η&prime;&prime;(t) = 4
&int;
</p>
<p>Ω
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&part;u
</p>
<p>&part;t
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>2
</p>
<p>dn x,
</p>
<p>so that the inequality from (a) becomes
</p>
<p>η&prime;(t)2 &le; η(t)η&prime;&prime;(t). (7.28)
</p>
<p>(c) Suppose that η(0) &gt; 0. Then by continuity log η(t) is defined at least in some
</p>
<p>neighborhood of t = 0. Using (7.28), show that
</p>
<p>(log η(t))&prime;&prime; &ge; 0.
</p>
<p>This implies that log η(t) is bounded below by its tangent lines. In particular
</p>
<p>log η(t) &ge; log η(0)+
η&prime;(0)
</p>
<p>η(0)
t,</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_6">http://dx.doi.org/10.1007/978-3-319-48936-0_6</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_6">http://dx.doi.org/10.1007/978-3-319-48936-0_6</a></div>
</div>
<div class="page"><p/>
<p>130 7 Function Spaces
</p>
<p>which implies
</p>
<p>η(t) &ge; η(0)e&minus;ct ,
</p>
<p>for c = &minus;η&prime;(0)/η(0) &gt; 0. Thus if η(0) &gt; 0 then η is strictly positive for all
t &ge; 0.
</p>
<p>(d) Conclude from (c) that if η(T ) = 0, then η(t) = 0 for all t , and deduce that
u = 0.
</p>
<p>7.7 Recall the radial decomposition formula (2.10). We can use this to get a basic
</p>
<p>picture of the degree of singularity or decay at infinity that is allowed in each L p.
</p>
<p>(a) For γ &isin; R consider the function
</p>
<p>g(x) :=
{
</p>
<p>rγ, r &le; 1,
0, r &ge; 1.
</p>
<p>For what values of γ and p &isin; [1,&infin;] is g &isin; L p(Rn)?
</p>
<p>(b) For γ &isin; R consider the function
</p>
<p>h(x) :=
{
</p>
<p>0, r &le; 1,
rγ, r &ge; 1.
</p>
<p>For what values of γ and p &isin; [1,&infin;] is h &isin; L p(Rn)?
</p>
<p>7.8 Consider the eigenfunctions given by (5.5) with ℓ = π.
</p>
<p>(a) Show that
</p>
<p>φn(x) :=
&radic;
</p>
<p>2
</p>
<p>π
sin(nx), n &isin; N,
</p>
<p>defines an orthonormal sequence in L2(0,π). (Hint: recall the trigonometric
</p>
<p>identity sin(α) sin(β) = 1
2
</p>
<p>[
</p>
<p>cos(α &minus; β)&minus; cos(α + β)
]
</p>
<p>.)
</p>
<p>(b) For the function u &equiv; 1, compute the corresponding expansion coefficients,
</p>
<p>ck[1] := 〈1,φk〉 .
</p>
<p>Under Theorem 7.9, what explicit summation condition corresponds to the con-
</p>
<p>vergence Sn[1] &rarr; 1 in L2(0,π)?</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_5">http://dx.doi.org/10.1007/978-3-319-48936-0_5</a></div>
</div>
<div class="page"><p/>
<p>Chapter 8
</p>
<p>Fourier Series
</p>
<p>In his study of heat flow in 1807, Fourier made the radical claim that it should be
</p>
<p>possible to represent all solutions of the one-dimensional heat equation by trigono-
</p>
<p>metric series. As we noted in the introduction to Chap. 7, trigonometric series had
</p>
<p>been studied earlier by other mathematicians. Fourier&rsquo;s innovation was to suggest
</p>
<p>that the general solution could be obtained this way.
</p>
<p>This claim proved difficult to resolve, because the tools of functional analysis that
</p>
<p>we discussed in Chap. 7 were not yet available in Fourier&rsquo;s time. Indeed, the difficult
</p>
<p>problem of Fourier series convergence provided some of the strongest motivation for
</p>
<p>the development of these tools.
</p>
<p>In this chapter we will analyze Fourier series in more detail, and show that the
</p>
<p>Fourier approach yields a general solution for the one-dimensional heat equation. The
</p>
<p>primary significance of this approach to PDE is the philosophy of spectral analysis
</p>
<p>that it inspired. The decomposition of functions with respect to the spectrum of
</p>
<p>a differential operator is a tool with enormous applications, both theoretical and
</p>
<p>practical.
</p>
<p>8.1 Series Solution of the Heat Equation
</p>
<p>Consider the heat equation
&part;u
</p>
<p>&part;t
&minus;�u = 0, (8.1)
</p>
<p>on a domain Ω &sub; Rn , with Dirichlet or Neumann boundary conditions. According
to Lemma 5.1 the product solutions of (8.1) have the form
</p>
<p>u(t, x) = v(t)φ(x),
</p>
<p>&copy; Springer International Publishing AG 2016
</p>
<p>D. Borthwick, Introduction to Partial Differential Equations,
</p>
<p>Universitext, DOI 10.1007/978-3-319-48936-0_8
</p>
<p>131</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_5">http://dx.doi.org/10.1007/978-3-319-48936-0_5</a></div>
</div>
<div class="page"><p/>
<p>132 8 Fourier Series
</p>
<p>where φ solves the Helmholtz equation
</p>
<p>&minus;�φ = λφ
</p>
<p>on Ω . The temporal equation is a simple ODE,
</p>
<p>&part;v
</p>
<p>&part;t
= &minus;λv,
</p>
<p>with the family of solutions
</p>
<p>v(t) = v(0)e&minus;λt . (8.2)
</p>
<p>Let us assume that the equation for φ admits a sequence of solutions φk , with
</p>
<p>eigenvalues λk . We have seen specific examples of this in Chap. 5, including the
</p>
<p>trigonometric case in Theorem 5.2. By (8.2), the corresponding product solutions of
</p>
<p>the heat equation are
</p>
<p>uk(t, x) := e&minus;λk tφk(x).
</p>
<p>Fourier&rsquo;s strategy calls for us to express the general solution as a series,
</p>
<p>u(t, x) =
&infin;
&sum;
</p>
<p>n=1
</p>
<p>ane
&minus;λn tφn(x), (8.3)
</p>
<p>for some choice of coefficients an . To fix the coefficients an in (8.3) we assume an
</p>
<p>initial condition u(0, x) = h(x). Setting t = 0 gives
</p>
<p>h(x) =
&infin;
&sum;
</p>
<p>n=1
</p>
<p>anφn(x). (8.4)
</p>
<p>If we can show that {φn} forms an orthonormal basis of L2(Ω), then this gives us
a way to assign coefficients to h such that (8.4) holds, at least in the sense of L2
</p>
<p>convergence.
</p>
<p>Even if the orthonormal basis property is established, some big issues still remain.
</p>
<p>The fact that each term uk satisfies the heat equation does not guarantee that u does,
</p>
<p>because of the infinite series summation. Similarly, the limit of (8.3) as t &rarr; 0
is not necessarily (8.4), because the limit cannot necessarily be taken inside the
</p>
<p>summation. In this chapter we will explain how to resolve these problems in the
</p>
<p>context of trigonometric series.
</p>
<p>Example 8.1 Consider the case of a one-dimensional metal rod with insulated ends.
</p>
<p>For convenience take the length to beπ , so that x &isin; [0, π ] and the Neumann boundary
conditions are
</p>
<p>&part;u
</p>
<p>&part;x
(t, 0) =
</p>
<p>&part;u
</p>
<p>&part;x
(t, π) = 0.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_5">http://dx.doi.org/10.1007/978-3-319-48936-0_5</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_5">http://dx.doi.org/10.1007/978-3-319-48936-0_5</a></div>
</div>
<div class="page"><p/>
<p>8.1 Series Solution of the Heat Equation 133
</p>
<p>Let us consider the initial condition
</p>
<p>h(x) = 3πx2 &minus; 2x3, (8.5)
</p>
<p>as pictured on the left in Fig. 8.1.
</p>
<p>The boundary conditions are satisfied by cosines,
</p>
<p>φn(x) = cos(nx), n &isin; N0.
</p>
<p>Hence the strategy outlined above calls for us to represent the initial condition as a
</p>
<p>series
</p>
<p>h(x) =
&infin;
&sum;
</p>
<p>n=0
</p>
<p>an cos(nx). (8.6)
</p>
<p>To choose the coefficients, we recall the discussion of basis expansion from
</p>
<p>Sect. 7.5. The cosines satisfy an orthogonality condition with respect to the L2 inner
</p>
<p>product on [0, π ],
</p>
<p>&int; π
</p>
<p>0
</p>
<p>cos(mx) cos(nx) dx =
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>0 m �= n,
π m = n = 0,
π/2 m = n &ge; 1,
</p>
<p>. (8.7)
</p>
<p>This could be checked with trigonometric identities, but it is perhaps easier is to use
</p>
<p>the complex form cos(kx) = 1
2
(eikx + e&minus;ikx ).
</p>
<p>Since the sequence {φn} is not normalized, the coefficient formula (7.18) must be
interpreted as
</p>
<p>an =
1
</p>
<p>‖φn‖2
〈h, φn〉 .
</p>
<p>By (8.7) the Fourier coefficients are thus given by
</p>
<p>a0 =
1
</p>
<p>π
</p>
<p>&int; π
</p>
<p>0
</p>
<p>h(x) dx,
</p>
<p>an =
2
</p>
<p>π
</p>
<p>&int; π
</p>
<p>0
</p>
<p>h(x) cos(nx) dx, n &ge; 1.
(8.8)
</p>
<p>After substituting (8.5) into (8.8), integration by parts yields
</p>
<p>an =
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>π3
</p>
<p>2
, n = 0,
</p>
<p>&minus; 48
πn4
</p>
<p>, n &ge; 1, odd,
0, n &ge; 2, even.
</p>
<p>(8.9)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
</div>
<div class="page"><p/>
<p>134 8 Fourier Series
</p>
<p>h(x) = 3πx2 &minus; 2x3 π
3
</p>
<p>2
&minus;
</p>
<p>48
</p>
<p>π
cosx
</p>
<p>Fig. 8.1 Comparison of the initial condition h(x) with the first two terms of its cosine series
</p>
<p>Figure 8.1 shows a comparison between h and the partial sum S1[h]. The close match
between these functions is clearly evident. And since the higher coefficients decay
</p>
<p>by a factor of n&minus;4, convergence of this series seems quite plausible. The resulting
</p>
<p>solution would be given by
</p>
<p>u(t, x) =
π3
</p>
<p>2
&minus;
</p>
<p>&sum;
</p>
<p>n&isin;Nodd
</p>
<p>48
</p>
<p>πn4
e&minus;n
</p>
<p>2t cos(nx).
</p>
<p>Note that the convergence rate improves dramatically as t increases. &diams;
</p>
<p>8.2 Periodic Fourier Series
</p>
<p>We saw examples of Fourier series based on sines in Theorem 5.2 and cosines in
</p>
<p>Example 8.1. To account for both cases, it is convenient to consider periodic functions
</p>
<p>on R. We define
</p>
<p>T := R/(2πZ), (8.10)
</p>
<p>where the quotient notation means that points separated by an integer multiple of
</p>
<p>2π are considered equivalent. The space Cm(T) consists of the functions in Cm(R)
</p>
<p>which are 2π -periodic.
</p>
<p>Integrals of functions on T are defined by restricting the range of integration to
</p>
<p>an arbitrary interval of length 2π in R. We will write the inner product on L2(T) as
</p>
<p>〈 f, g〉 =
&int; π
</p>
<p>&minus;π
f g dx,
</p>
<p>but the range of integration could be shifted if needed.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_5">http://dx.doi.org/10.1007/978-3-319-48936-0_5</a></div>
</div>
<div class="page"><p/>
<p>8.2 Periodic Fourier Series 135
</p>
<p>The Helmholtz equation on T is
</p>
<p>&minus;
&part;2φ
</p>
<p>&part;x2
= λφ,
</p>
<p>for φ &isin; C2(T), with no need for additional boundary conditions because of the
periodicity. The eigenfunctions are the complex exponentials
</p>
<p>φk(x) := eikx , (8.11)
</p>
<p>for k &isin; Z, with λk = k2.
It is possible to recover cosine and sine Fourier series from the periodic case,
</p>
<p>by restricting our attention to even or odd functions on T. We will demonstrate this
</p>
<p>specialization in the examples and exercises.
</p>
<p>The complex exponentials satisfy a simple orthogonality relation,
</p>
<p>〈φk, φl〉 =
&int; π
</p>
<p>&minus;π
ei(k&minus;l)x dx
</p>
<p>=
</p>
<p>{
</p>
<p>2π, k = l,
0, k �= l.
</p>
<p>We did not include a normalizing factor in (8.11), so ‖φk‖2 = 2π and the Fourier
coefficients of an integrable function f &isin; L1(T) are defined by
</p>
<p>ck[ f ] :=
1
</p>
<p>2π
〈 f, φk〉
</p>
<p>=
1
</p>
<p>2π
</p>
<p>&int; π
</p>
<p>&minus;π
f (x)e&minus;ikx dx .
</p>
<p>(8.12)
</p>
<p>Because the index set is Z rather than N, we define the partial sums of the periodic
</p>
<p>Fourier series by truncating on both sides,
</p>
<p>Sn[ f ](x) :=
n
</p>
<p>&sum;
</p>
<p>k=&minus;n
</p>
<p>ck[ f ]eikx . (8.13)
</p>
<p>For the sequence {φk}, Bessel&rsquo;s inequality (Theorem 7.9) takes the form
</p>
<p>&sum;
</p>
<p>k&isin;Z
</p>
<p>∣
</p>
<p>∣ck[ f ]
∣
</p>
<p>∣
</p>
<p>2 &le;
1
</p>
<p>2π
‖ f ‖22 , (8.14)
</p>
<p>with equality if and only if Sn[ f ] &rarr; f in L2(T).</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
</div>
<div class="page"><p/>
<p>136 8 Fourier Series
</p>
<p>In the specific example considered in Example 8.1, the Fourier series appeared
</p>
<p>to converge very quickly. To illustrate potential complications with the convergence,
</p>
<p>let us consider a function with a jump discontinuity.
</p>
<p>Example 8.2 On the interval [0, π ], define the function
</p>
<p>h(x) =
</p>
<p>{
</p>
<p>0, x &isin; [0, π
2
],
</p>
<p>1, x &isin; (π
2
, π ],
</p>
<p>as pictured on the left in Fig. 8.2. As noted above, in order to represent h as a cosine
</p>
<p>series using the periodic eigenfunctions, we first extend h to T as an even function,
</p>
<p>i.e.,
</p>
<p>h(x) :=
</p>
<p>{
</p>
<p>0, x &isin; [&minus;π
2
, π
</p>
<p>2
] + 2πZ,
</p>
<p>1, x &isin; (π
2
, 3π
</p>
<p>2
] + 2πZ.
</p>
<p>By (8.12), with a shift to the more convenient interval [0, 2π ], the Fourier coefficients
of h are
</p>
<p>ck[h] =
1
</p>
<p>2π
</p>
<p>&int; 3π
2
</p>
<p>π
2
</p>
<p>e&minus;ikx dx
</p>
<p>=
</p>
<p>{
</p>
<p>1
2
, k = 0,
</p>
<p>(&minus;1)k
πk
</p>
<p>sin
(
</p>
<p>πk
2
</p>
<p>)
</p>
<p>, k �= 0.
</p>
<p>Since c&minus;k[h] = ck[h], we can combine terms in the partial sums (8.13) to give
</p>
<p>Sn[h](x) =
1
</p>
<p>2
+ 2
</p>
<p>n
&sum;
</p>
<p>k=1
</p>
<p>(&minus;1)k
</p>
<p>πk
sin
</p>
<p>(
</p>
<p>πk
</p>
<p>2
</p>
<p>)
</p>
<p>cos(kx).
</p>
<p>Figure 8.2 shows a sample of these partial sums. In contrast to the case of Exam-
</p>
<p>ple 8.1, where 2 terms of the Fourier series were enough to give a very convincing
</p>
<p>approximation, we can see significant issues with convergence in the vicinity of the
</p>
<p>jump, even with 40 terms. &diams;
</p>
<p>The Fourier series computed in Example 8.2 makes for a good illustration of some
</p>
<p>different notions of convergence. Consider the sequence of differences h &minus; Sn[h], as
</p>
<p>h(x) S20[h](x) S40[h](x)
</p>
<p>Fig. 8.2 Fourier series expansion for a step function</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Periodic Fourier Series 137
</p>
<p>n = 20 n = 40 n = 100
</p>
<p>Fig. 8.3 Plots of the differences h &minus; Sn[h]
</p>
<p>illustrated in Fig. 8.3. If {φk} forms an L2(T) basis, then we would have
</p>
<p>lim
n&rarr;&infin;
</p>
<p>&int; π
</p>
<p>&minus;π
</p>
<p>∣
</p>
<p>∣h &minus; Sn[h]
∣
</p>
<p>∣
</p>
<p>2
dx
</p>
<p>?= 0.
</p>
<p>It is not easy to judge such a limit visually, but this claim is true, as we will prove in
</p>
<p>Sect. 8.6.
</p>
<p>We could instead focus our attention the values of Sn[h](x) for some fixed x . A
sequence of functions fn is said to converge pointwise to f (assuming these functions
</p>
<p>have a common domain) if for each fixed x in the domain,
</p>
<p>lim
n&rarr;&infin;
</p>
<p>fn(x) = f (x).
</p>
<p>In Fig. 8.3, if we focus our attention on some point x away from the center, then the
</p>
<p>bumps at this point do seem to be decreasing in size as n gets larger. We will verify
</p>
<p>in Sect. 8.3 that this Fourier series converges pointwise except at x = π
2
</p>
<p>.
</p>
<p>Another feature that is quite apparent in Fig. 8.3 is the spike near the center. It is
</p>
<p>possible to prove that such a spike persists, with height essentially constant, for all
</p>
<p>values of n. The historical term for this effect, which is caused by the jump discon-
</p>
<p>tinuity, is the Gibbs phenomenon. It was actually first observed in 1848 by Henry
</p>
<p>Wilbraham, but remained generally unknown until it was rediscovered independently
</p>
<p>by J. Willard Gibbs in 1899.
</p>
<p>The Gibbs phenomenon relates to yet a third definition of convergence. A sequence
</p>
<p>of bounded functions fn is said to converge uniformly to a function f on a set W if
</p>
<p>lim
n&rarr;&infin;
</p>
<p>sup
x&isin;W
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣ fn(x)&minus; f (x)
∣
</p>
<p>∣
</p>
<p>∣ = 0. (8.15)
</p>
<p>In the cases plotted in Fig. 8.3 we can see that
</p>
<p>sup
x&isin;T
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣h(x)&minus; Sn[h](x)
∣
</p>
<p>∣
</p>
<p>∣ &asymp;
1
</p>
<p>2
.
</p>
<p>Since this does not decrease, uniform convergence fails for this series. However, the
</p>
<p>sequence does converge uniformly on domains that exclude a neighborhood of the
</p>
<p>jump point, for example on the interval [0, π
2
&minus; ε] for ε &gt; 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>138 8 Fourier Series
</p>
<p>8.3 Pointwise Convergence
</p>
<p>The basic theory of pointwise convergence of Fourier series was worked out by
</p>
<p>Dirichlet in the mid-19th century. In this section we will establish a criterion for
</p>
<p>pointwise convergence of periodic Fourier series.
</p>
<p>Theorem 8.3 Suppose f &isin; L2(T), and that for x &isin; T the estimate,
</p>
<p>ess-sup
y&isin;[&minus;ε,ε]
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>f (x)&minus; f (x &minus; y)
y
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&lt; &infin;, (8.16)
</p>
<p>holds for some ε &gt; 0. Then
</p>
<p>lim
n&rarr;&infin;
</p>
<p>Sn[ f ](x) = f (x).
</p>
<p>The essential supremum was defined in (7.10). The inequality (8.16) means that,
</p>
<p>after possibly replacing f by an equivalent function in the sense of (7.6), we can
</p>
<p>assume that
</p>
<p>sup
0&lt;|y|&le;ε
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>f (x)&minus; f (x &minus; y)
y
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&lt; &infin;. (8.17)
</p>
<p>This bound holds automatically for f &isin; C1(T), by the estimate
∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>f (x)&minus; f (x &minus; y)
y
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>=
∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>1
</p>
<p>y
</p>
<p>&int; x
</p>
<p>x&minus;y
f &prime;(t) dt
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&le; sup
t&isin;T
</p>
<p>∣
</p>
<p>∣ f &prime;(t)
∣
</p>
<p>∣ .
</p>
<p>Thus Theorem 8.3 shows that the Fourier series a C1 function converges pointwise
</p>
<p>on all of T. The same argument can be extended to functions on T which are merely
</p>
<p>piecewise C1.
</p>
<p>It is possible to prove pointwise convergence with a weaker hypothesis than
</p>
<p>that of Theorem 8.3. However, there are counterexamples, discovered by Fej&eacute;r and
</p>
<p>Lebesgue, that show that pointwise convergence of the Fourier series may fail for
</p>
<p>f &isin; C0(T).
Before getting into the proof of Theorem 8.3, let us consider the structure of the
</p>
<p>partial sums in more detail. Plugging the coefficient formula (8.14) into (8.13) gives
</p>
<p>Sn[ f ](x) =
n
</p>
<p>&sum;
</p>
<p>k=&minus;n
</p>
<p>eikx
1
</p>
<p>2π
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>f (y)e&minus;iky dy
</p>
<p>=
&int; 2π
</p>
<p>0
</p>
<p>f (y)
</p>
<p>(
</p>
<p>1
</p>
<p>2π
</p>
<p>n
&sum;
</p>
<p>k=&minus;n
</p>
<p>eik(x&minus;y)
</p>
<p>)
</p>
<p>dy.
</p>
<p>(8.18)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
</div>
<div class="page"><p/>
<p>8.3 Pointwise Convergence 139
</p>
<p>The function that appears in parentheses is called the Dirichlet kernel,
</p>
<p>Dn(t) :=
1
</p>
<p>2π
</p>
<p>n
&sum;
</p>
<p>k=&minus;n
</p>
<p>eikt . (8.19)
</p>
<p>With this definition the formula for the partial sum becomes
</p>
<p>Sn[ f ](x) =
&int; 2π
</p>
<p>0
</p>
<p>f (y)Dn(x &minus; y) dy. (8.20)
</p>
<p>This could be written as a convolution,
</p>
<p>Sn[ f ] = f &lowast; Dn.
</p>
<p>Because the sum (8.19) is finite, it is clear that the Dirichlet kernel is a smooth
</p>
<p>function on T. It is also easy to compute that
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>Dn(t) dt = 1 (8.21)
</p>
<p>for n &isin; N, since only the k = 0 term in (8.19) contributes to the integral. Applying
the polynomial identity
</p>
<p>1 + z + z2 + &middot; &middot; &middot; + zm =
zm+1 &minus; 1
</p>
<p>z &minus; 1
.
</p>
<p>to (8.19) with z = ei t gives the explicit formula
</p>
<p>Dn(t) =
1
</p>
<p>2π
</p>
<p>ei(n+1)t &minus; e&minus;int
</p>
<p>ei t &minus; 1
. (8.22)
</p>
<p>Factoring ei t/2 out of the numerator and denominator reduces this to
</p>
<p>Dn(t) =
1
</p>
<p>2π
</p>
<p>sin((n + 1
2
)t)
</p>
<p>sin( 1
2
t)
</p>
<p>,
</p>
<p>which makes it clear that Dn is real-valued.
</p>
<p>A plot of Dn(y) for various values of n, as shown in Fig. 8.4, gives some intuition
</p>
<p>as to why we might expect (8.23) to converge to f (x) as n &rarr; &infin;. The function
Dn(y) concentrates at y = 0, and oscillates with increasing frequency away from
this point. These oscillations will cause cancellation as n &rarr; &infin;, except at y = 0.
</p>
<p>Proof of Theorem 8.3 Because both f and Dn are periodic, a change of variables
</p>
<p>y &rarr; x &minus; y allows us to rewrite the convolution in the opposite order:</p>
<p/>
</div>
<div class="page"><p/>
<p>140 8 Fourier Series
</p>
<p>D10 D20 D50
</p>
<p>Fig. 8.4 The Dirichlet kernel for increasing values of n
</p>
<p>Sn[ f ](x) =
&int; π
</p>
<p>&minus;π
Dn(y) f (x &minus; y) dy. (8.23)
</p>
<p>Thus, by (8.21) and (8.23),
</p>
<p>f (x)&minus; Sn[ f ](x) =
&int; π
</p>
<p>&minus;π
[ f (x)&minus; f (x &minus; y)]Dn(y) dy.
</p>
<p>Substituting in with the explicit formula (8.22) for Dn(t) gives
</p>
<p>f (x)&minus; Sn[ f ](x) =
1
</p>
<p>2π
</p>
<p>&int; π
</p>
<p>&minus;π
</p>
<p>f (x)&minus; f (x &minus; y)
eiy &minus; 1
</p>
<p>[
</p>
<p>ei(n+1)y &minus; e&minus;iny
]
</p>
<p>dy. (8.24)
</p>
<p>The crucial observation here is that if we separate the terms inside the brackets, then
</p>
<p>this looks like a formula for Fourier coefficients.
</p>
<p>Assuming that the hypothesis of the theorem is satisfied at x &isin; T, consider the
function
</p>
<p>h(y) :=
f (x)&minus; f (x &minus; y)
</p>
<p>eiy &minus; 1
, (8.25)
</p>
<p>defined for y &isin; T with y �= 0. We can split this into factors as
</p>
<p>h(y) =
f (x)&minus; f (x &minus; y)
</p>
<p>y
</p>
<p>y
</p>
<p>eiy &minus; 1
,
</p>
<p>note that the first factor is essentially bounded near y = 0 by the assumption (8.16).
Since eiy &minus; 1 &sim; iy as y &rarr; 0 by Taylor&rsquo;s approximation, the second factor is also
bounded near y = 0. The hypothesis (8.16) thus guarantees that h(y) is equivalent to
a function that is bounded on the interval [&minus;ε, ε]. Since f &isin; L2(T) and (eiy &minus; 1)&minus;1
is bounded for y &isin; &plusmn; [ε, π ], we conclude from this that h &isin; L2(T).
</p>
<p>We can thus interpret (8.24) in terms of Fourier coefficients,
</p>
<p>f (x)&minus; Sn[ f ](x) = c&minus;n&minus;1[h] &minus; cn[h]. (8.26)
</p>
<p>Bessel&rsquo;s inequality, which takes the form (8.14) here, implies that ck[h] &rarr; 0 as
k &rarr; &plusmn;&infin;. By (8.26) this establishes pointwise convergence at x . �</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 Uniform Convergence 141
</p>
<p>8.4 Uniform Convergence
</p>
<p>According to the definition (8.15) of uniform convergence, fn &rarr; f uniformly on T
if
</p>
<p>sup
x&isin;T
</p>
<p>∣
</p>
<p>∣ fn(x)&minus; f (x)
∣
</p>
<p>∣ &rarr; 0
</p>
<p>as n &rarr; &infin;. This is closely related to the convergence with respect the L&infin; norm, as
introduced in Sect. 7.3. If a sequence converges in the L&infin; sense, then after possibly
</p>
<p>modifying the functions on a set of measure zero we can assume that the convergence
</p>
<p>is uniform.
</p>
<p>Continuity is not necessarily preserved under pointwise limits. For example the
</p>
<p>sequence e&minus;nx
2
</p>
<p>converges pointwise on R but the limit function is discontinuous
</p>
<p>at x = 0. On the other hand, uniform convergence of continuous functions does
guarantee continuity.
</p>
<p>Lemma 8.4 Suppose { fn} &sub; C0(Ω) for a domain Ω &sub; Rn . If { fn} converges
uniformly to a function f : Ω &rarr; R, then f is also continuous.
</p>
<p>Proof The goal is to show that f ( y) can be made close to f (x) by taking y close
</p>
<p>to x. To make use of the uniform convergence, we note that the triangle inequality
</p>
<p>implies
</p>
<p>| f (x)&minus; f ( y)| &le;
∣
</p>
<p>∣ f (x)&minus; fn(x)
∣
</p>
<p>∣+
∣
</p>
<p>∣ fn(x)&minus; fn( y)
∣
</p>
<p>∣+
∣
</p>
<p>∣ fn( y)&minus; f ( y)
∣
</p>
<p>∣. (8.27)
</p>
<p>For n large we can control the first and third terms on the right by the assumption of
</p>
<p>uniform convergence. To control the middle term we can use the continuity of fn .
</p>
<p>Fix x &isin; Ω and ε &gt; 0. By uniform convergence there exists n so that
</p>
<p>sup
y&isin;Ω
</p>
<p>∣
</p>
<p>∣ fn( y)&minus; f ( y)
∣
</p>
<p>∣ &lt; ε. (8.28)
</p>
<p>The fact that fn is continuous at x means that we can find δ &gt; 0 (depending on x)
</p>
<p>such that for y &isin; Ω satisfying |x &minus; y| &lt; δ,
∣
</p>
<p>∣ fn(x)&minus; fn( y)
∣
</p>
<p>∣ &lt; ε. (8.29)
</p>
<p>Combining (8.28) and (8.29) with (8.27) shows that for y &isin; Ω satisfying
|x &minus; y| &lt; δ,
</p>
<p>| f (x)&minus; f ( y)| &lt; 3ε.
</p>
<p>Thus f is continuous at x. �
</p>
<p>Uniform convergence is particularly easy to check for periodic Fourier series,
</p>
<p>because the eigenfunctions φk satisfy
</p>
<p>∣
</p>
<p>∣eikx
∣
</p>
<p>∣ = 1.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
</div>
<div class="page"><p/>
<p>142 8 Fourier Series
</p>
<p>Theorem 8.5 For f &isin; C1(T), the sequence of partial sums Sn[ f ] convergences
uniformly to f .
</p>
<p>Proof The assumption that f &isin; C1(T) implies f &prime; &isin; C0(T). By integration by parts,
</p>
<p>ck[ f &prime;] =
1
</p>
<p>2π
</p>
<p>&int; π
</p>
<p>&minus;π
f &prime;(y)e&minus;iky dy
</p>
<p>=
1
</p>
<p>2π
f (y)e&minus;iky
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>π
</p>
<p>&minus;π
+
</p>
<p>ik
</p>
<p>2π
</p>
<p>&int; π
</p>
<p>&minus;π
f (y)e&minus;iky dy.
</p>
<p>The boundary term cancels by periodicity, leaving
</p>
<p>ck[ f &prime;] = ikck[ f ]. (8.30)
</p>
<p>Since f &prime; &isin; L2(T) also, applying Bessel&rsquo;s inequality in the form (8.14) to the
coefficients (8.30) implies that
</p>
<p>&sum;
</p>
<p>k&isin;Z
</p>
<p>∣
</p>
<p>∣kck[ f ]
∣
</p>
<p>∣
</p>
<p>2
&lt; &infin;. (8.31)
</p>
<p>Let ℓ2(Z\{0}) denote the discrete L2 space on the set consisting of functions
Z\{0} &rarr; C. (The ℓp spaces were introduced in Sect. 7.4.) The sequence
</p>
<p>ak := |kck[ f ]|
</p>
<p>defines an element of ℓ2(Z\{0}) by (8.31). If we define b &isin; ℓ2(Z\{0}) by bk := k&minus;1,
then the sum of the coefficients ck[ f ] with k �= 0 can be expressed as an ℓ2 pairing,
</p>
<p>&sum;
</p>
<p>k �=0
</p>
<p>|ck[ f ]| = 〈a, b〉ℓ2 .
</p>
<p>By the Cauchy-Schwarz inequality on ℓ2(Z\{0}),
</p>
<p>〈a, b〉ℓ2 &le; ‖a‖ℓ2 ‖b‖ℓ2 &lt; &infin;.
</p>
<p>Since the norms of a and b are finite, we conclude that
</p>
<p>&sum;
</p>
<p>k&isin;Z
</p>
<p>|ck[ f ]| &lt; &infin;. (8.32)
</p>
<p>Note that we already know that Sn[ f ] &rarr; f pointwise by Theorem 8.10. This
implies that
</p>
<p>Sn[ f ](x)&minus; f (x) =
&sum;
</p>
<p>|k|&gt;n
</p>
<p>ck[ f ]φk(x)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
</div>
<div class="page"><p/>
<p>8.4 Uniform Convergence 143
</p>
<p>for each x &isin; T. Because |φk(x)| = 1,
∣
</p>
<p>∣Sn[ f ](x)&minus; f (x)
∣
</p>
<p>∣ &le;
&sum;
</p>
<p>|k|&gt;n
</p>
<p>∣
</p>
<p>∣ck[ f ]
∣
</p>
<p>∣. (8.33)
</p>
<p>The right-hand side of (8.33) is independent of x and tends to zero as n &rarr; &infin; by
(8.32), proving that Sn[ f ] &rarr; f uniformly. �
</p>
<p>8.5 Convergence in L2
</p>
<p>The uniform convergence provided by Theorem 8.5 proves to be very helpful in
</p>
<p>resolving the L2 basis question. This is because uniform convergence on T implies
</p>
<p>L2 convergence also, by the integral estimate
</p>
<p>‖ fn &minus; f ‖22 =
&int; π
</p>
<p>&minus;π
| fn &minus; f |2 dx
</p>
<p>&le; 2π sup
x&isin;T
</p>
<p>∣
</p>
<p>∣ fn(x)&minus; f (x)
∣
</p>
<p>∣
</p>
<p>2
.
</p>
<p>Hence Theorem 8.5 gives convergence of Fourier series in L2 for C1 functions. In
</p>
<p>this section we will extend this result to all of L2.
</p>
<p>Theorem 8.6 The normalized periodic Fourier eigenfunctions
</p>
<p>1
&radic;
</p>
<p>2π
eikx , k &isin; Z,
</p>
<p>form an orthonormal basis for L2(T).
</p>
<p>Proof Suppose that u &isin; L2(T) satisfies
</p>
<p>〈u, φk〉 = 0 (8.34)
</p>
<p>for each k &isin; Z. By Theorem 7.10 the conclusion will follow if we can deduce that
this implies u = 0.
</p>
<p>As noted above, for ψ &isin; C1(T) Theorem 8.5 implies that Sn[ψ] &rarr; ψ in L2(T).
In particular, this gives
</p>
<p>lim
n&rarr;&infin;
</p>
<p>〈u, Sn[ψ]〉 = 〈u, ψ〉 . (8.35)
</p>
<p>However, since Sn[ψ] is a finite linear combination of the φk , the assumption (8.34)
implies that
</p>
<p>〈u, Sn[ψ]〉 = 0.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
</div>
<div class="page"><p/>
<p>144 8 Fourier Series
</p>
<p>Hence from (8.35) we deduce that
</p>
<p>〈u, ψ〉 = 0.
</p>
<p>Now recall Theorem 7.5, which says that C&infin;cpt(0, 2π) forms a dense subset of
</p>
<p>L2(0, 2π). This implies also that C1(T) is dense in L2(T). Therefore we can choose
</p>
<p>a sequence {ψl} in C1(T) such that ψl &rarr; u in L2(T). Thus
</p>
<p>‖u‖22 = lim
l&rarr;&infin;
</p>
<p>〈u, ψl〉 ,
</p>
<p>and we just showed that all terms on the right are zero under the assumption (8.34).
</p>
<p>Therefore u = 0. �
</p>
<p>The combination of Theorems 8.6 and 7.9 immediately yields the following:
</p>
<p>Corollary 8.7 (Parseval&rsquo;s identity) For f &isin; L2(T), the periodic Fourier coefficients
satisfy
</p>
<p>&sum;
</p>
<p>k&isin;Z
</p>
<p>|ck[ f ]|2 =
1
</p>
<p>2π
‖ f ‖L2 .
</p>
<p>Applying Parseval&rsquo;s identity to f + g, where f, g &isin; L2(T), and separating out
the cross-term yields the corresponding result for the inner product,
</p>
<p>〈 f, g〉 = 2π
&sum;
</p>
<p>k&isin;Z
</p>
<p>ck[ f ]ck[g]. (8.36)
</p>
<p>Example 8.8 In Example 8.2, we found for the step function h that ck[h] = &plusmn; 1πk for
k odd, c0[h] = 12 , and otherwise ck[h] = 0. So for this case,
</p>
<p>&sum;
</p>
<p>k&isin;Z
</p>
<p>|ck[h]|2 =
1
</p>
<p>4
+ 2
</p>
<p>&sum;
</p>
<p>k&isin;Nodd
</p>
<p>1
</p>
<p>π2k2
.
</p>
<p>On the other hand, ‖h‖22 = π , so Parseval&rsquo;s identity implies
</p>
<p>1
</p>
<p>4
+
</p>
<p>2
</p>
<p>π2
</p>
<p>&sum;
</p>
<p>k&isin;Nodd
</p>
<p>1
</p>
<p>k2
=
</p>
<p>1
</p>
<p>2
.
</p>
<p>Thus we obtain the summation formula
</p>
<p>&sum;
</p>
<p>k&isin;Nodd
</p>
<p>1
</p>
<p>k2
=
</p>
<p>π2
</p>
<p>8
. (8.37)
</p>
<p>&diams;</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
</div>
<div class="page"><p/>
<p>8.5 Convergence in L2 145
</p>
<p>The space L2(0, 2π) can be identified with L2(T) by extending functions peri-
</p>
<p>odically. Hence Theorem 8.6 also implies that
{
</p>
<p>1
2π
</p>
<p>eikx
}
</p>
<p>is an orthonormal basis for
</p>
<p>L2(0, 2π). We can also specialize the periodic results to show that cosine or sine
</p>
<p>series give orthonormal bases for L2(0, ℓ) with basis functions that satisfy Dirichlet
</p>
<p>or Neumann boundary conditions, respectively. We will discuss these cases in the
</p>
<p>exercises.
</p>
<p>8.6 Regularity and Fourier Coefficients
</p>
<p>In the preceding sections we have made some progress in understanding the repre-
</p>
<p>sentation of a function by Fourier series. However, we still have not addressed one of
</p>
<p>the primary questions raised in Sect. 8.1: when does a Fourier series yield a classical
</p>
<p>solution to the original PDE? In this section we will resolve this issue by studying
</p>
<p>the relationship between the regularity of a function and the decay of its Fourier
</p>
<p>coefficients.
</p>
<p>The starting point for this discussion is the computation used in the proof of
</p>
<p>Theorem 8.5,
</p>
<p>ck[ f &prime;] = ikck[ f ]
</p>
<p>for f &isin; C1(T). Repeating this computation inductively gives the following:
</p>
<p>Lemma 8.9 Suppose that f &isin; Cm(T). Then
</p>
<p>ck[ f (m)] = (ik)mck[ f ]. (8.38)
</p>
<p>To describe the decay rates of coefficients, we introduce some convenient order
</p>
<p>notation. For α &isin; R,
</p>
<p>ak = o(kα) means lim
|k|&rarr;&infin;
</p>
<p>|ak |
|k|α
</p>
<p>= 0.
</p>
<p>This is commonly referred to as the &ldquo;little-o&rdquo; notion of order. There is a corresponding
</p>
<p>&ldquo;big-O&rdquo; definition,
</p>
<p>ak = O(kα) means |ak | &le; C |k|α ,
</p>
<p>for all sufficiently large |k|, with C independent of k. Note that the little-o condition
is stronger. The content of the statement ak = o(kα) is that the ratio ak/kα tends to
zero, while ak = O(kα) says only that the ratio is bounded.
</p>
<p>Theorem 8.10 For f &isin; Cm(T) with m &isin; N0,
&sum;
</p>
<p>k&isin;Z
</p>
<p>k2m
∣
</p>
<p>∣ck[ f ]
∣
</p>
<p>∣
</p>
<p>2
&lt; &infin;, (8.39)</p>
<p/>
</div>
<div class="page"><p/>
<p>146 8 Fourier Series
</p>
<p>and
</p>
<p>ck[ f ] = o(k&minus;m).
</p>
<p>Proof The inequality (8.39) follows immediately from a combination of Lemma 8.9
</p>
<p>and and Bessel&rsquo;s inequality in the form (8.14). Since the terms in a convergent series
</p>
<p>must approach zero,
</p>
<p>lim
|k|&rarr;&infin;
</p>
<p>kmck[ f ] = 0,
</p>
<p>which gives the claimed decay estimate. �
</p>
<p>Example 8.11 Consider the cosine series (8.6) computed in Example 8.1 for the
</p>
<p>function h(x) = 3πx2 &minus; 2x3 on (0, π). Although h &isin; C&infin;(0, π), the extension of h
to T as an even function is merely C2. Theorem 8.10 thus implies that
</p>
<p>&sum;
</p>
<p>k4 |ck[h]|2 &lt;
&infin;.
</p>
<p>The periodic Fourier coefficients corresponding to (8.9) are
</p>
<p>ck[h] =
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>π3
</p>
<p>2
, k = 0,
</p>
<p>&minus; 24
πk4
</p>
<p>, k odd,
</p>
<p>0, k �= 0, even.
</p>
<p>This shows much faster decay than predicted, but not the rapid decay we would have
</p>
<p>seen if the even periodic extension had been smooth. &diams;
</p>
<p>Our next goal is to develop a converse to Theorem 8.10 that says that a certain
</p>
<p>level of decay rate of Fourier coefficients guarantees a corresponding level of dif-
</p>
<p>ferentiability for the function. In fact, the first stage of this result has already been
</p>
<p>worked out. Suppose f &isin; L2(T) and its coefficients satisfy
&sum;
</p>
<p>k&isin;Z
</p>
<p>|ck[ f ]| &lt; &infin;. (8.40)
</p>
<p>We know that Sn[ f ] &rarr; f in the L2 sense by Theorem 8.6. By Theorem 8.5 we also
know that {Sn[ f ]} converges uniformly, and so the limit is continuous by Lemma 8.4.
Hence we can conclude that (8.40) implies f &isin; C0(T). Recall from Sect. 7.3 that
when we say an L2 function is Cm we mean this only up to equivalence, i.e., the
</p>
<p>original function might require modification on a set of measure zero to make it Cm .
</p>
<p>Theorem 8.12 Suppose f &isin; L2(T) has Fourier coefficients satisfying
&sum;
</p>
<p>k&isin;Z
</p>
<p>∣
</p>
<p>∣kmck[ f ]
∣
</p>
<p>∣ &lt; &infin;, (8.41)
</p>
<p>for m &isin; N0. Then f &isin; Cm(T).
</p>
<p>Proof As remarked above, the m = 0 case is already taken care of by Theorems 8.5
and 8.6.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
</div>
<div class="page"><p/>
<p>8.6 Regularity and Fourier Coefficients 147
</p>
<p>Assume that (8.41) is satisfied for m = 1. For convenience, let ck := ck[ f ]
and fn = Sn[ f ]. Since fn is a (finite) linear combination of smooth functions, the
derivatives are given by
</p>
<p>f &prime;n(x) =
n
</p>
<p>&sum;
</p>
<p>k=&minus;n
</p>
<p>ikcke
ikx .
</p>
<p>By the m = 0 result, the sequence
{
</p>
<p>f &prime;n
}
</p>
<p>converges uniformly to some g &isin; C0(T).
Our goal is to show that g = f &prime;, which means
</p>
<p>g(x) = lim
y&rarr;0
</p>
<p>f (x + y)&minus; f (x)
t
</p>
<p>,
</p>
<p>for every x &isin; T. To argue this we will decompose the difference quotient as
</p>
<p>f (x + t)&minus; f (x)
t
</p>
<p>&minus; g(x) =
[
</p>
<p>fn(x + y)&minus; fn(x)
y
</p>
<p>&minus; f &prime;n(x)
]
</p>
<p>+
(
</p>
<p>f &prime;n(x)&minus; g(x)
)
</p>
<p>+ Rn(x, y).
(8.42)
</p>
<p>The first term on the right approaches zero by the definition of f &prime;n , and the second
</p>
<p>term approaches zero as n &rarr; &infin; by the construction of g. The remainder term is
</p>
<p>Rn(x, y) :=
&sum;
</p>
<p>|k|&gt;n
</p>
<p>ck
(eik(x+y) &minus; eikx )
</p>
<p>y
,
</p>
<p>which converges absolutely for each y �= 0 by (8.41).
We can estimate the remainder by
</p>
<p>|Rn(x, y)| &le;
&sum;
</p>
<p>|k|&gt;n
</p>
<p>|ck |
∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>eiky &minus; 1
y
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>.
</p>
<p>By noting that
∣
</p>
<p>∣eiky &minus; 1
∣
</p>
<p>∣ = 2 sin(ky/2), a simple calculus estimate gives
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>eiky &minus; 1
y
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&le; |k|
</p>
<p>for all y �= 0. This implies a uniform estimate on the remainder term,
</p>
<p>|Rn(x, y)| &le;
&sum;
</p>
<p>|k|&gt;n
</p>
<p>|kck | . (8.43)
</p>
<p>In particular, by the assumption (8.41) the remainder term is arbitrarily small for n
</p>
<p>large.</p>
<p/>
</div>
<div class="page"><p/>
<p>148 8 Fourier Series
</p>
<p>Fix x &isin; T and ε &gt; 0. By (8.43) and the fact that f &prime;n(x) &rarr; g(x), we can pick n
so that
</p>
<p>∣
</p>
<p>∣ f &prime;n(x)&minus; g(x)
∣
</p>
<p>∣ &lt; ε and |Rn(x, y)| &lt; ε.
</p>
<p>for all y �= 0. For this n and x , the definition of f &prime;n(x) says that we can choose δ such
that 0 &lt; |y| &lt; δ implies
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>fn(x + y)&minus; fn(x)
y
</p>
<p>&minus; f &prime;n(x)
∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&lt; ε.
</p>
<p>Applying these estimates to (8.42) shows that for 0 &lt; |y| &lt; δ,
∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>f (x + y)&minus; f (x)
y
</p>
<p>&minus; g(x)
∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&le; 3ε.
</p>
<p>Since εwas arbitrarily small, this shows that f &prime;(x) = g(x). And since g is continuous,
we conclude that f &isin; C1(T).
</p>
<p>The same argument can now be repeated for higher derivatives, assuming (8.41)
</p>
<p>holds for larger m. �
</p>
<p>The hypothesis (8.41) can be reformulated in terms of a decay condition on the
</p>
<p>coefficients, although this gives a slightly weaker result. If ck[ f ] = O(k&minus;α), then
&sum;
</p>
<p>k&isin;Z
</p>
<p>∣
</p>
<p>∣kmck[ f ]
∣
</p>
<p>∣ &le; C
&sum;
</p>
<p>k&isin;Z
</p>
<p>|k|&minus;α+m ,
</p>
<p>and this series converges provided α &gt; m + 1. Hence Theorem 8.12 implies that
f &isin; Cm(T) under the condition that
</p>
<p>ck[ f ] = O(k&minus;m&minus;1&minus;ǫ)
</p>
<p>for some ε &gt; 0.
</p>
<p>Let us finally return to the one-dimensional heat equation that motivated this
</p>
<p>discussion, first considering the periodic case.
</p>
<p>Theorem 8.13 For h &isin; C0(T), the heat equation on [0,&infin;)&times; T,
</p>
<p>&part;u
</p>
<p>&part;t
&minus;
</p>
<p>&part;2u
</p>
<p>&part;x2
= 0,
</p>
<p>admits a solution u &isin; C&infin;((0,&infin;)&times; T), defined for t &gt; 0 by
</p>
<p>u(t, x) :=
&sum;
</p>
<p>k&isin;Z
</p>
<p>ck[h]e&minus;k
2t eikx , (8.44)
</p>
<p>and satisfying</p>
<p/>
</div>
<div class="page"><p/>
<p>8.6 Regularity and Fourier Coefficients 149
</p>
<p>lim
t&rarr;0
</p>
<p>u(t, x) = h(x) (8.45)
</p>
<p>for each x &isin; T.
</p>
<p>Proof For t &gt; 0, the Fourier coefficients of u(t, &middot;) decay exponentially, and Theo-
rem 8.12 shows that u(t, &middot;) &isin; C&infin;(T) for each t . The same arguments used in the
proof of that theorem apply to the t derivatives. To see this, let un denote the partial
</p>
<p>sum of (8.44),
</p>
<p>un(t, x) :=
n
</p>
<p>&sum;
</p>
<p>k=&minus;n
</p>
<p>cke
&minus;k2t eikx .
</p>
<p>where ck := ck[h]. As a finite sum, this can be differentiated directly,
</p>
<p>&part;un
</p>
<p>&part;t
(t, x) =
</p>
<p>n
&sum;
</p>
<p>k=&minus;n
</p>
<p>(&minus;k2)cke&minus;k
2t eikx . (8.46)
</p>
<p>By Theorem 8.10 the Fourier coefficients of h satisfy ck = o(k&minus;1), so that
∣
</p>
<p>∣
</p>
<p>∣&minus;k2cke&minus;k
2t eikx
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣ &le; Cke&minus;k
2t . (8.47)
</p>
<p>As n &rarr; &infin; the series (8.46) thus converges absolutely for t &gt; 0, allowing us to
define a function
</p>
<p>g := lim
n&rarr;&infin;
</p>
<p>&part;un
</p>
<p>&part;t
.
</p>
<p>For ε &gt; 0, the estimate (8.47) shows that the convergence is uniform for t &ge; ε.
Lemma 8.4 shows that the limit is continuous for t &ge; ε. Since ε &gt; 0 is arbitrary, this
implies g &isin; C0((0,&infin;)&times; T).
</p>
<p>We can argue that g = &part;u/&part;t by considering
</p>
<p>u(t + s, x)&minus; u(t, x)
s
</p>
<p>&minus; h(t, x) =
[
</p>
<p>un(t + s, x)&minus; un(t, x)
s
</p>
<p>&minus;
&part;un
</p>
<p>&part;t
(t, x)
</p>
<p>]
</p>
<p>+
(
</p>
<p>&part;un
</p>
<p>&part;t
(t, x)&minus; h(t, x)
</p>
<p>)
</p>
<p>+ Rn(t, s, x),
</p>
<p>where
</p>
<p>Rn(t, s, x) :=
&sum;
</p>
<p>|k|&gt;n
</p>
<p>cke
&minus;k2t eikx
</p>
<p>(
</p>
<p>e&minus;k
2s &minus; 1
s
</p>
<p>)
</p>
<p>.
</p>
<p>At this point the argument becomes essentially parallel to the analysis of (8.42), so
</p>
<p>we will omit the details. The conclusion is that &part;u/&part;t is continuous on (0,&infin;)&times; T.
The argument can be repeated for higher t derivatives, allowing us to conclude
</p>
<p>that u &isin; C&infin;((0,&infin;) &times; T). Moreover, the partial derivatives of un converge to the</p>
<p/>
</div>
<div class="page"><p/>
<p>150 8 Fourier Series
</p>
<p>corresponding derivatives of u, pointwise on (0,&infin;)&times;T, and uniformly if we restrict
to t &ge; ε for some ε &gt; 0. Since un satisfies the wave equation for each n by
construction, this shows that u satisfies the wave equation also.
</p>
<p>At the moment we only have the tools to prove (8.45) under the stronger assump-
</p>
<p>tion that h &isin; C1(T). In this case we can argue exactly in the proof of Theorem 8.5
that (8.44) converges uniformly for (t, x) &isin; [0,&infin;)&times; T. By Lemma 8.4 this shows
u &isin; C0([0,&infin;)&times; T) and we can just set t = 0 to obtain (8.45).
</p>
<p>If h is merely continuous, then this approach breaks down, because the series
</p>
<p>(8.44) may actually diverge for t = 0. We will cover the C0 case in Chap. 13, after
developing an alternate formula for (8.44). �
</p>
<p>The one-dimensional heat equation derived in Sect. 6.1 involved Dirichlet or Neu-
</p>
<p>mann boundary conditions on an interval [0, ℓ]. By rescaling the interval to [0, π ] and
then extending functions to T with either even or odd symmetry, we apply the results
</p>
<p>for periodic Fourier series results to these cases. In particular, from Theorem 8.13
</p>
<p>we deduce the following:
</p>
<p>Corollary 8.14 Suppose h &isin; C[0, ℓ] and satisfies Dirichlet or Neumann boundary
conditions. The heat equation on [0,&infin;)&times;[0, ℓ] admits a solution u &isin; C&infin;((0,&infin;)&times;
[0, ℓ]), under the same boundary condition, such that
</p>
<p>lim
t&rarr;0
</p>
<p>u(t, x) = h(x)
</p>
<p>for each x &isin; [0, ℓ].
</p>
<p>The solutions obtained in Theorem 8.13 and Corollary 8.14 are uniquely deter-
</p>
<p>mined by the initial condition h. (See Exercise 6.4.) The fact that solutions are smooth
</p>
<p>for t &gt; 0, even when h is merely continuous, is a characteristic property of diffu-
</p>
<p>sion equations. In fact the smoothing phenomenon carries over to cases where h is
</p>
<p>L2 but not even continuous. This is illustrated in Fig. 8.5 for the case considered in
</p>
<p>Example 8.2.
</p>
<p>As these applications show, the Fourier series approach (and spectral analysis
</p>
<p>in general) is well suited to analyzing the regularity of solutions. However, other
</p>
<p>qualitative features are perhaps obscured from this viewpoint. For example, we would
</p>
<p>expect solutions to reflect the physical principle that heat flows from hot to cold. We
</p>
<p>can see this behavior quite clearly in the plots of Fig. 8.5, but it is not at all apparent
</p>
<p>in the series formula (8.44).
</p>
<p>t=0 t=.001 t=.005
</p>
<p>Fig. 8.5 Solutions of the heat equation become smooth for t &gt; 0</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_13">http://dx.doi.org/10.1007/978-3-319-48936-0_13</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_6">http://dx.doi.org/10.1007/978-3-319-48936-0_6</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_6">http://dx.doi.org/10.1007/978-3-319-48936-0_6</a></div>
</div>
<div class="page"><p/>
<p>8.7 Exercises 151
</p>
<p>8.7 Exercises
</p>
<p>8.1 For x &isin; (0, π), let
f (x) = x .
</p>
<p>(a) Extend f to an odd function on T and compute the periodic Fourier coeffi-
</p>
<p>cients ck[ f ] according to (8.12). (Note that the case k = 0 needs to be treated
separately.) Show that the periodic series reduces to a sine series in this case.
</p>
<p>(b) Show that the convergence of the Fourier series at x = π
2
</p>
<p>, which is guaranteed
</p>
<p>by Theorem 8.3, yields the summation formula
</p>
<p>π
</p>
<p>4
= 1 &minus;
</p>
<p>1
</p>
<p>3
+
</p>
<p>1
</p>
<p>5
&minus;
</p>
<p>1
</p>
<p>7
+ &middot; &middot; &middot; .
</p>
<p>(c) Show the Parseval identity (Corollary 8.7) leads to the formula
</p>
<p>&infin;
&sum;
</p>
<p>k=1
</p>
<p>1
</p>
<p>k2
=
</p>
<p>π2
</p>
<p>6
.
</p>
<p>8.2 For x &isin; (0, π), let
g(x) = x .
</p>
<p>(a) Extend g to an even function on T and compute the periodic Fourier coeffi-
</p>
<p>cients ck[g] according to (8.12). (Note that the case k = 0 needs to be treated
separately.) Show that the periodic series reduces to a cosine series in this case.
</p>
<p>(b) Show that the convergence of the Fourier series at x = 0, which is guaranteed
by Theorem 8.3, reproduces the formula (8.37).
</p>
<p>(c) Show the Parseval identity (Corollary 8.7) implies the formula
</p>
<p>&sum;
</p>
<p>k&isin;Nodd
</p>
<p>1
</p>
<p>k4
=
</p>
<p>π4
</p>
<p>96
.
</p>
<p>8.3 Consider the periodic wave equation
</p>
<p>&part;2u
</p>
<p>&part;t2
&minus;
</p>
<p>&part;2u
</p>
<p>&part;x2
= 0
</p>
<p>for t &isin; R and x &isin; T. Suppose the initial conditions are
</p>
<p>u(0, x) = g(x),
&part;u
</p>
<p>&part;t
(0, x) = h(x),
</p>
<p>for g &isin; Cm+1(T) and h &isin; Cm(T), for m &isin; N.</p>
<p/>
</div>
<div class="page"><p/>
<p>152 8 Fourier Series
</p>
<p>(a) Assuming that u(t, x) can be represented as a Fourier series
</p>
<p>u(t, x) =
&sum;
</p>
<p>k&isin;Z
</p>
<p>ak(t)e
ikx , (8.48)
</p>
<p>find an expression for ak(t) in terms of the Fourier coefficients of g and h.
</p>
<p>(b) Using the assumptions on g and h, together with Theorem 8.10, show that the
</p>
<p>coefficients ak(t) satisfy an estimate
</p>
<p>&sum;
</p>
<p>k&isin;Z
</p>
<p>k2m |ak(t)|2 &le; M &lt; &infin;,
</p>
<p>uniformly for t &isin; R.
(c) By the arguments used in Theorem 8.13, (b) implies that the series (8.48) con-
</p>
<p>verges to a solution u satisfying the initial conditions. What could you conclude
</p>
<p>about the differentiability of u?
</p>
<p>8.4 In L2(0, π) consider the sequence
</p>
<p>ψk(x) :=
&radic;
</p>
<p>2
</p>
<p>π
sin kx,
</p>
<p>for k &isin; N.
(a) Show that {ψk} is an orthonormal sequence.
(b) Suppose that f &isin; L2(0, π) and 〈 f, ψk〉 = 0 for all k &isin; N. Show that f &equiv; 0.
</p>
<p>(Hint: extend f to an odd 2π -periodic function on R, which can be regarded as
</p>
<p>an element of L2(T). Then apply Theorem 8.6.)
</p>
<p>(c) Conclude that {ψk} is an orthonormal basis for L2(0, π).
</p>
<p>8.5 Suppose that f &isin; L2(&minus;π, π) satisfies
&int; π
</p>
<p>&minus;π
x l f (x) dx = 0
</p>
<p>for all l &isin; N0.
(a) Show that
</p>
<p>&lang;
</p>
<p>qm,k, f
&rang;
</p>
<p>= 0 for all m &isin; N and k &isin; Z, where
</p>
<p>qm,k(x) :=
m
&sum;
</p>
<p>l=0
</p>
<p>(&minus;ikx)l
</p>
<p>l!
.
</p>
<p>(b) Note that
</p>
<p>lim
m&rarr;&infin;
</p>
<p>qm,k(x) = e&minus;ikx
</p>
<p>by the definition of the complex exponential. Show that this convergence is
</p>
<p>uniform for x &isin; [&minus;π, π ] (with k fixed).</p>
<p/>
</div>
<div class="page"><p/>
<p>8.7 Exercises 153
</p>
<p>(c) Use (a) and (b) to show that for f &isin; L2(&minus;π, π),
&int; π
</p>
<p>&minus;π
e&minus;ikx f (x) dx = 0
</p>
<p>for all k &isin; Z.
(d) Conclude from Theorem 8.6 that f &equiv; 0. (In other words, the monomials
</p>
<p>1, x, x2, . . . form a basis for L2(&minus;π, π), although not an orthonormal one.)
</p>
<p>8.6 The Legendre polynomials are functions of z &isin; [&minus;1, 1] defined by
</p>
<p>Pk(z) :=
1
</p>
<p>2kk!
dk
</p>
<p>dzk
(z2 &minus; 1)k,
</p>
<p>for k &isin; N0. (This corresponds to the case m = 0 in (5.31).) These are solutions of
the eigenvalue equation
</p>
<p>L Pk = k(k + 1)Pk,
</p>
<p>where
</p>
<p>L :=
d
</p>
<p>dz
</p>
<p>[
</p>
<p>(z2 &minus; 1)
d
</p>
<p>dz
</p>
<p>]
</p>
<p>.
</p>
<p>(a) For u, v &isin; C2[&minus;1, 1], check that L satisfies a formal self-adjointness condition,
</p>
<p>〈u, Lv〉L2 = 〈Lu, v〉L2 .
</p>
<p>Conclude that the Pk&rsquo;s with distinct values of k are orthogonal in L
2(&minus;1, 1).
</p>
<p>(b) Use the result of Exercise 8.5 to show that {Pk} forms an orthogonal basis for
L2(&minus;1, 1). (The Pk are normalized by the condition Pk(1) = 0, rather than by
unit L2 norm.)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_5">http://dx.doi.org/10.1007/978-3-319-48936-0_5</a></div>
</div>
<div class="page"><p/>
<p>Chapter 9
</p>
<p>Maximum Principles
</p>
<p>We saw in Sect. 4.7 that conservation of energy can be used to derive uniqueness for
</p>
<p>solutions of the wave equation. In this chapter we will consider another approach to
</p>
<p>issues of uniqueness and stability, based on maximum values. This method applies
</p>
<p>generally to elliptic equations, which describe equilibrium states, and to parabolic
</p>
<p>equations, which are generally used to model diffusion.
</p>
<p>9.1 Model Problem: The Laplace Equation
</p>
<p>As noted in Sect. 5.2, the classical evolution equations such as the heat or wave
</p>
<p>equation have the form
</p>
<p>Pt u &minus;�u = 0,
</p>
<p>where Pt denotes some combination of time derivatives. In an equilibrium state, for
</p>
<p>which the solution is independent of time, these equations all reduce to
</p>
<p>�u = 0,
</p>
<p>which is called the Laplace equation. A solution of the Laplace equation is also called
</p>
<p>a harmonic function. The Laplace equation on a bounded domain Ω is generally
</p>
<p>formulated with an inhomogeneous Dirichlet boundary condition,
</p>
<p>u|&part;Ω = f
</p>
<p>for f : &part;Ω &rarr; R.
</p>
<p>&copy; Springer International Publishing AG 2016
</p>
<p>D. Borthwick, Introduction to Partial Differential Equations,
</p>
<p>Universitext, DOI 10.1007/978-3-319-48936-0_9
</p>
<p>155</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_4">http://dx.doi.org/10.1007/978-3-319-48936-0_4</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_5">http://dx.doi.org/10.1007/978-3-319-48936-0_5</a></div>
</div>
<div class="page"><p/>
<p>156 9 Maximum Principles
</p>
<p>The Laplace equation frequently appears in applications involving vector fields.
</p>
<p>A conservative vector field v &isin; C0(Ω;Rn) can be represented as the gradient of a
</p>
<p>potential function φ &isin; C1(Ω;R),
</p>
<p>v = &nabla;φ,
</p>
<p>If the vector field v is also solenoidal (&nabla; &middot; v = 0), then the potential satisfies the
</p>
<p>Laplace equation
</p>
<p>�φ = 0.
</p>
<p>In fluid dynamics in R3, for example, the velocity field is solenoidal for an incom-
</p>
<p>pressible fluid, such as water, and conservative precisely when the flow is irrotational
</p>
<p>(&nabla; &times; v = 0).
</p>
<p>Electrostatics provides another important source of Laplace problems. In the
</p>
<p>absence of charges, the electric field E is conservative and is commonly written
</p>
<p>as
</p>
<p>E = &minus;&nabla;φ
</p>
<p>where φ is the electric potential. On the other hand, Gauss&rsquo;s law of electrostatics says
</p>
<p>that &nabla; &middot; E is proportional to the electric charge density. Hence, the electric potential
</p>
<p>for a charge-free region satisfies the Laplace equation.
</p>
<p>In the remainder of this section we will consider a particular classical case, the
</p>
<p>Laplace problem on the unit disk. Circular symmetry allows us to solve the equation
</p>
<p>explicitly using Fourier series, and the resulting formula gives some insight into the
</p>
<p>general behavior of harmonic functions.
</p>
<p>Let D denote the open unit disk in R2. Given g &isin; C0(&part;D), our goal is to solve
</p>
<p>�u = 0, u|&part;D = g. (9.1)
</p>
<p>In Sect. 5.3 we used separation of variables in polar coordinates to find the family of
</p>
<p>harmonic functions,
</p>
<p>φk(r, θ) := r
|k|eikθ,
</p>
<p>for k &isin; Z. The boundary &part;D is naturally identified with the space T := R/2πZ
</p>
<p>introduced in Sect. 8.2, parametrized by θ.
</p>
<p>Consider the periodic Fourier series expansion,
</p>
<p>g(θ) =
&sum;
</p>
<p>k&isin;Z
</p>
<p>ck[g]e
ikθ, (9.2)
</p>
<p>where
</p>
<p>ck[g] :=
1
</p>
<p>2π
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>e&minus;ikθg(θ) dθ.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_5">http://dx.doi.org/10.1007/978-3-319-48936-0_5</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_8">http://dx.doi.org/10.1007/978-3-319-48936-0_8</a></div>
</div>
<div class="page"><p/>
<p>9.1 Model Problem: The Laplace Equation 157
</p>
<p>Given that
</p>
<p>eikθ = φk(1, θ),
</p>
<p>we might hope to construct a solution of (9.1) by setting
</p>
<p>u(r, θ) =
&sum;
</p>
<p>k&isin;Z
</p>
<p>ck[g]φk(r, θ). (9.3)
</p>
<p>Theorem 8.10 shows that the sequence {ck[g]} is bounded for g continuous. Note
</p>
<p>also that
</p>
<p>|φk(r, θ)| = r
|k|
</p>
<p>and
&sum;
</p>
<p>k&isin;Z
</p>
<p>r |k| &lt; &infin;
</p>
<p>for r &lt; 1 by geometric series. This implies that (9.3) converges absolutely for r &lt; 1.
</p>
<p>In fact the convergence is uniform on {r &le; R} for R &lt; 1.
</p>
<p>We can write u(r, θ) more explicitly by substituting the definition of ck[g] into
</p>
<p>the integral,
</p>
<p>u(r, θ) =
&sum;
</p>
<p>k&isin;Z
</p>
<p>1
</p>
<p>2π
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>r |k|eik(θ&minus;η)g(η) dη.
</p>
<p>Uniform convergence in θ for r &lt; 1 allows us to move the sum inside the integral,
</p>
<p>yielding the formula
</p>
<p>u(r, θ) =
1
</p>
<p>2π
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>Pr (θ &minus; η)g(η) dη, (9.4)
</p>
<p>where
</p>
<p>Pr (θ) :=
&sum;
</p>
<p>k&isin;Z
</p>
<p>r |k|eikθ. (9.5)
</p>
<p>This function is called the Poisson kernel. Its behavior as r &rarr; 1 is illustrated in
</p>
<p>Fig. 9.1.
</p>
<p>Summing by geometric series gives the formula
</p>
<p>Pr (θ) = 1 +
</p>
<p>&infin;
&sum;
</p>
<p>k=1
</p>
<p>(reiθ)k +
</p>
<p>&infin;
&sum;
</p>
<p>k=1
</p>
<p>(re&minus;iθ)k
</p>
<p>= 1 +
reiθ
</p>
<p>1 &minus; reiθ
+
</p>
<p>re&minus;iθ
</p>
<p>1 &minus; re&minus;iθ
(9.6)
</p>
<p>=
1 &minus; r2
</p>
<p>1 &minus; 2r cos θ + r2
.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_8">http://dx.doi.org/10.1007/978-3-319-48936-0_8</a></div>
</div>
<div class="page"><p/>
<p>158 9 Maximum Principles
</p>
<p>r=.9
</p>
<p>r=.8
</p>
<p>r=.6
</p>
<p>r=.4
θ
</p>
<p>Fig. 9.1 The Poisson kernel Pr (θ) for a succession of radii
</p>
<p>From the series formula (9.5) we can also deduce directly that
</p>
<p>1
</p>
<p>2π
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>Pr (θ) dθ = 1, (9.7)
</p>
<p>since the only nonzero contribution comes from the term k = 0.
</p>
<p>By periodicity, a change of variables η &rarr; θ &minus; η in (9.4) gives the alternate form
</p>
<p>u(r, θ) =
1
</p>
<p>2π
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>Pr (η)g(θ &minus; η) dη. (9.8)
</p>
<p>In view of (9.7), this could be interpreted as a weighted average of f with a weight
</p>
<p>function that depends on r . As r &rarr; 1&minus; this weight function becomes concentrated
</p>
<p>at 0, as Fig. 9.1 demonstrates. This is the mechanism by which we expect to have
</p>
<p>u(r, θ) &rarr; g(θ) as r &rarr; 1&minus;.
</p>
<p>Theorem 9.1 For f &isin; C0(&part;D), the Laplace equation,
</p>
<p>�u = 0 in D, u|&part;D = g,
</p>
<p>admits a classical solution u &isin; C&infin;(D) &cap; C0(D) given by the Poisson integral (9.4).
</p>
<p>Proof The function Pr (θ) is smooth for r &lt; 1, and it follows from (9.5) that
</p>
<p>�Pr (θ) = 0,
</p>
<p>where Pr (θ) is interpreted as a function on D written in polar coordinates. By passing
</p>
<p>derivatives inside the integral, we can deduce from (9.4) that u &isin; C&infin;(D) and</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Model Problem: The Laplace Equation 159
</p>
<p>�u = 0.
</p>
<p>To complete the proof we need to check that
</p>
<p>lim
r&rarr;1&minus;
</p>
<p>u(r, θ) = g(θ) (9.9)
</p>
<p>for every θ &isin; &part;D, which will also show that u &isin; C0(D). Note that (9.9) is not the
</p>
<p>same as claiming that the Fourier series for g converges, which is not necessarily
</p>
<p>true. The difference lies in the order of the limits. In (9.9) we take the limit of the
</p>
<p>Fourier series first for r &lt; 1, and then the limit r &rarr; 1&minus;. This limit exists, as we will
</p>
<p>see below, but if we first set r = 1 in (9.3) then the sum over k may diverge.
</p>
<p>By (9.7) and (9.8) we can write
</p>
<p>u(r, θ)&minus; g(θ) =
1
</p>
<p>2π
</p>
<p>&int; π
</p>
<p>&minus;π
</p>
<p>Pr (η)
[
</p>
<p>g(θ &minus; η)&minus; g(θ)
]
</p>
<p>dη. (9.10)
</p>
<p>The goal is to estimate the left-hand side for r close to 1. Fix θ &isin; D and let ε &gt; 0.
</p>
<p>Since g is continuous, there exists δ &gt; 0 so that
</p>
<p>|g(θ &minus; η)&minus; g(θ)| &lt; ε (9.11)
</p>
<p>for |η| &lt; δ. For |η| &ge; δ we can estimate
</p>
<p>max
δ&le;|η|&le;π
</p>
<p>Pr (η) = Pr (δ). (9.12)
</p>
<p>Thus, splitting the integral (9.10) at |η| = δ gives
</p>
<p>|u(r, θ)&minus; g(θ)| &le;
1
</p>
<p>2π
</p>
<p>&int; δ
</p>
<p>&minus;δ
</p>
<p>Pr (η)
∣
</p>
<p>∣
</p>
<p>∣
g(θ &minus; η)&minus; g(θ)
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
dη
</p>
<p>+
1
</p>
<p>2π
</p>
<p>&int;
</p>
<p>δ&lt;|η|&lt;π
</p>
<p>Pr (η)
∣
</p>
<p>∣
</p>
<p>∣
g(θ &minus; η)&minus; g(θ)
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
dη
</p>
<p>&le;
ε
</p>
<p>2π
</p>
<p>&int; δ
</p>
<p>&minus;δ
</p>
<p>Pr (η)dη +
Pr (δ)
</p>
<p>2π
</p>
<p>&int;
</p>
<p>δ&lt;|η|&lt;π
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
g(θ &minus; η)&minus; g(θ)
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
dη.
</p>
<p>By (9.7) and the fact that Pr &gt; 0,
</p>
<p>1
</p>
<p>2π
</p>
<p>&int; δ
</p>
<p>&minus;δ
</p>
<p>Pr (η) dη &le; 1.
</p>
<p>Furthermore, since g is continuous, |g(θ &minus; η)&minus; g(θ)| is bounded by some constant
</p>
<p>M for all θ and η. This reduces the bound to
</p>
<p>|u(r, θ)&minus; g(θ)| &le; ε + M Pr (δ).</p>
<p/>
</div>
<div class="page"><p/>
<p>160 9 Maximum Principles
</p>
<p>We can now use the fact that
</p>
<p>lim
r&rarr;1&minus;
</p>
<p>Pr (δ) = 0
</p>
<p>to choose R &lt; 1 so that
</p>
<p>M Pr (δ) &le; ε
</p>
<p>for R &lt; r &lt; 1. We conclude that
</p>
<p>|u(r, θ)&minus; g(θ)| &le; 2ε
</p>
<p>for R &lt; r &lt; 1. Since ε was arbitrary, this shows
</p>
<p>lim
r&rarr;1&minus;
</p>
<p>|u(r, θ)&minus; g(θ)| = 0.
</p>
<p>�
</p>
<p>For students who know some complex analysis, we note that the formula (9.4)
</p>
<p>could be deduced from the Cauchy integral formula, because any harmonic function
</p>
<p>on D is the real part of a holomorphic function.
</p>
<p>Example 9.2 For 0 &lt; a &lt; π, suppose the boundary function is given by
</p>
<p>g(θ) =
</p>
<p>{
</p>
<p>1 &minus; |θ|
a
, |θ| &le; a,
</p>
<p>0, a &lt; |θ| &le; π,
</p>
<p>as shown in Fig. 9.2.
</p>
<p>This boundary condition could represent, for example, a hot spot at one point on
</p>
<p>the edge of a metal plate. The corresponding equilibrium temperature distribution
</p>
<p>within the plate is given by calculating the Fourier coefficients of g and substituting
</p>
<p>into (9.3). The resulting solution,
</p>
<p>u(r, θ) =
a
</p>
<p>2π
+
</p>
<p>2
</p>
<p>πa
</p>
<p>&infin;
&sum;
</p>
<p>k=1
</p>
<p>1 &minus; cos(ka)
</p>
<p>k2
cos(kθ),
</p>
<p>is illustrated in Fig. 9.3. &diams;
</p>
<p>Fig. 9.2 Boundary function
</p>
<p>with a triangular peak
</p>
<p>&minus;a a</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Mean Value Formula 161
</p>
<p>Fig. 9.3 Contour plot of the
</p>
<p>harmonic function from
</p>
<p>Example 9.2
</p>
<p>9.2 Mean Value Formula
</p>
<p>Setting r = 0 in the Poisson formula (9.4) gives
</p>
<p>u(0) =
1
</p>
<p>2π
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>g(θ) dθ, (9.13)
</p>
<p>because P0(θ) = 1. In other words, the value of a harmonic function at the center of
</p>
<p>the disk is equal to its average value on the boundary. This phenomenon is illustrated
</p>
<p>in Fig. 9.4. In this section we will extend (9.13) to an averaging formula that works
</p>
<p>in any dimension.
</p>
<p>The (n &minus; 1)-dimensional volume of a sphere of radius r is
</p>
<p>vol[&part;B(x0; r)] = Anr
n&minus;1, (9.14)
</p>
<p>where An denotes the volume of the unit sphere in R
n , as defined in (2.13). It follows
</p>
<p>from the radial integral formula (2.10) that
</p>
<p>Fig. 9.4 Mean value
</p>
<p>property of a harmonic
</p>
<p>function</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>162 9 Maximum Principles
</p>
<p>vol[B(x0; r)] =
Anr
</p>
<p>n
</p>
<p>n
. (9.15)
</p>
<p>To state the mean value formula for a ball of radius R, we introduce the family of
</p>
<p>radial functions,
</p>
<p>G R(x) :=
</p>
<p>{
</p>
<p>1
2π
</p>
<p>ln( r
R
), n = 2,
</p>
<p>1
(n&minus;2)An
</p>
<p>[
</p>
<p>1
Rn&minus;2
</p>
<p>&minus; 1
rn&minus;2
</p>
<p>]
</p>
<p>, n &ge; 3.
(9.16)
</p>
<p>The function G R is the unique solution of the equations
</p>
<p>&part;G R
</p>
<p>&part;r
=
</p>
<p>1
</p>
<p>Anrn&minus;1
, G R
</p>
<p>∣
</p>
<p>∣
</p>
<p>r=R
= 0. (9.17)
</p>
<p>Note that G R is integrable on B(0; R), despite the singularity at the origin, because
</p>
<p>the radial volume element is Anr
n&minus;1 dr by (2.10).
</p>
<p>Theorem 9.3 (Mean value formula) Assume that u &isin; C2(Ω) on a domain Ω &sub; Rn
</p>
<p>with n &ge; 2. For R &gt; 0 such that B(x0; R)&sub; Ω ,
</p>
<p>u(x0) =
1
</p>
<p>An Rn&minus;1
</p>
<p>&int;
</p>
<p>&part;B(x0;R)
</p>
<p>u(x)d S +
</p>
<p>&int;
</p>
<p>B(x0;R)
</p>
<p>G R(x &minus; x0)�u(x)d
n x.
</p>
<p>Proof By a change of variables, it suffices to consider the case x0 = 0. The formula
</p>
<p>(2.15) for the radial component of the Laplacian implies that
</p>
<p>�G R(x) = 0
</p>
<p>for x 
= 0. For ε &gt; 0, we can therefore apply Green&rsquo;s second identity (Theorem 2.11)
</p>
<p>on the domain {ε &lt; r &lt; R} to obtain
</p>
<p>&int;
</p>
<p>{ε&lt;r&lt;R}
</p>
<p>G R�u d
n x =
</p>
<p>&int;
</p>
<p>{r=R}
</p>
<p>(
</p>
<p>G R
&part;u
</p>
<p>&part;r
&minus; u
</p>
<p>&part;G R
</p>
<p>&part;r
</p>
<p>)
</p>
<p>d S
</p>
<p>&minus;
</p>
<p>&int;
</p>
<p>{r=ε}
</p>
<p>(
</p>
<p>G R
&part;u
</p>
<p>&part;r
&minus; u
</p>
<p>&part;G R
</p>
<p>&part;r
</p>
<p>)
</p>
<p>d S.
</p>
<p>(9.18)
</p>
<p>Because G R is integrable on B(0; R), on the left-hand side of (9.18) we can take
</p>
<p>ε &rarr; 0 to obtain
</p>
<p>lim
ε&rarr;0
</p>
<p>&int;
</p>
<p>{ε&lt;r&lt;R}
</p>
<p>G R�u d
n x =
</p>
<p>&int;
</p>
<p>B(0;R)
</p>
<p>G R�u d
n x. (9.19)
</p>
<p>By (9.17), the first term on the right in (9.18) reduces to
</p>
<p>&int;
</p>
<p>{r=R}
</p>
<p>(
</p>
<p>G R
&part;u
</p>
<p>&part;r
&minus; u
</p>
<p>&part;G R
</p>
<p>&part;r
</p>
<p>)
</p>
<p>d S = &minus;
1
</p>
<p>An Rn&minus;1
</p>
<p>&int;
</p>
<p>{r=R}
</p>
<p>u d S. (9.20)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>9.2 Mean Value Formula 163
</p>
<p>The second term on the right in (9.18) is
</p>
<p>&int;
</p>
<p>{r=ε}
</p>
<p>(
</p>
<p>G R
&part;u
</p>
<p>&part;r
&minus; u
</p>
<p>&part;G R
</p>
<p>&part;r
</p>
<p>)
</p>
<p>d S
</p>
<p>= G R(ε)
</p>
<p>&int;
</p>
<p>{r=ε}
</p>
<p>&part;u
</p>
<p>&part;r
d S +
</p>
<p>1
</p>
<p>Anεn&minus;1
</p>
<p>&int;
</p>
<p>{r=ε}
</p>
<p>u d S.
</p>
<p>The first of these integrals can be estimated by noting that &part;u/&part;r is a directional
</p>
<p>derivative and thus bounded by the magnitude of |&nabla;u|. By the assumption that u &isin;
</p>
<p>C2(Ω), |&part;u/&part;r | is therefore bounded by a constant C for r &le; R, yielding the estimate
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&int;
</p>
<p>{r=ε}
</p>
<p>&part;u
</p>
<p>&part;r
d S
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
&le; C Anε
</p>
<p>n&minus;1.
</p>
<p>Since the divergent term in G R(ε) as ε &rarr; 0 is proportional to ε
2&minus;n for n &ge; 3 and
</p>
<p>log ε for n = 2, this implies
</p>
<p>lim
ε&rarr;0
</p>
<p>[
</p>
<p>G R(ε)
</p>
<p>&int;
</p>
<p>{r=ε}
</p>
<p>&part;u
</p>
<p>&part;r
d S
</p>
<p>]
</p>
<p>= 0.
</p>
<p>Hence
</p>
<p>lim
ε&rarr;0
</p>
<p>&int;
</p>
<p>{r=ε}
</p>
<p>(
</p>
<p>G R
&part;u
</p>
<p>&part;r
&minus; u
</p>
<p>&part;G R
</p>
<p>&part;r
</p>
<p>)
</p>
<p>d S = lim
ε&rarr;0
</p>
<p>[
</p>
<p>1
</p>
<p>Anεn&minus;1
</p>
<p>&int;
</p>
<p>{r=ε}
</p>
<p>u d S
</p>
<p>]
</p>
<p>.
</p>
<p>The term in brackets is the average of u over a sphere of radius ε. Since u is continuous,
</p>
<p>this average approaches u(0) as ε &rarr; 0, so that
</p>
<p>lim
ε&rarr;0
</p>
<p>&int;
</p>
<p>{r=ε}
</p>
<p>(
</p>
<p>G R
&part;u
</p>
<p>&part;r
&minus; u
</p>
<p>&part;G R
</p>
<p>&part;r
</p>
<p>)
</p>
<p>d S = u(0). (9.21)
</p>
<p>Applying (9.19), (9.20), and (9.21) to (9.18) gives
</p>
<p>&int;
</p>
<p>B(0;R)
</p>
<p>G R�u d
n x = u(0)&minus;
</p>
<p>1
</p>
<p>An Rn&minus;1
</p>
<p>&int;
</p>
<p>&part;B(0;R)
</p>
<p>u d S,
</p>
<p>which completes the proof. �
</p>
<p>For harmonic functions, Theorem 9.3 gives a generalization of the circle formula
</p>
<p>(Theorem 9.3) to spherical averages in higher dimensions. As we will now show, the
</p>
<p>mean value property can be stated in a equivalent form in terms of averages over a
</p>
<p>ball.
</p>
<p>Corollary 9.4 (Mean value for harmonic functions) Suppose Ω &sub; Rn for n &ge; 2.
</p>
<p>For u &isin; C2(Ω) the following properties are equivalent:</p>
<p/>
</div>
<div class="page"><p/>
<p>164 9 Maximum Principles
</p>
<p>(A) The function u is harmonic on Ω .
</p>
<p>(B) For B(x0; R)&sub; Ω ,
</p>
<p>u(x0) =
1
</p>
<p>An Rn&minus;1
</p>
<p>&int;
</p>
<p>&part;B(x0;R)
</p>
<p>u d S.
</p>
<p>(C) For B(x0; R)&sub; Ω ,
</p>
<p>u(x0) =
n
</p>
<p>An Rn
</p>
<p>&int;
</p>
<p>B(x0;R)
</p>
<p>u dn x.
</p>
<p>Proof The fact that (A) implies (B) follows immediately by setting �u = 0 in the
</p>
<p>formula of Theorem 9.3.
</p>
<p>To see that (B) and (C) are equivalent, fix some x0 &isin; Ω and define
</p>
<p>h(r) :=
</p>
<p>&int;
</p>
<p>B(x0;r)
</p>
<p>u dn x,
</p>
<p>for r &ge; 0 such that B(x0; r)&sub; Ω . As we saw in Exercise 2.4, the derivative of h(r)
</p>
<p>is given by a surface integral
</p>
<p>h&prime;(r) =
</p>
<p>&int;
</p>
<p>&part;B(x0;r)
</p>
<p>u d S.
</p>
<p>Hence property (B) says that
</p>
<p>h&prime;(r) = Anr
n&minus;1u(x0),
</p>
<p>while property (C) says that
</p>
<p>h(r) =
Anr
</p>
<p>n
</p>
<p>n
u(x0).
</p>
<p>Since h(0) = 0 by definition, these are two statements are equivalent.
</p>
<p>Finally, we need to show that (B) implies (A). Assuming that (B) holds, Theo-
</p>
<p>rem 9.3 gives
&int;
</p>
<p>B(x0;R)
</p>
<p>G R(x &minus; x0)�u(x)d
n x = 0 (9.22)
</p>
<p>provided B(x0; R)&sub; Ω . Suppose �u(x0) &lt; 0 for some x0 &isin; Ω . Then by continuity
</p>
<p>there exists some ε &gt; 0 and δ &gt; 0 such that �u &le; &minus;ε on B(x0; δ). Since G R is
</p>
<p>strictly negative and decreasing as r &rarr; 0, this implies
</p>
<p>&int;
</p>
<p>B(x0;δ)
</p>
<p>G R(x &minus; x0)�u(x)d
n x &gt; &minus;εG R
</p>
<p>∣
</p>
<p>∣
</p>
<p>r=δ
&gt; 0,</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>9.2 Mean Value Formula 165
</p>
<p>which contradicts (9.22). The same argument applies if �u(x0) &gt; 0. We thus con-
</p>
<p>clude that (B) implies �u &equiv; 0. �
</p>
<p>9.3 Strong Principle for Subharmonic Functions
</p>
<p>A real-valued C2 function that satisfies
</p>
<p>&minus;�u &le; 0
</p>
<p>is called subharmonic. The case &minus;�u &ge; 0 is similarly called superharmonic. We
</p>
<p>will focus on the subharmonic case. The results can easily be translated to the super-
</p>
<p>harmonic case by replacing u by &minus;u.
</p>
<p>The &ldquo;weak&rdquo; maximum principle says that for a subharmonic function the max-
</p>
<p>imum value occurs at a boundary point. We will prove here a &ldquo;strong&rdquo; version of
</p>
<p>this principle, which says furthermore that if that the global maximum occurs at an
</p>
<p>interior point then the function is constant.
</p>
<p>Theorem 9.5 (Strong maximum principle) Let Ω &sub; Rn be a bounded domain. If
</p>
<p>u &isin; C2(Ω;R) &cap; C0(Ω) is subharmonic then
</p>
<p>max
Ω
</p>
<p>u = max
&part;Ω
</p>
<p>u.
</p>
<p>The maximum is attained at an interior point only if u is a constant function.
</p>
<p>Proof By the extreme value theorem (Theorem A.2), u achieves a global maximum
</p>
<p>at some point x0 &isin;Ω. If x0 &isin; &part;Ω then the claimed equality clearly holds. The goal
</p>
<p>is thus to show that x0 &isin; Ω implies that u is constant.
</p>
<p>Because Ω is open, an interior point x0 has a neighborhood contained in Ω . We
</p>
<p>may thus assume that B(x0; R)&sub; Ω for some R &gt; 0. Applying Theorem 9.3 to this
</p>
<p>ball gives
</p>
<p>u(x0) =
1
</p>
<p>An Rn&minus;1
</p>
<p>&int;
</p>
<p>&part;B(x0;R)
</p>
<p>u(x)d S +
</p>
<p>&int;
</p>
<p>B(x0;R)
</p>
<p>G R(x &minus; x0)�u(x)d
n x.
</p>
<p>By the definition (9.16), G R &le; 0 for 0 &lt; r &le; R. Therefore, since �u &ge; 0 by
</p>
<p>assumption,
</p>
<p>u(x0) &le;
1
</p>
<p>An Rn&minus;1
</p>
<p>&int;
</p>
<p>&part;B(x0;R)
</p>
<p>u(x)d S. (9.23)
</p>
<p>Using (9.14), we can subtract u(x0) from both sides to obtain
</p>
<p>1
</p>
<p>An Rn&minus;1
</p>
<p>&int;
</p>
<p>&part;B(x0;R)
</p>
<p>[u(x)&minus; u(x0)] d S &ge; 0. (9.24)</p>
<p/>
</div>
<div class="page"><p/>
<p>166 9 Maximum Principles
</p>
<p>By assumption u(x0) is the global maximum of u, implying that the integrand of
</p>
<p>(9.24) is nonpositive. The inequality therefore shows that the integrand vanishes, and
</p>
<p>we conclude that u(x) = u(x0) on &part;B(x0; R).
</p>
<p>Note that the same argument works for every radius r &lt; R, so this argument
</p>
<p>shows that u &equiv; u(x0) on all of B(x0; R).
</p>
<p>To extend the conclusion to the full domain, let M denote the maximum value of
</p>
<p>u onΩ. We can write Ω as a disjoint union E &cup; F , where
</p>
<p>E := {x &isin; Ω; u(x) &lt; M} ,
</p>
<p>F := {x &isin; Ω; u(x) = M} .
</p>
<p>By the argument given above, a point x &isin; F has a neighborhood B(x; R) &sub; Ω on
</p>
<p>which u is equal to M . Hence F is open.
</p>
<p>On the other hand, for x &isin; E we can set ε = M &minus; u(x) and use the continuity of
</p>
<p>u to find a δ &gt; 0 such that
</p>
<p>|u(x)&minus; u( y)| &lt; ε
</p>
<p>for y &isin; B(x; δ). This implies in particular that u( y) &lt; M , so that B(x; δ) &isin; E .
</p>
<p>Thus E is open also.
</p>
<p>Recall from Sect. 2.3 that the fact that Ω is connected means that the domain
</p>
<p>cannot be written as a disjoint union of nonempty open sets. Since Ω = E &cup; F with
</p>
<p>E and F both open, one of the two sets is empty. If E is empty then u is constant on
</p>
<p>Ω , while if F is empty then the maximum of u is not attained in the interior. �
</p>
<p>For a superharmonic function u &isin; C2(Ω;R) &cap; C0(Ω), reversing the sign yields
</p>
<p>a minimum principle,
</p>
<p>min
Ω
</p>
<p>u = min
&part;Ω
</p>
<p>u.
</p>
<p>Both principles apply to a harmonic function u, which therefore satisfies
</p>
<p>min
&part;Ω
</p>
<p>u &le; u(x) &le; max
&part;Ω
</p>
<p>u
</p>
<p>for all x &isin; Ω .
</p>
<p>The maximum principle implies the following stability result for the Laplace
</p>
<p>equation.
</p>
<p>Corollary 9.6 Suppose that u1, u2 &isin; C
2(Ω) &cap; C0(Ω) are solutions of the Laplace
</p>
<p>equation �u = 0 with boundary values
</p>
<p>u1|&part;Ω = g1, u2|&part;Ω = g2,
</p>
<p>for g1, g2 &isin; C
0(&part;Ω). Then
</p>
<p>max
Ω
</p>
<p>|u2 &minus; u1| &le; max
&part;Ω
</p>
<p>|g2 &minus; g1| . (9.25)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>9.3 Strong Principle for Subharmonic Functions 167
</p>
<p>In particular, a solution the Laplace equation is uniquely determined by its boundary
</p>
<p>data.
</p>
<p>Proof By superposition, u2 &minus;u1 is a harmonic function with boundary data g2 &minus; g1.
</p>
<p>Theorem 9.5 applies to &plusmn;Re(u2 &minus; u1) as well as &plusmn; Im(u2 &minus; u1). Combining these
</p>
<p>estimates yields the inequality (9.25). �
</p>
<p>Note that uniqueness of solutions of the Laplace equation also follows directly
</p>
<p>from Green&rsquo;s first identity (Theorem 2.10), in the case where Ω has piecewise C1
</p>
<p>boundary and u &isin; C2(Ω;R). If �u = 0, then setting v = u in Green&rsquo;s formula
</p>
<p>gives
&int;
</p>
<p>Ω
</p>
<p>‖&nabla;u‖2 dn x =
</p>
<p>&int;
</p>
<p>&part;Ω
</p>
<p>u
&part;u
</p>
<p>&part;ν
d S.
</p>
<p>Thus if u = 0 on &part;Ω , then
&int;
</p>
<p>Ω
</p>
<p>‖&nabla;u‖2 dn x = 0.
</p>
<p>Since the integrand is positive, this implies &nabla;u &equiv; 0, so that u is constant. The
</p>
<p>assumption u|&part;Ω = 0 then gives u &equiv; 0 on the full domain. (This is the &ldquo;energy
</p>
<p>method&rdquo; argument, as introduced in Sect. 4.7.)
</p>
<p>One advantage that maximum principle has over the energy method is the explicit
</p>
<p>stability formula (9.25). In terms of the L&infin; norm introduced in Sect. 7.3, this inequal-
</p>
<p>ity could be written
</p>
<p>‖u2 &minus; u1‖&infin; &le; ‖g2 &minus; g1‖&infin; .
</p>
<p>This is an explicit formulation of the continuity requirement for well-posedness:
</p>
<p>a small change in boundary data results in a correspondingly small change in the
</p>
<p>solution.
</p>
<p>9.4 Weak Principle for Elliptic Equations
</p>
<p>Although the mean value formula gives a direct proof of the strong maximum princi-
</p>
<p>ple, this approach applies only to the Laplacian itself. In this section we will present
</p>
<p>an alternative approach that generalizes quite easily to operators with variable coef-
</p>
<p>ficients.
</p>
<p>On a domain Ω &sub; Rn let us consider a second order elliptic operator of the form
</p>
<p>L = &minus;
</p>
<p>n
&sum;
</p>
<p>i, j=1
</p>
<p>ai j (x)
&part;2
</p>
<p>&part;xi&part;x j
+
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>b j (x)
&part;
</p>
<p>&part;x j
, (9.26)
</p>
<p>where the coefficients ai j and b j are continuous functions on Ω . As defined in Sect.
</p>
<p>1.3, ellipticity means that the symmetric matrix [ai j ] is positive definite at each point.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_4">http://dx.doi.org/10.1007/978-3-319-48936-0_4</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_1">http://dx.doi.org/10.1007/978-3-319-48936-0_1</a></div>
</div>
<div class="page"><p/>
<p>168 9 Maximum Principles
</p>
<p>For the maximum principle we need a stronger assumption, called uniform ellip-
</p>
<p>ticity, that says that for some fixed constant κ &gt; 0,
</p>
<p>n
&sum;
</p>
<p>i, j=1
</p>
<p>ai j (x)viv j &ge; κ ‖v‖
2 (9.27)
</p>
<p>for all x &isin; Ω and v &isin; Rn . An equivalent way to say this is that the smallest eigenvalue
</p>
<p>of [ai j ] is bounded below by κ at each point x.
</p>
<p>Theorem 9.7 (Weak maximum principle) Suppose Ω &sub; Rn is bounded, and L is
</p>
<p>an operator of the form (9.26) satisfying the uniform ellipticity condition (9.27). If
</p>
<p>u &isin; C2(Ω;R) &cap; C0(Ω) satisfies
</p>
<p>Lu &le; 0
</p>
<p>in Ω , then
</p>
<p>max
Ω
</p>
<p>u = max
&part;Ω
</p>
<p>u.
</p>
<p>Proof For the moment let u be a general function in C2(Ω;R). Suppose that u has
</p>
<p>a local maximum at x0 &isin; Ω . The first partial derivatives of u vanish at a local
</p>
<p>maximum, so that
</p>
<p>Lu(x0) = &minus;
</p>
<p>n
&sum;
</p>
<p>i, j=1
</p>
<p>ai j (x0)
&part;2u
</p>
<p>&part;xi&part;x j
(x0). (9.28)
</p>
<p>Furthermore, we claim that the matrix of second partials of u is negative definite at
</p>
<p>x0, meaning
n
</p>
<p>&sum;
</p>
<p>i, j=1
</p>
<p>&part;2u
</p>
<p>&part;xi&part;x j
(x0)viv j &le; 0
</p>
<p>for v &isin; Rn . To see this, set h(t) := u(x0 + tv) and note that h has a local maximum
</p>
<p>at t = 0, implying h&prime;&prime;(0) &le; 0. Evaluating h&prime;&prime;(0) yields the inequality stated above.
</p>
<p>The right-hand side of (9.28) could be written as tr(AB) where A and B are the
</p>
<p>positive symmetric matrices
</p>
<p>A =
[
</p>
<p>ai j (x0)
]
</p>
<p>, B =
</p>
<p>[
</p>
<p>&minus;
&part;2u
</p>
<p>&part;xi&part;x j
(x0)
</p>
<p>]
</p>
<p>.
</p>
<p>By switching to a basis in which A is diagonal, tr(AB) can be written in terms of
</p>
<p>the eigenvalues
{
</p>
<p>λ j
}
</p>
<p>of A as
</p>
<p>tr(AB) =
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>λ j b j j .</p>
<p/>
</div>
<div class="page"><p/>
<p>9.4 Weak Principle for Elliptic Equations 169
</p>
<p>If the eigenvalues are ordered λ1 &le; &middot; &middot; &middot; &le; λn , then
</p>
<p>tr(AB) &ge; λ1 tr B.
</p>
<p>The positivity of B implies tr B &ge; 0, so we conclude that
</p>
<p>Lu(x0) &ge; 0.
</p>
<p>Thus argument shows that Lu(x0) &ge; 0 for x0 a local interior maximum. Therefore,
</p>
<p>the strict inequality Lu &lt; 0 implies that u cannot have a local interior maximum and
</p>
<p>that
</p>
<p>max
Ω
</p>
<p>u = max
&part;Ω
</p>
<p>u. (9.29)
</p>
<p>To complete the proof, we must relax the hypothesis to Lu &le; 0. The strategy is
</p>
<p>to perturb u slightly to reduce to the previous case. For M &gt; 0, let
</p>
<p>h(x) := eMx1 .
</p>
<p>By the definition of L ,
</p>
<p>Lh =
[
</p>
<p>&minus;a11 M
2 + b1 M
</p>
<p>]
</p>
<p>h.
</p>
<p>The ellipticity condition (9.27) implies that a11 &ge; κ, so by choosing
</p>
<p>M &gt;
1
</p>
<p>κ
max
Ω
</p>
<p>b1,
</p>
<p>we can guarantee that
</p>
<p>Lh &lt; 0.
</p>
<p>If we now assume now that u satisfies the hypothesis Lu &le; 0, then
</p>
<p>L(u + εh) &lt; 0
</p>
<p>for ε &gt; 0. Applying (9.29) to u + εh gives
</p>
<p>max
Ω
</p>
<p>(u + εh) = max
&part;Ω
</p>
<p>(u + εh). (9.30)
</p>
<p>Since h &ge; 0, clearly
</p>
<p>max
Ω
</p>
<p>u &le; max
Ω
</p>
<p>(u + εh).
</p>
<p>On the other hand, since Ω is bounded, we may assume x1 &lt; R inΩ, for some R
</p>
<p>sufficiently large. This implies
</p>
<p>h &le; eM R,</p>
<p/>
</div>
<div class="page"><p/>
<p>170 9 Maximum Principles
</p>
<p>so that
</p>
<p>max
&part;Ω
</p>
<p>(u + εh) &le; max
&part;Ω
</p>
<p>u + εeM R .
</p>
<p>From (9.30) we therefore conclude that
</p>
<p>max
Ω
</p>
<p>u &le; max
&part;Ω
</p>
<p>u + εeM R
</p>
<p>for all ε &gt; 0. Since M and R are independent of ε, we can take ε &rarr; 0 to conclude
</p>
<p>that
</p>
<p>max
Ω
</p>
<p>u &le; max
&part;Ω
</p>
<p>u,
</p>
<p>and the result follows. �
</p>
<p>The maximum principle implies in particular the only solution of Lu = 0 with
</p>
<p>u|&part;Ω = 0 is u &equiv; 0. Hence a solution of the equation
</p>
<p>Lu = f, u|&part;Ω = g,
</p>
<p>is uniquely determined by f and g if it exists.
</p>
<p>9.5 Application to the Heat Equation
</p>
<p>Fourier&rsquo;s law of heat conduction, as introduced in Sect. 6.1, suggests a maximum
</p>
<p>principle for solutions of the heat equation. Because heat flows away from a spatial
</p>
<p>maximum of the temperature, a local spatial maximum of the temperature should be
</p>
<p>impossible at time t &gt; 0. The global maximum of the temperature therefore must
</p>
<p>occur either at t = 0 or on the boundary.
</p>
<p>Although it is possible to prove the maximum principle via a mean value formula
</p>
<p>as in the proof of Theorem 9.5, in this section we will follow the more direct approach
</p>
<p>from Sect. 9.4, which has the advantage of generalizing to operators with variable
</p>
<p>coefficients.
</p>
<p>Because the heat equation is second order with respect to spatial variables and first
</p>
<p>order in the time variable, it makes sense to define a domain for classical solutions
</p>
<p>that takes this structure into account. For Ω &sub; Rn , define
</p>
<p>Cheat(Ω) :=
{
</p>
<p>u &isin; C0
(
</p>
<p>[0,&infin;)&times;Ω;R
)
</p>
<p>; u(&middot;, x) &isin; C1(0,&infin;), u(t, &middot;) &isin; C2(Ω)
}
</p>
<p>.
</p>
<p>Note that this definition includes only real-valued functions.
</p>
<p>Theorem 9.8 Suppose Ω &sub; Rn is a bounded domain and u &isin; Cheat(Ω) satisfies
</p>
<p>&part;u
</p>
<p>&part;t
&minus;�u &le; 0, (9.31)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_6">http://dx.doi.org/10.1007/978-3-319-48936-0_6</a></div>
</div>
<div class="page"><p/>
<p>9.5 Application to the Heat Equation 171
</p>
<p>on (0, T ) &times; Ω . Then the maximum value of u within [0, T ] &times;Ω occurs at a point
</p>
<p>(t0, x0) with either t0 = 0 or x0 &isin; &part;Ω .
</p>
<p>Proof Suppose that u attains a maximum at (t0, x0) &sub; (0, T ) &times; Ω . By the same
</p>
<p>calculus argument used in Sect. 9.4, this implies
</p>
<p>&part;u
</p>
<p>&part;t
(t0, x0) = 0, (9.32)
</p>
<p>as well as
&part;u
</p>
<p>&part;x j
(t0, x0) = 0,
</p>
<p>&part;2u
</p>
<p>&part;x2j
(t0, x0) &le; 0. (9.33)
</p>
<p>In particular,
[
</p>
<p>&part;u
</p>
<p>&part;t
&minus;�u
</p>
<p>]
</p>
<p>(t0, x0) &ge; 0. (9.34)
</p>
<p>If (9.31) were a strict inequality this would complete the proof.
</p>
<p>To proceed we use a perturbation strategy as in the proof of Theorem 9.7. For
</p>
<p>ε &gt; 0, set
</p>
<p>uε := u + ε |x|
2 .
</p>
<p>Because � |x|2 = 2n, the hypothesis on u gives
</p>
<p>&part;uε
</p>
<p>&part;t
&minus;�uε =
</p>
<p>&part;u
</p>
<p>&part;t
&minus;�u &minus; 2nε &lt; 0. (9.35)
</p>
<p>The existence of a local maximum for uε within (0, T ) &times;Ω is ruled out by (9.34).
</p>
<p>We conclude that uε attains a global maximum at a boundary point of [0, T ] &times;Ω.
</p>
<p>Let us label this point (tε, xε), so that
</p>
<p>max
[0,T ]&times;Ω
</p>
<p>uε = uε(tε, xε). (9.36)
</p>
<p>Since (tε, xε) is on the boundary, either tε = 0, tε = T , or xε &isin; &part;Ω .
</p>
<p>Suppose that tε = T and xε &isin; Ω . Then uε(t, xε) &le; uε(T, xε) for t &isin; [0, T ],
</p>
<p>implying that
&part;uε
</p>
<p>&part;t
(T, xε) &ge; 0.
</p>
<p>By (9.35), this implies also that �uε(T, xε) &gt; 0, which is ruled out by (9.33). Hence
</p>
<p>tε 
= T if xε &isin; Ω .
</p>
<p>Therefore (tε, xε) lies in the set
</p>
<p>Γ := ({0} &times;Ω) &cap; ([0, T ] &times; &part;Ω) . (9.37)</p>
<p/>
</div>
<div class="page"><p/>
<p>172 9 Maximum Principles
</p>
<p>Let R be sufficiently large so that Ω &sub; B(0; R). This means that |x| &le; R onΩ, so
</p>
<p>the inequality
</p>
<p>u &le; uε &le; u + εR
2
</p>
<p>holds at every point in [0, T ] &times;Ω. From (9.36) we can thus conclude that
</p>
<p>max
[0,T ]&times;Ω
</p>
<p>u &le; uε(tε, xε)
</p>
<p>&le; u(tε, xε)+ εR
2.
</p>
<p>(9.38)
</p>
<p>This implies that
</p>
<p>max
[0,T ]&times;Ω
</p>
<p>u &le; max
Γ
</p>
<p>u + εR2,
</p>
<p>because (tε, xε) &isin; Γ . Since this inequality holds for every ε &gt; 0, this proves
</p>
<p>max
[0,T ]&times;Ω
</p>
<p>u &le; max
Γ
</p>
<p>u.
</p>
<p>�
</p>
<p>For a solution of the heat equation, both&plusmn;u satisfy the hypothesis of Theorem 9.8,
</p>
<p>which implies that
</p>
<p>min
Γ
</p>
<p>u &le; u(t, x) &le; max
Γ
</p>
<p>u,
</p>
<p>for (t, x) &isin; (0, T ) &times;Ω , where Γ is defined by (9.37). In particular this yields the
</p>
<p>following:
</p>
<p>Corollary 9.9 Let Ω &isin; Rn be a bounded domain. A solution of the heat equation
</p>
<p>u &isin; Cheat(Ω) is uniquely determined by u|&part;Ω and u|t=0.
</p>
<p>The same arguments could be applied to the more general parabolic equation
</p>
<p>&part;u
</p>
<p>&part;t
&minus; Lu = 0,
</p>
<p>where L is a uniformly elliptic operator as defined in (9.27).
</p>
<p>In Sect. 6.3, we stated without proof a uniqueness result for solutions of the heat
</p>
<p>equation on Rn . We now have the means to prove this, by establishing a maximum
</p>
<p>principle for Rn as a corollary of Theorem 9.8.
</p>
<p>Corollary 9.10 Suppose that u is a classical solution of the heat equation
</p>
<p>&part;u
</p>
<p>&part;t
&minus;�u = 0, u|t=0 = g, (9.39)
</p>
<p>on [0,&infin;)&times; Rn , and that u is bounded on [0, T ] &times; Rn for T &gt; 0. Then</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_6">http://dx.doi.org/10.1007/978-3-319-48936-0_6</a></div>
</div>
<div class="page"><p/>
<p>9.5 Application to the Heat Equation 173
</p>
<p>max
[0,&infin;)&times;Rn
</p>
<p>u &le; max
Rn
</p>
<p>g. (9.40)
</p>
<p>Proof Assume that u satisfies (9.39) and also
</p>
<p>u(t, x) &le; M
</p>
<p>for t &isin; [0, T ] and x &isin; Rn . For y &isin; Rn and ε &gt; 0 set
</p>
<p>v(t, x) := u(t, x)&minus; ε(T &minus; t)&minus;
n
2 e
</p>
<p>|x&minus; y|2
</p>
<p>4(T&minus;t) .
</p>
<p>The ε term resembles the heat kernel defined by (6.16), except that the sign in the
</p>
<p>exponential is reversed. Direct differentiation shows that this expression satisfies the
</p>
<p>heat equation on (0, T )&times; Rn , and hence v does also.
</p>
<p>For R &gt; 0, let us apply the maximum principle of Theorem 9.8 to v on the domain
</p>
<p>(0, T )&times; B( y; R). By construction,
</p>
<p>v(0, x) &le; g(x),
</p>
<p>and for x &isin; &part;B( y; R),
</p>
<p>v(t, x) &le; M &minus; ε(T &minus; t)&minus;
n
2 e
</p>
<p>R2
</p>
<p>4(T&minus;t)
</p>
<p>&le; M &minus; εT &minus;
n
2 eR
</p>
<p>2/4T .
</p>
<p>With T fixed, the right-hand side of this second inequality is arbitrarily negative for
</p>
<p>large R. Therefore, for sufficiently large R, Theorem 9.8 implies that
</p>
<p>max
[0,T ]&times;B( y;R)
</p>
<p>v &le; max
B( y;R)
</p>
<p>g.
</p>
<p>In particular, setting x = y in this inequality gives
</p>
<p>v(t, y) &le; max
Rn
</p>
<p>g.
</p>
<p>for t &isin; [0, T ] and y &isin; Rn . By the definition of v, this implies that
</p>
<p>u(t, y) &le; max
Rn
</p>
<p>g + ε(T &minus; t)&minus;
n
2 .
</p>
<p>We can now take ε &rarr; 0 and T &rarr; &infin; to conclude that
</p>
<p>u(t, y) &le; max
Rn
</p>
<p>g
</p>
<p>for all t &isin; [0,&infin;) and y &isin; Rn . �</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_6">http://dx.doi.org/10.1007/978-3-319-48936-0_6</a></div>
</div>
<div class="page"><p/>
<p>174 9 Maximum Principles
</p>
<p>The argument given here can be refined to show that conclusion (9.40) holds under
</p>
<p>the weaker growth condition
</p>
<p>u(t, x) &le; Mec|x|
2
</p>
<p>for t &isin; [0, T ].
</p>
<p>Corollary 9.10 implies Theorem 6.3 by the argument used in Corollary 9.9. That
</p>
<p>is, if u1 and u2 are bounded solutions of (6.19), then &plusmn;(u1 &minus; u2) solves (6.19) with
</p>
<p>g = 0. It then follows from (9.40) that u1 = u2.
</p>
<p>9.6 Exercises
</p>
<p>9.1 Suppose that u,φ &isin; C2(Ω;R)&cap;C0(Ω) on a bounded domainΩ &sub; Rn . Assume
</p>
<p>that u subharmonic and φ harmonic, with matching boundary values:
</p>
<p>u|&part;Ω = φ|&part;Ω .
</p>
<p>Show that
</p>
<p>u &le; φ
</p>
<p>at all points of Ω . (This is the motivation for the term &ldquo;subharmonic&rdquo;.)
</p>
<p>9.2 Liouville&rsquo;s theorem says that a bounded harmonic function on Rn is constant.
</p>
<p>To show this, assume u &isin; C2(Rn) is harmonic and satisfies
</p>
<p>|u(x)| &le; M
</p>
<p>for all x &isin; Rn .
</p>
<p>(a) For x0 &isin; R
n , set r0 = |x0|. Use Corollary 9.4 at the centers 0 and x0 to show
</p>
<p>that
</p>
<p>u(0)&minus; u(x0) =
n
</p>
<p>An Rn
</p>
<p>[&int;
</p>
<p>B(0;R)
</p>
<p>u dn x &minus;
</p>
<p>&int;
</p>
<p>B(x0;R)
</p>
<p>u dn x
</p>
<p>]
</p>
<p>(9.41)
</p>
<p>for R &gt; 0. Note that the integrals cancel on the intersection of the two balls.
</p>
<p>(b) Show that
</p>
<p>vol [B(0; R)\B(x0; R)] &le; vol
[
</p>
<p>B(0; R)\B( x0
2
; R &minus; r0
</p>
<p>2
)
]
</p>
<p>=
An
</p>
<p>n
</p>
<p>[
</p>
<p>Rn &minus;
(
</p>
<p>R &minus; r0
2
</p>
<p>)n]
</p>
<p>,
</p>
<p>and the same for B(x0; R)\B(0; R).</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_6">http://dx.doi.org/10.1007/978-3-319-48936-0_6</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_6">http://dx.doi.org/10.1007/978-3-319-48936-0_6</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_6">http://dx.doi.org/10.1007/978-3-319-48936-0_6</a></div>
</div>
<div class="page"><p/>
<p>9.6 Exercises 175
</p>
<p>(c) Apply the volume estimates and the fact that |u| &le; M to (9.41) to estimate
</p>
<p>|u(0)&minus; u(x0)| &le; 2M
</p>
<p>[
</p>
<p>Rn &minus; (R &minus; r0
2
)n
</p>
<p>Rn
</p>
<p>]
</p>
<p>.
</p>
<p>Take the limit R &rarr; &infin; to show that u(x0) = u(0).
</p>
<p>9.3 Suppose that Ω &sub; Rn is bounded, with Ω &sub; B(0; R), and assume that u &isin;
</p>
<p>C2(Ω;R) &cap; C0(Ω) satisfies
</p>
<p>&minus;�u = f, u|&part;Ω = 0,
</p>
<p>for f &isin; C0(Ω).
</p>
<p>(a) Find a constant c &gt; 0 (depending on f and R), such that u+c |x|2 is subharmonic
</p>
<p>on Ω .
</p>
<p>(b) For this value of c, apply the maximum principle to u + c |x|2 to deduce that
</p>
<p>max
Ω
</p>
<p>|u| &le; C max
Ω
</p>
<p>| f | ,
</p>
<p>where C depends only on R.
</p>
<p>9.4 Suppose u is a harmonic function on a domain that includes B(0; 4R) for some
</p>
<p>R &gt; 0, and assume u &ge; 0. Show that
</p>
<p>max
B(0;R)
</p>
<p>u &le; 3n min
B(0;R)
</p>
<p>u.
</p>
<p>Hint: For x &isin; B(0; R), apply the maximum principle to write u(x) as an integral
</p>
<p>over the balls B(x; R) and B(x; 3R). Then show that
</p>
<p>B(x; R) &sub; B(0; 2R) &sub; B(x; 3R),
</p>
<p>and use this to estimate the integrals.
</p>
<p>9.5 Suppose u &isin; C2(B; R) &cap; C0(B) is a nonconstant subharmonic function and
</p>
<p>assume that the maximum of u on B is attained at the point x0 &isin; &part;B. This automat-
</p>
<p>ically implies that &part;u
&part;r
(x0) &ge; 0. Hopf&rsquo;s lemma says that this inequality is strict,
</p>
<p>&part;u
</p>
<p>&part;r
(x0) &gt; 0.
</p>
<p>To show this, let B := B(0; R) &sub; Rn for some R &gt; 0, and set
</p>
<p>A := {R/2 &lt; |x| &lt; R} .</p>
<p/>
</div>
<div class="page"><p/>
<p>176 9 Maximum Principles
</p>
<p>(a) Consider the function
</p>
<p>h(x) := e&minus;2n|x|
2/R2 &minus; e&minus;2n .
</p>
<p>Compute �h and show that h is subharmonic on A.
</p>
<p>(b) Set
</p>
<p>m = max
{r=R/2}
</p>
<p>u, M = max
{r=R}
</p>
<p>u,
</p>
<p>and show that m &lt; M .
</p>
<p>(c) For ε &gt; 0 set
</p>
<p>uε := u + εh,
</p>
<p>and show that by taking ε sufficiently small we may assume that
</p>
<p>max
&part; A
</p>
<p>uε &le; M.
</p>
<p>(d) Show that uε(x) &le; M for x &isin; A, and hence that
</p>
<p>&part;uε
</p>
<p>&part;r
(x0) &ge; 0.
</p>
<p>(e) By computing &part;uε/&part;r and taking ε &rarr; 0, conclude that
</p>
<p>&part;u
</p>
<p>&part;r
(x0) &gt; 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 10
</p>
<p>Weak Solutions
</p>
<p>In Sect. 1.2we observed that d&rsquo;Alembert&rsquo;s formula for a solution of thewave equation
</p>
<p>makes sense even when the initial data are not differentiable. This concept of a weak
</p>
<p>solution that is not actually required to solve the equation literally has comeup in other
</p>
<p>contexts as well, for example in the discussion of the traffic equation in Sect. 3.4. In
</p>
<p>this chapter we will discuss the mathematical formulation of this generalized notion
</p>
<p>of solution.
</p>
<p>Weak solutions first appeared in physical applications as idealized, limiting cases
</p>
<p>of true solutions. For example, one might replace a smooth density function by a
</p>
<p>simpler piecewise linear approximation, as illustrated in Fig. 10.1, in order to simplify
</p>
<p>computations. (We used this idea in Example3.9.)
</p>
<p>Up until the late 19th century, the limiting process by which weak solutions were
</p>
<p>obtained was understood rather loosely, and justified mainly by physical intuition.
</p>
<p>Weak solutions proved to be extremely useful, and eventually a consistent mathe-
</p>
<p>matical framework was developed.
</p>
<p>10.1 Test Functions and Weak Derivatives
</p>
<p>Consider a linear equation of the form Lu = f , where L is a differential operator
on a domainΩ &sub; Rn . Suppose that u represents a physical quantity such as temper-
ature or density. Direct observation of such quantities at a single point is a practical
</p>
<p>impossibility. Even the most sensitive instrument will only be able to measure the
</p>
<p>weighted average over some small region.
</p>
<p>To formalize this notion of a local average, we use the concept of a test function
</p>
<p>ψ &isin; C&infin;cpt(Ω). The test function defines a local measurement of a quantity u through
the integral &int;
</p>
<p>Ω
</p>
<p>uψ dn x. (10.1)
</p>
<p>&copy; Springer International Publishing AG 2016
</p>
<p>D. Borthwick, Introduction to Partial Differential Equations,
</p>
<p>Universitext, DOI 10.1007/978-3-319-48936-0_10
</p>
<p>177</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_1">http://dx.doi.org/10.1007/978-3-319-48936-0_1</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_3">http://dx.doi.org/10.1007/978-3-319-48936-0_3</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_3">http://dx.doi.org/10.1007/978-3-319-48936-0_3</a></div>
</div>
<div class="page"><p/>
<p>178 10 Weak Solutions
</p>
<p>Fig. 10.1 A smooth function and its piecewise linear approximation
</p>
<p>The function ψ plays the role of a experimental probe that takes a particular sample
</p>
<p>of the values of u.
</p>
<p>Let us consider how we would &ldquo;detect&rdquo; a derivative using test functions. Suppose
</p>
<p>for themoment that u &isin; C1(R), with u&prime; = f . If wemeasure this derivative associated
using the test function ψ &isin; C&infin;cpt(R), the result is
</p>
<p>&int;
</p>
<p>R
</p>
<p>u&prime;ψ dx =
&int;
</p>
<p>R
</p>
<p>f ψ dx . (10.2)
</p>
<p>The fact that u&prime; = f is equivalent to the statement that (10.2) holds for all ψ &isin;
C&infin;cpt(R).
</p>
<p>Note that the left-hand side could be integrated by parts, since ψ has compact
</p>
<p>support, yielding
</p>
<p>&minus;
&int;
</p>
<p>R
</p>
<p>uψ&prime; dx =
&int;
</p>
<p>R
</p>
<p>f ψ dx . (10.3)
</p>
<p>This condition now makes sense even when u fails to be differentiable. The only
</p>
<p>requirement is that u and f be integrable on compact sets, a property we refer to as
</p>
<p>local integrability. We can say that locally integrable functions satisfy u&prime; = f in the
weak sense provided (10.3) holds for all ψ &isin; C&infin;cpt(R).
</p>
<p>To generalize this definition to a domainΩ &sub; Rn , let us define the space of locally
integrable functions,
</p>
<p>L1loc(Ω) :=
{
</p>
<p>f : Ω &rarr; C; f |K &isin; L1(K ) for all compact K &sub; Ω
}
.
</p>
<p>The same equivalence relation (7.6) used for L p spaces applies to L1loc, i.e., functions
</p>
<p>that differ on a set of measure zero are considered to be the same.
</p>
<p>Inspired by (10.3), for u and f &isin; L1loc(Ω) we say that
</p>
<p>&part;u
</p>
<p>&part;x j
= f
</p>
<p>as a weak derivative if &int;
</p>
<p>Ω
</p>
<p>u
&part;ψ
</p>
<p>&part;x j
dx = &minus;
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>f ψ dx (10.4)
</p>
<p>for all ψ &isin; C&infin;cpt(R). The condition (10.4) determines f uniquely as an element of
L1loc(Ω), by the following:</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
</div>
<div class="page"><p/>
<p>10.1 Test Functions and Weak Derivatives 179
</p>
<p>Lemma 10.1 If f &isin; L1loc(Ω) satisfies
&int;
</p>
<p>Ω
</p>
<p>f ψ dn x = 0
</p>
<p>for all ψ &isin; C&infin;cpt(Ω), then f &equiv; 0.
</p>
<p>Proof It suffices to consider the case when Ω is bounded, since a larger domain
</p>
<p>could be subdivided into bounded pieces. For boundedΩ the local integrability of f
</p>
<p>implies that f &isin; L2(Ω). By Theorem7.5 we can choose a sequence ψk in C&infin;cpt(Ω)
such that ψk &rarr; f in L2(Ω). This implies that
</p>
<p>lim
k&rarr;&infin;
</p>
<p>〈 f,ψk〉 = ‖ f ‖2.
</p>
<p>The inner products 〈 f,ψk〉 are zero by hypothesis, so we conclude that f &equiv; 0. ⊓⊔
</p>
<p>Example 10.2 In R, consider the piecewise linear function
</p>
<p>g(x) :=
</p>
<p>⎧
⎪⎨
⎪⎩
</p>
<p>0, x &lt; 0,
</p>
<p>x, 0 &le; x &le; 1,
1, x &gt; 0.
</p>
<p>If we ignore the points where g is not differentiable, then we would expect that the
</p>
<p>derivative of g is given by
</p>
<p>f (x) =
{
1, 0 &le; x &le; 1,
0, |x | &gt; 1.
</p>
<p>These functions are illustrated in Fig. 10.2.
</p>
<p>Let us check that this works in the sense of weak derivatives.We takeψ &isin; C&infin;cpt(R)
and compute &int; &infin;
</p>
<p>&minus;&infin;
gψ&prime; dx =
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>xψ&prime;(x) dx +
&int; &infin;
</p>
<p>1
</p>
<p>ψ&prime; dx .
</p>
<p>g(x) g&prime;(x)
</p>
<p>Fig. 10.2 A piecewise linear function and its weak derivative</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
</div>
<div class="page"><p/>
<p>180 10 Weak Solutions
</p>
<p>Using integration by parts on the first term and evaluating the second gives
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
gψ&prime; dx = ψ(1)&minus;
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>ψ dx &minus; ψ(1)
</p>
<p>= &minus;
&int; &infin;
</p>
<p>&infin;
f ψ dx .
</p>
<p>This verifies that g&prime; = f in the weak sense. &diams;
</p>
<p>Example 10.3 For t &isin; R, define w &isin; L1loc(Ω) by
</p>
<p>w(t) =
{
w&minus;(t), t &lt; 0
</p>
<p>w+(t), t &ge; 0,
(10.5)
</p>
<p>where w&plusmn; &isin; C1(R). For ψ &isin; C&infin;cpt(R),
</p>
<p>&minus;
&int; &infin;
</p>
<p>&minus;&infin;
wψ&prime; dt = &minus;
</p>
<p>&int; 0
</p>
<p>&minus;&infin;
w&minus;ψ
</p>
<p>&prime; dt &minus;
&int; &infin;
</p>
<p>0
</p>
<p>w+ψ
&prime; dt
</p>
<p>= [w+(0)&minus; w&minus;(0)]ψ(0)+
&int; 0
</p>
<p>&minus;&infin;
w&prime;&minus;ψ dt +
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>w&prime;+ψ dt.
</p>
<p>The term proportional toψ(0) could not possibly come from the integral ofψ against
</p>
<p>a locally integrable function, because the value of the integrand at a single point does
</p>
<p>not affect the integral. Hence w admits a weak derivative only under the matching
</p>
<p>condition
</p>
<p>w&minus;(0) = w+(0).
</p>
<p>If this is satisfied, then the derivative is
</p>
<p>w&prime;(t) =
{
w&prime;&minus;(t), t &lt; 0,
</p>
<p>w&prime;+(t), t &gt; 0.
</p>
<p>&diams;
</p>
<p>Weak derivatives of higher order are defined by an extension of (10.4). Towrite the
</p>
<p>corresponding formulas, it is helpful to have a simplified notation for higher partials.
</p>
<p>For each multi-index α = (α1, . . . ,αn) with α j &isin; N0, we define the differential
operator on Rn ,
</p>
<p>Dα :=
&part;α1
</p>
<p>&part;x
α1
1
</p>
<p>&middot; &middot; &middot;
&part;αn
</p>
<p>&part;x
αn
n
</p>
<p>. (10.6)
</p>
<p>The order of this operator is denoted by
</p>
<p>|α| := α1 + &middot; &middot; &middot; + αn.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.1 Test Functions and Weak Derivatives 181
</p>
<p>Repeated integration by parts introduces a minus sign for each derivative. Therefore,
</p>
<p>a function u &isin; L1loc(Ω) admits a weak derivative Dαu &isin; L1loc(Ω) if
&int;
</p>
<p>Ω
</p>
<p>(Dαu)ψ dn x = (&minus;1)|α|
&int;
</p>
<p>Ω
</p>
<p>u Dαψ dn x (10.7)
</p>
<p>for all ψ &isin; C&infin;cpt(Ω).
It might seem that we should distinguish between classical and weak derivatives
</p>
<p>in the notation. This is made unnecessary by the following:
</p>
<p>Theorem 10.4 (Consistency of weak derivatives) If u &isin; Cm(Ω) then u is weakly
differentiable to order k and the weak derivatives equal the classical derivatives.
</p>
<p>Conversely, if u &isin; L1loc(Ω) admits weak derivatives Dαu for |α| &le; m, and each
Dαu can be represented by a continuous function, then u is equivalent to a function
</p>
<p>in Cm(Ω) whose classical derivatives match the weak derivatives.
</p>
<p>In one direction the argument is straightfoward. Classical derivatives satisfy the
</p>
<p>criterion (10.7) by integration by parts, so they automatically qualify as weak deriv-
</p>
<p>atives. Lemma10.1 shows that weak derivatives are uniquely defined.
</p>
<p>The argument for the converse statement, that continuity of the weak derivatives
</p>
<p>Dαu implies classical differentiability, is much more technical and we will not be
</p>
<p>able to give the details. The basic idea is to show that one can approximate u by a
</p>
<p>sequence ψk &isin; C&infin;cpt(Ω) such that Dαψk &rarr; Dαu uniformly on every compact subset
of Ω for |α| &le; m. The fact that the weak derivatives Dαu are continuous makes
this possible. Uniform convergence then allows the classical derivatives of u to be
</p>
<p>computed as limits of the functions Dαψk .
</p>
<p>10.2 Weak Solutions of Continuity Equations
</p>
<p>Consider the continuity equation on R introduced in Sect. 3.1,
</p>
<p>&part;u
</p>
<p>&part;t
+
</p>
<p>&part;q
</p>
<p>&part;x
= 0, u|t=0 = g. (10.8)
</p>
<p>The flux q could depend on u as well as t and x . To allow for the nonlinear case, we
</p>
<p>will assume that u is real-valued here.
</p>
<p>Suppose for the moment that q is differentiable and u is a classical solution of
</p>
<p>(10.8). Letψ be a test function in C&infin;cpt([0,&infin;)&times;R). Use of the closed interval [0,&infin;)
means that ψ and its derivatives are not necessarily zero at t = 0. Pairing &part;u
</p>
<p>&part;t
with ψ
</p>
<p>and integrating by parts thus generates a boundary term,
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&part;u
</p>
<p>&part;t
ψ dt = &minus;uψ
</p>
<p>∣∣
t=0 &minus;
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>u
&part;ψ
</p>
<p>&part;t
dt.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_3">http://dx.doi.org/10.1007/978-3-319-48936-0_3</a></div>
</div>
<div class="page"><p/>
<p>182 10 Weak Solutions
</p>
<p>On the other hand, the spatial integration parts has no boundary term,
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&part;q
</p>
<p>&part;x
ψ dx = &minus;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
q
&part;ψ
</p>
<p>&part;x
dx .
</p>
<p>When the left-hand side of (10.8) is paired with ψ and integrated over both t and x ,
</p>
<p>the result is thus
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>[&part;u
&part;t
</p>
<p>+
&part;q
</p>
<p>&part;x
</p>
<p>]
ψ dx dt = &minus;
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>[
u
&part;ψ
</p>
<p>&part;t
+ q
</p>
<p>&part;ψ
</p>
<p>&part;x
</p>
<p>]
ψ dx dt
</p>
<p>&minus;
&int; &infin;
</p>
<p>&minus;&infin;
uψ|t=0 dx .
</p>
<p>If u is a classical solution of (10.8), then
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>[
u
&part;ψ
</p>
<p>&part;t
+ q
</p>
<p>&part;ψ
</p>
<p>&part;x
</p>
<p>]
dx dt +
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
gψ|t=0 dx = 0 (10.9)
</p>
<p>for all ψ &isin; C&infin;cpt([0,&infin;)&times; R).
The t = 0 integral in (10.9) makes sense for g &isin; L1loc(R). Under this assumption,
</p>
<p>we define u &isin; L1loc((0,&infin;) &times; R;R) to be a weak solution of (10.8) provided q &isin;
L1loc((0,&infin;)&times; R;R) and (10.9) holds for all test functions.
</p>
<p>Example 10.5 Consider the linear conservation equation with constant velocity,
</p>
<p>which means q = cu in (10.8). By the method of characteristics (Theorem3.2),
the solution is
</p>
<p>u(t, x) = g(x &minus; ct).
</p>
<p>Let us check that this defines a weak solution for g &isin; L1loc(R).
For ψ &isin; C&infin;cpt(R) the first term in (10.9) is
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
g(x &minus; ct)
</p>
<p>[&part;ψ
&part;t
</p>
<p>(t, x)&minus; c
&part;ψ
</p>
<p>&part;x
(t, x)
</p>
<p>]
dx dt. (10.10)
</p>
<p>To evaluate the integral, introduce the variables
</p>
<p>τ = t, y = x &minus; ct,
</p>
<p>and define
</p>
<p>ψ̃(τ , y) := ψ(τ , y + cτ ).
</p>
<p>By the chain rule,
</p>
<p>&part;ψ̃
</p>
<p>&part;τ
=
</p>
<p>&part;ψ
</p>
<p>&part;t
&minus; c
</p>
<p>&part;ψ
</p>
<p>&part;x
. (10.11)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_3">http://dx.doi.org/10.1007/978-3-319-48936-0_3</a></div>
</div>
<div class="page"><p/>
<p>10.2 Weak Solutions of Continuity Equations 183
</p>
<p>The Jacobian determinant of the transformation (τ , y) �&rarr; (t, x) is 1, so that
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
u
[&part;ψ
&part;t
</p>
<p>&minus; c
&part;ψ
</p>
<p>&part;x
</p>
<p>]
dx dt =
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
g(y)
</p>
<p>&part;ψ̃
</p>
<p>&part;τ
dy dτ .
</p>
<p>The τ integration can now be done directly,
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&part;ψ̃
</p>
<p>&part;τ
(τ , y) dτ = &minus;ψ̃(0, y).
</p>
<p>This gives
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
u
[&part;ψ
&part;t
</p>
<p>&minus; c
&part;ψ
</p>
<p>&part;x
</p>
<p>]
dx dt = &minus;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
g(y)ψ̃(0, y) dy
</p>
<p>= &minus;
&int; &infin;
</p>
<p>&minus;&infin;
g(y)ψ(0, y) dy,
</p>
<p>which verifies (10.9). &diams;
</p>
<p>We saw in Example10.3 that in one dimension a jump discontinuity precludes
</p>
<p>the existence of a weak derivative. Example10.5 shows that this is not the case in
</p>
<p>higher dimension. For g &isin; L1loc(R), the solution u(t, x) = g(x &minus; ct) could be highly
discontinuous. The direction of the derivative is crucial here; regularity is required
</p>
<p>only along the characteristics.
</p>
<p>As an application of the weak formulation (10.9), let us return to an issue that
</p>
<p>arose in the traffic model in Sect. 3.4. For certain initial conditions the characteristic
</p>
<p>lines crossed each other, ruling out a classical solution of the PDE. We will see that
</p>
<p>weak solutions can still exist in this case.
</p>
<p>Consider a one-dimensional quasilinear equation of the form
</p>
<p>&part;u
</p>
<p>&part;t
&minus;
</p>
<p>&part;
</p>
<p>&part;x
q(u) = 0 (10.12)
</p>
<p>with the flux q(u) a smooth function of u which is independent of t and x . As we saw
</p>
<p>in Sect. 3.4, the characteristics are straight lines whose slope depends on the initial
</p>
<p>conditions. Let us study the situation pictured in Fig. 3.11, where a shock forms as
</p>
<p>characteristic lines cross at some point. For simplicity we assume that the initial
</p>
<p>crossing occurs at the origin.
</p>
<p>One possible way to resolve the issue of crossing characteristics is to subdivide the
</p>
<p>(t, x) plane into two regions by drawing a shock curve C , as illustrated in Fig. 10.3.
</p>
<p>Suppose that classical solutions u&plusmn; are derived by themethod of characteristics above
and below this curve. We will show that this combination yields a weak solution
</p>
<p>provided a certain jump condition is satisfied along C . The jump condition was
</p>
<p>discovered in the 19th century by engineers William Rankine and Pierre Hugoniot,
</p>
<p>who developed the first theories of shock waves in the context of gas dynamics.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_3">http://dx.doi.org/10.1007/978-3-319-48936-0_3</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_3">http://dx.doi.org/10.1007/978-3-319-48936-0_3</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_3">http://dx.doi.org/10.1007/978-3-319-48936-0_3</a></div>
</div>
<div class="page"><p/>
<p>184 10 Weak Solutions
</p>
<p>Fig. 10.3 Shock curve with
</p>
<p>solutions u&plusmn; on either side
</p>
<p>t
</p>
<p>x
</p>
<p>u&minus;
</p>
<p>u+
</p>
<p>C
</p>
<p>Theorem 10.6 (Rankine-Hugoniot condition) Let C be a curve parametrized as
</p>
<p>x = σ(t) with σ &isin; C1[0,&infin;). Suppose that u is a weak solution of (10.12) given by
</p>
<p>u(t, x) =
{
</p>
<p>u&minus;(t, x), x &lt; σ(t),
</p>
<p>u+(t, x), x &gt; σ(t),
</p>
<p>where u&plusmn; are classical solutions. Then, at each point of C,
</p>
<p>q(u+)&minus; q(u&minus;) = (u+ &minus; u&minus;)σ&prime;. (10.13)
</p>
<p>Proof Since we are not concerned with the boundary conditions, we consider a test
</p>
<p>function ψ &isin; C&infin;cpt((0,&infin;)&times; R), for which (10.9) specializes to
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>[
u
&part;ψ
</p>
<p>&part;t
+ q(u)
</p>
<p>&part;ψ
</p>
<p>&part;x
</p>
<p>]
dx dt = 0. (10.14)
</p>
<p>Since the solutions u&plusmn; are classical and σ is C
1, we can separate the integral (10.14)
</p>
<p>at the shock curve and integrate by parts on either side.
</p>
<p>Consider first the u&minus; side. For the term involving the x derivative the integration
by parts is straightforward:
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; σ(t)
</p>
<p>&minus;&infin;
q(u&minus;)
</p>
<p>&part;ψ
</p>
<p>&part;x
dx dt = &minus;
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; σ(t)
</p>
<p>&minus;&infin;
ψ
</p>
<p>&part;
</p>
<p>&part;x
q(u&minus;) dx dt
</p>
<p>+
&int; &infin;
</p>
<p>0
</p>
<p>ψq(u&minus;)
∣∣
x=σ(t) dt.
</p>
<p>For the t-derivative term we start by using the fundamental theorem of calculus to
</p>
<p>derive</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 Weak Solutions of Continuity Equations 185
</p>
<p>d
</p>
<p>dt
</p>
<p>&int; σ(t)
</p>
<p>&minus;&infin;
ψu&minus; dx =
</p>
<p>&int; σ(t)
</p>
<p>&minus;&infin;
</p>
<p>[&part;ψ
&part;t
</p>
<p>u&minus; + ψ
&part;u&minus;
</p>
<p>&part;t
</p>
<p>]
dx + σ&prime;(t)ψu&minus;
</p>
<p>∣∣
x=σ(t).
</p>
<p>By the compact support of ψ, the integral over t of the left-hand side vanishes,
</p>
<p>yielding &int; &infin;
</p>
<p>0
</p>
<p>&int; σ(t)
</p>
<p>&minus;&infin;
</p>
<p>&part;ψ
</p>
<p>&part;t
u&minus; dx dt = &minus;
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; σ(t)
</p>
<p>&minus;&infin;
ψ
&part;u&minus;
</p>
<p>&part;t
dx dt
</p>
<p>&minus;
&int; &infin;
</p>
<p>0
</p>
<p>σ&prime;(t)ψu&minus;
∣∣
x=σ(t) dt.
</p>
<p>Combining these integration by parts formulas gives
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; σ(t)
</p>
<p>&minus;&infin;
</p>
<p>[
u&minus;
</p>
<p>&part;ψ
</p>
<p>&part;t
+ q(u&minus;)
</p>
<p>&part;ψ
</p>
<p>&part;x
</p>
<p>]
dx dt
</p>
<p>= &minus;
&int; &infin;
</p>
<p>0
</p>
<p>&int; σ(t)
</p>
<p>&minus;&infin;
ψ
[&part;u&minus;
&part;t
</p>
<p>&minus;
&part;
</p>
<p>&part;x
q(u&minus;)
</p>
<p>]
dx dt
</p>
<p>&minus;
&int; &infin;
</p>
<p>0
</p>
<p>[
σ&prime;(t)u&minus; &minus; q(u&minus;)
</p>
<p>]
ψ
∣∣∣
x=σ(t)
</p>
<p>dt.
</p>
<p>The first term on the right vanishes by the assumption that u&minus; is a classical solution,
leaving
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; σ(t)
</p>
<p>&minus;&infin;
</p>
<p>[
u&minus;
</p>
<p>&part;ψ
</p>
<p>&part;t
+ q(u&minus;)
</p>
<p>&part;ψ
</p>
<p>&part;x
</p>
<p>]
dx dt = &minus;
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>[
σ&prime;u&minus; &minus; q(u&minus;)
</p>
<p>]
ψ
∣∣∣
x=σ(t)
</p>
<p>dt.
</p>
<p>The corresponding calculation on the u+ side yields
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; &infin;
</p>
<p>σ(t)
</p>
<p>[
u+
</p>
<p>&part;ψ
</p>
<p>&part;t
+ q(u+)
</p>
<p>&part;ψ
</p>
<p>&part;x
</p>
<p>]
dx dt =
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>[
σ&prime;u+ &minus; q(u+)
</p>
<p>]
ψ
∣∣∣
x=σ(t)
</p>
<p>dt.
</p>
<p>By (10.14) the sum of the u&minus; and u+ integrals is zero, which implies
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>[
(u+ &minus; u&minus;)σ&prime; &minus; (q(u+)&minus; q(u&minus;))
</p>
<p>]
ψ
∣∣∣
x=σ(t)
</p>
<p>dt.
</p>
<p>Since this holds for all ψ &isin; C&infin;((0,&infin;)&times; R), we conclude that
[
(u+ &minus; u&minus;)σ&prime; &minus; (q(u+)&minus; q(u&minus;))
</p>
<p>]∣∣∣
x=σ(t)
</p>
<p>= 0
</p>
<p>for all t &gt; 0. ⊓⊔
</p>
<p>Example 10.7 Consider the traffic equation introduced in Sect. 3.4,</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_3">http://dx.doi.org/10.1007/978-3-319-48936-0_3</a></div>
</div>
<div class="page"><p/>
<p>186 10 Weak Solutions
</p>
<p>&part;u
</p>
<p>&part;t
+ (1&minus; 2u)
</p>
<p>&part;u
</p>
<p>&part;x
= 0,
</p>
<p>for which q(u) = u &minus; u2. For the initial condition take a step function,
</p>
<p>g(x) :=
{
</p>
<p>a, x &lt; 0,
</p>
<p>b, x &gt; 0.
(10.15)
</p>
<p>From 3.28 the characteristic lines are given by
</p>
<p>x(t) =
{
</p>
<p>x0 + (1&minus; 2a)t, x0 &lt; 0,
x0 + (1&minus; 2b)t, x0 &gt; 0.
</p>
<p>These intersect to form a shock provided a &lt; b.
</p>
<p>The solutions above and below the shock line are given by constants,
</p>
<p>u&minus;(t, x) = a, u+(t, x) = b.
</p>
<p>The Rankine-Hugoniot condition (10.13) thus reduces to
</p>
<p>q(b)&minus; q(a) = (b &minus; a)σ&prime;.
</p>
<p>Substituting with q(u) = u &minus; u2 reduces this condition to
</p>
<p>σ&prime; = 1&minus; b &minus; a.
</p>
<p>Since the discontinuity starts at the origin, the shock curve is thus given by
</p>
<p>σ(t) = (1&minus; b &minus; a)t.
</p>
<p>Hence the weak solution is
</p>
<p>u(t, x) =
{
</p>
<p>a, x &lt; (1&minus; b &minus; a)t,
b, x &gt; (1&minus; b &minus; a)t.
</p>
<p>Some cases are illustrated in Fig. 10.4. In the plot on the left, the shock wave
</p>
<p>propagates backwards. &diams;
</p>
<p>For certain initial conditions, the definition (10.9) of a weak solution is not suf-
</p>
<p>ficient to determine the solution uniquely. For example, if we had taken a &gt; b in
</p>
<p>(10.15), then instead of overlapping the characteristics originating from t = 0 would
separate, leaving a triangular region with no characteristic lines. An additional physi-
</p>
<p>cal condition is required to specify the solution uniquely in this case. We will discuss
</p>
<p>this further in the exercises.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_3">http://dx.doi.org/10.1007/978-3-319-48936-0_3</a></div>
</div>
<div class="page"><p/>
<p>10.3 Sobolev Spaces 187
</p>
<p>t
</p>
<p>x
</p>
<p>u&minus; = 0.4
</p>
<p>u+ = 0.7
</p>
<p>t
</p>
<p>x
</p>
<p>u&minus; = 0.3
</p>
<p>u+ = 0.5
</p>
<p>Fig. 10.4 Characteristic lines meeting at the shock wave
</p>
<p>10.3 Sobolev Spaces
</p>
<p>Boundary values are not well defined for locally integrable functions. We were able
</p>
<p>to avoid this issue in the discussion of the continuity equation in Sect. 10.2, because
</p>
<p>solutions were required to be constant along characteristics. In general, the formu-
</p>
<p>lation of boundary or initial conditions requires a class of functions with greater
</p>
<p>regularity.
</p>
<p>The most obvious class to consider consists of functions that admit weak higher
</p>
<p>partial derivatives.However, it proves to be very helpful to strengthen the integrability
</p>
<p>requirements as well. Such function spaces were introduced by Sergei Sobolev in
</p>
<p>the mid 20th century and have since become fundamental tools of analysis.
</p>
<p>The Sobolev spaces based on L2 are defined by
</p>
<p>H m(Ω) :=
{
u &isin; L2(Ω); Dαu &isin; L2(Ω) for all |α| &le; m
</p>
<p>}
,
</p>
<p>for m &isin; N0, with derivatives interpreted in the weak sense. An extended family of
Sobolev spaces W m,p is given by replacing L2 with L p in the definition. The extended
</p>
<p>family is important in the analysis of nonlinear PDE, but our focus will be limited to
</p>
<p>linear applications involving H m .
</p>
<p>Sobolev spaces are useful as theoretical tools, but they also have a practical side.
</p>
<p>For a bounded domainΩ , the space H 1(Ω) includes the continuous piecewise linear
</p>
<p>functions. A function is called piecewise linear if the domain can be decomposed into
</p>
<p>a finite number of polygonal subdomains, on which the function is linear. Figure10.5
</p>
<p>shows a two-dimensional example. Sobolev spaces provide a natural framework for
</p>
<p>the approximation of solutions by computationally simple classes of functions.
</p>
<p>The space H m(Ω) carries a natural inner product,</p>
<p/>
</div>
<div class="page"><p/>
<p>188 10 Weak Solutions
</p>
<p>Fig. 10.5 Graph of a
</p>
<p>piecewise linear H1 function
</p>
<p>on the unit square
</p>
<p>〈u, v〉H m :=
&sum;
</p>
<p>|α|&le;m
〈Dαu, Dαv〉. (10.16)
</p>
<p>(Our conventionwill be that a bracketwithout subscript denotes the L2 inner product.)
</p>
<p>The corresponding norm is
</p>
<p>‖u‖H m :=
( &sum;
</p>
<p>|α|&le;m
‖Dαu‖22
</p>
<p>)1
2
</p>
<p>. (10.17)
</p>
<p>Lebesgue integration theory gives us the following completeness result, analogous
</p>
<p>to Theorem7.7.
</p>
<p>Theorem 10.8 For Ω &sub; Rn and m &isin; N0, H m(Ω) is a Hilbert space.
</p>
<p>Recall that Theorem7.5 says that C&infin;cpt(Ω) is a dense subspace of L
2(Ω). This
</p>
<p>means that the closure of C&infin;cpt(Ω) with respect to the L
2 norm is L2(Ω). This result
</p>
<p>no longer holds for the Sobolev space H m(Ω) with m &ge; 1. In particular, the closure
of C&infin;cpt(Ω) with respect to the H
</p>
<p>1 norm defines a subspace
</p>
<p>H 10 (Ω) =
{
</p>
<p>u &isin; H m(Ω); lim
k&rarr;&infin;
</p>
<p>‖u &minus; ψk‖H 1 = 0 for ψk &isin; C&infin;cpt(Ω)
}
. (10.18)
</p>
<p>By Lemma7.8 H 10 (Ω) is also a Hilbert space with respect to the H
1 norm.
</p>
<p>If &part;Ω is piecewise C1, then for functions in H 1(Ω) it is possible to define bound-
</p>
<p>ary restrictions in L2(&part;Ω) that generalize the boundary restriction of a continuous
</p>
<p>function. In this case, H 10 (Ω) consists precisely of the functions whose boundary
</p>
<p>restriction vanishes. Thus the space H 10 (Ω) can be interpreted as the class of H
1
</p>
<p>functions satisfying Dirichlet boundary conditions on &part;Ω .
</p>
<p>The theory of boundary restrictions is too technical for us to cover here, but we
</p>
<p>can at least show how this works in the one-dimensional case.
</p>
<p>Theorem 10.9 If u &isin; H 10 (a, b) then u is continuous on [a, b] and equal to zero at
the endpoints.
</p>
<p>Proof Suppose u &isin; H 10 (a, b). By definition, there exists a sequence of C&infin;cpt(a, b)
such that</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
</div>
<div class="page"><p/>
<p>10.3 Sobolev Spaces 189
</p>
<p>lim
k&rarr;&infin;
</p>
<p>‖ψk &minus; u‖H 1 = 0.
</p>
<p>For x &isin; [a, b],
ψ j (x)&minus; ψk(x) =
</p>
<p>&int; x
</p>
<p>a
</p>
<p>[ψ&prime;j (t)&minus; ψ&prime;k(t)] dt
</p>
<p>The integral on the right could be expressed as an inner product on R,
</p>
<p>&int; x
</p>
<p>a
</p>
<p>[ψ&prime;j (t)&minus; ψ&prime;k(t)] dt = 〈ψ&prime;j &minus; ψ&prime;k,χ[a,x]〉,
</p>
<p>where χI denotes the characteristic function of the interval I . Thus, by the Cauchy-
</p>
<p>Schwarz inequality (Theorem7.1),
</p>
<p>|ψ j (x)&minus; ψk(x)| &le;
&radic;
</p>
<p>x &minus; a ‖ψ&prime;j &minus; ψ&prime;k‖2.
</p>
<p>In view of the definition of the H 1 norm, this implies the uniform bound
</p>
<p>‖ψ j &minus; ψk‖&infin; &le;
&radic;
</p>
<p>b &minus; a ‖ψ j &minus; ψk‖H 1 (10.19)
</p>
<p>Since {ψk} converges and is therefore Cauchy with respect to the H 1 norm, it
follows from (10.19) implies that the sequence {ψk} is also Cauchy in the uniform
sense. By the completeness of L&infin;(a, b) (Theorem7.7) and Lemma8.4, this implies
that ψk &rarr; g uniformly for some g &isin; C0[a, b].
</p>
<p>At this point we haveψk &rarr; u in H 1 andψk &rarr; g uniformly. Uniform convergence
on a bounded interval implies convergence in L2, by a simple integral estimate.
</p>
<p>Therefore ψk &rarr; g in L2 also, implying that u = g in L2. Hence u &isin; C0[a, b].
To show that u vanishes at the endpoints, note that
</p>
<p>max {|u(a)|, |u(b)|} &le; sup
[a,b]
</p>
<p>|ψk &minus; u|. (10.20)
</p>
<p>because ψk(a) = ψk(b) = 0. By uniform convergence, the left-hand side of (10.20)
approaches zero as k &rarr; &infin;, showing that
</p>
<p>u(a) = u(b) = 0.
</p>
<p>⊓⊔
</p>
<p>In higher dimensions, functions in H 1 are not necessarily continuous. However,
</p>
<p>H m does imply continuity if m is sufficiently large relative to the dimension. We will
</p>
<p>develop this regularity theory in Sect. 10.4.
</p>
<p>We conclude this section with an extension property that will prove useful in
</p>
<p>Chap.11.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_8">http://dx.doi.org/10.1007/978-3-319-48936-0_8</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_11">http://dx.doi.org/10.1007/978-3-319-48936-0_11</a></div>
</div>
<div class="page"><p/>
<p>190 10 Weak Solutions
</p>
<p>Lemma 10.10 For Ω &sub; Ω̃ &sub; Rn , the extension by zero of an element of H 10 (Ω)
gives an element of H 10 (Ω̃).
</p>
<p>Proof For u &isin; H 10 (Ω), let ũ denote the extension by zero to Ω̃ . The weak gradient
&nabla;u &isin; L2(Ω;Rn) can also be extended by zero to &nabla;̃u &isin; L2(Ω̃;Rn). We need to
show that &nabla;̃u is the weak gradient of ũ. This is the condition that
</p>
<p>&int;
</p>
<p>Ω̃
</p>
<p>φ&nabla;̃u dn x = &minus;
&int;
</p>
<p>Ω̃
</p>
<p>ũ&nabla;φ dn x, (10.21)
</p>
<p>for all φ &isin; C&infin;cpt(Ω̃).
By the definition of H 10 (Ω), there exists a sequence of ψk &isin; C&infin;cpt(Ω) such that
</p>
<p>ψk &rarr; u in the H 1 norm. Since ψk has compact support within Ω , integration by
parts gives &int;
</p>
<p>Ω
</p>
<p>φ&nabla;ψk dn x = &minus;
&int;
</p>
<p>Ω
</p>
<p>ψk&nabla;φ dn x. (10.22)
</p>
<p>By the H 1 convergence ψk &rarr; u, we can take the limit k &rarr; &infin; on both sides of
(10.22) to obtain &int;
</p>
<p>Ω
</p>
<p>φ&nabla;u dn x = &minus;
&int;
</p>
<p>Ω
</p>
<p>u&nabla;φ dn x.
</p>
<p>Since ũ and &nabla;̃u are equal to u and &nabla;u onΩ and vanish on Ω̃ &minus;Ω , this is equivalent
to (10.21). ⊓⊔
</p>
<p>10.4 Sobolev Regularity
</p>
<p>In this section we will consider the relationship between weak regularity, defined in
</p>
<p>terms of Sobolev spaces, and regularity in the classical sense. This connection plays
</p>
<p>a central role in the application of Sobolev spaces to PDE.
</p>
<p>Theorem 10.11 (Sobolev embedding theorem) Suppose Ω &sub; Rn is a bounded
domain. If m &gt; k + n
</p>
<p>2
, then
</p>
<p>H m(Ω) &sub; Ck(Ω).
</p>
<p>This result can be sharpened and extended in various ways. One important variant
</p>
<p>includes differentiability up to the boundary under certain conditions on &part;Ω . For
</p>
<p>example, if the boundary &part;Ω is piecewise C1 then it is possible to show that
</p>
<p>H m(Ω) &sub; Ck(Ω).
</p>
<p>These boundary results are quite important but too technically difficult for us to
</p>
<p>include here.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4 Sobolev Regularity 191
</p>
<p>The strategy we will use for Theorem10.11 is based on the connection estab-
</p>
<p>lished in Sect. 8.6 between regularity and the decay of Fourier coefficients. Recall
</p>
<p>the definitionT := R/2πZ introduced in Sect. 8.2. To extend Fourier series to higher
dimensions we introduce the corresponding space
</p>
<p>T
n := Rn/(2πZ)n.
</p>
<p>A function on Tn is a function on Rn which is 2π-periodic in each coordinate.
</p>
<p>The periodic Fourier series theory from can be carried over to Tn directly. For
</p>
<p>f &isin; L2(Tn) and k &isin; Zn we define the coefficients
</p>
<p>ck[ f ] :=
1
</p>
<p>(2π)n
</p>
<p>&int;
</p>
<p>Tn
</p>
<p>e&minus;i k&middot;x f (x) dn x. (10.23)
</p>
<p>The integral over Tn can be taken over [&minus;π,π]n , or any translate of this cube. The
argument from Theorem8.6 can be adapted, with minor notational changes, to prove
</p>
<p>the following:
</p>
<p>Theorem 10.12 For f &isin; L2(Tn), the series
&sum;
</p>
<p>k&isin;Zn
ck[ f ]ei k&middot;x
</p>
<p>converges to f in the L2 norm.
</p>
<p>As a corollary, we obtain the generalization of the Parseval identity (8.36),
</p>
<p>〈 f, g〉 = (2π)n
&sum;
</p>
<p>k&isin;Zn
ck[ f ]ck[g] (10.24)
</p>
<p>for f, g &isin; L2(Tn).
Because of the periodic structure ofTn , it is not necessary to assume that test func-
</p>
<p>tions have compact support. For f &isin; L1loc(T) the weak derivative Dα f &isin; L1loc(T) is
defined by the condition that
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>ψDα f dx = (&minus;1)|α|
&int; 2π
</p>
<p>0
</p>
<p>f Dαψ dx (10.25)
</p>
<p>for all ψ &isin; C&infin;(T). The space H m(Tn) consists of functions in L2(Tn) which have
weak partial derivatives up to order m contained in L2(Tn).
</p>
<p>It is convenient notate powers of the components of k by analogy with Dα,
</p>
<p>k
α := kα11 &middot; &middot; &middot; kαnn ,
</p>
<p>for α = (α1, . . . ,αn) with α j &isin; N0. A simple computation shows that</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_8">http://dx.doi.org/10.1007/978-3-319-48936-0_8</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_8">http://dx.doi.org/10.1007/978-3-319-48936-0_8</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_8">http://dx.doi.org/10.1007/978-3-319-48936-0_8</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_8">http://dx.doi.org/10.1007/978-3-319-48936-0_8</a></div>
</div>
<div class="page"><p/>
<p>192 10 Weak Solutions
</p>
<p>Dαei k&middot;x = (i k)αei k&middot;x .
</p>
<p>Thus, for f &isin; H m(T), substituting ei k&middot;x into (10.25) gives
</p>
<p>ck[Dα f ] = (i k)αck[ f ] (10.26)
</p>
<p>for |α| &le; m. This generalizes the integration by parts formula (8.30).
</p>
<p>Theorem 10.13 A function f &isin; L2(T) lies in H m(T) for m &isin; N if and only if
&sum;
</p>
<p>k&isin;Zn
|k|2m |ck[ f ]|2 &lt; &infin;. (10.27)
</p>
<p>Proof By (10.26) andBessel&rsquo;s inequality (Proposition7.9), the condition that Dα f &isin;
L2(Tn) implies that &sum;
</p>
<p>k&isin;Zn
</p>
<p>∣∣kαck[ f ]
∣∣2 &lt; &infin;. (10.28)
</p>
<p>This holds for all |α| &le; m, implying (10.27).
Conversely, if f &isin; L2(Tn) satisfies (10.27), then (10.28) holds for |α| &le; m. We
</p>
<p>can therefore define functions gα &isin; L2(Tn) by the Fourier series
</p>
<p>gα(x) :=
&sum;
</p>
<p>k&isin;Zn
(i k)αck[ f ]ei k&middot;x .
</p>
<p>By Parseval&rsquo;s identity (10.24), the inner product of gα with ψ &isin; C&infin;cpt(Tn) gives
</p>
<p>〈gα,ψ〉 = (2π)n
&sum;
</p>
<p>k&isin;Zn
(i k)αck[ f ]ck[ψ]
</p>
<p>= (&minus;1)|α|2π
&sum;
</p>
<p>k&isin;Z
ck[ f ]ck[Dαψ]
</p>
<p>= (&minus;1)|α|〈 f, Dαψ〉.
</p>
<p>This shows that the weak derivative Dα f exists and is equal to gα. ⊓⊔
</p>
<p>Theorem10.13 makes the connection between Sobolev regularity and decay of
</p>
<p>Fourier coefficients. Our task is now to translate this back into classical regularity.
</p>
<p>Theorem 10.14 (Periodic Sobolev embedding) If m &gt; q + n
2
, then
</p>
<p>H m(Tn) &sub; Cq(Tn).
</p>
<p>Proof Using the notation for discrete spaces introduced in Sect. 7.4, the space ℓ2(Zn)
</p>
<p>is defined as the Hilbert space of functionsZn &rarr; C, equipped with the inner product</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_8">http://dx.doi.org/10.1007/978-3-319-48936-0_8</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
</div>
<div class="page"><p/>
<p>10.4 Sobolev Regularity 193
</p>
<p>〈β, γ〉ℓ2 :=
&sum;
</p>
<p>k&isin;Zn
β(k)γ(k).
</p>
<p>Consider the function
</p>
<p>β(k) := (1+ |k|)&minus;m .
</p>
<p>The ℓ2 norm of β can be estimated with an integral,
</p>
<p>‖β‖2ℓ2 :=
&sum;
</p>
<p>k&isin;Zn
(1+ |k|)&minus;2m
</p>
<p>&le;
&int;
</p>
<p>Rn
</p>
<p>(1+ |x|)&minus;2m dn x
</p>
<p>= An
&int; &infin;
</p>
<p>0
</p>
<p>(1+ r)&minus;2mrn&minus;1 dr.
</p>
<p>The integral is finite if 2m &gt; n, implying that β &isin; ℓ2(Zn) for m &gt; n
2
.
</p>
<p>By Theorem10.13, for f &isin; H m(Tn) we can also define an element of ℓ2(Zn) by
</p>
<p>γ(k) := (1+ |k|)m |ck[ f ]|,
</p>
<p>so that
</p>
<p>〈β, γ〉ℓ2 =
&sum;
</p>
<p>k&isin;Zn
|ck[ f ]|.
</p>
<p>It then follows from the Cauchy-Schwarz inequality on ℓ2 that
</p>
<p>&sum;
</p>
<p>k&isin;Zn
|ck[ f ]| &le; ‖β‖ℓ2‖γ‖ℓ2 , (10.29)
</p>
<p>which is finite for m &gt; n
2
.
</p>
<p>Since |ei k&middot;x | = 1, the estimate (10.29) implies that the Fourier series for f con-
verges uniformly. By Lemma8.4 the limit of this series is continuous. Thus, after
</p>
<p>possible replacement by an equivalent function in L2, f is continuous.
</p>
<p>This argument shows that
</p>
<p>H m(Tn) &sub; C0(Tn) (10.30)
</p>
<p>form &gt; n
2
. To apply it to higher derivativeswe note that if f &isin; H m(Tn) form &gt; q+ n
</p>
<p>2
</p>
<p>then for |α| &le; q the weak derivatives Dα f will lie in H m&minus;q(TN ). For m &gt; q + n
2
</p>
<p>it follows from (10.30) that these derivatives are continuous. By Theorem10.4, this
</p>
<p>shows that u &isin; Cq(Tn). ⊓⊔
</p>
<p>Weare nowprepared to derive theSobolev embedding result for a bounded domain
</p>
<p>as a consequence of Theorem10.14.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_8">http://dx.doi.org/10.1007/978-3-319-48936-0_8</a></div>
</div>
<div class="page"><p/>
<p>194 10 Weak Solutions
</p>
<p>Proof of Theorem 10.11 Suppose u &isin; H m(Ω) forΩ &isin; Rn , let x0 &isin; Ω . BecauseΩ
is open, we can choose ε &gt; 0 small enough that
</p>
<p>B(x0; ε) &sub; Ω.
</p>
<p>Suppose that ψ &isin; C&infin;cpt(Ω) has support contained in B(x0; ε) and is equal to 1
inside B(x0; ε/2). (Such a function can be constructed as in Example2.2.) Since
ψ is smooth, uψ &isin; H m(Ω) also. Thus, assuming ε &lt; 2π, we can extend uψ by
periodicity to a function in H m(Tn). Theorem10.14 then shows that uψ &isin; Ck(Tn)
if m &gt; k + n/2. Since uψ and u agree in a neighborhood of x0, this shows that u
is k-times continuously differentiable at x0. This argument applies at every interior
</p>
<p>point of Ω , so we conclude that u &isin; Ck(Ω). ⊓⊔
</p>
<p>10.5 Weak Formulation of Elliptic Equations
</p>
<p>The Laplace equation introduced in Sect. 9.1 is the prototypical elliptic equation.
</p>
<p>Another classic example is the Poisson equation &minus;�u = f , which we will discuss
in more detail in Sect. 11.1.
</p>
<p>If Ω &sub; Rn is a bounded domain, then for u,ψ &isin; C&infin;cpt(Ω), Green&rsquo;s first identity
(Theorem2.10) gives
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>ψ�u dn x = &minus;
&int;
</p>
<p>Ω
</p>
<p>&nabla;u &middot; &nabla;ψ dn x. (10.31)
</p>
<p>On the other hand, the H 1 inner product on Ω is given by
</p>
<p>〈u,ψ〉H 1 := 〈u,ψ〉 +
&int;
</p>
<p>Ω
</p>
<p>&nabla;u &middot; &nabla;ψ dn x. (10.32)
</p>
<p>The right-hand side of (10.31) is thus well-defined for u &isin; H 10 (Ω).
To account for applications to the Helmholtz equation as well as the Laplace
</p>
<p>equation, let us consider the PDE
</p>
<p>&minus;�u = λu + f, u|&part;Ω = 0. (10.33)
</p>
<p>For f &isin; L2(Ω), we say that u &isin; H 10 (Ω) constitutes a weak solution of (10.33) if
&int;
</p>
<p>Ω
</p>
<p>[
&nabla;u &middot; &nabla;ψ &minus; λuψ &minus; f ψ
</p>
<p>]
dn x = 0 (10.34)
</p>
<p>for every ψ &isin; C&infin;cpt(Ω). This definition could be extended to more general elliptic
equations of the form Lu = f , but for simplicity we restrict our attention to the case
of the Laplacian. We will study the existence and regularity of solutions of (10.34)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_9">http://dx.doi.org/10.1007/978-3-319-48936-0_9</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_11">http://dx.doi.org/10.1007/978-3-319-48936-0_11</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>10.5 Weak Formulation of Elliptic Equations 195
</p>
<p>extensively in Chap.11. For now, we consider a simple one-dimensional case to
</p>
<p>illustrate how the definition works.
</p>
<p>Example 10.15 On the interval [0, 2], consider the equation
</p>
<p>u&prime;&prime; = f, u(0) = u(2) = 0,
</p>
<p>with
</p>
<p>f (x) =
{
</p>
<p>x, 0 &le; x &le; 1,
&minus;1, 1 &le; x &le; 2.
</p>
<p>Since f is piecewise linear, it makes sense to try using classical solutions on the two
</p>
<p>subintervals. Imposing the boundary and continuity requirements gives a family of
</p>
<p>possible solutions
</p>
<p>u(x) =
{
</p>
<p>1
6
x3 &minus; ax, 0 &le; x &le; 1,
</p>
<p>&minus; 1
2
</p>
<p>x2 + (a + 4
3
)x &minus; 2a &minus; 2
</p>
<p>3
, 1 &le; x &le; 2.
</p>
<p>To determine a we apply the weak solution condition,
</p>
<p>&int; 2
</p>
<p>0
</p>
<p>[
u&prime;ψ&prime; + f ψ
</p>
<p>]
dx = 0, (10.35)
</p>
<p>for ψ &isin; C&infin;cpt(0, 2). Using integration by parts, the first term evaluates to
</p>
<p>&int; 2
</p>
<p>0
</p>
<p>u&prime;ψ&prime; dx =
&int; 1
</p>
<p>0
</p>
<p>( 1
2
</p>
<p>x2 &minus; a)ψ&prime;(x) dx +
&int; 2
</p>
<p>1
</p>
<p>(&minus;x + a + 4
3
)ψ&prime;(x) dx
</p>
<p>&int; 2
</p>
<p>0
</p>
<p>u&prime;ψ&prime; dx = ( 1
2
&minus; a)ψ(1)&minus;
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>xψ(x) dx &minus; (a + 1
3
)ψ(1)+
</p>
<p>&int; 2
</p>
<p>1
</p>
<p>ψ(x) dx
</p>
<p>= ( 1
6
&minus; 2a)ψ(1)&minus;
</p>
<p>&int; 2
</p>
<p>0
</p>
<p>f ψ dx .
</p>
<p>The weak solution condition (10.35) requires a = 1
12
. This gives
</p>
<p>u(x) =
{
</p>
<p>1
6
x3 &minus; 1
</p>
<p>12
x, 0 &le; x &le; 1,
</p>
<p>&minus; 1
2
</p>
<p>x2 + 17
12
</p>
<p>x &minus; 5
6
, 1 &le; x &le; 2.
</p>
<p>This result is illustrated in Fig. 10.6. Note that the condition on a corresponds to
</p>
<p>a matching of the first derivatives at x = 1, so that u &isin; C1[0, 2]. &diams;
We will show in Sect. 11.3 that solutions of (10.34) are unique, so the function
</p>
<p>obtained in Example10.15 is the only possible solution. The matching of derivatives
</p>
<p>required for this solution is indicative of a more general regularity property for
</p>
<p>solutions of elliptic equation, which we will discuss in detail in Sect. 11.4.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_11">http://dx.doi.org/10.1007/978-3-319-48936-0_11</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_11">http://dx.doi.org/10.1007/978-3-319-48936-0_11</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_11">http://dx.doi.org/10.1007/978-3-319-48936-0_11</a></div>
</div>
<div class="page"><p/>
<p>196 10 Weak Solutions
</p>
<p>Fig. 10.6 One-dimensional
</p>
<p>weak solution
</p>
<p>10.6 Weak Formulation of Evolution Equations
</p>
<p>The heat and wave equations are the primary examples of linear evolution equations.
</p>
<p>Weak solutions for these equations can be defined by essentially the same strategy
</p>
<p>used in Sect. 10.5. Starting from a classical solution, we pair with a test function and
</p>
<p>use integration by parts to find the corresponding integral equation. Unfortunately,
</p>
<p>the time dependence creates some technicalities in the definition that we are not
</p>
<p>equipped to fully resolve here, but we can at least illustrate the basic philosophy by
</p>
<p>working through some examples.
</p>
<p>Consider first the wave equation on a bounded domain Ω &sub; Rn with Dirichlet
boundary conditions,
</p>
<p>&part;2u
</p>
<p>&part;t2
&minus;�u = 0, u|x&isin;&part;Ω = 0, (10.36)
</p>
<p>subject to the initial conditions
</p>
<p>u|t=0 = g,
&part;u
</p>
<p>&part;t
</p>
<p>∣∣∣
t=0
</p>
<p>= h.
</p>
<p>Assuming u is a classical solution, pairing thewave equation for u with a test function
</p>
<p>ψ &isin; C&infin;cpt([0,&infin;)&times;Ω) gives
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>[
ψ
&part;2u
</p>
<p>&part;t2
&minus; ψ�u
</p>
<p>]
dn x dt = 0.
</p>
<p>Integration by parts for the �u term works just as in (10.31), yielding
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>ψ�u dn x = &minus;
&int;
</p>
<p>Ω
</p>
<p>&nabla;u &middot; &nabla;ψ dn x.
</p>
<p>In the t variable, we integrate by parts twice and pick up a boundary term each time
</p>
<p>because the test function is not assumed to vanish at t = 0. The result is</p>
<p/>
</div>
<div class="page"><p/>
<p>10.6 Weak Formulation of Evolution Equations 197
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>ψ
&part;2u
</p>
<p>&part;t2
dt = &minus;g
</p>
<p>&part;ψ
</p>
<p>&part;t
</p>
<p>∣∣∣
t=0
</p>
<p>&minus;
&int; &infin;
</p>
<p>0
</p>
<p>&part;ψ
</p>
<p>&part;t
</p>
<p>&part;u
</p>
<p>&part;t
dt
</p>
<p>= &minus;g
&part;ψ
</p>
<p>&part;t
</p>
<p>∣∣∣
t=0
</p>
<p>+ hψ|t=0 +
&int; &infin;
</p>
<p>0
</p>
<p>u
&part;2ψ
</p>
<p>&part;t2
dt.
</p>
<p>Combining this with the spatial integral yields
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>[
u
&part;2ψ
</p>
<p>&part;t2
+&nabla;u &middot; &nabla;ψ
</p>
<p>]
dn x dt
</p>
<p>= &minus;
&int;
</p>
<p>Ω
</p>
<p>g
&part;ψ
</p>
<p>&part;t
</p>
<p>∣∣∣
t=0
</p>
<p>dn x +
&int;
</p>
<p>Ω
</p>
<p>hψ|t=0 dn x.
(10.37)
</p>
<p>As in Sect. 10.5, the Dirichlet boundary condition is imposed by assuming that
</p>
<p>u(t, &middot;) &isin; H 10 (Ω) for all t . To make sense of the boundary and initial terms we need
to assume at least that g &isin; L1loc(Ω) and h &isin; L1loc[0,&infin;). To interpret (10.37) we
also need to require that the spatial pairing of &nabla;u with &nabla;ψ is integrable over t . This
condition is more technical. It turns out to be sufficient to assume that ‖u(t, &middot;)‖H 1
is integrable as a function of t , but we will not attempt to justify this here. Instead
</p>
<p>we will limit our discussion to examples for which the existence of the integrals in
</p>
<p>(10.37) is clear.
</p>
<p>Example 10.16 Consider the piecewise linear d&rsquo;Alembert solution for the wave
</p>
<p>equation introduced in Sect. 1.2. On [0, 2] we take the initial conditions h = 0
and
</p>
<p>g(x) :=
{
</p>
<p>x, 0 &le; x &le; 1,
2&minus; x, 1 &le; x &le; 2.
</p>
<p>By Theorem4.5, d&rsquo;Alembert&rsquo;s solution is given by extending g to an odd periodic
</p>
<p>function on R with period 4, and then setting
</p>
<p>u(t, x) =
1
</p>
<p>2
</p>
<p>[
g(x + t)+ g(x &minus; t)
</p>
<p>]
.
</p>
<p>The linear components of the resulting solution are shown in Fig. 10.7. Because u is
</p>
<p>piecewise linear and vanishes at x = 0 and 2, it is clear that u(t, &middot;) &isin; H 10 (0, 2) for
each t .
</p>
<p>For this case the weak solution condition (10.37) specializes to
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; 2
</p>
<p>0
</p>
<p>[
u
&part;2ψ
</p>
<p>&part;t2
+
</p>
<p>&part;u
</p>
<p>&part;x
</p>
<p>&part;ψ
</p>
<p>&part;x
</p>
<p>]
dx dt = &minus;
</p>
<p>&int; 2
</p>
<p>0
</p>
<p>g
&part;ψ
</p>
<p>&part;t
</p>
<p>∣∣∣
t=0
</p>
<p>dx . (10.38)
</p>
<p>Checking this is essentially a matter of integration by parts, but the integrals must
</p>
<p>be broken into many pieces for large t . As a sample case, let us assume that ψ has
</p>
<p>support in [0, 1)&times; (0, 2).</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_1">http://dx.doi.org/10.1007/978-3-319-48936-0_1</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_4">http://dx.doi.org/10.1007/978-3-319-48936-0_4</a></div>
</div>
<div class="page"><p/>
<p>198 10 Weak Solutions
</p>
<p>Fig. 10.7 Piecewise linear
</p>
<p>wave solution u(t, x)
</p>
<p>u = x u = &minus;x
</p>
<p>u = 1 &minus; t
</p>
<p>u = 2 &minus; x u = x &minus; 2
</p>
<p>t
</p>
<p>x
</p>
<p>The first integral in (10.38) becomes
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>&int; 2
</p>
<p>0
</p>
<p>u
&part;2ψ
</p>
<p>&part;t2
dx dt =
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>[&int; 1&minus;x
</p>
<p>0
</p>
<p>x
&part;2ψ
</p>
<p>&part;t2
dt +
</p>
<p>&int; 1
</p>
<p>1&minus;x
(1&minus; t)
</p>
<p>&part;2ψ
</p>
<p>&part;t2
dt
]
dx
</p>
<p>&int; 2
</p>
<p>1
</p>
<p>[&int; 1
</p>
<p>x&minus;1
(1&minus; t)
</p>
<p>&part;2ψ
</p>
<p>&part;t2
dt +
</p>
<p>&int; x&minus;1
</p>
<p>0
</p>
<p>(2&minus; x)
&part;2ψ
</p>
<p>&part;t2
dt
]
dx
</p>
<p>=
&int; 1
</p>
<p>0
</p>
<p>[
&minus;x
</p>
<p>&part;ψ
</p>
<p>&part;t
(0, x)&minus; ψ(1&minus; x, x)
</p>
<p>]
dx
</p>
<p>&int; 2
</p>
<p>1
</p>
<p>[
&minus;(2&minus; x)
</p>
<p>&part;ψ
</p>
<p>&part;t
(0, x)&minus; ψ(x &minus; 1, x)
</p>
<p>]
dx
</p>
<p>Similarly, the second term in (10.38) evaluates to
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; 2
</p>
<p>0
</p>
<p>&part;u
</p>
<p>&part;x
</p>
<p>&part;ψ
</p>
<p>&part;x
dx dt =
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>&int; 1&minus;t
</p>
<p>0
</p>
<p>&part;ψ
</p>
<p>&part;x
dx dt &minus;
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>&int; 2
</p>
<p>1+t
</p>
<p>&part;ψ
</p>
<p>&part;x
dx dt
</p>
<p>=
&int; 1
</p>
<p>0
</p>
<p>ψ(t, 1&minus; t) dt +
&int; 1
</p>
<p>0
</p>
<p>ψ(t, 1+ t) dt
</p>
<p>=
&int; 1
</p>
<p>0
</p>
<p>ψ(1&minus; x, x) dx +
&int; 2
</p>
<p>1
</p>
<p>ψ(x &minus; 1, x) dx .
</p>
<p>Adding these pieces together gives
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; 2
</p>
<p>0
</p>
<p>[
u
&part;2ψ
</p>
<p>&part;t2
+
</p>
<p>&part;u
</p>
<p>&part;x
</p>
<p>&part;ψ
</p>
<p>&part;x
</p>
<p>]
dx dt = &minus;
</p>
<p>&int; 1
</p>
<p>0
</p>
<p>x
&part;ψ
</p>
<p>&part;t
(0, x) dx &minus;
</p>
<p>&int; 2
</p>
<p>1
</p>
<p>(2&minus; x)
&part;ψ
</p>
<p>&part;t
(0, x) dx
</p>
<p>= &minus;
&int; 2
</p>
<p>0
</p>
<p>g(x)
&part;ψ
</p>
<p>&part;t
(0, x) dx,
</p>
<p>which verifies (10.38) for this case. &diams;</p>
<p/>
</div>
<div class="page"><p/>
<p>10.6 Weak Formulation of Evolution Equations 199
</p>
<p>Now let us consider the weak formulation of the heat equation with Dirichlet
</p>
<p>boundary conditions,
</p>
<p>&part;u
</p>
<p>&part;t
&minus;�u = 0, u|x&isin;&part;Ω = 0, u|t=0 = h. (10.39)
</p>
<p>Derivation of the integral equation works just as for the wave equation, except that
</p>
<p>there is only a single integration by parts in the time variable. Assuming that u(t, &middot;) &isin;
H 10 (Ω) for each t &gt; 0 and h &isin; L1loc(Ω), the weak solution condition is
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>[
&minus;u
</p>
<p>&part;ψ
</p>
<p>&part;t
+&nabla;u &middot; &nabla;ψ
</p>
<p>]
dn x dt =
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>hψ|t=0 dn x (10.40)
</p>
<p>for all ψ &isin; C&infin;cpt([0,&infin;)&times;Ω).
</p>
<p>Example 10.17 Consider the heat equation on the interval (0,π), with initial con-
</p>
<p>dition h &isin; L2(0,π;R). In view of the Dirichlet boundary conditions, we use the
orthonormal basis for L2(0,π) developed in Exercise8.4, given by the sine func-
</p>
<p>tions
</p>
<p>φk(x) :=
&radic;
2
</p>
<p>π
sin(kx)
</p>
<p>for k &isin; N. The coefficients associated to h are
</p>
<p>ak :=
&int; π
</p>
<p>0
</p>
<p>h(x)φk(x) dx,
</p>
<p>and
&sum;
</p>
<p>akφk converges to h in the L
2 sense by Theorem8.6. The corresponding heat
</p>
<p>solution is
</p>
<p>u(t, x) =
&infin;&sum;
</p>
<p>k=1
ake
</p>
<p>&minus;k2tφk(x). (10.41)
</p>
<p>For example, solution corresponding to a step function h is illustrated in Fig. 10.8.
</p>
<p>Fig. 10.8 Heat solution with
</p>
<p>L2 initial data
</p>
<p>t
</p>
<p>x</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_8">http://dx.doi.org/10.1007/978-3-319-48936-0_8</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_8">http://dx.doi.org/10.1007/978-3-319-48936-0_8</a></div>
</div>
<div class="page"><p/>
<p>200 10 Weak Solutions
</p>
<p>As noted in Corollary8.14, this yields a classical solution if h is continuous. If h
</p>
<p>is not continuous then u might not have a well-defined limit as t &rarr; 0. Nevertheless,
we can check that the weak solution condition is satisfied. Given a (real-valued) test
</p>
<p>functionψ &isin; C&infin;cpt([0,&infin;)&times;(0,π);R), define the time-dependent Fourier coefficients
</p>
<p>bk(t) :=
&int; π
</p>
<p>0
</p>
<p>ψ(t, x)φk(x) dx .
</p>
<p>By the smoothness of ψ, Theorem8.10 implies that the coefficients satisfy bk(t) =
O(k&minus;&infin;), uniformly in t , and so the series
</p>
<p>ψ(t, x) =
&infin;&sum;
</p>
<p>k=1
bk(t)φk(x)
</p>
<p>converges uniformly as well as in L2. By the same principle, the series
</p>
<p>&part;ψ
</p>
<p>&part;t
(t, x) =
</p>
<p>&infin;&sum;
</p>
<p>k=1
b&prime;k(t)φk(x)
</p>
<p>is also uniformly convergent. Since {φk} is an orthonormal basis, we deduce from
Parseval&rsquo;s identity (8.36) that
</p>
<p>&int; π
</p>
<p>0
</p>
<p>u
&part;ψ
</p>
<p>&part;t
dt =
</p>
<p>&infin;&sum;
</p>
<p>k=1
ake
</p>
<p>&minus;k2t b&prime;k(t) (10.42)
</p>
<p>for t &ge; 0.
Similarly, for t &ge; 0 we have L2 convergent series
</p>
<p>&part;ψ
</p>
<p>&part;x
(t, x) =
</p>
<p>&infin;&sum;
</p>
<p>k=1
bk(t)k
</p>
<p>&radic;
2
</p>
<p>π
cos(kx),
</p>
<p>&part;u
</p>
<p>&part;x
(t, x) =
</p>
<p>&infin;&sum;
</p>
<p>k=1
ak(t)e
</p>
<p>&minus;k2t k
</p>
<p>&radic;
2
</p>
<p>π
cos(kx).
</p>
<p>By the Parseval identity for the cosine basis, this gives
</p>
<p>&int; π
</p>
<p>0
</p>
<p>&part;u
</p>
<p>&part;x
</p>
<p>&part;ψ
</p>
<p>&part;x
dx =
</p>
<p>&infin;&sum;
</p>
<p>k=1
k2ake
</p>
<p>&minus;k2t bk(t). (10.43)
</p>
<p>Applying (10.42) and (10.43) to the left-hand side of (10.40) yields</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_8">http://dx.doi.org/10.1007/978-3-319-48936-0_8</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_8">http://dx.doi.org/10.1007/978-3-319-48936-0_8</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_8">http://dx.doi.org/10.1007/978-3-319-48936-0_8</a></div>
</div>
<div class="page"><p/>
<p>10.6 Weak Formulation of Evolution Equations 201
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; π
</p>
<p>0
</p>
<p>[
&minus;u
</p>
<p>&part;ψ
</p>
<p>&part;t
+
</p>
<p>&part;u
</p>
<p>&part;x
</p>
<p>&part;ψ
</p>
<p>&part;x
ψ
]
dn x dt
</p>
<p>=
&int; &infin;
</p>
<p>0
</p>
<p>[ &infin;&sum;
</p>
<p>k=1
ake
</p>
<p>&minus;k2t
(
</p>
<p>k2bk(t)&minus; b&prime;k(t)
)]
</p>
<p>dt.
</p>
<p>(10.44)
</p>
<p>Switching theorder of the summation and integration is justified if series converges
</p>
<p>uniformly on the domain of the integral, but that is not necessarily the case here. To
</p>
<p>check this carefully, we break the sum at some value k = N . For the finite sum there
is no convergence issue, so that
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>[ N&sum;
</p>
<p>k=1
ake
</p>
<p>&minus;k2t
(
</p>
<p>k2bk(t)&minus; b&prime;k(t)
)]
</p>
<p>dt =
N&sum;
</p>
<p>k=1
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>ak
d
</p>
<p>dt
</p>
<p>(
&minus;e&minus;k2t bk(t)
</p>
<p>)
dt
</p>
<p>=
N&sum;
</p>
<p>k=1
akbk(0).
</p>
<p>To estimate the tail of the sum, note that the sequence {ak} is bounded because&sum;
|ak |2 &lt; &infin;. For bk(t), we apply repeated integration by parts to deduce
</p>
<p>&int; π
</p>
<p>0
</p>
<p>φk(x)
( &part;
&part;x
</p>
<p>)2m
ψ(t, x) dx =
</p>
<p>&int; π
</p>
<p>0
</p>
<p>ψ(t, x)
( &part;
&part;x
</p>
<p>)2m
φk(x) dx
</p>
<p>= (&minus;1)mk2m
&int; π
</p>
<p>0
</p>
<p>ψ(t, x)φk(x) dx
</p>
<p>= (&minus;1)mk2mbk(t).
</p>
<p>for m &isin; N. Since ψ &isin; C&infin;cpt([0,&infin;)&times; (0,π)), this gives an estimate
</p>
<p>|bk(t)| &le; Cmk&minus;2m,
</p>
<p>where Cm is independent of t . The same reasoning applies to b
&prime;
k(t). Combining the
</p>
<p>m = 2 estimate for bk with the m = 1 case for b&prime;k gives
∣∣∣∣∣ake
</p>
<p>&minus;k2t
(
</p>
<p>k2bk(t)&minus; b&prime;k(t)
)∣∣∣∣∣ &le; Ck
</p>
<p>&minus;2.
</p>
<p>This shows that
</p>
<p>∣∣∣∣∣
</p>
<p>&infin;&sum;
</p>
<p>k=N+1
ake
</p>
<p>&minus;k2t
(
</p>
<p>k2bk(t)&minus; b&prime;k(t)
)∣∣∣∣∣ &le; C N
</p>
<p>&minus;1, (10.45)
</p>
<p>independently of t .</p>
<p/>
</div>
<div class="page"><p/>
<p>202 10 Weak Solutions
</p>
<p>Now fix M &gt; 0 so the support of ψ is contained in [0, M]. Applying (10.45) to
the integral gives
</p>
<p>∣∣∣∣∣
</p>
<p>&int; M
</p>
<p>0
</p>
<p>[ &infin;&sum;
</p>
<p>k=N+1
ake
</p>
<p>&minus;k2t
(
</p>
<p>k2bk(t)&minus; b&prime;k(t)
)]
</p>
<p>dt
</p>
<p>∣∣∣∣∣ &le; C M N
&minus;1.
</p>
<p>Returning to (10.44), our analysis of the sum over k now gives
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; π
</p>
<p>0
</p>
<p>[
&minus;u
</p>
<p>&part;ψ
</p>
<p>&part;t
+
</p>
<p>&part;u
</p>
<p>&part;x
</p>
<p>&part;ψ
</p>
<p>&part;x
ψ
</p>
<p>]
dn x dt =
</p>
<p>N&sum;
</p>
<p>k=1
akbk(0)+ O(N&minus;1).
</p>
<p>By taking N &rarr; &infin;, we deduce
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&int; π
</p>
<p>0
</p>
<p>[
&minus;u
</p>
<p>&part;ψ
</p>
<p>&part;t
+
</p>
<p>&part;u
</p>
<p>&part;x
</p>
<p>&part;ψ
</p>
<p>&part;x
ψ
</p>
<p>]
dn x dt =
</p>
<p>&infin;&sum;
</p>
<p>k=1
akbk(0).
</p>
<p>On the other hand Parseval&rsquo;s identity gives
</p>
<p>&int; π
</p>
<p>0
</p>
<p>h(x)ψ(0, x) dx =
&infin;&sum;
</p>
<p>k=1
akbk(0),
</p>
<p>so the weak solution condition (10.40) is satisfied. &diams;
</p>
<p>10.7 Exercises
</p>
<p>10.1 On R consider the ordinary differential equation
</p>
<p>x
du
</p>
<p>dx
= 1.
</p>
<p>(a) Develop a weak formulation of this ODE in terms of pairing with a test function
</p>
<p>ψ &isin; C&infin;cpt(R)
</p>
<p>(b) Show that u(x) = log |x | is locally integrable and solves the equation in the
weak sense.
</p>
<p>10.2 In Exercise3.6 we studied Burger&rsquo;s equation,
</p>
<p>&part;u
</p>
<p>&part;t
+ u
</p>
<p>&part;u
</p>
<p>&part;x
= 0,
</p>
<p>with the initial condition</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_3">http://dx.doi.org/10.1007/978-3-319-48936-0_3</a></div>
</div>
<div class="page"><p/>
<p>10.7 Exercises 203
</p>
<p>u(0, x) =
</p>
<p>⎧
⎪⎨
⎪⎩
</p>
<p>a, x &le; 0,
a(1&minus; x)+ bx, 0 &lt; x &lt; 1,
b, x &ge; 1.
</p>
<p>For a &gt; b a shock forms at some positive time. Assuming u is a weak solution, find
</p>
<p>the equation of the shock curve starting from this point.
</p>
<p>10.3 Consider the traffic equation
</p>
<p>&part;u
</p>
<p>&part;t
+ (1&minus; 2u)
</p>
<p>&part;u
</p>
<p>&part;x
= 0
</p>
<p>with initial data
</p>
<p>g(x) =
{
0, x &gt; 0,
</p>
<p>1, x &lt; 0.
</p>
<p>(a) Sketch the characteristic lines for this initial condition, and show that they leave
</p>
<p>a triangular region uncovered.
</p>
<p>(b) Show that the constant solution u(t, x) = g(x) satisfies the Rankine-Hugoniot
condition for the shock curve σ(t) = 0 and thus gives a weak-solution of the
traffic equation. (This solution is considered unphysical because characteristic
</p>
<p>lines emerge from the shock line to fill the triangular region.)
</p>
<p>(c) The physical solution is specified by an entropy condition that says that charac-
</p>
<p>teristic lines may only intersect when followed forwards in time. Show that the
</p>
<p>continuous function
</p>
<p>u(t, x) =
</p>
<p>⎧
⎪⎨
⎪⎩
</p>
<p>0, x &gt; t,
1
2
&minus; x
</p>
<p>2t
, &minus;t &lt; x &lt; t,
</p>
<p>1, x &lt; &minus;t,
</p>
<p>satisfies the weak solution condition (10.9) (with q = u &minus; u2), as well as this
entropy condition. (This type of solution is called a rarefaction wave.)
</p>
<p>10.4 Define f &isin; L1loc(Rn) by
</p>
<p>f (x) =
{
</p>
<p>f+(x), xn &gt; 0,
</p>
<p>f&minus;(x), xn &lt; 0.
</p>
<p>with f&plusmn; &isin; C1(Rn).
</p>
<p>(a) For j = 1, . . . , n &minus; 1, show that f has weak partial derivatives given by &part; f&plusmn;
&part;x j
</p>
<p>for
</p>
<p>&plusmn;xn &gt; 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>204 10 Weak Solutions
</p>
<p>(b) Show that the weak partial
&part; f
</p>
<p>&part;xn
exists and is given by
</p>
<p>&part; f&plusmn;
&part;xn
</p>
<p>for &plusmn;xn &gt; 0 only if f
extends to a continuous function at xn = 0.
</p>
<p>10.5 Let D &sub; R2 be the unit disk {r &lt; 1} with r := |x|. Consider the function
u(x) = rα with α &isin; R constant.
</p>
<p>(a) Compute the ordinary partial derivatives &part;u
&part;x j
</p>
<p>, j = 1, 2, for r �= 0.
</p>
<p>(b) Show that for α &gt; &minus;1 these partials lie in L1(D) and define weak derivatives.
</p>
<p>(c) For what values of α is u &isin; H 1(D)?
</p>
<p>10.6 In R3 consider the equation
</p>
<p>�u =
{
1, r &le; a,
0, r &gt; a,
</p>
<p>with r := |x| and a &gt; 0. (With appropriate physical constants this is the equation
for the gravitational potential of a spherical planet of radius a.)
</p>
<p>(a) Assuming that u depends only on r , formulate a weak solution condition in terms
</p>
<p>of pairing with a test function ψ(r) with ψ &isin; C&infin;cpt[0,&infin;).
</p>
<p>(b) Find the unique solution which is smooth at r = 0 and for which u(r) &rarr; 0 as
r &rarr; &infin;.
</p>
<p>10.7 Let Ω &sub; Rn be bounded with &part;Ω piecewise C1. For u &isin; C2(Ω) and f &isin;
C0(Ω), suppose that &int;
</p>
<p>Ω
</p>
<p>[
&nabla;u &middot; &nabla;ψ &minus; f ψ
</p>
<p>]
dn x = 0. (10.46)
</p>
<p>for allψ &isin; C&infin;(Ω). Show that u satisfies the Poisson equation with Neumann bound-
ary condition,
</p>
<p>&minus;�u = f,
&part;u
</p>
<p>&part;ν
</p>
<p>∣∣∣
&part;Ω
</p>
<p>= 0. (10.47)
</p>
<p>(Thus (10.46) allows a weak formulation of (10.47) for u &isin; H 1(Ω).)</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 11
</p>
<p>Variational Methods
</p>
<p>Recall the formulas for the kinetic and potential energy of a solution of the wave
</p>
<p>equation derived in Sect. 4.7. At equilibrium the kinetic energy is zero, and by phys-
</p>
<p>ical reasoning the system should occupy a state of minimum potential energy. This
</p>
<p>suggests a strategy of reformulating the Laplace equation, which models the equi-
</p>
<p>librium state, as a minimization problem for the energy.
</p>
<p>In this application, the potential energy term from the wave equation is called the
</p>
<p>Dirichlet energy. For a bounded domain Ω &sub; Rn and w &isin; C2(Ω), let
</p>
<p>E[w] :=
1
</p>
<p>2
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>|&nabla;w|2 dn x. (11.1)
</p>
<p>The term functional is used to describe functions such as E[&middot;], to indicate that the
domain is a function space.
</p>
<p>To see how minimization of energy is related to the Laplace equation, let us
</p>
<p>suppose that u &isin; C2(Ω;R) satisfies
</p>
<p>E[u] &le; E[u + ϕ]
</p>
<p>for all ϕ &isin; C&infin;cpt(Ω;R). This implies that for t &isin; R the function t &rarr; E[u + tϕ]
achieves a global minimum at t = 0. Hence
</p>
<p>d
</p>
<p>dt
E[u + tϕ]
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>t=0
= 0. (11.2)
</p>
<p>Differentiation under the integral in the definition of E[u + tϕ] gives
</p>
<p>The original version of the book was revised: Belated corrections from author have been incorpo-
</p>
<p>rated. The erratum to the book is available at https://doi.org/10.1007/978-3-319-48936-0_14
</p>
<p>&copy; Springer International Publishing AG 2016
</p>
<p>D. Borthwick, Introduction to Partial Differential Equations,
</p>
<p>Universitext, DOI 10.1007/978-3-319-48936-0_11
</p>
<p>205</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_4">http://dx.doi.org/10.1007/978-3-319-48936-0_4</a></div>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-319-48936-0_14">https://doi.org/10.1007/978-3-319-48936-0_14</a></div>
</div>
<div class="page"><p/>
<p>206 11 Variational Methods
</p>
<p>d
</p>
<p>dt
E[u + tϕ]
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>t=0
=
</p>
<p>1
</p>
<p>2
</p>
<p>d
</p>
<p>dt
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>(
</p>
<p>|&nabla;u|2 + 2t&nabla;u &middot; &nabla;ϕ+ t2 |&nabla;ϕ|2
)
</p>
<p>dn x
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>t=0
</p>
<p>=
&int;
</p>
<p>Ω
</p>
<p>&nabla;u &middot; &nabla;ϕ dn x.
</p>
<p>By Green&rsquo;s first identity (Theorem 2.10) and the fact that ϕ vanishes on &part;Ω ,
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>&nabla;u &middot; &nabla;ϕ dn x = &minus;
&int;
</p>
<p>Ω
</p>
<p>ϕ�u dn x.
</p>
<p>Thus (11.2) is equivalent to
&int;
</p>
<p>Ω
</p>
<p>ϕ�u dn x = 0.
</p>
<p>This holds for all ϕ &isin; C&infin;cpt(Ω;R) if and only if �u = 0 on Ω .
</p>
<p>11.1 Model Problem: The Poisson Equation
</p>
<p>The empirical law describing the electric field in the presence of a charge distribution
</p>
<p>was formulated by Gauss in the mid-19th century. Gauss&rsquo;s law states that the outward
</p>
<p>flux of the electric field through a closed surface is proportional to the total electric
</p>
<p>charge contained within the region bounded by the surface. More specifically, if
</p>
<p>Ω &sub; R3 is a bounded domain with piecewise C1 boundary &part;Ω and ρ is the charge
density within Ω , then
</p>
<p>&int;
</p>
<p>&part;Ω
</p>
<p>E &middot; ν d S = 4πk
&int;
</p>
<p>Ω
</p>
<p>ρ d3x,
</p>
<p>where k is called Coulomb&rsquo;s constant.
</p>
<p>Using the divergence theorem (Theorem 2.6), we can rewrite the flux integral as
</p>
<p>&int;
</p>
<p>&part;Ω
</p>
<p>E &middot; ν d S =
&int;
</p>
<p>Ω
</p>
<p>&nabla; &middot; E d3x,
</p>
<p>so that Gauss&rsquo;s law becomes
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>&nabla; &middot; E d3x = 4πk
&int;
</p>
<p>Ω
</p>
<p>ρ d3x.
</p>
<p>This holds for an arbitrary region if and only if
</p>
<p>&nabla; &middot; E = 4πkρ (11.3)
</p>
<p>(the differential form of Gauss&rsquo;s law).</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>11.1 Model Problem: The Poisson Equation 207
</p>
<p>In Sect. 9.1 we noted that the electric potential φ and electric field E are related
</p>
<p>by
</p>
<p>E = &minus;&nabla;φ.
</p>
<p>Substituting this into (11.3) gives the Poisson equation
</p>
<p>&minus;�φ = 4πkρ.
</p>
<p>A common electrostatics problem is to find the electric potential caused by a distribu-
</p>
<p>tion of charges within a regionΩ bounded by a conducting material. The electric field
</p>
<p>must be perpendicular to a conducting surface, implying that the boundary restriction
</p>
<p>φ|&part;Ω is constant. If the boundary is connected then we can set this constant to 0, so
the potential satisfies the Poisson equation with Dirichlet boundary conditions.
</p>
<p>Other forms of the Poisson equation appear in various contexts. For example in
</p>
<p>Newtonian gravity the relationship between gravitational potential Φ and the mass
</p>
<p>density ρ is
</p>
<p>�Φ = 4πGρ,
</p>
<p>where G is Newton&rsquo;s gravitational constant. In this application the domain is R3,
</p>
<p>and the physical assumption that Φ &rarr; 0 at infinity plays the role of a boundary
condition.
</p>
<p>11.2 Dirichlet&rsquo;s Principle
</p>
<p>To solve Poisson&rsquo;s equation using a minimization argument, it proves to be very
</p>
<p>helpful to use the weak formulation of the equation. This is because H 10 (Ω) is a
</p>
<p>Hilbert space. The completeness of H 10 (Ω) with respect to the H
1 norm will play an
</p>
<p>essential role in establishing the existence of a minimizing function.
</p>
<p>For Ω &sub; Rn bounded, and f &isin; C0(Ω), the classical Poisson problem is to find
a function u &isin; C2(Ω) &cap; C0(Ω) so that
</p>
<p>&minus;�u = f, u|&part;Ω = 0. (11.4)
</p>
<p>The weak formulation of (11.4) is a special case of (10.34), We take f &isin; L2(Ω) and
the goal is to find u &isin; H 10 (Ω) such that
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>[
</p>
<p>&nabla;u &middot; &nabla;ψ &minus; f ψ
]
</p>
<p>dn x = 0, (11.5)
</p>
<p>for every ψ &isin; C&infin;cpt(Ω).
For convenience let us consider real-valued functions. (In the complex case we
</p>
<p>could split the Poisson problem into real and imaginary parts.) In view of (11.5), we
</p>
<p>define the functional</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_9">http://dx.doi.org/10.1007/978-3-319-48936-0_9</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_10">http://dx.doi.org/10.1007/978-3-319-48936-0_10</a></div>
</div>
<div class="page"><p/>
<p>208 11 Variational Methods
</p>
<p>D f [w] := E[w] &minus; 〈 f, w〉 , (11.6)
</p>
<p>for f &isin; L2(Ω;R) and w &isin; H 10 (Ω;R), where E[&middot;] is the Dirichlet energy (11.1).
</p>
<p>Theorem 11.1 (Dirichlet&rsquo;s principle) Suppose Ω &sub; Rn is a bounded domain and
f &isin; L2(Ω;R). If u &isin; H 10 (Ω;R) satisfies
</p>
<p>D f [u] &le; D f [w]
</p>
<p>for all w &isin; H 10 (Ω;R), then u is a weak solution of the Poisson equation, in the sense
of (11.5).
</p>
<p>Proof Since C&infin;cpt(Ω;R) &sub; H 10 (Ω), the assumption on u implies
</p>
<p>D f [u] &le; D f [u + tψ]
</p>
<p>for ψ &isin; C&infin;cpt(Ω;R) and t &isin; R. Therefore
</p>
<p>0 =
d
</p>
<p>dt
D f [u + tψ]
</p>
<p>∣
</p>
<p>∣
</p>
<p>t=0
</p>
<p>=
d
</p>
<p>dt
</p>
<p>(
</p>
<p>E[u + tψ] &minus; 〈 f, u + tψ〉
)
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>t=0
</p>
<p>=
&int;
</p>
<p>Ω
</p>
<p>[
</p>
<p>&nabla;u &middot; &nabla;ψ &minus; f ψ
]
</p>
<p>dn x.
</p>
<p>�
</p>
<p>Dirichlet&rsquo;s principle is a classic example of a variational method. This terminol-
</p>
<p>ogy refers to the family of variations u + tψ used to derive the PDE from the min-
imization problem. In Sect. 11.3 we will show that D f [&middot;] attains a minimum within
H 10 (Ω), guaranteeing the existence of a weak solution. Furthermore, in Sect. 11.4 we
</p>
<p>will see that the weak solution is actually a classical solution under certain conditions.
</p>
<p>11.3 Coercivity and Existence of a Minimum
</p>
<p>The functional D f defined in (11.6) consists of a quadratic term plus a linear term.
</p>
<p>The Dirichlet minimization problem is thus analogous to minimizing the polynomial
</p>
<p>ax2 +bx for x &isin; R. This polynomial obviously has a minimum if and only if a &gt; 0.
For the Dirichlet case the analogous condition is a lower bound on the quadratic term
</p>
<p>E[&middot;]. The original form of this result was proven by Henri Poincar&eacute;.
</p>
<p>Theorem 11.2 (Poincar&eacute; Inequality) For a bounded domain Ω &sub; Rn , there is a
constant κ &gt; 0, depending only on Ω , such that</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Coercivity and Existence of a Minimum 209
</p>
<p>‖u‖22 &le; κ2E[u]
</p>
<p>for all u &isin; H 10 (Ω).
</p>
<p>Proof Because C&infin;cpt(Ω) is dense in H
1
0 (Ω), we can restrict our attention to smooth
</p>
<p>compactly supported functions at first and extend to the general case later. To illustrate
</p>
<p>the core argument we start with the n = 1 case.
Consider a bounded interval (a, b) &sub; R. Forψ &isin; C&infin;cpt(a, b), our goal is to compare
</p>
<p>the size of ψ(x) to values of its derivative ψ&prime;. The obvious connection between them
comes from the fundamental theorem of calculus:
</p>
<p>ψ(x) =
&int; x
</p>
<p>a
</p>
<p>ψ&prime;(t) dt.
</p>
<p>The right-hand side can be written as an L2 pairing with the characteristic function
</p>
<p>of [a, x],
ψ(x) =
</p>
<p>&lang;
</p>
<p>ψ&prime;,χ[a,x]
&rang;
</p>
<p>.
</p>
<p>The Cauchy-Schwarz inequality (Theorem 7.1) then gives
</p>
<p>|ψ(x)|2 &le; ‖χ[a,x]‖22 ‖ψ&prime;‖22
&le; (b &minus; a)E[ψ],
</p>
<p>for all x &isin; [a, b]. We can integrate this estimate over x to obtain
</p>
<p>‖ψ‖22 &le; (b &minus; a)2E[ψ]
</p>
<p>for ψ &isin; C&infin;cpt(a, b).
Now let us consider the higher dimensional caseΩ &sub; Rn . The domain is assumed
</p>
<p>to be bounded, so
</p>
<p>Ω &sub; R := [&minus;M, M]n
</p>
<p>for some large M . Functions in C&infin;cpt(Ω) can be extended by zero to smooth functions
on R, so it suffices to derive the Poincar&eacute; inequality for ψ &isin; C&infin;cpt(R). Following
the one-dimensional case, we apply the fundamental theorem of calculus in the x1
variable to write
</p>
<p>ψ(x) =
&int; x1
</p>
<p>&minus;M
</p>
<p>&part;ψ
</p>
<p>&part;x1
(y, x2, . . . , xn) dy.
</p>
<p>By the Cauchy-Schwarz inequality on L2(&minus;M, M),
</p>
<p>|ψ(x)|2 &le; 2M
&int; M
</p>
<p>&minus;M
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&part;ψ
</p>
<p>&part;x1
(y, x2, . . . , xn)
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>2
</p>
<p>dy,</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
</div>
<div class="page"><p/>
<p>210 11 Variational Methods
</p>
<p>for all x &isin; R. Integrating this estimate over x &isin; R yields
</p>
<p>‖ψ‖22 &le; 4M2
∥
</p>
<p>∥
</p>
<p>∥
</p>
<p>&part;ψ
</p>
<p>&part;x1
</p>
<p>∥
</p>
<p>∥
</p>
<p>∥
</p>
<p>2
</p>
<p>2
. (11.7)
</p>
<p>By the definition (11.1), the energy is given by
</p>
<p>E[ψ] =
1
</p>
<p>2
</p>
<p>&int;
</p>
<p>R
</p>
<p>&nabla;ψ &middot; &nabla;ψ dn x
</p>
<p>=
1
</p>
<p>2
</p>
<p>&int;
</p>
<p>R
</p>
<p>[
</p>
<p>(
</p>
<p>&part;ψ
</p>
<p>&part;x1
</p>
<p>)2
</p>
<p>+ &middot; &middot; &middot; +
(
</p>
<p>&part;ψ
</p>
<p>&part;xn
</p>
<p>)2
]
</p>
<p>dn x
</p>
<p>=
1
</p>
<p>2
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>∥
</p>
<p>∥
</p>
<p>∥
</p>
<p>&part;ψ
</p>
<p>&part;x j
</p>
<p>∥
</p>
<p>∥
</p>
<p>∥
</p>
<p>2
</p>
<p>2
.
</p>
<p>Thus the bound (11.7) implies the estimate,
</p>
<p>‖ψ‖22 &le; 8M2E[ψ], (11.8)
</p>
<p>for ψ &isin; C&infin;cpt(Ω).
To complete the argument, suppose that u &isin; H 10 (Ω). By the definition of H 10
</p>
<p>there exists an approximating sequence {ψk} &isin; C&infin;cpt(Ω) such that ψk &rarr; u in the H 1
norm. By (11.8) the inequality
</p>
<p>‖ψk‖22 &le; 8M2E[ψk] (11.9)
</p>
<p>holds for each k &isin; N. Our goal is thus to take the limit k &rarr; &infin; on both sides.
For the energy side note that
</p>
<p>E[u] &minus; E[ψk] =
n
</p>
<p>&sum;
</p>
<p>j=1
</p>
<p>(&lang;
</p>
<p>&part;u
</p>
<p>&part;x j
,
&part;u
</p>
<p>&part;x j
</p>
<p>&rang;
</p>
<p>&minus;
&lang;
</p>
<p>&part;ψk
</p>
<p>&part;x j
,
&part;ψk
</p>
<p>&part;x j
</p>
<p>&rang;)
</p>
<p>=
n
</p>
<p>&sum;
</p>
<p>j=1
</p>
<p>(&lang;
</p>
<p>&part;(u &minus; ψk)
&part;x j
</p>
<p>,
&part;u
</p>
<p>&part;x j
</p>
<p>&rang;
</p>
<p>+
&lang;
</p>
<p>&part;ψk
</p>
<p>&part;x j
,
&part;(u &minus; ψk)
</p>
<p>&part;x j
</p>
<p>&rang;)
</p>
<p>Hence by the Cauchy-Schwarz inequality
</p>
<p>|E[u] &minus; E[ψk]| &le;
n
</p>
<p>&sum;
</p>
<p>j=1
</p>
<p>∥
</p>
<p>∥
</p>
<p>∥
</p>
<p>&part;(u &minus; ψk)
&part;x j
</p>
<p>∥
</p>
<p>∥
</p>
<p>∥
</p>
<p>2
</p>
<p>(
</p>
<p>∥
</p>
<p>∥
</p>
<p>∥
</p>
<p>&part;u
</p>
<p>&part;x j
</p>
<p>∥
</p>
<p>∥
</p>
<p>∥
</p>
<p>2
+
</p>
<p>∥
</p>
<p>∥
</p>
<p>∥
</p>
<p>&part;ψk
</p>
<p>&part;x j
</p>
<p>∥
</p>
<p>∥
</p>
<p>∥
</p>
<p>2
</p>
<p>)
</p>
<p>.
</p>
<p>By the definition of the H 1 norm this yields
</p>
<p>|E[u] &minus; E[ψk]| &le; ‖u &minus; ψk‖H 1 (‖u‖H 1 + ‖ψk‖H 1) .</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Coercivity and Existence of a Minimum 211
</p>
<p>In particular, ψk &rarr; u in H 1 implies that
</p>
<p>lim
k&rarr;&infin;
</p>
<p>E[ψk] = E[u].
</p>
<p>Thus taking the limit k &rarr; &infin; in (11.9) gives
</p>
<p>‖u‖22 &le; 8M2E[u].
</p>
<p>�
</p>
<p>The Poincar&eacute; constant κ(Ω) for a bounded domain Ω is defined to be the optimal
</p>
<p>choice of κ in Theorem 11.2. In other words,
</p>
<p>κ(Ω) := sup
u&isin;H 10 (Ω)\{0}
</p>
<p>‖u‖2
E[u] 12
</p>
<p>.
</p>
<p>Our proof gives the rough estimate
</p>
<p>κ(Ω) &le;
&radic;
</p>
<p>8 diam(Ω),
</p>
<p>which is rather poor compared to the best known bounds. In Sect. 11.5 we will estab-
</p>
<p>lish a direct relationship between the Poincar&eacute; constant and the lowest eigenvalue of
</p>
<p>� on Ω .
</p>
<p>Since our goal is to minimize D f [&middot;] over H 10 , it is useful to express the conclusion
of Theorem 11.2 in terms of the H 1 norm. Because
</p>
<p>‖u‖2H 1 = ‖u‖
2
2 + E[u], (11.10)
</p>
<p>(11.8) is equivalent to
</p>
<p>‖u‖2H 1 &le; (κ
2 + 1)E[u] (11.11)
</p>
<p>for all u &isin; H 10 (Ω). A quadratic functional on a Hilbert space is called coercive if its
ratio to the norm squared is bounded below. The Poincar&eacute; inequality thus states that
</p>
<p>E[&middot;] is coercive on H 10 (Ω).
The identity (11.10) also gives an upper bound,
</p>
<p>E[u] &le; ‖u‖2H 1 . (11.12)
</p>
<p>A quadratic functional is called bounded if its ratio to the norm squared is bounded
</p>
<p>above. For the energy this condition is automatic.
</p>
<p>We are now prepared to tackle the minimization problem for D f [&middot;], by exploiting
the fact that E[&middot;] is both coercive and bounded.</p>
<p/>
</div>
<div class="page"><p/>
<p>212 11 Variational Methods
</p>
<p>Theorem 11.3 For a bounded domain Ω &sub; Rn and f &isin; L2(Ω) there is a unique
function u &isin; H 10 (Ω) such that
</p>
<p>D f [u] &le; D f [w]
</p>
<p>for all w &isin; H 10 (Ω), where D f [&middot;] is defined by (11.6).
</p>
<p>Proof By the triangle inequality,
</p>
<p>D f [w] &ge; E[w] &minus; |〈 f, w〉| .
</p>
<p>Applying (11.11) to the energy and the Cauchy-Schwarz inequality to the inner
</p>
<p>product gives
</p>
<p>D f [w] &ge;
1
</p>
<p>κ2 + 1
‖w‖2H 1 &minus; ‖ f ‖2‖w‖2
</p>
<p>&ge;
1
</p>
<p>κ2 + 1
‖w‖2H 1 &minus; ‖ f ‖2‖w‖H 1 .
</p>
<p>(11.13)
</p>
<p>The right-hand side has the form cx2 &minus; bx where c = 1/(κ2 + 1), b = ‖ f ‖2, and
x = ‖w‖H 1 . According to the minimization formula for a quadratic polynomial,
</p>
<p>min
x&isin;R
</p>
<p>(cx2 &minus; bx) = &minus;
b
</p>
<p>4c
</p>
<p>for c &gt; 0. Applying this to (11.13) gives
</p>
<p>D f [w] &ge; &minus;
κ2 + 1
</p>
<p>4
‖ f ‖22, (11.14)
</p>
<p>for w &isin; H 10 (Ω).
If we set
</p>
<p>d0 := inf
w&isin;H 10 (Ω)
</p>
<p>D f [w],
</p>
<p>then (11.14) shows that d0 &gt; &minus;&infin;. By Lemma 2.1, there exists a sequence of
wk &sub; H 10 (Ω) so that
</p>
<p>lim
k&rarr;&infin;
</p>
<p>D f [wk] = d0. (11.15)
</p>
<p>Our strategy is to argue that the sequence {wk} is Cauchy in H 10 (Ω), and therefore
converges by completeness.
</p>
<p>The quadratic structure of E[&middot;] implies that
</p>
<p>E
</p>
<p>[
</p>
<p>u + v
2
</p>
<p>]
</p>
<p>=
1
</p>
<p>2
E[u] +
</p>
<p>1
</p>
<p>2
E[v] &minus;
</p>
<p>1
</p>
<p>4
E[u &minus; v] (11.16)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>11.3 Coercivity and Existence of a Minimum 213
</p>
<p>for all u, v &isin; H 10 (Ω). This allows us to compute
</p>
<p>D f
</p>
<p>[
</p>
<p>wk + wm
2
</p>
<p>]
</p>
<p>=
1
</p>
<p>2
E[wk] +
</p>
<p>1
</p>
<p>2
E[wm] &minus;
</p>
<p>1
</p>
<p>4
E[wk &minus; wm] &minus;
</p>
<p>1
</p>
<p>2
〈 f, wk + wm〉
</p>
<p>=
1
</p>
<p>2
D f [wk] +
</p>
<p>1
</p>
<p>2
D f [wm] &minus;
</p>
<p>1
</p>
<p>4
E[wk &minus; wm].
</p>
<p>Because d0 is the infimum of D f [&middot;], this implies
</p>
<p>d0 &le;
1
</p>
<p>2
D f [wk] +
</p>
<p>1
</p>
<p>2
D f [wm] &minus;
</p>
<p>1
</p>
<p>4
E[wk &minus; wm]. (11.17)
</p>
<p>Turning this inequality around gives
</p>
<p>E[wk &minus; wm] &le; 2D f [wk] + 2D f [wm] &minus; 4d0.
</p>
<p>By (11.15),
</p>
<p>lim
k,m&rarr;&infin;
</p>
<p>(
</p>
<p>2D f [wk] + 2D f [wm] &minus; 4d0
)
</p>
<p>= 0,
</p>
<p>and since E[&middot;] &ge; 0 this yields
</p>
<p>lim
k,m&rarr;0
</p>
<p>E[wk &minus; wm] = 0. (11.18)
</p>
<p>Using the coercivity estimate (11.11), it follows from (11.18) that
</p>
<p>lim
k,m&rarr;0
</p>
<p>‖wk &minus; wm‖H 1 = 0,
</p>
<p>i.e., the sequence {wk} is Cauchy in H 10 (Ω). Therefore, by completeness, there exists
a function u &isin; H 10 (Ω) such that
</p>
<p>u := lim
k&rarr;&infin;
</p>
<p>wk .
</p>
<p>Convergence in H 1 implies that
</p>
<p>D f [u] = lim
k&rarr;&infin;
</p>
<p>D f [wk] = d0.
</p>
<p>Therefore u minimizes D f [&middot;].
To see that the minimizing function is unique, suppose that both u1 and u2 satisfy
</p>
<p>D f [u j ] = d0. By the same reasoning used to derive (11.17) we have
</p>
<p>d0 =
1
</p>
<p>2
D f [u1] +
</p>
<p>1
</p>
<p>2
D f [u2] &minus;
</p>
<p>1
</p>
<p>4
E[u1 &minus; u2].</p>
<p/>
</div>
<div class="page"><p/>
<p>214 11 Variational Methods
</p>
<p>By assumption both D f [u1] and D f [u2] are equal to d0, so this implies
</p>
<p>E[u1 &minus; u2] = 0.
</p>
<p>Theorem 11.2 then implies ‖u1 &minus; u2‖2 = 0, so that u1 &equiv; u2. �
</p>
<p>Note the crucial role that completeness plays in the proof of Theorem 11.3. If we
</p>
<p>had taken the domain of the Dirichlet energy to C2(Ω), then there would be no way
</p>
<p>to deduce convergence of the sequence {wk} from the energy limit (11.18).
</p>
<p>Corollary 11.4 For f &isin; L2(Ω) the weak formulation (11.5) of the Poisson equation
admits a unique solution u &isin; H 10 (Ω).
</p>
<p>Proof Existence of the solution follows from Theorems 11.1 and 11.3. To prove
</p>
<p>uniqueness, suppose that u1 and u2 both satisfy (11.5). Subtracting the equations
</p>
<p>gives
&int;
</p>
<p>Ω
</p>
<p>&nabla;(u1 &minus; u2) &middot; &nabla;ψ dn x = 0
</p>
<p>for all ψ &isin; C&infin;cpt(Ω). By the definition (10.18) of H 10 (Ω) we can take a sequence of
ψk &isin; H 10 (Ω) such that ψk &rarr; u1 &minus;u2 in the H 1 norm. This implies in particular that
&nabla;ψk &rarr; &nabla;(u1 &minus; u2) in the L2 sense, so that
</p>
<p>E[u1 &minus; u2] = lim
k&rarr;&infin;
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>&nabla;(u1 &minus; u2) &middot; &nabla;ψk dn x
</p>
<p>= 0.
</p>
<p>It follows from the Poincar&eacute; inequality (Theorem 11.2) that ‖u1 &minus; u2‖ = 0, hence
u1 &equiv; u2. �
</p>
<p>11.4 Elliptic Regularity
</p>
<p>If &minus;�u = f in the classical sense, then f has a level of differentiability 2 orders
below that of u. Our goal in this section is to develop a converse to this statement
</p>
<p>that allows us to deduce the Sobolev regularity of a weak solution u from that of f .
</p>
<p>This type of regularity result holds for elliptic equations in general, but we will focus
</p>
<p>on the Laplacian for simplicity.
</p>
<p>To avoid complications near the boundary of the domain, we introduce local
</p>
<p>versions of the Sobolev spaces. For Ω &sub; Rn let
</p>
<p>H mloc(Ω) :=
{
</p>
<p>u &isin; L1loc(Ω); uψ &isin; H m(Ω) for all ψ &isin; C&infin;cpt(Ω)
}
</p>
<p>.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_10">http://dx.doi.org/10.1007/978-3-319-48936-0_10</a></div>
</div>
<div class="page"><p/>
<p>11.4 Elliptic Regularity 215
</p>
<p>Theorem 11.5 (Interior regularity) Suppose that u &isin; H 1loc(Ω) is a weak solution of
</p>
<p>&minus;�u = f.
</p>
<p>If f &isin; H mloc(Ω) for m &ge; 0 then u &isin; H
m+2
loc (Ω).
</p>
<p>The Sobolev embedding theorem (Theorem 10.11) gives
</p>
<p>H mloc(Ω) &sub; Ck(Ω)
</p>
<p>for k &lt; m &minus; n
2
. Thus Theorem 11.5 shows that the weak solution of the Poisson
</p>
<p>equation obtained in Corollary 11.4 is a classical solution provided f &isin; H mloc(Ω)
with m &gt; n
</p>
<p>2
. Furthermore, if f &isin; C&infin;(Ω) then u &isin; C&infin;(Ω) also.
</p>
<p>It is possible to include regularity up to the boundary, although this is technically
</p>
<p>much more difficult. For example, if &part;Ω is C&infin; and u &isin; H 10 (Ω) is a weak solution
of &minus;�u = f for f &isin; C&infin;(Ω), then u &isin; C&infin;(Ω).
</p>
<p>Our proof of Theorem 11.5 makes use of the Fourier analysis on Tn that we
</p>
<p>introduced in Sect. 10.4.
</p>
<p>Lemma 11.6 Suppose that u &isin; H 1(Tn) solves &minus;�u = f in the weak sense. If
f &isin; H m(Tn) for m &isin; N0, then u &isin; H m+2(Tn).
</p>
<p>Proof For u &isin; H 1(Tn) and f &isin; L2(Tn), we assume that
&int;
</p>
<p>Tn
</p>
<p>[&nabla;u &middot; &nabla;ψ &minus; f ψ] dn x = 0, (11.19)
</p>
<p>for all ψ &isin; C&infin;(T). Setting ψ(x) = e&minus;i k&middot;x in this equation gives
&int;
</p>
<p>Tn
</p>
<p>[&minus;i k &middot; &nabla;u(x)&minus; f (x)] e&minus;i k&middot;x dn x = 0.
</p>
<p>Using 10.26, we can translate this into a relation between the Fourier coefficients,
</p>
<p>ck[ f ] = &minus;
n
</p>
<p>&sum;
</p>
<p>j=1
ik j ck
</p>
<p>[
</p>
<p>&part;u
</p>
<p>&part;x j
</p>
<p>]
</p>
<p>= |k|2 ck[u].
</p>
<p>(11.20)
</p>
<p>According to Theorem 10.13, if f &isin; H m(Tn) then
&sum;
</p>
<p>k&isin;Zn
|k|2m |ck[ f ]|2 &lt; &infin;.
</p>
<p>By (11.20) this implies that</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_10">http://dx.doi.org/10.1007/978-3-319-48936-0_10</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_10">http://dx.doi.org/10.1007/978-3-319-48936-0_10</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_10">http://dx.doi.org/10.1007/978-3-319-48936-0_10</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_10">http://dx.doi.org/10.1007/978-3-319-48936-0_10</a></div>
</div>
<div class="page"><p/>
<p>216 11 Variational Methods
</p>
<p>&sum;
</p>
<p>k&isin;Zn
|k|2m+4 |ck[u]|2 &lt; &infin;,
</p>
<p>which gives u &isin; H m+2(Tn) by Theorem 10.13. �
</p>
<p>We can now deduce the interior elliptic regularity result from Lemma 11.6 by
</p>
<p>localizing.
</p>
<p>Proof of Theorem 11.5. Suppose that &minus;�u = f in the weak sense of (11.5), with
u &isin; H 10 (Ω) and f &isin; L2loc(Ω). By rescaling and translating, if necessary, we can
assume that Ω &sub; [0, 2π]n , which allows us to identify Ω with a subset of Tn . For
χ &isin; C&infin;cpt(Ω) we can extend uχ by zero to a function on [0, 2π]n . This allows us
to consider uχ as a function in H 1(Tn). Our goal is to apply Lemma 11.6 to the
</p>
<p>localized function uχ.
</p>
<p>We must first show that uχ satisfies a weak Poisson equation on Tn . Given ψ &isin;
C&infin;(Tn) we can use the test function χψ &isin; C&infin;cpt(Ω) in the weak solution condition
(11.5) to obtain
</p>
<p>&int;
</p>
<p>Tn
</p>
<p>[&nabla;u &middot; &nabla;(χψ)&minus; f χψ] dn x = 0.
</p>
<p>Using the product rule for the gradient, we can rewrite this as
</p>
<p>0 =
&int;
</p>
<p>Tn
</p>
<p>[χ&nabla;u &middot; &nabla;ψ + ψ&nabla;u &middot; &nabla;χ&minus; f χψ] dn x
</p>
<p>=
&int;
</p>
<p>Tn
</p>
<p>[&nabla;(uχ) &middot; &nabla;ψ &minus; u&nabla;χ &middot; &nabla;ψ + ψ&nabla;u &middot; &nabla;χ&minus; f χψ] dn x.
(11.21)
</p>
<p>In order to interpret this as a weak equation for uχ, we need to rewrite the second
</p>
<p>term as an integral involving ψ rather than &nabla;ψ.
Since the components of u&nabla;χ are in C&infin;cpt(Ω), the definition of the weak derivative
</p>
<p>&nabla;u gives
&int;
</p>
<p>Ω
</p>
<p>&nabla;u &middot; (ψ&nabla;χ) dn x = &minus;
&int;
</p>
<p>Ω
</p>
<p>u&nabla; &middot; (ψ&nabla;χ) dn x
</p>
<p>= &minus;
&int;
</p>
<p>Ω
</p>
<p>[u&nabla;ψ &middot; &nabla;χ+ uψ�χ] dn x.
</p>
<p>Using this formula on the term u&nabla;χ &middot; &nabla;ψ in (11.21) gives
</p>
<p>0 =
&int;
</p>
<p>Tn
</p>
<p>[
</p>
<p>&nabla;(uχ) &middot; &nabla;ψ +
(
</p>
<p>2&nabla;u &middot; &nabla;χ+ u�χ&minus; f χ
)
</p>
<p>ψ
]
</p>
<p>dn x. (11.22)
</p>
<p>This holds for all ψ &isin; C&infin;(Tn), implying that uχ is a weak solution, in the sense of
(11.19), of the equation
</p>
<p>&minus;�(uχ) = &minus;2&nabla;u &middot; &nabla;χ&minus; u�χ+ f χ. (11.23)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_10">http://dx.doi.org/10.1007/978-3-319-48936-0_10</a></div>
</div>
<div class="page"><p/>
<p>11.4 Elliptic Regularity 217
</p>
<p>Because χ &isin; C&infin;cpt(Ω), the right-hand side of (11.23) lies in L2(Tn) by the
assumptions on u and f . Hence uχ &isin; H 2(Tn) by Lemma 11.6. This holds for
all χ &isin; C&infin;cpt(Ω), implying that u &isin; H 2loc(Tn).
</p>
<p>We can now apply the same argument inductively. If u &isin; H qloc(Ω), and f &isin;
H
</p>
<p>q&minus;1
loc (Ω), then the right-hand side of (11.23) lies in H
</p>
<p>q&minus;1(Tn) for χ &isin; C&infin;cpt(Ω).
Lemma 11.6 then gives u &isin; H q+1loc (Ω). �
</p>
<p>11.5 Eigenvalues by Minimization
</p>
<p>In Sect. 7.6, we mentioned that the spectral theorem for finite-dimensional matrices
</p>
<p>has an extension to certain differential operators. In this section, we will prove this
</p>
<p>result in the classical setting of the Laplacian on a bounded domain with Dirichlet
</p>
<p>boundary conditions.
</p>
<p>Theorem 11.7 (Spectral theorem for the Dirichlet Laplacian) Let Ω &sub; Rn be a
bounded domain. There exists an orthonormal basis {φk}k&isin;N for L2(Ω) such that
</p>
<p>&minus;�φk = λkφk
</p>
<p>with φk &isin; H 10 (Ω;R) &cap; C&infin;(Ω). Furthermore, λk &gt; 0 for all k and
</p>
<p>lim
k&rarr;&infin;
</p>
<p>λk = &infin;.
</p>
<p>In the case of L2[0,π], the sequence of Dirichlet eigenfunctions is given by
φk(x) = sin(kx) for k &isin; N, with eigenvalues λk = k2. If follows from Theorem 8.6
that this sequence yields a basis, as shown in Exercise 8.4.
</p>
<p>We can solve the eigenvalue problem in the general case by adapting the vari-
</p>
<p>ational method used for the Poisson equation earlier in this chapter. This method
</p>
<p>gives more than just existence of the basis; it also suggests a natural strategy for
</p>
<p>numerical approximation of eigenvalues and eigenfunctions, which we will explore
</p>
<p>in Sect. 11.7.
</p>
<p>The weak formulation of the eigenvalue equation &minus;�φ = λφ with Dirichlet
boundary conditions is a special case of (10.34). For φ &isin; H 10 (Ω), the condition is
that
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>[&nabla;φ &middot; &nabla;ψ &minus; λφψ] dn x = 0 (11.24)
</p>
<p>for all ψ &isin; C&infin;cpt(Ω).
If we substituteψ in place of ψ then the second term in (11.24) is λ times the L2
</p>
<p>inner product 〈φ,ψ〉. By the same token we could interpret the first term as an &ldquo;inner
product&rdquo; form of the Dirichlet energy (11.1). We will denote this by</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_8">http://dx.doi.org/10.1007/978-3-319-48936-0_8</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_8">http://dx.doi.org/10.1007/978-3-319-48936-0_8</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_10">http://dx.doi.org/10.1007/978-3-319-48936-0_10</a></div>
</div>
<div class="page"><p/>
<p>218 11 Variational Methods
</p>
<p>E[u, v] :=
&int;
</p>
<p>Ω
</p>
<p>&nabla;u &middot;&nabla;vdn x,
</p>
<p>so that E[u] = E[u, u]. The L2 and H 1 inner products are related by
</p>
<p>〈u, v〉H 1 = 〈u, v〉 + E[u, v].
</p>
<p>With this convention we can write the weak eigenvalue equation (11.24) in an equiv-
</p>
<p>alent form as
</p>
<p>E[φ,ψ] = λ 〈φ,ψ〉 (11.25)
</p>
<p>for all ψ &isin; C&infin;cpt(Ω).
For the minimization argument, it will prove convenient to enlarge the space of test
</p>
<p>functions from C&infin;cpt(Ω) to H
1
0 (Ω). To justify this, note that by definition a function
</p>
<p>v &isin; H 10 (Ω) can be approximated by a sequence of ψk &isin; C&infin;cpt(Ω) with respect to the
H 1 norm. This implies in particular that
</p>
<p>lim
k&rarr;&infin;
</p>
<p>E[φ,ψk] = E[φ, v], lim
k&rarr;&infin;
</p>
<p>〈φ,ψk〉 = 〈φ, v〉 .
</p>
<p>We can thus conclude that φ solves (11.24) if and only if
</p>
<p>E[φ, v] = λ 〈φ, v〉 (11.26)
</p>
<p>for all v &isin; H 10 (Ω).
The formulation of the eigenvalue equation as a minimization problem is known
</p>
<p>as Rayleigh&rsquo;s principle, after the physicist Lord Rayleigh. For v &isin; H 10 (Ω), v �= 0,
the ratio
</p>
<p>R[v] :=
E[v]
‖v‖22
</p>
<p>is called the Rayleigh quotient for v. Note also that if φ satisfies (11.26) then
</p>
<p>R[φ] = λ. (11.27)
</p>
<p>Furthermore, the Poincar&eacute; inequality (Theorem 11.2) shows that
</p>
<p>R[v] &ge;
1
</p>
<p>κ2
&gt; 0 (11.28)
</p>
<p>for v &isin; H 10 (Ω), v �= 0. This suggests that the smallest eigenvalue is related to the
Poincar&eacute; constant, and that we can locate it by minimizing R[&middot;].
</p>
<p>The argument for existence of a minimum is a little trickier than the analysis of
</p>
<p>the Dirichlet principle in Sect. 11.3. To understand why, note that
</p>
<p>R[cv] = R[v]</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 Eigenvalues by Minimization 219
</p>
<p>for c &isin; C\{0}, so the minimizing function is not unique. Therefore it is quite possible
to have a sequence that minimizes the Rayleigh quotient but does not converge in H 1.
</p>
<p>The tool that allows us to resolve this issue was developed by Franz Rellich in the
</p>
<p>early 20th century.
</p>
<p>Theorem 11.8 (Rellich&rsquo;s theorem) Suppose Ω &sub; Rn is a bounded domain and {vk}
is a sequence in H 10 (Ω) that satisfies a uniform bound
</p>
<p>‖vk‖H 1 &le; M
</p>
<p>for all k &isin; N. Then {vk} has a subsequence that converges in L2(Ω).
</p>
<p>It is important to note that Rellich&rsquo;s theorem refers to two different norms. The
</p>
<p>sequence is assumed bounded in the H 1 norm, and the subsequence is guaranteed
</p>
<p>to converge with respect to the L2 norm. This is crucial to the result and not a mere
</p>
<p>technicality. We will defer the proof of Rellich&rsquo;s theorem to Sect. 11.6. The remainder
</p>
<p>of this section is devoted to its application to the Rayleigh minimization scheme.
</p>
<p>Theorem 11.9 (First eigenvalue) There exists φ1 &isin; H 10 (Ω;R)&cap;C&infin;(Ω) satisfying
</p>
<p>&minus;�φ1 = λ1φ1
</p>
<p>for λ1 &gt; 0, such that
</p>
<p>λ1 &le; R[v] (11.29)
</p>
<p>for all v &isin; H 10 (Ω), v �= 0.
</p>
<p>Proof By (11.28) the values of R[&middot;] are bounded below by the Poincar&eacute; constant.
Therefore, the infimum
</p>
<p>λ1 := inf
v&isin;H 10 (Ω)\{0}
</p>
<p>R[v]
</p>
<p>exists and is strictly positive.
</p>
<p>By Lemma 2.1 there exists a sequence {vk} &sub; H 10 (Ω)\ {0} such that
</p>
<p>lim
k&rarr;&infin;
</p>
<p>R[vk] = λ1. (11.30)
</p>
<p>After rescaling each vk by a constant we can assume that ‖vk‖2 = 1, so that
</p>
<p>R[vk] = E[vk].
</p>
<p>The sequence of energies E[vk] is bounded by (11.30). This also implies that the
sequence {vk} is bounded with respect to the H 1 norm, because
</p>
<p>‖vk‖2H 1 = 1 + E[vk]
</p>
<p>by the relation (11.10).</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>220 11 Variational Methods
</p>
<p>According to Theorem 11.8 there exists a subsequence of {vk} that converges in
the L2 sense to some function φ1 &isin; L2. By restrict our attention to this subsequence
and relabeling if needed, we can assume that
</p>
<p>lim
k&rarr;&infin;
</p>
<p>‖vk &minus; φ1‖2 = 0. (11.31)
</p>
<p>The next goal is to improve this to a statement of convergence in H 1. We use the
</p>
<p>same strategy as in Sect. 11.3, starting from
</p>
<p>E
</p>
<p>[
</p>
<p>vk + vm
2
</p>
<p>]
</p>
<p>=
1
</p>
<p>2
E[vk] +
</p>
<p>1
</p>
<p>2
E[vm] &minus;
</p>
<p>1
</p>
<p>4
E[vk &minus; vm]. (11.32)
</p>
<p>By the definition of λ1,
</p>
<p>R
</p>
<p>[
</p>
<p>vk + vm
2
</p>
<p>]
</p>
<p>&ge; λ1.
</p>
<p>Therefore
</p>
<p>E
</p>
<p>[
</p>
<p>vk + vm
2
</p>
<p>]
</p>
<p>&ge;
λ1
</p>
<p>4
‖vk + vm‖22.
</p>
<p>Using this in (11.32) gives
</p>
<p>E[vk &minus; vm] &le; 2E[vk] + 2E[vm] &minus; λ1‖vk + vm‖22. (11.33)
</p>
<p>By (11.31), we have
</p>
<p>lim
k,m&rarr;&infin;
</p>
<p>‖vk + vm‖22 = ‖2φ1‖22 = 4,
</p>
<p>and by construction E[vk] &rarr; λ1. Hence (11.33) implies that
</p>
<p>lim
k,m&rarr;&infin;
</p>
<p>E[vk &minus; vm] &rarr; 0.
</p>
<p>Since we already know that {vk} converges in the L2 norm, we conclude that
</p>
<p>lim
k,m&rarr;&infin;
</p>
<p>‖vk &minus; vm‖H 1 &rarr; 0.
</p>
<p>That is, the sequence {vk} is Cauchy with respect to the H 1 norm.
By completeness {vk} converges with respect to the H 1 norm to some u &isin; H 10 (Ω).
</p>
<p>Since the L2 norm is bounded above by the H 1 norm, this means vk &rarr; u in the
L2 sense also. Hence u &equiv; φ1, which proves that φ1 &isin; H 10 (Ω). It then follows from
(11.30) that
</p>
<p>E[φ1] = R[φ1] = λ1.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 Eigenvalues by Minimization 221
</p>
<p>To see that this implies the weak solution condition, suppose w &isin; H 10 (Ω). Using
the inner product form of E[&middot;], we can expand
</p>
<p>E[φ1 + tw] = λ1 + 2t Re E[φ1, w] + t2E[w], (11.34)
</p>
<p>for t &isin; R. Similarly,
</p>
<p>‖φ1 + tw‖22 = 1 + 2t Re 〈φ1, w〉 + t2‖w‖22. (11.35)
</p>
<p>By the definition of λ1, the function t �&rarr; R[φ1 + tw] has a minimum at t = 0, so
that
</p>
<p>d
</p>
<p>dt
R[φ1 + tw]
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>t=0
= 0.
</p>
<p>Computing this derivative using (11.34) and (11.35) gives
</p>
<p>d
</p>
<p>dt
R[φ1 + tw]
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>t=0
= 2 Re E[φ1, w] &minus; 2λ1 Re 〈φ1, w〉 .
</p>
<p>We thus conclude that
</p>
<p>Re E[φ1, w] = λ1 Re 〈φ1, w〉
</p>
<p>for all w &isin; H 10 (Ω). By replacing w by iw, we can deduce also that
</p>
<p>Im E[φ1, w] = λ1 Im 〈φ1, w〉
</p>
<p>for all w &isin; H 10 (Ω). In combination these give (11.26), so φ1 is a weak solution of
</p>
<p>&minus;�φ1 = λ1φ1.
</p>
<p>In principle, φ1 could be complex-valued at this point, but since its real and imag-
</p>
<p>inary parts each satisfy (11.26) separately, we can select one of these to specialize
</p>
<p>to the real-valued case.
</p>
<p>To deduce the regularity of φ1, we apply Theorem 11.5 with f = λφ1. The fact
that φ1 &isin; H 10 (Ω) then implies that φ1 &isin; H 3loc(Ω). Starting from H 3loc(Ω) then gives
φ1 &isin; H 5loc(Ω), and so on. This inductive argument shows that φ1 &isin; H
</p>
<p>q
</p>
<p>loc(Ω) for each
</p>
<p>q &isin; N. We conclude that φ1 &isin; C&infin;(Ω) by Theorem 10.11. �
</p>
<p>It is clear that the λ1 produced in Theorem 11.9 is the smallest eigenvalue, since
</p>
<p>all eigenvalues occur as values of the Rayleigh quotient by (11.27). An example of φ1
is shown in Fig. 11.1. We will see in the exercises that the first eigenfunction cannot
</p>
<p>have zeros in Ω . This can be used to show that the first eigenfunction is unique up to
</p>
<p>a multiplicative constant, i.e., λ1 has multiplicity one.
</p>
<p>To find other eigenvalues, the strategy is to restrict to subspaces and then apply the
</p>
<p>same construction used forλ1. For a subset A &isin; L2(Ω) the orthogonal complement is</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_10">http://dx.doi.org/10.1007/978-3-319-48936-0_10</a></div>
</div>
<div class="page"><p/>
<p>222 11 Variational Methods
</p>
<p>Fig. 11.1 The first
</p>
<p>eigenfunction of a equilateral
</p>
<p>triangle domain
</p>
<p>A&perp; :=
{
</p>
<p>w &isin; L2(Ω); 〈w, v〉 = 0 for all v &isin; A
}
</p>
<p>.
</p>
<p>The orthogonal complement of a set is a subspace, by the linearity of the inner
</p>
<p>product.
</p>
<p>In the argument below we will consider subspaces of H 10 (Ω) of the form
</p>
<p>W = A&perp; &cap; H 10 (Ω),
</p>
<p>where A is a finite list of eigenfunctions. We claim that W is closed as a subspace
</p>
<p>of H 10 (Ω). To see this, suppose that wk &rarr; w in the H 1 norm, with wk &isin; W . Since
‖wk &minus; w‖2 &le; ‖wk &minus; w‖H 1 by (11.10), this implies that wk &rarr; w with respect to the
L2 norm also. Thus for v &isin; A,
</p>
<p>〈w, v〉 = lim
k&rarr;&infin;
</p>
<p>〈wk, v〉 = 0.
</p>
<p>This shows that w &isin; W . Therefore W is closed. By Lemma 7.8, this implies that W
is a Hilbert space with respect to the H 1 inner product
</p>
<p>Proof of Theorem 11.7 Let φ1 &isin; H 10 (Ω;R) be the eigenvector obtained in
Theorem 11.9, normalized so that ‖φ1‖2 = 1. The subspace
</p>
<p>W1 := {φ1}&perp; &cap; H 10 (Ω)
</p>
<p>is a Hilbert space with respect to the H 1 norm, by the remarks above.
</p>
<p>Applying the minimization procedure used in Theorem 11.9 to the restriction of
</p>
<p>the Rayleigh quotient to W1 gives φ2 &isin; W1 such that ‖φ2‖2 = 1 and
</p>
<p>λ2 := R[φ2] &le; R[w] (11.36)
</p>
<p>for all w &isin; W1\ {0}. By the same variational argument used for φ1, this implies that
</p>
<p>E[φ2, w] = λ2 〈φ2, w〉 (11.37)
</p>
<p>for all w &isin; W1.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
</div>
<div class="page"><p/>
<p>11.5 Eigenvalues by Minimization 223
</p>
<p>To extend this formula to the full weak solution condition, note that 〈φ2,φ1〉 = 0
because φ2 &isin; W1. The fact that φ1 satisfies (11.26) also gives
</p>
<p>E[φ1,φ2] = λ1 〈φ1,φ2〉 = 0. (11.38)
</p>
<p>Now consider a general v &isin; H 10 (Ω). For c := 〈v,φ1〉 we have
</p>
<p>〈v &minus; cφ1,φ1〉 = 0,
</p>
<p>so that v &minus; cφ1 &isin; W1. After setting w := v &minus; cφ1, we can expand
</p>
<p>E[φ2, v] &minus; λ2 〈φ2, v〉L2 = c
(
</p>
<p>E[φ2,φ1] &minus; λ2 〈φ2,φ1〉
)
</p>
<p>+ E[φ2, w] &minus; λ2 〈φ2, w〉
</p>
<p>The first line on the right is zero by (11.38) and the second is zero by (11.37). Thus
</p>
<p>(11.26) is satisfied for all v &isin; H 10 (Ω), showing that &minus;�φ2 = λ2φ2 in the weak
sense. By taking the real or imaginary part we can assume that φ2 is real-valued.
</p>
<p>Subsequent eigenvalues are obtained by repeating this process inductively. After
</p>
<p>k eigenfunctions have been found, we set
</p>
<p>Wk := {φ1, . . . ,φk}&perp; &cap; H 10 (Ω), (11.39)
</p>
<p>and minimize the Rayleigh quotient over Wk to find λk+1 and φk+1. Note that Wk
always contains nonzero vectors, because H 10 (Ω) is infinite-dimensional. The reg-
</p>
<p>ularity argument from the end of the proof of Theorem 11.9 applies to any solution
</p>
<p>of (11.26), so that φk &isin; C&infin;(Ω) for each k.
This process produces an orthonormal sequence of eigenfunctions {φk}with eigen-
</p>
<p>values satisfying
</p>
<p>λ1 &le; λ2 &le; λ3 &le; . . . .
</p>
<p>To see that λk &rarr; &infin;, note that ‖φk‖2 = 1 and R[φk] = λk by construction, so that
</p>
<p>‖φk‖2H 1 = 1 + λk . (11.40)
</p>
<p>Suppose the sequence {λk} is bounded. Then (11.40) shows that the sequence {φk} is
bounded with respect to the H 1 norm. Theorem 11.8 then implies that a subsequence
</p>
<p>of {φk} converges in L2(Ω). But the φk are orthonormal with respect to the L2 norm,
so that
</p>
<p>‖φk &minus; φm‖2 =
&radic;
</p>
<p>2
</p>
<p>for all k �= m. Convergence of a subsequence of {φk} is therefore impossible in
L2(Ω). This contradiction shows that {λk} cannot be bounded. Since the sequence
is increasing, this implies
</p>
<p>lim
k&rarr;&infin;
</p>
<p>λk = &infin;.</p>
<p/>
</div>
<div class="page"><p/>
<p>224 11 Variational Methods
</p>
<p>The final claim is that {φk} forms an orthonormal basis of L2(Ω). After obtaining
the full sequence from the inductive procedure, let us set
</p>
<p>W&infin; := {φ1,φ2, . . . }&perp; &cap; H 10 (Ω).
</p>
<p>Suppose that W&infin; contains a nonzero vector. Applying the Rayleigh quotient mini-
mization as above produces yet another eigenvalueλ. Since theλk&rsquo;s were constructed
</p>
<p>by minimizing the Rayleigh quotient on subspaces Wk &sup; W&infin;, this new eigenvalue
satisfiesλ &ge; λk for all k &isin; N. This is impossible becauseλk &rarr; &infin;. Hence W&infin; = {0}.
</p>
<p>In other words, the only vector in H 10 (Ω) that is orthogonal to all of the φk is
</p>
<p>0. Since C&infin;cpt(Ω) &sub; H 10 (Ω) and C&infin;cpt(Ω) is dense in L2(Ω) by Theorem 7.5, this
implies that the only vector in L2(Ω) that is orthogonal to all of the φk is 0. Hence
</p>
<p>{φk} is a basis by Theorem 7.10. �
</p>
<p>11.6 Sequential Compactness
</p>
<p>In this section we take up the proof of Rellich&rsquo;s theorem (Theorem 11.8). Results
</p>
<p>of this type, that force convergence of an approximating sequence, are a crucial
</p>
<p>component of variational strategies for PDE.
</p>
<p>In a normed vector space, a subset A is said to be sequentially compact if every
</p>
<p>sequence within A contains a subsequence converging to a limit in A. A fundamental
</p>
<p>result in analysis called the Bolzano-Weierstrass theorem (Theorem A.1) says that in
</p>
<p>R
n this is equivalent to the definition of compact given in Sect. 2.3. That is, a subset
</p>
<p>of Rn is sequentially compact if and only if it is closed and bounded.
</p>
<p>Rellich&rsquo;s theorem could be paraphrased as the statement that a closed and bounded
</p>
<p>subset of H 10 (Ω) is sequentially compact as a subset of L
2(Ω), provided we are
</p>
<p>careful about the two different norms referenced in this statement. Our strategy will
</p>
<p>be to reduce Rellich&rsquo;s theorem to an application of Bolzano-Weierstrass using Fourier
</p>
<p>series. We start with the periodic case.
</p>
<p>Theorem 11.10 Suppose that
{
</p>
<p>v j
}
</p>
<p>is a sequence in H 1(Tn) that satisfies a uniform
</p>
<p>bound
</p>
<p>‖v j‖H 1 &le; M (11.41)
</p>
<p>for all j &isin; N. Then {v j } has a subsequence that converges in L2(Tn).
</p>
<p>Proof The argument is essentially the same in any dimension, so let us take n = 1 to
simplify the notation. Suppose that
</p>
<p>{
</p>
<p>v j
}
</p>
<p>is a sequence in H 1(T) satisfying (11.41).
</p>
<p>The periodic Fourier coefficients are defined by
</p>
<p>ck[v j ] :=
1
</p>
<p>2π
</p>
<p>&int; π
</p>
<p>&minus;π
v j (x)e
</p>
<p>&minus;ikx dx (11.42)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>11.6 Sequential Compactness 225
</p>
<p>for k &isin; Z, with corresponding partial sums
</p>
<p>Sm[v j ] =
m
&sum;
</p>
<p>k=&minus;m
ck[v j ]eikx .
</p>
<p>Applying the Cauchy-Schwarz inequality (Theorem 7.1) to (11.42) gives
</p>
<p>∣
</p>
<p>∣ck[v j ]
∣
</p>
<p>∣ &le;
1
</p>
<p>&radic;
2π
</p>
<p>‖v j‖2.
</p>
<p>The assumption (11.41) implies also that the L2 norms ‖v j‖2 are bounded by M , so
that
</p>
<p>∣
</p>
<p>∣ck[ f j ]
∣
</p>
<p>∣ &le;
M
</p>
<p>&radic;
2π
</p>
<p>for all j &isin; N and k &isin; Z.
We start the process of finding a convergent subsequence by making the first
</p>
<p>few coefficients converge. The collection of points (c&minus;1[v j ], c0[v j ], c1[v j ]) forms
a bounded sequence in C3. Applying Bolzano-Weierstrass (Theorem A.1) to this
</p>
<p>sequence gives a point (a&minus;1, a0, a1) &sub; C3 and a subsequence
{
</p>
<p>v
(1)
j
</p>
<p>}
</p>
<p>&sub;
{
</p>
<p>v j
}
</p>
<p>,
</p>
<p>such that
</p>
<p>lim
j&rarr;&infin;
</p>
<p>ck
</p>
<p>[
</p>
<p>v
(1)
j
</p>
<p>]
</p>
<p>= ak
</p>
<p>for k = &minus;1, 0, 1. Since the partial sum S1 involves only these three coefficients, this
implies the uniform convergence
</p>
<p>lim
j&rarr;&infin;
</p>
<p>S1
</p>
<p>[
</p>
<p>v
(1)
j
</p>
<p>]
</p>
<p>=
1
</p>
<p>&sum;
</p>
<p>k=&minus;1
ake
</p>
<p>ikx ,
</p>
<p>which also gives convergence in the L2 sense.
</p>
<p>The same reasoning can be applied to the Fourier coefficients with k = &minus;2, . . . , 2
to obtain a subsequence
</p>
<p>{
</p>
<p>v
(2)
j
</p>
<p>}
</p>
<p>&sub;
{
</p>
<p>v
(1)
j
</p>
<p>}
</p>
<p>,
</p>
<p>such that
</p>
<p>lim
j&rarr;&infin;
</p>
<p>ck
</p>
<p>[
</p>
<p>v
(2)
j
</p>
<p>]
</p>
<p>= ak
</p>
<p>for k = &minus;2, . . . , 2. This process can be continued inductively to produce a family
of subsequences
</p>
<p>{
</p>
<p>v
(l)
j
</p>
<p>}
</p>
<p>such that</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
</div>
<div class="page"><p/>
<p>226 11 Variational Methods
</p>
<p>lim
j&rarr;&infin;
</p>
<p>Sl
</p>
<p>[
</p>
<p>v
(l)
j
</p>
<p>]
</p>
<p>=
l
</p>
<p>&sum;
</p>
<p>k=&minus;l
ake
</p>
<p>ikx (11.43)
</p>
<p>in L2(T).
</p>
<p>To complete the proof, set
</p>
<p>w j := v( j)j .
</p>
<p>Because w j is an element of the lth subsequence for l &le; j , we deduce from (11.43)
that
</p>
<p>lim
j&rarr;&infin;
</p>
<p>Sm
[
</p>
<p>w j
]
</p>
<p>=
m
&sum;
</p>
<p>k=&minus;m
ake
</p>
<p>ikx (11.44)
</p>
<p>in L2(T) for all m &isin; N.
We now claim that the sequence w j converges in L
</p>
<p>2(T). In order to deduce this
</p>
<p>from (11.44) we need to control the rate at which Sm[w j ] converges tow j as m &rarr; &infin;.
This is where the H 1 bound (11.41) becomes crucial. In terms of Fourier coefficients,
</p>
<p>the H 1 norm could be written
</p>
<p>‖ f ‖2H 1 =
&infin;
&sum;
</p>
<p>k=&minus;&infin;
(1 + k2) |ck[ f ]|2 .
</p>
<p>Hence (11.41) implies that
</p>
<p>&infin;
&sum;
</p>
<p>k=&minus;&infin;
(1 + k2)
</p>
<p>∣
</p>
<p>∣ck[w j ]
∣
</p>
<p>∣
</p>
<p>2 &le; M2
</p>
<p>for all j . This leads to an estimate:
</p>
<p>‖w j &minus; Sm[w j ]‖22 =
&sum;
</p>
<p>|k|&gt;m
</p>
<p>∣
</p>
<p>∣ck[w j ]
∣
</p>
<p>∣
</p>
<p>2
</p>
<p>&le;
&sum;
</p>
<p>|k|&gt;m
</p>
<p>1 + k2
</p>
<p>1 + m2
∣
</p>
<p>∣ck[w j ]
∣
</p>
<p>∣
</p>
<p>2
</p>
<p>&le;
M2
</p>
<p>1 + m2
,
</p>
<p>(11.45)
</p>
<p>independent of j .
</p>
<p>By the triangle inequality,
</p>
<p>‖wi &minus; w j‖2 &le; ‖wi &minus; Sm[wi ]‖2 + ‖Sm[wi ] &minus; Sm[w j ]‖2 + ‖w j &minus; Sm[w j ]‖2.
</p>
<p>Given ε &gt; 0, fix m large enough that</p>
<p/>
</div>
<div class="page"><p/>
<p>11.6 Sequential Compactness 227
</p>
<p>M
&radic;
</p>
<p>1 + m2
&lt; ε.
</p>
<p>By (11.45) this reduces our triangle estimate to
</p>
<p>‖wi &minus; w j‖2 &lt; ‖Sm[wi ] &minus; Sm[w j ]‖2 + 2ε. (11.46)
</p>
<p>Since Sm[w j ] converges in L2(T) by (11.44), we can choose N sufficiently large so
that
</p>
<p>‖Sm[wi ] &minus; Sm[w j ]‖L2 &lt; ε
</p>
<p>for all i, j &ge; N . By (11.46) this implies that
</p>
<p>‖wi &minus; w j‖2 &lt; 3ε
</p>
<p>for i. j &ge; N . This shows that the subsequence
{
</p>
<p>w j
}
</p>
<p>is Cauchy with respect to the L2
</p>
<p>norm. By completeness the subsequence converges in L2(T). �
</p>
<p>Proof of Theorem 11.8 We can assumeΩ &sub; (&minus;π,π)n , after rescaling if needed. By
Lemma 10.10, elements of H 10 (Ω) can be extended by zero to H
</p>
<p>1([&minus;π,π]n). We
can then make these functions periodic to give the inclusion
</p>
<p>H 10 (Ω) &sub; H 1(Tn). (11.47)
</p>
<p>Given a sequence
{
</p>
<p>v j
}
</p>
<p>&sub; H 10 (Ω) that is uniformly bounded in the H 1 norm, applying
Theorem 11.10 to the extension given by (11.47) gives a subsequence
</p>
<p>{
</p>
<p>w j
}
</p>
<p>that
</p>
<p>converges in L2(Tn). By construction the restriction of w j to [&minus;π,π]n vanishes
outside Ω , so this also gives a convergent subsequence in L2(Ω). �
</p>
<p>11.7 Estimation of Eigenvalues
</p>
<p>As we saw in Sect. 11.5, the Rayleigh principle for the eigenvalue problem is very
</p>
<p>useful as a theoretical tool. It also leads to some very practical applications in terms
</p>
<p>of estimating eigenvalues or calculating them numerically.
</p>
<p>The basic strategy is to exploit the formula for eigenvalues that appeared in the
</p>
<p>proof of Theorem 11.7. Assuming the Dirichlet eigenvalues {λk} of Ω are written in
increasing order and repeated according to multiplicity,
</p>
<p>λk = min
w&isin;Wk&minus;1\{0}
</p>
<p>R[w], (11.48)
</p>
<p>where
</p>
<p>Wk&minus;1 := {φ1, . . . ,φk&minus;1}&perp; &cap; H 10 (Ω),</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_10">http://dx.doi.org/10.1007/978-3-319-48936-0_10</a></div>
</div>
<div class="page"><p/>
<p>228 11 Variational Methods
</p>
<p>with φk the eigenfunction corresponding to λk The only problem with this formula
</p>
<p>is that determining the kth eigenfunction requires knowledge of the first k &minus;1 eigen-
functions. This issue is resolved by the following:
</p>
<p>Theorem 11.11 (Minimax principle) For a bounded domainΩ &sub; Rn , let�k denote
the set of all k-dimensional subspaces of H 10 (Ω). Let {λk} denote the sequence of
Dirichlet eigenvalues of Ω in increasing order. Then
</p>
<p>λk = min
V&isin;�k
</p>
<p>{
</p>
<p>max
u&isin;V \{0}
</p>
<p>R[u]
}
</p>
<p>(11.49)
</p>
<p>for each k &isin; N.
</p>
<p>Proof Let {φk} &sub; H 10 (Ω) denote the eigenfunction basis. By the weak solution
condition (11.25), orthonomality in L2(Ω) implies also that
</p>
<p>E[φi ,φ j ] =
{
</p>
<p>λi , i = j,
0, i �= j.
</p>
<p>(11.50)
</p>
<p>Let us set
</p>
<p>Veig = [φ1, . . . ,φk] &sub; H 10 (Ω),
</p>
<p>where [. . . ] denotes the linear span of a collection of vectors. For u &isin; Veig, expanded
as
</p>
<p>u =
k
</p>
<p>&sum;
</p>
<p>j=1
c jφk,
</p>
<p>we see from (11.50) that
</p>
<p>E[u] =
k
</p>
<p>&sum;
</p>
<p>j=1
λ j
</p>
<p>∣
</p>
<p>∣c j
∣
</p>
<p>∣
</p>
<p>2
.
</p>
<p>The Rayleigh quotient is thus
</p>
<p>R[u] =
&sum;k
</p>
<p>j=1 λ j
∣
</p>
<p>∣c j
∣
</p>
<p>∣
</p>
<p>2
</p>
<p>&sum;k
j=1
</p>
<p>∣
</p>
<p>∣c j
∣
</p>
<p>∣
</p>
<p>2
.
</p>
<p>Since λ1 &le; &middot; &middot; &middot; &le; λk by assumption, it is clear that
</p>
<p>R[u] &le; λk
</p>
<p>for u &isin; Veig\ {0}. Moreover, R[φk] = λk , so that
</p>
<p>max
u&isin;Veig\{0}
</p>
<p>R[u] = λk . (11.51)</p>
<p/>
</div>
<div class="page"><p/>
<p>11.7 Estimation of Eigenvalues 229
</p>
<p>Now consider a general subspace V &isin; �k . Since dim[φ1, . . . ,φk&minus;1] = k &minus; 1,
there exists a nonzero vector
</p>
<p>w &isin; V &cap; [φ1, . . . ,φk&minus;1]&perp;.
</p>
<p>Since w &isin; [φ1, . . . ,φk&minus;1]&perp;, it follows from (11.48) that
</p>
<p>λk &le; R[w].
</p>
<p>Hence,
</p>
<p>λk &le; max
u&isin;V \{0}
</p>
<p>R[u] (11.52)
</p>
<p>for V &isin; �k .
In combination, (11.51) and (11.52) show that the minimum on the right-hand
</p>
<p>side of (11.49) exists and is equal to λk . �
</p>
<p>As a sample application, we can use the minimax principle to compare eigenvalues
</p>
<p>of nested domains. This is possible because of the inclusion
</p>
<p>H 10 (Ω) &sub; H 10 (Ω̃), (11.53)
</p>
<p>provided by Lemma 10.10.
</p>
<p>Corollary 11.12 Consider two bounded domains in Rn satisfying Ω &sub; Ω̃ . Assum-
ing the Dirichlet eigenvalue sequences are arranged in increasing order,
</p>
<p>λk(Ω̃) &le; λk(Ω)
</p>
<p>for all k &isin; N.
</p>
<p>Proof Let {φk} &sub; H 10 (Ω) be the sequence of eigenfunctions of Ω . By (11.53),
we can consider [φ1, . . . ,φk] to be a subspace of H 10 (Ω̃). If �̃k denotes the set of
k-dimensional subspaces of H 10 (Ω̃), then this implies
</p>
<p>min
W&isin;�̃k
</p>
<p>{
</p>
<p>max
u&isin;W\{0}
</p>
<p>R[u]
}
</p>
<p>&le; max
u&isin;[φ1,...,φk ]\{0}
</p>
<p>R[u]
</p>
<p>By Theorem 11.11, the left-hand side is λk(Ω̃), while the right-hand side equals
</p>
<p>λk(Ω) by (11.51). �
</p>
<p>Another way to make use the Rayleigh and minimax principles is to approxi-
</p>
<p>mate eigenvalues by restricting subspaces of computationally simple functions within
</p>
<p>H 10 (Ω). This approach can give surprisingly good estimates even when the subspaces
</p>
<p>are small.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_10">http://dx.doi.org/10.1007/978-3-319-48936-0_10</a></div>
</div>
<div class="page"><p/>
<p>230 11 Variational Methods
</p>
<p>Example 11.13 On the unit disk D = {r &lt; 1} &sub; R2, consider the family of functions
</p>
<p>wα(r) := 1 &minus; rα
</p>
<p>for α &gt; 0. Because wα is radial, the energy can be computed by
</p>
<p>E[wα] =
&int; 1
</p>
<p>0
</p>
<p>(
</p>
<p>&part;wα
</p>
<p>&part;r
</p>
<p>)2
</p>
<p>2πr dr
</p>
<p>= 2πα2
&int; 1
</p>
<p>0
</p>
<p>r2α&minus;1 dr
</p>
<p>= πα.
</p>
<p>The L2 norm is
</p>
<p>‖wα‖22 =
&int; 1
</p>
<p>0
</p>
<p>(1 &minus; rα)2 2πr dr
</p>
<p>=
πα2
</p>
<p>2 + 3α+ α2
.
</p>
<p>Hence the Rayleigh quotient gives the bound
</p>
<p>λ1 &le; R[wα] =
2
</p>
<p>α
+ 3 + α
</p>
<p>for α &gt; 0. The optimal choice is α =
&radic;
</p>
<p>2, which gives
</p>
<p>λ1 &le; 3 + 2
&radic;
</p>
<p>2
.= 5.828. (11.54)
</p>
<p>Compare this to the exact value computed in Example 5.5 in terms of zeros of the
</p>
<p>Bessel J -function,
</p>
<p>λ1 = j20,1
.= 5.783. (11.55)
</p>
<p>The optimal choice ofwα gives a reasonable approximation to the true eigenfunction,
</p>
<p>as Fig. 11.2 demonstrates. &diams;
</p>
<p>Fig. 11.2 The Bessel
</p>
<p>function for the first
</p>
<p>eigenfunction and w&radic;2 1 &minus; r
&radic;
</p>
<p>2
</p>
<p>J0(j0,1r)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_5">http://dx.doi.org/10.1007/978-3-319-48936-0_5</a></div>
</div>
<div class="page"><p/>
<p>11.7 Estimation of Eigenvalues 231
</p>
<p>We can estimate higher eigenvalues and improve the accuracy by using a larger
</p>
<p>subspace. This computational strategy was introduced by Walter Ritz in 1909, and
</p>
<p>is referred to as the Rayleigh-Ritz method. Given a finite dimensional subspace A &sub;
H 10 (Ω), we let �k(A) denote the k-dimensional subspaces of A. The approximate
</p>
<p>eigenvalues associated to A are then given by
</p>
<p>ηk := min
W&isin;�k (A)
</p>
<p>{
</p>
<p>max
u&isin;W\{0}
</p>
<p>R[u]
}
</p>
<p>, (11.56)
</p>
<p>for k = 1, . . . , dim A.
Since A is finite dimensional, the calculation of (11.56) can be recast as a matrix
</p>
<p>eigenvalue problem. By the same arguments used in the proof of Theorem 11.7,
</p>
<p>the values ηk are associated to vectors vk &isin; A satisfying the approximate weak
eigenvalue equation,
</p>
<p>E[vk, w] = ηk 〈vk, w〉 (11.57)
</p>
<p>for all w &isin; A.
To interpret (11.57) as a matrix eigenvalue equation we fix a basis {w j }mj=1 for A.
</p>
<p>In terms of this basis, the energy functional and L2 inner product define matrices
</p>
<p>Ei j := E[wi , w j ], Fi j :=
&lang;
</p>
<p>wi , w j
&rang;
</p>
<p>. (11.58)
</p>
<p>If vk is expanded as
</p>
<p>vk =
m
&sum;
</p>
<p>j=1
c jw j ,
</p>
<p>then (11.57) is equivalent to
</p>
<p>m
&sum;
</p>
<p>i=1
ci (Ei j &minus; ηk Fi j ) = 0 (11.59)
</p>
<p>for j = 1, . . . ,m. This equation has a nontrivial solution only if the rows of the
matrix E &minus; ηk F are linearly dependent, which is equivalent to the vanishing of the
determinant. The values ηk can thus be calculated as the roots of a polynomial
</p>
<p>{η1, . . . , ηk} = {η : det(E &minus; ηF) = 0} . (11.60)
</p>
<p>In other words, the η j are the eigenvalues of E F
&minus;1.
</p>
<p>Example 11.14 Consider D as in Example 11.13, but now take the subspace A =
[w1, w2, w3], where
</p>
<p>w j (r) := 1 &minus; r2 j .</p>
<p/>
</div>
<div class="page"><p/>
<p>232 11 Variational Methods
</p>
<p>Straightforward computations give the matrices (11.58) as
</p>
<p>E =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>2π 8π
3
</p>
<p>3π
8π
3
</p>
<p>4π 24π
5
</p>
<p>3π 24π
5
</p>
<p>6π
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎠
, F =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>π
3
</p>
<p>5π
12
</p>
<p>9π
20
</p>
<p>5π
12
</p>
<p>8π
15
</p>
<p>7π
12
</p>
<p>9π
20
</p>
<p>7π
12
</p>
<p>9π
4
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎠
.
</p>
<p>The roots of det(E &minus; ηF) are
</p>
<p>η1
.= 5.783, η2
</p>
<p>.= 30.712, η3
.= 113.505.
</p>
<p>The estimate η1 matches the exact value (11.55) very closely; in fact,
</p>
<p>|λ1 &minus; η1| &asymp; 10&minus;6.
</p>
<p>The second value η2 is a reasonable approximation to
</p>
<p>λ3 = j20,2
.= 30.471.
</p>
<p>However, we missed the second eigenvalue
</p>
<p>λ2 = j21,1
.= 14.682.
</p>
<p>The problem is that the space A consists entirely of radial functions, so that the
</p>
<p>second eigenfunction,
</p>
<p>φ2(r, θ) = J1(
&radic;
</p>
<p>λ2r)e
iθ,
</p>
<p>is orthogonal to A. &diams;
</p>
<p>The missing eigenvalue in Example 11.14 illustrates a potential flaw in the
</p>
<p>Rayleigh-Ritz scheme. We need to make sure the subspace A covers H 10 (Ω) suf-
</p>
<p>ficiently well in order to catch all low-lying eigenvalues. At the same time, we also
</p>
<p>need a means of producing this subspace efficiently. The finite element method is an
</p>
<p>approach that addresses both of these concerns.
</p>
<p>To set up the finite element method we subdivideΩ into small polygonal domains,
</p>
<p>producing a mesh (or triangulation). Figure 11.3 illustrates a mesh for a two-
</p>
<p>dimensional domain. To each interior vertex is associated a piecewise linear function
</p>
<p>called an &ldquo;element&rdquo; that is positive at the vertex and decays linearly to zero on the
</p>
<p>neighboring faces. The span of these elements defines a subspace A with dimension
</p>
<p>equal to the number of interior vertices.
</p>
<p>Example 11.15 Let us consider the domain (0,π), for which the exact Dirichlet
</p>
<p>eigenfunctions are φk(x) = sin kx for k &isin; N, with λk = k2. Define a mesh by
subdividing (0,π) into m + 1 intervals of length π/(m + 1). To the j th vertex we
associate the element</p>
<p/>
</div>
<div class="page"><p/>
<p>11.7 Estimation of Eigenvalues 233
</p>
<p>Fig. 11.3 Discrete mesh for
</p>
<p>an oval domain in R2
</p>
<p>Fig. 11.4 Piecewise linear
</p>
<p>elements for (0,π) with
</p>
<p>m = 3
w1 w2 w3
</p>
<p>w j (x) =
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>x &minus; j&minus;1
m+1π,
</p>
<p>j&minus;1
m+1π &le; x &le;
</p>
<p>j
</p>
<p>m+1π,
</p>
<p>&minus;x + j+1
m+1π,
</p>
<p>j
</p>
<p>m+1π &le; x &le;
j+1
</p>
<p>m+1π,
</p>
<p>0, otherwise,
</p>
<p>for j = 1, . . .m. These elements are illustrated in Fig. 11.4.
The energy and inner product matrices can be computed by straightforward inte-
</p>
<p>grals,
</p>
<p>Ei j =
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>2π
m+1 i = j,
&minus; π
</p>
<p>m+1 |i &minus; j | = 1,
0 otherwise,
</p>
<p>Fi j =
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>2π3
</p>
<p>3(m+1)3 i = j,
π3
</p>
<p>6(m+1)3 |i &minus; j | = 1,
0 otherwise.
</p>
<p>Table 11.1 shows the resulting approximate eigenvalues as a function of the number
</p>
<p>of elements m. &diams;
</p>
<p>The finite element method produces approximate eigenfunctions as well as eigen-
</p>
<p>values. Once the eigenvalue ηk is determined, the coefficients c j can be computed
</p>
<p>from (11.59). These coefficients represent the eigenvectors of (E F&minus;1)T in the basis
{
</p>
<p>w j
}
</p>
<p>. The function vk =
&sum;m
</p>
<p>j=1 c jw j is best approximation within the subspace A
to the true eigenfunction φk . Figure 11.5 shows some approximate eigenfunctions
</p>
<p>determined by the calculations in Example 11.15.
</p>
<p>The Rayleigh-Ritz strategy proves to be quite adaptable to more complicated
</p>
<p>problems. The procedure consists of: (1) generating a mesh for a given domain,
</p>
<p>(2) computing the matrix entries Ei j and Fi j corresponding to the elements of the</p>
<p/>
</div>
<div class="page"><p/>
<p>234 11 Variational Methods
</p>
<p>Table 11.1 Approximate Dirichlet eigenvalues for (0,π) computed using m elements
</p>
<p>m 3 10 25 50 exact
</p>
<p>η1 1.05 1.01 1.00 1.00 1
</p>
<p>η2 4.86 4.11 4.02 4.01 4
</p>
<p>η3 12. 84 9.56 9.10 9.03 9
</p>
<p>η4 &ndash; 17.80 16.31 16.08 16
</p>
<p>η5 &ndash; 29.45 25.77 25.20 25
</p>
<p>m=10 m=20 m=30
</p>
<p>Fig. 11.5 Piecewise linear approximations to the eigenfunction φ3(x) = cos 3x
</p>
<p>Fig. 11.6 A approximation
</p>
<p>of the eigenfunction φ6 for a
</p>
<p>star-shaped domain
</p>
<p>mesh, and (3) solving a finite-dimensional eigenvalue problem. A two-dimensional
</p>
<p>example is shown in Fig. 11.6.
</p>
<p>11.8 Euler-Lagrange Equations
</p>
<p>In the mid-18th century, Lagrange and Euler jointly developed a framework for
</p>
<p>expressing problems in classical mechanics in terms of the minimization of an action
</p>
<p>functional. Euler coined the term calculus of variations to describe this approach,
</p>
<p>which proved adaptable to a great variety of problems.
</p>
<p>In the original classical mechanics setting, the action functional was the integral
</p>
<p>of a Lagrangian function, defined as kinetic energy minus potential energy. These
</p>
<p>might be energies of a single particle or a system.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.8 Euler-Lagrange Equations 235
</p>
<p>In a typical PDE application on a bounded domain Ω &sub; Rn , we take the
Lagrangian L( p, w, x) to be a smooth function
</p>
<p>L : Rn &times; R &times;Ω&rarr; R.
</p>
<p>The action functional is defined by
</p>
<p>S[w] :=
&int;
</p>
<p>Ω
</p>
<p>L(&nabla;w(x), w(x), x) dn x
</p>
<p>for w &isin; C1(Ω;R). Suppose that u is a critical point of S, in the sense that
</p>
<p>d
</p>
<p>dt
S[u + tψ]
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>t=0
= 0
</p>
<p>for every ψ &isin; C&infin;cpt(Ω). This implies a (possibly nonlinear) PDE for u, called the
Euler-Lagrange equation of L .
</p>
<p>The Dirichlet principle gives the most basic example of this setup. For
</p>
<p>L( p, w, x) :=
| p|2
</p>
<p>2
(11.61)
</p>
<p>the action functional is the Dirichlet energy E[w], and the Euler-Lagrange equation is
the Laplace equation. To formulate the Poisson equation, we modify the Lagrangian
</p>
<p>to include the forcing term f &isin; L2(Ω;R),
</p>
<p>L( p, w, x) :=
| p|2
</p>
<p>2
&minus; f w. (11.62)
</p>
<p>In this case the action is the functional D f [w] defined in (11.6).
A classic nonlinear example is the surface area minimization problem. For Ω &isin;
</p>
<p>R
2, the graph of a function w : Ω &rarr; R defines a surface in R3. According to (2.8),
</p>
<p>the surface area of this patch of surface is given by
</p>
<p>A[w] :=
&int;
</p>
<p>Ω
</p>
<p>&radic;
</p>
<p>1 + |&nabla;w|2 d2x.
</p>
<p>We can interpret this as an action functional corresponding to the Lagrangian
</p>
<p>L( p, w, x) :=
&radic;
</p>
<p>1 + | p|2.
</p>
<p>For f : &part;Ω &rarr; R, the problem of minimizing A[w] under the constraint w|&part;Ω = f
was first studied by Lagrange. But historically this is called the Plateau problem
</p>
<p>after the 19th century physicist Joseph Plateau who conduct experiments on minimal
</p>
<p>surfaces using soap films.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>236 11 Variational Methods
</p>
<p>Let us work out the Euler-Lagrange equation for the surface area functional. For
</p>
<p>ψ &isin; C&infin;cpt(Ω), we have
</p>
<p>d
</p>
<p>dt
A[u + tψ]
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>t=0
=
</p>
<p>d
</p>
<p>dt
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>&radic;
</p>
<p>1 + |&nabla;u + t&nabla;ψ|2 d2x
∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>t=0
</p>
<p>=
&int;
</p>
<p>Ω
</p>
<p>&nabla;u &middot; &nabla;ψ
&radic;
</p>
<p>1 + |&nabla;u|2
d2x.
</p>
<p>By Green&rsquo;s first identity (Theorem 2.10), and the fact thatψ vanishes near the bound-
</p>
<p>ary,
</p>
<p>d
</p>
<p>dt
A[u + tψ]
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>t=0
=
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>ψ&nabla; &middot;
(
</p>
<p>&nabla;u
&radic;
</p>
<p>1 + |&nabla;u|2
</p>
<p>)
</p>
<p>d2x.
</p>
<p>Setting this equal to zero for all ψ gives the Euler-Lagrange equation
</p>
<p>&nabla; &middot;
(
</p>
<p>&nabla;u
&radic;
</p>
<p>1 + |&nabla;u|2
</p>
<p>)
</p>
<p>= 0. (11.63)
</p>
<p>This nonlinear PDE is called the minimal surface equation.
</p>
<p>There is a well-developed existence and regularity theory for general Euler-
</p>
<p>Lagrange equations, but this is too technical for us to go into here. In many cases, the
</p>
<p>finite element method can be used to effectively reduce the numerical approximation
</p>
<p>of solutions to linear algebra. Figure 11.7 shows a solution of the minimal surface
</p>
<p>equation calculated using finite elements.
</p>
<p>Fig. 11.7 The minimal
</p>
<p>surface over the unit disk
</p>
<p>associated to the boundary
</p>
<p>function f (θ) = cos 6θ</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>11.9 Exercises 237
</p>
<p>11.9 Exercises
</p>
<p>11.1 For a finite-dimensional subspace A &sub; H 10 (Ω), suppose we approximate the
solution of the Poisson equation (11.4) for f &isin; L2(Ω) by setting
</p>
<p>u = min
w&isin;A
</p>
<p>D f [w].
</p>
<p>Given a basis w1, . . . wm for A (not necessarily orthonormal), set u =
&sum;m
</p>
<p>i=1 ciwi .
Find equation an equation for (c1, . . . , cm) in terms of f and the matrices E and F
</p>
<p>defined in (11.58).
</p>
<p>11.2 To demonstrate the role that ellipticity plays in Theorem 11.5, consider the
</p>
<p>operator L = r2� on the unit ball B = {r &lt; 1} &sub; R3, where r := |x|. For
f &isin; L2(B), a weak solution of the equation Lu = f with Dirichlet boundary
conditions is defined as a function u &isin; H 10 (B) satisfying
</p>
<p>&int;
</p>
<p>B
</p>
<p>[
</p>
<p>&nabla;u &middot; &nabla;(r2ψ)+ f ψ
]
</p>
<p>d2x = 0 (11.64)
</p>
<p>for all ψ &isin; C&infin;cpt(B).
</p>
<p>(a) Compute the weak partial derivatives of the function log r and show that log r &isin;
H 10 (B).
</p>
<p>(b) Show that u(x) = log r satisfies (11.64) with f = 1. Note that even though &part;B
and f are C&infin;, the solution u is not even H 2. (It is not a coincidence that the
singularity of u occurs at the point where ellipticity of the operator fails.)
</p>
<p>11.3 LetΩ &sub; R2 be the equilateral triangle with vertices (0, 0), (2, 0), and (1,
&radic;
</p>
<p>3).
</p>
<p>Define w &isin; H 10 (Ω) to be the piecewise linear function whose graph forms a tetrahe-
dron over Ω , with the top vertex at (1, 1/
</p>
<p>&radic;
3, 1/
</p>
<p>&radic;
3). Approximate the first eigen-
</p>
<p>value by computing the Rayleigh quotient R[w]. (For comparison, the exact value
is λ1 = 4π
</p>
<p>2
</p>
<p>3
; this corresponds to the eigenfunction shown in Fig. 11.1.)
</p>
<p>11.4 Let B3 denote the unit ball {r &lt; 1} &sub; R3. Find an upper bound on the first
eigenvalue by computing the Rayleigh quotient of the radial function w(r) = 1&minus; r .
(Compare your answer to the exact value λ1 = π2 found in Exercise 5.8.)
</p>
<p>11.5 For a bounded domain Ω &sub; Rn , let φ1 &isin; H 10 (Ω;R) &cap; C&infin;(Ω) be the first
eigenfunction as obtained in Theorem 11.9, normalized so that ‖φ1‖2 = 1. In this
problem we will show that φ1 has no zeros in Ω and that this eigenfunction is unique
</p>
<p>up to a multiplicative constant.
</p>
<p>(a) Set
</p>
<p>φ&plusmn;(x) := max {&plusmn;φ1(x), 0}</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_5">http://dx.doi.org/10.1007/978-3-319-48936-0_5</a></div>
</div>
<div class="page"><p/>
<p>238 11 Variational Methods
</p>
<p>for x &isin; Ω , and show that
φ1 = φ+ &minus; φ&minus;
</p>
<p>with φ&plusmn; &ge; 0 and φ+φ&minus; &equiv; 0.
(b) Note Lemma 10.10 implies that φ&plusmn; &isin; H 10 (Ω;R). Assuming that φ1 is normal-
</p>
<p>ized ‖φ1‖2 = 1, show that
</p>
<p>‖φ+‖2 + ‖φ&minus;‖2 = 1,
E[φ+] + E[φ&minus;] = λ1.
</p>
<p>(c) Since λ1 minimizes the Rayleigh quotient,
</p>
<p>E[φ&plusmn;] &ge; λ1‖φ&plusmn;‖2.
</p>
<p>Use this, together with the fact that E[φ1] = λ1, to deduce that
</p>
<p>E[φ&plusmn;] = λ1‖φ&plusmn;‖2.
</p>
<p>Hence, by the proof of Theorem 11.9, φ&plusmn; &isin; C&infin;(Ω) and
</p>
<p>&minus;�φ&plusmn; = λ1φ&plusmn;. (11.65)
</p>
<p>(d) Use the strong maximum principle of Theorem 9.5 to deduce from (11.65) that
</p>
<p>if φ&plusmn; has a zero within Ω then φ&plusmn; &equiv; 0. Conclude that φ1 has no zeros in Ω .
(e) If u &isin; H 10 (Ω;R) &cap; C&infin;(Ω) is some other eigenfunction with eigenvalue λ1,
</p>
<p>then u &minus; cφ1 is also an eigenfunction for each c &isin; R. Show that c can be chosen
so that u &minus; cφ1 has a zero in Ω . Conclude that u &equiv; cφ1.
</p>
<p>11.6 Determine the Euler-Lagrange equations on Ω &sub; Rn corresponding to the
following Lagrangians:
</p>
<p>(a) L( p, w, x) =
1
</p>
<p>2
| p|2 +
</p>
<p>1
</p>
<p>2
a(x)w2 for a &isin; C0(Ω).
</p>
<p>(b) L( p, w, x) =
1
</p>
<p>2
</p>
<p>n
&sum;
</p>
<p>i, j=1
ai j (x)pi p j with ai j &isin; C1(Ω).
</p>
<p>(c) L( p, w, x) = | p|.
(d) L( p, w, x) =
</p>
<p>1
</p>
<p>2
| p|2 + F(w) for F &isin; C1(R).</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_10">http://dx.doi.org/10.1007/978-3-319-48936-0_10</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0">http://dx.doi.org/10.1007/978-3-319-48936-0</a></div>
</div>
<div class="page"><p/>
<p>Chapter 12
</p>
<p>Distributions
</p>
<p>To define weak derivatives in Chap. 10, we measured the values of a function f &isin;
</p>
<p>L1loc(Ω) by integrating against test functions. One way to interpret this process is
</p>
<p>that f defines a functional C&infin;cpt(Ω) &rarr; C given by
</p>
<p>ψ �&rarr;
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>f ψ dnx.
</p>
<p>A distribution onΩ &sub; Rn is a more general functionalC&infin;cpt(Ω) &rarr; C, not necessarily
</p>
<p>expressible as an integral. To qualify as a distribution, a functional is required to
</p>
<p>satisfy conditions that insure that weak derivatives and other basic operations are
</p>
<p>well defined.
</p>
<p>As with weak derivatives, the concept of a distribution was inspired by idealized
</p>
<p>situations in physics. Indeed, the term &ldquo;distribution&rdquo; was inspired by charge distrib-
</p>
<p>utions in electrostatics, an example that we will discuss in Sect. 12.1. Distributions
</p>
<p>generalize the notion of weak solutions, in the sense that every function in L1loc(Ω)
</p>
<p>also defines a distribution. The trade-off for the increased generality is that some
</p>
<p>basic operations for functions cannot be applied to distributions. The product of two
</p>
<p>distributions is not generally well defined, for example.
</p>
<p>There are some technicalities in the mathematical theory of distributions that
</p>
<p>require more background on the topology of function spaces than we assume for this
</p>
<p>text. We will treat these technicalities rather lightly; our focus will be on exploring
</p>
<p>the PDE applications.
</p>
<p>12.1 Model Problem: Coulomb&rsquo;s Law
</p>
<p>Coulomb&rsquo;s law of electrostatics is an empirical observation developed by 18th century
</p>
<p>physicist Charles-Augustin de Coulomb. It says that a particle with electric charge
</p>
<p>q0, located at the origin, generates an electric field given by
</p>
<p>&copy; Springer International Publishing AG 2016
</p>
<p>D. Borthwick, Introduction to Partial Differential Equations,
</p>
<p>Universitext, DOI 10.1007/978-3-319-48936-0_12
</p>
<p>239</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_10">http://dx.doi.org/10.1007/978-3-319-48936-0_10</a></div>
</div>
<div class="page"><p/>
<p>240 12 Distributions
</p>
<p>E(x) =
kq0x
</p>
<p>|x|3
, (12.1)
</p>
<p>where k (Coulomb&rsquo;s constant) depends on the properties of the medium surrounding
</p>
<p>the charges.
</p>
<p>In Sect. 11.1 we discussed another important empirical law of electrostatics,
</p>
<p>Gauss&rsquo;s law. With the same convention for physical constants as in (12.1), the dif-
</p>
<p>ferential form of the law says that
</p>
<p>&nabla; &middot; E = 4πkρ, (12.2)
</p>
<p>where ρ is the charge per unit volume as a function of position.
</p>
<p>These two empirical laws present something of a mathematical conundrum, in
</p>
<p>that the field specified by Coulomb is not differentiable at x = 0, not even weakly.
</p>
<p>On the other hand, for x �= 0,
</p>
<p>&nabla; &middot;
x
</p>
<p>r3
=
</p>
<p>&nabla; &middot; x
</p>
<p>r3
&minus;
</p>
<p>3x
</p>
<p>r4
&middot; &nabla;r
</p>
<p>=
3
</p>
<p>r3
&minus;
</p>
<p>3x
</p>
<p>r4
&middot;
</p>
<p>x
</p>
<p>r
</p>
<p>= 0
</p>
<p>(12.3)
</p>
<p>This is consistent with (12.2), in that Coulomb assumes the charge density is zero
</p>
<p>for x �= 0. However, if a function in L1loc vanishes except at a single point, then
</p>
<p>that function is zero by the equivalence (7.6). Thus a point charge density has no
</p>
<p>meaningful interpretation as a locally integrable function.
</p>
<p>To reconcile (12.1) with Gauss&rsquo;s law, let us consider the weak form of (12.2),
</p>
<p>&int;
</p>
<p>R3
</p>
<p>E &middot; &nabla;ψ d3x = &minus;4πk
</p>
<p>&int;
</p>
<p>R3
</p>
<p>ρψ d3x (12.4)
</p>
<p>for all ψ &isin; C&infin;cpt(R
3). The left side of (12.4) is well defined because the components
</p>
<p>of E are locally integrable.
</p>
<p>Since the Coulomb field is smooth away from the origin, we can integrate by parts
</p>
<p>as long as we exclude the origin from the region of integration by writing the integral
</p>
<p>as a limit,
&int;
</p>
<p>R3
</p>
<p>E &middot; &nabla;ψ d3x = lim
ε&rarr;0
</p>
<p>&int;
</p>
<p>{r&ge;ε}
</p>
<p>E &middot; &nabla;ψ d3x. (12.5)
</p>
<p>The region {r &ge; ε} has boundary given by the sphere {r = ε}. In this case the
</p>
<p>&ldquo;outward&rdquo; unit normal is a radial unit vector pointing towards the origin,
</p>
<p>ν = &minus;
x
</p>
<p>r
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>r=ε
. (12.6)
</p>
<p>By the divergence theorem (Theorem 2.6), and the fact that &nabla; &middot; E = 0 for r &gt; 0 by
</p>
<p>(12.3),</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_11">http://dx.doi.org/10.1007/978-3-319-48936-0_11</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>12.1 Model Problem: Coulomb&rsquo;s Law 241
</p>
<p>&int;
</p>
<p>{r&ge;ε}
</p>
<p>E &middot; &nabla;ψ d3x =
</p>
<p>&int;
</p>
<p>{r=ε}
</p>
<p>ν &middot; E ψ dS.
</p>
<p>Hence by (12.1) and (12.6),
</p>
<p>&int;
</p>
<p>{r&ge;ε}
</p>
<p>E &middot; &nabla;ψ d3x = &minus;
</p>
<p>&int;
</p>
<p>{r=ε}
</p>
<p>kq0
</p>
<p>ε2
ψ dS
</p>
<p>Taking ε &rarr; 0 now gives
</p>
<p>&int;
</p>
<p>R3
</p>
<p>E &middot; &nabla;ψ d3x = &minus; lim
ε&rarr;0
</p>
<p>&int;
</p>
<p>{r=ε}
</p>
<p>kq0
</p>
<p>ε2
ψ dS. (12.7)
</p>
<p>Because ψ is continuous, the average of ψ over the sphere {r = ε} approaches ψ(0)
</p>
<p>as ε &rarr; 0, i.e.,
</p>
<p>lim
ε&rarr;0
</p>
<p>1
</p>
<p>4πε2
</p>
<p>&int;
</p>
<p>{r=ε}
</p>
<p>ψ dS = ψ(0).
</p>
<p>Applying this to (12.7) gives
</p>
<p>&int;
</p>
<p>R3
</p>
<p>kqx
</p>
<p>r3
&middot; &nabla;ψ d3x = &minus;4πkq0ψ(0). (12.8)
</p>
<p>The weak condition (12.4) thus requires that
</p>
<p>&int;
</p>
<p>R3
</p>
<p>ρψ d3x = q0ψ(0),
</p>
<p>for every ψ &isin; C&infin;cpt(R
3). This is consistent with the physical interpretation of ρ as a
</p>
<p>charge located exactly at the origin.
</p>
<p>The concept of a &ldquo;point density&rdquo; was widely used in physics applications in the
</p>
<p>18th and 19th centuries. In a 1930 book on quantum mechanics, the physicist Paul
</p>
<p>Dirac described such densities in terms of a delta function δ(x), whose defining
</p>
<p>property is that
&int;
</p>
<p>Rn
</p>
<p>f (x)δ(x) dnx := f (0), (12.9)
</p>
<p>for a continuous function f . This terminology and notation are potentially mislead-
</p>
<p>ing, because δ is not a function and (12.9) is not actually an integral. However, Dirac&rsquo;s
</p>
<p>formulation hints at the proper mathematical interpretation, which is that δ should
</p>
<p>be understood as a functional f �&rarr; f (0).
</p>
<p>If we accept the intuitive definition of the delta function for the moment, then we
</p>
<p>can interpret the calculation (12.8) as showing that
</p>
<p>&nabla; &middot;
x
</p>
<p>r3
= 4πδ. (12.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>242 12 Distributions
</p>
<p>12.2 The Space of Distributions
</p>
<p>A distribution on a domain Ω &sub; Rn is a continuous linear functional C&infin;cpt(Ω) &rarr; C.
</p>
<p>The map defined by a distribution u is usually written as a pairing of u with a test
</p>
<p>function, i.e.,
</p>
<p>ψ �&rarr; (u,ψ) &isin; C (12.11)
</p>
<p>for ψ &isin; C&infin;cpt(Ω). Linearity means that
</p>
<p>(u, c1ψ1 + c2ψ2) = c1( f,ψ1)+ c2( f,ψ2),
</p>
<p>for all c1, c2 &isin; C and ψ1,ψ2 &isin; C
&infin;
cpt(Ω).
</p>
<p>The definition of distribution also includes the word &ldquo;continuous&rdquo;. To define con-
</p>
<p>tinuity for functionals we must first specify what convergence means inC&infin;cpt(Ω). The
</p>
<p>standard definition is that for a sequence {ψk} to converge to ψ in C
&infin;
cpt(Ω) means that
</p>
<p>all ψk have support in some fixed compact set K &sub; Ω , and the sequence of functions
</p>
<p>and all sequences of partial derivatives converge uniformly on K . Continuity of the
</p>
<p>functional (12.11) is then defined by the condition that convergence of a sequence
</p>
<p>ψk &rarr; ψ in C
&infin;
cpt(Ω) implies that
</p>
<p>lim
k&rarr;&infin;
</p>
<p>(u,ψk) = (u,ψ). (12.12)
</p>
<p>In finite dimensions continuity is implied by linearity. That is not the case here, but
</p>
<p>in practice it is quite difficult to come up with a functional that is linear but not
</p>
<p>continuous.
</p>
<p>The set of distributions on Ω forms a vector space denoted by D&prime;(Ω). Linear
</p>
<p>combinations of distributions are defined in the obvious way by
</p>
<p>(c1u1 + c2u2,ψ) := c1(u1,ψ)+ c2(u2,ψ),
</p>
<p>for u1, u2 &isin; D
&prime;(Ω) and c1, c2 &isin; C. The mathematical theory of distributions was
</p>
<p>developed independently in the mid-20th century by Sergei Sobolev and Laurent
</p>
<p>Schwartz. Schwartz used D as a notation for C&infin;cpt, and the prime accent on D
&prime; comes
</p>
<p>from the notation for the dual of a vector space in linear algebra.
</p>
<p>A locally integrable function f &isin; L1loc(Ω) defines a distribution through the
</p>
<p>integral pairing
</p>
<p>( f,ψ) :=
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>f ψ dnx. (12.13)
</p>
<p>Under this convention there is an inclusion
</p>
<p>L1loc(Ω) &sub; D
&prime;(Ω).
</p>
<p>In particular, all L p functions can be interpreted as distributions.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 The Space of Distributions 243
</p>
<p>As we saw with the point charge density in Sect. 12.1, not all distributions are
</p>
<p>given by functions. We use the notation δx for the delta function centered at x &isin; Ω ,
</p>
<p>defined by
</p>
<p>(δx,ψ) := ψ(x). (12.14)
</p>
<p>By convention the subscript is dropped for x = 0, i.e., δ := δ0.
</p>
<p>Multiplication by smooth functions preserves the space C&infin;cpt(Ω). Therefore it
</p>
<p>makes sense to multiply a distribution u &isin; D&prime;(Ω) by a function f &isin; C&infin;(Ω). The
</p>
<p>product distribution is defined by
</p>
<p>( f u,ψ) := (u, f φ).
</p>
<p>It does not make sense, however, to multiply two distributions together. This fact was
</p>
<p>intuitively clear in early applications: the product of two charge densities makes no
</p>
<p>physical sense.
</p>
<p>Convergence of a sequence of distributions is defined in a very straightforward
</p>
<p>way. We say that uk &rarr; u in D
&prime;(Ω) if
</p>
<p>lim
k&rarr;&infin;
</p>
<p>(uk,ψ) = (u,ψ)
</p>
<p>for allψ &isin; C&infin;cpt(Ω). All distributions can in fact be approximated by smooth functions
</p>
<p>by such a limit, although we are not equipped to prove that here. We will present
</p>
<p>one useful special case, a construction of the delta function as a limit of integrable
</p>
<p>functions.
</p>
<p>Lemma 12.1 Given f &isin; L1(Rn) satisfying
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>f dnx = 1, (12.15)
</p>
<p>define the rescaled function,
</p>
<p>fa(x) := a
n f (ax)
</p>
<p>for a &gt; 0. Then
</p>
<p>lim
a&rarr;&infin;
</p>
<p>fa = δ, (12.16)
</p>
<p>as a distributional limit.
</p>
<p>Proof Forψ &isin; C&infin;cpt(R
n)we can evaluate the pairing with fa using a change variables,
</p>
<p>( fa,ψ) =
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>an f (ax)ψ(x) dnx
</p>
<p>=
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>f (x)ψ(x/a) dnx.</p>
<p/>
</div>
<div class="page"><p/>
<p>244 12 Distributions
</p>
<p>By the assumption (12.15), we can also write
</p>
<p>ψ(0) =
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>f (x)ψ(0) dnx,
</p>
<p>which gives the estimate
</p>
<p>∣
</p>
<p>∣( fa,ψ)&minus; ψ(0)
∣
</p>
<p>∣ &le;
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>| f (x)|
∣
</p>
<p>∣ψ(x/a)&minus; ψ(0)
∣
</p>
<p>∣ dnx. (12.17)
</p>
<p>Given ε &gt; 0, the fact that f is integrable implies that exists R sufficiently large
</p>
<p>so that
&int;
</p>
<p>|x|&ge;R
</p>
<p>| f | dnx &lt; ε. (12.18)
</p>
<p>By the continuity of ψ we can also choose δ &gt; 0 so that
</p>
<p>|ψ(x)&minus; ψ(0)| &lt; ε
</p>
<p>for |x| &lt; δ. For a &ge; R/δ this implies that
</p>
<p>∣
</p>
<p>∣ψ(x/a)&minus; ψ(0)
∣
</p>
<p>∣ &lt; ε (12.19)
</p>
<p>for all |x| &le; R. Using (12.18) and (12.19) to estimate the difference (12.17) gives
</p>
<p>∣
</p>
<p>∣( fa,ψ)&minus; ψ(0)
∣
</p>
<p>∣ &le; 2‖ψ‖&infin;
</p>
<p>&int;
</p>
<p>|x|&ge;R
</p>
<p>| f (x)| dnx + ε
</p>
<p>&int;
</p>
<p>|x|&le;R
</p>
<p>| f (x)| dnx
</p>
<p>&le;
(
</p>
<p>2‖ψ‖&infin; + ‖ f ‖1
)
</p>
<p>ε,
</p>
<p>for a &ge; R/δ. Since ε was arbitrary, this shows that
</p>
<p>lim
a&rarr;&infin;
</p>
<p>( fa,ψ) = ψ(0).
</p>
<p>�
</p>
<p>The rescaling used in Lemma 12.1 is illustrated in Fig. 12.1. Note that this looks
</p>
<p>very similar to Fig. 9.1, and in fact the proof of Lemma 12.1 uses essentially the
</p>
<p>same argument as that of Theorem 9.1. We saw another case of this construction in
</p>
<p>the proof of Theorem 6.2. Indeed, we can now interpret the result of Theorem 6.2 as
</p>
<p>a distributional limit of the heat kernel,
</p>
<p>lim
t&rarr;0
</p>
<p>Ht = δ,
</p>
<p>where Ht was defined in (6.16).</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_9">http://dx.doi.org/10.1007/978-3-319-48936-0_9</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_9">http://dx.doi.org/10.1007/978-3-319-48936-0_9</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_6">http://dx.doi.org/10.1007/978-3-319-48936-0_6</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_6">http://dx.doi.org/10.1007/978-3-319-48936-0_6</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_6">http://dx.doi.org/10.1007/978-3-319-48936-0_6</a></div>
</div>
<div class="page"><p/>
<p>12.3 Distributional Derivatives 245
</p>
<p>Fig. 12.1 Rescaled
</p>
<p>functions fa for
</p>
<p>f (x) = 1
2
e&minus;|x |
</p>
<p>f16
</p>
<p>f8
</p>
<p>f4
</p>
<p>f2 f1
</p>
<p>12.3 Distributional Derivatives
</p>
<p>The distributional derivative extends the concept of the weak derivative introduced
</p>
<p>in Sect. 10.1. By analogy with (10.7), for u &isin; D&prime;(Ω) and we define the distribution
</p>
<p>Dαu by
</p>
<p>(Dαu,ψ) := (&minus;1)|α|(u, Dαψ), (12.20)
</p>
<p>with
</p>
<p>Dα :=
&part;α1
</p>
<p>&part;x
α1
1
</p>
<p>&middot; &middot; &middot;
&part;αn
</p>
<p>&part;x
αn
n
</p>
<p>and |α| := α1+&middot; &middot; &middot;+αn , as before. The pairing (12.20) is well defined as a distribution
</p>
<p>because Dα is both linear and continuous as a map C&infin;cpt(Ω) &rarr; C
&infin;
cpt(Ω).
</p>
<p>The terms &ldquo;distributional&rdquo; and &ldquo;weak&rdquo; are frequently used interchangeably to
</p>
<p>describe derivatives, since the definitions overlap to a considerable extent. The only
</p>
<p>difference is that a weak derivative is representable as a locally integrable function.
</p>
<p>Weak derivatives may not exist, whereas all distributions are infinitely differentiable.
</p>
<p>Example 12.2 Let us reconsider Example 10.3, where we considered the derivative
</p>
<p>of w &isin; L1loc(Ω) defined by
</p>
<p>w(t) =
</p>
<p>{
</p>
<p>w&minus;(t), t &lt; 0,
</p>
<p>w+(t), t &ge; 0,
</p>
<p>where w&plusmn; &isin; C
1(R). As part of that calculation we showed that
</p>
<p>&minus;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>wψ&prime; dt =
[
</p>
<p>w+(0)&minus; w&minus;(0)
]
</p>
<p>ψ(0)+
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>hψ dt, (12.21)
</p>
<p>where h is the piecewise derivative</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_10">http://dx.doi.org/10.1007/978-3-319-48936-0_10</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_10">http://dx.doi.org/10.1007/978-3-319-48936-0_10</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_10">http://dx.doi.org/10.1007/978-3-319-48936-0_10</a></div>
</div>
<div class="page"><p/>
<p>246 12 Distributions
</p>
<p>h(t) :=
</p>
<p>{
</p>
<p>w&prime;&minus;(t), t &lt; 0,
</p>
<p>w&prime;+(t), t &gt; 0.
</p>
<p>The left-hand side of (12.21) is the pairing (w&prime;,ψ) by the definition (12.20). From
</p>
<p>the right-hand side we can thus see that the distributional derivative is
</p>
<p>w&prime; = h +
[
</p>
<p>w+(0)&minus; w&minus;(0)
]
</p>
<p>δ.
</p>
<p>&diams;
</p>
<p>Example 12.3 For δx &isin; D
&prime;(Rn), the derivatives Dαδx are easily computed from the
</p>
<p>definition (12.20). For ψ &isin; C&infin;cpt(R
n),
</p>
<p>(Dαδx,ψ) = (&minus;1)
|α|(δx, D
</p>
<p>αψ)
</p>
<p>= (&minus;1)|α|Dαψ(x).
</p>
<p>In other words, the distribution Dαδx evaluates the derivative of the test function at
</p>
<p>the point x, up to a sign. &diams;
</p>
<p>Example 12.4 The function ln |x | is locally integrable on R and so defines a distri-
</p>
<p>bution in D&prime;(R). Therefore (ln |x |)&prime; exists in the distribution sense. This is puzzling
</p>
<p>because
d
</p>
<p>dx
ln |x | =
</p>
<p>1
</p>
<p>x
</p>
<p>for x �= 0, and x&minus;1 is not locally integrable.
</p>
<p>To understand what is happening here, we must return to the distributional defin-
</p>
<p>ition,
(
</p>
<p>(ln |x |)&prime;,ψ
)
</p>
<p>:= &minus;(ln |x |,ψ&prime;)
</p>
<p>= &minus;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>ψ&prime;(x) ln |x | dx
</p>
<p>for ψ &isin; C&infin;cpt(R). To compute this we avoid the singularity at 0 by writing
</p>
<p>(
</p>
<p>(ln |x |)&prime;,ψ
)
</p>
<p>= &minus; lim
ε&rarr;0
</p>
<p>&int;
</p>
<p>|x |&ge;ε
</p>
<p>ψ&prime;(x) ln |x | dx . (12.22)
</p>
<p>Integration by parts gives
</p>
<p>&minus;
</p>
<p>&int; &minus;ε
</p>
<p>&minus;&infin;
</p>
<p>ψ&prime;(x) ln |x | dx = &minus;ψ(x) ln |x |
∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&minus;ε
</p>
<p>&minus;&infin;
+
</p>
<p>&int; &minus;ε
</p>
<p>&minus;&infin;
</p>
<p>ψ(x)
</p>
<p>x
dx
</p>
<p>= &minus;ψ(&minus;ε) ln ε+
</p>
<p>&int; &minus;ε
</p>
<p>&minus;&infin;
</p>
<p>ψ(x)
</p>
<p>x
dx,</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 Distributional Derivatives 247
</p>
<p>and similarly
</p>
<p>&minus;
</p>
<p>&int; &infin;
</p>
<p>ε
</p>
<p>ψ&prime;(x) ln |x | dx = ψ(ε) ln ε+
</p>
<p>&int; &infin;
</p>
<p>ε
</p>
<p>ψ(x)
</p>
<p>x
dx .
</p>
<p>After combining these two halves, we obtain
</p>
<p>&int;
</p>
<p>|x |&ge;ε
</p>
<p>ψ&prime;(x) ln |x | dx =
[
</p>
<p>ψ(ε)&minus; ψ(&minus;ε)
]
</p>
<p>ln ε+
</p>
<p>&int;
</p>
<p>|x |&ge;ε
</p>
<p>ψ(x)
</p>
<p>x
dx .
</p>
<p>By the definition of the derivative,
</p>
<p>lim
ε&rarr;0
</p>
<p>ψ(ε)&minus; ψ(&minus;ε)
</p>
<p>2ε
= ψ&prime;(0).
</p>
<p>Therefore
lim
ε&rarr;0
</p>
<p>[
</p>
<p>ψ(ε)&minus; ψ(&minus;ε)
]
</p>
<p>ln ε = 2ψ&prime;(0) lim
ε&rarr;0
</p>
<p>ε ln ε
</p>
<p>= 0.
</p>
<p>Hence (12.22) reduces to
</p>
<p>(
</p>
<p>(ln |x |)&prime;,ψ
)
</p>
<p>= lim
ε&rarr;0
</p>
<p>&int;
</p>
<p>|x |&ge;ε
</p>
<p>ψ(x)
</p>
<p>x
dx . (12.23)
</p>
<p>The limit on the right exists for ψ &isin; C&infin;cpt(R), even though x
&minus;1 is not integrable,
</p>
<p>because the limit is taken symmetrically. This limiting procedure defines a distribu-
</p>
<p>tion called the principal value of x&minus;1, written as PV[x&minus;1]. We could rephrase (12.23)
</p>
<p>as
d
</p>
<p>dx
ln |x | = PV
</p>
<p>[
</p>
<p>x&minus;1
]
</p>
<p>.
</p>
<p>&diams;
</p>
<p>Example 12.5 Let us reinterpret the discussion from Sect. 12.1 in terms of distrib-
</p>
<p>utional derivatives. We already noted that the components of x/r3 are locally inte-
</p>
<p>grable, so we can consider the Coulomb formula (12.1) for E as the definition of a
</p>
<p>vector-valued distribution. The distributional divergence of x/r3 is defined by the
</p>
<p>condition that
(
</p>
<p>&nabla; &middot;
x
</p>
<p>r3
,ψ
</p>
<p>)
</p>
<p>:= &minus;
</p>
<p>&int;
</p>
<p>R3
</p>
<p>x
</p>
<p>r3
&middot; &nabla;ψ d3x,
</p>
<p>for ψ &isin; C&infin;cpt(R
3). The derivation of (12.8) thus shows that
</p>
<p>&nabla; &middot;
x
</p>
<p>r3
= 4πδ. (12.24)</p>
<p/>
</div>
<div class="page"><p/>
<p>248 12 Distributions
</p>
<p>We can also consider the corresponding result for the Coulomb electric potential
</p>
<p>φ(x) =
1
</p>
<p>r
.
</p>
<p>(ignoring the physical constants). The gradient of φ exists in the weak sense and is
</p>
<p>given by
</p>
<p>&nabla;
</p>
<p>(
</p>
<p>1
</p>
<p>r
</p>
<p>)
</p>
<p>= &minus;
x
</p>
<p>r3
.
</p>
<p>Since � = &nabla; &middot; &nabla;, we deduce from (12.24) that
</p>
<p>&minus;�
</p>
<p>(
</p>
<p>1
</p>
<p>4πr
</p>
<p>)
</p>
<p>= δ. (12.25)
</p>
<p>&diams;
</p>
<p>12.4 Fundamental Solutions
</p>
<p>Because the Poisson equation is linear, it makes sense to construct a solution with
</p>
<p>a continuous density by superimposing a field of point sources. With a change of
</p>
<p>variables, we can see from (12.25) that the potential function corresponding to a
</p>
<p>point source at y &isin; R3 is
</p>
<p>φy(x) :=
1
</p>
<p>4π|x &minus; y|
.
</p>
<p>Weighting the point sources by the density ρ and summing them with an integral
</p>
<p>gives
</p>
<p>u(x) =
1
</p>
<p>4π
</p>
<p>&int;
</p>
<p>R3
</p>
<p>ρ(y)
</p>
<p>|x &minus; y|
d3y. (12.26)
</p>
<p>This formula, which is often stated as the integral form of Coulomb&rsquo;s law, does
</p>
<p>indeed yield a solution of the Poisson equation on R3 under certain conditions. For
</p>
<p>example if ρ &isin; C1cpt(R
3) then one can confirm that &minus;�u = ρ by direct computation.
</p>
<p>The C1 condition is stronger than necessary here, but continuity alone would not
</p>
<p>be sufficient. (The precise notion of regularity needed for this problem is something
</p>
<p>called H&ouml;lder continuity.)
</p>
<p>This idea of constructing of general solutions by superposition of point sources is
</p>
<p>the inspiration for the concept of a fundamental solution. For a constant-coefficient
</p>
<p>differential operator L acting on Rn , of the form
</p>
<p>L =
&sum;
</p>
<p>|α|&le;m
</p>
<p>aαD
α,</p>
<p/>
</div>
<div class="page"><p/>
<p>12.4 Fundamental Solutions 249
</p>
<p>with aα &isin; C, a fundamental solution is a distribution Φ &isin; D&prime;(Rn) such that
</p>
<p>LΦ = δ. (12.27)
</p>
<p>For example, in the Coulomb case the calculation (12.25) gives the fundamental
</p>
<p>solution of &minus;� on R3. Fundamental solutions are especially important for classical
</p>
<p>problems involving the Laplacian.
</p>
<p>The solution formula (12.26) resembles the convolution used to solve the heat
</p>
<p>equation in Sect. 6.3. For f, g &isin; L1(Rn) the convolution is defined as
</p>
<p>f &lowast; g(x) :=
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>f (y)g(x &minus; y) dny.
</p>
<p>A simple change of variables shows that this product is symmetric,
</p>
<p>f &lowast; g = g &lowast; f.
</p>
<p>In order to produce solution formulas from fundamental solutions, we need to under-
</p>
<p>stand how to take convolutions with distributions.
</p>
<p>For f, g &isin; L1(Rn), the distributional pairing of f &lowast; g with ψ &isin; C&infin;cpt(Ω) gives
</p>
<p>( f &lowast; g,ψ) =
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>f (y)g(x &minus; y)ψ(x) dny dnx. (12.28)
</p>
<p>The x integration looks almost like the convolution of ψ with g, except with the
</p>
<p>argument switched from y &minus; x to x &minus; y. With the reflection defined by
</p>
<p>g&minus;(x) := g(&minus;x),
</p>
<p>we have
</p>
<p>g&minus; &lowast; ψ(y) =
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>g(x &minus; y)ψ(x) dnx.
</p>
<p>Thus (12.28) reduces to
</p>
<p>( f &lowast; g,ψ) := ( f, g&minus; &lowast; ψ).
</p>
<p>If φ,ψ &isin; C&infin;cpt(R
n), then it is easy to check that φ&minus;&lowast;ψ &isin; C&infin;cpt(R
</p>
<p>n) also. Moreover,
</p>
<p>the mapψ �&rarr; φ&minus;&lowast;ψ is linear and continuous. We can thus defineu &lowast;φ foru &isin; D&prime;(Rn)
</p>
<p>and φ &isin; C&infin;cpt(R
n) by
</p>
<p>(u &lowast; φ,ψ) := (u,φ&minus; &lowast; ψ) (12.29)
</p>
<p>for ψ &isin; C&infin;cpt(R
n).
</p>
<p>The distribution δ plays a special role with regard to convolutions. By the definition
</p>
<p>(12.29),</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_6">http://dx.doi.org/10.1007/978-3-319-48936-0_6</a></div>
</div>
<div class="page"><p/>
<p>250 12 Distributions
</p>
<p>(δ &lowast; φ,ψ) := (δ,φ&minus; &lowast; ψ)
</p>
<p>= φ&minus; &lowast; ψ(0)
</p>
<p>=
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>φ(x)ψ(x) dnx
</p>
<p>= (φ,ψ).
</p>
<p>This shows that
</p>
<p>δ &lowast; φ = φ. (12.30)
</p>
<p>In other words, convolution by δ is the identity map.
</p>
<p>Let Φ be the fundamental solution for the constant coefficient operator L . Our
</p>
<p>goal is to show that the equation Lu = f is solved by the convolution u = Φ &lowast; f , at
</p>
<p>least for f &isin; C&infin;cpt(R
n). To check this, we need to know how to evaluate derivatives
</p>
<p>of the convolution.
</p>
<p>Lemma 12.6 For w &isin; D&prime;(Rn) and ψ &isin; C&infin;cpt(R
n),
</p>
<p>Dα(w &lowast; f ) = (Dαw) &lowast; φ = w &lowast; (Dα f ).
</p>
<p>Proof For φ,ψ &isin; C&infin;cpt(Ω), we compute directly that
</p>
<p>Dα(φ &lowast; ψ)(x) =
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>φ(x &minus; y)ψ(y) dny
</p>
<p>=
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>Dαφ(x &minus; y)ψ(y) dny
</p>
<p>= (Dαφ) &lowast; ψ(x).
</p>
<p>Since the convolution is symmetric, the same formula holds with ψ and φ switched.
</p>
<p>Thus the formula
</p>
<p>Dα(ψ &lowast; φ) = (Dαψ) &lowast; φ = ψ &lowast; (Dαφ) (12.31)
</p>
<p>holds for test functions.
</p>
<p>For w &isin; D&prime;(R) and ψ &isin; C&infin;cpt(R
n), it follows from the definitions that
</p>
<p>(Dα( f &lowast; φ),ψ) = (&minus;1)|α|( f,φ&minus; &lowast; (Dαψ)).
</p>
<p>By (12.31) this gives
</p>
<p>(Dα( f &lowast; φ),ψ) = (&minus;1)|α|( f, Dα(φ&minus; &lowast; ψ))
</p>
<p>= (Dα f,φ&minus; &lowast; ψ)
</p>
<p>= ((Dα f ) &lowast; φ,ψ),</p>
<p/>
</div>
<div class="page"><p/>
<p>12.4 Fundamental Solutions 251
</p>
<p>and also
(Dα( f &lowast; φ),ψ) = (&minus;1)|α|( f, (Dαφ)&minus; &lowast; ψ))
</p>
<p>= ( f &lowast; (Dαφ),ψ).
</p>
<p>�
</p>
<p>Theorem 12.7 If L is a constant coefficient operator on Rn with fundamental solu-
</p>
<p>tion Φ, then for f &isin; C&infin;cpt(R
n) the equation
</p>
<p>Lu = f
</p>
<p>is solved by
</p>
<p>u = Φ &lowast; f.
</p>
<p>Proof By Lemma 12.6,
</p>
<p>L(Φ &lowast; f ) =
&sum;
</p>
<p>|α|&le;m
</p>
<p>aαD
α(Φ &lowast; f )
</p>
<p>=
&sum;
</p>
<p>|α|&le;m
</p>
<p>aα(D
αΦ) &lowast; f
</p>
<p>= (LΦ) &lowast; f.
</p>
<p>Note that the second step only works because the coefficients aα are assumed to be
</p>
<p>constant. Since LΦ = δ, we see from (12.30) that
</p>
<p>L(Φ &lowast; f ) = f.
</p>
<p>�
</p>
<p>A result called the Malgrange-Ehrenpreis theorem, proven in the 1950s, says that
</p>
<p>every constant coefficient differential operator on Rn admits a fundamental solution.
</p>
<p>The fundamental solution of the Laplacian, which we will now work out for any
</p>
<p>dimension, is the most important case.
</p>
<p>Theorem 12.8 On Rn the operator &minus;� has the fundamental solution
</p>
<p>Φ(x) =
</p>
<p>{
</p>
<p>&minus; 1
2π
</p>
<p>ln r, n = 2,
1
</p>
<p>(n&minus;2)Anrn&minus;2
, n &ge; 3,
</p>
<p>(12.32)
</p>
<p>where An denotes the volume of the unit sphere in dimension n.
</p>
<p>Proof We start from the distributional derivative,
</p>
<p>(&minus;�Φ,ψ) = &minus;(Φ,�ψ)</p>
<p/>
</div>
<div class="page"><p/>
<p>252 12 Distributions
</p>
<p>for ψ &isin; C&infin;cpt(R
n). To evaluate this, it is useful to first compute the gradient,
</p>
<p>&nabla;Φ(x) = &minus;
x
</p>
<p>Anrn
.
</p>
<p>The function x/rn is locally integrable in Rn andψ has compactly support. Therefore
</p>
<p>we can deduce from Green&rsquo;s first identity (Theorem 2.10) that
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>Φ�ψ dnx = &minus;
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>&nabla;Φ &middot; &nabla;ψ dnx
</p>
<p>=
1
</p>
<p>An
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>x
</p>
<p>rn
&middot; &nabla;ψ dnx
</p>
<p>=
1
</p>
<p>An
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>1
</p>
<p>rn&minus;1
</p>
<p>&part;ψ
</p>
<p>&part;r
dnx.
</p>
<p>The integral can be evaluated using radial coordinates as in (2.10):
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>Φ�ψ dnx =
1
</p>
<p>An
</p>
<p>&int;
</p>
<p>Sn&minus;1
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>&part;ψ
</p>
<p>&part;r
dr dS
</p>
<p>= &minus;
1
</p>
<p>An
</p>
<p>&int;
</p>
<p>Sn&minus;1
</p>
<p>ψ(0) dS
</p>
<p>= &minus;ψ(0).
</p>
<p>This shows that
</p>
<p>(&minus;�Φ,ψ) = ψ(0),
</p>
<p>hence &minus;�Φ = δ. �
</p>
<p>12.5 Green&rsquo;s Functions
</p>
<p>Although fundamental solutions are defined only for the domain Rn , one of their
</p>
<p>principle applications is to boundary value problems on a bounded domain Ω &sub; Rn .
</p>
<p>The connection comes from a integral formula introduced in 1828 by George Green.
</p>
<p>For this section, let Φ denote the fundamental solution of the Laplacian on Rn ,
</p>
<p>as given by (12.32). For y &isin; Rn we set
</p>
<p>Φy(x) := Φ(x &minus; y). (12.33)
</p>
<p>Theorem 12.9 (Green&rsquo;s representation formula) Suppose thatΩ &sub; Rn is a bounded
</p>
<p>domain with piecewise C1 boundary. For u &isin; C2(Ω),</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>12.5 Green&rsquo;s Functions 253
</p>
<p>u(y) = &minus;
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>Φy�u d
nx +
</p>
<p>&int;
</p>
<p>&part;Ω
</p>
<p>[
</p>
<p>Φy
&part;u
</p>
<p>&part;ν
&minus; u
</p>
<p>&part;Φy
</p>
<p>&part;ν
</p>
<p>]
</p>
<p>dS
</p>
<p>for y &isin; Ω .
</p>
<p>Proof Because the point y &isin; Ω is fixed, for notational convenience we can change
</p>
<p>variables to assume y = 0. For ε &gt; 0 set
</p>
<p>Bε := B(0; ε),
</p>
<p>and assume that ε is small enough that Bε&sub; Ω .
</p>
<p>On Ω &minus; Bε, Φ is smooth and satisfies �Φ = 0. Therefore, applying Green&rsquo;s
</p>
<p>second identity (Theorem 2.11) on this domain with v = Φ gives
</p>
<p>&int;
</p>
<p>Ω&minus;Bε
Φ�u dnx =
</p>
<p>&int;
</p>
<p>&part;Ω
</p>
<p>(
</p>
<p>Φ
&part;u
</p>
<p>&part;ν
&minus; u
</p>
<p>&part;Φ
</p>
<p>&part;ν
</p>
<p>)
</p>
<p>dS +
</p>
<p>&int;
</p>
<p>&part;Bε
</p>
<p>(
</p>
<p>Φ
&part;u
</p>
<p>&part;ν
&minus; u
</p>
<p>&part;Φ
</p>
<p>&part;ν
</p>
<p>)
</p>
<p>dS.
</p>
<p>(12.34)
</p>
<p>Because �u is continuous and Φ is locally integrable,
</p>
<p>lim
ε&rarr;0
</p>
<p>&int;
</p>
<p>Ω&minus;Bε
Φ�u dnx =
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>Φ�u dnx,
</p>
<p>To prove the representation formula we must therefore show that
</p>
<p>lim
ε&rarr;0
</p>
<p>&int;
</p>
<p>&part;Bε
</p>
<p>(
</p>
<p>Φ
&part;u
</p>
<p>&part;r
&minus; u
</p>
<p>&part;Φ
</p>
<p>&part;r
</p>
<p>)
</p>
<p>dS = u(0). (12.35)
</p>
<p>To handle the first term in (12.35), note that
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&part;u
</p>
<p>&part;r
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&le; |&nabla;u|,
</p>
<p>for r &gt; 0. Therefore, since &nabla;u is continuous by assumption, we have a bound
</p>
<p>max
&part;Bε
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&part;u
</p>
<p>&part;r
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&le; M
</p>
<p>for ε &gt; 0, with M independent of ε. Using the fact that vol(&part;Bε) = Anε
n&minus;1 and the
</p>
<p>formula (12.32) for Φ, we can estimate
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&int;
</p>
<p>&part;Bε
</p>
<p>Φ
&part;u
</p>
<p>&part;r
dS
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&le; M
</p>
<p>{
</p>
<p>ε ln ε, n = 2,
ε
</p>
<p>n&minus;2
, n &ge; 3.
</p>
<p>This shows that
</p>
<p>lim
ε&rarr;0
</p>
<p>&int;
</p>
<p>&part;Bε
</p>
<p>Φ
&part;u
</p>
<p>&part;r
dS = 0.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>254 12 Distributions
</p>
<p>For the second term in (12.35), we use the fact that
</p>
<p>&part;Φ
</p>
<p>&part;r
= &minus;
</p>
<p>1
</p>
<p>Anrn&minus;1
,
</p>
<p>for r &gt; 0, to compute
</p>
<p>&minus;
</p>
<p>&int;
</p>
<p>&part;Bε
</p>
<p>u
&part;Φ
</p>
<p>&part;r
dS =
</p>
<p>1
</p>
<p>Anεn&minus;1
</p>
<p>&int;
</p>
<p>&part;Bε
</p>
<p>u dS.
</p>
<p>The right-hand side is the average value of u over the sphere &part;Bε. By continuity,
</p>
<p>lim
ε&rarr;0
</p>
<p>1
</p>
<p>vol(&part;Bε)
</p>
<p>&int;
</p>
<p>&part;Bε
</p>
<p>u dS = u(0).
</p>
<p>This proves (12.35), and thus establishes the representation formula. �
</p>
<p>The representation formula of Theorem 12.9 has many applications. The original
</p>
<p>goal that Green had in mind was a solution formula for the Poisson problem with
</p>
<p>inhomogeneous Dirichlet boundary conditions, which we will now describe.
</p>
<p>Suppose there exists a family of functions Hy &isin; C
2(Ω), for y &isin; Ω , satisfying
</p>
<p>�Hy = 0, Hy
∣
</p>
<p>∣
</p>
<p>&part;Ω
= Φy
</p>
<p>∣
</p>
<p>∣
</p>
<p>&part;Ω
. (12.36)
</p>
<p>Then the Green&rsquo;s function of Ω is
</p>
<p>Gy := Φy &minus; Hy. (12.37)
</p>
<p>It is possible to show that Hy exists under general regularity conditions on &part;Ω , but
</p>
<p>this is too technical for us to get into here. We will focus on cases where Hy can be
</p>
<p>computed explicitly, which requires the geometry of Ω to be very simple.
</p>
<p>Theorem 12.10 SupposeΩ &sub; Rn is a bounded domainwith piecewiseC1 boundary
</p>
<p>that admits a Green&rsquo;s function Gy , Then the Poisson problem on Ω ,
</p>
<p>&minus;�u = f, u
∣
</p>
<p>∣
</p>
<p>&part;Ω
= g,
</p>
<p>for f &isin; C0(Ω), g &isin; C0(&part;Ω), is solved by the function
</p>
<p>u(y) = &minus;
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>f Gy d
nx &minus;
</p>
<p>&int;
</p>
<p>&part;Ω
</p>
<p>g
&part;Gy
</p>
<p>&part;ν
dS.
</p>
<p>Proof Setting v = Hy in Green&rsquo;s second identity (Theorem 2.11) gives
</p>
<p>&int;
</p>
<p>Ω
</p>
<p>(
</p>
<p>Hy�u &minus; u�Hy
</p>
<p>)
</p>
<p>dny =
</p>
<p>&int;
</p>
<p>&part;Ω
</p>
<p>Hy
&part;u
</p>
<p>&part;ν
dS. (12.38)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>12.5 Green&rsquo;s Functions 255
</p>
<p>The result then follows by subtracting (12.38) from the representation formula of
</p>
<p>Theorem 12.9. �
</p>
<p>Example 12.11 The Green&rsquo;s function for the unit disk D &sub; R2 can be derived using
</p>
<p>a trick from electrostatics called the method of images. This involves placing charges
</p>
<p>outside the domain in order to solve the boundary value problem. For the unit disk,
</p>
<p>in order to find Hy we consider a charge placed at the point ỹ given by &ldquo;reflecting&rdquo;
</p>
<p>y &isin; C\{0} across the unit circle, i.e.,
</p>
<p>ỹ :=
y
</p>
<p>|y|2
. (12.39)
</p>
<p>Note that Φỹ is harmonic on D because ỹ /&isin; D.
</p>
<p>For x &isin; &part;D, let ϕ denote the angle from y to x, as shown in Fig. 12.2. By the law
</p>
<p>of cosines on the triangle made by 0,y and x,
</p>
<p>|x &minus; y|2 = 1 + |y|2 &minus; 2|y| cosϕ.
</p>
<p>If y is replaced by ỹ, the corresponding formula is
</p>
<p>|x &minus; ỹ|2 = 1 + |ỹ|2 &minus; 2|ỹ| cosϕ
</p>
<p>= 1 + |y|&minus;2 &minus; 2|y|&minus;1 cosϕ.
</p>
<p>Solving for cosϕ in these expressions gives the relation
</p>
<p>|x &minus; y|2 &minus; 1 &minus; |y|2
</p>
<p>2|y|
=
</p>
<p>|x &minus; ỹ|2 &minus; 1 &minus; |y|&minus;2
</p>
<p>2|y|&minus;1
,
</p>
<p>which simplifies to
</p>
<p>|x &minus; ỹ| =
|x &minus; y|
</p>
<p>|y|
(12.40)
</p>
<p>for x &isin; &part;D.
</p>
<p>Fig. 12.2 Geometry for the
</p>
<p>method of images on the unit
</p>
<p>disk
</p>
<p>0
</p>
<p>y
</p>
<p>x
</p>
<p>ỹ
</p>
<p>ϕ</p>
<p/>
</div>
<div class="page"><p/>
<p>256 12 Distributions
</p>
<p>Since Φ(x) = &minus; 1
2π
</p>
<p>ln |x| in R2, taking the logarithm of (12.40) gives
</p>
<p>Φỹ(x) = Φy(x)+
1
</p>
<p>2π
ln |y|
</p>
<p>for y �= 0 or x. Thus we can solve (12.36) for y �= 0 by setting
</p>
<p>Hy := Φỹ &minus;
1
</p>
<p>2π
ln |y|.
</p>
<p>For y = 0 the obvious solution is H0 := 0 because Φ|&part;D = 0.
</p>
<p>The Green&rsquo;s function is thus
</p>
<p>Gy(x) =
</p>
<p>{
</p>
<p>&minus; 1
2π
</p>
<p>ln
(
</p>
<p>|x&minus;y|
</p>
<p>|x&minus;ỹ| |y|
</p>
<p>)
</p>
<p>, y �= 0
</p>
<p>&minus; 1
2π
</p>
<p>ln |x|, y = 0.
</p>
<p>To apply this in the solution formula, we need the radial derivative of G. For y fixed
</p>
<p>and r := |x| we compute
</p>
<p>&part;
</p>
<p>&part;r
ln |x &minus; y| = x &middot; &nabla; ln |x &minus; y|
</p>
<p>= x &middot;
(x &minus; y)
</p>
<p>|x &minus; y|2
</p>
<p>=
1 &minus; x &middot; y
</p>
<p>|x &minus; y|2
.
</p>
<p>Applying the corresponding result for |x &minus; ỹ| and subtracting gives
</p>
<p>&part;Gy
</p>
<p>&part;r
(x) = &minus;
</p>
<p>1
</p>
<p>2π
</p>
<p>(
</p>
<p>1 &minus; x &middot; y
</p>
<p>|x &minus; y|2
&minus;
</p>
<p>1 &minus; x &middot; ỹ
</p>
<p>|x &minus; ỹ|2
</p>
<p>)
</p>
<p>,
</p>
<p>which by (12.40) simplifies to
</p>
<p>&part;Gy
</p>
<p>&part;r
(x) = &minus;
</p>
<p>1 &minus; |y|2
</p>
<p>2π|x &minus; y|2
. (12.41)
</p>
<p>Conveniently, the calculation for y = 0 leads to the same expression.
</p>
<p>In the case of the Laplace equation on D, Theorem 12.10 gives the formula for a
</p>
<p>harmonic function u with boundary value g as
</p>
<p>u(y) =
1
</p>
<p>2π
</p>
<p>&int;
</p>
<p>&part;D
</p>
<p>1 &minus; |y|2
</p>
<p>|x &minus; y|2
g(x) dS(x).
</p>
<p>This will look more familiar in polar coordinates. With y = (r cos θ, r sin θ) and
</p>
<p>x = (cos η, sin η), the formula becomes</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Green&rsquo;s Functions 257
</p>
<p>u(r, θ) =
1
</p>
<p>2π
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>1 &minus; r2
</p>
<p>1 + r2 &minus; 2r cos(η &minus; θ)
g(cos η, sin η) dη.
</p>
<p>This is the classical Poisson formula (9.4) that we derived from Fourier series. &diams;
</p>
<p>12.6 Time-Dependent Fundamental Solutions
</p>
<p>To adapt the concept of a fundamental solution to evolution equations, we need
</p>
<p>to consider time-dependent distributions on Rn . We will use a subscript to denote
</p>
<p>the time dependence, to avoid confusion with the spatial variables. Thus a map
</p>
<p>R &rarr; D&prime;(Rn) will be written
</p>
<p>t �&rarr; wt .
</p>
<p>For ψ &isin; C&infin;cpt(R
n) the pairing (wt ,ψ) is a complex-valued function of t .
</p>
<p>The function t �&rarr; wt is differentiable with respect to time if there exists a family
</p>
<p>of distributions &part;wt
&part;t
</p>
<p>&isin; D&prime;(Rn) such that
</p>
<p>d
</p>
<p>dt
(wt ,ψ) =
</p>
<p>(
</p>
<p>&part;wt
</p>
<p>&part;t
,ψ
</p>
<p>)
</p>
<p>, (12.42)
</p>
<p>for all ψ &isin; C&infin;cpt(Ω). Higher derivatives are defined in the same way.
</p>
<p>Example 12.12 In R, consider the derivatives of δt , the delta function supported at
</p>
<p>t . By definition,
</p>
<p>(δt ,ψ) := ψ(t),
</p>
<p>so that
(
</p>
<p>&part;n
</p>
<p>&part;tn
δt ,ψ
</p>
<p>)
</p>
<p>= ψ(n)(t).
</p>
<p>Compare this to the spatial derivatives, defined according to (12.20),
</p>
<p>(
</p>
<p>&part;n
</p>
<p>&part;xn
δt ,ψ
</p>
<p>)
</p>
<p>:= (&minus;1)n
(
</p>
<p>δt ,ψ
(n)
</p>
<p>)
</p>
<p>= (&minus;1)nψ(n)(t).
</p>
<p>We conclude that
&part;n
</p>
<p>&part;tn
δt = (&minus;1)
</p>
<p>n &part;
n
</p>
<p>&part;xn
δt . (12.43)
</p>
<p>&diams;
</p>
<p>Let us try to deduce the fundamental solution for the one-dimensional wave equa-
</p>
<p>tion from d&rsquo;Alembert&rsquo;s formula (4.8),</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_9">http://dx.doi.org/10.1007/978-3-319-48936-0_9</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_4">http://dx.doi.org/10.1007/978-3-319-48936-0_4</a></div>
</div>
<div class="page"><p/>
<p>258 12 Distributions
</p>
<p>u(t, x) =
1
</p>
<p>2
</p>
<p>[
</p>
<p>g(x + t)+ g(x &minus; t)
]
</p>
<p>+
1
</p>
<p>2
</p>
<p>&int; x+t
</p>
<p>x&minus;t
</p>
<p>h(τ ) dτ , (12.44)
</p>
<p>The second term in (12.44) could be interpreted as a convolution
</p>
<p>&int; x+t
</p>
<p>x&minus;t
</p>
<p>h(τ ) dτ =
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>χ[&minus;t,t](x &minus; τ )h(τ ) dτ
</p>
<p>= χ[&minus;t,t] &lowast; h(x),
</p>
<p>where χI denotes a characteristic function as in (7.5). Therefore it makes sense to
</p>
<p>define this component of the fundamental solution as
</p>
<p>Wt :=
1
</p>
<p>2
χ(&minus;t,t). (12.45)
</p>
<p>The time derivatives of Wt are computed from the pairing
</p>
<p>(Wt ,ψ) =
1
</p>
<p>2
</p>
<p>&int; t
</p>
<p>&minus;t
</p>
<p>ψ dx
</p>
<p>for ψ &isin; C&infin;cpt(R). By the fundamental theorem of calculus,
</p>
<p>d
</p>
<p>dt
(Wt ,ψ) =
</p>
<p>1
</p>
<p>2
</p>
<p>[
</p>
<p>ψ(t)+ ψ(&minus;t)
]
</p>
<p>, (12.46)
</p>
<p>which shows that
&part;Wt
</p>
<p>&part;t
=
</p>
<p>1
</p>
<p>2
</p>
<p>(
</p>
<p>δt + δ&minus;t
)
</p>
<p>. (12.47)
</p>
<p>Differentiating again using (12.43) gives
</p>
<p>&part;2Wt
</p>
<p>&part;t2
=
</p>
<p>1
</p>
<p>2
</p>
<p>(
</p>
<p>&minus;δ&prime;t + δ
&prime;
&minus;t
</p>
<p>)
</p>
<p>. (12.48)
</p>
<p>On the other hand, x-derivatives of Wt are defined by (12.20). In particular,
</p>
<p>(
</p>
<p>&part;2Wt
</p>
<p>&part;x2
,ψ
</p>
<p>)
</p>
<p>:=
(
</p>
<p>Wt ,ψ
&prime;&prime;
)
</p>
<p>This can be evaluated by direct integration,
</p>
<p>(
</p>
<p>&part;2Wt
</p>
<p>&part;x2
,ψ
</p>
<p>)
</p>
<p>:=
1
</p>
<p>2
</p>
<p>&int; t
</p>
<p>&minus;t
</p>
<p>ψ&prime;&prime; dx
</p>
<p>=
1
</p>
<p>2
</p>
<p>[
</p>
<p>ψ&prime;(t)&minus; ψ&prime;(&minus;t)
]
</p>
<p>.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
</div>
<div class="page"><p/>
<p>12.6 Time-Dependent Fundamental Solutions 259
</p>
<p>This shows that
&part;2Wt
</p>
<p>&part;x2
=
</p>
<p>1
</p>
<p>2
</p>
<p>(
</p>
<p>&minus;δ&prime;t + δ
&prime;
&minus;t
</p>
<p>)
</p>
<p>. (12.49)
</p>
<p>By (12.48) and (12.49), Wt is a distributional solution of the wave equation,
</p>
<p>(
</p>
<p>&part;2
</p>
<p>&part;t2
&minus;
</p>
<p>&part;2
</p>
<p>&part;x2
</p>
<p>)
</p>
<p>Wt = 0.
</p>
<p>In contrast to the definition (12.27) of a fundamental solution in the spatial case, Wt
satisfies a homogeneous equation. The delta function appears only in the boundary
</p>
<p>conditions,
</p>
<p>W0 = 0,
&part;Wt
</p>
<p>&part;t
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>t=0
= δ.
</p>
<p>The distribution Wt , which is analogous to a fundamental solution, is called the
</p>
<p>wave kernel. By (12.47), the g component of the d&rsquo;Alembert solution formula (12.44)
</p>
<p>could be written in terms of Wt as
</p>
<p>1
</p>
<p>2
</p>
<p>[
</p>
<p>g(x + t)+ g(x &minus; t)
]
</p>
<p>=
&part;Wt
</p>
<p>&part;t
&lowast; g(x).
</p>
<p>Thus the full convolution formula for the solution reads
</p>
<p>u(t, &middot;) =
&part;Wt
</p>
<p>&part;t
&lowast; g +Wt &lowast; h.
</p>
<p>12.7 Exercises
</p>
<p>12.1 Define the distribution u &isin; D&prime;(R) by
</p>
<p>(u,ψ) :=
</p>
<p>&int; 1
</p>
<p>&minus;1
</p>
<p>ψ(x)&minus; ψ(0)
</p>
<p>x
dx +
</p>
<p>&int;
</p>
<p>|x |&ge;1
</p>
<p>ψ(x)
</p>
<p>x
dx .
</p>
<p>Show that u = PV[x&minus;1].
</p>
<p>12.2 Let f &isin; L1loc(R) be the function
</p>
<p>f (x) =
</p>
<p>{
</p>
<p>log x, x &gt; 0,
</p>
<p>&minus; log(&minus;x), x &lt; 0.
</p>
<p>For x �= 0, f &prime;(x) = |x |&minus;1, but this is not locally integrable. Show that the distribu-
</p>
<p>tional derivative is</p>
<p/>
</div>
<div class="page"><p/>
<p>260 12 Distributions
</p>
<p>( f &prime;,ψ) =
</p>
<p>&int; 1
</p>
<p>&minus;1
</p>
<p>ψ(x)&minus; ψ(0)
</p>
<p>|x |
+
</p>
<p>&int;
</p>
<p>|x |&ge;1
</p>
<p>ψ(x)
</p>
<p>|x |
dx .
</p>
<p>12.3 Let H denote the upper half-plane {x2 &gt; 0} &sub; R
2. The goal of this problem is
</p>
<p>to show that the Laplace equation on H,
</p>
<p>�u = 0, u(&middot;, 0) = g,
</p>
<p>has the solution
</p>
<p>u(y) =
1
</p>
<p>π
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>y2
</p>
<p>(x &minus; y1)2 + y
2
2
</p>
<p>g(x) dx
</p>
<p>for g &isin; C&infin;cpt(R).
</p>
<p>(a) Derive this formula from Theorem 12.10 using the method of images as in
</p>
<p>Example 12.11. In this case the reflection of y &isin; H is given by (y1, y2) =
</p>
<p>(y1,&minus;y2) (the complex conjugate).
</p>
<p>(b) Show that the fact that u(&middot;, 0) = g could also be derived by using Lemma 12.1
</p>
<p>to deduce that
</p>
<p>lim
x&rarr;0
</p>
<p>y
</p>
<p>π(x2 + y2)
= δ(y).
</p>
<p>12.4 In R3 show that
</p>
<p>(&minus;�&minus; k2)
eikr
</p>
<p>4πr
= δ
</p>
<p>for all k &isin; R.
</p>
<p>12.5 For n &ge; 3 let Bn denote the unit ball {r &lt; 1} &sub; Rn .
</p>
<p>(a) Apply the method of images as in Example 12.11 to derive the solution Hy of
</p>
<p>(12.36) for y &isin; Bn . and compute the Green&rsquo;s function. (Note that the formulas
</p>
<p>(12.39) and (12.40) remain valid in any dimension.)
</p>
<p>(b) Show that the radial derivative of the Green&rsquo;s function satisfies
</p>
<p>&part;Gy
</p>
<p>&part;r
(x) = &minus;
</p>
<p>1 &minus; |y2|
</p>
<p>An|x &minus; y|n&minus;1
.
</p>
<p>(c) Find the resulting solution formula from Theorem 12.10, and show that this gen-
</p>
<p>eralizes the mean value formula for harmonic functions obtained in Theorem 9.3.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_9">http://dx.doi.org/10.1007/978-3-319-48936-0_9</a></div>
</div>
<div class="page"><p/>
<p>Chapter 13
</p>
<p>The Fourier Transform
</p>
<p>For a bounded domain Ω &sub; Rn , Theorem 11.7 shows that we can effectively
&ldquo;diagonalize&rdquo; the Laplacian by choosing an orthonormal basis for L2(Ω) consisting
</p>
<p>of eigenfunctions. Such a result is not possible on Rn itself; the Laplacian has no
</p>
<p>eigenfunctions in L2(Rn).
</p>
<p>The closest analog to eigenfunctions onRn are the spatial components of the plane
</p>
<p>wave solutions introduced in Exercise 4.8,
</p>
<p>φξ(x) := eiξ&middot;x,
</p>
<p>associated to a frequency vector ξ &isin; Rn . These functions satisfy a convenient dif-
ferentiation formula,
</p>
<p>Dαφξ = (iξ)αφξ,
</p>
<p>and in particular
</p>
<p>&minus;�eiξ&middot;x = |ξ|2 eiξ&middot;x .
</p>
<p>The appropriate generalization of the Fourier series to L2(Rn) is an integral trans-
</p>
<p>form based on these plane waves. Although the technical details are quite different
</p>
<p>from Fourier series, the transform serves a similar purpose in that it exchanges the
</p>
<p>roles of differentiation and multiplication.
</p>
<p>13.1 Fourier Transform
</p>
<p>The Fourier transform of a function in f &isin; L1(Rn) is a function of the frequency
ξ &isin; Rn defined by
</p>
<p>f̂ (ξ) :=
&int;
</p>
<p>Rn
</p>
<p>e&minus;iξ&middot;x f (x) dn x. (13.1)
</p>
<p>&copy; Springer International Publishing AG 2016
</p>
<p>D. Borthwick, Introduction to Partial Differential Equations,
</p>
<p>Universitext, DOI 10.1007/978-3-319-48936-0_13
</p>
<p>261</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_11">http://dx.doi.org/10.1007/978-3-319-48936-0_11</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_4">http://dx.doi.org/10.1007/978-3-319-48936-0_4</a></div>
</div>
<div class="page"><p/>
<p>262 13 The Fourier Transform
</p>
<p>Note that the integral is well defined by the integrability of f , and in fact
∣
</p>
<p>∣
</p>
<p>∣ f̂ (ξ)
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣ &le; ‖ f ‖L1 (13.2)
</p>
<p>for all ξ &isin; Rn . As a map the transform is denoted by
</p>
<p>F : f �&rarr; f̂ .
</p>
<p>To develop the properties of the Fourier transform, it proves convenient to intro-
</p>
<p>duce a particular class of test functions, called Schwartz functions. The space S con-
</p>
<p>sists of smooth functions which, along with all derivatives, decay rapidly at infinity.
</p>
<p>The precise meaning of &ldquo;rapid&rdquo; is &ldquo;faster than any power of r .&rdquo; An alternate form of
</p>
<p>this definition is
</p>
<p>S(Rn) :=
{
</p>
<p>f &isin; C&infin;(Rn);
∥
</p>
<p>∥
</p>
<p>x
αDβ f
</p>
<p>∥
</p>
<p>∥
</p>
<p>&infin; &lt; &infin; for all α,β
}
</p>
<p>, (13.3)
</p>
<p>where ‖&middot;‖&infin; is the sup norm introduced in Sect. 7.3.
A basic example of a Schwartz function is a Gaussian function of the form
</p>
<p>f (x) = e&minus;a|x|2 ,
</p>
<p>with a &gt; 0. We also have
</p>
<p>C&infin;cpt(R
n) &sub; S(Rn),
</p>
<p>because compactly supported functions obviously satisfy the decay requirement.
</p>
<p>As in the discrete case, the Fourier transform interchanges the operations of dif-
</p>
<p>ferentiation and multiplication in a convenient way. For this statement, we let Dα
x
</p>
<p>and Dαξ denote partial derivatives with respect to x or ξ, respectively.
</p>
<p>Lemma 13.1 For ψ &isin; S(Rn),
</p>
<p>F[Dα
x
ψ](ξ) = (iξ)αψ̂(ξ), (13.4)
</p>
<p>and
</p>
<p>F[xαψ](ξ) = (i Dξ)αψ̂(ξ). (13.5)
</p>
<p>Proof The first identity follows from integration by parts,
</p>
<p>F[Dαψ](ξ) =
&int;
</p>
<p>Rn
</p>
<p>e&minus;iξ&middot;x Dα
x
ψ(x) dn x
</p>
<p>=
&int;
</p>
<p>Rn
</p>
<p>ψ(x)(i Dξ)
α(e&minus;iξ&middot;x) dn x
</p>
<p>=
&int;
</p>
<p>Rn
</p>
<p>ψ(x)(iξ)αe&minus;iξ&middot;x dn x
</p>
<p>= (iξ)αψ̂(ξ).</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
</div>
<div class="page"><p/>
<p>13.1 Fourier Transform 263
</p>
<p>The second is also a direct computation,
</p>
<p>F[xα f ](ξ) =
&int;
</p>
<p>Rn
</p>
<p>e&minus;iξ&middot;x xα f (x) dn x
</p>
<p>=
&int;
</p>
<p>Rn
</p>
<p>(i Dξ)
α(e&minus;iξ&middot;x) f (x) dn x
</p>
<p>= (i Dξ)α f̂ (ξ).
</p>
<p>Pulling the differentiation outside the integral in the final step is justified by the
</p>
<p>smoothness and decay assumptions on ψ. �
</p>
<p>In Lemma 13.1 we can see Schwartz&rsquo;s motivation for the definition of S. Under
</p>
<p>the Fourier transform, smoothness translates to rapid decay, and vice versa. These
</p>
<p>properties are balanced in the definition of S, which leads to the following result.
</p>
<p>Lemma 13.2 The Fourier transform F maps S(Rn) &rarr; S(Rn).
Proof Suppose that f &isin; S. In order to show that f̂ is Schwartz, we need to produce
a bound on the function ξβ Dα f̂ for each α,β. By (13.4) and (13.5),
</p>
<p>ξβ Dαξ f̂ (ξ) = i |α|+|β|
&int;
</p>
<p>Rn
</p>
<p>e&minus;iξ&middot;x xαDβ
x
</p>
<p>f (x) dn x. (13.6)
</p>
<p>To estimate, we set
</p>
<p>MN ,α,β :=
∥
</p>
<p>∥
</p>
<p>∥(1+ |x|2)N xαDβ
x
</p>
<p>f
</p>
<p>∥
</p>
<p>∥
</p>
<p>∥
</p>
<p>&infin;
,
</p>
<p>which is finite by the definition (13.3). Because (1 + |x |2)&minus;N is integrable for N
sufficiently large, we can estimate (13.6) by
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣ξ
β Dα f̂ (ξ)
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣ &le; MN ,α,β
&int;
</p>
<p>Rn
</p>
<p>1
</p>
<p>(1+ |x |2)N
dn x.
</p>
<p>The right-hand side is independent of ξ, so this yields the required estimate. �
</p>
<p>Example 13.3 Consider the one-dimensional Gaussian function
</p>
<p>ϕ(x) := e&minus;ax2
</p>
<p>for a &gt; 0. Note that ϕ satisfies the ODE
</p>
<p>dϕ
</p>
<p>dx
= &minus;2axϕ.
</p>
<p>Taking the Fourier transform of both sides and applying Lemma 13.1 gives
</p>
<p>iξϕ̂ = &minus;2ai dϕ̂
dξ
</p>
<p>,</p>
<p/>
</div>
<div class="page"><p/>
<p>264 13 The Fourier Transform
</p>
<p>which reduces to
dϕ̂
</p>
<p>dξ
= &minus; ξ
</p>
<p>2a
ϕ̂.
</p>
<p>Separating variables and integrating yields the solution
</p>
<p>ϕ̂(ξ) = ϕ̂(0)e&minus;ξ2/4a .
</p>
<p>To fix the constant, we can use (2.19) with n = 1 to compute
</p>
<p>ϕ̂(0) =
&int; &infin;
</p>
<p>&minus;&infin;
e&minus;ax
</p>
<p>2
</p>
<p>dx
</p>
<p>= 1&radic;
a
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
e&minus;x
</p>
<p>2
</p>
<p>dx
</p>
<p>=
&radic;
</p>
<p>π
</p>
<p>a
.
</p>
<p>(13.7)
</p>
<p>Thus,
</p>
<p>ϕ̂(ξ) =
&radic;
</p>
<p>π
</p>
<p>a
e&minus;ξ
</p>
<p>2/4a .
</p>
<p>&diams;
</p>
<p>The computation from Example 13.3 can be generalized to Rn by factoring the
</p>
<p>integrals,
</p>
<p>F
</p>
<p>[
</p>
<p>e&minus;a|x|
2
]
</p>
<p>(ξ) =
&int;
</p>
<p>Rn
</p>
<p>e&minus;i x&middot;ξ&minus;a|x|
2
</p>
<p>dn x
</p>
<p>=
n
</p>
<p>&prod;
</p>
<p>j=1
</p>
<p>(&int; &infin;
</p>
<p>&minus;&infin;
e&minus;i x j ξ j&minus;ax
</p>
<p>2
j dx j
</p>
<p>)
</p>
<p>=
n
</p>
<p>&prod;
</p>
<p>j=1
</p>
<p>&radic;
</p>
<p>π
</p>
<p>a
e&minus;ξ
</p>
<p>2
j /4a .
</p>
<p>Thus
</p>
<p>F
</p>
<p>[
</p>
<p>e&minus;a|x|
2
]
</p>
<p>(ξ) =
(π
</p>
<p>a
</p>
<p>)n
2
</p>
<p>e&minus;|ξ|
2/4a (13.8)
</p>
<p>for a &gt; 0.
</p>
<p>For f, g &isin; S(Rn), consider the integral
&int;
</p>
<p>Rn
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>f (x)e&minus;i x&middot;yg(y) dn x dny. (13.9)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
</div>
<div class="page"><p/>
<p>13.1 Fourier Transform 265
</p>
<p>The integrals over x and y can be taken in either order, yielding the useful identity:
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>f ĝ dn x =
&int;
</p>
<p>Rn
</p>
<p>f̂ g dny (13.10)
</p>
<p>for f, g &isin; S(Rn),
Theorem 13.4 The Fourier transform on S(Rn) has an inverse F&minus;1 given by
</p>
<p>f (x) = (2π)&minus;n
&int;
</p>
<p>Rn
</p>
<p>eiξ&middot;x f̂ (ξ) dnξ. (13.11)
</p>
<p>Proof In (13.10) let us set g = e&minus;ax2 for a &gt; 0, By (13.8), this implies
(π
</p>
<p>a
</p>
<p>)n
2
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>f (x)e&minus;x
2/4a dn x =
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>f̂ (y)e&minus;ay
2
</p>
<p>dny. (13.12)
</p>
<p>On the left-hand side we can use the same argument as in the proof of Lemma 12.1
</p>
<p>to show that
</p>
<p>lim
a&rarr;0
</p>
<p>[
</p>
<p>(π
</p>
<p>a
</p>
<p>)n
2
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>f (x)e&minus;x
2/4a dn x
</p>
<p>]
</p>
<p>= π n2 lim
a&rarr;0
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>f (
&radic;
</p>
<p>ax)e&minus;x
2/4 dn x
</p>
<p>= (2π)n f (0).
</p>
<p>We claim that the corresponding limit on the right-hand side of (13.12) is
</p>
<p>lim
a&rarr;0
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>f̂ (y)e&minus;ay
2
</p>
<p>dny =
&int;
</p>
<p>Rn
</p>
<p>f̂ (y) dny. (13.13)
</p>
<p>Because the convergence is not uniform, we will check this carefully. The difference
</p>
<p>of the two sides can be estimated by
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>f̂ (y)e&minus;ay
2
</p>
<p>dny &minus;
&int;
</p>
<p>Rn
</p>
<p>f̂ (y) dny
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&le;
&int;
</p>
<p>Rn
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣ f̂ (y)
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>(
</p>
<p>1&minus; e&minus;ay2
)
</p>
<p>dny.
</p>
<p>Given ε &gt; 0 we can choose R large enough that
</p>
<p>&int;
</p>
<p>|x|&gt;R
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣ f̂ (y)
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣ d
ny &lt; ε,
</p>
<p>since f̂ is integrable. Splitting the integral at |x| = R gives the estimate
&int;
</p>
<p>Rn
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣ f̂ (y)
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>(
</p>
<p>1&minus; e&minus;ay2
)
</p>
<p>dny &le; ε+
&int;
</p>
<p>B(0;R)
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣ f̂ (y)
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>(
</p>
<p>1&minus; e&minus;ay2
)
</p>
<p>dny
</p>
<p>&le; ε+
(
</p>
<p>1&minus; e&minus;a R2
)
</p>
<p>∥
</p>
<p>∥ f̂
∥
</p>
<p>∥
</p>
<p>1
.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_12">http://dx.doi.org/10.1007/978-3-319-48936-0_12</a></div>
</div>
<div class="page"><p/>
<p>266 13 The Fourier Transform
</p>
<p>The second term approaches zero as a &rarr; 0, so that
&int;
</p>
<p>Rn
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣ f̂ (y)
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>(
</p>
<p>1&minus; e&minus;ay2
)
</p>
<p>dny &lt; 2ε,
</p>
<p>for a sufficiently small. This establishes (13.13).
</p>
<p>By these calculations, the limit of (13.12) as a &rarr; 0 yields
</p>
<p>(2π)n f (0) =
&int;
</p>
<p>Rn
</p>
<p>f̂ (x) dn x. (13.14)
</p>
<p>This is a special case of the desired formula.
</p>
<p>The general inverse formula can be deduced from (13.14) by a simple translation
</p>
<p>argument. For w &isin; Rn , define the translation operator Tw on S(Rn) by
</p>
<p>Tw f (y) := f (y + w).
</p>
<p>A change of variables shows that
</p>
<p>T̂w f (x) =
&int;
</p>
<p>Rn
</p>
<p>e&minus;i x&middot;y f (y + w) dny
</p>
<p>=
&int;
</p>
<p>Rn
</p>
<p>e&minus;i x&middot;(y&minus;w) f (y) dny
</p>
<p>= ei x&middot;w f̂ (x).
</p>
<p>Since Tw f (0) = f (w), plugging Tw f into (13.14) gives
</p>
<p>(2π)n f (w) =
&int;
</p>
<p>Rn
</p>
<p>ei x&middot;w f̂ (x) dn x.
</p>
<p>�
</p>
<p>The pairing formula (13.10) suggests that the L2 inner product will behave natu-
</p>
<p>rally under the Fourier transform. Indeed, by Theorem 13.4 we can compute
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>f̂ (ξ)ĝ(ξ) dnξ =
&int;
</p>
<p>Rn
</p>
<p>(&int;
</p>
<p>Rn
</p>
<p>f (x)e&minus;i x&middot;ξ dn x
</p>
<p>)
</p>
<p>ĝ(ξ) dnξ
</p>
<p>=
&int;
</p>
<p>Rn
</p>
<p>(&int;
</p>
<p>Rn
</p>
<p>ei x&middot;ξ ĝ(ξ) dnξ
</p>
<p>)
</p>
<p>f (x) dn x
</p>
<p>= (2π)n
&int;
</p>
<p>Rn
</p>
<p>f (x)g(x) dn x,
</p>
<p>for f, g &isin; S(Rn). In other words,
</p>
<p>〈 f̂ , ĝ〉 = (2π)n〈 f, g〉. (13.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.1 Fourier Transform 267
</p>
<p>The integral (13.1) defining the Fourier transform does not necessarily converge
</p>
<p>for f &isin; L2(Rn), but the identity (13.15) makes it possible to define transforms on
L2 by taking limits.
</p>
<p>Theorem 13.5 (Plancherel&rsquo;s theorem) The Fourier transform extends from S(Rn)
</p>
<p>to an invertible map on L2(Rn), such that (13.15) holds for all f, g &isin; L2(Rn).
</p>
<p>Proof First note that Theorem 7.5 implies that S(Rn) is dense in L2(Rn) because
</p>
<p>it includes the compactly supported smooth functions. Hence for f &isin; L2(Rn) there
exists a sequence of Schwartz functions φk &rarr; f in L2. As a convergent sequence,
{φk} is automatically Cauchy, i.e.,
</p>
<p>lim
k,m&rarr;&infin;
</p>
<p>‖φk &minus; φm‖2 = 0.
</p>
<p>By (13.15),
∥
</p>
<p>∥
</p>
<p>∥φ̂k &minus; φ̂m
∥
</p>
<p>∥
</p>
<p>∥
</p>
<p>2
= (2π)n/2 ‖φk &minus; φm‖2 ,
</p>
<p>implying that {φ̂k} is also Cauchy in L2(Rn). Since L2(Rn) is complete by Theorem
7.7, this implies convergence, and we can then define
</p>
<p>f̂ := lim
k&rarr;&infin;
</p>
<p>φ̂k,
</p>
<p>with the limit taken in the L2 sense.
</p>
<p>To show that (13.15) extends to L2, suppose for f, g &isin; L2 thatφk &rarr; f andψm &rarr;
g are approximating sequences of Schwartz functions. By the property (13.15),
</p>
<p>〈φ̂k, ψ̂m〉 = (2π)n〈φk,ψm〉.
</p>
<p>Taking the limit k,m &rarr; &infin; then gives
</p>
<p>〈 f̂ , ĝ〉 = (2π)n〈 f, g〉.
</p>
<p>The same argument can be used to show that f̂ is independent of the choice of
</p>
<p>approximating sequence. �
</p>
<p>13.2 Tempered Distributions
</p>
<p>Since F maps S(Rn) to itself, to extend the Fourier transform to distributions it is
</p>
<p>natural replace C&infin;cpt(R
n) by S(Rn) as the space of test functions. The result is the
</p>
<p>space of tempered distributions
</p>
<p>S
&prime;(Rn) :=
</p>
<p>{
</p>
<p>continuous linear functionals S(Rn) &rarr; C
}
</p>
<p>.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
</div>
<div class="page"><p/>
<p>268 13 The Fourier Transform
</p>
<p>Here the word &ldquo;tempered&rdquo; refers to a restriction on the growth at infinity. Because the
</p>
<p>Schwartz functions decay rapidly, a locally integrable function is essentially required
</p>
<p>to have a polynomial growth rate at infinity in order to define an element of S &prime;(Rn).
The definition of continuity of a functional on S(Rn) depends on a notion of
</p>
<p>convergence for Schwartz functions. A sequence {ψk} &sub; S(Rn) converges if the
sequences
</p>
<p>{
</p>
<p>x
αDβψk
</p>
<p>}
</p>
<p>converge uniformly for each α, β. To say that u &isin; S &prime;(Rn) is
continuous means that (u,ψk) &rarr; (u,ψ) whenever ψk &rarr; ψ in S(Rn).
</p>
<p>The delta function δx and its derivatives are clearly tempered distributions. We
</p>
<p>claim also that
</p>
<p>L p(Rn) &sub; S &prime;(Rn),
</p>
<p>for p &isin; [1,&infin;]. This follows fairly directly from the fact that S(Rn) &sub; L p(Rn) for
p &isin; [1,&infin;].
</p>
<p>The pairing formula (13.10) gives the prescription for extendingF to the tempered
</p>
<p>distributions. For u &isin; S &prime;(Rn), we define û by
</p>
<p>(û,φ) := (u, φ̂) (13.16)
</p>
<p>for φ &isin; S(Rn). To justify this definition one needs to check that the Fourier trans-
form is continuous as a map S(Rn) &rarr; S(Rn). This essentially follows from the
calculations in the proof of Lemma 13.2.
</p>
<p>As an example, consider the function u = 1 as an element of S &prime;(Rn). For ψ &isin;
S(Rn),
</p>
<p>(1̂,ψ) := (1, ψ̂)
</p>
<p>=
&int;
</p>
<p>Rn
</p>
<p>ψ̂(x) dn x.
</p>
<p>According to the inverse Fourier transform formula (13.11),
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>ψ̂(x) dn x = (2π)nψ(0).
</p>
<p>Therefore
</p>
<p>1̂ = (2π)nδ. (13.17)
</p>
<p>Physicists often express this fact by writing
</p>
<p>δ(x) = (2π)&minus;n
&int;
</p>
<p>Rn
</p>
<p>e&minus;i x&middot;ξ dnξ,
</p>
<p>with the understanding that the integral on the right is not to be taken literally.
</p>
<p>The Fourier transform of δ is a similar calculation. For ψ &isin; S,</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 Tempered Distributions 269
</p>
<p>(δ̂,ψ) := (δ, ψ̂)
= ψ̂(0)
</p>
<p>=
&int;
</p>
<p>Rn
</p>
<p>ψ(x) dn x.
</p>
<p>(13.18)
</p>
<p>Therefore
</p>
<p>δ̂ = 1. (13.19)
</p>
<p>Because differentiation and multiplication by polynomials are continuous opera-
</p>
<p>tions on S(Rn), they extend to tempered distributions. From Lemma 13.1 we imme-
</p>
<p>diately derive the following:
</p>
<p>Lemma 13.6 For u &isin; S &prime;(Rn),
</p>
<p>F[Dα
x
</p>
<p>u] = (iξ)αû,
</p>
<p>and
</p>
<p>F[xαu] = (i D)αξ û.
</p>
<p>The Fourier transform on S &prime;(Rn) is particularly useful in the construction of
fundamental solutions. Consider the constant coefficient operator
</p>
<p>L =
&sum;
</p>
<p>|α|&le;m
aαD
</p>
<p>α
</p>
<p>with aα &isin; C. According to Lemma 13.6 and (13.19), the Fourier transform of the
equation
</p>
<p>LΦ = δ
</p>
<p>is
</p>
<p>P(ξ)Φ̂ = 1,
</p>
<p>where
</p>
<p>P(ξ) :=
&sum;
</p>
<p>|α|&le;m
aα(iξ)
</p>
<p>α.
</p>
<p>If the reciprocal of P(ξ) makes sense as a tempered distribution then we can set
</p>
<p>Φ̂(ξ) = 1/P(ξ) and take the inverse Fourier transform to construct a fundamental
solution Φ as an element of S &prime;(Rn).
</p>
<p>Example 13.7 The polynomial corresponding to &minus;� on Rn is P(ξ) = |ξ|2. For
n &ge; 3 the function |ξ|&minus;2 is locally integrable and decays at infinity, so this defines a
tempered distribution. Hence we should be able to computeΦ as the inverse Fourier
</p>
<p>transform of |ξ|&minus;2.</p>
<p/>
</div>
<div class="page"><p/>
<p>270 13 The Fourier Transform
</p>
<p>Because |ξ|&minus;2 is not globally integrable, we cannot apply the formula (13.11)
directly. A trick to get around this is based on the fact that
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>e&minus;αt dt = α&minus;1
</p>
<p>for α &gt; 0. Setting α = |ξ|2 gives
</p>
<p>|ξ|&minus;2 =
&int; &infin;
</p>
<p>0
</p>
<p>e&minus;t |ξ|
2
</p>
<p>dt
</p>
<p>for ξ �= 0. We can pair both sides with a Schwartz function ψ(ξ) and integrate to
show that
</p>
<p>F
&minus;1 [|ξ|&minus;2
</p>
<p>]
</p>
<p>=
&int; &infin;
</p>
<p>0
</p>
<p>F
&minus;1
</p>
<p>[
</p>
<p>e&minus;t |ξ|
2
]
</p>
<p>dt. (13.20)
</p>
<p>Setting a = 1/(4t) in (13.8) gives
</p>
<p>F
&minus;1
</p>
<p>[
</p>
<p>e&minus;t |ξ|
2
]
</p>
<p>(x) = (4πt)&minus; n2 e&minus;|x|2/4t ,
</p>
<p>so that (13.20) reduces to
</p>
<p>F
&minus;1 [|ξ|&minus;2
</p>
<p>]
</p>
<p>=
&int; &infin;
</p>
<p>0
</p>
<p>(4πt)&minus;
n
2 e&minus;|x|
</p>
<p>2/4t dt.
</p>
<p>To evaluate the integral we substitute s = |x|2 /4t to obtain
</p>
<p>F
&minus;1 [|ξ|&minus;2
</p>
<p>]
</p>
<p>=
&int; &infin;
</p>
<p>0
</p>
<p>(
</p>
<p>π |x|2
s
</p>
<p>)&minus; n
2
</p>
<p>e&minus;s
|x|2
4s2
</p>
<p>ds
</p>
<p>= 1
4
π&minus;
</p>
<p>n
2 |x|2&minus;n
</p>
<p>&int; &infin;
</p>
<p>0
</p>
<p>s
n
2
&minus;2e&minus;s ds.
</p>
<p>In terms of the gamma function (2.17) this calculation gives the fundamental solution
</p>
<p>Φ(x) = 1
4
π&minus;
</p>
<p>n
2Ŵ( n
</p>
<p>2
&minus; 1) |x|2&minus;n . (13.21)
</p>
<p>This agrees with the formula for Φ from Theorem 12.8, because
</p>
<p>An =
2π
</p>
<p>n
2
</p>
<p>Ŵ( n
2
)
</p>
<p>and
</p>
<p>Ŵ( n
2
&minus; 1) =
</p>
<p>Ŵ( n
2
)
</p>
<p>n
2
&minus; 1 . &diams;</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_12">http://dx.doi.org/10.1007/978-3-319-48936-0_12</a></div>
</div>
<div class="page"><p/>
<p>13.3 The Wave Kernel 271
</p>
<p>13.3 The Wave Kernel
</p>
<p>Following the discussion in Sect. 12.6, we can try to define the wave kernel Wt on
</p>
<p>Rn by solving the distributional equations
</p>
<p>(
</p>
<p>&part;2
</p>
<p>&part;t2
&minus;�
</p>
<p>)
</p>
<p>Wt = 0, W0 = 0,
&part;Wt
</p>
<p>&part;t
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>t=0
= δ. (13.22)
</p>
<p>If we assume that Wt &isin; S &prime;(Rn), then the spatial Fourier transform allows us to
analyze this equation by turning it into a simple ODE.
</p>
<p>For each t define Ŵt &isin; S &prime;(Rn) by the (spatial) distributional transform (13.16).
By Lemma 13.6 and (13.19), (13.22) transforms to
</p>
<p>(
</p>
<p>&part;2
</p>
<p>&part;t2
+ |ξ|2
</p>
<p>)
</p>
<p>Ŵt = 0, Ŵ0 = 0,
&part;Ŵt
</p>
<p>&part;t
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>t=0
= 1.
</p>
<p>The unique solution to this ODE is
</p>
<p>Ŵt (ξ) =
{
</p>
<p>sin(t |ξ|)
|ξ| , ξ �= 0,
</p>
<p>t, ξ = 0.
(13.23)
</p>
<p>The function Ŵt is smooth and bounded, and therefore defines a tempered distribution
</p>
<p>on Rn . The inverse Fourier transform Wt &isin; S &prime;(Rn) thus yields a general solution
formula for the wave equation on Rn . For initial conditions g, h &isin; S(Rn),
</p>
<p>u(t, &middot;) = &part;Wt
&part;t
</p>
<p>&lowast; g + Wt &lowast; h. (13.24)
</p>
<p>The direct computation of the inverse Fourier transform of (13.23) is rather tricky,
</p>
<p>but we can check this formula against the results we already know. For n = 1 we
have Wt = 12χ[&minus;t,t] from the d&rsquo;Alembert formula. Since this is integrable the Fourier
transform can be computed directly:
</p>
<p>Ŵt (ξ) =
&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>1
</p>
<p>2
χ[&minus;t,t](x) e
</p>
<p>&minus;i xξ dx
</p>
<p>= 1
2
</p>
<p>&int; t
</p>
<p>&minus;t
e&minus;i xξ dx
</p>
<p>=
{
</p>
<p>sin(tξ)
ξ
</p>
<p>, ξ �= 0,
t, ξ = 0.
</p>
<p>For n = 3, the Kirchhoff formula from Theorem 4.10 shows that the wave kernel
is the distribution defined by</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_12">http://dx.doi.org/10.1007/978-3-319-48936-0_12</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_4">http://dx.doi.org/10.1007/978-3-319-48936-0_4</a></div>
</div>
<div class="page"><p/>
<p>272 13 The Fourier Transform
</p>
<p>(Wt ,ψ) :=
1
</p>
<p>4πt
</p>
<p>&int;
</p>
<p>&part;B(0;t)
ψ d S,
</p>
<p>for ψ &isin; S(R3). By definition, the Fourier transform is given by
(
</p>
<p>Ŵt ,ψ
)
</p>
<p>:= 1
4πt
</p>
<p>&int;
</p>
<p>&part;B(0;t)
ψ̂(x) d S(x)
</p>
<p>= 1
4πt
</p>
<p>&int;
</p>
<p>&part;B(0;t)
</p>
<p>(&int;
</p>
<p>R3
</p>
<p>e&minus;i x&middot;ξψ(ξ) d3ξ
</p>
<p>)
</p>
<p>d S(x).
</p>
<p>Since ψ(y) has rapid decay as y &rarr; &infin; and the x integral is restricted to a sphere,
we can switch the order of integration and conclude that
</p>
<p>Ŵt (ξ) =
1
</p>
<p>4πt
</p>
<p>&int;
</p>
<p>&part;B(0;t)
e&minus;i x&middot;ξ d S(x).
</p>
<p>To compute this surface integral, note thatwe could rotate the x coordinatewithout
</p>
<p>changing the result of the integration. It therefore suffices to consider the case where
</p>
<p>ξ is parallel to the x3 axis. If we then use the spherical coordinates (r, θ,φ) for the
</p>
<p>x variables, this gives
</p>
<p>x &middot; ξ = |ξ| r cosφ.
</p>
<p>For the surface integral at radius r = t ,
</p>
<p>d S(x) = t2 sin φ dφ dθ.
</p>
<p>The Fourier transform is thus
</p>
<p>Ŵt (ξ) =
1
</p>
<p>4πt
</p>
<p>&int; 2π
</p>
<p>0
</p>
<p>&int; π
</p>
<p>0
</p>
<p>e&minus;i t |ξ| cos θ t2 sin φ dφ dθ
</p>
<p>= t
2
</p>
<p>&int; π
</p>
<p>0
</p>
<p>e&minus;i t |ξ| cos θ sin φ dφ.
</p>
<p>With the substitution u = cosφ this becomes
</p>
<p>Ŵt (ξ) :=
t
</p>
<p>2
</p>
<p>&int; 1
</p>
<p>&minus;1
e&minus;i t |ξ|u du
</p>
<p>=
{
</p>
<p>sin(t |ξ|)
|ξ| , ξ �= 0,
</p>
<p>t, ξ = 0.
</p>
<p>Hence the Kirchhoff formula agrees with the transform solution (13.23).</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4 The Heat Kernel 273
</p>
<p>13.4 The Heat Kernel
</p>
<p>By analogy with (13.22), the heat kernel Ht is defined as the solution of the distrib-
</p>
<p>utional equation
(
</p>
<p>&part;
</p>
<p>&part;t
&minus;�
</p>
<p>)
</p>
<p>Ht = 0, H0 = δ. (13.25)
</p>
<p>Assuming Ht &isin; S &prime;(Rn), let Ĥt denote the spatial Fourier transform of Ht . By Lemma
13.6 and (13.19), (13.25) transforms to
</p>
<p>(
</p>
<p>&part;
</p>
<p>&part;t
+ |ξ|2
</p>
<p>)
</p>
<p>Ĥt = 0, Ĥ0 = 1.
</p>
<p>This simple ODE has the unique solution
</p>
<p>Ĥt (ξ) = e&minus;t |ξ|
2
</p>
<p>. (13.26)
</p>
<p>Because Ĥt is a Schwartz function for t &gt; 0, we can compute the inverse Fourier
</p>
<p>transform by the direct integral formula (13.11), which gives
</p>
<p>Ht (x) = (2π)&minus;n
&int;
</p>
<p>Rn
</p>
<p>eiξ&middot;xe&minus;t |ξ|
2
</p>
<p>dnξ.
</p>
<p>According to (13.8), this inverse transform is
</p>
<p>Ht (x) = (4πt)&minus;
n
2 e&minus;|x|
</p>
<p>2/4t . (13.27)
</p>
<p>In Sect. 6.3 we guessed this formula from a calculation in the one-dimensional case.
</p>
<p>The Fourier transform allows for a systematic derivation.
</p>
<p>13.5 Exercises
</p>
<p>13.1 LetH &sub; R2 denote the upper half space {x2 &gt; 0}. The Poisson kernel onH is
the distributional solution of the equation
</p>
<p>�P = 0, P|x2=0 = δ.
</p>
<p>(a) Let P̂(ξ, x2) denote the distributional Fourier transform of P with respect to the
</p>
<p>x1 variable. Find the corresponding equation for P̂ .
</p>
<p>(b) Show that the unique solution of the ODE with P̂(&middot;, x2) &isin; S &prime;(R) is the function
</p>
<p>P̂(ξ, x2) = e&minus;x2|ξ|.</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_6">http://dx.doi.org/10.1007/978-3-319-48936-0_6</a></div>
</div>
<div class="page"><p/>
<p>274 13 The Fourier Transform
</p>
<p>(c) Compute the inverse transform to show that
</p>
<p>P(x) = x2
π(x21 + x22 )
</p>
<p>.
</p>
<p>(d) For f &isin; S(R), use P to write an integral formula for the solution of the Laplace
problem on H:
</p>
<p>�u = 0, u|x2=0 = f.
</p>
<p>13.2 For ψ &isin; S(R), the Poisson summation formula says that
</p>
<p>&infin;
&sum;
</p>
<p>k=&minus;&infin;
ψ(k) =
</p>
<p>&infin;
&sum;
</p>
<p>m=&minus;&infin;
ψ̂(2πm).
</p>
<p>Derive this formula using the steps below.
</p>
<p>(a) Define a periodic function f &isin; C&infin;(T) (where T := R/2πZ as in Sect. 8.2) by
averaging ψ̂,
</p>
<p>f (x) :=
&infin;
</p>
<p>&sum;
</p>
<p>m=&minus;&infin;
ψ̂(x + 2πm).
</p>
<p>Show that
</p>
<p>ck[ f ] = ψ(&minus;k).
</p>
<p>(b) Obtain the summation formula by comparing f to its Fourier series expansion
</p>
<p>at x = 0.
</p>
<p>13.3 Recall that the heat equation on T was solved by Fourier series in Theorem
</p>
<p>8.13.
</p>
<p>(a) Use the solution formula (8.44) to show that the heat kernel on T is given by the
</p>
<p>series
</p>
<p>ht (x) :=
1
</p>
<p>2π
</p>
<p>&infin;
&sum;
</p>
<p>k=&minus;&infin;
e&minus;k
</p>
<p>2t+ikx .
</p>
<p>(b) Use the Poisson summation formula fromExercise 13.2 to show that the periodic
</p>
<p>heat kernel ht and the heat kernel Ht on R are related by averaging
</p>
<p>ht (x) =
&infin;
</p>
<p>&sum;
</p>
<p>m=&minus;&infin;
Ht (x + 2πm)
</p>
<p>for t &gt; 0. (Note that this shows ht (x) &gt; 0 for all x &isin; T, t &gt; 0, which is not
clear in the formula from (a).)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_8">http://dx.doi.org/10.1007/978-3-319-48936-0_8</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_8">http://dx.doi.org/10.1007/978-3-319-48936-0_8</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_8">http://dx.doi.org/10.1007/978-3-319-48936-0_8</a></div>
</div>
<div class="page"><p/>
<p>13.5 Exercises 275
</p>
<p>13.4 The Schr&ouml;dinger equation on Rn ,
</p>
<p>&minus;i &part;u
&part;t
</p>
<p>&minus;�u = 0, u|t=0 = g,
</p>
<p>was introduced in Exercise 4.7.
</p>
<p>(a) Assuming that g &isin; S(Rn), find a formula for the spatial Fourier transform û(t, ξ).
(b) Show that the result from Exercise 4.7,
</p>
<p>&int;
</p>
<p>Rn
</p>
<p>|u(t, x)|2 dn x =
&int;
</p>
<p>Rn
</p>
<p>|g|2 dn x
</p>
<p>for all t &ge; 0, follows from the Plancherel theorem (Theorem 13.5).</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_4">http://dx.doi.org/10.1007/978-3-319-48936-0_4</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_4">http://dx.doi.org/10.1007/978-3-319-48936-0_4</a></div>
</div>
<div class="page"><p/>
<p>Erratum to: Introduction to Partial
</p>
<p>Differential Equations
</p>
<p>Erratum to:
</p>
<p>D. Borthwick, Introduction to Partial Differential Equations,
</p>
<p>Universitext, http://doi.org/10.1007/978-3-319-48936-0
</p>
<p>In the original version of the book, the belated corrections from author for Chaps. 2,
</p>
<p>3, 4, 6 and 11 have been incorporated.
</p>
<p>The updated online version of these chapters can be found at
http://doi.org/10.1007/978-3-319-48936-0
http://doi.org/10.1007/978-3-319-48936-0_2
http://doi.org/10.1007/978-3-319-48936-0_3
http://doi.org/10.1007/978-3-319-48936-0_4
http://doi.org/10.1007/978-3-319-48936-0_6
http://doi.org/10.1007/978-3-319-48936-0_11
</p>
<p>&copy; Springer International Publishing AG 2018
</p>
<p>D. Borthwick, Introduction to Partial Differential Equations,
</p>
<p>Universitext, http://doi.org/10.1007/978-3-319-48936-0_14
</p>
<p>E1</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0">http://dx.doi.org/10.1007/978-3-319-48936-0</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_2">http://dx.doi.org/10.1007/978-3-319-48936-0_2</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_3">http://dx.doi.org/10.1007/978-3-319-48936-0_3</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_4">http://dx.doi.org/10.1007/978-3-319-48936-0_4</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_6">http://dx.doi.org/10.1007/978-3-319-48936-0_6</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_11">http://dx.doi.org/10.1007/978-3-319-48936-0_11</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_11">http://dx.doi.org/10.1007/978-3-319-48936-0_11</a></div>
</div>
<div class="page"><p/>
<p>Appendix A
</p>
<p>Analysis Foundations
</p>
<p>In this section we will develop some implications of the completeness axiom for R
</p>
<p>which are referenced in the text.
</p>
<p>The fundamental result from which the others follow is the equivalence of com-
</p>
<p>pactness and sequential compactness for subsets of Rn . Recall from Sect. 11.6 that
</p>
<p>a set A is sequentially compact if every sequence within A contains a subsequence
</p>
<p>converging to a limit in A. The equivalence was first proven by Bernard Bolzano in
</p>
<p>the early 19th century, and later rediscovered by Karl Weierstrass.
</p>
<p>Theorem A.1 (Bolzano-Weierstrass) In Rn a subset is sequentially compact if and
</p>
<p>only if it is closed and bounded.
</p>
<p>Proof If A &sub; Rn is unbounded, then there exists a sequence of points
{
</p>
<p>x j
</p>
<p>}
</p>
<p>&sub; A
</p>
<p>with
∣
</p>
<p>∣
</p>
<p>x j
</p>
<p>∣
</p>
<p>∣ &rarr; &infin;. Any subsequence has the same property, so
{
</p>
<p>x j
</p>
<p>}
</p>
<p>has no convergent
</p>
<p>subsequence. If A is not closed, then there is some w /&isin; A which is a boundary
</p>
<p>point of A. Every neighborhood of w thus includes points of A, so there exists a
</p>
<p>sequence
{
</p>
<p>x j
</p>
<p>}
</p>
<p>&sub; A converging to w. All subsequences of
{
</p>
<p>x j
</p>
<p>}
</p>
<p>also converge to
</p>
<p>w, and therefore no subsequence converges in A. We conclude that a sequentially
</p>
<p>compact subset of Rn is closed and bounded.
</p>
<p>For the converse argument, let us first consider the one-dimensional case. Let
{
</p>
<p>x j
}
</p>
<p>be a sequence in a bounded set A &sub; R. For each n the real number
</p>
<p>bn := sup {xk; k &ge; n} (A.1)
</p>
<p>exists by the completeness axiom. The sequence {bn} is decreasing, because the
</p>
<p>supremum is taken over successively smaller sets, and also bounded by the hypothesis
</p>
<p>on A. Therefore the number
</p>
<p>α := inf
n&isin;N
</p>
<p>bn
</p>
<p>is well-defined in R. The fact that {bn} is decreasing implies bn &ge; α for all n.
</p>
<p>We claim that a subsequence of xk converges to α. For this purpose it suffices to
</p>
<p>show that the interval (α &minus; ε,α + ε) contains infinitely many xk for each ε &gt; 0.
</p>
<p>&copy; Springer International Publishing AG 2016
</p>
<p>D. Borthwick, Introduction to Partial Differential Equations,
</p>
<p>Universitext, DOI 10.1007/978-3-319-48936-0
</p>
<p>277</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_11">http://dx.doi.org/10.1007/978-3-319-48936-0_11</a></div>
</div>
<div class="page"><p/>
<p>278 Appendix A: Analysis Foundations
</p>
<p>If this were not the case, then for some n we would have xk /&isin; (α &minus; ε,α + ε) for
</p>
<p>all k &ge; n. This would imply either bn &le; α &minus; ε or bn &ge; α + ε, both of which are
</p>
<p>impossible by the definition of α.
</p>
<p>This proves the existence of a subsequence converging to α. The fact that A is
</p>
<p>closed implies α &isin; A, so this completes the argument that a closed bounded subset
</p>
<p>of R is sequentially compact.
</p>
<p>To extend this argument to higher dimensions, consider a sequence {xk} in a
</p>
<p>compact subset A &sub; Rn . The sequence of first coordinates of the xk is a bounded
</p>
<p>sequence in R, so the above argument yields a subsequence such that the first coor-
</p>
<p>dinates converge. We can then restrict our attention to this subsequence and apply
</p>
<p>the same reasoning to the second coordinate, and so on. After n steps this procedure
</p>
<p>produces a subsequence which converges to an element of A. �
</p>
<p>Bolzano used sequential compactness to prove the following result, which serves
</p>
<p>as the foundation for applications of calculus to optimization problems.
</p>
<p>Theorem A.2 (Extreme value theorem) For a compact set K &sub; Rn , a continuous
</p>
<p>function K &rarr; R achieves a maximum and minimum value on K .
</p>
<p>Proof Assume that f : K &rarr; R is continuous. We will show first that f is bounded.
</p>
<p>Suppose there is a sequence x j &isin; K such that
∣
</p>
<p>∣ f (x j )
∣
</p>
<p>∣ &rarr; &infin;. By Theorem A.1,
</p>
<p>after restricting to a subsequence if necessary, we can assume that x j &rarr; w &isin;
</p>
<p>K . Continuity implies f (x j ) &rarr; f (w), but this is impossible if
∣
</p>
<p>∣ f (x j )
∣
</p>
<p>∣ &rarr; &infin;.
</p>
<p>Therefore a continuous function on K is bounded.
</p>
<p>Since f (K ) is a bounded subset of R, b := sup f (K ) exists in R by the com-
</p>
<p>pleteness axiom. To prove that f achieves a maximum, we need to show b &isin; f (K ).
</p>
<p>If b /&isin; f (K ) then the function
</p>
<p>h(x) :=
1
</p>
<p>b &minus; f (x)
</p>
<p>is continuous on K , and therefore bounded by the above argument. However, h(x) &le;
</p>
<p>M for x &isin; K would imply that sup f (K ) &le; b &minus; 1/M , contradicting the definition
</p>
<p>of b. Therefore b &isin; f (K ), so f achieves a maximum. A similar argument applies to
</p>
<p>the minimum. �
</p>
<p>The final result is the completeness of Rn as a normed vector space, as noted in
</p>
<p>Sect. 7.4.
</p>
<p>Theorem A.3 In Rn a sequence converges if and only if it is Cauchy.
</p>
<p>Proof We have already noted that a convergent sequence is Cauchy in a normed
</p>
<p>vector space. Suppose that {xk} is a Cauchy sequence in R
n . This implies in particular
</p>
<p>that the sequence is bounded. Therefore, by Theorem A.1, there exists a subsequence
</p>
<p>converging to some w &isin; Rn .</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-48936-0_7">http://dx.doi.org/10.1007/978-3-319-48936-0_7</a></div>
</div>
<div class="page"><p/>
<p>Appendix A: Analysis Foundations 279
</p>
<p>By the definition of Cauchy, for ε &gt; 0 there exists N sufficiently large such that
</p>
<p>∣
</p>
<p>∣
</p>
<p>x j &minus; xk
∣
</p>
<p>∣ &lt; ε
</p>
<p>for all j, k &ge; N . We can also choose an element xl in the subsequence such that
</p>
<p>l &ge; N and
</p>
<p>|xl &minus; w| &lt; ε.
</p>
<p>The triangle inequality then gives
</p>
<p>∣
</p>
<p>∣
</p>
<p>x j &minus; w
∣
</p>
<p>∣ &le;
∣
</p>
<p>∣
</p>
<p>x j &minus; xl
∣
</p>
<p>∣ + |xl &minus; w|
</p>
<p>&lt; 2ε,
</p>
<p>for all j &ge; N . Since the choice of ε was arbitrary, this shows that the full sequence
</p>
<p>converges to w. �</p>
<p/>
</div>
<div class="page"><p/>
<p>References
</p>
<p>For additional analysis background
</p>
<p>Apostol, T. M. (1974). Mathematical analysis (2nd ed.). Addison-Wesley.
</p>
<p>Edwards, R. E. (1979). Fourier series. A modern introduction (2nd ed.). Graduate Texts in Mathe-
</p>
<p>matics 64. Springer.
</p>
<p>Folland, G. B. (1999). Real analysis (2nd ed.). Pure and Applied Mathematics. Wiley.
</p>
<p>Rudin, W. (1976). Principles of mathematical analysis (3rd ed.). McGraw-Hill.
</p>
<p>Stein, E. M., &amp; Shakarchi, R. (2003). Fourier analysis. Princeton Lectures in Analysis I: Princeton.
</p>
<p>Stein, E. M., &amp; Shakarchi, R. (2005). Real analysis. Princeton Lectures in Analysis III: Princeton.
</p>
<p>Stein, E. M., &amp; Shakarchi, R. (2011). Functional analysis. Princeton Lectures in Analysis IV:
</p>
<p>Princeton.
</p>
<p>Strichartz, R.S. (2003). A Guide to Distribution Theory and Fourier Transforms. World Scientific.
</p>
<p>For further study of PDE
</p>
<p>DiBenedetto, E. (2010). Partial differential equations (3nd ed.). Birkhuser.
</p>
<p>Evans, L. C. (2010). Partial differential equations (2nd ed.). Graduate Studies in Mathematics 19.
</p>
<p>American Mathematical Society.
</p>
<p>Gilbarg, D., &amp; Trudinger, N. S. (2001). Elliptic partial differential equations of second order.
</p>
<p>Classics in Mathematics: Springer.
</p>
<p>Jost, J. (2013). Partial differential equations (3rd ed.). Graduate Texts in Mathematics 214. Springer.
</p>
<p>Strauss, W. A. (2008). Partial differential equations. An introduction (2nd ed.). Wiley.
</p>
<p>Taylor, M. E. (2011). Partial differential equations I. Basic theory, Applied Mathematical Sciences
</p>
<p>115. Springer.
</p>
<p>Vasy, A. (2015). Partial differential equations, Graduate Studies in Mathematics 169. American
</p>
<p>Mathematical Society.
</p>
<p>&copy; Springer International Publishing AG 2016
</p>
<p>D. Borthwick, Introduction to Partial Differential Equations,
</p>
<p>Universitext, DOI 10.1007/978-3-319-48936-0
</p>
<p>281</p>
<p/>
</div>
<div class="page"><p/>
<p>Index
</p>
<p>A
</p>
<p>acoustic waves, 59
</p>
<p>action functional, 234
</p>
<p>advection, 27, 97
</p>
<p>almost everywhere, 115
</p>
<p>B
</p>
<p>ball, 12
</p>
<p>Banach space, 121
</p>
<p>Bessel
</p>
<p>functions, 82
</p>
<p>inequality, 123, 135
</p>
<p>zeros, 86
</p>
<p>Bolzano-Weierstrass theorem, 277
</p>
<p>boundary conditions
</p>
<p>Dirichlet, 4
</p>
<p>Neumann, 4
</p>
<p>self-adjoint, 126
</p>
<p>boundary point, 12
</p>
<p>Burger&rsquo;s equation, 43
</p>
<p>C
</p>
<p>calculus of variations, 234
</p>
<p>Cauchy-Schwarz inequality, 113
</p>
<p>Cauchy sequence, 120
</p>
<p>characteristic, 27, 33
</p>
<p>characteristic function, 115
</p>
<p>classical solution, 1
</p>
<p>closed, 12, 121
</p>
<p>closure, 13, 188
</p>
<p>coercive, 211
</p>
<p>compact, 13
</p>
<p>sequential, 224
</p>
<p>sequentially, 224, 277
</p>
<p>support, 13
</p>
<p>completeness, 9, 121, 278
</p>
<p>conduction, 97
</p>
<p>conjugate, 10
</p>
<p>connected, 12
</p>
<p>continuity equation, 27, 33
</p>
<p>convection, 97
</p>
<p>convergence
</p>
<p>pointwise, 137
</p>
<p>uniform, 137, 141
</p>
<p>convolution, 103, 139, 249
</p>
<p>Coulomb, 239
</p>
<p>D
</p>
<p>Darboux, 61
</p>
<p>delta function, 241, 243
</p>
<p>dense, 119
</p>
<p>Dirichlet, 138
</p>
<p>eigenvalues, 217, 228
</p>
<p>energy, 205
</p>
<p>kernel, 139
</p>
<p>Dirichlet&rsquo;s principle, 207
</p>
<p>dispersive estimate, 118
</p>
<p>distribution, 239, 242
</p>
<p>tempered, 267
</p>
<p>distributional derivative, 245
</p>
<p>divergence-free, 33
</p>
<p>divergence theorem, 20
</p>
<p>domain, 12
</p>
<p>domain of dependence, 55
</p>
<p>Duhamel&rsquo;s method, 54
</p>
<p>E
</p>
<p>eigenfunction, 77
</p>
<p>eigenvalue, 77
</p>
<p>elliptic, 3, 155, 167, 194, 237
</p>
<p>&copy; Springer International Publishing AG 2016
</p>
<p>D. Borthwick, Introduction to Partial Differential Equations,
</p>
<p>Universitext, DOI 10.1007/978-3-319-48936-0
</p>
<p>283</p>
<p/>
</div>
<div class="page"><p/>
<p>284 Index
</p>
<p>regularity, 214
</p>
<p>uniformly, 167, 172
</p>
<p>energy, 67, 73, 97, 166
</p>
<p>equilibrium, 155
</p>
<p>essential supremum, 118
</p>
<p>Euler, 60, 234
</p>
<p>formula, 11
</p>
<p>Euler-Lagrange equation, 235
</p>
<p>evolution equations, 3
</p>
<p>F
</p>
<p>finite element method, 6, 232
</p>
<p>flux, 25, 32
</p>
<p>forcing term, 42, 54
</p>
<p>Fourier, 97
</p>
<p>series, 134, 191, 215
</p>
<p>transform, 261
</p>
<p>functional, 205
</p>
<p>fundamental solution, 248
</p>
<p>G
</p>
<p>gamma function, 24, 270
</p>
<p>Gauss, 206
</p>
<p>Gaussian function, 262
</p>
<p>gradient, 18
</p>
<p>Green&rsquo;s function, 254
</p>
<p>Green&rsquo;s identities, 23
</p>
<p>H
</p>
<p>Hamilton-Jacobi equation, 43
</p>
<p>harmonic function, 155
</p>
<p>harmonic ODE, 17
</p>
<p>harmonic oscillator, 78
</p>
<p>heat equation, 93, 97, 104, 131, 148, 150,
</p>
<p>170
</p>
<p>heat kernel, 104, 273
</p>
<p>Heaviside step function, 102
</p>
<p>Helmholtz equation, 76, 132, 194
</p>
<p>Hermite polynomials, 94
</p>
<p>Hilbert space, 121
</p>
<p>H&ouml;lder continuity, 248
</p>
<p>Hopf&rsquo;s lemma, 175
</p>
<p>Huygens&rsquo; principle, 50, 64
</p>
<p>hyperbolic, 4
</p>
<p>I
</p>
<p>infimum, 9
</p>
<p>infinite propagation speed, 106
</p>
<p>inflow boundary, 42
</p>
<p>initial conditions, 4
</p>
<p>inner product, 111
</p>
<p>integrable, 116
</p>
<p>integral kernel, 104
</p>
<p>interior point, 12
</p>
<p>irrotational, 156
</p>
<p>K
</p>
<p>Kirchhoff, 63
</p>
<p>Klein-Gordon equation, 72
</p>
<p>L
</p>
<p>Lagrange, 28, 234
</p>
<p>Lagrangian derivative, 28, 34, 60
</p>
<p>Lagrangian function, 234
</p>
<p>Laplace equation, 155
</p>
<p>Laplacian, 3, 18, 59
</p>
<p>eigenvalue equation, 77
</p>
<p>polar coordinate, 82
</p>
<p>spherical, 88
</p>
<p>Legendre functions, 88
</p>
<p>linear conservation equation, 27, 33
</p>
<p>linear equation, 3
</p>
<p>Liouville&rsquo;s theorem, 174
</p>
<p>local integrability, 178
</p>
<p>M
</p>
<p>material derivative, 28
</p>
<p>maximum, 10
</p>
<p>measurable function, 115
</p>
<p>measure, 114
</p>
<p>Mersenne, 76, 79
</p>
<p>method of characteristics, 29, 47
</p>
<p>method of descent, 65
</p>
<p>method of images, 255
</p>
<p>minimal surface equation, 236
</p>
<p>minimax principle, 228
</p>
<p>minimum, 10
</p>
<p>Minkowski inequality, 117
</p>
<p>multi-index, 180
</p>
<p>N
</p>
<p>Navier-Stokes, 6
</p>
<p>neighborhood, 12
</p>
<p>Neumann
</p>
<p>boundary conditions, 4
</p>
<p>Newton, 207
</p>
<p>nodes, 79
</p>
<p>norm, 112
</p>
<p>normal vector, 19</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 285
</p>
<p>O
</p>
<p>open set, 12
</p>
<p>order, 1
</p>
<p>order notation, 145
</p>
<p>orthogonal, 112
</p>
<p>complement, 221
</p>
<p>orthonormal, 122
</p>
<p>basis, 122
</p>
<p>overtones, 76
</p>
<p>P
</p>
<p>parabolic, 4, 155, 172
</p>
<p>parallelogram law, 128
</p>
<p>piecewise C1 boundary, 14
</p>
<p>piecewise linear, 187
</p>
<p>plane wave, 72, 261
</p>
<p>Plateau problem, 235
</p>
<p>Poincar&eacute; constant, 211
</p>
<p>Poisson
</p>
<p>equation, 206, 214
</p>
<p>integral formula, 65
</p>
<p>kernel, 157, 273
</p>
<p>summation formula, 274
</p>
<p>polar coordinates, 23, 81
</p>
<p>potential function, 156
</p>
<p>principal value, 247
</p>
<p>propagation speed, 47, 49
</p>
<p>finite, 67
</p>
<p>Q
</p>
<p>quasilinear equation, 36
</p>
<p>R
</p>
<p>range of influence, 57, 64
</p>
<p>Rankine-Hugoniot condition, 184
</p>
<p>rapidly, 262
</p>
<p>rarefaction wave, 203
</p>
<p>Rayleigh
</p>
<p>principle, 218
</p>
<p>quotient, 218
</p>
<p>Rayleigh-Ritz method, 231
</p>
<p>reaction-diffusion equations, 109
</p>
<p>reaction term, 42
</p>
<p>regularity
</p>
<p>elliptic, 214
</p>
<p>Rellich&rsquo;s theorem, 219, 224
</p>
<p>resonance, 58
</p>
<p>S
</p>
<p>Schr&ouml;dinger equation, 71
</p>
<p>Schwartz functions, 262
</p>
<p>separation of variables, 75
</p>
<p>sequential compactness, 224
</p>
<p>sequentially compact, 224, 277
</p>
<p>sesquilinear, 112
</p>
<p>sesquilinearity, 112
</p>
<p>shock, 41, 43
</p>
<p>curve, 183
</p>
<p>smooth, 13
</p>
<p>Sobolev
</p>
<p>embedding theorem, 190
</p>
<p>regularity, 190
</p>
<p>spaces, 187
</p>
<p>solenoidal, 33, 156
</p>
<p>spectral
</p>
<p>analysis, 131
</p>
<p>theorem, 125, 217
</p>
<p>spectrum, 79
</p>
<p>spherical harmonics, 89
</p>
<p>stability, 5
</p>
<p>subharmonic, 164
</p>
<p>sup norm, 114
</p>
<p>superharmonic, 164
</p>
<p>superposition principle, 3
</p>
<p>support, 13
</p>
<p>supremum, 9
</p>
<p>surface integral, 19
</p>
<p>T
</p>
<p>telegraph equation, 70
</p>
<p>tempered distribution, 267
</p>
<p>test function, 177
</p>
<p>traffic equation, 37
</p>
<p>transport equation, 27
</p>
<p>V
</p>
<p>variation of parameters, 54
</p>
<p>vector space, 111
</p>
<p>W
</p>
<p>wave equation, 2, 47, 85, 271
</p>
<p>acoustic, 61
</p>
<p>damped, 93
</p>
<p>wave kernel, 259, 271
</p>
<p>weak derivative, 178
</p>
<p>weak solution, 1, 51, 177
</p>
<p>Weierstrass, 277
</p>
<p>well posed, 5, 166</p>
<p/>
</div>
<ul>	<li>Preface</li>
	<li>Contents</li>
	<li>Notations</li>
	<li>1 Introduction</li>
<ul>	<li>1.1 Partial Differential Equations</li>
	<li>1.2 Example: d'Alembert's Wave Equation</li>
	<li>1.3 Types of Equations</li>
	<li>1.4 Well Posed Problems</li>
	<li>1.5 Approaches</li>
</ul>
	<li>2 Preliminaries</li>
<ul>	<li>2.1 Real Numbers</li>
	<li>2.2 Complex Numbers</li>
	<li>2.3 Domains in mathbbRn</li>
	<li>2.4 Differentiability</li>
	<li>2.5 Ordinary Differential Equations</li>
	<li>2.6 Vector Calculus</li>
	<li>2.7 Exercises</li>
</ul>
	<li>3 Conservation Equations and Characteristics</li>
<ul>	<li>3.1 Model Problem: Oxygen in the Bloodstream</li>
	<li>3.2 Lagrangian Derivative and Characteristics</li>
	<li>3.3 Higher-Dimensional Equations</li>
	<li>3.4 Quasilinear Equations</li>
	<li>3.5 Exercises</li>
</ul>
	<li>4 &#13;TheWave Equation</li>
<ul>	<li>4.1 Model Problem: Vibrating String</li>
	<li>4.2 Characteristics</li>
	<li>4.3 Boundary Problems</li>
	<li>4.4 Forcing Terms</li>
	<li>4.5 Model Problem: Acoustic Waves</li>
	<li>4.6 Integral Solution Formulas</li>
	<li>4.7 Energy and Uniqueness</li>
	<li>4.8 Exercises</li>
</ul>
	<li>5 Separation of Variables</li>
<ul>	<li>5.1 Model Problem: Overtones</li>
	<li>5.2 Helmholtz Equation</li>
	<li>5.3 Circular Symmetry</li>
	<li>5.4 Spherical Symmetry</li>
	<li>5.5 Exercises</li>
</ul>
	<li>6 &#13;The Heat Equation</li>
<ul>	<li>6.1 Model Problem: Heat Flow in a Metal Rod</li>
	<li>6.2 Scale-Invariant Solution</li>
	<li>6.3 Integral Solution Formula</li>
	<li>6.4 Inhomogeneous Problem</li>
	<li>6.5 Exercises</li>
</ul>
	<li>7 Function Spaces</li>
<ul>	<li>7.1 Inner Products and Norms</li>
	<li>7.2 Lebesgue Integration</li>
	<li>7.3 Lp Spaces</li>
	<li>7.4 Convergence and Completeness</li>
	<li>7.5 Orthonormal Bases</li>
	<li>7.6 Self-adjointness</li>
	<li>7.7 Exercises</li>
</ul>
	<li>8 Fourier Series</li>
<ul>	<li>8.1 Series Solution of the Heat Equation</li>
	<li>8.2 Periodic Fourier Series</li>
	<li>8.3 Pointwise Convergence</li>
	<li>8.4 Uniform Convergence</li>
	<li>8.5 Convergence in L2</li>
	<li>8.6 Regularity and Fourier Coefficients</li>
	<li>8.7 Exercises</li>
</ul>
	<li>9 Maximum Principles</li>
<ul>	<li>9.1 Model Problem: The Laplace Equation</li>
	<li>9.2 Mean Value Formula</li>
	<li>9.3 Strong Principle for Subharmonic Functions</li>
	<li>9.4 Weak Principle for Elliptic Equations</li>
	<li>9.5 Application to the Heat Equation</li>
	<li>9.6 Exercises</li>
</ul>
	<li>10 Weak Solutions</li>
<ul>	<li>10.1 Test Functions and Weak Derivatives</li>
	<li>10.2 Weak Solutions of Continuity Equations</li>
	<li>10.3 Sobolev Spaces</li>
	<li>10.4 Sobolev Regularity</li>
	<li>10.5 Weak Formulation of Elliptic Equations</li>
	<li>10.6 Weak Formulation of Evolution Equations</li>
	<li>10.7 Exercises</li>
</ul>
	<li>11 Variational Methods</li>
<ul>	<li>11.1 Model Problem: The Poisson Equation</li>
	<li>11.2 Dirichlet's Principle</li>
	<li>11.3 Coercivity and Existence of a Minimum</li>
	<li>11.4 Elliptic Regularity</li>
	<li>11.5 Eigenvalues by Minimization</li>
	<li>11.6 Sequential Compactness</li>
	<li>11.7 Estimation of Eigenvalues</li>
	<li>11.8 Euler-Lagrange Equations</li>
	<li>11.9 Exercises</li>
</ul>
	<li>12 Distributions</li>
<ul>	<li>12.1 Model Problem: Coulomb's Law</li>
	<li>12.2 The Space of Distributions</li>
	<li>12.3 Distributional Derivatives</li>
	<li>12.4 Fundamental Solutions</li>
	<li>12.5 Green's Functions</li>
	<li>12.6 Time-Dependent Fundamental Solutions</li>
	<li>12.7 Exercises</li>
</ul>
	<li>13 The Fourier Transform</li>
<ul>	<li>13.1 Fourier Transform</li>
	<li>13.2 Tempered Distributions</li>
	<li>13.3 The Wave Kernel</li>
	<li>13.4 The Heat Kernel</li>
	<li>13.5 Exercises</li>
</ul>
	<li>14 Erratum to: Introduction to Partial Differential Equations</li>
<ul>	<li>Erratum to:&amp;#6;D. Borthwick, Introduction to Partial Differential Equations, Universitext, http://doi.org/10.1007/978-3-319-48936-0</li>
</ul>
	<li>Appendix A Analysis Foundations</li>
	<li>References</li>
	<li>Index</li>
</ul>
</body></html>