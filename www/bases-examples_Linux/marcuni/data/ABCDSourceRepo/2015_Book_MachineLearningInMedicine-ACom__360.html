<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Untitled</title>
</head>
<body><div class="page"><p/>
<p>Ton&nbsp;J.&nbsp;Cleophas&nbsp;&middot; Aeilko&nbsp;H.&nbsp;Zwinderman
</p>
<p>Machine 
Learning in 
Medicine - 
a Complete 
Overview</p>
<p/>
</div>
<div class="page"><p/>
<p>  Machine Learning in Medicine - a Complete 
Overview </p>
<p/>
</div>
<div class="page"><p/>
<p>                               </p>
<p/>
</div>
<div class="page"><p/>
<p>       Ton     J.     Cleophas     &bull;      Aeilko     H.     Zwinderman     
</p>
<p> Machine Learning in 
Medicine - a Complete 
Overview
</p>
<p>With the help from HENNY I. CLEOPHAS-ALLERS, 
</p>
<p>BChem                           </p>
<p/>
</div>
<div class="page"><p/>
<p>Additional material to this book can be downloaded from http://extras.springer.com.
</p>
<p> ISBN 978-3-319-15194-6      ISBN 978-3-319-15195-3 (eBook) 
 DOI 10.1007/978-3-319-15195-3 
</p>
<p> Library of Congress Control Number: 2015930334 
</p>
<p> Springer Cham Heidelberg New York Dordrecht London 
 &copy; Springer International Publishing Switzerland   2015 
 This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of 
the material is concerned, specifi cally the rights of translation, reprinting, reuse of illustrations, recitation, 
broadcasting, reproduction on microfi lms or in any other physical way, and transmission or information 
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology 
now known or hereafter developed. 
 The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication 
does not imply, even in the absence of a specifi c statement, that such names are exempt from the relevant 
protective laws and regulations and therefore free for general use. 
 The publisher, the authors and the editors are safe to assume that the advice and information in this book 
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the 
editors give a warranty, express or implied, with respect to the material contained herein or for any errors 
or omissions that may have been made. 
</p>
<p> Printed on acid-free paper 
</p>
<p> Springer International Publishing AG Switzerland is part of Springer Science+Business Media (www.
springer.com) 
</p>
<p>   Ton     J.     Cleophas   
  Department Medicine 
Albert Schweitzer Hospital 
  Sliedrecht ,  The Netherlands   
</p>
<p>   Aeilko     H.     Zwinderman   
  Department Biostatistics and Epidemiology 
 Academic Medical Center 
  Amsterdam ,  The Netherlands     </p>
<p/>
<div class="annotation"><a href="www.springer.com">www.springer.com</a></div>
<div class="annotation"><a href="www.springer.com">www.springer.com</a></div>
</div>
<div class="page"><p/>
<p>v
</p>
<p>  Pref ace    
</p>
<p> The amount of data stored in the world&rsquo;s databases doubles every 20 months, as 
</p>
<p>estimated by Usama Fayyad, one of the founders of machine learning and co-author 
</p>
<p>of the book  Advances in Knowledge Discovery and Data Mining  (ed. by the 
</p>
<p>American Association for Artifi cial Intelligence, Menlo Park, CA, USA, 1996), and 
</p>
<p>clinicians, familiar with traditional statistical methods, are at a loss to analyze them. 
</p>
<p> Traditional methods have, indeed, diffi culty to identify outliers in large datasets, 
</p>
<p>and to fi nd patterns in big data and data with multiple exposure/outcome variables. 
</p>
<p>In addition, analysis-rules for surveys and questionnaires, which are currently com-
</p>
<p>mon methods of data collection, are, essentially, missing. Fortunately, the new dis-
</p>
<p>cipline, machine learning, is able to cover all of these limitations. 
</p>
<p> So far, medical professionals have been rather reluctant to use machine learning. 
</p>
<p>Ravinda Khattree, co-author of the book  Computational Methods in Biomedical 
</p>
<p>Research  (ed. by Chapman &amp; Hall, Baton Rouge, LA, USA, 2007) suggests that 
</p>
<p>there may be historical reasons: technological (doctors are better than computers 
</p>
<p>(?)), legal, cultural (doctors are better trusted). Also, in the fi eld of diagnosis mak-
</p>
<p>ing, few doctors may want a computer checking them, are interested in collabora-
</p>
<p>tion with a computer or with computer engineers. 
</p>
<p> Adequate health and health care will, however, soon be impossible without 
</p>
<p>proper data supervision from modern machine learning methodologies like cluster 
</p>
<p>models, neural networks, and other data mining methodologies. The current book is 
</p>
<p>the fi rst publication of a complete overview of machine learning methodologies for 
</p>
<p>the medical and health sector, and it was written as a training companion, and as a 
</p>
<p>must-read, not only for physicians and students, but also for anyone involved in the 
</p>
<p>process and progress of health and health care. 
</p>
<p> Some of the 80 chapters have already appeared in Springer&rsquo;s Cookbook Briefs, 
</p>
<p>but they have been rewritten and updated. All of the chapters have two core charac-
</p>
<p>teristics. First, they are intended for current usage, and they are, particularly, con-
</p>
<p>cerned with improving that usage. Second, they try and tell what readers need to 
</p>
<p>know in order to understand the methods. </p>
<p/>
</div>
<div class="page"><p/>
<p>vi
</p>
<p> In a nonmathematical way, stepwise analyses of the below three most important 
</p>
<p>classes of machine learning methods will be reviewed:
</p>
<p>   Cluster and classifi cation models (Chaps.   1    ,   2    ,   3    ,   4    ,   5    ,   6    ,   7    ,   8    ,   9    ,   10    ,   11    ,   12    ,   13    ,   14    , 
</p>
<p>  15    ,   16    ,   17    , and   18    ),  
</p>
<p>  (Log)linear models (Chaps.   19    ,   20    ,   21    ,   22    ,   23    ,   24    ,   25    ,   26    ,   27    ,   28    ,   29    ,   30    ,   31    ,   32    ,   33    , 
</p>
<p>  34    ,   35    ,   36    ,   37    ,   38    ,   39    ,   40    ,   41    ,   42    ,   43    ,   44    ,   45    ,   46    ,   47    ,   48    , and   49    ),  
</p>
<p>  Rules models (Chaps.   50    ,   51    ,   52    ,   53    ,   54    ,   55    ,   56    ,   57    ,   58    ,   59    ,   60    ,   61    ,   62    ,   63    ,   64    ,   65    , 
</p>
<p>  66    ,   67    ,   68    ,   69    ,   70    ,   71    ,   72    ,   73    ,   74    ,   75    ,   76    ,   77    ,   78    ,   79    , and   80    ).    
</p>
<p> The book will include basic methodologies like typology of medical data, 
</p>
<p>quantile- quantile plots for making a start with your data, rate analysis and trend 
</p>
<p>analysis as more powerful alternatives to risk analysis and traditional tests, probit 
</p>
<p>models for binary effects on treatment frequencies, higher order polynomes for cir-
</p>
<p>cadian phenomena, contingency tables and its myriad applications. Particularly, 
</p>
<p>Chaps.   9    ,   14    ,   15    ,   18    ,   45    ,   48    ,   49    ,   79    , and   80     will review these methodologies. 
</p>
<p> Chapter   7     describes the use of visualization processes instead of calculus meth-
</p>
<p>ods for data mining. Chapter   8     describes the use of trained clusters, a scientifi cally 
</p>
<p>more appropriate alternative to traditional cluster analysis. Chapter   69     describes 
</p>
<p>evolutionary operations (evops), and the evop calculators, already widely used for 
</p>
<p>chemical and technical process improvement. 
</p>
<p> Various automated analyses and simulation models are in Chaps.   4    ,   29    ,   31    , and 
</p>
<p>  32    . Chapters   67    ,   70    ,   71     review spectral plots, Bayesian networks, and support vec-
</p>
<p>tor machines. A fi rst description of several methods already employed by technical 
</p>
<p>and market scientists, and of their suitabilities for clinical research, is given in 
</p>
<p>Chaps.   37    ,   38    ,   39    , and   56     (ordinal scalings for inconsistent intervals, loglinear mod-
</p>
<p>els for varying incident risks, and iteration methods for cross-validations). 
</p>
<p> Modern methodologies like interval censored analyses, exploratory analyses 
</p>
<p>using pivoting trays, repeated measures logistic regression, doubly multivariate 
</p>
<p>analyses for health assessments, and gamma regression for best fi t prediction of 
</p>
<p>health parameters are reviewed in Chaps.   10    ,   11    ,   12    ,   13    ,   16    ,   17    ,   42    ,   46    , and   47    . 
</p>
<p> In order for the readers to perform their own analyses, SPSS data fi les of the 
</p>
<p>examples are given in extras.springer.com, as well as XML (eXtended Markup 
</p>
<p>Language), SPS (Syntax), and ZIP (compressed) fi les for outcome predictions in 
</p>
<p>future patients. Furthermore, four csv type excel fi les are available for data analysis 
</p>
<p>in the Konstanz information miner (Knime) and Weka (Waikato University New 
</p>
<p>Zealand) miner, widely approved free machine learning software packages on the 
</p>
<p>internet since 2006. Also a fi rst introduction is given to SPSS modeler (SPSS&rsquo; data 
</p>
<p>mining workbench, Chaps.   61    ,   64    ,   65    ), and to SPSS Amos, the graphical and non- 
</p>
<p>graphical data analyzer for the identifi cation of cause-effect relationships as prin-
</p>
<p>ciple goal of research (Chaps.   48     and   49    ). The free Davidwees polynomial grapher 
</p>
<p>is used in Chap.   79    . 
</p>
<p> This book will demonstrate that machine learning performs sometimes better 
</p>
<p>than traditional statistics does. For example, if the data perfectly fi t the cut-offs 
</p>
<p>for node splitting, because, e.g., ages &gt; 55 years give an exponential rise in 
</p>
<p>infarctions, then decision trees, optimal binning, and optimal scaling will be better 
</p>
<p>Preface </p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_1">http://dx.doi.org/10.1007/978-3-319-15195-3_1</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_2">http://dx.doi.org/10.1007/978-3-319-15195-3_2</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_3">http://dx.doi.org/10.1007/978-3-319-15195-3_3</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_4">http://dx.doi.org/10.1007/978-3-319-15195-3_4</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_5">http://dx.doi.org/10.1007/978-3-319-15195-3_5</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_6">http://dx.doi.org/10.1007/978-3-319-15195-3_6</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_7">http://dx.doi.org/10.1007/978-3-319-15195-3_7</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_8">http://dx.doi.org/10.1007/978-3-319-15195-3_8</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_9">http://dx.doi.org/10.1007/978-3-319-15195-3_9</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_10">http://dx.doi.org/10.1007/978-3-319-15195-3_10</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_11">http://dx.doi.org/10.1007/978-3-319-15195-3_11</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_12">http://dx.doi.org/10.1007/978-3-319-15195-3_12</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_13">http://dx.doi.org/10.1007/978-3-319-15195-3_13</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_14">http://dx.doi.org/10.1007/978-3-319-15195-3_14</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_15">http://dx.doi.org/10.1007/978-3-319-15195-3_15</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_16">http://dx.doi.org/10.1007/978-3-319-15195-3_16</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_17">http://dx.doi.org/10.1007/978-3-319-15195-3_17</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_18">http://dx.doi.org/10.1007/978-3-319-15195-3_18</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_19">http://dx.doi.org/10.1007/978-3-319-15195-3_19</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_20">http://dx.doi.org/10.1007/978-3-319-15195-3_20</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_21">http://dx.doi.org/10.1007/978-3-319-15195-3_21</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_22">http://dx.doi.org/10.1007/978-3-319-15195-3_22</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_23">http://dx.doi.org/10.1007/978-3-319-15195-3_23</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_24">http://dx.doi.org/10.1007/978-3-319-15195-3_24</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_25">http://dx.doi.org/10.1007/978-3-319-15195-3_25</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_26">http://dx.doi.org/10.1007/978-3-319-15195-3_26</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_27">http://dx.doi.org/10.1007/978-3-319-15195-3_27</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_28">http://dx.doi.org/10.1007/978-3-319-15195-3_28</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_29">http://dx.doi.org/10.1007/978-3-319-15195-3_29</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_30">http://dx.doi.org/10.1007/978-3-319-15195-3_30</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_31">http://dx.doi.org/10.1007/978-3-319-15195-3_31</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_32">http://dx.doi.org/10.1007/978-3-319-15195-3_32</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_33">http://dx.doi.org/10.1007/978-3-319-15195-3_33</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_34">http://dx.doi.org/10.1007/978-3-319-15195-3_34</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_35">http://dx.doi.org/10.1007/978-3-319-15195-3_35</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_36">http://dx.doi.org/10.1007/978-3-319-15195-3_36</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_37">http://dx.doi.org/10.1007/978-3-319-15195-3_37</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_38">http://dx.doi.org/10.1007/978-3-319-15195-3_38</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_39">http://dx.doi.org/10.1007/978-3-319-15195-3_39</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_40">http://dx.doi.org/10.1007/978-3-319-15195-3_40</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_41">http://dx.doi.org/10.1007/978-3-319-15195-3_41</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_42">http://dx.doi.org/10.1007/978-3-319-15195-3_42</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_43">http://dx.doi.org/10.1007/978-3-319-15195-3_43</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_44">http://dx.doi.org/10.1007/978-3-319-15195-3_44</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_45">http://dx.doi.org/10.1007/978-3-319-15195-3_45</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_46">http://dx.doi.org/10.1007/978-3-319-15195-3_46</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_47">http://dx.doi.org/10.1007/978-3-319-15195-3_47</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_48">http://dx.doi.org/10.1007/978-3-319-15195-3_48</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_49">http://dx.doi.org/10.1007/978-3-319-15195-3_49</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_50">http://dx.doi.org/10.1007/978-3-319-15195-3_50</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_51">http://dx.doi.org/10.1007/978-3-319-15195-3_51</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_52">http://dx.doi.org/10.1007/978-3-319-15195-3_52</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_53">http://dx.doi.org/10.1007/978-3-319-15195-3_53</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_54">http://dx.doi.org/10.1007/978-3-319-15195-3_54</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_55">http://dx.doi.org/10.1007/978-3-319-15195-3_55</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_56">http://dx.doi.org/10.1007/978-3-319-15195-3_56</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_57">http://dx.doi.org/10.1007/978-3-319-15195-3_57</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_58">http://dx.doi.org/10.1007/978-3-319-15195-3_58</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_59">http://dx.doi.org/10.1007/978-3-319-15195-3_59</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_60">http://dx.doi.org/10.1007/978-3-319-15195-3_60</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_61">http://dx.doi.org/10.1007/978-3-319-15195-3_61</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_62">http://dx.doi.org/10.1007/978-3-319-15195-3_62</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_63">http://dx.doi.org/10.1007/978-3-319-15195-3_63</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_64">http://dx.doi.org/10.1007/978-3-319-15195-3_64</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_65">http://dx.doi.org/10.1007/978-3-319-15195-3_65</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_66">http://dx.doi.org/10.1007/978-3-319-15195-3_66</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_67">http://dx.doi.org/10.1007/978-3-319-15195-3_67</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_68">http://dx.doi.org/10.1007/978-3-319-15195-3_68</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_69">http://dx.doi.org/10.1007/978-3-319-15195-3_69</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_70">http://dx.doi.org/10.1007/978-3-319-15195-3_70</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_71">http://dx.doi.org/10.1007/978-3-319-15195-3_71</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_72">http://dx.doi.org/10.1007/978-3-319-15195-3_72</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_73">http://dx.doi.org/10.1007/978-3-319-15195-3_73</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_74">http://dx.doi.org/10.1007/978-3-319-15195-3_74</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_75">http://dx.doi.org/10.1007/978-3-319-15195-3_75</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_76">http://dx.doi.org/10.1007/978-3-319-15195-3_76</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_77">http://dx.doi.org/10.1007/978-3-319-15195-3_77</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_78">http://dx.doi.org/10.1007/978-3-319-15195-3_78</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_79">http://dx.doi.org/10.1007/978-3-319-15195-3_79</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_80">http://dx.doi.org/10.1007/978-3-319-15195-3_80</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_9">http://dx.doi.org/10.1007/978-3-319-15195-3_9</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_14">http://dx.doi.org/10.1007/978-3-319-15195-3_14</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_15">http://dx.doi.org/10.1007/978-3-319-15195-3_15</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_18">http://dx.doi.org/10.1007/978-3-319-15195-3_18</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_45">http://dx.doi.org/10.1007/978-3-319-15195-3_45</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_48">http://dx.doi.org/10.1007/978-3-319-15195-3_48</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_49">http://dx.doi.org/10.1007/978-3-319-15195-3_49</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_79">http://dx.doi.org/10.1007/978-3-319-15195-3_79</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_80">http://dx.doi.org/10.1007/978-3-319-15195-3_80</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_7">http://dx.doi.org/10.1007/978-3-319-15195-3_7</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_8">http://dx.doi.org/10.1007/978-3-319-15195-3_8</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_69">http://dx.doi.org/10.1007/978-3-319-15195-3_69</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_4">http://dx.doi.org/10.1007/978-3-319-15195-3_4</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_29">http://dx.doi.org/10.1007/978-3-319-15195-3_29</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_31">http://dx.doi.org/10.1007/978-3-319-15195-3_31</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_32">http://dx.doi.org/10.1007/978-3-319-15195-3_32</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_67">http://dx.doi.org/10.1007/978-3-319-15195-3_67</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_70">http://dx.doi.org/10.1007/978-3-319-15195-3_70</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_71">http://dx.doi.org/10.1007/978-3-319-15195-3_71</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_37">http://dx.doi.org/10.1007/978-3-319-15195-3_37</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_38">http://dx.doi.org/10.1007/978-3-319-15195-3_38</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_39">http://dx.doi.org/10.1007/978-3-319-15195-3_39</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_56">http://dx.doi.org/10.1007/978-3-319-15195-3_56</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_10">http://dx.doi.org/10.1007/978-3-319-15195-3_10</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_11">http://dx.doi.org/10.1007/978-3-319-15195-3_11</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_12">http://dx.doi.org/10.1007/978-3-319-15195-3_12</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_13">http://dx.doi.org/10.1007/978-3-319-15195-3_13</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_16">http://dx.doi.org/10.1007/978-3-319-15195-3_16</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_17">http://dx.doi.org/10.1007/978-3-319-15195-3_17</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_42">http://dx.doi.org/10.1007/978-3-319-15195-3_42</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_46">http://dx.doi.org/10.1007/978-3-319-15195-3_46</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_47">http://dx.doi.org/10.1007/978-3-319-15195-3_47</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_61">http://dx.doi.org/10.1007/978-3-319-15195-3_61</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_64">http://dx.doi.org/10.1007/978-3-319-15195-3_64</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_65">http://dx.doi.org/10.1007/978-3-319-15195-3_65</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_48">http://dx.doi.org/10.1007/978-3-319-15195-3_48</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_49">http://dx.doi.org/10.1007/978-3-319-15195-3_49</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_79">http://dx.doi.org/10.1007/978-3-319-15195-3_79</a></div>
</div>
<div class="page"><p/>
<p>vii
</p>
<p>analysis- methods than traditional regression methods with age as continuous 
</p>
<p>predictor. Machine learning may have little options for adjusting confounding and 
</p>
<p>interaction, but you can add propensity scores and interaction variables to almost 
</p>
<p>any machine learning method. 
</p>
<p> Each chapter will start with purposes and scientifi c questions. Then, step-by-step 
</p>
<p>analyses, using both real data and simulated data examples, will be given. Finally, a 
</p>
<p>paragraph with conclusion, and references to the corresponding sites of three intro-
</p>
<p>ductory textbooks previously written by the same authors, is given.  
</p>
<p>  Lyon, France     Ton     J.     Cleophas   
</p>
<p> December 2015     Aeilko     H.     Zwinderman     
</p>
<p>Preface </p>
<p/>
</div>
<div class="page"><p/>
<p>                               </p>
<p/>
</div>
<div class="page"><p/>
<p>ix
</p>
<p>   Contents 
</p>
<p>   Part I Cluster and Classification Models
</p>
<p>1      Hierarchical Clustering and K-Means Clustering to Identify 
</p>
<p>Subgroups in Surveys (50 Patients) .......................................................  3   
</p>
<p>  General Purpose ........................................................................................  3   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  3   
</p>
<p>  Hierarchical Cluster Analysis ....................................................................  4   
</p>
<p>  K-Means Cluster Analysis.........................................................................  6   
</p>
<p>  Conclusion.................................................................................................  7   
</p>
<p>  Note ...........................................................................................................  8   
</p>
<p>    2      Density-Based Clustering to Identify Outlier Groups 
</p>
<p>in Otherwise Homogeneous Data (50 Patients) ....................................  9   
</p>
<p>  General Purpose ........................................................................................  9   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  9   
</p>
<p>  Density-Based Cluster Analysis ................................................................  10   
</p>
<p>  Conclusion.................................................................................................  11   
</p>
<p>  Note ...........................................................................................................  11   
</p>
<p>    3      Two Step Clustering to Identify Subgroups and Predict Subgroup 
</p>
<p>Memberships in Individual Future Patients (120 Patients) ................  13   
</p>
<p>  General Purpose ........................................................................................  13   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  13   
</p>
<p>  The Computer Teaches Itself to Make Predictions ...................................  14   
</p>
<p>  Conclusion.................................................................................................  15   
</p>
<p>  Note ...........................................................................................................  15   
</p>
<p>    4      Nearest Neighbors for Classifying New Medicines 
</p>
<p>(2 New and 25 Old Opioids) ...................................................................  17   
</p>
<p>  General Purpose ........................................................................................  17   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  17   </p>
<p/>
</div>
<div class="page"><p/>
<p>x
</p>
<p>  Example.....................................................................................................  17   
</p>
<p>  Conclusion.................................................................................................  24   
</p>
<p>  Note ...........................................................................................................  24   
</p>
<p>    5      Predicting High-Risk-Bin Memberships (1,445 Families) ...................  25   
</p>
<p>  General Purpose ........................................................................................  25   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  25   
</p>
<p>  Example.....................................................................................................  25   
</p>
<p>  Optimal Binning ........................................................................................  26   
</p>
<p>  Conclusion.................................................................................................  29   
</p>
<p>  Note ...........................................................................................................  29   
</p>
<p>    6      Predicting Outlier Memberships (2,000 Patients) ................................  31   
</p>
<p>  General Purpose ........................................................................................  31   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  31   
</p>
<p>  Example.....................................................................................................  31   
</p>
<p>  Conclusion.................................................................................................  34   
</p>
<p>  Note ...........................................................................................................  34   
</p>
<p>    7      Data Mining for Visualization of Health Processes (150 Patients)......  35   
</p>
<p>  General Purpose ........................................................................................  35   
</p>
<p>  Primary Scientifi c Question ......................................................................  35   
</p>
<p>  Example.....................................................................................................  36   
</p>
<p>  Knime Data Miner.....................................................................................  37   
</p>
<p>  Knime Workfl ow .......................................................................................  38   
</p>
<p>  Box and Whiskers Plots ............................................................................  39   
</p>
<p>  Lift Chart ...................................................................................................  39   
</p>
<p>  Histogram ..................................................................................................  40   
</p>
<p>  Line Plot ....................................................................................................  41   
</p>
<p>  Matrix of Scatter Plots ..............................................................................  42   
</p>
<p>  Parallel Coordinates ..................................................................................  43   
</p>
<p>  Hierarchical Cluster Analysis with SOTA (Self Organizing 
</p>
<p>Tree Algorithm) ........................................................................................  44   
</p>
<p>  Conclusion.................................................................................................  45   
</p>
<p>  Note ...........................................................................................................  46   
</p>
<p>    8      Trained Decision Trees for a More Meaningful Accuracy 
</p>
<p>(150 Patients) ...........................................................................................  47   
</p>
<p>  General Purpose ........................................................................................  47   
</p>
<p>  Primary Scientifi c Question ......................................................................  47   
</p>
<p>  Example.....................................................................................................  48   
</p>
<p>  Downloading the Knime Data Miner ........................................................  49   
</p>
<p>  Knime Workfl ow .......................................................................................  50   
</p>
<p>  Conclusion.................................................................................................  52   
</p>
<p>  Note ...........................................................................................................  52   
</p>
<p>Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>xi
</p>
<p>    9      Typology of Medical Data (51 Patients) ................................................  53   
</p>
<p>  General Purpose ........................................................................................  53   
</p>
<p>  Primary Scientifi c Question ......................................................................  54   
</p>
<p>  Example.....................................................................................................  54   
</p>
<p>  Nominal Variable ..................................................................................  55   
</p>
<p>  Ordinal Variable ....................................................................................  56   
</p>
<p>  Scale Variable .......................................................................................  57   
</p>
<p>  Conclusion.................................................................................................  59   
</p>
<p>  Note ...........................................................................................................  60   
</p>
<p>    10      Predictions from Nominal Clinical Data (450 Patients) ......................  61   
</p>
<p>  General Purpose ........................................................................................  61   
</p>
<p>  Primary Scientifi c Question ......................................................................  61   
</p>
<p>  Example.....................................................................................................  61   
</p>
<p>  Conclusion.................................................................................................  65   
</p>
<p>  Note ...........................................................................................................  65   
</p>
<p>    11      Predictions from Ordinal Clinical Data (450 Patients) ........................  67   
</p>
<p>  General Purpose ........................................................................................  67   
</p>
<p>  Primary Scientifi c Question ......................................................................  67   
</p>
<p>  Example.....................................................................................................  68   
</p>
<p>  Conclusion.................................................................................................  70   
</p>
<p>  Note ...........................................................................................................  70   
</p>
<p>    12      Assessing Relative Health Risks (3,000 Subjects) .................................  71   
</p>
<p>  General Purpose ........................................................................................  71   
</p>
<p>  Primary Scientifi c Question ......................................................................  71   
</p>
<p>  Example.....................................................................................................  71   
</p>
<p>  Conclusion.................................................................................................  75   
</p>
<p>  Note ...........................................................................................................  75   
</p>
<p>    13      Measuring Agreement (30 Patients) ......................................................  77   
</p>
<p>  General Purpose ........................................................................................  77   
</p>
<p>  Primary Scientifi c Question ......................................................................  77   
</p>
<p>  Example.....................................................................................................  77   
</p>
<p>  Conclusion.................................................................................................  79   
</p>
<p>  Note ...........................................................................................................  79   
</p>
<p>    14      Column Proportions for Testing Differences Between 
</p>
<p>Outcome Scores (450 Patients) ...............................................................  81   
</p>
<p>  General Purpose ........................................................................................  81   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  81   
</p>
<p>  Example.....................................................................................................  81   
</p>
<p>  Conclusion.................................................................................................  85   
</p>
<p>  Note ...........................................................................................................  85   
</p>
<p>Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>xii
</p>
<p>    15      Pivoting Trays and Tables for Improved Analysis 
</p>
<p>of Multidimensional Data (450 Patients) ...............................................  87   
</p>
<p>  General Purpose ........................................................................................  87   
</p>
<p>  Primary Scientifi c Question ......................................................................  87   
</p>
<p>  Example.....................................................................................................  87   
</p>
<p>  Conclusion.................................................................................................  94   
</p>
<p>  Note ...........................................................................................................  94   
</p>
<p>    16      Online Analytical Procedure Cubes, a More Rapid Approach 
</p>
<p>to Analyzing Frequencies (450 Patients) ...............................................  95   
</p>
<p>  General Purpose ........................................................................................  95   
</p>
<p>  Primary Scientifi c Question ......................................................................  95   
</p>
<p>  Example.....................................................................................................  95   
</p>
<p>  Conclusion.................................................................................................  99   
</p>
<p>  Note ...........................................................................................................  99   
</p>
<p>    17      Restructure Data Wizard for Data Classified the Wrong Way 
</p>
<p>(20 Patients) .............................................................................................  101   
</p>
<p>  General Purpose ........................................................................................  101   
</p>
<p>  Primary Scientifi c Question ......................................................................  103   
</p>
<p>  Example.....................................................................................................  103   
</p>
<p>  Conclusion.................................................................................................  104   
</p>
<p>  Note ...........................................................................................................  104   
</p>
<p>    18      Control Charts for Quality Control of Medicines 
</p>
<p>(164 Tablet Desintegration Times) .........................................................  105   
</p>
<p>  General Purpose ........................................................................................  105   
</p>
<p>  Primary Scientifi c Question ......................................................................  105   
</p>
<p>  Example.....................................................................................................  106   
</p>
<p>  Conclusion.................................................................................................  109   
</p>
<p>  Note ...........................................................................................................  110   
</p>
<p>    Part II (Log) Linear Models
</p>
<p>19      Linear, Logistic, and Cox Regression for Outcome Prediction 
</p>
<p>with Unpaired Data (20, 55, and 60 Patients) .......................................  113   
</p>
<p>  General Purpose ........................................................................................  113   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  113   
</p>
<p>  Linear Regression, the Computer Teaches Itself to Make Predictions ......  114   
</p>
<p>  Conclusion.................................................................................................  116   
</p>
<p>  Note ...........................................................................................................  116   
</p>
<p>  Logistic Regression, the Computer Teaches Itself to Make Predictions ...  116   
</p>
<p>  Conclusion.................................................................................................  118   
</p>
<p>  Note ...........................................................................................................  118   
</p>
<p>  Cox Regression, the Computer Teaches Itself to Make Predictions .........  118   
</p>
<p>  Conclusion.................................................................................................  121   
</p>
<p>  Note ...........................................................................................................  121   
</p>
<p>Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>xiii
</p>
<p>    20      Generalized Linear Models for Outcome Prediction 
</p>
<p>with Paired Data (100 Patients and 139 Physicians) ............................  123   
</p>
<p>  General Purpose ........................................................................................  123   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  123   
</p>
<p>  Generalized Linear Modeling, the Computer Teaches 
</p>
<p>Itself to Make Predictions .........................................................................  123   
</p>
<p>  Conclusion.................................................................................................  125   
</p>
<p>  Generalized Estimation Equations, the Computer Teaches 
</p>
<p>Itself to Make Predictions .........................................................................  126   
</p>
<p>  Conclusion.................................................................................................  129   
</p>
<p>  Note ...........................................................................................................  129   
</p>
<p>    21      Generalized Linear Models Event-Rates (50 Patients) ........................  131   
</p>
<p>  General Purpose ........................................................................................  131   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  131   
</p>
<p>  Example.....................................................................................................  131   
</p>
<p>  The Computer Teaches Itself to Make Predictions ...................................  132   
</p>
<p>  Conclusion.................................................................................................  135   
</p>
<p>  Note ...........................................................................................................  135   
</p>
<p>    22      Factor Analysis and Partial Least Squares (PLS) 
</p>
<p>for Complex-Data Reduction (250 Patients) .........................................  137   
</p>
<p>  General Purpose ........................................................................................  137   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  137   
</p>
<p>  Factor Analysis ..........................................................................................  138   
</p>
<p>  Partial Least Squares Analysis (PLS) ........................................................  140   
</p>
<p>  Traditional Linear Regression ...................................................................  142   
</p>
<p>  Conclusion.................................................................................................  142   
</p>
<p>  Note ...........................................................................................................  142   
</p>
<p>    23      Optimal Scaling of High-Sensitivity Analysis 
</p>
<p>of Health Predictors (250 Patients) ........................................................  143   
</p>
<p>  General Purpose ........................................................................................  143   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  143   
</p>
<p>  Traditional Multiple Linear Regression ....................................................  144   
</p>
<p>  Optimal Scaling Without Regularization ..................................................  145   
</p>
<p>  Optimal Scaling With Ridge Regression ...................................................  146   
</p>
<p>  Optimal Scaling With Lasso Regression ...................................................  147   
</p>
<p>  Optimal Scaling With Elastic Net Regression...........................................  147   
</p>
<p>  Conclusion.................................................................................................  148   
</p>
<p>  Note ...........................................................................................................  148   
</p>
<p>    24      Discriminant Analysis for Making a Diagnosis 
</p>
<p>from Multiple Outcomes (45 Patients) ..................................................  149   
</p>
<p>  General Purpose ........................................................................................  149   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  149   
</p>
<p>  The Computer Teaches Itself to Make Predictions ...................................  150   
</p>
<p>  Conclusion.................................................................................................  153   
</p>
<p>  Note ...........................................................................................................  153   
</p>
<p>Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>xiv
</p>
<p>    25      Weighted Least Squares for Adjusting Efficacy Data 
</p>
<p>with Inconsistent Spread (78 Patients) ..................................................  155   
</p>
<p>  General Purpose ........................................................................................  155   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  155   
</p>
<p>  Weighted Least Squares ............................................................................  156   
</p>
<p>  Conclusion.................................................................................................  158   
</p>
<p>  Note ...........................................................................................................  158   
</p>
<p>    26      Partial Correlations for Removing Interaction Effects 
</p>
<p>from Efficacy Data (64 Patients) ............................................................  159   
</p>
<p>  General Purpose ........................................................................................  159   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  159   
</p>
<p>  Partial Correlations ....................................................................................  160   
</p>
<p>  Conclusion.................................................................................................  162   
</p>
<p>  Note ...........................................................................................................  163   
</p>
<p>    27      Canonical Regression for Overall Statistics 
</p>
<p>of Multivariate Data (250 Patients) .......................................................  165   
</p>
<p>  General Purpose ........................................................................................  165   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  165   
</p>
<p>  Canonical Regression ................................................................................  166   
</p>
<p>  Conclusion.................................................................................................  169   
</p>
<p>  Note ...........................................................................................................  169   
</p>
<p>    28      Multinomial Regression for Outcome Categories (55 Patients) ..........  171   
</p>
<p>  General Purpose ........................................................................................  171   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  171   
</p>
<p>  The Computer Teaches Itself to Make Predictions ...................................  172   
</p>
<p>  Conclusion.................................................................................................  174   
</p>
<p>  Note ...........................................................................................................  174   
</p>
<p>    29      Various Methods for Analyzing Predictor Categories 
</p>
<p>(60 and 30 Patients) .................................................................................  175   
</p>
<p>  General Purpose ........................................................................................  175   
</p>
<p>  Specifi c Scientifi c Questions .....................................................................  175   
</p>
<p>  Example 1..................................................................................................  175   
</p>
<p>  Example 2..................................................................................................  179   
</p>
<p>  Conclusion.................................................................................................  182   
</p>
<p>  Note ...........................................................................................................  182   
</p>
<p>    30      Random Intercept Models for Both Outcome 
</p>
<p>and Predictor Categories (55 patients) ..................................................  183   
</p>
<p>  General Purpose ........................................................................................  183   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  184   
</p>
<p>  Example.....................................................................................................  184   
</p>
<p>  Conclusion.................................................................................................  187   
</p>
<p>  Note ...........................................................................................................  187   
</p>
<p>Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>xv
</p>
<p>    31      Automatic Regression for Maximizing Linear Relationships 
</p>
<p>(55 patients)..............................................................................................  189   
</p>
<p>  General Purpose ........................................................................................  189   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  189   
</p>
<p>  Data Example  ...........................................................................................  189   
</p>
<p>  The Computer Teaches Itself to Make Predictions ...................................  192   
</p>
<p>  Conclusion.................................................................................................  193   
</p>
<p>  Note ...........................................................................................................  194   
</p>
<p>    32      Simulation Models for Varying Predictors (9,000 Patients) ................  195   
</p>
<p>  General Purpose ........................................................................................  195   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  195   
</p>
<p>  Instead of Traditional Means and Standard Deviations, Monte 
</p>
<p>Carlo Simulations of the Input and Outcome Variables are Used 
</p>
<p>to Model the Data. This Enhances Precision, Particularly, 
</p>
<p>With non-Normal Data .............................................................................  196   
</p>
<p>  Conclusion.................................................................................................  200   
</p>
<p>  Note ...........................................................................................................  201   
</p>
<p>    33      Generalized Linear Mixed Models for Outcome Prediction 
</p>
<p>from Mixed Data (20 Patients) ...............................................................  203   
</p>
<p>  General Purpose ........................................................................................  203   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  203   
</p>
<p>  Example.....................................................................................................  203   
</p>
<p>  Conclusion.................................................................................................  206   
</p>
<p>  Note ...........................................................................................................  206   
</p>
<p>    34      Two-Stage Least Squares (35 Patients) .................................................  207   
</p>
<p>  General Purpose ........................................................................................  207   
</p>
<p>  Primary Scientifi c Question ......................................................................  207   
</p>
<p>  Example.....................................................................................................  208   
</p>
<p>  Conclusion.................................................................................................  210   
</p>
<p>  Note ...........................................................................................................  210   
</p>
<p>    35      Autoregressive Models for Longitudinal Data 
</p>
<p>(120 Mean Monthly Population Records) .............................................  211   
</p>
<p>  General Purpose ........................................................................................  211   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  211   
</p>
<p>  Example.....................................................................................................  212   
</p>
<p>  Conclusion.................................................................................................  216   
</p>
<p>  Note ...........................................................................................................  217   
</p>
<p>    36      Variance Components for Assessing the Magnitude 
</p>
<p>of Random Effects (40 Patients) .............................................................  219   
</p>
<p>  General Purpose ........................................................................................  219   
</p>
<p>  Primary Scientifi c Question ......................................................................  219   
</p>
<p>  Example.....................................................................................................  220   
</p>
<p>  Conclusion.................................................................................................  222   
</p>
<p>  Note ...........................................................................................................  222   
</p>
<p>Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>xvi
</p>
<p>    37      Ordinal Scaling for Clinical Scores with Inconsistent 
</p>
<p>Intervals (900 Patients) ...........................................................................  223   
</p>
<p>  General Purpose ........................................................................................  223   
</p>
<p>  Primary Scientifi c Questions .....................................................................  223   
</p>
<p>  Example.....................................................................................................  223   
</p>
<p>  Conclusion.................................................................................................  227   
</p>
<p>  Note ...........................................................................................................  227   
</p>
<p>    38      Loglinear Models for Assessing Incident Rates 
</p>
<p>with Varying Incident Risks (12 Populations) ......................................  229   
</p>
<p>  General Purpose ........................................................................................  229   
</p>
<p>  Primary Scientifi c Question ......................................................................  230   
</p>
<p>  Example.....................................................................................................  230   
</p>
<p>  Conclusion.................................................................................................  232   
</p>
<p>  Note ...........................................................................................................  232   
</p>
<p>    39      Loglinear Modeling for Outcome Categories (445 Patients) ...............  233   
</p>
<p>  General Purpose ........................................................................................  233   
</p>
<p>  Primary Scientifi c Question ......................................................................  233   
</p>
<p>  Example.....................................................................................................  234   
</p>
<p>  Conclusion.................................................................................................  239   
</p>
<p>  Note ...........................................................................................................  239   
</p>
<p>    40      Heterogeneity in Clinical Research: Mechanisms 
</p>
<p>Responsible (20 Studies) .........................................................................  241   
</p>
<p>  General Purpose ........................................................................................  241   
</p>
<p>  Primary Scientifi c Question ......................................................................  241   
</p>
<p>  Example.....................................................................................................  242   
</p>
<p>  Conclusion.................................................................................................  244   
</p>
<p>  Note ...........................................................................................................  244   
</p>
<p>    41      Performance Evaluation of Novel Diagnostic Tests 
</p>
<p>(650 and 588 Patients) .............................................................................  245   
</p>
<p>  General Purpose ........................................................................................  245   
</p>
<p>  Primary Scientifi c Question ......................................................................  245   
</p>
<p>  Example.....................................................................................................  245   
</p>
<p>  Binary Logistic Regression ..................................................................  248   
</p>
<p>  C-Statistics ...........................................................................................  249   
</p>
<p>  Conclusion.................................................................................................  251   
</p>
<p>  Note ...........................................................................................................  251   
</p>
<p>    42      Quantile-Quantile Plots, a Good Start for Looking 
</p>
<p>at Your Medical Data (50 Cholesterol Measurements 
</p>
<p>and 58 Patients) .......................................................................................  253   
</p>
<p>  General Purpose ........................................................................................  253   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  253   
</p>
<p>  Q-Q Plots for Assessing Departures from Normality ...............................  253   
</p>
<p>Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>xvii
</p>
<p>  Q-Q Plots as Diagnostics for Fitting Data to Normal 
</p>
<p>(and Other Theoretical) Distributions .......................................................  256   
</p>
<p>  Conclusion.................................................................................................  258   
</p>
<p>  Note ...........................................................................................................  259   
</p>
<p>    43      Rate Analysis of Medical Data Better than Risk Analysis 
</p>
<p>(52 Patients) .............................................................................................  261   
</p>
<p>  General Purpose ........................................................................................  261   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  261   
</p>
<p>  Example.....................................................................................................  261   
</p>
<p>  Conclusion.................................................................................................  264   
</p>
<p>  Note ...........................................................................................................  264   
</p>
<p>    44      Trend Tests Will Be Statistically Significant if Traditional 
</p>
<p>Tests Are Not (30 and 106 Patients) .......................................................  265   
</p>
<p>  General Purpose ........................................................................................  265   
</p>
<p>  Specifi c Scientifi c Questions .....................................................................  265   
</p>
<p>  Example 1..................................................................................................  265   
</p>
<p>  Example 2..................................................................................................  267   
</p>
<p>  Conclusion.................................................................................................  269   
</p>
<p>  Note ...........................................................................................................  269   
</p>
<p>    45      Doubly Multivariate Analysis of Variance for Multiple 
</p>
<p>Observations from Multiple Outcome Variables (16 Patients) ...........  271   
</p>
<p>  General Purpose ........................................................................................  271   
</p>
<p>  Primary Scientifi c Question ......................................................................  271   
</p>
<p>  Example.....................................................................................................  272   
</p>
<p>  Conclusion.................................................................................................  276   
</p>
<p>  Note ...........................................................................................................  276   
</p>
<p>    46      Probit Models for Estimating Effective Pharmacological 
</p>
<p>Treatment Dosages (14 Tests) .................................................................  279   
</p>
<p>  General Purpose ........................................................................................  279   
</p>
<p>  Primary Scientifi c Question ......................................................................  279   
</p>
<p>  Example.....................................................................................................  279   
</p>
<p>  Simple Probit Regression .....................................................................  279   
</p>
<p>  Multiple Probit Regression ...................................................................  282   
</p>
<p>  Conclusion.................................................................................................  286   
</p>
<p>  Note ...........................................................................................................  287   
</p>
<p>    47      Interval Censored Data Analysis for Assessing Mean 
</p>
<p>Time to Cancer Relapse (51 Patients) ...................................................  289   
</p>
<p>  General Purpose ........................................................................................  289   
</p>
<p>  Primary Scientifi c Question ......................................................................  289   
</p>
<p>  Example.....................................................................................................  290   
</p>
<p>  Conclusion.................................................................................................  292   
</p>
<p>  Note ...........................................................................................................  293   
</p>
<p>Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>xviii
</p>
<p>    48      Structural Equation Modeling (SEM) with SPSS Analysis 
</p>
<p>of Moment Structures (Amos) for Cause Effect 
</p>
<p>Relationships I (35 Patients) ...................................................................  295   
</p>
<p>  General Purpose ........................................................................................  295   
</p>
<p>  Primary Scientifi c Question ......................................................................  296   
</p>
<p>  Example.....................................................................................................  296   
</p>
<p>  Conclusion.................................................................................................  300   
</p>
<p>  Note ...........................................................................................................  300   
</p>
<p>    49      Structural Equation Modeling (SEM) with SPSS Analysis 
</p>
<p>of Moment Structures (Amos) for Cause Effect Relationships 
</p>
<p>in Pharmacodynamic Studies II (35 Patients) ......................................  301   
</p>
<p>  General Purpose ........................................................................................  301   
</p>
<p>  Primary Scientifi c Question ......................................................................  302   
</p>
<p>  Example.....................................................................................................  302   
</p>
<p>  Conclusion.................................................................................................  306   
</p>
<p>  Note ...........................................................................................................  306   
</p>
<p>    Part III Rules Models
</p>
<p>50      Neural Networks for Assessing Relationships That Are Typically 
</p>
<p>Nonlinear (90 Patients) ...........................................................................  309   
</p>
<p>  General Purpose ........................................................................................  309   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  309   
</p>
<p>  The Computer Teaches Itself to Make Predictions ...................................  310   
</p>
<p>  Conclusion.................................................................................................  311   
</p>
<p>  Note ...........................................................................................................  312   
</p>
<p>    51      Complex Samples Methodologies for Unbiased Sampling 
</p>
<p>(9,678 Persons) .........................................................................................  313   
</p>
<p>  General Purpose ........................................................................................  313   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  313   
</p>
<p>  The Computer Teaches Itself to Predict Current Health Scores 
</p>
<p>from Previous Health Scores ....................................................................  315   
</p>
<p>  The Computer Teaches Itself to Predict Individual Odds Ratios 
</p>
<p>of Current Health Scores Versus Previous Health Scores .........................  317   
</p>
<p>  Conclusion.................................................................................................  318   
</p>
<p>  Note ...........................................................................................................  319   
</p>
<p>    52      Correspondence Analysis for Identifying the Best 
</p>
<p>of Multiple Treatments in Multiple Groups (217 Patients) .................  321   
</p>
<p>  General Purpose ........................................................................................  321   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  321   
</p>
<p>  Correspondence Analysis ..........................................................................  322   
</p>
<p>  Conclusion.................................................................................................  325   
</p>
<p>  Note ...........................................................................................................  325   
</p>
<p>Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>xix
</p>
<p>    53      Decision Trees for Decision Analysis (1,004 and 953 Patients) ............  327   
</p>
<p>  General Purpose ........................................................................................  327   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  327   
</p>
<p>  Decision Trees with a Binary Outcome ....................................................  327   
</p>
<p>  Decision Trees with a Continuous Outcome .............................................  331   
</p>
<p>  Conclusion.................................................................................................  334   
</p>
<p>  Note ...........................................................................................................  334   
</p>
<p>    54      Multidimensional Scaling for Visualizing Experienced 
</p>
<p>Drug Efficacies (14 Pain-Killers and 42 Patients) ................................  335   
</p>
<p>  General Purpose ........................................................................................  335   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  335   
</p>
<p>  Proximity Scaling ......................................................................................  336   
</p>
<p>  Preference Scaling .....................................................................................  338   
</p>
<p>  Conclusion.................................................................................................  343   
</p>
<p>  Note ...........................................................................................................  344   
</p>
<p>    55      Stochastic Processes for Long Term Predictions 
</p>
<p>from Short Term Observations ..............................................................  345   
</p>
<p>  General Purpose ........................................................................................  345   
</p>
<p>  Specifi c Scientifi c Questions .....................................................................  345   
</p>
<p>  Example 1..................................................................................................  345   
</p>
<p>  Example 2..................................................................................................  347   
</p>
<p>  Example 3..................................................................................................  349   
</p>
<p>  Conclusion.................................................................................................  351   
</p>
<p>  Note ...........................................................................................................  351   
</p>
<p>    56      Optimal Binning for Finding High Risk Cut- offs 
</p>
<p>(1,445 Families) ........................................................................................  353   
</p>
<p>  General Purpose ........................................................................................  353   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  353   
</p>
<p>  Optimal Binning ........................................................................................  354   
</p>
<p>  Conclusion.................................................................................................  357   
</p>
<p>  Note ...........................................................................................................  357   
</p>
<p>    57      Conjoint Analysis for Determining the Most Appreciated 
</p>
<p>Properties of Medicines to Be Developed (15 Physicians) ...................  359   
</p>
<p>  General Purpose ........................................................................................  359   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  359   
</p>
<p>  Constructing an Analysis Plan ..................................................................  359   
</p>
<p>  Performing the Final Analysis ...................................................................  361   
</p>
<p>  Conclusion.................................................................................................  364   
</p>
<p>  Note ...........................................................................................................  364   
</p>
<p>Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>xx
</p>
<p>    58      Item Response Modeling for Analyzing Quality of Life 
</p>
<p>with Better Precision (1,000 Patients) ...................................................  365   
</p>
<p>  General Purpose ........................................................................................  365   
</p>
<p>  Primary Scientifi c Question ......................................................................  365   
</p>
<p>  Example.....................................................................................................  365   
</p>
<p>  Conclusion.................................................................................................  369   
</p>
<p>  Note ...........................................................................................................  369   
</p>
<p>    59      Survival Studies with Varying Risks of Dying 
</p>
<p>(50 and 60 Patients) .................................................................................  371   
</p>
<p>  General Purpose ........................................................................................  371   
</p>
<p>  Primary Scientifi c Question ......................................................................  371   
</p>
<p>  Examples ...................................................................................................  371   
</p>
<p>  Cox Regression with a Time-Dependent Predictor ..............................  371   
</p>
<p>  Cox Regression with a Segmented Time-Dependent Predictor ...........  373   
</p>
<p>  Conclusion.................................................................................................  374   
</p>
<p>  Note ...........................................................................................................  375   
</p>
<p>    60      Fuzzy Logic for Improved Precision of Dose- Response 
</p>
<p>Data (8 Induction Dosages) ....................................................................  377   
</p>
<p>  General Purpose ........................................................................................  377   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  377   
</p>
<p>  Example.....................................................................................................  378   
</p>
<p>  Conclusion.................................................................................................  381   
</p>
<p>  Note ...........................................................................................................  381   
</p>
<p>    61      Automatic Data Mining for the Best Treatment 
</p>
<p>of a Disease (90 Patients) ........................................................................  383   
</p>
<p>  General Purpose ........................................................................................  383   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  383   
</p>
<p>  Example.....................................................................................................  383   
</p>
<p>  Step 1 Open SPSS Modeler.......................................................................  385   
</p>
<p>  Step 2 The Distribution Node ....................................................................  385   
</p>
<p>  Step 3 The Data Audit Node .....................................................................  386   
</p>
<p>  Step 4 The Plot Node ................................................................................  387   
</p>
<p>  Step 5 The Web Node ................................................................................  388   
</p>
<p>  Step 6 The Type and c5.0 Nodes ...............................................................  389   
</p>
<p>  Step 7 The Output Node ............................................................................  390   
</p>
<p>  Conclusion.................................................................................................  390   
</p>
<p>  Note ...........................................................................................................  390   
</p>
<p>    62      Pareto Charts for Identifying the Main Factors 
</p>
<p>of Multifactorial Outcomes (2,000 Admissions to Hospital)................  391   
</p>
<p>  General Purpose ........................................................................................  391   
</p>
<p>  Primary Scientifi c Question ......................................................................  391   
</p>
<p>  Example.....................................................................................................  392   
</p>
<p>  Conclusion.................................................................................................  396   
</p>
<p>  Note ...........................................................................................................  396   
</p>
<p>Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>xxi
</p>
<p>    63      Radial Basis Neural Networks for Multidimensional 
</p>
<p>Gaussian Data (90 Persons) ....................................................................  397   
</p>
<p>  General Purpose ........................................................................................  397   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  397   
</p>
<p>  Example.....................................................................................................  397   
</p>
<p>  The Computer Teaches Itself to Make Predictions ...................................  398   
</p>
<p>  Conclusion.................................................................................................  400   
</p>
<p>  Note ...........................................................................................................  400   
</p>
<p>    64      Automatic Modeling of Drug Efficacy Prediction (250 Patients)........  401   
</p>
<p>  General Purpose ........................................................................................  401   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  401   
</p>
<p>  Example.....................................................................................................  401   
</p>
<p>  Step 1 Open SPSS Modeler (14.2) ............................................................  402   
</p>
<p>  Step 2 The Statistics File Node .................................................................  403   
</p>
<p>  Step 3 The Type Node ...............................................................................  403   
</p>
<p>  Step 4 The Auto Numeric Node ................................................................  404   
</p>
<p>  Step 5 The Expert Node ............................................................................  405   
</p>
<p>  Step 6 The Settings Tab .............................................................................  407   
</p>
<p>  Step 7 The Analysis Node .........................................................................  407   
</p>
<p>  Conclusion.................................................................................................  408   
</p>
<p>  Note ...........................................................................................................  408   
</p>
<p>    65      Automatic Modeling for Clinical Event Prediction (200 Patients) .....  409   
</p>
<p>  General Purpose ........................................................................................  409   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  409   
</p>
<p>  Example.....................................................................................................  409   
</p>
<p>  Step 1 Open SPSS Modeler (14.2) ............................................................  410   
</p>
<p>  Step 2 The Statistics File Node .................................................................  411   
</p>
<p>  Step 3 The Type Node ...............................................................................  411   
</p>
<p>  Step 4 The Auto Classifi er Node ...............................................................  412   
</p>
<p>  Step 5 The Expert Tab ...............................................................................  413   
</p>
<p>  Step 6 The Settings Tab .............................................................................  414   
</p>
<p>  Step 7 The Analysis Node .........................................................................  415   
</p>
<p>  Conclusion.................................................................................................  416   
</p>
<p>  Note ...........................................................................................................  416    
</p>
<p>   66   Automatic Newton Modeling in Clinical Pharmacology 
</p>
<p>(15 Alfentanil Dosages, 15 Quinidine 
</p>
<p>Time-Concentration Relationships) ......................................................  417   
</p>
<p>  General Purpose ........................................................................................  417   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  418   
</p>
<p>  Examples ...................................................................................................  418   
</p>
<p>  Dose-Effectiveness Study .....................................................................  418   
</p>
<p>  Time-Concentration Study ...................................................................  420   
</p>
<p>  Conclusion.................................................................................................  422   
</p>
<p>  Note ...........................................................................................................  422    
</p>
<p>Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>xxii
</p>
<p>   67      Spectral Plots for High Sensitivity Assessment of Periodicity 
</p>
<p>(6 Years&rsquo; Monthly C Reactive Protein Levels) ......................................  423   
</p>
<p>  General Purpose ........................................................................................  423   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  423   
</p>
<p>  Example.....................................................................................................  423   
</p>
<p>  Conclusion.................................................................................................  427   
</p>
<p>  Note ...........................................................................................................  428    
</p>
<p>   68      Runs Test for Identifying Best Regression Models (21 Estimates 
</p>
<p>of Quantity and Quality of Patient Care)..............................................  429   
</p>
<p>  General Purpose ........................................................................................  429   
</p>
<p>  Primary Scientifi c Question ......................................................................  429   
</p>
<p>  Example.....................................................................................................  429   
</p>
<p>  Conclusion.................................................................................................  433   
</p>
<p>  Note ...........................................................................................................  433   
</p>
<p>    69      Evolutionary Operations for Process Improvement 
</p>
<p>(8 Operation Room Air Condition Settings) .........................................  435   
</p>
<p>  General Purpose ........................................................................................  435   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  435   
</p>
<p>  Example.....................................................................................................  436   
</p>
<p>  Conclusion.................................................................................................  437   
</p>
<p>  Note ...........................................................................................................  438   
</p>
<p>    70      Bayesian Networks for Cause Effect Modeling (600 Patients) ............  439   
</p>
<p>  General Purpose ........................................................................................  439   
</p>
<p>  Primary Scientifi c Question ......................................................................  439   
</p>
<p>  Example.....................................................................................................  440   
</p>
<p>  Binary Logistic Regression in SPSS .........................................................  440   
</p>
<p>  Konstanz Information Miner (Knime) ......................................................  441   
</p>
<p>  Knime Workfl ow .......................................................................................  442   
</p>
<p>  Conclusion.................................................................................................  443   
</p>
<p>  Note ...........................................................................................................  444   
</p>
<p>    71      Support Vector Machines for Imperfect Nonlinear Data 
</p>
<p>(200 Patients with Sepsis) .......................................................................  445   
</p>
<p>  General Purpose ........................................................................................  445   
</p>
<p>  Primary Scientifi c Question ......................................................................  445   
</p>
<p>  Example.....................................................................................................  446   
</p>
<p>  Knime Data Miner.....................................................................................  446   
</p>
<p>  Knime Workfl ow .......................................................................................  447   
</p>
<p>  File Reader Node.......................................................................................  447   
</p>
<p>  The Nodes X-Partitioner, svm Learner, svm Predictor, X-Aggregator .....  448   
</p>
<p>  Error Rates ................................................................................................  448   
</p>
<p>  Prediction Table .........................................................................................  448   
</p>
<p>  Conclusion.................................................................................................  449   
</p>
<p>  Note ...........................................................................................................  449   
</p>
<p>Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>xxiii
</p>
<p>    72      Multiple Response Sets for Visualizing Clinical Data Trends 
</p>
<p>(811 Patient Visits) ...................................................................................  451   
</p>
<p>  General Purpose ........................................................................................  451   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  451   
</p>
<p>  Example.....................................................................................................  451   
</p>
<p>  Conclusion.................................................................................................  457   
</p>
<p>  Note ...........................................................................................................  457   
</p>
<p>    73      Protein and DNA Sequence Mining .......................................................  459   
</p>
<p>  General Purpose ........................................................................................  459   
</p>
<p>  Specifi c Scientifi c Question ......................................................................  459   
</p>
<p>  Data Base Systems on the Internet ............................................................  460   
</p>
<p>  Example 1..................................................................................................  461   
</p>
<p>  Example 2..................................................................................................  462   
</p>
<p>  Example 3..................................................................................................  462   
</p>
<p>  Example 4..................................................................................................  463   
</p>
<p>  Conclusion.................................................................................................  464   
</p>
<p>  Note ...........................................................................................................  464   
</p>
<p>    74      Iteration Methods for Crossvalidations (150 Patients 
</p>
<p>with Pneumonia)......................................................................................  465   
</p>
<p>  General Purpose ........................................................................................  465   
</p>
<p>  Primary Scientifi c Question ......................................................................  465   
</p>
<p>  Example.....................................................................................................  465   
</p>
<p>  Downloading the Knime Data Miner ........................................................  466   
</p>
<p>  Knime Workfl ow .......................................................................................  467   
</p>
<p>  Crossvalidation ..........................................................................................  467   
</p>
<p>  Conclusion.................................................................................................  469   
</p>
<p>  Note ...........................................................................................................  469   
</p>
<p>    75      Testing Parallel-Groups with Different Sample Sizes 
</p>
<p>and Variances (5 Parallel-Group Studies) .............................................  471   
</p>
<p>  General Purpose ........................................................................................  471   
</p>
<p>  Primary Scientifi c Question ......................................................................  471   
</p>
<p>  Examples ...................................................................................................  472   
</p>
<p>  Conclusion.................................................................................................  473   
</p>
<p>  Note ...........................................................................................................  473   
</p>
<p>    76      Association Rules Between Exposure and Outcome 
</p>
<p>(50 and 60 Patients) .................................................................................  475   
</p>
<p>  General Purpose ........................................................................................  475   
</p>
<p>  Primary Scientifi c Question ......................................................................  475   
</p>
<p>  Example.....................................................................................................  475   
</p>
<p>  Example One ........................................................................................  477   
</p>
<p>  Example Two ........................................................................................  478   
</p>
<p>  Conclusion.................................................................................................  479   
</p>
<p>  Note ...........................................................................................................  479    
</p>
<p>Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>xxiv
</p>
<p>   77      Confidence Intervals for Proportions and Differences 
</p>
<p>in Proportions (100 and 75 Patients) .....................................................  481   
</p>
<p>  General Purpose ........................................................................................  481   
</p>
<p>  Primary Scientifi c Question ......................................................................  481   
</p>
<p>  Example.....................................................................................................  482   
</p>
<p>  Confi dence Intervals of Proportions .....................................................  482   
</p>
<p>  Confi dence Intervals of Differences in Proportions .............................  483   
</p>
<p>  Conclusion.................................................................................................  484   
</p>
<p>  Note ...........................................................................................................  484    
</p>
<p>   78      Ratio Statistics for Efficacy Analysis of New Drugs (50 Patients) ......  485   
</p>
<p>  General Purpose ........................................................................................  485   
</p>
<p>  Primary Scientifi c Question ......................................................................  485   
</p>
<p>  Example.....................................................................................................  485   
</p>
<p>  Conclusion.................................................................................................  489   
</p>
<p>  Note ...........................................................................................................  489    
</p>
<p>   79      Fifth Order Polynomes of Circadian Rhythms 
</p>
<p>(1 Patient with Hypertension) ................................................................  491   
</p>
<p>  General Purpose ........................................................................................  491   
</p>
<p>  Primary Scientifi c Question ......................................................................  492   
</p>
<p>  Example.....................................................................................................  492   
</p>
<p>  Conclusion.................................................................................................  495   
</p>
<p>  Note ...........................................................................................................  496   
</p>
<p>    80      Gamma Distribution for Estimating the Predictors 
</p>
<p>of Medical Outcome Scores (110 Patients) ............................................  497   
</p>
<p>  General Purpose ........................................................................................  497   
</p>
<p>  Primary Scientifi c Question ......................................................................  498   
</p>
<p>  Example.....................................................................................................  498   
</p>
<p>  Conclusion.................................................................................................  503   
</p>
<p>  Note ...........................................................................................................  503
</p>
<p>Index  ................................................................................................................ 505     
</p>
<p>Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>       
</p>
<p>   Part I 
</p>
<p>   Cluster and Classifi cation Models </p>
<p/>
</div>
<div class="page"><p/>
<p>3&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_1
</p>
<p>    Chapter 1   
</p>
<p> Hierarchical Clustering and K-Means 
Clustering to Identify Subgroups in Surveys 
(50 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> Clusters are subgroups in a survey estimated by the distances between the values 
</p>
<p>needed to connect the patients, otherwise called cases. It is an important methodol-
</p>
<p>ogy in explorative data mining.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> In a survey of patients with mental depression of different ages and depression 
</p>
<p>scores, how do different clustering methods perform in identifying so far unob-
</p>
<p>served subgroups   . 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 1&rdquo; as Chap. 1, 
</p>
<p>2013. </p>
<p/>
</div>
<div class="page"><p/>
<p>4
</p>
<p> 1  2  3 
</p>
<p> 20,00  8,00  1 
</p>
<p> 21,00  7,00  2 
</p>
<p> 23,00  9,00  3 
</p>
<p> 24,00  10,00  4 
</p>
<p> 25,00  8,00  5 
</p>
<p> 26,00  9,00  6 
</p>
<p> 27,00  7,00  7 
</p>
<p> 28,00  8,00  8 
</p>
<p> 24,00  9,00  9 
</p>
<p> 32,00  9,00  10 
</p>
<p> 30,00  1,00  11 
</p>
<p> 40,00  2,00  12 
</p>
<p> 50,00  3,00  13 
</p>
<p> 60,00  1,00  14 
</p>
<p> 70,00  2,00  15 
</p>
<p> 76,00  3,00  16 
</p>
<p> 65,00  2,00  17 
</p>
<p> 54,00  3,00  18 
</p>
<p>  Var 1 age 
</p>
<p> Var 2 depression score (0 = very mild, 10 = severest) 
</p>
<p> Var 3 patient number (called cases here) 
</p>
<p>    Only the fi rst 18 patients are given, the entire data fi le is entitled &ldquo;hierk- 
</p>
<p>meansdensity&rdquo; and is in extras.springer.com.  
</p>
<p>    Hierarchical Cluster Analysis 
</p>
<p> SPSS 19.0 will be used for data analysis. Start by opening the data fi le.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Classify&hellip;.Hierarchical Cluster Analysis&hellip;.enter variables&hellip;.Label 
</p>
<p>Case by: case variable with the values 1-50&hellip;.Plots: mark Dendrogram&hellip;.Method   
</p>
<p>  &hellip;.Cluster Method: Between-group linkage&hellip;.Measure: Squared Euclidean 
</p>
<p>Distance&hellip;.Save: click Single solution&hellip;.Number of clusters: enter 3&hellip;.Continue 
</p>
<p>&hellip;.OK.    
</p>
<p>1 Hierarchical Clustering and K-Means Clustering to Identify Subgroups in Surveys&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>5
</p>
<p>  
</p>
<p>Num
</p>
<p>0
</p>
<p>28
</p>
<p>46
</p>
<p>45
</p>
<p>37
</p>
<p>43
</p>
<p>47
</p>
<p>48
</p>
<p>15
</p>
<p>17
</p>
<p>14
</p>
<p>33
</p>
<p>19
</p>
<p>41
</p>
<p>18
</p>
<p>39
</p>
<p>44
</p>
<p>16
</p>
<p>50
</p>
<p>49
</p>
<p>36
</p>
<p>42
</p>
<p>30
</p>
<p>34
</p>
<p>29
</p>
<p>38
</p>
<p>12
</p>
<p>31
</p>
<p>26
</p>
<p>27
</p>
<p>40
</p>
<p>13
</p>
<p>32
</p>
<p>20
</p>
<p>35
</p>
<p>1
</p>
<p>2
</p>
<p>4
</p>
<p>9
</p>
<p>3
</p>
<p>5
</p>
<p>6
</p>
<p>22
</p>
<p>23
</p>
<p>7
</p>
<p>8
</p>
<p>24
</p>
<p>11
</p>
<p>21
</p>
<p>10
</p>
<p>25
</p>
<p>5 10
</p>
<p>Rescaled Distance Cluster Combine
</p>
<p>15 20 25
</p>
<p>  
</p>
<p>    In the output a dendrogram of the results is given. The actual distances between 
</p>
<p>the cases are rescaled to fall into a range of 0&ndash;25 units (0 = minimal distance, 
</p>
<p>25 = maximal distance). The cases no. 1&ndash;11, 21&ndash;25 are clustered together in cluster 
</p>
<p>1, the cases 12, 13, 20, 26, 27, 31, 32, 35, 40 in cluster 2, both at a rescaled distance 
</p>
<p>from 0 at approximately 3 units, the remainder of the cases is clustered at approxi-
</p>
<p>mately 6 units. And so, as requested, three clusters have been identifi ed with cases 
</p>
<p>more similar to one another than to the other clusters. When minimizing the output, 
</p>
<p>the data fi le comes up and it now shows the cluster membership of each case. We 
</p>
<p>will use SPSS again to draw a Dotter graph of the data.
</p>
<p>Hierarchical Cluster Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>6
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Graphs&hellip;.Legacy Dialogs: click Simple Scatter&hellip;.Defi ne&hellip;.Y-axis:   
</p>
<p>  enter Depression Score&hellip;.X-axis: enter Age&hellip;.OK.    
</p>
<p> The graph (with age on the x-axis and severity score on the y-axis) produced by 
</p>
<p>SPSS shows the cases. Using Microsoft&rsquo;s drawing commands we can encircle the 
</p>
<p>clusters as identifi ed. All of them are oval and even, approximately, round, because 
</p>
<p>variables have similar scales, but they are different in size. 
</p>
<p>  
</p>
<p>10,00
</p>
<p>8,00
</p>
<p>6,00
</p>
<p>4,00
</p>
<p>2,00
</p>
<p>0,00
</p>
<p>20,00 30,00 40,00 50,00 60,00 70,00 80,00   
</p>
<p>        K-Means Cluster Analysis 
</p>
<p>   Command: 
</p>
<p>  Analyze&hellip;.Classify&hellip;.K-means Cluster Analysis&hellip;.Variables: enter Age and 
</p>
<p>Depression score&hellip;.Label Cases by: patient number as a string variable&hellip;.Number 
</p>
<p>of clusters: 3 (in our example chosen for comparison with the above method)&hellip;.
</p>
<p>click Method: mark Iterate&hellip;.click Iterate: Maximal Iterations: mark 10&hellip;.
</p>
<p>Convergence criterion: mark 0&hellip;.click Continue&hellip;.click Save: mark Cluster 
</p>
<p>Membership&hellip;.click Continue&hellip;.click Options: mark Initiate cluster centers&hellip;.
</p>
<p>mark ANOVA table&hellip;.mark Cluster information for each case&hellip;.click Continue&hellip;.
</p>
<p>OK.    
</p>
<p> The output shows that the three clusters identifi ed by the k-means cluster model 
</p>
<p>were signifi cantly different from one another both by testing the y-axis (depression 
</p>
<p>score) and the x-axis variable (age). When minimizing the output sheets, the data 
</p>
<p>fi le comes up and shows the cluster membership of the three clusters.
</p>
<p>1 Hierarchical Clustering and K-Means Clustering to Identify Subgroups in Surveys&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>7
</p>
<p> ANOVA 
</p>
<p> Cluster  Error 
</p>
<p> Mean square  df  Mean square  df  F  Sig. 
</p>
<p> Age  8712,723  2  31,082  47  280,310  ,000 
</p>
<p> Depression score  39,102  2  4,593  47  8,513  ,001 
</p>
<p> We will use SPSS again to draw a Dotter graph of the data.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Graphs&hellip;.Legacy Dialogs: click Simple Scatter&hellip;.Defi ne&hellip;.Y-axis: 
</p>
<p>enter Depression Score&hellip;.X-axis: enter Age&hellip;.OK.    
</p>
<p> The graph (with age on the x-axis and severity score on the y-axis) produced by 
</p>
<p>SPSS shows the cases. Using Microsoft&rsquo;s drawing commands we can encircle the 
</p>
<p>clusters as identifi ed. All of them are oval and even approximately round because 
</p>
<p>variables have similar scales, and they are approximately equal in size. 
</p>
<p>  
</p>
<p>10,00
</p>
<p>8,00
</p>
<p>D
e
</p>
<p>p
re
</p>
<p>s
s
</p>
<p>io
n
</p>
<p> S
c
</p>
<p>o
re
</p>
<p>6,00
</p>
<p>4,00
</p>
<p>2,00
</p>
<p>0,00
</p>
<p>20,00 30,00 40,00 50,00 60,00 70,00 80,00   
</p>
<p>        Conclusion 
</p>
<p> Clusters are estimated by the distances between the values needed to connect the 
</p>
<p>cases. It is an important methodology in explorative data mining. Hierarchical clus-
</p>
<p>tering is adequate if subgroups are expected to be different in size, k-means cluster-
</p>
<p>ing if approximately similar in size. Density-based clustering is more appropriate if 
</p>
<p>small outlier groups between otherwise homogenous populations are expected. The 
</p>
<p>latter method is in Chap.   2    .  
</p>
<p>Conclusion</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_2">http://dx.doi.org/10.1007/978-3-319-15195-3_2</a></div>
</div>
<div class="page"><p/>
<p>8
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of the two methods is 
</p>
<p>given in Machine learning in medicine part two, Chap. 8 Two-dimensional 
</p>
<p>Clustering, pp 65&ndash;75, Springer Heidelberg Germany 2013. Density-based cluster-
</p>
<p>ing will be reviewed in the next chapter.    
</p>
<p>1 Hierarchical Clustering and K-Means Clustering to Identify Subgroups in Surveys&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>9&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_2
</p>
<p>    Chapter 2   
</p>
<p> Density-Based Clustering to Identify Outlier 
Groups in Otherwise Homogeneous Data 
(50 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> Clusters are subgroups in a survey estimated by the distances between the values 
</p>
<p>needed to connect the patients, otherwise called cases. It is an important methodol-
</p>
<p>ogy in explorative data mining. Density-based clustering is used.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> In a survey of patients with mental depression of different ages and depression 
</p>
<p>scores, how does density-based clustering perform in identifying so far unobserved 
</p>
<p>subgroups. 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 1&rdquo; as Chap. 2, 
</p>
<p>2013. </p>
<p/>
</div>
<div class="page"><p/>
<p>10
</p>
<p> 1  2  3 
</p>
<p> 20,00  8,00  1 
</p>
<p> 21,00  7,00  2 
</p>
<p> 23,00  9,00  3 
</p>
<p> 24,00  10,00  4 
</p>
<p> 25,00  8,00  5 
</p>
<p> 26,00  9,00  6 
</p>
<p> 27,00  7,00  7 
</p>
<p> 28,00  8,00  8 
</p>
<p> 24,00  9,00  9 
</p>
<p> 32,00  9,00  10 
</p>
<p> 30,00  1,00  11 
</p>
<p> 40,00  2,00  12 
</p>
<p> 50,00  3,00  13 
</p>
<p> 60,00  1,00  14 
</p>
<p> 70,00  2,00  15 
</p>
<p> 76,00  3,00  16 
</p>
<p> 65,00  2,00  17 
</p>
<p> 54,00  3,00  18 
</p>
<p>  Var 1 age 
</p>
<p> Var 2 depression score (0 = very mild, 10 = severest) 
</p>
<p> Var 3 patient number (called cases here) 
</p>
<p>    Only the fi rst 18 patients are given, the entire data fi le is entitled &ldquo;hierk- 
</p>
<p>meansdensity&rdquo; and is in extras.springer.com.  
</p>
<p>    Density-Based Cluster Analysis 
</p>
<p> The DBSCAN method was used (density based spatial clustering of application 
</p>
<p>with noise). As this method is not available in SPSS, an interactive JAVA Applet 
</p>
<p>freely available at the Internet was used [Data Clustering Applets.   http://webdocs.
</p>
<p>cs.ualberts.ca/~yaling/Cluster/applet    ]. The DBSCAN connects points that satisfy a 
</p>
<p>density criterion given by a minimum number of patients within a defi ned radius 
</p>
<p>(radius = Eps; minimum number = Min pts).
</p>
<p>  Command: 
</p>
<p>  User Defi ne&hellip;.Choose data set: remove values given&hellip;.enter you own x and y val-
</p>
<p>ues&hellip;.Choose algorithm: select DBSCAN&hellip;.Eps: mark 25&hellip;.Min pts: mark 3&hellip;.
</p>
<p>Start&hellip;.Show.    
</p>
<p> Three cluster memberships are again shown. We will use SPSS 19.0 again to 
</p>
<p>draw a Dotter graph of the data.
</p>
<p>2 Density-Based Clustering to Identify Outlier Groups in Otherwise Homogeneous&hellip;</p>
<p/>
<div class="annotation"><a href="http://webdocs.cs.ualberts.ca/~yaling/Cluster/applet">http://webdocs.cs.ualberts.ca/~yaling/Cluster/applet</a></div>
<div class="annotation"><a href="http://webdocs.cs.ualberts.ca/~yaling/Cluster/applet">http://webdocs.cs.ualberts.ca/~yaling/Cluster/applet</a></div>
</div>
<div class="page"><p/>
<p>11
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Graphs&hellip;.Legacy Dialogs: click Simple Scatter&hellip;.Defi ne&hellip;.Y-axis: 
</p>
<p>enter Depression Score&hellip;.X-axis: enter Age&hellip;.OK.    
</p>
<p> The graph (with age on the x-axis and severity score on the y-axis) shows the 
</p>
<p>cases. Using Microsoft&rsquo;s drawing commands we can encircle the clusters as identi-
</p>
<p>fi ed. Two very small ones, one large one. All of the clusters identifi ed are non- 
</p>
<p>circular and, are, obviously, based on differences in patient-density. 
</p>
<p>  
</p>
<p>10,00
</p>
<p>8,00
</p>
<p>6,00
</p>
<p>4,00
</p>
<p>2,00
</p>
<p>0,00
</p>
<p>20,00 30,00 40,00 50,00
</p>
<p>Age
</p>
<p>60,00 70,00 80,00
</p>
<p>  
</p>
<p>        Conclusion 
</p>
<p> Clusters are estimated by the distances between the values needed to connect the 
</p>
<p>cases. It is an important methodology in explorative data mining. Density-based 
</p>
<p>clustering is suitable if small outlier groups between otherwise homogeneous popu-
</p>
<p>lations are expected. Hierarchical and k-means clustering are more appropriate if 
</p>
<p>subgroups have Gaussian-like patterns (Chap.   1    ).  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of the three methods is 
</p>
<p>given in Machine learning in medicine part two, Chap. 8 Two-dimensional cluster-
</p>
<p>ing, pp 65&ndash;75, Springer Heidelberg Germany 2013. Hierarchical and k-means clus-
</p>
<p>tering are reviewed in the previous chapter.    
</p>
<p>Note</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_1">http://dx.doi.org/10.1007/978-3-319-15195-3_1</a></div>
</div>
<div class="page"><p/>
<p>13&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_3
</p>
<p>    Chapter 3   
</p>
<p> Two Step Clustering to Identify Subgroups 
and Predict Subgroup Memberships 
in Individual Future Patients (120 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> To assess whether two step clustering of survey data can be trained to identify sub-
</p>
<p>groups and subgroup membership.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> In patients with mental depression, can the item scores of depression severity be 
</p>
<p>used to classify subgroups and to predict subgroup membership of future patients. 
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5  Var 6  Var 7  Var 8  Var 9 
</p>
<p> 9,00  9,00  9,00  2,00  2,00  2,00  2,00  2,00  2,00 
</p>
<p> 8,00  8,00  6,00  3,00  3,00  3,00  3,00  3,00  3,00 
</p>
<p> 7,00  7,00  7,00  4,00  4,00  4,00  4,00  4,00  4,00 
</p>
<p> 4,00  9,00  9,00  2,00  2,00  6,00  2,00  2,00  2,00 
</p>
<p> 8,00  8,00  8,00  3,00  3,00  3,00  3,00  3,00  3,00 
</p>
<p> 7,00  7,00  7,00  4,00  4,00  4,00  4,00  4,00  4,00 
</p>
<p> 9,00  5,00  9,00  9,00  2,00  2,00  2,00  2,00  2,00 
</p>
<p> 8,00  8,00  8,00  3,00  3,00  3,00  3,00  3,00  3,00 
</p>
<p> 7,00  7,00  7,00  4,00  6,00  4,00  4,00  4,00  4,00 
</p>
<p> 9,00  9,00  9,00  2,00  2,00  2,00  2,00  2,00  2,00 
</p>
<p> 4,00  4,00  4,00  9,00  9,00  9,00  3,00  3,00  3,00 
</p>
<p> 3,00  3,00  3,00  8,00  8,00  8,00  4,00  4,00  4,00 
</p>
<p>  Var 1&ndash;9 = depression score 1&ndash;9 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 1&rdquo; as Chap. 3, 
</p>
<p>2013. </p>
<p/>
</div>
<div class="page"><p/>
<p>14
</p>
<p>    Only the fi rst 12 patients are given, the entire data fi le is entitled &ldquo;twostepcluster-
</p>
<p>ing&rdquo; and is in extras.springer.com.  
</p>
<p>    The Computer Teaches Itself to Make Predictions 
</p>
<p> SPSS 19.0 is used for data analysis. It will use XML (eXtended Markup Language) 
</p>
<p>fi les to store data. Now start by opening the data fi le.
</p>
<p>  Command: 
</p>
<p>  Click Transform&hellip;.click Random Number Generators&hellip;.click Set Starting Point 
</p>
<p>&hellip;.click Fixed Value (2000000)&hellip;.click OK&hellip;.click Analyze&hellip;.Classify&hellip;.TwoStep 
</p>
<p>Cluster&hellip;.Continuous Variables: enter depression 1-9&hellip;.click Output: in Working 
</p>
<p>Data File click Create cluster membership&hellip;.in XML Files click Export fi nal 
</p>
<p>model&hellip;.click Browse&hellip;.File name: enter "export2step"&hellip;.click Save&hellip;.click 
</p>
<p>Continue&hellip;.click OK.    
</p>
<p> Returning to the data fi le we will observe that three subgroups have been identi-
</p>
<p>fi ed and for each patient the subgroup membership is given as a novel variable, and 
</p>
<p>the name of this novel variable is TSC (two step cluster). The saved XML fi le will 
</p>
<p>now be used to compute the predicted subgroup membership in fi ve future patients. 
</p>
<p>For convenience the XML fi le is given in extras.springer.com. 
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5  Var 6  Var 7  Var 8  Var 9 
</p>
<p> 4,00  5,00  3,00  4,00  6,00  9,00  8,00  7,00  6,00 
</p>
<p> 2,00  2,00  2,00  2,00  2,00  2,00  2,00  2,00  2,00 
</p>
<p> 5,00  4,00  6,00  7,00  6,00  5,00  3,00  4,00  5,00 
</p>
<p> 9,00  8,00  7,00  6,00  5,00  4,00  3,00  2,00  2,00 
</p>
<p> 7,00  7,00  7,00  3,00  3,00  3,00  9,00  9,00  9,00 
</p>
<p>  Var 1&ndash;9 = Depression score 1&ndash;9 
</p>
<p>    Enter the above data in a new SPSS data fi le.
</p>
<p>  Command: 
</p>
<p>  Utilities&hellip;.click Scoring Wizard&hellip;.click Browse&hellip;.click Select&hellip;.Folder: enter the 
</p>
<p>export2step.xml fi le&hellip;.click Select&hellip;.in Scoring Wizard click Next&hellip;.click Use 
</p>
<p>value substitution&hellip;.click Next&hellip;.click Finish.    
</p>
<p> The above data fi le now gives subgroup memberships of the fi ve patients as com-
</p>
<p>puted by the two step cluster model with the help of the XML fi le. 
</p>
<p>3 Two Step Clustering to Identify Subgroups and Predict Subgroup Memberships&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>15
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5  Var 6  Var 7  Var 8  Var 9  Var 10 
</p>
<p> 4,00  5,00  3,00  4,00  6,00  9,00  8,00  7,00  6,00  2,00 
</p>
<p> 2,00  2,00  2,00  2,00  2,00  2,00  2,00  2,00  2,00  2,00 
</p>
<p> 5,00  4,00  6,00  7,00  6,00  5,00  3,00  4,00  5,00  3,00 
</p>
<p> 9,00  8,00  7,00  6,00  5,00  4,00  3,00  2,00  2,00  1,00 
</p>
<p> 7,00  7,00  7,00  3,00  3,00  3,00  9,00  9,00  9,00  2,00 
</p>
<p>  Var 1&ndash;9 Depression score 1&ndash;9 
</p>
<p> Var 10 predicted value 
</p>
<p>        Conclusion 
</p>
<p> Two step clustering can be readily trained to identify subgroups in patients with 
</p>
<p>mental depression, and, with the help of an XML fi le, it can, subsequently, be used 
</p>
<p>to identify subgroup memberships in individual future patients.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of two step and other 
</p>
<p>methods of clustering is available in Machine learning in medicine part two, Chaps. 
</p>
<p>8 and 9, entitled &ldquo;Two-dimensional clustering&rdquo; and &ldquo;Multidimensional clustering&rdquo;, 
</p>
<p>pp 65&ndash;75 and 77&ndash;91, Springer Heidelberg Germany 2013.    
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>17&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_4
</p>
<p>    Chapter 4   
</p>
<p> Nearest Neighbors for Classifying New 
Medicines (2 New and 25 Old Opioids) 
</p>
<p>                      General Purpose 
</p>
<p> Nearest neighbor methodology has a long history, and has, initially, been used for 
</p>
<p>data imputation in demographic data fi les. This chapter is to assess whether it can 
</p>
<p>also been used for classifying new medicines.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> For most diseases a whole class of drugs rather than a single compound is available. 
</p>
<p>Nearest neighbor methods can be used for identifying the place of a new drug within 
</p>
<p>its class.  
</p>
<p>    Example 
</p>
<p> Two newly developed opioid compounds are assessed for their similarities with the 
</p>
<p>standard opioids in order to determine their potential places in therapeutic regimens. 
</p>
<p>Underneath are the characteristics of 25 standard opioids and two newly developed 
</p>
<p>opioid compounds. 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 2&rdquo; as Chap. 1, 
</p>
<p>2014. </p>
<p/>
</div>
<div class="page"><p/>
<p>18
</p>
<p> Drugname 
</p>
<p> analgesia 
</p>
<p>score 
</p>
<p> antitussive 
</p>
<p>score 
</p>
<p> constipation 
</p>
<p>score 
</p>
<p> respiratory 
</p>
<p>score 
</p>
<p> abuse 
</p>
<p>score 
</p>
<p> eliminate 
</p>
<p>time 
</p>
<p> duration 
</p>
<p>time 
</p>
<p> buprenorphine  7,00  4,00  5,00  7,00  4,00  5,00  9,00 
</p>
<p> butorphanol  7,00  3,00  4,00  7,00  4,00  2,70  4,00 
</p>
<p> codeine  5,00  6,00  6,00  5,00  4,00  2,90  7,00 
</p>
<p> heroine  8,00  6,00  8,00  8,00  10,00  9,00  15,00 
</p>
<p> hydromorphone  8,00  6,00  6,00  8,00  8,00  2,60  5,00 
</p>
<p> levorphanol  8,00  6,00  6,00  8,00  8,00  11,00  20,00 
</p>
<p> mepriridine  7,00  2,00  4,00  8,00  6,00  3,20  14,00 
</p>
<p> methadone  9,00  6,00  6,00  8,00  6,00  25,00  5,00 
</p>
<p> morphine  8,00  6,00  8,00  8,00  8,00  3,10  5,00 
</p>
<p> nalbuphine  7,00  2,00  4,00  7,00  4,00  5,10  4,50 
</p>
<p> oxycodone  6,00  6,00  6,00  6,00  8,00  5,00  4,00 
</p>
<p> oxymorphine  8,00  5,00  6,00  8,00  8,00  5,20  3,50 
</p>
<p> pentazocine  7,00  2,00  4,00  7,00  5,00  2,90  3,00 
</p>
<p> propoxyphene  5,00  2,00  4,00  5,00  5,00  3,30  2,00 
</p>
<p> nalorphine  2,00  3,00  6,00  8,00  1,00  1,40  3,20 
</p>
<p> levallorphan  3,00  2,00  5,00  4,00  1,00  11,00  5,00 
</p>
<p> cyclazocine  2,00  3,00  6,00  3,00  2,00  1,60  2,80 
</p>
<p> naloxone  1,00  2,00  5,00  8,00  1,00  1,20  3,00 
</p>
<p> naltrexon  1,00  3,00  5,00  8,00  ,00  9,70  14,00 
</p>
<p> alfentanil  7,00  6,00  7,00  4,00  6,00  1,60  ,50 
</p>
<p> alphaprodine  6,00  5,00  6,00  3,00  5,00  2,20  2,00 
</p>
<p> fentanyl  6,00  5,00  7,00  5,00  4,00  3,70  ,50 
</p>
<p> meptazinol  4,00  3,00  5,00  5,00  3,00  1,60  2,00 
</p>
<p> norpropoxyphene  8,00  6,00  8,00  5,00  7,00  6,00  4,00 
</p>
<p> sufentanil  7,00  6,00  8,00  6,00  8,00  2,60  5,00 
</p>
<p> newdrug1  5,00  5,00  4,00  3,00  6,00  5,00  12,00 
</p>
<p> newdrug2  8,00  6,00  3,00  4,00  5,00  7,00  16,00 
</p>
<p>  Var = variable 
</p>
<p> Var 1 analgesia score (0&ndash;10) 
</p>
<p> Var 2 antitussive score (0&ndash;10) 
</p>
<p> Var 3 constipation score (0&ndash;10) 
</p>
<p> Var 4 respiratory depression score (1&ndash;10) 
</p>
<p> Var 5 abuse liability score (1&ndash;10) 
</p>
<p> Var 6 elimination time (t 1/2  in hours) 
</p>
<p> Var 7 duration time analgesia (hours) 
</p>
<p>    The data fi le is entitled &ldquo;nearestneighbor&rdquo; and is in extras.springer.com. 
</p>
<p> SPSS statistical software is used for data analysis. Start by opening the data fi le. 
</p>
<p>The drug names included, eight variables are in the fi le. A ninth variable entitled 
</p>
<p>&ldquo;partition&rdquo; must be added with the value 1 for the opioids 1&ndash;25 and 0 for the two 
</p>
<p>new compounds (cases 26 and 27).
</p>
<p>4 Nearest Neighbors for Classifying New Medicines (2 New and 25 Old Opioids)</p>
<p/>
</div>
<div class="page"><p/>
<p>19
</p>
<p>  Then command: 
</p>
<p>  Analyze&hellip;.Classify&hellip;.Nearest Neighbor Analysis&hellip;.enter the variable "drugsname" 
</p>
<p>in Target&hellip;.enter the variables "analgesia" to "duration of analgesia" in Features&hellip;.
</p>
<p>click Partitions&hellip;.click Use variable to assign cases&hellip;.enter the variable 
</p>
<p>"Partition"&hellip;.click OK.    
</p>
<p>    
</p>
<p>    The above fi gure shows as an example the place of the two new compounds (the 
</p>
<p>small triangles) as compared with those of the standard opioids. Lines connect them 
</p>
<p>to their 3 nearest neighbors. In SPSS&rsquo; original output sheets the graph can by double- 
</p>
<p>clicking be placed in the &ldquo;model viewer&rdquo;, and, then, (after again clicking on it) be 
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>20
</p>
<p>interactively rotated in order to improve the view of the distances. SPSS uses 3 near-
</p>
<p>est neighbors by default, but you can change this number if you like. The names of 
</p>
<p>the compounds are given in alphabetical order. Only three of seven variables are 
</p>
<p>given in the initial fi gure, but if you click on one of the small triangles in this fi gure, 
</p>
<p>an auxiliary view comes up right from the main view. Here are all the details of the 
</p>
<p>analysis. The upper left graph of it shows that the opioids 21, 3, and 23 have the best 
</p>
<p>average nearest neighbor records for case 26 (new drug 1). The seven fi gures alongside 
</p>
<p>and underneath this fi gure give the distances between these three and case 26 for 
</p>
<p>each of the seven features (otherwise called predictor variables). 
</p>
<p>4 Nearest Neighbors for Classifying New Medicines (2 New and 25 Old Opioids)</p>
<p/>
</div>
<div class="page"><p/>
<p>21
</p>
<p>  
</p>
<p>sufentanil
</p>
<p>oxymorphine
</p>
<p>naltrexone
</p>
<p>nalbuphine
</p>
<p>meptazinol
</p>
<p>levallorphan
</p>
<p>fentanyl
</p>
<p>butorphanol
</p>
<p>alfentanil
</p>
<p>6,00
</p>
<p>23
</p>
<p>drugname
</p>
<p>antitussive(score) constipation(score)
</p>
<p>respiratorydepression(score) abuseliability(score)
</p>
<p>elimination(hours) duration(horus)
</p>
<p>analgesia(score)
</p>
<p>23
</p>
<p>23
</p>
<p>23
23
</p>
<p>21
</p>
<p>21
</p>
<p>21
</p>
<p>21
</p>
<p>21
</p>
<p>21
</p>
<p>21
</p>
<p>26
</p>
<p>26
</p>
<p>23
</p>
<p>213
</p>
<p>23
</p>
<p>26
</p>
<p>26
</p>
<p>26
</p>
<p>2326
</p>
<p>26
</p>
<p>3
</p>
<p>3
</p>
<p>3
</p>
<p>3
</p>
<p>3 3
</p>
<p>3
</p>
<p>6,00
</p>
<p>6,00
</p>
<p>5,50
5,50
</p>
<p>5,00
</p>
<p>5,004,50
</p>
<p>4,50
</p>
<p>4,00
</p>
<p>4,00
</p>
<p>5,50
</p>
<p>5,00
</p>
<p>5,00 12,00
</p>
<p>10,00
</p>
<p>8,00
</p>
<p>6,00
</p>
<p>4,00
</p>
<p>2,00
</p>
<p>4,00
</p>
<p>3,00
</p>
<p>2,00
</p>
<p>1,00
</p>
<p>4,50
</p>
<p>4,00
</p>
<p>3,50
</p>
<p>3,00
</p>
<p>5,00
</p>
<p>4,50
</p>
<p>4,00
</p>
<p>3,50
</p>
<p>3,00
</p>
<p>6,00
</p>
<p>5,50
</p>
<p>5,00
</p>
<p>4,50
</p>
<p>4,00
</p>
<p>3,50
</p>
<p>3,00
</p>
<p>  
</p>
<p> Example</p>
<p/>
</div>
<div class="page"><p/>
<p>22
</p>
<p>       
</p>
<p>    If you click on the other triangle (representing case 27 (newdrug 2) in the initial 
</p>
<p>fi gure), the connecting lines with the nearest neighbors of this drug comes up. This 
</p>
<p>is shown in the above fi gure, which is the main view for drug 2. Using the same 
</p>
<p>manoeuvre as above produces again the auxiliary view showing that the opioids 3, 
</p>
<p>1, and 11 have the best average nearest neighbor records for case 27 (new drug 2). 
</p>
<p>The seven fi gures alongside and underneath this fi gure give again the distances 
</p>
<p>between these three and case 27 for each of the seven features (otherwise called 
</p>
<p>predictor variables). The auxiliary view is shown underneath. 
</p>
<p>4 Nearest Neighbors for Classifying New Medicines (2 New and 25 Old Opioids)</p>
<p/>
</div>
<div class="page"><p/>
<p>23
</p>
<p>  
</p>
<p>sufentanil
</p>
<p>oxymorphine
</p>
<p>naltrexone
</p>
<p>nalbuphine
</p>
<p>meptazinol
</p>
<p>levallorphan
</p>
<p>fentanyl
</p>
<p>butorphanol
</p>
<p>alfentanil
</p>
<p>6,00
</p>
<p>11
</p>
<p>drugname
</p>
<p>Peers Chart
</p>
<p>Focal Records and Nearest Neighbors
</p>
<p>antitussive(score) constipation(score)
</p>
<p>respiratorydepression(score) abuseliability(score)
</p>
<p>elimination(hours) duration(hours)
</p>
<p>analgesia(score)
</p>
<p>27
</p>
<p>1
</p>
<p>11
</p>
<p>3
</p>
<p>311
</p>
<p>1
</p>
<p>27
</p>
<p>11
</p>
<p>27
</p>
<p>1
3
</p>
<p>27
</p>
<p>1
</p>
<p>3
</p>
<p>11
</p>
<p>1
3
</p>
<p>311 27
</p>
<p>1
</p>
<p>1
</p>
<p>11
</p>
<p>3
</p>
<p>27
</p>
<p>27
</p>
<p>11
</p>
<p>1
</p>
<p>3
</p>
<p>8,00
</p>
<p>7,50
</p>
<p>7,00
</p>
<p>6,50
</p>
<p>6,00
</p>
<p>5,50
</p>
<p>5,00
</p>
<p>6,00
</p>
<p>5,50
</p>
<p>5,00
</p>
<p>4,50
</p>
<p>4,00
</p>
<p>3,50
</p>
<p>3,00
</p>
<p>5,50
</p>
<p>5,00
</p>
<p>4,50
</p>
<p>4,00
</p>
<p>7,00 8,00
</p>
<p>7,00
</p>
<p>6,00
</p>
<p>5,00
</p>
<p>4,00
</p>
<p>15,00
</p>
<p>12,50
</p>
<p>10,00
</p>
<p>7,50
</p>
<p>5,00
</p>
<p>6,50
</p>
<p>6,00
</p>
<p>5,50
</p>
<p>5,00
</p>
<p>4,50
</p>
<p>4,00
</p>
<p>7,00
</p>
<p>6,00
</p>
<p>5,00
</p>
<p>4,00
</p>
<p>3,00
</p>
<p>2,00
  
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>24
</p>
<p>        Conclusion 
</p>
<p> Nearest neighbor methodology enables to readily identify the places of new drugs 
</p>
<p>within their classes of drugs. For example, newly developed opioid compounds can 
</p>
<p>be compared with standard opioids in order to determine their potential places in 
</p>
<p>therapeutic regimens.  
</p>
<p>    Note 
</p>
<p> Nearest neighbor cluster methodology has a long history and has initially been used 
</p>
<p>for missing data imputation in demographic data fi les (see Statistics applied to clini-
</p>
<p>cal studies 5th edition, 2012, Chap. 22, Missing data, pp 253&ndash;266, Springer 
</p>
<p>Heidelberg Germany, from the same authors).    
</p>
<p>4 Nearest Neighbors for Classifying New Medicines (2 New and 25 Old Opioids)</p>
<p/>
</div>
<div class="page"><p/>
<p>25&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_5
</p>
<p>    Chapter 5   
</p>
<p> Predicting High-Risk-Bin Memberships 
(1,445 Families) 
</p>
<p>                      General Purpose 
</p>
<p> Optimal bins describe continuous predictor variables in the form of best fi t catego-
</p>
<p>ries for making predictions, e.g., about families at high risk of bank loan defaults. In 
</p>
<p>addition, it can be used for, e.g., predicting health risk cut-offs about individual 
</p>
<p>future families, based on their characteristics (Chap.   56    ).  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Can optimal binning also be applied for other medical purposes, e.g., for fi nding 
</p>
<p>high risk cut-offs for overweight children in particular families?  
</p>
<p>    Example 
</p>
<p> A data fi le of 1,445 families was assessed for learning the best fi t cut-off values of 
</p>
<p>unhealthy lifestyle estimators to maximize the difference between low and high risk 
</p>
<p>of overweight children. These cut-off values were, subsequently, used to determine 
</p>
<p>the risk profi les (the characteristics) in individual future families. 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 2&rdquo; as Chap. 2, 
</p>
<p>2014. </p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_56">http://dx.doi.org/10.1007/978-3-319-15195-3_56</a></div>
</div>
<div class="page"><p/>
<p>26
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5 
</p>
<p> 0  11  1  8  0 
</p>
<p> 0  7  1  9  0 
</p>
<p> 1  25  7  0  1 
</p>
<p> 0  11  4  5  0 
</p>
<p> 1  5  1  8  1 
</p>
<p> 0  10  2  8  0 
</p>
<p> 0  11  1  6  0 
</p>
<p> 0  7  1  8  0 
</p>
<p> 0  7  0  9  0 
</p>
<p> 0  15  3  0  0 
</p>
<p>  Var = variable 
</p>
<p> Var 1 fruitvegetables (times per week) 
</p>
<p> Var 2 unhealthysnacks (times per week) 
</p>
<p> Var 3 fastfoodmeal (times per week) 
</p>
<p> Var 4 physicalactivities (times per week) 
</p>
<p> Var 5 overweightchildren (0 = no, 1 = yes) 
</p>
<p>    Only the fi rst 10 families of the original learning data fi le are given, the entire 
</p>
<p>data fi le is entitled &ldquo;optimalbinning1&rdquo; and is in extras.springer.com.  
</p>
<p>    Optimal Binning 
</p>
<p> SPSS 19.0 is used for analysis. Start by opening the data fi le.
</p>
<p>  Command: 
</p>
<p>  Transform&hellip;.Optimal Binning&hellip;.Variables into Bins: enter fruitvegetables, 
</p>
<p>unhealthysnacks, fastfoodmeal, physicalactivities&hellip;.Optimize Bins with Respect 
</p>
<p>to: enter "overweightchildren"&hellip;.click Output&hellip;.Display: mark Endpoints&hellip;.mark 
</p>
<p>Descriptive statistics&hellip;.mark Model Entropy&hellip;.click Save: mark Create variables 
</p>
<p>that contain binned data&hellip;.Save Binning Rules in a Syntax fi le: click Browse&hellip;.
</p>
<p>open appropriate folder&hellip;.File name: enter, e.g.,  "exportoptimalbinning"&hellip;.click 
</p>
<p>Save&hellip;.click OK.   
</p>
<p> fruitvegetables/wk 
</p>
<p> Bin 
</p>
<p> End point 
</p>
<p> Number of cases by level of overweight 
</p>
<p>children 
</p>
<p> Lower  Upper  No  Yes  Total 
</p>
<p> 1   a   14  802  340  1142 
</p>
<p> 2  14   a   274  29  303 
</p>
<p> Total  1076  369  1445 
</p>
<p>5 Predicting High-Risk-Bin Memberships (1,445 Families)</p>
<p/>
</div>
<div class="page"><p/>
<p>27
</p>
<p> unhealthysnacks/wk 
</p>
<p> Bin 
</p>
<p> End point 
</p>
<p> Number of cases by level of overweight 
</p>
<p>children 
</p>
<p> Lower  Upper  No  Yes  Total 
</p>
<p> 1   a   12  830  143  973 
</p>
<p> 2  12  19  188  126  314 
</p>
<p> 3  19   a   58  100  158 
</p>
<p> Total  1076  369  1445 
</p>
<p> fastfoodmeal/wk 
</p>
<p> Bin 
</p>
<p> End point 
</p>
<p> Number of cases by level of overweight 
</p>
<p>children 
</p>
<p> Lower  Upper  No  Yes  Total 
</p>
<p> 1   a   2  896  229  1125 
</p>
<p> 2  2   a   180  140  320 
</p>
<p> Total  1076  369  1445 
</p>
<p> physicalactivities/wk 
</p>
<p> Bin 
</p>
<p> End point 
</p>
<p> Number of cases by level of overweight 
</p>
<p>children 
</p>
<p> Lower  Upper  No  Yes  Total 
</p>
<p> 1   a   8  469  221  690 
</p>
<p> 2  8   a   607  148  755 
</p>
<p> Total  1076  369  1445 
</p>
<p>  Each bin is computed as Lower &lt;= physicalactivities/wk &lt;Upper 
</p>
<p> a. Unbounded 
</p>
<p>    In the output sheets the above table is given. It shows the high risk cut-offs for 
</p>
<p>overweight children of the four predicting factors. E.g., in 1,142 families scoring 
</p>
<p>under 14 units of (1) fruit/vegetable per week, are put into bin 1 and 303 scoring 
</p>
<p>over 14 units per week, are put into bin 2. The proportion of overweight children in 
</p>
<p>bin 1 is much larger than it is in bin 2: 340/1,142 = 0.298 (30 %) and 29/303 = 0.096 
</p>
<p>(10 %). Similarly high risk cut-offs are found for (2) unhealthy snacks less than 12, 
</p>
<p>12&ndash;19, and over 19 per week, (3) fastfood meals less than 2, and over 2 per week, 
</p>
<p>(4) physical activities less than 8 and over 8 per week. These cut-offs will be used 
</p>
<p>as meaningful recommendation limits to 11 future families. 
</p>
<p>Optimal Binning</p>
<p/>
</div>
<div class="page"><p/>
<p>28
</p>
<p> fruit  snacks  fastfood  physical 
</p>
<p> 13  11  4  5 
</p>
<p> 2  5  3  9 
</p>
<p> 12  23  9  0 
</p>
<p> 17  9  6  5 
</p>
<p> 2  3  3  3 
</p>
<p> 10  8  4  3 
</p>
<p> 15  9  3  6 
</p>
<p> 9  5  3  8 
</p>
<p> 2  5  2  7 
</p>
<p> 9  13  5  0 
</p>
<p> 28  3  3  9 
</p>
<p>  Var 1 fruitvegetables (times per week) 
</p>
<p> Var 2 unhealthysnacks (times per week) 
</p>
<p> Var 3 fastfoodmeal (times per week) 
</p>
<p> Var 4 physicalactivities (times per week) 
</p>
<p>    The saved syntax fi le entitled "exportoptimalbinning.sps" will now be used to 
</p>
<p>compute the predicted bins of some future families. Enter the above values in a new 
</p>
<p>data fi le, entitled, e.g., "optimalbinning2", and save in the appropriate folder in your 
</p>
<p>computer. Then open up the data fi le "exportoptimalbinning.sps"&hellip;.subsequently 
</p>
<p>click File&hellip;.click Open&hellip;.click Data&hellip;.Find the data fi le entitled "optimalbin-
</p>
<p>ning2"&hellip;.click Open&hellip;.click "exportoptimalbinning.sps" from the fi le palette at the 
</p>
<p>bottom of the screen&hellip;.click Run&hellip;.click All. 
</p>
<p> When returning to the Data View of "optimalbinning2", we will fi nd the under-
</p>
<p>neath overview of all of the bins selected for our 11 future families. 
</p>
<p> fruit  snacks  fastfood  physical  fruit _bin  snacks _bin  fastfood _bin  physical _bin 
</p>
<p> 13  11  4  5  1  1  2  1 
</p>
<p> 2  5  3  9  1  1  2  2 
</p>
<p> 12  23  9  0  1  3  2  1 
</p>
<p> 17  9  6  5  2  1  2  1 
</p>
<p> 2  3  3  3  1  1  2  1 
</p>
<p> 10  8  4  3  1  1  2  1 
</p>
<p> 15  9  3  6  2  1  2  1 
</p>
<p> 9  5  3  8  1  1  2  2 
</p>
<p> 2  5  2  7  1  1  2  1 
</p>
<p> 9  13  5  0  1  2  2  1 
</p>
<p> 28  3  3  9  2  1  2  2 
</p>
<p>   This overview is relevant, since families in high risk bins would particularly 
</p>
<p>qualify for counseling.  
</p>
<p>5 Predicting High-Risk-Bin Memberships (1,445 Families)</p>
<p/>
</div>
<div class="page"><p/>
<p>29
</p>
<p>    Conclusion 
</p>
<p> Optimal bins describe continuous predictor variables in the form of best fi t catego-
</p>
<p>ries for making predictions, and SPSS statistical software can be used to generate a 
</p>
<p>syntax fi le, called SPS fi le, for predicting risk cut-offs in future families. In this way 
</p>
<p>families highly at risk for overweight can be readily identifi ed. The nodes of deci-
</p>
<p>sion trees can be used for similar purposes (Machine learning in medicine Cookbook 
</p>
<p>One, Chap. 16, Decision trees for decision analysis, pp 97&ndash;104, Springer Heidelberg 
</p>
<p>Germany, 2014), but it has subgroups of cases, rather than multiple bins for a single 
</p>
<p>case.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of optimal binning is 
</p>
<p>given in Machine Learning in Medicine Part Three, Chap. 5, Optimal binning, 
</p>
<p>pp 37&ndash;48, Springer Heidelberg Germany 2013, and Machine learning in medicine 
</p>
<p>Cookbook One, Optimal binning, Chap. 19, pp 101&ndash;106, Springer Heidelberg 
</p>
<p>Germany, 2014, both from the same authors.    
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>31&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_6
</p>
<p>    Chapter 6   
</p>
<p> Predicting Outlier Memberships 
(2,000 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> With large data fi les outlier recognition requires a more sophisticated approach than 
</p>
<p>the traditional data plots and regression lines. This chapter is to examine whether 
</p>
<p>BIRCH (balanced iterative reducing and clustering using hierarchies) clustering is 
</p>
<p>able to predict outliers in future patients from a known population.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Is the XML (eXtended Markup Language) fi le from a 2,000 patient sample capable 
</p>
<p>of making predictions about cluster memberships and outlierships in future patients 
</p>
<p>from the target population.  
</p>
<p>    Example 
</p>
<p> In a 2,000 patient study of hospital admissions 576 possibly iatrogenic admissions 
</p>
<p>were identifi ed. Based on age and numbers of co-medications a two step BIRCH 
</p>
<p>cluster analysis will be performed. SPSS version 19 and up can be used for the 
</p>
<p> purpose. Only the fi rst 10 patients&rsquo; data are shown underneath. The entire data fi le 
</p>
<p>is in extras.springer.com, and is entitled &ldquo;outlierdetection&rdquo;. 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 2&rdquo; as Chap. 3, 
</p>
<p>2014. </p>
<p/>
</div>
<div class="page"><p/>
<p>32
</p>
<p> age  gender  admis  duration  mort  iatro  comorb  comed 
</p>
<p> 1939,00  2,00  7,00  ,00  ,00  1,00  2,00  1,00 
</p>
<p> 1939,00  2,00  7,00  2,00  1,00  1,00  2,00  1,00 
</p>
<p> 1943,00  2,00  11,00  1,00  ,00  1,00  ,00  ,00 
</p>
<p> 1921,00  2,00  9,00  17,00  ,00  1,00  3,00  3,00 
</p>
<p> 1944,00  2,00  21,00  30,00  ,00  1,00  3,00  3,00 
</p>
<p> 1977,00  2,00  4,00  1,00  ,00  1,00  1,00  1,00 
</p>
<p> 1930,00  1,00  20,00  7,00  ,00  1,00  2,00  2,00 
</p>
<p> 1932,00  1,00  3,00  2,00  ,00  1,00  4,00  4,00 
</p>
<p> 1927,00  1,00  9,00  13,00  1,00  1,00  1,00  2,00 
</p>
<p> 1920,00  2,00  23,00  8,00  ,00  1,00  3,00  3,00 
</p>
<p>  admis = admission indication code 
</p>
<p> duration = days of admission 
</p>
<p> mort = mortality 
</p>
<p> iatro = iatrogenic admission 
</p>
<p> comorb = number of comorbidities 
</p>
<p> comed = number of comedications 
</p>
<p>    Start by opening the fi le.
</p>
<p>  Then command: 
</p>
<p>  click Transform&hellip;.click Random Number Generators&hellip;.click Set Starting Point&hellip;.
</p>
<p>click Fixed Value (2000000)&hellip;.click OK&hellip;.click Analyze&hellip;. Classify&hellip;.Two Step 
</p>
<p>Cluster Analysis &hellip;.Continuous Variables: enter age and co-medications&hellip;.Distance 
</p>
<p>Measure: mark Euclidean&hellip;.Clustering Criterion: mark Schwarz&rsquo;s Bayesian 
</p>
<p>Criterion&hellip;.click Options: mark Use noise handling&hellip;.percentage: enter 25&hellip;.
</p>
<p>Assumed Standardized: enter age and co-medications&hellip;.click Continue&hellip;.mark 
</p>
<p>Pivot tables&hellip;.mark Charts and tables in Model Viewer&hellip;.Working Data File: mark 
</p>
<p>Create Cluster membership variable&hellip;.XML Files: mark Export fi nal model&hellip;.
</p>
<p>click Browse&hellip;.select the appropriate folder in your computer&hellip;.File Name: enter, 
</p>
<p>e.g., "exportanomalydetection"&hellip;.click Save&hellip;.click Continue&hellip;.click OK.    
</p>
<p> In the output sheets the underneath distribution of clusters is given.
</p>
<p> Cluster distribution 
</p>
<p> N  % of combined  % of total 
</p>
<p> Cluster  1  181  31,4 %  9,1 % 
</p>
<p>       2  152  26,4 %  7,6 % 
</p>
<p>       3  69  12,0 %  3,5 % 
</p>
<p>       Outlier (&minus;1)  174  30,2 %  8,7 % 
</p>
<p>       Combined  576  100,0 %  28,8 % 
</p>
<p> Excluded cases  1,424  71,2 % 
</p>
<p> Total  2,000  100,0 % 
</p>
<p>6 Predicting Outlier Memberships (2,000 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>33
</p>
<p>   Additional details are given in Machine learning in medicine Part Two, Chap. 10, 
</p>
<p>Anomaly detection, pp 93&ndash;103, Springer Heidelberg Germany, 2013. The large 
</p>
<p>outlier category consisted mainly of patients of all ages and extremely many 
</p>
<p> co- medications. When returning to the Data View screen, we will observe that SPSS 
</p>
<p>has created a novel variable entitled &ldquo;TSC_5980&rdquo; containing the patients&rsquo; cluster 
</p>
<p>memberships. The patients given the value &minus;1 are the outliers. 
</p>
<p> With Scoring Wizard and the exported XML (eXtended Markup Language) fi le 
</p>
<p>entitled &ldquo;exportanomalydetection&rdquo; we can now try and predict from age and number 
</p>
<p>of co-medications of future patients the best fi t cluster membership according to the 
</p>
<p>computed XML model. 
</p>
<p> age  comed 
</p>
<p> 1954,00  1,00 
</p>
<p> 1938,00  7,00 
</p>
<p> 1929,00  8,00 
</p>
<p> 1967,00  1,00 
</p>
<p> 1945,00  2,00 
</p>
<p> 1936,00  3,00 
</p>
<p> 1928,00  4,00 
</p>
<p>  comed = number of co-medications  
</p>
<p>    Enter the above data in a novel data fi le and command:
</p>
<p>   Utilities&hellip;.click Scoring Wizard&hellip;.click Browse&hellip;.Open the appropriate folder 
</p>
<p>with the XML fi le entitled "exportanomalydetection"&hellip;.click on the latter and click 
</p>
<p>Select&hellip;.in Scoring Wizard double-click Next&hellip;.mark Predicted Value&hellip;.click 
</p>
<p>Finish.    
</p>
<p> age  comed  PredictedValue 
</p>
<p> 1954,00  1,00  3,00 
</p>
<p> 1938,00  7,00  &minus;1,00 
</p>
<p> 1929,00  8,00  &minus;1,00 
</p>
<p> 1967,00  1,00  3,00 
</p>
<p> 1945,00  2,00  &minus;1,00 
</p>
<p> 1936,00  3,00  1,00 
</p>
<p> 1928,00  4,00  &minus;1,00 
</p>
<p>  PredictedValue = predicted cluster membership 
</p>
<p>    In the above novel data fi le SPSS has provided the new variable as requested. 
</p>
<p>One patient is in cluster 1, two are in cluster 3, and 4 patients are in the outlier 
</p>
<p>cluster.  
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>34
</p>
<p>    Conclusion 
</p>
<p> An XML (eXtended Markup Language) fi le from a 2,000 patient sample is capable 
</p>
<p>of making predictions about cluster memberships and outlierships in future patients 
</p>
<p>from the same target population.  
</p>
<p>    Note 
</p>
<p> More background theoretical and mathematical information of outlier detection. 
</p>
<p> Is available Machine learning in medicine part two, Chap. 10, Anomaly  detection, 
</p>
<p>pp 93&ndash;103, Springer Heidelberg Germany, 2013, from the same authors.    
</p>
<p>6 Predicting Outlier Memberships (2,000 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>35&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_7
</p>
<p>    Chapter 7   
</p>
<p> Data Mining for Visualization of Health 
Processes (150 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> Computer fi les of clinical data are often complex and multi-dimensional, and they 
</p>
<p>are, frequently, hard to statistically test. Instead, visualization processes can be 
</p>
<p> successfully used as an alternative approach to traditional statistical data analysis. 
</p>
<p> For example, Knime (Konstanz information miner) software has been developed 
</p>
<p>by computer scientists from Silicon Valley in collaboration with technicians from 
</p>
<p>Konstanz University at the Bodensee in Switzerland, and it pays particular attention 
</p>
<p>to visual data analysis. It is used since 2006 as a freely available package through 
</p>
<p>the Internet. So far, it is mainly used by chemists and pharmacists, but not by  clinical 
</p>
<p>investigators. This chapter is to assess, whether visual processing of clinical data 
</p>
<p>may, sometimes, perform better than traditional statistical analysis.  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> Can visualization processes of clinical data provide insights that remained hidden 
</p>
<p>with traditional statistical tests?  
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 3&rdquo; as Chap. 1, 
</p>
<p>2014. </p>
<p/>
</div>
<div class="page"><p/>
<p>36
</p>
<p>    Example 
</p>
<p> Four infl ammatory markers (CRP (C-reactive protein), ESR (erythrocyte sedimen-
</p>
<p>tation rate), leucocyte count (leucos), and fi brinogen) were measured in 150 patients 
</p>
<p>with pneumonia. Based on x-ray chest clinical severity was classifi ed as A (mild 
</p>
<p>infection), B (medium severity), C (severe infection). One scientifi c question was to 
</p>
<p>assess whether the markers could adequately predict the severity of infection. 
</p>
<p> CRP  leucos  fi brinogen  ESR  x-ray severity 
</p>
<p> 120,00  5,00  11,00  60,00  A 
</p>
<p> 100,00  5,00  11,00  56,00  A 
</p>
<p> 94,00  4,00  11,00  60,00  A 
</p>
<p> 92,00  5,00  11,00  58,00  A 
</p>
<p> 100,00  5,00  11,00  52,00  A 
</p>
<p> 108,00  6,00  17,00  48,00  A 
</p>
<p> 92,00  5,00  14,00  48,00  A 
</p>
<p> 100,00  5,00  11,00  54,00  A 
</p>
<p> 88,00  5,00  11,00  54,00  A 
</p>
<p> 98,00  5,00  8,00  60,00  A 
</p>
<p> 108,00  5,00  11,00  68,00  A 
</p>
<p> 96,00  5,00  11,00  62,00  A 
</p>
<p> 96,00  5,00  8,00  46,00  A 
</p>
<p> 86,00  4,00  8,00  60,00  A 
</p>
<p> 116,00  4,00  11,00  50,00  A 
</p>
<p> 114,00  5,00  17,00  52,00  A 
</p>
<p>  CRP = C-reactive protein (mg/l) 
</p>
<p> leucos = leucyte count (*10 9 /l) 
</p>
<p> fi brinogen = fi brinogen level (mg/l) 
</p>
<p> ESR = erythrocyte sedimentation rate (mm) 
</p>
<p> x-ray severity = x-chest severity pneumonia score (A&ndash;C = mild to severe) 
</p>
<p>    The data fi le is entitled &ldquo;decisiontree&rdquo;, and is available in extras.springer.com. 
</p>
<p>Data analysis of these data in SPSS is rather limited. Start by opening the data fi le 
</p>
<p>in SPSS statistical software.
</p>
<p>  Command: 
</p>
<p>  click Graphs&hellip;.Legacy Dialogs&hellip;.Bar Charts&hellip;.click Simple&hellip;.click Defi ne&hellip;.
</p>
<p>Category Axis: enter "severity score"&hellip;.Variable: enter CRP&hellip;.mark Other statis-
</p>
<p>tics&hellip;.click OK.    
</p>
<p> After performing the same procedure for the other variables four graphs are pro-
</p>
<p>duced as shown underneath. The mean levels of all of the infl ammatory markers 
</p>
<p>consistently tended to rise with increasing severities of infection. Univariate multi-
</p>
<p>nomial logistic regression with severity as outcome gives a signifi cant effect of all 
</p>
<p>7 Data Mining for Visualization of Health Processes (150 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>37
</p>
<p>of the markers. However, this effect is largely lost in the multiple multinomial logis-
</p>
<p>tic regression, probably due to interactions. 
</p>
<p>  
</p>
<p>125,00
</p>
<p>20,00
</p>
<p>15,00
</p>
<p>10,00
</p>
<p>5,00
</p>
<p>,00
</p>
<p>100,00
</p>
<p>75,00
</p>
<p>M
e
a
n
</p>
<p> C
R
</p>
<p>P
 m
</p>
<p>g
/I
</p>
<p>M
e
a
n
</p>
<p> f
ib
</p>
<p>ri
n
</p>
<p>o
g
</p>
<p>e
n
</p>
<p> m
g
</p>
<p>/I
</p>
<p>M
e
a
n
</p>
<p> E
S
</p>
<p>R
 m
</p>
<p>m
M
</p>
<p>e
a
n
</p>
<p> l
e
u
</p>
<p>c
o
</p>
<p>s
 1
</p>
<p>0
^
</p>
<p>9
/l
</p>
<p>severity score severity score
</p>
<p>severity scoreseverity score
</p>
<p>50,00
</p>
<p>25,00
</p>
<p>60,00
</p>
<p>40,00
</p>
<p>20,00
</p>
<p>,00
</p>
<p>60,00
</p>
<p>80,00
</p>
<p>40,00
</p>
<p>20,00
</p>
<p>,00
</p>
<p>,00
A B C
</p>
<p>A B C A B C
</p>
<p>A B C
</p>
<p>  
</p>
<p>    We are interested to explore these results for additional effects, for example, hid-
</p>
<p>den data effects, like different predictive effects and frequency distributions for dif-
</p>
<p>ferent subgroups. For that purpose Knime data miner will be applied. SPSS data 
</p>
<p>fi les can not be downloaded directly in the Knime software, but excel fi les can, and 
</p>
<p>SPSS data can be saved as an excel fi le (the csv fi le type available in your computer 
</p>
<p>must be used).
</p>
<p>  Command in SPSS: 
</p>
<p>  click File....click Save as....in "Save as" type: enter Comma Delimited (*.csv)....
</p>
<p>click Save.     
</p>
<p>    Knime Data Miner 
</p>
<p> In Google enter the term &ldquo;knime&rdquo;. Click Download and follow instructions. After 
</p>
<p>completing the pretty easy download procedure, open the knime workbench by 
</p>
<p>clicking the knime welcome screen. The center of the screen displays the workfl ow 
</p>
<p>editor like the canvas in SPSS modeler. It is empty, and can be used to build a stream 
</p>
<p>Knime Data Miner</p>
<p/>
</div>
<div class="page"><p/>
<p>38
</p>
<p>of nodes, called workfl ow in knime. The node repository is in the left lower angle of 
</p>
<p>the screen, and the nodes can be dragged to the workfl ow editor simply by left-
</p>
<p>clicking. The nodes are computer tools for data analysis like visualization and sta-
</p>
<p>tistical processes. Node description is in the right upper angle of the screen. Before 
</p>
<p>the nodes can be used, they have to be connected with the &ldquo;fi le reader&rdquo; node, and 
</p>
<p>with one another by arrows drawn again simply by left clicking the small triangles 
</p>
<p>attached to the nodes. Right clicking on the fi le reader enables to confi gure from 
</p>
<p>your computer a requested data fi le....click Browse....and download from the appro-
</p>
<p>priate folder a csv type Excel fi le. You are set for analysis now. For convenience an 
</p>
<p>CSV fi le entitled &ldquo;decisiontree&rdquo; has been made available at extras.springer.com.  
</p>
<p>    Knime Workfl ow 
</p>
<p> A knime workfl ow for the analysis of the above data example will be built, and the 
</p>
<p>fi nal result is shown in the underneath fi gure. 
</p>
<p>    
</p>
<p>7 Data Mining for Visualization of Health Processes (150 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>39
</p>
<p>        Box and Whiskers Plots 
</p>
<p> In the node repository fi nd the node Box Plot. First click the IO option (import/
</p>
<p>export option nodes). Then click &ldquo;Read&rdquo;, then the File Reader node is displayed, 
</p>
<p>and can be dragged by left clicking to the workfl ow editor. Enter the requested data 
</p>
<p>fi le as described above. A Node dialog is displayed underneath the node entitled 
</p>
<p>Node 1. Its light is orange at this stage, and should turn green before it can be 
</p>
<p>applied. If you right click the node&rsquo;s center, and then left click File Table a preview 
</p>
<p>of the data is supplied. 
</p>
<p> Now, in the search box of the node repository fi nd and click Data Views....then 
</p>
<p>&ldquo;Box plot&rdquo;....drag to workfl ow editor....connect with arrow to File reader....right 
</p>
<p>click File reader....right click execute....right click Box Plot node....right click 
</p>
<p>Confi gurate....right click Execute and open view.... 
</p>
<p>    
</p>
<p>    The above box plots with 95 % confi dence intervals of the four variable are dis-
</p>
<p>played. The ESR plot shows that also outliers have been displayed The smallest 
</p>
<p>confi dence interval has the leucocyte count, and it may, thus, be the best predictor.  
</p>
<p>    Lift Chart 
</p>
<p> In the node repository....click Lift Chart and drag to workfl ow editor.... connect with 
</p>
<p>arrow to File reader....right click execute Lift Chart node....right click Confi gurate....
</p>
<p>right click Execute and open view.... 
</p>
<p>Lift Chart</p>
<p/>
</div>
<div class="page"><p/>
<p>40
</p>
<p>    
</p>
<p>    The lift chart shows the predictive performance of the data assuming that the four 
</p>
<p>infl ammatory markers are predictors and the severity score is the outcome. If the 
</p>
<p>predictive performance is no better than random, the ratio successful prediction 
</p>
<p>with/without the model = 1.000 (the green line) The x-axis give dociles (1 = 10 = 10 % 
</p>
<p>of the entire sample etc.). It can be observed that at 7 or more dociles the predictive 
</p>
<p>performance start to be pretty good (with ratios of 2.100&ndash;2.400). Logistic regression 
</p>
<p>(here multinomial logistic regression) is being used by Knime for making 
</p>
<p>predictions.  
</p>
<p>    Histogram 
</p>
<p> In the node repository click type color....click the color manager node and drag to 
</p>
<p>workfl ow editor....in node repository click color....click the Esc button of your com-
</p>
<p>puter....click Data Views....select interactive histogram and transfer to workfl ow 
</p>
<p>editor....connect color manager node with File Reader&hellip;connect color manager 
</p>
<p>with &ldquo;interactive histogram node&rdquo;....right click Confi gurate....right click Execute 
</p>
<p>and open view.... 
</p>
<p>7 Data Mining for Visualization of Health Processes (150 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>41
</p>
<p>    
</p>
<p>    Interactive histograms with bins of ESR values are given. The colors provide the 
</p>
<p>proportions of cases with mild severity (A, red), medium severity (B, green), and 
</p>
<p>severe pneumonias (C, blue). It can be observed that many mild cases (red) are in 
</p>
<p>the ESR 44&ndash;71 mm cut-off. Above ESR of 80 mm blue (severe pneumonia) is 
</p>
<p>increasingly present. The software program has selected only the ESR values 
</p>
<p>44&ndash;134. Instead of histograms with ESR, those with other predictor variables can be 
</p>
<p>made.  
</p>
<p>    Line Plot 
</p>
<p> In the node repository click Data Views....select the node Line plots and transfer to 
</p>
<p>workfl ow editor....connect color manager with &ldquo;Line plots&rdquo;....right click 
</p>
<p>Confi gurate....right click Execute and open view.... 
</p>
<p>Line Plot</p>
<p/>
</div>
<div class="page"><p/>
<p>42
</p>
<p>    
</p>
<p>    The line plot gives the values of all cases along the x-axis. The upper curve are 
</p>
<p>the CRP values, The middle one the ESR values. The lower part are the leucos and 
</p>
<p>fi brinogen values. The rows 0&ndash;50 are the cases with mild pneumonia, the rows 
</p>
<p>51&ndash;100 the medium severity cases, and the rows 101&ndash;150 the severe cases. It can be 
</p>
<p>observed that particularly the CRP-, fi brinogen-, and leucos levels increase with 
</p>
<p>increased severity of infection. This is not observed with the ESR levels.  
</p>
<p>    Matrix of Scatter Plots 
</p>
<p> In the node repository click Data Views....select &ldquo;Matrix of scatter plots&rdquo; and trans-
</p>
<p>fer to workfl ow editor....connect color manager with &ldquo;Matrix of scatter plots&rdquo; ....
</p>
<p>right click Confi gurate....right click Execute and open view.... 
</p>
<p>7 Data Mining for Visualization of Health Processes (150 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>43
</p>
<p>    
</p>
<p>    The above fi gure gives the results. The four predictors variables are plotted 
</p>
<p>against one another. by the colors (blue for severest, red for mildest pneumonias) the 
</p>
<p>fi elds show that the severest pneumonias are predominantly in the right upper quad-
</p>
<p>rant, the mildest in the left lower quadrant.  
</p>
<p>    Parallel Coordinates 
</p>
<p> In the node repository click Data Views....select &ldquo;Parallel coordinates&rdquo; and transfer 
</p>
<p>to workfl ow editor....connect color manager with &ldquo;Parallel coordinates&rdquo; ....right 
</p>
<p>click Confi gurate....right click Execute and open view....click Appearance....click 
</p>
<p>Draw (spline) Curves instead of lines.... 
</p>
<p>Parallel Coordinates</p>
<p/>
</div>
<div class="page"><p/>
<p>44
</p>
<p>    
</p>
<p>    The above fi gure is given. It shows that the leucocyte count and fi brinogen level 
</p>
<p>are excellent predictors of infection severities. CRP and ESR are also adequate pre-
</p>
<p>dictors of infections with mild and medium severities, however, poor predictors of 
</p>
<p>levels of severe infections.  
</p>
<p>    Hierarchical Cluster Analysis with SOTA (Self Organizing 
</p>
<p>Tree Algorithm) 
</p>
<p> In the node repository click Mining....select the node SOTA (Self Organizing tree 
</p>
<p>Algorithm) Learner and transfer to workfl ow editor....connect color manager with 
</p>
<p>&ldquo;SOTA learner&rdquo;....right click Confi gurate....right click Execute and open view.... 
</p>
<p>7 Data Mining for Visualization of Health Processes (150 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>45
</p>
<p>    
</p>
<p>    SOTA learning is a modifi ed hierarchical cluster analysis, and it uses in this 
</p>
<p>example the between-case distances of fi brinogen as variable. On the y-axis the 
</p>
<p>standardized distances of the cluster combinations. Clicking the small squares inter-
</p>
<p>actively demonstrates the row numbers of the individual cases. It can be observed at 
</p>
<p>the bottom of the fi gure that the severity classes very well cluster, with the mild 
</p>
<p>cases (red) left, medium severity (green) in the middle, and severe cases (blue) right.  
</p>
<p>    Conclusion 
</p>
<p> Clinical computer fi les are complex, and hard to statistically test. Instead, visualiza-
</p>
<p>tion processes can be successfully used as an alternative approach to traditional 
</p>
<p>statistical data analysis. For example, Knime (Konstanz information miner) soft-
</p>
<p>ware developed by computer scientists at Konstanz University Technical Department 
</p>
<p>at the Bodensee, although mainly used by chemists and pharmacists, is able to visu-
</p>
<p>alize multidimensional clinical data, and this approach may, sometimes, perform 
</p>
<p>better than traditional statistical testing. In the current example it was able to dem-
</p>
<p>onstrate the clustering of infl ammatory markers to identify different classes of pneu-
</p>
<p>monia severity. Also to demonstrate that leucocyte count and fi brinogen were the 
</p>
<p>best markers, and that ESR was a poor marker. In all of the markers the best predic-
</p>
<p>tive performance was obtained in the severest cases of disease. All of these observa-
</p>
<p>tions were unobserved in the traditional statistical analysis in SPSS.  
</p>
<p>Conclusion</p>
<p/>
</div>
<div class="page"><p/>
<p>46
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of splines and hierar-
</p>
<p>chical cluster modeling are in Machine learning in medicine part one, Chap. 11, 
</p>
<p>Non-linear modeling, pp 127&ndash;143, and Chap. 15, Hierarchical cluster analysis for 
</p>
<p>unsupervised data, pp 183&ndash;195, Springer Heidelberg Germany, from the same 
</p>
<p>authors.    
</p>
<p>7 Data Mining for Visualization of Health Processes (150 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>47&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_8
</p>
<p>    Chapter 8   
</p>
<p> Trained Decision Trees for a More Meaningful 
Accuracy (150 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> Traditionally, decision trees are used for fi nding the best predictors of health risks 
</p>
<p>and improvements (Chap.   53    ). However, this method is not entirely appropriate, 
</p>
<p>because a decision tree is built from a data fi le, and, subsequently, the same data fi le 
</p>
<p>is applied once more for computing the health risk probabilities from the built tree. 
</p>
<p>Obviously, the accuracy must be close to 100 %, because the test sample is 100 % 
</p>
<p>identical to the sample used for building the tree, and, therefore, this accuracy does 
</p>
<p>not mean too much. With neural networks this problem of duplicate usage of the 
</p>
<p>same data is solved by randomly splitting the data into two samples, a training 
</p>
<p>sample and a test sample (Chap. 12 in Machine learning in medicine part one, 
</p>
<p>pp 145&ndash;156, Artifi cial intelligence, multilayer perceptron modeling, Springer 
</p>
<p>Heidelberg Germany, 2013, from the same authors). The current chapter is to assess 
</p>
<p>whether the splitting methodology, otherwise called partitioning, is also feasible for 
</p>
<p>decision trees, and to assess its level of accuracy. Decision trees are both appropriate 
</p>
<p>for data with categorical and continuous outcome (Chap.   53    ).  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> Can infl ammatory markers adequately predict pneumonia severities wit the help of 
</p>
<p>a decision tree. Can partitioning of the data improve the methodology and is suffi -
</p>
<p>cient accuracy of the methodology maintained.  
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 3&rdquo; as Chap. 2, 
</p>
<p>2014. </p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_53">http://dx.doi.org/10.1007/978-3-319-15195-3_53</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_53">http://dx.doi.org/10.1007/978-3-319-15195-3_53</a></div>
</div>
<div class="page"><p/>
<p>48
</p>
<p>    Example 
</p>
<p> Four infl ammatory markers (CRP (C-reactive protein), ESR (erythrocyte sedimen-
</p>
<p>tation rate), leucocyte count (leucos), and fi brinogen) were measured in 150 patients. 
</p>
<p>Based on x-ray chest clinical severity was classifi ed as A (mild infection), B 
</p>
<p>(medium severity), C (severe infection). A major scientifi c question was to assess 
</p>
<p>what markers were the best predictors of the severity of infection. 
</p>
<p> CRP  leucos  fi brinogen  ESR  x-ray severity 
</p>
<p> 120,00  5,00  11,00  60,00  A 
</p>
<p> 100,00  5,00  11,00  56,00  A 
</p>
<p> 94,00  4,00  11,00  60,00  A 
</p>
<p> 92,00  5,00  11,00  58,00  A 
</p>
<p> 100,00  5,00  11,00  52,00  A 
</p>
<p> 108,00  6,00  17,00  48,00  A 
</p>
<p> 92,00  5,00  14,00  48,00  A 
</p>
<p> 100,00  5,00  11,00  54,00  A 
</p>
<p> 88,00  5,00  11,00  54,00  A 
</p>
<p> 98,00  5,00  8,00  60,00  A 
</p>
<p> 108,00  5,00  11,00  68,00  A 
</p>
<p> 96,00  5,00  11,00  62,00  A 
</p>
<p> 96,00  5,00  8,00  46,00  A 
</p>
<p> 86,00  4,00  8,00  60,00  A 
</p>
<p> 116,00  4,00  11,00  50,00  A 
</p>
<p> 114,00  5,00  17,00  52,00  A 
</p>
<p>  CRP = C-reactive protein (mg/l) 
</p>
<p> leucos = leucyte count (*10 9 /l) 
</p>
<p> fi brinogen = fi brinogen level (mg/l) 
</p>
<p> ESR = erythrocyte sedimentation rate (mm) 
</p>
<p> x-ray severity = x-chest severity pneumonia score (A&ndash;C = mild to severe) 
</p>
<p>    The fi rst 16 patients are in the above table, the entire data fi le is in &ldquo;decisiontree&rdquo; 
</p>
<p>and can be obtained from &ldquo;extras.springer.com&rdquo; on the internet. We will start by 
</p>
<p>opening the data fi le in SPSS.
</p>
<p>  Command: 
</p>
<p>  click Classify&hellip;.Tree&hellip;.Dependent Variable: enter severity score&hellip;.Independent 
</p>
<p>Variables: enter CRP, Leucos, fi brinogen, ESR&hellip;.Growing Method: select 
</p>
<p>CHAID&hellip;.click Output: mark Tree in table format&hellip;.Criteria: Parent Node type 50, 
</p>
<p>Child Node type 15&hellip;.click Continue&hellip;. &hellip;.click OK.    
</p>
<p>8 Trained Decision Trees for a More Meaningful Accuracy (150 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>49
</p>
<p>  
</p>
<p>severity score
</p>
<p>fibrinogen mg/I
</p>
<p>Adj. P-value=0,000, Chi-
</p>
<p>square=258,700, df=6
</p>
<p>Node 0
</p>
<p>&lt;= 17,000 (17,000, 44,000] (44,000, 56,000] &gt; 56,000
</p>
<p>A 33,3
33,3
33,3
</p>
<p>100,0
</p>
<p>100,0 6,7 0,0 0,0
</p>
<p>2,280,0
</p>
<p>19,2 97,8
</p>
<p>30,7 46
</p>
<p>45
</p>
<p>1
</p>
<p>0
</p>
<p>17,3
</p>
<p>21
</p>
<p>5
</p>
<p>02
</p>
<p>28
</p>
<p>0
</p>
<p>93,3
</p>
<p>0,0
</p>
<p>0,0
</p>
<p>0,0
</p>
<p>0
</p>
<p>48
</p>
<p>4832,0 20,0 30 26
</p>
<p>0
</p>
<p>150Total
</p>
<p>Total Total Total Total
</p>
<p>50
50
50
</p>
<p>B
</p>
<p>C
</p>
<p>A
</p>
<p>B
</p>
<p>C
</p>
<p>A
</p>
<p>B
</p>
<p>C
</p>
<p>A
</p>
<p>B
</p>
<p>C
</p>
<p>A
</p>
<p>B
</p>
<p>C
</p>
<p>A
</p>
<p>B
</p>
<p>C
</p>
<p>Category % n
</p>
<p>Node 1
Category % n
</p>
<p>Node 2
Category % n
</p>
<p>Node 3
Category % n
</p>
<p>Node 4
Category % n
</p>
<p>  
</p>
<p>    The above decision tree is displayed. A fi brinogen level &lt;17 is 100 % predictor 
</p>
<p>of severity score A (mild disease). Fibrinogen 17&ndash;44 gives 93 % chance of severity 
</p>
<p>B, fi brinogen 44&ndash;56 gives 81 % chance of severity B, and fi brinogen &gt;56 gives 98 % 
</p>
<p>chance of severity score C. The output also shows that the overall accuracy of the 
</p>
<p>model is 94.7 %, but we have to account that this model is somewhat fl awed, because 
</p>
<p>all of the data are used twice, one, for building the tree, and, second, for using the 
</p>
<p>tree for making predictions.  
</p>
<p>    Downloading the Knime Data Miner 
</p>
<p> In Google enter the term &ldquo;knime&rdquo;. Click Download and follow instructions. After 
</p>
<p>completing the pretty easy download procedure, open the knime workbench by 
</p>
<p>clicking the knime welcome screen. The center of the screen displays the workfl ow 
</p>
<p>editor. Like the canvas in SPSS Modeler, it is empty., and can be used to build a 
</p>
<p>stream of nodes, called workfl ow in knime. The node repository is in the left lower 
</p>
<p>angle of the screen, and the nodes can be dragged to the workfl ow editor simply by 
</p>
<p>left-clicking. The nodes are computer tools for data analysis like visualization and 
</p>
<p>statistical processes. Node description is in the right upper angle of the screen. 
</p>
<p>Before the nodes can be used, they have to be connected with the &ldquo;fi le reader&rdquo; node, 
</p>
<p>and with one another by arrows, drawn, again, simply by left clicking the small tri-
</p>
<p>angles attached to the nodes. Right clicking on the fi le reader enables to confi gure 
</p>
<p>from your computer a requested data fi le....click Browse....and download from the 
</p>
<p>appropriate folder a csv type Excel fi le. You are set for analysis now. 
</p>
<p>Downloading the Knime Data Miner</p>
<p/>
</div>
<div class="page"><p/>
<p>50
</p>
<p> Note: the above data fi le cannot be read by the fi le reader, and must fi rst be saved 
</p>
<p>as csv type Excel fi le. For that purpose command in SPSS: click File....click Save 
</p>
<p>as....in &ldquo;Save as&rdquo; type: enter Comma Delimited (*.csv)....click Save. For your 
</p>
<p> convenience it has been made available in extras.springer.com, and entitled 
</p>
<p>&ldquo;decisiontree&rdquo;.  
</p>
<p>    Knime Workfl ow 
</p>
<p> A knime workfl ow for the analysis of the above data example is built, and the fi nal 
</p>
<p>result is shown in the underneath fi gure. 
</p>
<p>    
</p>
<p>    In the node repository click and type color....click the color manager node and 
</p>
<p>drag to workfl ow editor....in node repository click again color....click the Esc button 
</p>
<p>of your computer....in the node repository click again and type partitioning....the 
</p>
<p>partitioning node is displayed....drag it to the workfl ow editor....perform the same 
</p>
<p>actions and type respectively Decision Tree Learner, Decision Tree Predictor, and 
</p>
<p>Scorer....Connect, by left clicking, all of the nodes with arrows as indicated above....
</p>
<p>Confi gurate and execute all of the nodes by right clicking the nodes and then the 
</p>
<p>texts &ldquo;Confi gurate&rdquo; and &ldquo;Execute&rdquo;....the red lights will successively turn orange and 
</p>
<p>then green....right click the Decision Tree Predictor again....right click the text 
</p>
<p>&ldquo;View: Decision Tree View&rdquo;. 
</p>
<p> The underneath decision tree comes up. It is pretty much similar to the above 
</p>
<p>SPSS tree, although it does not use 150 cases but only 45 cases (the test sample from 
</p>
<p>which 100 were resampled). Fibrinogen is again the best predictor. A level &lt;29 mg/l 
</p>
<p>gives you 100 % chance of severity score A. A level 29&ndash;57.5 gives 92.1 % chance 
</p>
<p>of Severity B, and a level over 57.5 gives 100 % chance of severity C. 
</p>
<p> Right clicking the scorer node gives you the accuracy statistics, and shows that 
</p>
<p>the sensitivity of A, B, an C are respectively 100, 93.3, and 90.5 %, and that the 
</p>
<p>overall accuracy is 94 %, slightly less than that of the SPSS tree (94.7 %), but still 
</p>
<p>pretty good. In addition, the current analysis is appropriate, and does not use identi-
</p>
<p>cal data twice. 
</p>
<p>8 Trained Decision Trees for a More Meaningful Accuracy (150 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>51
</p>
<p>    
</p>
<p>Knime Workfl ow</p>
<p/>
</div>
<div class="page"><p/>
<p>52
</p>
<p>        Conclusion 
</p>
<p> Traditionally, decision trees are used for fi nding the best predictors of health risks 
</p>
<p>and improvements. However, this method is not entirely appropriate, because a 
</p>
<p>decision tree is built from a data fi le, and, subsequently, the same data fi le is applied 
</p>
<p>once more for computing the health risk probabilities from the built tree. Obviously, 
</p>
<p>the accuracy must be close to 100 %, because the test sample is 100 % identical to 
</p>
<p>the sample used for building the tree, and, therefore, this accuracy does not mean 
</p>
<p>too much. A decision tree with partitioning of a training and a test sample provides 
</p>
<p>similar results, but is scientifi cally less fl awed, because each datum is used only 
</p>
<p>once. In spite of this, little accuracy is lost.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of decision trees and 
</p>
<p>neural networks are in Chap.   53    , and in Machine learning in medicine part one, 
</p>
<p>Chap. 12, pp 145&ndash;156, Artifi cial intelligence, multilayer perceptron modeling, 
</p>
<p>Springer Heidelberg Germany, 2013, both from the same authors.    
</p>
<p>8 Trained Decision Trees for a More Meaningful Accuracy (150 Patients)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_53">http://dx.doi.org/10.1007/978-3-319-15195-3_53</a></div>
</div>
<div class="page"><p/>
<p>53&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_9
</p>
<p>    Chapter 9   
</p>
<p> Typology of Medical Data (51 Patients) 
</p>
<p>                       General Purpose 
</p>
<p> Apart from histograms (see Chap. 1, Statistics applied to clinical studies 5th edition, 
</p>
<p>&ldquo;Hypotheses, data, stratifi cation&rdquo;. pp 1&ndash;14, Springer Heidelberg Germany, 2012), 
</p>
<p>and Q-Q plots (Chap.   42     of current work), the typology of data and frequency 
</p>
<p> procedures (to be reviewed in the Chaps.   10    , and   11     of the current work) are a good 
</p>
<p>way to start looking at your data. First, we will address the typology of the data.
</p>
<p>  Nominal Data 
</p>
<p>  Nominal data are discrete data without a stepping pattern, like genders, age 
</p>
<p>classes, family names. They can be assessed with pie charts, frequency tables 
</p>
<p>and bar charts.   
</p>
<p>  Ordinal Data 
</p>
<p>  Ordinal data are also discrete data, however, with a stepping pattern, like severity 
</p>
<p>scores, intelligence levels, physical strength scores. They are usually assessed 
</p>
<p>with frequency tables and bar charts.   
</p>
<p>  Scale Data 
</p>
<p>  Scale data also have a stepping pattern, but, unlike ordinal data, they have steps 
</p>
<p>with equal intervals. With small steps they are called continuous data. They are 
</p>
<p>sometimes called quantitative data, while nominal and ordinal data are tradition-
</p>
<p>ally called qualitative data. The scale data are assessed with summary tables and 
</p>
<p>histograms.    
</p>
<p> The typology of the data values become particularly important when it comes to 
</p>
<p>statistical analyses. E.g., means and standard deviations makes no sense with nomi-
</p>
<p>nal data. The problem with ordinal data is that the steps are usually not equal, like 
</p>
<p>with scale data. With ordinal data you will usually have a mix-up of larger and 
</p>
<p>smaller steps. This biases the outcome if you use a scale data test for their analysis. </p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_42">http://dx.doi.org/10.1007/978-3-319-15195-3_42</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_10">http://dx.doi.org/10.1007/978-3-319-15195-3_10</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_11">http://dx.doi.org/10.1007/978-3-319-15195-3_11</a></div>
</div>
<div class="page"><p/>
<p>54
</p>
<p>The Chap.   37     of the current book, entitled &ldquo;Ordinal scaling for clinical scores with 
</p>
<p>inconsistent intervals&rdquo;, shows how this problem can mathematically be largely 
</p>
<p>solved by complementary log-log transformations.  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> In econometrics and marketing research (Foroni, Econometric models for mixed- 
</p>
<p>frequency data, edited by European University of Economics, Florence, 2012), fre-
</p>
<p>quency procedures are routinely used for the assessment of nominal, ordinal and 
</p>
<p>scale data. Can they also be adequately applied for assessing medical data?  
</p>
<p>    Example 
</p>
<p> The patients of an internist&rsquo;s outpatient clinic are reviewed. 
</p>
<p> nominal variable  ordinal variable  scale variable 
</p>
<p> agegroup  severity  time 
</p>
<p> 2,00  2,00  2,50 
</p>
<p> 2,00  1,00  6,00 
</p>
<p> 2,00  1,00  2,50 
</p>
<p> 1,00  3,00  2,00 
</p>
<p> 1,00  1,00  5,00 
</p>
<p> 2,00  1,00  4,00 
</p>
<p> 2,00  3,00  ,50 
</p>
<p> 1,00  1,00  2,50 
</p>
<p> 2,00  3,00  4,00 
</p>
<p> 2,00  2,00  1,50 
</p>
<p>  agegroup: 1 = senior, 2 = adult, 3 = adolescent, 4 = child 
</p>
<p> severity: complaint severity scores 1&ndash;4 
</p>
<p> time: consulting time (minutes) 
</p>
<p>    The fi rst 10 patients are in the above table. The entire fi le (51 patients) is entitled 
</p>
<p>&ldquo;frequencies&rdquo;, and is available at extras.springer.com. We will start by opening the 
</p>
<p>fi le in SPSS statistical software. 
</p>
<p>9 Typology of Medical Data (51 Patients)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_37">http://dx.doi.org/10.1007/978-3-319-15195-3_37</a></div>
</div>
<div class="page"><p/>
<p>55
</p>
<p>    Nominal Variable 
</p>
<p>   Command: 
</p>
<p>  click Analyze....Descriptive Statistics....Frequencies....Variable(s): enter  agegroups....
</p>
<p>mark Display frequency tables....click Charts....click Pie charts....click OK.    
</p>
<p> The underneath pie chart shows that seniors and adults predominate and that 
</p>
<p>children are just a small portion of the outpatient clinic population. 
</p>
<p>  
</p>
<p>age group
</p>
<p>senior
</p>
<p>adult
</p>
<p>adolescent
</p>
<p>child
</p>
<p>  
</p>
<p>    The frequency table shows precise frequencies of the nominal categories.
</p>
<p> Age group 
</p>
<p> Frequency  Percent  Valid percent  Cumulative percent 
</p>
<p> Valid  Senior  17  33,3  33,3  33,3 
</p>
<p> Adult  20  39,2  39,2  72,5 
</p>
<p> Adolescent  11  21,6  21,6  94,1 
</p>
<p> Child  3  5,9  5,9  100,0 
</p>
<p> Total  51  100,0  100,0 
</p>
<p>   If you wish, you could present your data in the form of descending or ascending 
</p>
<p>frequencies.
</p>
<p>  Command: 
</p>
<p>  click Analyze....Descriptive Statistics....Frequencies....Variable(s): enter agegroups 
</p>
<p>....mark Display frequency tables....click Charts....click Bar charts....click Continue 
</p>
<p>....click Format....click Descending counts....click Continue.......click OK.    
</p>
<p> Example</p>
<p/>
</div>
<div class="page"><p/>
<p>56
</p>
<p> The underneath graph is in the output sheet. It shows an ordered bar chart with 
</p>
<p>adults as largest category. 
</p>
<p>  
</p>
<p>20
</p>
<p>15
</p>
<p>10
</p>
<p>5
</p>
<p>0
</p>
<p>adult senior adolescent
</p>
<p>age group
</p>
<p>age group
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>e
n
</p>
<p>c
y
</p>
<p>child
</p>
<p>  
</p>
<p>        Ordinal Variable 
</p>
<p>   Command: 
</p>
<p>  click Analyze....Descriptive Statistics....Frequencies....Variable(s): enter severity ....
</p>
<p>mark Display frequency tables....click Charts....click Bar charts....click Continue ....
</p>
<p>click Format....click Ascending counts....click Continue.......click OK.    
</p>
<p> According to the severity score count the underneath graph shows the percent-
</p>
<p>ages of patients. Most of them are in the score one category, least of them in the 
</p>
<p>score fi ve category. 
</p>
<p>9 Typology of Medical Data (51 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>57
</p>
<p>  
</p>
<p>25
</p>
<p>20
</p>
<p>15
</p>
<p>10
</p>
<p>5
</p>
<p>0
</p>
<p>1,00 2,00 3,00
</p>
<p>complaint severity score
</p>
<p>complaint severity score
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>e
n
</p>
<p>c
y
</p>
<p>4,00 5,00
</p>
<p>  
</p>
<p>    The table gives the precise numbers of patients in each category as well as the 
</p>
<p>percentages. If we have missing values, the valid percent column will give the 
</p>
<p>adjusted percentages, while the cumulative percentage gives the categories one and 
</p>
<p>two, one and two and three etc. percentages.
</p>
<p> Complaint severity score 
</p>
<p> Frequency  Percent  Valid percent  Cumulative percent 
</p>
<p> Valid  1,00  23  45,1  45,1  45,1 
</p>
<p> 2,00  12  23,5  23,5  68,6 
</p>
<p> 3,00  11  21,6  21,6  90,2 
</p>
<p> 4,00  3  5,9  5,9  96,1 
</p>
<p> 5,00  2  3,9  3,9  100,0 
</p>
<p> Total  51  100,0  100,0 
</p>
<p>       Scale Variable 
</p>
<p>   Command: 
</p>
<p>  click Analyze....Descriptive Statistics....Frequencies....Variable(s): enter time ....
</p>
<p>remove mark from "Display frequency tables"....click Statistics....mark Quartiles....
</p>
<p>Std.deviations....Minimum....Maximum.... Mean....Median .... Skewness....
</p>
<p>Kurtosis....click Continue....then click Charts....Histograms&hellip;mark Show normal 
</p>
<p>curve on histogram....click Continue....click OK.    
</p>
<p> Example</p>
<p/>
</div>
<div class="page"><p/>
<p>58
</p>
<p> The statistics table tells us that the consulting time is 3,42 min on average, and 
</p>
<p>50 % of the consults are between 2 and 4 min. The most extreme consults took 0,5 
</p>
<p>and 15,0 min.
</p>
<p> Statistics 
</p>
<p> consulting time (min) 
</p>
<p> N  Valid  51 
</p>
<p> Missing  0 
</p>
<p> Mean  3,4216 
</p>
<p> Median  2,5000 
</p>
<p> Std. Deviation  2,99395 
</p>
<p> Skewness  2,326 
</p>
<p> Std. Error of Skewness  ,333 
</p>
<p> Kurtosis  5,854 
</p>
<p> Std. Error of Kurtosis  ,656 
</p>
<p> Minimum  ,50 
</p>
<p> Maximum  15,00 
</p>
<p> Percentiles  25  2,0000 
</p>
<p> 50  2,5000 
</p>
<p> 75  4,0000 
</p>
<p>   The histogram shows the frequency distribution of the data and suggests skew-
</p>
<p>ness to the right. Most of the consults took as little as less than 5 min but some took 
</p>
<p>no less than 5&ndash;15 min. This means that the data are not very symmetric and means, 
</p>
<p>and that standard deviation are not very accurate to summarize these data. 
</p>
<p>  
</p>
<p>15
</p>
<p>Histogram
</p>
<p>consulting time (min)
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>e
n
</p>
<p>c
y
</p>
<p>10
</p>
<p>5
</p>
<p>0
</p>
<p>,00 5,00 10,00 15,00 20,00
</p>
<p>Mean = 3,42
</p>
<p>Std. Dev. = 2,994
</p>
<p>N = 51
</p>
<p>  
</p>
<p>    Indeed, a signifi cant level of skewness to the right is in the data, because 
</p>
<p>2,326/0,333 = 6,985 is much larger than 1,96 (see the above table). We will try and 
</p>
<p>9 Typology of Medical Data (51 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>59
</p>
<p>use a logarithmic transformation of these skewed data. because this often 
</p>
<p> &ldquo;normalizes&rdquo; the skewness.
</p>
<p>  Command: 
</p>
<p>  click Transform....Compute Variable....type logtime in Target Variable....type 
</p>
<p>ln(time) in Numeric Expression....click OK.    
</p>
<p> In the main screen it can be observed that SPSS now has produced a novel 
</p>
<p> variable entitled &ldquo;logtime&rdquo;. We will perform the scale variable analysis again, and 
</p>
<p>replace the variable &ldquo;time&rdquo; with &ldquo;logtime&rdquo;.
</p>
<p>  Command: 
</p>
<p>  click Analyze....Descriptive Statistics....Frequencies....Variable(s): enter logtime....
</p>
<p>click Charts....Histograms&hellip;mark Show normal curve on histogram....click 
</p>
<p>Continue....click OK.    
</p>
<p>10
</p>
<p>8
</p>
<p>6
</p>
<p>4
</p>
<p>2
</p>
<p>0
-1,00 ,00 1,00
</p>
<p>logtime
</p>
<p>Histogram
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>e
n
</p>
<p>c
y
</p>
<p>2,00 3,00
</p>
<p>Mean = ,94
</p>
<p>Std. Dev. = ,766
</p>
<p>N = 51
</p>
<p> In the output sheets the underneath graph is shown. The data distribution looks 
</p>
<p>less skewed and much closer to a normal distribution now. The logtime data can 
</p>
<p>now be used for data analysis using normal statistical tests.   
</p>
<p>    Conclusion 
</p>
<p> Data can be classifi ed as nominal, ordinal and scale. For each type frequencies and 
</p>
<p>frequency distributions can readily be calculated, and they enable an unbiased view 
</p>
<p>of their patterns. Nominal data have no mean value. Ordinal data are tricky, because, 
</p>
<p>Conclusion</p>
<p/>
</div>
<div class="page"><p/>
<p>60
</p>
<p>although they have a stepping pattern, they offer a mix-up of larger and smaller 
</p>
<p>steps. Ordinal regression can largely adjust this irregularity. Skewed scale data often 
</p>
<p>benefi t from log-data transformations.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of ordinal data is given 
</p>
<p>in Chap.   37     of the current book, entitled &ldquo;Ordinal scaling for clinical scores with 
</p>
<p>inconsistent intervals&rdquo;.    
</p>
<p>9 Typology of Medical Data (51 Patients)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_37">http://dx.doi.org/10.1007/978-3-319-15195-3_37</a></div>
</div>
<div class="page"><p/>
<p>61&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_10
</p>
<p>    Chapter 10   
</p>
<p> Predictions from Nominal Clinical Data 
</p>
<p>(450 Patients) 
</p>
<p>                       General Purpose 
</p>
<p> In Chap.   9     the typology of medical data was reviewed. Nominal data are discrete 
</p>
<p>data without a stepping function like genders, age classes, family names. They can 
</p>
<p>be assessed with pie charts, frequency tables and bar charts. Statistical testing is not 
</p>
<p>of much interest. Statistical testing becomes, however, interesting, if we want to 
</p>
<p>know whether two nominal variables like treatment modality and treatment  outcome 
</p>
<p>are differently distributed between one another. An interaction matrix of these two 
</p>
<p>nominal variables could, then, be used to test, whether one treatment performs 
</p>
<p> better than the other.  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> This chapter assesses the relationship between four treatment modalities, and, as 
</p>
<p>outcome, fi ve levels of quality of life (qol). Can an interaction matrix, otherwise 
</p>
<p>called contingency table or crosstab, be used to assess whether some treatment 
</p>
<p>modalities are associated with a better qol score than others, and to assess the 
</p>
<p> directions of the differences in distribution of the variables.  
</p>
<p>    Example 
</p>
<p> In 450 patients with coronary artery disease four complementary treatment modali-
</p>
<p>ties, including cardiac fi tness, physiotherapy, wellness, and hydrotherapy, were 
</p>
<p>assessed for quality of life scores. The fi rst 10 patients are in the table underneath. </p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_9">http://dx.doi.org/10.1007/978-3-319-15195-3_9</a></div>
</div>
<div class="page"><p/>
<p>62
</p>
<p>The entire data fi le is entitled &ldquo;Qol.sav&rdquo;, and is in extras.springer.com. The example 
</p>
<p>is also used in the Chap.   11    . SPSS is applied for analysis. 
</p>
<p>    Start by opening the data fi le in SPSS statistical software.
</p>
<p>  Command 
</p>
<p>  Analyze&hellip;.Descriptive Statistics&hellip;.Crosstabs&hellip;.Rows: enter &ldquo;treatment&rdquo;&hellip;. Columns: 
</p>
<p>enter &ldquo;qol score&rdquo;&hellip;.click Statistics&hellip;.mark Chi-square&hellip;.click Continue&hellip;.click OK.    
</p>
<p> In the output sheets the underneath tables are given.
</p>
<p> Treatment * qol score crosstabulation 
</p>
<p> Count 
</p>
<p> Qol score 
</p>
<p> Total  Very low  Low  Medium  High  Very high 
</p>
<p> Treatment  Cardiac fi tness  21  21  16  24  36  118 
</p>
<p> Physiotherapy  22  20  18  20  20  100 
</p>
<p> Wellness  23  14  12  30  25  104 
</p>
<p> Hydrotherapy  20  18  25  35  30  128 
</p>
<p> Total  86  73  71  109  111  450 
</p>
<p>   Both hydrotherapy and cardiac fi tness produce highest qol scores.
</p>
<p> treatment  counseling  qol  sat doctor 
</p>
<p> 3  1  4  4 
</p>
<p> 4  0  2  1 
</p>
<p> 2  1  5  4 
</p>
<p> 3  0  4  4 
</p>
<p> 2  1  2  1 
</p>
<p> 2  0  1  4 
</p>
<p> 4  0  4  1 
</p>
<p> 3  0  4  1 
</p>
<p> 4  1  4  4 
</p>
<p> 2  1  3  4 
</p>
<p>  treatment = treatment modality (1 = cardiac fi tness, 
</p>
<p>2 = physiotherapy, 3 = wellness, 4 = hydrotherapy, 
</p>
<p>5 = nothing) 
</p>
<p> counseling = counseling given (0 = no, 1 = yes) 
</p>
<p> qol = quality of life score (1 = very low, 5 = vey high) 
</p>
<p> sat doctor = satisfaction with doctor (1 = very low, 
</p>
<p>5 = very high)  
</p>
<p>10 Predictions from Nominal Clinical Data (450 Patients)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_11">http://dx.doi.org/10.1007/978-3-319-15195-3_11</a></div>
</div>
<div class="page"><p/>
<p>63
</p>
<p> Chi-Square tests 
</p>
<p> Value  df  Asymp. Sig. (2-sided) 
</p>
<p> Pearson Chi-Square  12,288 a   12  ,423 
</p>
<p> Likelihood ratio  12,291  12  ,423 
</p>
<p> Linear-by-Linear Association  ,170  1  ,680 
</p>
<p> N of valid cases  450 
</p>
<p>   a 0 cells (,0 %) have expected countless than 5. The minimum expected count is 15,78 
</p>
<p>    However, the cells are not signifi cantly different from one another, and so the 
</p>
<p>result is due to chance. We have clinical arguments that counseling may support the 
</p>
<p>benefi cial effects of treatments, and, therefore, perform an analysis with two layers, 
</p>
<p>one in the patients with and one in those without counseling.
</p>
<p>  Command 
</p>
<p>  Analyze&hellip;.Descriptive Statistics&hellip;.Crosstabs&hellip;.Rows: enter &ldquo;treatment&rdquo;&hellip;. Columns: 
</p>
<p>enter &ldquo;qol score&rdquo;&hellip;.Layer 1 of 1: enter &ldquo;counseling&rdquo;&hellip;.click Statistics &hellip;.mark 
</p>
<p> Chi-square&hellip;.mark Contingency coeffi cient&hellip;.mark Phi and Cramer&rsquo;s V&hellip;.mark 
</p>
<p>Lambda&hellip;.mark Uncertainty coeffi cient&hellip;.click Continue&hellip;.click OK.    
</p>
<p> The underneath tables are in the output sheets.
</p>
<p> Treatment * qol score * counseling crosstabulation 
</p>
<p> Count 
</p>
<p> Counseling 
</p>
<p> Qol score 
</p>
<p> Total 
</p>
<p> Very 
</p>
<p>low  Low  Medium  High 
</p>
<p> Very 
</p>
<p>high 
</p>
<p> No  Treatment  Cardiac fi tness  19  16  8  8  14  65 
</p>
<p> Physiotherapy  8  8  7  7  15  45 
</p>
<p> Wellness  23  8  6  15  9  61 
</p>
<p> Hydrotherapy  15  14  9  10  11  59 
</p>
<p> Total  65  46  30  40  49  230 
</p>
<p> Yes  Treatment  Cardiac fi tness  2  5  8  16  22  53 
</p>
<p> Physiotherapy  14  12  11  13  5  55 
</p>
<p> Wellness  0  6  6  15  16  43 
</p>
<p> Hydrotherapy  5  4  16  25  19  69 
</p>
<p> Total  21  27  41  69  62  220 
</p>
<p>    Chi-Square tests 
</p>
<p> Counseling  Value  df  Asymp. Sig. (2-sided) 
</p>
<p> No  Pearson Chi-Square  14,831  12  ,251 
</p>
<p> Likelihood ratio  14,688  12  ,259 
</p>
<p> Linear-by-Linear Association  ,093  1  ,760 
</p>
<p> N of valid cases  230 
</p>
<p>(continued)
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>64
</p>
<p>    Chi-Square tests 
</p>
<p> Counseling  Value  df  Asymp. Sig. (2-sided) 
</p>
<p> Yes  Pearson Chi-Square  42,961  12  ,000 
</p>
<p> Likelihood ratio  44,981  12  ,000 
</p>
<p> Linear-by-Linear Association  ,517  1  ,472 
</p>
<p> N of valid cases  220 
</p>
<p>   Obviously, if we assess the subjects who received counseling, then the high 
</p>
<p>scores appear to appear very signifi cantly more often in the hydrotherapy and car-
</p>
<p>diac fi tness patients than in the physiotherapy and wellness groups.
</p>
<p> Symmetric measures 
</p>
<p> Counseling  Value  Approx. Sig. 
</p>
<p> No  Nominal by nominal  Phi  ,254  ,251 
</p>
<p> Cramer&rsquo;s V  ,147  ,251 
</p>
<p> Contingency coeffi cient  ,246  ,251 
</p>
<p> N of Valid Cases  230 
</p>
<p> Yes  Nominal by nominal  Phi  ,442  ,000 
</p>
<p> Cramer&rsquo;s V  ,255  ,000 
</p>
<p> Contingency coeffi cient  ,404  ,000 
</p>
<p> N of valid cases  220 
</p>
<p>   Also the phi value, which is the ratio of the computed Pearson chi-square value 
</p>
<p>and the number of observations, are statistically signifi cant. They support that the 
</p>
<p>differences observed in the yes-counseling group are real fi ndings, not chance fi nd-
</p>
<p>ings. Cramer&rsquo;s V and contingency coeffi cient are rescaled phi values, and further-
</p>
<p>more support this conclusion.
</p>
<p>    Directional measures 
</p>
<p> Counseling  Value 
</p>
<p> Asymp. 
</p>
<p>Std. Error 
</p>
<p> Approx 
</p>
<p>T 
</p>
<p> Approx 
</p>
<p>Sig. 
</p>
<p> No  Nominal by 
</p>
<p>nominal 
</p>
<p> Lambda  Symmetric  ,061  ,038  1,570  ,116 
</p>
<p> Treatment 
</p>
<p>dependent 
</p>
<p> ,079  ,061  1,238  ,216 
</p>
<p> Qol score 
</p>
<p>dependent 
</p>
<p> ,042  ,028  1,466  ,143 
</p>
<p> Goodman and 
</p>
<p>Kruskal tau 
</p>
<p> Treatment 
</p>
<p>dependent 
</p>
<p> ,021  ,011  ,277 
</p>
<p> Qol score 
</p>
<p>dependent 
</p>
<p> ,018  ,009  ,182 
</p>
<p> Uncertainty 
</p>
<p>coeffi cient 
</p>
<p> Symmetric  ,022  ,011  1,933  ,259 
</p>
<p> Treatment 
</p>
<p>dependent 
</p>
<p> ,023  ,012  1,933  ,259 
</p>
<p> Qol score 
</p>
<p>dependent 
</p>
<p> ,020  ,010  1,933  ,259 
</p>
<p>(continued)
</p>
<p>10 Predictions from Nominal Clinical Data (450 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>65
</p>
<p>    Directional measures 
</p>
<p> Counseling  Value 
</p>
<p> Asymp. 
</p>
<p>Std. Error 
</p>
<p> Approx 
</p>
<p>T 
</p>
<p> Approx 
</p>
<p>Sig. 
</p>
<p> Yes  Nominal by 
</p>
<p>nominal 
</p>
<p> Lambda  Symmetric  ,093  ,050  1,806  ,071 
</p>
<p> Treatment 
</p>
<p>dependent 
</p>
<p> ,132  ,054  2,322  ,020 
</p>
<p> Qol score 
</p>
<p>dependent 
</p>
<p> ,053  ,063  ,818  ,414 
</p>
<p> Goodman and 
</p>
<p>Kruskal tau 
</p>
<p> Treatment 
</p>
<p>dependent 
</p>
<p> ,065  ,019  ,000 
</p>
<p> Qol score 
</p>
<p>dependent 
</p>
<p> ,042  ,013  ,000 
</p>
<p> Uncertainty 
</p>
<p>coeffi cient 
</p>
<p> Symmetric  ,071  ,018  3,839  ,000 
</p>
<p> Treatment 
</p>
<p>dependent 
</p>
<p> ,074  ,019  3,839  ,000 
</p>
<p> Qol score 
</p>
<p>dependent 
</p>
<p> ,067  ,017  3,839  ,000 
</p>
<p>   The lambda value is also given. It shows the percentages of misclassifi cations in 
</p>
<p>the row if you would know the column values, is also statistically signifi cant in the 
</p>
<p> yes- counseling subgroup at p = 0.020. The value of 0.132 would mean 1.32 % reduc-
</p>
<p>tion of misclassifi cation, which is, however, not very much. Goodman and uncer-
</p>
<p>tainty coeffi cients serve similar purpose and are also statistically signifi cant.  
</p>
<p>    Conclusion 
</p>
<p> In conclusion, many high qol levels are in the hydrotherapy and physiotherapy 
</p>
<p>groups, and, correspondingly, very few low qol levels are a major factor for the 
</p>
<p>overall result of this study assessing the effects of treatment modalities on qol 
</p>
<p>scores. The interaction matrix can be used to assess whether some treatment modal-
</p>
<p>ities are associated with a better qol score than others, and to assess the directions 
</p>
<p>of the differences in distribution of the variables.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of crosstabs is given in 
</p>
<p>Statistics applied to clinical studies 5th edition, Chap. 3, The analysis of safety data, 
</p>
<p>pp 41&ndash;59, Edited by Springer Heidelberg Germany, 2012, from the same authors.    
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>67&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_11
</p>
<p>    Chapter 11   
</p>
<p> Predictions from Ordinal Clinical Data 
</p>
<p>(450 Patients) 
</p>
<p>                       General Purpose 
</p>
<p> In Chap.   9     the typology of medical data was reviewed. Ordinal data are, like nominal 
</p>
<p>data (Chap.   10    ), discrete data, however, with a stepping pattern, like severity scores, 
</p>
<p>intelligence levels, physical strength scores. They are usually assessed with frequency 
</p>
<p>tables and bar charts. Unlike scale data, that also have a stepping pattern, they do not 
</p>
<p>necessarily have to have steps with equal intervals. Statistical testing is not of much 
</p>
<p>interest. Statistical testing becomes, however, interesting, if we want to know 
</p>
<p>whether two ordinal variables like levels of satisfaction with treatment and treat-
</p>
<p>ment outcome are differently distributed between one another. An interaction matrix 
</p>
<p>of these two ordinal variables could then be used to test whether one treatment level 
</p>
<p>performs better than the other. We should add that sometimes an ordinal variable 
</p>
<p>can very well be analyzed as a nominal one (e.g., treatment outcome in the current 
</p>
<p>Chap. and in Chap.   10    ).  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> This chapter assesses the relationship between fi ve levels of satisfaction with the 
</p>
<p>treating doctor, and, as outcome, fi ve levels of quality of life (qol). Can an interac-
</p>
<p>tion matrix, otherwise called contingency table or crosstab, be used to assess 
</p>
<p>whether some &ldquo;satisfaction-with-treating-doctor&rdquo; levels are associated with a better 
</p>
<p>qol score than others, and to assess the directions of the differences in distribution 
</p>
<p>of the variables.  </p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_9">http://dx.doi.org/10.1007/978-3-319-15195-3_9</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_10">http://dx.doi.org/10.1007/978-3-319-15195-3_10</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_10">http://dx.doi.org/10.1007/978-3-319-15195-3_10</a></div>
</div>
<div class="page"><p/>
<p>68
</p>
<p>    Example 
</p>
<p> In 450 patients with coronary artery disease the satisfaction level of patients with 
</p>
<p>their doctor was assumed to be an important predictor of patient qol (quality of life). 
</p>
<p>    The above table gives the fi rst 10 patients of a 450 patients study of the effects of 
</p>
<p>doctors&rsquo; satisfaction level and qol. The data are also used in the Chap.   16    . The entire 
</p>
<p>data fi le is in extras.springer.com and is entitled &ldquo;qol.sav&rdquo;. 
</p>
<p> SPSS is used for analysis.
</p>
<p>  Command 
</p>
<p>  Analyze&hellip;.Descriptive Statistics&hellip;.Crosstabs&hellip;.Rows: enter &ldquo;sat doctor&rdquo;&hellip;. 
</p>
<p>Columns: enter &ldquo;qol score&rdquo;&hellip;.click Statistics&hellip;.mark Gamma, Somer&rsquo;s d, Kendall&rsquo;s 
</p>
<p>tau-b, Kendall&rsquo;s tau-c&hellip;.click Continue&hellip;.click OK.   
</p>
<p> Sat with doctor * qol score crosstabulation 
</p>
<p> Count 
</p>
<p> Qol score 
</p>
<p> Total  Very low  Low  Medium  High  Very high 
</p>
<p> Sat with doctor  Very low  11  12  12  11  4  50 
</p>
<p> Low  24  16  23  28  15  106 
</p>
<p> Medium  21  23  17  22  27  110 
</p>
<p> High  18  16  15  32  36  117 
</p>
<p> Very high  12  6  4  16  29  67 
</p>
<p> Total  86  73  71  109  111  450 
</p>
<p>   The above matrix of observed counts is shown in the output sheets. Very high qol 
</p>
<p>was frequently observed in patients who were very satisfi ed with their doctor, while 
</p>
<p> treatment  counseling  qol  sat doctor 
</p>
<p> 3  1  4  4 
</p>
<p> 4  0  2  1 
</p>
<p> 2  1  5  4 
</p>
<p> 3  0  4  4 
</p>
<p> 2  1  2  1 
</p>
<p> 2  0  1  4 
</p>
<p> 4  0  4  1 
</p>
<p> 3  0  4  1 
</p>
<p> 4  1  4  4 
</p>
<p> 2  1  3  4 
</p>
<p>  Treatment = treatment modality (1 = cardiac fi tness, 2 = phys-
</p>
<p>iotherapy, 3 = wellness, 4 = hydrotherapy, 5 = nothing) 
</p>
<p> counseling = counseling given (0 = no, 1 = yes) 
</p>
<p> qol = quality of life score (1 = very low, 5 = vey high) 
</p>
<p> sat doctor = satisfaction with doctor (1 = very low, 
</p>
<p>5 = very high)  
</p>
<p>11 Predictions from Ordinal Clinical Data (450 Patients)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_16">http://dx.doi.org/10.1007/978-3-319-15195-3_16</a></div>
</div>
<div class="page"><p/>
<p>69
</p>
<p>few patients with very high qol (only 4) had a very low satisfaction with their doctor. 
</p>
<p>We wish to assess whether this association is chance or statistically signifi cant. 
</p>
<p> &ldquo;Ordinal x ordinal crosstabs&rdquo; work differently from &ldquo;nominal x nominal cross-
</p>
<p>tabs&rdquo; (Chap.   16    ). The latter compares the magnitude of the cells, the former 
</p>
<p> compares the magnitude of the concordant and those of the discordant cells, whereby 
</p>
<p>the concordant cells are, e.g., &ldquo;very low versus very low&rdquo;, &ldquo;low versus low&rdquo;, etc.
</p>
<p> Directional measures 
</p>
<p> Value 
</p>
<p> Asymp. Std. 
</p>
<p>Error a   Approx T b  
</p>
<p> Approx 
</p>
<p>Sig. 
</p>
<p> Ordinal by 
</p>
<p>ordinal 
</p>
<p> Somers&rsquo; d  Symmetric  ,178  ,037  4,817  ,000 
</p>
<p> Sat with doctor 
</p>
<p>dependent 
</p>
<p> ,177  ,037  4,817  ,000 
</p>
<p> Qol score 
</p>
<p>dependent 
</p>
<p> ,179  ,037  4,817  ,000 
</p>
<p>   a Not assuming the null hypothesis 
</p>
<p>  b Using the asymptotic standard error assuming the null hypothesis 
</p>
<p> Symmetric measures 
</p>
<p> Value 
</p>
<p> Asymp. Std. 
</p>
<p>Error a   Approx. T b   Approx Sig. 
</p>
<p> Ordinal by ordinal  Kendall&rsquo;s tau-b  ,178  ,037  4,817  ,000 
</p>
<p> Kendall&rsquo;s tau-c  ,175  ,036  4,817  ,000 
</p>
<p> Gamma  ,225  ,046  4,817  ,000 
</p>
<p> N of valid cases  450 
</p>
<p>   a Not assuming the null hypothesis 
</p>
<p>  b Using the asymptotic standard error assuming the null hypothesis 
</p>
<p>    The above tables are also in the output. The gamma value equals probability concor-
</p>
<p>dance  &ndash; probability discordance , whereby the tied cells are excluded (the cells that have the 
</p>
<p>same order of both variables). Somer&rsquo;s d measures the same but includes the ties. 
</p>
<p>The measures demonstrate that the association of the two variables is closer than 
</p>
<p>could happen by chance. A positive value means a positive correlation, the higher 
</p>
<p>the order in one variable, the higher it will be in the other one. Tau b and c have simi-
</p>
<p>lar meanings, but are more appropriate for data where numbers of categories 
</p>
<p>between the two variables are different. Both directional and symmetry measures 
</p>
<p>are statistically very signifi cant. This means that high satisfaction levels with the 
</p>
<p>treating doctors are strongly associated with high qol levels, and that low satisfac-
</p>
<p>tion levels are strongly associated with low qol levels.  
</p>
<p> Example</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_16">http://dx.doi.org/10.1007/978-3-319-15195-3_16</a></div>
</div>
<div class="page"><p/>
<p>70
</p>
<p>    Conclusion 
</p>
<p> We can conclude from this analysis that there is a statistically signifi cant positive 
</p>
<p>association between the qol score levels and the levels of satisfaction with the 
</p>
<p>patients&rsquo; doctors, can make predictions from the levels of satisfaction with the doc-
</p>
<p>tor about the expected quality of life in future patients, and could consider to recom-
</p>
<p>mend doctors to try and perform better to that aim. An interaction matrix, otherwise 
</p>
<p>called contingency table or crosstab, can be used to assess whether treatment levels 
</p>
<p>are associated with a better outcome score than others, and to assess the directions 
</p>
<p>of the differences in distribution of the variables.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of crosstabs is given in 
</p>
<p>Statistics applied to clinical studies 5th edition, Chap. 3, The analysis of safety data, 
</p>
<p>pp 41&ndash;59, Edited by Springer Heidelberg Germany, 2012, from the same authors.    
</p>
<p>11 Predictions from Ordinal Clinical Data (450 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>71&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_12
</p>
<p>Chapter 12
</p>
<p>Assessing Relative Health Risks 
(3,000 Subjects)
</p>
<p> General Purpose
</p>
<p>This chapter is to assess whether interaction matrices, otherwise called contingency 
</p>
<p>tables or simply crosstabs, can be used to test the effect of personal characteristics 
</p>
<p>like gender, age, married status etc. on a person&rsquo;s health risks.
</p>
<p> Primary Scientific Question
</p>
<p>Can marital status affect a person&rsquo;s health risks.
</p>
<p> Example
</p>
<p>In 3,000 subjects the effect of married status on being healthy was assessed.</p>
<p/>
</div>
<div class="page"><p/>
<p>72
</p>
<p>ageclass married healthy
</p>
<p>4,00 1 0
</p>
<p>3,00 0 0
</p>
<p>2,00 1 0
</p>
<p>1,00 1 0
</p>
<p>4,00 1 0
</p>
<p>3,00 0 0
</p>
<p>2,00 1 0
</p>
<p>1,00 0 0
</p>
<p>4,00 1 0
</p>
<p>3,00 1 0
</p>
<p>ageclass 1 = 30&ndash;40, 2 = 40&ndash;50, 3 = 50&ndash;60, 4 = 60&ndash;70
</p>
<p>married 0 = no, 1 = yes
</p>
<p>healthy 0 = no, 1 = yes
</p>
<p>In the above table the first 10 patients are given. The entire data file is entitled 
</p>
<p>&ldquo;healthrisk.sav&rdquo; and is in extras.springer.com. We will start the analysis by opening 
</p>
<p>the data file in SPSS.
</p>
<p>Command
</p>
<p>Analyze....Descriptive Statistics....Crosstabs....Row(s): enter married....Column(s): 
</p>
<p>enter health....Statistics: mark Observed....mark Rows....click Continue....click OK.
</p>
<p>Married * healthy crosstabulation
</p>
<p>Healthy
</p>
<p>TotalNo Yes
</p>
<p>Married No Count 192 1,104 1,296
</p>
<p>% within married 14,8 % 85,2 % 100,0 %
</p>
<p>Yes Count 167 1,537 1,704
</p>
<p>% within married 9,8 % 90,2 % 100,0 %
</p>
<p>The crosstab is in the output sheets. It shows that 14.8 % of the unmarried sub-
</p>
<p>jects were unhealthy, leaving 85,2 % being healthy. In contrast, 9.8% of the married 
</p>
<p>subjects were unhealthy, 90.2% being healthy. And so, the risk of being unhealthy 
</p>
<p>in this population was 14.8 % in the unmarried and 9.8% in the married subjects. 
</p>
<p>The relative risk of being unhealthy in unmarried versus married subjects was, thus, 
</p>
<p>14.8/9.8 = 1.512. Similarly, the relative risk of being healthy in unmarried versus 
</p>
<p>married subjects was 85.2/90.2 = 0.944.
</p>
<p>Risk estimate
</p>
<p>Value
</p>
<p>95% Confidence interval
</p>
<p>Lower Upper
</p>
<p>Odds ratio for married (no/yes) 1,601 1,283 1,997
</p>
<p>For cohort healthy = no 1,512 1,245 1,836
</p>
<p>For cohort healthy = yes ,944 ,919 ,971
</p>
<p>N of valid cases 3,000
</p>
<p>12 Assessing Relative Health Risks (3,000 Subjects)</p>
<p/>
</div>
<div class="page"><p/>
<p>73
</p>
<p>The odds of being unhealthy in unmarried subjects was 192/1,104 = 0.1739.
</p>
<p>The odds of being unhealthy in married subjects was 167/1,537 = 0.1087.
</p>
<p>The ratio of the two, the odds ratio was thus 0.1739/0.1087 = 1.601, as shown in 
</p>
<p>the above table. It is easy to see that this odds ratio is equal to
</p>
<p> 
</p>
<p>=
the relative risk of being unhealthy in the unmarried versusmarrieddsubjects
</p>
<p>the relative risk of being healthy in the unmarried versussmarried subjects
</p>
<p>= =1 512 0 944 1 601. / . . .
 
</p>
<p>In order to assess whether this finding is robust, we will add age classes as a layer 
</p>
<p>variable, and test whether different age classes have similar odds ratios.
</p>
<p>Command
</p>
<p>Analyze....Descriptive Statistics....Crosstabs....Row(s): enter married....Column(s): 
</p>
<p>enter health....Layer 1 of 1: enter ageclass....Statistics: mark Observed....mark Rows 
</p>
<p>....mark Cochran and Mantel Haenszel Statistics....click Continue....click OK.
</p>
<p>Married * healthy * ageclass crosstabulation
</p>
<p>Ageclass
</p>
<p>Healthy
</p>
<p>No Yes Total
</p>
<p>30&ndash;40 Married No Count 52 138 190
</p>
<p>% within married 27,4 % 72,6 % 100,0 %
</p>
<p>Yes Count 53 327 380
</p>
<p>% within married 13,9 % 86,1 % 100,0 %
</p>
<p>Total Count 105 465 570
</p>
<p>% within married 18,4 % 81,6 % 100,0 %
</p>
<p>40&ndash;50 Married No Count 69 352 421
</p>
<p>% within married 16,4 % 83,6 % 100,0 %
</p>
<p>Yes Count 67 593 660
</p>
<p>% within married 10,2 % 89,8 % 100,0 %
</p>
<p>Total Count 136 945 1,081
</p>
<p>% within married 12,6 % 87,4 % 100,0 %
</p>
<p>50&ndash;60 married No Count 28 201 229
</p>
<p>% within married 12,2 % 87,8 % 100,0 %
</p>
<p>Yes Count 17 287 304
</p>
<p>% within married 5,6 % 94,4 % 100,0 %
</p>
<p>Total Count 45 488 533
</p>
<p>% within married 8,4 % 91,6 % 100,0 %
</p>
<p>60&ndash;70 Married No Count 43 413 456
</p>
<p>% within married 9,4 % 90,6 % 100,0 %
</p>
<p>Yes Count 30 330 360
</p>
<p>% within married 8,3 % 91,7 % 100,0 %
</p>
<p>Total Count 73 743 816
</p>
<p>% within married 8,9 % 91,1 % 100,0 %
</p>
<p>* Symbol of multiplication
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>74
</p>
<p>Risk estimate
</p>
<p>Ageclass Value
</p>
<p>95% Confidence 
</p>
<p>interval
</p>
<p>Lower Upper
</p>
<p>30&ndash;40 Odds ratio for married (no/yes) 2,325 1,511 3,578
</p>
<p>For cohort healthy = no 1,962 1,396 2,759
</p>
<p>For cohort healthy = yes ,844 ,767 ,929
</p>
<p>N of valid cases 570
</p>
<p>40&ndash;50 Odds ratio for married (no/yes) 1,735 1,209 2,490
</p>
<p>For cohort healthy = no 1,614 1,180 2,208
</p>
<p>For cohort healthy = yes ,931 ,886 ,978
</p>
<p>N of valid cases 1,081
</p>
<p>50&ndash;60 Odds ratio for married (no/yes) 2,352 1,254 4,411
</p>
<p>For cohort healthy = no 2,186 1,227 3,896
</p>
<p>For cohort healthy = yes ,930 ,879 ,983
</p>
<p>N of valid cases 533
</p>
<p>60&ndash;70 Odds ratio for married (no/yes) 1,145 ,703 1,866
</p>
<p>For cohort healthy = no 1,132 ,725 1,766
</p>
<p>For cohort healthy = yes ,988 ,946 1,031
</p>
<p>N of valid cases 816
</p>
<p>In the output are the crosstabs the odds ratios of the four ageclasses. The odds 
</p>
<p>ratios are pretty heterogeneous, between 1.145 and 2.352, but 95 % confidence 
</p>
<p>intervals were pretty wide. Yet, it is tested whether these odds ratios are signifi-
</p>
<p>cantly different from one another.
</p>
<p>Tests of homogeneity of the odds ratio
</p>
<p>Chi-Squared df Asymp. Sig. (2-sided)
</p>
<p>Breslow-Day 5,428 3 ,143
</p>
<p>Tarone&rsquo;s 5,422 3 ,143
</p>
<p>The above Breslow and the Tarone&rsquo;s tests are the heterogeneity tests. They were 
</p>
<p>insignificant. The differences could, thus, be ascribed to chance findings, rather than 
</p>
<p>real effects. It seems appropriate, therefore, to say that an overall odds ratio of these 
</p>
<p>data adjusted for age classes is meaningful. For that purpose a Mantel Haenszel 
</p>
<p>(MH) odds ratio (OR) will be calculated.
</p>
<p>healthy
</p>
<p>no yes
</p>
<p>unmarried no a b
</p>
<p>yes c d
</p>
<p>Having 4 odds ratios with the above structure, it is calculated as follows 
</p>
<p>(n = a + b + c + d):
</p>
<p> 
</p>
<p>OddsRatio
ad n
</p>
<p>cd n
MH
</p>
<p>=

</p>
<p>
</p>
<p>/
</p>
<p>/  
</p>
<p>12 Assessing Relative Health Risks (3,000 Subjects)</p>
<p/>
</div>
<div class="page"><p/>
<p>75
</p>
<p>Tests of conditional independence
</p>
<p>Chi-Squared df Asymp. Sig. (2-sided)
</p>
<p>Cochran&rsquo;s 26,125 1 ,000
</p>
<p>Mantel-Haenszel 25,500 1 ,000
</p>
<p>Under the conditional independence assumption, Cochran&rsquo;s statistic is asymptotically distributed 
</p>
<p>as a 1 df chi-squared distribution, only if the number of strata is fixed, while the Mantel-Haenszel 
</p>
<p>statistic is always asymptotically distributed as a 1 df chi-squared distribution. Note that the conti-
</p>
<p>nuity correction is removed from the Mantel-Haenszel statistic when the sum of the differences 
</p>
<p>between the observed and the expected is 0
</p>
<p>Mantel-Haenszel common odds ratio estimate
</p>
<p>Estimate 1,781
</p>
<p>In(Estimate) ,577
</p>
<p>Std. Error of In(Estimate) ,115
</p>
<p>Asymp. Sig. (2-sided) ,000
</p>
<p>Asymp. 95% confidence interval Common odds ratio Lower bound 1,422
</p>
<p>Upper bound 2,230
</p>
<p>In(Common odds ratio) Lower bound ,352
</p>
<p>Upper bound ,802
</p>
<p>The Mantel-Haenszel common odds ratio estimate is asymptotically normally distributed under 
</p>
<p>the common odds ratio of 1,000 assumption. So is the natural log of the estimate
</p>
<p>The Cochran&rsquo;s and Mantel Haenszel tests assess whether married status remains 
</p>
<p>an independent predictor of health after adjustment for ageclasses. They are signifi-
</p>
<p>cantly larger than an odds ratio (OR) of 0 at p &lt; 0.0001. The lower graph gives the 
</p>
<p>ORMH is thus 1.781. This OR is adjusted, and, therefore, more adequate than the 
</p>
<p>unadjusted OR of page 1 of this chapter.
</p>
<p> Conclusion
</p>
<p>Interaction matrices, otherwise called contingency tables or simply crosstabs, can 
</p>
<p>be used to test the effect of personal characteristics like gender, age, married status 
</p>
<p>etc. on a person&rsquo;s health risks. Results can be adjusted for concomitant effects like 
</p>
<p>the effect of age classes on the relationship between married status and health status. 
</p>
<p>Prior to assessment the homogeneity of the concomitant factors have to tested.
</p>
<p> Note
</p>
<p>More background, theoretical and mathematical information of relative risk assess-
</p>
<p>ments are in Statistics applied to clinical studies, Chap. 3, The analysis of safety 
</p>
<p>data, pp 41&ndash;59, Edited by Springer Heidelberg Germany, 2012, from the same 
</p>
<p>authors.
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>77&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_13
</p>
<p>    Chapter 13   
</p>
<p> Measuring Agreement (30 Patients) 
</p>
<p>                       General Purpose 
</p>
<p> Interaction matrices have myriad applications. In the Chap.   12     it can be observed 
</p>
<p>that they perform well for assessing relative health risks, making predictions from 
</p>
<p>nominal and ordinal clinical data (Chap. 9&ndash;11), and statistical testing of outcome 
</p>
<p>scores. In this chapter we will assess, if they also can be applied to measure agree-
</p>
<p>ment. Agreement, otherwise called reproducibility or reliability, of duplicate obser-
</p>
<p>vations is the fundament of diagnostic procedures, and, therefore, also the fundament 
</p>
<p>of much of scientifi c research.  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> Can a 2 &times; 2 interaction matrix also be used to demonstrate the level of agreement 
</p>
<p>between duplicate observations of the effect of antihypertensive treatment.  
</p>
<p>    Example 
</p>
<p> In 30 patients with hypertension the effect of an antihypertensive treatment was 
</p>
<p>measured with normotension as outcome. Each patients was tested twice in order to 
</p>
<p>assess the reproducibility of the procedure. the example was used before (Chap. 19, 
</p>
<p>Reliability assessment of qualitative diagnostic tests, in: SPSS for starters part 1, 
</p>
<p>pp 69&ndash;70, Springer Heidelberg Germany, 2010, from the same authors as the cur-
</p>
<p>rent work). </p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_12">http://dx.doi.org/10.1007/978-3-319-15195-3_12</a></div>
</div>
<div class="page"><p/>
<p>78
</p>
<p>    The above table shows the results of fi rst 11 patients. The entire data fi le is entitled 
</p>
<p>agreement, and is in extras.springer.com. We will start by opening the fi le in SPSS.
</p>
<p>  Command 
</p>
<p>  Analyze....Descriptive Statistics....Crosstabs....Row(s): enter Variable 1....Column(s): 
</p>
<p>enter Variable 2....click Statistics....Mark: kappa....click Continue....click Cells.....
</p>
<p>mark Observed....click continue....click OK.   
</p>
<p> VAR00001 * VAR00002 crosstabulation 
</p>
<p> Count 
</p>
<p> VAR00002 
</p>
<p> Total  ,00  1,00 
</p>
<p> VAR00001  ,00  11  4  15 
</p>
<p> 1,00  5  10  15 
</p>
<p> Total  16  14  30 
</p>
<p>   In the output sheets a interaction matrix of the data is shown. If agreement is 
</p>
<p>100 %, then the cells b and c would be empty, and the cells a and d would contain 
</p>
<p>30 patients. 
</p>
<p> variable 2 
</p>
<p> 0  1 
</p>
<p> variable 1  0  a  b 
</p>
<p> 1  c  d 
</p>
<p>   However, the cells a and d contain only 21 patients. 
</p>
<p> Variables 
</p>
<p> 1  2 
</p>
<p> 1,00  1,00 
</p>
<p> 1,00  1,00 
</p>
<p> 1,00  1,00 
</p>
<p> 1,00  1,00 
</p>
<p> 1,00  1,00 
</p>
<p> 1,00  1,00 
</p>
<p> 1,00  1,00 
</p>
<p> 1,00  1,00 
</p>
<p> 1,00  1,00 
</p>
<p> 1,00  1,00 
</p>
<p> 1,00  ,00 
</p>
<p>  Variable 1 = responder after fi rst test 
</p>
<p>(0 = non responder, 1 = responder) 
</p>
<p> Variable 2 = responder after second test  
</p>
<p>13 Measuring Agreement (30 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>79
</p>
<p> If agreement would be 0 %, then the cells a and d would contain 15 patients. 
</p>
<p>However, 21 is more than 15, and so, this may indicate that agreement is better than 
</p>
<p>0 %, although less than 100 %. Cohen&rsquo;s Kappa is computed by SPSS to estimate the 
</p>
<p>exact level of agreement.
</p>
<p> Symmetric measures 
</p>
<p> Value  Asymp. Std. Error a   Approx. T b   Approx. Sig. 
</p>
<p> Measure of agreement  Kappa  ,400  ,167  2,196  ,028 
</p>
<p> N of valid cases  30 
</p>
<p>   a Not assuming the null hypothesis 
</p>
<p>  b Using the asymptotic standard error assuming the null hypothesis 
</p>
<p>    The above table shows that the kappa-value equals 0.400. A kappa-value of 0 
</p>
<p>means poor reproducibility or agreement, a kappa-value of 1 means excellent. This 
</p>
<p>result of 0.400 is moderate. This result is signifi cantly different from an agreement 
</p>
<p>of 0 at p = 0.028.  
</p>
<p>    Conclusion 
</p>
<p> In this chapter it is assessed if interaction matrices can be applied to measure 
</p>
<p> agreement. Agreement, otherwise called reproducibility or reliability, of duplicate 
</p>
<p>observations is the fundament of diagnostic procedures, and, therefore, also the fun-
</p>
<p>dament of much of scientifi c research. 
</p>
<p> A 2 &times; 2 interaction matrix can be used to demonstrate the level of agreement 
</p>
<p>between duplicate observations of the effect of antihypertensive treatment. We 
</p>
<p>should add that kappa-values can also be computed from larger interaction matrices, 
</p>
<p>like 3 &times; 3, 4 &times; 4 contingency tables, etc.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of correct and incor-
</p>
<p>rect methods for assessing reproducibility or agreement are given in Chap. 45, 
</p>
<p>Testing reproducibility, pp 499&ndash;508, in: Statistics applied to clinical studies 5th 
</p>
<p>edition, Springer Heidelberg Germany, 2012, from the same authors.    
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>81&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_14
</p>
<p>    Chapter 14   
</p>
<p> Column Proportions for Testing Differences 
Between Outcome Scores (450 Patients) 
</p>
<p>                       General Purpose 
</p>
<p> In the Chap.   10     the relationship between treatment modality and quality of life (qol) 
</p>
<p>score levels were assessed using a chi-square test of the interaction matrix. Many 
</p>
<p>high qol scores were in the hydrotherapy and physiotherapy treatments, and in the 
</p>
<p>subgroup that received counseling the overall differences from other treatments 
</p>
<p>were statistically signifi cant at p &lt; 0.0001. In this chapter, using the same data, we 
</p>
<p>will try and test what levels of qol scores were signifi cantly different from one 
</p>
<p>another, and, thus, provide a more detailed about differences in effects.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Can the effects of different treatment modalities on outcome score levels previously 
</p>
<p>assessed with a chi-square test of the interaction matrix, be assessed with better 
</p>
<p>precision applying column proportion comparisons using Bonferroni-adjusted 
</p>
<p>z-tests?  
</p>
<p>    Example 
</p>
<p> A parallel group study of 450 patients assessed the effect of different complemen-
</p>
<p>tary treatment modalities on qol score levels. The fi rst 11 patients of the data fi le is 
</p>
<p>underneath. The entire data fi le is entitled &ldquo;qol.sav&rdquo;, and is in extras.springer.com. </p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_10">http://dx.doi.org/10.1007/978-3-319-15195-3_10</a></div>
</div>
<div class="page"><p/>
<p>82
</p>
<p> treatment  counseling  qol  satdoctor 
</p>
<p> 3  1  4  4 
</p>
<p> 4  0  2  1 
</p>
<p> 2  1  5  4 
</p>
<p> 3  0  4  4 
</p>
<p> 2  1  2  1 
</p>
<p> 2  0  1  4 
</p>
<p> 4  0  4  1 
</p>
<p> 3  0  4  1 
</p>
<p> 4  1  4  4 
</p>
<p> 2  1  3  4 
</p>
<p> 4  1  5  5 
</p>
<p>  treatment = treatment modality (1 = cardiac fi tness, 2 = physiotherapy, 
</p>
<p>3 = wellness, 4 = hydrotherapy) 
</p>
<p> counseling = counseling given (0 = no, 1 = yes) 
</p>
<p> qol = quality of life scores (1 = very low, 5 = very high) 
</p>
<p> satdoctor = satisfaction with treating doctor (1 = very low, 5 = very high) 
</p>
<p>    We will start by opening the data fi le in SPSS.
</p>
<p>  Command: 
</p>
<p>  Analyze....Descriptive Statistics....Crosstabs....Row(s): enter treatment....Column(s): 
</p>
<p>enter qol....click Cells....mark Observed....mark Columns....mark: Compare column 
</p>
<p>properties....mark: Adjusted p-values (Bonferroni method)....click Continue....click 
</p>
<p>OK.   
</p>
<p>    Treatment  *  qol score crosstabulation 
</p>
<p> Qol score 
</p>
<p> Total 
</p>
<p> Very 
</p>
<p>low  Low  Medium  High 
</p>
<p> Very 
</p>
<p>high 
</p>
<p> Treatment  Cardiac fi tness  Count  21 a   21 a   24 a   36 a   118 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 24,4 %  28,8 %  22,5 %  22,0 %  32,4 %  26,2 % 
</p>
<p> Physiotherapy  Count  22 a   20 a   18 a   20 a   20 a   100 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 25,6 %  27,4 %  25,4 %  18,3 %  18,0 %  22,2 % 
</p>
<p>(continued)
</p>
<p>14 Column Proportions for Testing Differences Between Outcome Scores (450 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>83
</p>
<p>    Treatment  *  qol score crosstabulation 
</p>
<p> Qol score 
</p>
<p> Total 
</p>
<p> Very 
</p>
<p>low  Low  Medium  High 
</p>
<p> Very 
</p>
<p>high 
</p>
<p> Wellness  Count  22 a   14 a   12 a   30 a   25 a   104 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 26,7 %  19,2 %  16,9 %  27,5 %  22,5 %  23,1 % 
</p>
<p> Hydrotherapy  Count  20 a   18 a   25 a   35 a   30 a   128 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 23,3 %  24,7 %  35,2 %  32,1 %  27,0 %  28,4 % 
</p>
<p> Total  Count  86  73  71  109  111  450 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 100,0 %  100,0 %  100,0 %  100,0 %  100,0 %  100,0 % 
</p>
<p>   *  Symbol of multiplication 
</p>
<p> Each subscript letter denotes a subset of qol score categories whose column proportions do not 
</p>
<p>differ signifi cantly from each other at the ,05 level  
</p>
<p>   The above table is in the output sheets. All of the counts in the cells are given 
</p>
<p>with the subscript letter a. 
</p>
<p> The interpretation of the subscript letters are pretty obvious: 
</p>
<p> looking in a single row 
</p>
<p> a vs a  p &gt; 0.10 
</p>
<p> a vs a,b  0.05 &lt; p &lt; 0.10 
</p>
<p> a vs b  p &lt; 0.05 
</p>
<p> a vs c  p &lt; 0.01 
</p>
<p> a vs d  p &lt; 0.001 
</p>
<p> b vs a,b  0.05 &lt; p &lt; 0.10 
</p>
<p>   This means, that, in the above table, none of the counts is signifi cantly different 
</p>
<p>from one another. This is consistent with the insignifi cant chi-square test of Chap. 
</p>
<p>  16    . We have clinical arguments that counseling may support the benefi cial effects of 
</p>
<p>treatments, and, therefore, perform an analysis with two layers, one in the patients 
</p>
<p>with and one in those without counseling.
</p>
<p>  Command: 
</p>
<p>  Analyze....Descriptive Statistics....Crosstabs....Row(s): enter treatment.... Column(s): 
</p>
<p>enter qol....Layer 1 of 1: enter counseling....click Cells....mark Observed....mark 
</p>
<p>Columns....mark: Compare column properties....mark: Adjusted p-values (Bonferroni 
</p>
<p>method)....click Continue....click OK.   
</p>
<p>Example</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_16">http://dx.doi.org/10.1007/978-3-319-15195-3_16</a></div>
</div>
<div class="page"><p/>
<p>84
</p>
<p>    Treatment  *  qol score  *  counseling crosstabulation 
</p>
<p> Counseling 
</p>
<p> Qol score 
</p>
<p> Total  Very low  Low  Medium  High  Very high 
</p>
<p> No  Treatment  Cardiac fi tness  Count  19  16  8  8  14  65 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 292 %  34,8 %  26,7 %  20,0 %  28,6 %  28,3 % 
</p>
<p> Phototherapy  Count  8  8  7  7  15b  45 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 12,3 %  17,4 %  23,3 %  17,5 %  30,6 %  19,6 % 
</p>
<p> Wellness  Count  23  8  6  15  9  61 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 35,4 %  17,4 %  20,0 %  37,5 %  18,4 %  26,5 % 
</p>
<p> Hydrotherapy  Count  15  14  9  10  11  59 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 23,1 %  30,4 %  30,0 %  25,0 %  22,4 %  25,7 % 
</p>
<p> Total  Count  65  46  30  40  49  230 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 100,0 %  100,0 %  100,0 %  100,0 %  100,0 %  100,0 % 
</p>
<p> Yes  Treatment  Cardiac fi tness  Count  2  5  8  16  22  53 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 9,5 %  18,5 %  19,5 %  23,2 %  35,5 %  24,1 % 
</p>
<p> Physiotherapy  Count  14  12  11  13  5  55 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 66,7 %  44,4 %  26,8 %  18,8 %  8,1 %  25,0 % 
</p>
<p> Wellness  Count  0  6  6  15  16  43 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> ,0 %  22,2 %  14,6 %  21,7 %  25,8 %  19,5 % 
</p>
<p> Hydrotherapy  Count  5  4  16  25  19  69 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 23,8 %  14,8 %  39,0 %  36,2 %  30,6 %  31,4 % 
</p>
<p> Total  Count  21  27  41  69  62  220 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 100,0 %  100,0 %  100,0 %  100,0 %  100,0 %  100,0 % 
</p>
<p>   *  Symbol of multiplication    
</p>
<p> The above table is now shown. It gives the computations for the patients 
</p>
<p>previously counseled separately. Now differences particularly in the patients coun-
</p>
<p>seled were larger. 
</p>
<p> In the cardiac fi tness row the very low and very high qol cells the percentages of 
</p>
<p>patients present are 9.5 and 23.2 % (signifi cantly different at p &lt; 0.05 Bonferroni 
</p>
<p>adjusted), and &ldquo;very low&rdquo; versus the three scores in between have a trend to signifi -
</p>
<p>cance 0.05 &lt; p &lt;0.10. The same is true with &ldquo;very high&rdquo; versus (vs) the three scores 
</p>
<p>in between. In the physiotherapy row differences were even larger. In the physio-
</p>
<p>therapy row we have:
</p>
<p>    1.    both 14 vs 11 and 11 vs 5 signifi cantly different at p &lt; 0.05   
</p>
<p>   2.    14 vs 12, 12 vs 11, 11 vs 13, 13 vs 5 with a trend to signifi cantly different at 
</p>
<p>0.05 &lt; p &lt; 0.10.     
</p>
<p>14 Column Proportions for Testing Differences Between Outcome Scores (450 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>85
</p>
<p> Similarly, in the wellness and hydrotherapy rows signifi cant differences and 
</p>
<p>trends to signifi cance were observed. 
</p>
<p> In the no-counseling patients differences were smaller, but some trends, and two 
</p>
<p>signifi cant differences at p &lt; 0.05 (a vs b) were, nonetheless, observed. 
</p>
<p> In conclusion, only in the physiotherapy row the low qol fraction is large, in the 
</p>
<p>other three the high qol fractions are large. And so, with respect to qol physiotherapy 
</p>
<p>does not perform very well, and may better be skipped from the program. 
</p>
<p> Note: Bonferroni adjustment for multiple testing works as follows. In order for 
</p>
<p>p-values to be signifi cant, with two tests they need to be smaller than 0.025, with 
</p>
<p>four tests smaller than 0.0125, with ten tests smaller than 0.005, etc.  
</p>
<p>    Conclusion 
</p>
<p> When assessing the outcome effects of different treatments, column proportions 
</p>
<p>comparisons of interaction matrices can be applied to precisely fi nd what outcome 
</p>
<p>scores are signifi cantly different from one another. This may provide relevant infor-
</p>
<p>mation about some treatment modalities, and may give cause for some treatment 
</p>
<p>modalities to be skipped.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of interaction matrices 
</p>
<p>is given in Statistics applied to clinical studies 5th edition, Chap. 3, The analysis of 
</p>
<p>safety data, pp 41&ndash;59, Springer Heidelberg Germany, 2012, from the same authors, 
</p>
<p>and in the Chaps. 10&ndash;13 of this work.    
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>87&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_15
</p>
<p>    Chapter 15   
</p>
<p> Pivoting Trays and Tables for Improved 
Analysis of Multidimensional Data 
(450 Patients) 
</p>
<p>                       General Purpose 
</p>
<p> Pivot tables can visualize multiple variables data with the help of interactive 
</p>
<p>displays of multiple dimensions. They have been in SPSS statistical software for a 
</p>
<p>long time (from version 7.0), and, gradually, they take over in most modules. They 
</p>
<p>are helpful for improving the analysis by visualizing interaction patterns you so far 
</p>
<p>did not notice.  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> Are pivot tables able to visualize interaction pattern in your clinical data that so far 
</p>
<p>were unnoticed?  
</p>
<p>    Example 
</p>
<p> We will use as example the data also used in the Chaps. 10&ndash;14. A parallel group 
</p>
<p>study of 450 patients assessed the effect of different complementary treatment 
</p>
<p>modalities on qol score levels. The fi rst 11 patients of the data fi le is underneath. 
</p>
<p>The entire data fi le is entitled &ldquo;qol.sav&rdquo;, and is in extras.springer.com. </p>
<p/>
</div>
<div class="page"><p/>
<p>88
</p>
<p> treatment  counseling  qol  satdoctor 
</p>
<p> 3  1  4  4 
</p>
<p> 4  0  2  1 
</p>
<p> 2  1  5  4 
</p>
<p> 3  0  4  4 
</p>
<p> 2  1  2  1 
</p>
<p> 2  0  1  4 
</p>
<p> 4  0  4  1 
</p>
<p> 3  0  4  1 
</p>
<p> 4  1  4  4 
</p>
<p> 2  1  3  4 
</p>
<p> 4  1  5  5 
</p>
<p>  treatment = treatment modality (1 = cardiac fi tness, 
</p>
<p>2 = physiotherapy, 3 = wellness, 4 = hydrotherapy) 
</p>
<p> counseling = counseling given (0 = no, 1 = yes) 
</p>
<p> qol = quality of life scores (1 = very low, 5 = very high) 
</p>
<p> satdoctor = satisfaction with treating doctor (1 = very low, 
</p>
<p>5 = very high) 
</p>
<p>    We will start by opening the data fi le in SPSS.
</p>
<p>  Command: 
</p>
<p>  Analyze....Descriptive Statistics....Crosstabs....Row(s): enter treatment....Column(s): 
</p>
<p>enter qol....click Cells....mark Observed....mark Columns....click Continue....click OK.    
</p>
<p> The underneath table is shown in the output sheets.
</p>
<p> Treatment  *  qol score  *  counseling crosstabulation 
</p>
<p> Counseling 
</p>
<p> Qol score 
</p>
<p> Total 
</p>
<p> Very 
</p>
<p>low  Low  Medium  High 
</p>
<p> Very 
</p>
<p>high 
</p>
<p> No  Treatment  Cardiac fi tness  Count  19  16  8  8  14  65 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 29,2 %  34,8 %  26,7 %  20,0 %  28,6 %  28,3 % 
</p>
<p> Physiotherapy  Count  8  8  7  7  15  45 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 12,3 %  17,4 %  23,3 %  17,5 %  30,6 %  19,6 % 
</p>
<p> Wellness  Count  23  8  6  15  9  61 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 35,4 %  17,4 %  20,0 %  37,5 %  18,4 %  26,5 % 
</p>
<p> Hydrotherapy  Count  15  14  9  10  11  59 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 23,1 %  30,4 %  30,0 %  25,0 %  22,4 %  25,7 % 
</p>
<p> Total  Count  65  46  30  40  49  230 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 100,0 %  100,0 %  100,0 %  100,0 %  100,0 %  100,0 % 
</p>
<p>(continued)
</p>
<p>15 Pivoting Trays and Tables for Improved Analysis of Multidimensional Data&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>89
</p>
<p> Yes  Treatment  Cardiac fi tness  Count  2  5  8  16  22  53 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 9,5 %  18,5 %  19,5 %  23,2 %  35,5 %  24,1 % 
</p>
<p> Physiotherapy  Count  14  12  11  13  5  55 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 66,7 %  44,4 %  26,8 %  18,8 %  8,1 %  25,0 % 
</p>
<p> Wellness  Count  0  6  6  15  16  43 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> ,0 %  22,2 %  14,6 %  21,7 %  25,8 %  19,5 % 
</p>
<p> Hydrotherapy  Count  5  4  16  25  19  69 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 23,8 %  14,8 %  39,0 %  36,2 %  30,6 %  31,4 % 
</p>
<p> Total  Count  21  27  41  69  62  220 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 100,0 %  100,0 %  100,0 %  100,0 %  100,0 %  100,0 % 
</p>
<p> Total  treatment  Cardiac fi tness  Count  21  21  16  24  36  118 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 24,4 %  28,8 %  22,5 %  22,0 %  32,4 %  26,2 % 
</p>
<p> Physiotherapy  Count  22  20  18  20  20  100 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 25,6 %  27,4 %  25,4 %  18,3 %  18,0 %  22,2 % 
</p>
<p> Wellness  Count  23  14  12  30  25  104 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 26,7 %  19,2 %  16,9 %  27,5 %  22,5 %  23,1 % 
</p>
<p> Hydrotherapy  Count  20  18  25  35  30  128 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 23,3 %  24,7 %  35,2 %  32,1 %  27,0 %  28,4 % 
</p>
<p> Total  Count  86  73  71  109  111  450 
</p>
<p> % within 
</p>
<p>qol score 
</p>
<p> 100,0 %  100,0 %  100,0 %  100,0 %  100,0 %  100,0 % 
</p>
<p>   *  Symbol of multiplication 
</p>
<p>    We will apply this table for pivoting the data, using the &ldquo;user interface for pivot 
</p>
<p>tables&rdquo;, otherwise called the pivoting tray.
</p>
<p>Treatment * qol score * counseling crosstabulation
</p>
<p>Counseling
</p>
<p>Qol score
</p>
<p>Total
</p>
<p>Very 
</p>
<p>low Low Medium High
</p>
<p>Very 
</p>
<p>high
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>90
</p>
<p>  Command: 
</p>
<p>  Double-click the above table....the term Pivot is added to the menu bar....click Pivot 
</p>
<p>in the menu bar....the underneath Pivoting Tray consisting of all of the variables 
</p>
<p>appears....qol score is a column variables....counseling, treatment, and statistics are 
</p>
<p>row variables....left click the counseling icon and drag it to the column variables....
</p>
<p>similarly have statistics dragged to the layer dimension....close the Pivoting Tray.    
</p>
<p>    
</p>
<p>    A new table is shown in the output.
</p>
<p>15 Pivoting Trays and Tables for Improved Analysis of Multidimensional Data&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p> T
re
</p>
<p>at
m
</p>
<p>en
t 
</p>
<p> *   
q
o
l 
</p>
<p>sc
o
re
</p>
<p>  *
  c
</p>
<p>o
u
n
se
</p>
<p>li
n
g
 c
</p>
<p>ro
ss
</p>
<p>ta
b
u
la
</p>
<p>ti
o
n
 
</p>
<p> S
ta
</p>
<p>ti
st
</p>
<p>ic
s:
</p>
<p> %
 w
</p>
<p>it
h
in
</p>
<p> 
</p>
<p>q
o
l 
</p>
<p>sc
o
re
</p>
<p> 
</p>
<p> Q
o
l 
</p>
<p>sc
o
re
</p>
<p> 
</p>
<p> V
er
</p>
<p>y
 l
</p>
<p>o
w
</p>
<p> 
 L
</p>
<p>o
w
</p>
<p> 
 M
</p>
<p>ed
iu
</p>
<p>m
 
</p>
<p> H
ig
</p>
<p>h
 
</p>
<p> V
er
</p>
<p>y
 h
</p>
<p>ig
h
 
</p>
<p> C
o
u
n
se
</p>
<p>li
n
g
 
</p>
<p> C
o
u
n
se
</p>
<p>li
n
g
 
</p>
<p> C
o
u
n
se
</p>
<p>li
n
g
 
</p>
<p> C
o
u
n
se
</p>
<p>li
n
g
 
</p>
<p> C
o
u
n
se
</p>
<p>li
n
g
 
</p>
<p> N
o
 
</p>
<p> Y
es
</p>
<p> 
 T
</p>
<p>o
ta
</p>
<p>l 
 N
</p>
<p>o
 
</p>
<p> Y
es
</p>
<p> 
 T
</p>
<p>o
ta
</p>
<p>l 
 N
</p>
<p>o
 
</p>
<p> Y
es
</p>
<p> 
 T
</p>
<p>o
ta
</p>
<p>l 
 N
</p>
<p>o
 
</p>
<p> Y
es
</p>
<p> 
 T
</p>
<p>o
ta
</p>
<p>l 
 N
</p>
<p>o
 
</p>
<p> Y
es
</p>
<p> 
 T
</p>
<p>o
ta
</p>
<p>l 
</p>
<p> T
re
</p>
<p>at
m
</p>
<p>en
t 
</p>
<p> C
ar
</p>
<p>d
ia
</p>
<p>c 
fi 
</p>
<p>tn
es
</p>
<p>s 
 2
9
2
 %
</p>
<p> 
 9
,5
</p>
<p> %
 
</p>
<p> 2
4
,4
</p>
<p> %
 
</p>
<p> 3
4
8
 %
</p>
<p> 
 1
6
,5
</p>
<p> %
 
</p>
<p> 2
6
8
 %
</p>
<p> 
 2
6
,7
</p>
<p> %
 
</p>
<p> 1
9
,5
</p>
<p> %
 
</p>
<p> 2
2
3
 %
</p>
<p> 
 2
0
,0
</p>
<p> %
 
</p>
<p> 2
3
,2
</p>
<p> %
 
</p>
<p> 2
2
8
</p>
<p> %
 
</p>
<p> 2
8
,6
</p>
<p> %
 
</p>
<p> 3
5
,5
</p>
<p> %
 
</p>
<p> 3
2
,4
</p>
<p> %
 
</p>
<p> P
h
y
si
</p>
<p>o
th
</p>
<p>er
ap
</p>
<p>y
 
</p>
<p> 1
2
3
 %
</p>
<p> 
 6
6
,7
</p>
<p> %
 
</p>
<p> 2
5
,6
</p>
<p> %
 
</p>
<p> 1
7
,4
</p>
<p> %
 
</p>
<p> 4
4
,4
</p>
<p> %
 
</p>
<p> 2
7
,4
</p>
<p> %
 
</p>
<p> 2
3
3
 %
</p>
<p> 
 2
6
,6
</p>
<p> %
 
</p>
<p> 2
5
,4
</p>
<p> %
 
</p>
<p> 1
7
,5
</p>
<p> %
 
</p>
<p> 1
8
,6
</p>
<p> %
 
</p>
<p> 1
8
3
 %
</p>
<p> 
 3
0
,6
</p>
<p> %
 
</p>
<p> 8
,1
</p>
<p> %
 
</p>
<p> 1
8
,0
</p>
<p> %
 
</p>
<p> W
el
</p>
<p>ln
es
</p>
<p>s 
 3
5
,4
</p>
<p> %
 
</p>
<p> ,0
 %
</p>
<p> 
 2
6
,7
</p>
<p> %
 
</p>
<p> 1
7
,4
</p>
<p> %
 
</p>
<p> 2
2
2
 %
</p>
<p> 
 1
9
2
 %
</p>
<p> 
 2
0
,0
</p>
<p> %
 
</p>
<p> 1
4
,6
</p>
<p> %
 
</p>
<p> 1
6
3
 %
</p>
<p> 
 3
7
8
 %
</p>
<p> 
 2
1
,7
</p>
<p> %
 
</p>
<p> 2
7
3
</p>
<p> %
 
</p>
<p> 1
8
,4
</p>
<p> %
 
</p>
<p> 2
5
,8
</p>
<p> %
 
</p>
<p> 2
2
3
 %
</p>
<p> 
</p>
<p> H
y
d
ro
</p>
<p>th
er
</p>
<p>ap
y
 
</p>
<p> 2
3
,1
</p>
<p> %
 
</p>
<p> 2
3
,8
</p>
<p> %
 
</p>
<p> 2
3
,3
</p>
<p> %
 
</p>
<p> 3
0
,4
</p>
<p> %
 
</p>
<p> 1
4
,6
</p>
<p> %
 
</p>
<p> 2
4
,7
</p>
<p> %
 
</p>
<p> 3
0
,0
</p>
<p> %
 
</p>
<p> 3
9
,0
</p>
<p> %
 
</p>
<p> 3
5
2
 %
</p>
<p> 
 2
5
,0
</p>
<p> %
 
</p>
<p> 3
6
,2
</p>
<p> %
 
</p>
<p> 3
2
,1
</p>
<p> %
 
</p>
<p> 2
2
,4
</p>
<p> %
 
</p>
<p> 3
0
,6
</p>
<p> %
 
</p>
<p> 2
7
,0
</p>
<p> %
 
</p>
<p> T
o
ta
</p>
<p>l 
 1
0
0
,0
</p>
<p> %
 
 1
0
0
,0
</p>
<p> %
 
</p>
<p> 1
0
0
,0
</p>
<p> %
 
 1
0
0
,0
</p>
<p> %
 
</p>
<p> 1
0
0
,0
</p>
<p> %
 
</p>
<p> 1
0
0
8
 %
</p>
<p>  1
0
0
,0
</p>
<p> %
 
 1
0
0
,0
</p>
<p> %
 
 1
0
0
2
 %
</p>
<p>  1
0
0
,0
</p>
<p> %
 
</p>
<p> 1
0
0
,0
</p>
<p> %
 
 1
0
0
2
 %
</p>
<p> 
 1
0
0
,0
</p>
<p> %
 
</p>
<p> 1
0
0
,0
</p>
<p> %
 
 1
0
0
,0
</p>
<p> %
 
</p>
<p>  
 
</p>
<p>*
  S
</p>
<p>y
m
</p>
<p>b
o
l 
</p>
<p>o
f 
</p>
<p>m
u
lt
</p>
<p>ip
li
</p>
<p>ca
ti
</p>
<p>o
n
 </p>
<p/>
</div>
<div class="page"><p/>
<p>92
</p>
<p>    In it click Statistics and click &ldquo;% within qol score&rdquo; in the drop box of the table 
</p>
<p>next to Statistics. In the now upcoming table the cell counts have disappeared. They 
</p>
<p>are not relevant, anyway, only the percentages are so. From the Chaps.   2     and   6     we 
</p>
<p>already know, that there is an overall difference between the cells of the yes- 
</p>
<p>counseling matrix, and that the 67 % very low qol is signifi cantly different from the 
</p>
<p>5 % very high qol in the physical therapy treatment groups. What more can the 
</p>
<p>above pivoted table teach us? It underscores the fi nding from the Chaps.   2     and   6     by 
</p>
<p>showing next to each other the no-counseling and yes-counseling percentages; very 
</p>
<p>low qol with physiotherapy is observed in respectively 12.3 and 66.7 %, low qol in 
</p>
<p>17.4 and 44.4 %, and, in contrast, very high qol in respectively 30.6 and 8.1 %. 
</p>
<p> Restarting with the above unpivoted table once more, give the commands:
</p>
<p>   Double-click the table....the term Pivot is added to the menu bar....click Pivot in the 
</p>
<p>menu bar....the Pivoting Tray consisting of all of the variables appears....qol score is 
</p>
<p>a column variables....counseling, treatment, and statistics are row variables....drag 
</p>
<p>treatment to the layer dimension....similarly have statistics dragged to the layer 
</p>
<p>dimension....the Pivoting Tray now looks like shown underneath....close it.    
</p>
<p>    
</p>
<p>    A new pivot table is given in the output. In the left upper angle of it, it has a 
</p>
<p>Statistics and a treatment drop box. Statistics: click Count and select &ldquo;% within qol 
</p>
<p>score&rdquo;, treatment: select, subsequently, all of the four treatments given. The four 
</p>
<p>underneath tables are produced.
</p>
<p>15 Pivoting Trays and Tables for Improved Analysis of Multidimensional Data&hellip;</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_2">http://dx.doi.org/10.1007/978-3-319-15195-3_2</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_6">http://dx.doi.org/10.1007/978-3-319-15195-3_6</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_2">http://dx.doi.org/10.1007/978-3-319-15195-3_2</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_6">http://dx.doi.org/10.1007/978-3-319-15195-3_6</a></div>
</div>
<div class="page"><p/>
<p>93
</p>
<p> Treatment  *  qol score  *  counseling crosstabulation 
</p>
<p> Statistics: % within qol score, treatment: treatment cardiac fi tness 
</p>
<p> Counseling 
</p>
<p> Qol score 
</p>
<p> Total  Very low  Low  Medium  High  Very high 
</p>
<p> No  29,2 %  34,8 %  26,7 %  20,0 %  28,6 %  28,3 % 
</p>
<p> Yes  9,5 %  18,5 %  19,5 %  23,2 %  35,5 %  24,1 % 
</p>
<p> Total  24,4 %  28,8 %  22,5 %  22,0 %  32,4 %  26,2 % 
</p>
<p>   *  Symbol of multiplication 
</p>
<p> Treatment  *  qol score  *  counseling crosstabulation 
</p>
<p> Statistics: % within qol score, treatment: treatment physiotherapy 
</p>
<p> Counseling 
</p>
<p> Qol score 
</p>
<p> Total  Very low  Low  Medium  High  Very high 
</p>
<p> No  12,3 %  17,4 %  23,3 %  17,5 %  30,6 %  19,6 % 
</p>
<p> Yes  66,7 %  44,4 %  26,8 %  18,8 %  8,1 %  25,0 % 
</p>
<p> Total  25,6 %  27,4 %  25,4 %  18,3 %  18,0 %  22,2 % 
</p>
<p>   *  Symbol of multiplication 
</p>
<p> Treatment  *  qol score  *  counseling crosstabulation 
</p>
<p> Statistics: % within qol score, treatment: treatment wellness 
</p>
<p> Counseling 
</p>
<p> Qol score 
</p>
<p> Total  Very low  Low  Medium  High  Very high 
</p>
<p> No  35,4 %  17,4 %  20,0 %  37,5 %  18,4 %  26,5 % 
</p>
<p> Yes  ,0 %  22,2 %  14,6 %  21,7 %  25,8 %  19,5 % 
</p>
<p> Total  26,7 %  19,2 %  16,9 %  27,5 %  22,5 %  23,1 % 
</p>
<p>   *  Symbol of multiplication 
</p>
<p> Treatment  *  qol score  *  counseling crosstabulation 
</p>
<p> Statistics: % within qol score, treatment: treatment hydrotherapy 
</p>
<p> Counseling 
</p>
<p> Qol score 
</p>
<p> Total  Very low  Low  Medium  High  Very high 
</p>
<p> No  23,1 %  30,4 %  30,0 %  25,0 %  22,4 %  25,7 % 
</p>
<p> Yes  23,8 %  14,8 %  39,0 %  36,2 %  30,6 %  31,4 % 
</p>
<p> Total  23,3 %  24,7 %  35,2 %  32,1 %  27,0 %  28,4 % 
</p>
<p>   *  Symbol of multiplication 
</p>
<p>    They visualize a big downward trend of percentages from very low to very high 
</p>
<p>qol for the treatments cardiac fi tness, wellness and hydrotherapy, and an upward 
</p>
<p>trend for the treatment physiotherapy. Although this is in agreement with the fi nd-
</p>
<p>ings from the Chaps.   2     and   6    , the patterns are relevant, because they give you an 
</p>
<p>additional idea about how patients experience their treatments.  
</p>
<p>Example</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_2">http://dx.doi.org/10.1007/978-3-319-15195-3_2</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_6">http://dx.doi.org/10.1007/978-3-319-15195-3_6</a></div>
</div>
<div class="page"><p/>
<p>94
</p>
<p>    Conclusion 
</p>
<p> The interactive pivot tables are not only a way of showing the same in another 
</p>
<p>perspective, but they are also more than that, because they help you better notice 
</p>
<p>what is going on at difference levels of the analysis, and, so, they, actually, can 
</p>
<p>improve the analysis by visualizing data patterns you did not notice before.  
</p>
<p>    Note 
</p>
<p> Pivot tables are widely applied not only in SPSS and most of the larger statistical 
</p>
<p>software programs, but also in spreadsheets programs like Excel. In SPSS they are 
</p>
<p>being applied with Anova (analysis of variance), Correlations, Crosstabs, 
</p>
<p>Descriptives, Examine, Frequencies, General Linear Models, Nonparametric Tests, 
</p>
<p>Regression, T-tests. In the current book pivot tables were applied in many more 
</p>
<p>Chaps., e.g., the Chaps.   6    ,   16    , and   29    .    
</p>
<p>15 Pivoting Trays and Tables for Improved Analysis of Multidimensional Data&hellip;</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_6">http://dx.doi.org/10.1007/978-3-319-15195-3_6</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_16">http://dx.doi.org/10.1007/978-3-319-15195-3_16</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_29">http://dx.doi.org/10.1007/978-3-319-15195-3_29</a></div>
</div>
<div class="page"><p/>
<p>95&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_16
</p>
<p>    Chapter 16   
</p>
<p> Online Analytical Procedure Cubes, a More 
Rapid Approach to Analyzing Frequencies 
(450 Patients) 
</p>
<p>                       General Purpose 
</p>
<p> OLAP means online analytical procedures. Cubes is a term used to indicate 
</p>
<p>multidimensional datasets. OLAP cubes were fi rst used in 1970, by SQL Express a 
</p>
<p>software package for storing business data, like fi nancial data, in an electronic 
</p>
<p>warehouse, and, at the same time, turning raw data into meaningful information 
</p>
<p>(business intelligence), and was initially called layered reports. Generally, fi nancial 
</p>
<p>data or production data are being summarized, and from these summaries subsum-
</p>
<p>maries are computed like productions by time-periods, cities, and other subgroups. 
</p>
<p>Instead of quantities of business data, quantities of health outcomes could, similarly, 
</p>
<p>be analyzed. However, to date no such analyses have been performed. This chapter 
</p>
<p>is to assess whether online analytical procedures can also be applied on health 
</p>
<p>outcomes instead of business outcomes.  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> Can online analytical processing (OLAP) using summaries and subsummaries of 
</p>
<p>clinical outcome data support traditional crosstab analyses?  
</p>
<p>    Example 
</p>
<p> We will use as example the data also used in the Chaps.   10     and   11    . A parallel group 
</p>
<p>study of 450 patients assessed the effect of different complementary treatment 
</p>
<p>modalities on qol score levels. The fi rst 11 patients of the data fi le is underneath. 
</p>
<p>The entire data fi le is entitled &ldquo;qol.sav&rdquo;, and is in extras.springer.com. </p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_10">http://dx.doi.org/10.1007/978-3-319-15195-3_10</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_11">http://dx.doi.org/10.1007/978-3-319-15195-3_11</a></div>
</div>
<div class="page"><p/>
<p>96
</p>
<p> treatment  counseling  qol  satdoctor 
</p>
<p> 3  1  4  4 
</p>
<p> 4  0  2  1 
</p>
<p> 2  1  5  4 
</p>
<p> 3  0  4  4 
</p>
<p> 2  1  2  1 
</p>
<p> 2  0  1  4 
</p>
<p> 4  0  4  1 
</p>
<p> 3  0  4  1 
</p>
<p> 4  1  4  4 
</p>
<p> 2  1  3  4 
</p>
<p> 4  1  5  5 
</p>
<p>  treatment = treatment modality (1 = cardiac fi tness, 
</p>
<p>2 = physiotherapy, 3 = wellness, 4 = hydrotherapy) 
</p>
<p> counseling = counseling given (0 = no, 1 = yes) 
</p>
<p> qol = quality of life scores (1 = very low, 5 = very high) 
</p>
<p> satdoctor = satisfaction with treating doctor (1 = very 
</p>
<p>low, 5 = very high) 
</p>
<p>    We will start by opening the data fi le in SPSS.
</p>
<p>  Command: 
</p>
<p>  Analyze....Reports....OLAP Cubes....Summary Variable(s): enter qol score....
</p>
<p>Grouping Variable(s): enter treatment, counseling....click OK.   
</p>
<p> Case processing summary 
</p>
<p> Cases 
</p>
<p> Included  Excluded  Total 
</p>
<p> N  Percent  N  Percent  N  Percent 
</p>
<p> Qol score  *  treatment  *  counseling  450  100,0 %  0  ,0 %  450  100,0 % 
</p>
<p>   *  Symbol of multiplication 
</p>
<p> OLAP cubes 
</p>
<p> Treatment:Total 
</p>
<p> Counseling:Total 
</p>
<p> Sum  N  Mean  Std. Deviation  % of Total Sum  % of Total N 
</p>
<p> Qol score  1,436  450  3,19  1,457  100,0 %  100,0 % 
</p>
<p>   The above tables are in the output sheets. The add-up sum of all scores are given 
</p>
<p>(1436), and the overall mean score of the 450 patients (3.19). Next we can slice 
</p>
<p>these results into subgroups, and calculate mean scores by treatment modality. A 
</p>
<p>pivoting tray is used for that purpose.
</p>
<p>16 Online Analytical Procedure Cubes, a More Rapid Approach to Analyzing&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>97
</p>
<p>  Command: 
</p>
<p>  Double-click the above table....the term Pivot is added to the menu bar....click Pivot 
</p>
<p>in the menu bar....the underneath Pivoting Tray consisting of all of the variables 
</p>
<p>appears....close the Pivoting Tray.    
</p>
<p>    
</p>
<p>    The above table now has in the upper right corner two drop boxes: treatment, and 
</p>
<p>counseling. Treatment: click Total (in blue), and produce the underneath four tables 
</p>
<p>with summary statistics of the four treatment modalities.
</p>
<p> OLAP cubes 
</p>
<p> Treatment:cardiac fi tness 
</p>
<p> Counseling:Total 
</p>
<p> Sum  N  Mean  Std. Deviation  % of Total Sum  % of Total N 
</p>
<p> Qol score  387  118  3,28  1,501  26,9 %  26,2 % 
</p>
<p> OLAP cubes 
</p>
<p> Treatment:physiotherapy 
</p>
<p> Counseling:Total 
</p>
<p> Sum  N  Mean  Std. Deviation  % of Total Sum  % of Total N 
</p>
<p> Qol score  296  100  2,96  1,449  20,6 %  22,2 % 
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>98
</p>
<p> OLAP cubes 
</p>
<p> Treatment:wellness 
</p>
<p> Counseling:Total 
</p>
<p> Sum  N  Mean  Std. Deviation  % of Total Sum  % of Total N 
</p>
<p> Qol score  332  104  3,19  1,501  23,1 %  23,1 % 
</p>
<p> OLAP cubes 
</p>
<p> Treatment:hydrotherapy 
</p>
<p> Counseling:Total 
</p>
<p> Sum  N  Mean  Std. Deviation  % of Total Sum  % of Total N 
</p>
<p> Qol score  421  128  3,29  1,381  29,3 %  28,4 % 
</p>
<p>   It is easy to that the mean qol score of physiotherapy is signifi cantly lower than 
</p>
<p>that of hydrotherapy according to an unpaired t-test: 
</p>
<p> mean  Std Deviation  n  Std Error 
</p>
<p> 2.96  1.449  100  0.145 
</p>
<p> 3.29  1.381  128  0.122 
</p>
<p>  t = (3.29 &minus; 2.96)/&radic;(0.145^2 + 0.122^2) = 1.96 
 with (100 + 128 &minus; 2) = 226 degrees of freedom 
</p>
<p>    This would indicate that these two mean qol scores are signifi cantly different 
</p>
<p>from one another at p &lt; 0.05. 
</p>
<p> In addition to summary statistics of different treatments, we can also compute 
</p>
<p>summary statistics of qol scores by counseling yes or no.
</p>
<p>  Command: 
</p>
<p>  click the treatment drop box....select Total....next click the counseling drop box&hellip;
</p>
<p>fi rst select counseling no....then select counseling yes. 
</p>
<p>The underneath tables are given.   
</p>
<p> OLAP cubes 
</p>
<p> Treatment:Total 
</p>
<p> Counseling:Total 
</p>
<p> Sum  N  Mean  Std. Deviation  % of Total Sum  % of Total N 
</p>
<p> Qol score  1,436  450  3,19  1,457  100,0 %  100,0 % 
</p>
<p> OLAP cubes 
</p>
<p> Treatment:Total 
</p>
<p> Counseling:No 
</p>
<p> Sum  N  Mean  Std. Deviation  % of Total Sum  % of Total N 
</p>
<p> Qol score  652  230  2,83  1,530  45,4 %  51,1 % 
</p>
<p>16 Online Analytical Procedure Cubes, a More Rapid Approach to Analyzing&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>99
</p>
<p> OLAP cubes 
</p>
<p> Treatment:Total 
</p>
<p> Counseling:Yes 
</p>
<p> Sum  N  Mean  Std. Deviation  % of Total Sum  % of Total N 
</p>
<p> Qol score  784  220  3,56  1,279  54,6 %  48,9 % 
</p>
<p>   It is again easy to test whether the mean qol score of no-counseling is signifi -
</p>
<p>cantly lower than that of yes-counseling according to an unpaired t-test: 
</p>
<p> mean  Std Deviation  n  Std Error 
</p>
<p> 2.83  1.530  230  0.101 
</p>
<p> 3.56  1.279  220  0.086 
</p>
<p>  t = (3.56 &minus; 2.83)/&radic;(0.101^2 + 0.086^2) = 5.49 
 with (230 + 220 &minus; 2) = 448 degrees of freedom 
</p>
<p>    This would indicate that the two means are signifi cantly different from one 
</p>
<p>another at p &lt; 0.0001.  
</p>
<p>    Conclusion 
</p>
<p> In the current example the individual qol levels were estimated as 5 scores on a 
</p>
<p>5-points linear scale. In the Chaps.   2    ,   6     and   7     analyses took place by comparing 
</p>
<p>frequencies of different qol scores with one another. In the OLAP cubes analysis a 
</p>
<p>different approach is applied. Instead of working with frequencies of different qol 
</p>
<p>scores, it works with mean scores and standard deviations. Other summary measure 
</p>
<p>is also possible like sums of qol scores, medians, ranges or variances etc. Unpaired 
</p>
<p>t-test can be used to test the signifi cance of difference between various 
</p>
<p>subsummaries. 
</p>
<p> Although we have to admit that the crosstab analyses and OLAP cube lead to 
</p>
<p>essentially the same results, the OLAP cube procedure is faster, and few simple 
</p>
<p>table are enough to tell you what is going on. Also additional statistical testing with 
</p>
<p>the t-test is simple.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information about the analyses of 
</p>
<p>interaction matrices of frequencies are in the Chaps. 9&ndash;11. The OLAP cube is 
</p>
<p>another approach with similar results, but it works more rapidly than the other 
</p>
<p>methods.    
</p>
<p>Note</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_2">http://dx.doi.org/10.1007/978-3-319-15195-3_2</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_6">http://dx.doi.org/10.1007/978-3-319-15195-3_6</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_7">http://dx.doi.org/10.1007/978-3-319-15195-3_7</a></div>
</div>
<div class="page"><p/>
<p>101&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_17
</p>
<p>    Chapter 17   
</p>
<p> Restructure Data Wizard for Data Classifi ed 
the Wrong Way (20 Patients) 
</p>
<p>                       General Purpose 
</p>
<p> Underneath the opening page of the Restructure Data Wizard in SPSS is given. In 
</p>
<p>the current chapter this tool will be applied for restructuring multiple variables in a 
</p>
<p>single case to multiple cases with a single variables. 
</p>
<p>    </p>
<p/>
</div>
<div class="page"><p/>
<p>102
</p>
<p>    Suppose in a study the treatment outcome has been measured several times instead 
</p>
<p>of once. In current clinical research repeated measures in a single subject are com-
</p>
<p>mon. The problem with repeated measures is, that they are more close to one another 
</p>
<p>than unrepeated measures. If this is not taken into account, then data analysis will 
</p>
<p>lose power. The underneath table gives an example of a 2 group parallel- group study 
</p>
<p>comparing two treatments for cholesterol reduction of 5 weeks. The example is taken 
</p>
<p>from Chap. 6, Mixed linear models, pp 65&ndash;77, in: Machine learning in medicine part 
</p>
<p>one, Springer Heidelberg Germany, 2013, from the same authors. 
</p>
<p> It shows that 5 different variables present the 5 subsequent outcome measure-
</p>
<p>ments in each patient. In order to analyze these data in appropriately the table has to 
</p>
<p>be restructured with each week given a separate row. This is a pretty laborious exer-
</p>
<p>cise, and it will get really annoying if you have 100 or more patients instead of 20. 
</p>
<p>The restructure data wizard, however, should do the job within seconds. 
</p>
<p> patient no  week 1  week 2  week 3  week 4  week 5  treatment modality 
</p>
<p> 1  1,66  1,62  1,57  1,52  1,50  0,00 
</p>
<p> 2  1,69  1,71  1,60  1,55  1,56  0,00 
</p>
<p> 3  1,92  1,94  1,83  1,78  1,79  0,00 
</p>
<p> 4  1,95  1,97  1,86  1,81  1,82  0,00 
</p>
<p> 5  1,98  2,00  1,89  1,84  1,85  0,00 
</p>
<p> 6  2,01  2,03  1,92  1,87  1,88  0,00 
</p>
<p> 7  2,04  2,06  1,95  1,90  1,91  0,00 
</p>
<p> 8  2,07  2,09  1,98  1,93  1,94  0,00 
</p>
<p> 9  2,30  2,32  2,21  2,16  2,17  0,00 
</p>
<p> 10  2,36  2,35  2,26  2,23  2,20  0,00 
</p>
<p> 11  1,57  1,82  1,83  1,83  1,82  1,00 
</p>
<p> 12  1,60  1,85  1,89  1,89  1,85  1,00 
</p>
<p> 13  1,83  2,08  2,12  2,12  2,08  1,00 
</p>
<p> 14  1,86  2,11  2,16  2,15  2,11  1,00 
</p>
<p> 15  2,80  2,14  2,19  2,18  2,14  1,00 
</p>
<p> 16  1,92  2,17  2,22  2,21  2,17  1,00 
</p>
<p> 17  1,95  2,20  2,25  2,24  2,20  1,00 
</p>
<p> 18  1,98  2,23  2,28  2,27  2,24  1,00 
</p>
<p> 19  2,21  2,46  2,57  2,51  2,48  1,00 
</p>
<p> 20  2,34  2,51  2,55  2,55  2,52  1,00 
</p>
<p>  week 1 = hdl-cholesterol level after 1 week of trial 
</p>
<p> treatment modality = treatment modality (0 = treatment 0, 1 = treatment 1) 
</p>
<p>17 Restructure Data Wizard for Data Classifi ed the Wrong Way (20 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>103
</p>
<p>        Primary Scientifi c Question 
</p>
<p> Can the restructure data wizard provide a table suitable for testing treatment effi ca-
</p>
<p>cies adjusted for the repeated nature of the outcome data.  
</p>
<p>    Example 
</p>
<p> The above data fi le is entitled &ldquo;restructure.sav&rdquo;, and is in extras.springer.com. Start 
</p>
<p>by opening the data fi le in SPSS statistical software.
</p>
<p>  Command: 
</p>
<p>  click Data....click Restructure....mark Restructure selected variables into cases.... 
</p>
<p>click Next....mark One (for example, w1, w2, and w3)....click Next....Name: id (the 
</p>
<p>patient id variable is already provided)....Target Variable: enter "fi rstweek, second-
</p>
<p>week...... fi fthweek"....Fixed Variable(s): enter treatment....click Next.... How many 
</p>
<p>index variables do you want to create?....mark One....click Next....click Next 
</p>
<p>again....click Next again....click Finish....Sets from the original data will still be in 
</p>
<p>use&hellip;click OK.    
</p>
<p> Return to the main screen and observe that there are now 100 rows instead of 
</p>
<p>20 in the data fi le. The fi rst 10 rows are given underneath. 
</p>
<p> id  treatment  Index1  Trans1 
</p>
<p> 1  0,00  1  1,66 
</p>
<p> 1  0,00  2  1,62 
</p>
<p> 1  0,00  3  1,57 
</p>
<p> 1  0,00  4  1,52 
</p>
<p> 1  0,00  5  1,50 
</p>
<p> 2  0,00  1  1,69 
</p>
<p> 2  0,00  2  1,71 
</p>
<p> 2  0,00  3  1,60 
</p>
<p> 2  0,00  4  1,55 
</p>
<p> 2  0,00  5  1,56 
</p>
<p>  id = patient id 
</p>
<p> treatment = treatment modality 
</p>
<p> Index1 = week of treatment (1&ndash;5) 
</p>
<p> Trans1 = outcome values 
</p>
<p>    We will now perform a mixed linear analysis of the data.
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>104
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.mixed models&hellip;.linear&hellip;.specify subjects and repeated&hellip;.subject: enter 
</p>
<p>id &hellip;.continue&hellip;.linear mixed model&hellip;.dependent: Trans1&hellip;.factors: Index1, treat-
</p>
<p>ment&hellip;.fi xed&hellip;.build nested term&hellip;.treatment &hellip;.add&hellip;.Index1&hellip;.add&hellip;. Index1 
</p>
<p>build term by* treatment&hellip;.Index1 *treatment&hellip;.add&hellip;.continue&hellip;.OK (* = sign of 
</p>
<p>multiplication).    
</p>
<p> The underneath table shows the main results from the above analysis. After 
</p>
<p>adjustment for the repeated nature of the outcome data the treatment modality 0 
</p>
<p>performs much better than the treatment modality 1. The results from alternative 
</p>
<p>analyses for these data were not only less appropriate but also less sensitive. The 
</p>
<p>discussion of this is beyond the scope of the current chapter, but it can found in the 
</p>
<p>Chap. 6, Mixed linear models, pp 65&ndash;77, in: Machine learning in medicine part one, 
</p>
<p>Springer Heidelberg Germany, 2013, from the same authors.
</p>
<p> Type III tests of fi xed effects a  
</p>
<p> Source  Numerator df  Denominator df  F  Sig. 
</p>
<p> Intercept  1  76,570  6988,626  ,000 
</p>
<p> Week  4  31,149  ,384  ,818 
</p>
<p> Treatment  1  76,570  20,030  ,000 
</p>
<p> Week*treatment  4  31,149  1,337  ,278 
</p>
<p>   a Dependent variable: outcome 
</p>
<p>        Conclusion 
</p>
<p> The restructure data wizard provides a table suitable for testing treatment effi cacies 
</p>
<p>adjusted for the repeated nature of the outcome data. It is particularly pleasant if 
</p>
<p>your data fi le is big, and has many (repeated) observations.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of restructuring data 
</p>
<p>fi les is in the Chap. 6, Mixed linear models, pp 65&ndash;77, in: Machine learning in medi-
</p>
<p>cine part one, Springer Heidelberg Germany, 2013, from the same authors.    
</p>
<p>17 Restructure Data Wizard for Data Classifi ed the Wrong Way (20 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>105&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_18
</p>
<p>    Chapter 18   
</p>
<p> Control Charts for Quality Control 
of Medicines (164 Tablet Desintegration 
Times) 
</p>
<p>                       General Purpose 
</p>
<p> A consistent quality of a process or product, like the manufacturing of tablets is, 
</p>
<p>traditionally, tested by 1 sample chi-square tests of their weights, diameters, desin-
</p>
<p>tegration times. E.g., tablets may only be approved, if the standard deviation of their 
</p>
<p>diameters is less than 0.7 mm. E.g., a 50 tablet sample with a standard deviation of 
</p>
<p>0.9 mm is signifi cantly different from 0.7 (^ = symbol of power term).
</p>
<p>  
</p>
<p>Chi square
</p>
<p>degreesof freedom
</p>
<p>p o
</p>
<p>    

</p>
<p>
</p>
<p>50 1 0 9 2 0 7 2 81
</p>
<p>50 1
</p>
<p>0 01
</p>
<p>. ^ / . ^
</p>
<p>. nnesided 
   
</p>
<p>The example is from the Chap. 44, entitled &ldquo;Clinical data where variability is more 
</p>
<p>important than averages&rdquo;, pp 487&ndash;497, (in: Statistics applied to clinical studies 5th 
</p>
<p>edition, Springer Heidelberg Germany, 2012, from the same authors). Nowadays, 
</p>
<p>we live in an era of machine learning, and ongoing quality control, instead of testing 
</p>
<p>now and then, has become more easy, and, in addition, provides information of 
</p>
<p>process variations over time and process performance. Control charts available in 
</p>
<p>SPSS and other data mining software is helpful to that aim.  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> Control charts are currently routinely applied for the process control of larger facto-
</p>
<p>ries, but they are, virtually, unused in the medical fi eld. We will assess, whether they 
</p>
<p>can be helpful to process control of pharmaceuticals.  </p>
<p/>
</div>
<div class="page"><p/>
<p>106
</p>
<p>    Example 
</p>
<p> A important quality criterion of tablets is the desintegration time in water of 37 &deg;C 
</p>
<p>within 30 min or so. If it is considerably longer, the tablet will be too hard for 
</p>
<p> consumption, if shorter it will be too soft for storage. 164 Tablets were tested over a 
</p>
<p>period of 40 days. 
</p>
<p> day  desintegration (min) 
</p>
<p> 1  33,2 
</p>
<p> 1  31,0 
</p>
<p> 1  32,7 
</p>
<p> 1  30,8 
</p>
<p> 1  32,2 
</p>
<p> 1  31,3 
</p>
<p> 2  30,1 
</p>
<p> 2  31,5 
</p>
<p> 2  33,6 
</p>
<p> 2  32,2 
</p>
<p> 4  32,9 
</p>
<p> 4  32,2 
</p>
<p>   The desintegration times of the fi rst 11 tablets are above. The entire data fi le is in 
</p>
<p>&ldquo;qolcontrol.sav&rdquo;, and is in extras.springer.com. We will start the analysis by  opening 
</p>
<p>the data fi le in SPSS statistical software.
</p>
<p>  Command: 
</p>
<p>  Analyze....Quality Control....Control Charts....mark Cases are units....click Defi ne....
</p>
<p>Process Measurement: enter "desintegration"....Subgroups Defi ned by: enter 
</p>
<p>"days"....click Control Rules....mark: Above +3 sigma,   
</p>
<p>  
</p>
<p>Below sigma
</p>
<p>out of last above sigma
</p>
<p>out of last below sig
</p>
<p>


</p>
<p>3
</p>
<p>2 3 2
</p>
<p>2 3 2
</p>
<p>,
</p>
<p>,
</p>
<p>mma
</p>
<p>out of last above sigma
</p>
<p>out of last below sigma
</p>
<p>,
</p>
<p>,4 5 1
</p>
<p>4 5 1
</p>
<p>

</p>
<p>   
</p>
<p>   ....click Continue....click Statistics....Specifi cation Limits: Upper: type 36,0....
</p>
<p>Lower: type 30,0....Target: type 33,0....mark Actual % outside specifi cation limits....
</p>
<p>Process Capability Indices....in Capacity Indices mark   
</p>
<p>  
</p>
<p>CP
</p>
<p>CpL
</p>
<p>CpU
</p>
<p>k
</p>
<p>CpM....
   
</p>
<p>18 Control Charts for Quality Control of Medicines (164 Tablet Desintegration Times)</p>
<p/>
</div>
<div class="page"><p/>
<p>107
</p>
<p>   ....in Performance Indices mark   
</p>
<p>  
</p>
<p>PP
</p>
<p>PpL
</p>
<p>PpU
</p>
<p>PpM....    
</p>
<p>   ....click continue....click OK.    
</p>
<p> In the output sheets are two pivot fi gures and two pivot tables. Many details of 
</p>
<p>the analyses can be called up after double-clicking them, then clicking the term 
</p>
<p>pivot in the menu bar, and closing the upcoming pivoting tray. Drop boxes appear 
</p>
<p>everywhere, and are convenient to visualize statistical details and textual explana-
</p>
<p>tions about what is going on (see the Chap.   15     for additional information on the use 
</p>
<p>of pivoting fi gures and tables). 
</p>
<p>    
</p>
<p>    The above pivot fi gure shows a pattern of the mean desintegration times of the 
</p>
<p>daily subsamples. The pattern is mostly within the 3 standard deviation limits. The 
</p>
<p>straight interrupted lines give upper and lower specifi cation limits (= overall mean 
</p>
<p> Example</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_15">http://dx.doi.org/10.1007/978-3-319-15195-3_15</a></div>
</div>
<div class="page"><p/>
<p>108
</p>
<p>&plusmn; 3 standard deviations, the lower one coincides with the x-axis, and is therefore not 
</p>
<p>visible). The UCL (upper control limit) and LCL (lower control limit) curves 
</p>
<p>describe sample ranges used to monitor spread in the daily subsamples. There are 
</p>
<p>just three violations of the above set control rules of &plusmn;3 sigmas ( = standard devia-
</p>
<p>tions) etc. The underneath table gives the details of the violations.
</p>
<p> Rule violations for X-bar 
</p>
<p> Day  Violations for points 
</p>
<p> 34  2 points out of the last 3 above +2 sigma 
</p>
<p> 35  Greater than +3 sigma 
</p>
<p> 35  2 points out of the last 3 above +2 sigma 
</p>
<p> 36  4 points out of the last 5 above +1 sigma 
</p>
<p>  3 points violate control rules 
</p>
<p>       
</p>
<p>    The above table gives mean ranges of the daily subsamples. There are no viola-
</p>
<p>tions of the control rules here. 
</p>
<p> The underneath table gives the process statistics.
</p>
<p>18 Control Charts for Quality Control of Medicines (164 Tablet Desintegration Times)</p>
<p/>
</div>
<div class="page"><p/>
<p>109
</p>
<p> Process statistics 
</p>
<p> Act. % outside SL  8,5 % 
</p>
<p> Capability indices  CP a   ,674 
</p>
<p> CpL a   ,735 
</p>
<p> CpU a   ,613 
</p>
<p> K  ,091 
</p>
<p> CpM a,b   ,663 
</p>
<p> Performance indices  PP  ,602 
</p>
<p> PpL  ,657 
</p>
<p> PpU  ,548 
</p>
<p> PpM b   ,594 
</p>
<p>  The normal distribution is assumed. LSL = 30,0 and USL = 36,0 
</p>
<p>  a The estimated capability sigma is based on the mean of the sample group ranges 
</p>
<p>  b The target value is 33,0 
</p>
<p>    Regarding the process capability indices: 
</p>
<p> CP  ratio of differences between the specifi cation limits and the observed 
</p>
<p>process variation, it should be &gt;1, &lt;1 indicates too much variation. 
</p>
<p> CpL and CpU  answer whether the process variations are symmetric, they 
</p>
<p>should be close to CP. 
</p>
<p> K  measure of capability of the data, which should have their centers 
</p>
<p>close to the specifi ed target, a small K value is good (particularly 
</p>
<p>if CP is &gt;1). 
</p>
<p> CpM  same meaning as K, it should be close to CP. 
</p>
<p>   Regarding the process performance indices:
</p>
<p>  The values are similar to those of the process capability indices, but a bit smaller, 
</p>
<p>because they overall instead of sample variability is taken into account. If a lot 
</p>
<p>smaller, they indicate selection bias in the data. 
</p>
<p> PP  similar meaning as CP. 
</p>
<p> PpL and PpU  similar meaning as CpL and CpU. 
</p>
<p>   The above table shows that the process stability is pretty bad with CP and PP 
</p>
<p>values a lot &lt; 1. K is small, which is good, because it indicates that the center of the 
</p>
<p>data is close to the specifi ed target.  
</p>
<p>    Conclusion 
</p>
<p> The above analysis shows that control charts methodology may be helpful to process 
</p>
<p>control of pharmaceuticals. The example data were largely in control, and the pro-
</p>
<p>cess mean was close to the specifi ed target value of 33,0 min. Nonetheless, statistics 
</p>
<p>of process stability were pretty weak, and precision of the data was pretty bad.  
</p>
<p> Conclusion</p>
<p/>
</div>
<div class="page"><p/>
<p>110
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of the SPSS module 
</p>
<p>Quality Control is in the Chap. 62. More information of process quality control is 
</p>
<p>also in the Chap. 44, entitled &ldquo;Clinical data where variability is more important than 
</p>
<p>averages&rdquo;, pp 487&ndash;497, in: Statistics applied to clinical studies 5th edition, Springer 
</p>
<p>Heidelberg Germany 2012, from the same authors.    
</p>
<p>18 Control Charts for Quality Control of Medicines (164 Tablet Desintegration Times)</p>
<p/>
</div>
<div class="page"><p/>
<p>   Part II 
</p>
<p>   (Log) Linear Models        </p>
<p/>
</div>
<div class="page"><p/>
<p>113&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_19
</p>
<p>    Chapter 19   
</p>
<p> Linear, Logistic, and Cox Regression 
for Outcome Prediction with Unpaired 
Data (20, 55, and 60 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> To assess whether linear, logistic and Cox modeling can be used to train clinical 
</p>
<p>data samples to make predictions about groups and individual patients.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> How many hours will patients sleep, how large is the risk for patients to fall out of 
</p>
<p>bed, how large is the hazard for patients to die.  
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 1&rdquo; as Chap. 4, 
</p>
<p>2013. </p>
<p/>
</div>
<div class="page"><p/>
<p>114
</p>
<p>    Linear Regression, the Computer Teaches Itself 
</p>
<p>to Make Predictions 
</p>
<p>    SPSS 19.0 is used for analysis, with the help of an XML (eXtended Markup 
</p>
<p>Language) fi le. The data fi le is entitled &ldquo;linoutcomeprediction&rdquo; and is in extras.
</p>
<p>springer.com. Start by opening the data fi le.
</p>
<p>  Command: 
</p>
<p>  Click Transform&hellip;.click Random Number Generators&hellip;.click Set Starting Point 
</p>
<p>&hellip;.click Fixed Value (2000000)&hellip;.click OK&hellip;.click Analyze&hellip;.Regression&hellip;. 
</p>
<p>Linear&hellip;.Dependent: enter hoursofsleep&hellip;.Independent: enter treatment and age&hellip;.
</p>
<p>click Save&hellip;.Predicted Values: click Unstandardized&hellip;.in XML Files click Export 
</p>
<p>fi nal model&hellip;.click Browse&hellip;.File name: enter "exportlin"&hellip;.click Save&hellip;.click 
</p>
<p>Continue&hellip;.click OK.   
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5 
</p>
<p> 0,00  6,00  65,00  0,00  1,00 
</p>
<p> 0,00  7,10  75,00  0,00  1,00 
</p>
<p> 0,00  8,10  86,00  0,00  0,00 
</p>
<p> 0,00  7,50  74,00  0,00  0,00 
</p>
<p> 0,00  6,40  64,00  0,00  1,00 
</p>
<p> 0,00  7,90  75,00  1,00  1,00 
</p>
<p> 0,00  6,80  65,00  1,00  1,00 
</p>
<p> 0,00  6,60  64,00  1,00  0,00 
</p>
<p> 0,00  7,30  75,00  1,00  0,00 
</p>
<p> 0,00  5,60  56,00  0,00  0,00 
</p>
<p> 1,00  5,10  55,00  1,00  0,00 
</p>
<p> 1,00  8,00  85,00  0,00  1,00 
</p>
<p> 1,00  3,80  36,00  1,00  0,00 
</p>
<p> 1,00  4,40  47,00  0,00  1,00 
</p>
<p> 1,00  5,20  58,00  1,00  0,00 
</p>
<p> 1,00  5,40  56,00  0,00  1,00 
</p>
<p> 1,00  4,30  46,00  1,00  1,00 
</p>
<p> 1,00  6,00  64,00  1,00  0,00 
</p>
<p> 1,00  3,70  33,00  1,00  0,00 
</p>
<p> 1,00  6,20  65,00  0,00  1,00 
</p>
<p>  Var 1 = treatment 0 is placebo, treatment 1 is sleeping pill 
</p>
<p> Var 2 = hours of sleep 
</p>
<p> Var 3 = age 
</p>
<p> Var 4 = gender 
</p>
<p> Var 5 = comorbidity  
</p>
<p>19 Linear, Logistic, and Cox Regression for Outcome Prediction with Unpaired Data&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>115
</p>
<p> Coeffi cients a  
</p>
<p> Model 
</p>
<p> Unstandardized coeffi cients 
</p>
<p> Standardized 
</p>
<p>coeffi cients 
</p>
<p> t  Sig.  B  Std. Error  Beta 
</p>
<p> 1  (Constant)  ,989  ,366  2,702  ,015 
</p>
<p> Treatment  &minus;,411  ,143  &minus;,154  &minus;2,878  ,010 
</p>
<p> Age  ,085  ,005  ,890  16,684  ,000 
</p>
<p>   a Dependent variable: hoursofsleep 
</p>
<p>    The output sheets show in the coeffi cients table that both treatment and age are 
</p>
<p>signifi cant predictors at p &lt; 0.10. Returning to the data fi le we will observe that 
</p>
<p>SPSS has computed predicted values and gives them in a novel variable entitled 
</p>
<p>PRE_1. The saved XML fi le will now be used to compute the predicted hours of 
</p>
<p>sleep in 4 novel patients with the following characteristics. For convenience the 
</p>
<p>XML fi le is given in extras.springer.com. 
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5 
</p>
<p> ,00  6,00  66,00  ,00  1,00 
</p>
<p> ,00  7,10  74,00  ,00  1,00 
</p>
<p> ,00  8,10  86,00  ,00  ,00 
</p>
<p> ,00  7,50  74,00  ,00  ,00 
</p>
<p>  Var 1 = treatment 0 is placebo, treatment 1 is sleeping pill 
</p>
<p> Var 2 = hours of sleep 
</p>
<p> Var 3 = age 
</p>
<p> Var 4 = gender 
</p>
<p> Var 5 = comorbidity 
</p>
<p>    Enter the above data in a new SPSS data fi le.
</p>
<p>  Command: 
</p>
<p>  Utilities&hellip;.click Scoring Wizard&hellip;.click Browse&hellip;.click Select&hellip;.Folder: enter the 
</p>
<p>exportlin.xml fi le&hellip;.click Select&hellip;.in Scoring Wizard click Next&hellip;.click Use value 
</p>
<p>substitution&hellip;.click Next&hellip;.click Finish.    
</p>
<p> The above data fi le now gives individually predicted hours of sleep as computed 
</p>
<p>by the linear model with the help of the XML fi le. 
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5  Var 6 
</p>
<p> ,00  6,00  66,00  ,00  1,00  6,51 
</p>
<p> ,00  7,10  74,00  ,00  1,00  7,28 
</p>
<p> ,00  8,10  86,00  ,00  ,00  8,30 
</p>
<p> ,00  7,50  74,00  ,00  ,00  7,28 
</p>
<p>  Var 1 = treatment 0 is placebo, treatment 1 is sleeping pill 
</p>
<p> Var 2 = hours of sleep 
</p>
<p> Var 3 = age 
</p>
<p> Var 4 = gender 
</p>
<p> Var 5 = comorbidity 
</p>
<p> Var 6 = predicted hours of sleep 
</p>
<p>Linear Regression, the Computer Teaches Itself to Make Predictions</p>
<p/>
</div>
<div class="page"><p/>
<p>116
</p>
<p>        Conclusion 
</p>
<p> The module linear regression can be readily trained to predict hours of sleep both in 
</p>
<p>groups and, with the help of an XML fi le, in individual future patients.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of linear regression is 
</p>
<p>available in Statistics applied to clinical studies, 5th edition, Chaps. 14 and 15, 
</p>
<p>entitled &ldquo;Linear regression basic approach&rdquo; and &ldquo;Linear regression for assessing 
</p>
<p>precision, confounding, interaction&rdquo;, pp 161&ndash;176 and 177&ndash;185, Springer Heidelberg 
</p>
<p>Germany 2012, from the same authors.  
</p>
<p>    Logistic Regression, the Computer Teaches Itself 
</p>
<p>to Make Predictions 
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5 
</p>
<p> ,00  1,00  50,00  ,00  1,00 
</p>
<p> ,00  1,00  76,00  ,00  1,00 
</p>
<p> ,00  1,00  57,00  1,00  1,00 
</p>
<p> ,00  1,00  65,00  ,00  1,00 
</p>
<p> ,00  1,00  46,00  1,00  1,00 
</p>
<p> ,00  1,00  36,00  1,00  1,00 
</p>
<p> ,00  1,00  98,00  ,00  ,00 
</p>
<p> ,00  1,00  56,00  1,00  ,00 
</p>
<p> ,00  1,00  44,00  ,00  ,00 
</p>
<p> ,00  1,00  76,00  1,00  1,00 
</p>
<p> ,00  1,00  75,00  1,00  1,00 
</p>
<p> ,00  1,00  74,00  1,00  1,00 
</p>
<p> ,00  1,00  87,00  ,00  ,00 
</p>
<p>  Var 1 department type 
</p>
<p> Var 2 falling out of bed (1 = yes) 
</p>
<p> Var 3 age 
</p>
<p> Var 4 gender 
</p>
<p> Var 5 letter of complaint (1 = yes) 
</p>
<p>    Only the fi rst 13 patients are given, the entire data fi le is entitled &ldquo;logoutcomepre-
</p>
<p>diction&rdquo; and is in extras.springer.com. 
</p>
<p>19 Linear, Logistic, and Cox Regression for Outcome Prediction with Unpaired Data&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>117
</p>
<p> SPSS 19.0 is used for analysis, with the help of an XML (eXtended Markup 
</p>
<p>Language) fi le. Start by opening the data fi le.
</p>
<p>  Command: 
</p>
<p>  Click Transform&hellip;.click Random Number Generators&hellip;.click Set Starting Point 
</p>
<p>&hellip;.click Fixed Value (2000000)&hellip;.click OK&hellip;.click Analyze&hellip;.Regression &hellip;.
</p>
<p>Binary Logistic&hellip;.Dependent: enter fallingoutofbed &hellip;.Covariates: enter depart-
</p>
<p>menttype and letterofcomplaint&hellip;.click Save&hellip;.in Predicted Values click 
</p>
<p>Probabilities&hellip;.in Export model information to XML fi le click Browse&hellip;. File 
</p>
<p>name: enter "exportlog"&hellip;.click Save&hellip;.click Continue&hellip;.click OK.   
</p>
<p> Variables in the equation 
</p>
<p> B  S.E.  Wald  df  Sig.  Exp(B) 
</p>
<p> Step 1 a   Departmenttype  1,349  ,681  3,930  1  ,047  3,854 
</p>
<p> Letterofcomplaint  2,039  ,687  8,816  1  ,003  7,681 
</p>
<p> Constant  &minus;1,007  ,448  5,047  1  ,025  ,365 
</p>
<p>   a Variable(s) entered on step 1: departmenttype, letterofcomplaint 
</p>
<p>    In the above output table it is shown that both department type and letter of com-
</p>
<p>plaint are signifi cant predictors of the risk of falling out of bed. Returning to the data 
</p>
<p>fi le we will observe that SPSS has computed predicted values and gives them in a 
</p>
<p>novel variable entitled PRE_1. The saved XML fi le will now be used to compute the 
</p>
<p>predicted hours of sleep in 5 novel patients with the following characteristics. For 
</p>
<p>convenience the XML fi le is given in extras.springer.com. 
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5 
</p>
<p> ,00  ,00  67,00  ,00  ,00 
</p>
<p> 1,00  1,00  54,00  1,00  ,00 
</p>
<p> 1,00  1,00  65,00  1,00  ,00 
</p>
<p> 1,00  1,00  74,00  1,00  1,00 
</p>
<p> 1,00  1,00  73,00  ,00  1,00 
</p>
<p>  Var 1 department type 
</p>
<p> Var 2 falling out of bed (1 = yes) 
</p>
<p> Var 3 age 
</p>
<p> Var 4 gender 
</p>
<p> Var 5 letter of complaint (1 = yes) 
</p>
<p>    Enter the above data in a new SPSS data fi le.
</p>
<p>  Command: 
</p>
<p>  Utilities&hellip;.click Scoring Wizard&hellip;.click Browse&hellip;.click Select&hellip;.Folder: enter the 
</p>
<p>exportlog.xml fi le&hellip;.click Select&hellip;.in Scoring Wizard click Next&hellip;.mark Probability 
</p>
<p>of Predicted Category&hellip;.click Next&hellip;.click Finish.    
</p>
<p> The above data fi le now gives individually predicted probabilities of falling out 
</p>
<p>of bed as computed by the logistic model with the help of the XML fi le. 
</p>
<p>Logistic Regression, the Computer Teaches Itself to Make Predictions</p>
<p/>
</div>
<div class="page"><p/>
<p>118
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5  Var 6 
</p>
<p> ,00  ,00  67,00  ,00  ,00  ,73 
</p>
<p> 1,00  1,00  54,00  1,00  ,00  ,58 
</p>
<p> 1,00  1,00  65,00  1,00  ,00  ,58 
</p>
<p> 1,00  1,00  74,00  1,00  1,00  ,92 
</p>
<p> 1,00  1,00  73,00  ,00  1,00  ,92 
</p>
<p>  Var 1 department type 
</p>
<p> Var 2 falling out of bed (1 = yes) 
</p>
<p> Var 3 age 
</p>
<p> Var 4 gender 
</p>
<p> Var 5 letter of complaint (1 = yes) 
</p>
<p> Var 6 Predicted Probability 
</p>
<p>        Conclusion 
</p>
<p> The module binary logistic regression can be readily trained to predict probability 
</p>
<p>of falling out of bed both in groups and, with the help of an XML fi le, in individual 
</p>
<p>future patients.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of binary logistic 
</p>
<p>regression is available in Statistics applied to clinical studies 5th edition, Chaps. 17, 
</p>
<p>19, and 65, entitled &ldquo;Logistic and Cox regression, Markov models, Laplace trans-
</p>
<p>formations&rdquo;, &ldquo;Post-hoc analyses in clinical trials&rdquo;, and &ldquo;Odds ratios and multiple 
</p>
<p>regression&rdquo;, pp 199&ndash;218, 227&ndash;231, and 695&ndash;711, Springer Heidelberg Germany 
</p>
<p>2012, from the same authors.  
</p>
<p>    Cox Regression, the Computer Teaches Itself 
</p>
<p>to Make Predictions 
</p>
<p> Var 1  Var 2  Var 3  Var 4 
</p>
<p> 1,00  1,00  ,00  65,00 
</p>
<p> 1,00  1,00  ,00  66,00 
</p>
<p> 2,00  1,00  ,00  73,00 
</p>
<p> 2,00  1,00  ,00  91,00 
</p>
<p>19 Linear, Logistic, and Cox Regression for Outcome Prediction with Unpaired Data&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>119
</p>
<p> Var 1  Var 2  Var 3  Var 4 
</p>
<p> 2,00  1,00  ,00  86,00 
</p>
<p> 2,00  1,00  ,00  87,00 
</p>
<p> 2,00  1,00  ,00  54,00 
</p>
<p> 2,00  1,00  ,00  66,00 
</p>
<p> 2,00  1,00  ,00  64,00 
</p>
<p> 3,00  ,00  ,00  62,00 
</p>
<p> 4,00  1,00  ,00  57,00 
</p>
<p> 5,00  1,00  ,00  85,00 
</p>
<p> 6,00  1,00  ,00  85,00 
</p>
<p>  Var 1 follow up in months 
</p>
<p> Var 2 event (1 = yes) 
</p>
<p> Var 3 treatment modality 
</p>
<p> Var 4 age 
</p>
<p>    Only the fi rst 13 patients are given, the entire data fi le is entitled &ldquo;Coxoutcomeprediction&rdquo; 
</p>
<p>and is in extras.springer.com. 
</p>
<p> SPSS 19.0 is used for analysis, with the help of an XML (eXtended Markup 
</p>
<p>Language) fi le. Start by opening the data fi le.
</p>
<p>  Command: 
</p>
<p>  Click Transform&hellip;.click Random Number Generators&hellip;.click Set Starting Point 
</p>
<p>&hellip;.click Fixed Value (2000000)&hellip;.click OK&hellip;.click Analyze&hellip;.Survival&hellip;.Cox 
</p>
<p>Regression&hellip;.Time: followupmonth&hellip;.Status: event&hellip;.Defi ne event: enter 1&hellip;. 
</p>
<p>Covariates: enter treatment and age&hellip;.click Save&hellip;.mark: Survival function&hellip;. In 
</p>
<p>Export Model information to XML fi le click Browse&hellip;. File name: enter "export-
</p>
<p>Cox"&hellip;.click Save&hellip;.click Continue&hellip;.click OK.    
</p>
<p> Variables in the equation 
</p>
<p> B  SE  Wald  df  Sig.  Exp(B) 
</p>
<p> Treatment  &minus;,791  ,332  5,686  1  ,017  ,454 
</p>
<p> Age  ,028  ,012  5,449  1  ,020  1,028 
</p>
<p>   In the above output table it is shown that both treatment modality and age are 
</p>
<p>signifi cant predictors of survival. Returning to the data fi le we will now observe that 
</p>
<p>SPSS has computed individual probabilities of survival and gave them in a novel 
</p>
<p>variable entitled SUR_1. The probabilities vary from 0.00 to 1.00. E.g., for the fi rst 
</p>
<p>patient, based on follow up of 1 month, treatment modality 0, and age 65, the com-
</p>
<p>puter has computed a mean survival chance at the time of observation of 0.95741 
</p>
<p>(= over 95 %). Other patients had much less probability of survival. If you would 
</p>
<p>have limited sources for further treatment in this population, it would make sense 
</p>
<p>not to burden with continued treatment those with, e.g., less than 20 % survival 
</p>
<p>Cox Regression, the Computer Teaches Itself to Make Predictions</p>
<p/>
</div>
<div class="page"><p/>
<p>120
</p>
<p>probability. We should emphasize that the probability is based on the information of 
</p>
<p>the variables 1, 3, 4, and is assumed to be measured just prior to the event, and the 
</p>
<p>event is not taken into account here. 
</p>
<p> Var 1  Var 2  Var 3  Var 4  SUR_1 
</p>
<p> 1,00  1,00  ,00  65,00  ,95741 
</p>
<p>   The saved XML fi le will now be used to compute the predicted probabilities of 
</p>
<p>survival in 5 novel patients with the following characteristics. For convenience the 
</p>
<p>XML fi le is given in extras.springer.com. We will skip the variable 2 for the above 
</p>
<p>reason. 
</p>
<p> Var 1  Var 2  Var 3  Var 4 
</p>
<p> 30,00  1,00  88,00 
</p>
<p> 29,00  1,00  67,00 
</p>
<p> 29,00  1,00  56,00 
</p>
<p> 29,00  1,00  54,00 
</p>
<p> 28,00  1,00  57,00 
</p>
<p>  Var 1 follow up in months 
</p>
<p> Var 2 event (1 = yes) 
</p>
<p> Var 3 treatment modality 
</p>
<p> Var 4 age 
</p>
<p>    Enter the above data in a new SPSS data fi le.
</p>
<p>  Command: 
</p>
<p>  Utilities&hellip;.click Scoring Wizard&hellip;.click Browse&hellip;.click Select&hellip;.Folder: enter the 
</p>
<p>exportCox.xml fi le&hellip;.click Select&hellip;.in Scoring Wizard click Next&hellip;.mark Predicted 
</p>
<p>Value&hellip;.click Next&hellip;.click Finish.    
</p>
<p> The above data fi le now gives individually predicted probabilities of survival as 
</p>
<p>computed by the Cox regression model with the help of the XML fi le. 
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5 PredictedValue 
</p>
<p> 30,00  1,00  88,00  ,18 
</p>
<p> 29,00  1,00  67,00  ,39 
</p>
<p> 29,00  1,00  56,00  ,50 
</p>
<p> 29,00  1,00  54,00  ,51 
</p>
<p> 28,00  1,00  57,00  ,54 
</p>
<p>  Var 1 follow up in months 
</p>
<p> Var 2 event (1 = yes) 
</p>
<p> Var 3 treatment modality 
</p>
<p> Var 4 age 
</p>
<p> Var 5 predicted probability of survival (0.0&ndash;1.0) 
</p>
<p>19 Linear, Logistic, and Cox Regression for Outcome Prediction with Unpaired Data&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>121
</p>
<p>        Conclusion 
</p>
<p> The module Cox regression can be readily trained to predict probability of survival 
</p>
<p>both in groups and, with the help of an XML fi le, in individual future patients. Like 
</p>
<p>outcome prediction with linear and logistic regression models, Cox regression is an 
</p>
<p>important method to determine with limited health care sources, who of the patients 
</p>
<p>will be recommended expensive medications and other treatments.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of binary logistic 
</p>
<p>regression is available in Statistics applied to clinical studies 5th edition, Chaps. 17 
</p>
<p>and 31, entitled &ldquo;Logistic and Cox regression, Markov models, Laplace transforma-
</p>
<p>tions&rdquo;, and &ldquo;Time-dependent factor analysis&rdquo;, pp 199&ndash;218, and pp 353&ndash;364, 
</p>
<p>Springer Heidelberg Germany 2012, from the same authors.    
</p>
<p> Note</p>
<p/>
</div>
<div class="page"><p/>
<p>123&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_20
</p>
<p>    Chapter 20   
</p>
<p> Generalized Linear Models for Outcome 
Prediction with Paired Data (100 Patients 
and 139 Physicians) 
</p>
<p>                      General Purpose 
</p>
<p> With linear and logistic regression  unpaired  data can be used for outcome predic-
</p>
<p>tion. With generalized linear models  paired  data can be used for the purpose.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Can crossover studies (1) of sleeping pills and (2) of lifestyle treatments be used as 
</p>
<p>training samples to predict hours of sleep and lifestyle treatment in groups and 
</p>
<p>individuals.  
</p>
<p>    Generalized Linear Modeling, the Computer 
</p>
<p>Teaches Itself to Make Predictions 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 1&rdquo; as Chap. 5, 
</p>
<p>2013. 
</p>
<p> Var 1  Var 2  Var 3  Var 4 
</p>
<p> 6,10  79,00  1,00  1,00 
</p>
<p> 5,20  79,00  1,00  2,00 
</p>
<p> 7,00  55,00  2,00  1,00 
</p>
<p> 7,90  55,00  2,00  2,00 
</p>
<p> 8,20  78,00  3,00  1,00 
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>124
</p>
<p>    Only the data from fi rst 6 patients are given, the entire data fi le is entitled &ldquo;generalizedlm-
</p>
<p>pairedcontinuous&rdquo; and is in extras.springer.com. SPSS 19.0 is used for analysis, with the 
</p>
<p>help of an XML (eXtended Markup Language) fi le. Start by opening the data fi le.
</p>
<p>  Command: 
</p>
<p>  Click Transform&hellip;.click Random Number Generators&hellip;.click Set Starting Point 
</p>
<p>&hellip;.click Fixed Value (2000000)&hellip;.click OK&hellip;.click Analyze&hellip;.Generalized Linear 
</p>
<p>Models&hellip;.again click Generalized Linear models&hellip;.click Type of Model&hellip;.click 
</p>
<p>Linear&hellip;.click Response&hellip;.Dependent Variable: enter Outcome&hellip;.Scale Weight 
</p>
<p>Variable: enter patientid&hellip;.click Predictors&hellip;.Factors: enter treatment&hellip;. Covariates: 
</p>
<p>enter age&hellip;.click Model: Model: enter treatment and age&hellip;.click Save: mark 
</p>
<p>Predicted value of linear predictor&hellip;.click Export&hellip;.click Browse&hellip;.File name: 
</p>
<p>enter "exportpairedcontinuous"&hellip;.click Save&hellip;.click Continue&hellip;.click OK.   
</p>
<p> Parameter estimates 
</p>
<p> Parameter  B  Std. Error 
</p>
<p> 95 % Wald 
</p>
<p>confi dence interval  Hypothesis test 
</p>
<p> Lower  Upper 
</p>
<p> Wald 
</p>
<p>Chi- Square  df  Sig. 
</p>
<p> (Intercept)  6,178  ,5171  5,165  7,191  142,763  1  ,000 
</p>
<p> [treatment = 1,00]  2,003  ,2089  1,593  2,412  91,895  1  ,000 
</p>
<p> [treatment = 2,00]  0 a  
</p>
<p> age  &minus;,014  ,0075  &minus;,029  ,001  3,418  1  ,064 
</p>
<p> (Scale)  27,825 b   3,9351  21,089  36,713 
</p>
<p>  Dependent variable: outcome 
</p>
<p> Model: (Intercept), treatment, age 
</p>
<p>  a Set to zero because this parameter is redundant 
</p>
<p>  b Maximum likelihood estimate 
</p>
<p>    The output sheets show that both treatment and age are signifi cant predictors at 
</p>
<p>p &lt; 0.10. Returning to the data fi le we will observe that SPSS has computed  predicted 
</p>
<p>values of hours of sleep, and has given them in a novel variable entitled XBPredicted 
</p>
<p> Var 1  Var 2  Var 3  Var 4 
</p>
<p> 3,90  78,00  3,00  2,00 
</p>
<p> 7,60  53,00  4,00  1,00 
</p>
<p> 4,70  53,00  4,00  2,00 
</p>
<p> 6,50  85,00  5,00  1,00 
</p>
<p> 5,30  85,00  5,00  2,00 
</p>
<p> 8,40  85,00  6,00  1,00 
</p>
<p> 5,40  85,00  6,00  2,00 
</p>
<p>  Var 1 = outcome (hours of sleep after sleeping pill or 
</p>
<p> placebo) 
</p>
<p> Var 2 = age 
</p>
<p> Var 3 = patientnumber (patientid) 
</p>
<p> Var 4 = treatment modality (1 sleeping pill, 2 placebo)  
</p>
<p>20 Generalized Linear Models for Outcome Prediction with Paired Data&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>125
</p>
<p>(predicted values of linear predictor). The saved XML fi le (entitled 
</p>
<p>" exportpairedcontinuous") will now be used to compute the predicted hours of sleep 
</p>
<p>in fi ve novel patients with the following characteristics. For convenience the XML 
</p>
<p>fi le is given in extras.springer.com. 
</p>
<p>    Enter the above data in a new SPSS data fi le.
</p>
<p>  Command: 
</p>
<p>  Utilities&hellip;.click Scoring Wizard&hellip;.click Browse&hellip;.click Select&hellip;.Folder: enter the 
</p>
<p>exportpairedcontinuous.xml fi le&hellip;.click Select&hellip;.in Scoring Wizard click Next&hellip;.
</p>
<p>click Use value substitution&hellip;.click Next&hellip;.click Finish.    
</p>
<p> The above data fi le now gives individually predicted hours of sleep as computed 
</p>
<p>by the linear model with the help of the XML fi le. 
</p>
<p> Var 2  Var 3  Var 4  Var 5 
</p>
<p> 79,00  1,00  1,00  7,09 
</p>
<p> 55,00  2,00  1,00  7,42 
</p>
<p> 78,00  3,00  1,00  7,10 
</p>
<p> 53,00  4,00  2,00  5,44 
</p>
<p> 85,00  5,00  1,00  7,00 
</p>
<p>  Var 2 = age 
</p>
<p> Var 3 = patientnumber (patientid) 
</p>
<p> Var 4 = treatment modality (1 sleeping pill, 2 placebo) 
</p>
<p> Var 5 = predicted values of hours of sleep in individual patient     
</p>
<p>    Conclusion 
</p>
<p> The module generalized linear models can be readily trained to predict hours of 
</p>
<p>sleep in groups, and, with the help of an XML fi le, in individual future patients.  
</p>
<p> Var 2  Var 3  Var 4 
</p>
<p> 79,00  1,00  1,00 
</p>
<p> 55,00  2,00  1,00 
</p>
<p> 78,00  3,00  1,00 
</p>
<p> 53,00  4,00  2,00 
</p>
<p> 85,00  5,00  1,00 
</p>
<p>  Var 2 = age 
</p>
<p> Var 3 = patientnumber (patientid) 
</p>
<p> Var 4 = treatment modality (1 sleeping 
</p>
<p>pill, 2 placebo)  
</p>
<p> Conclusion</p>
<p/>
</div>
<div class="page"><p/>
<p>126
</p>
<p>    Generalized Estimation Equations, the Computer 
</p>
<p>Teaches Itself to Make Predictions 
</p>
<p> Var 1  Var 2  Var 3  Var 4 
</p>
<p> ,00  89,00  1,00  1,00 
</p>
<p> ,00  89,00  1,00  2,00 
</p>
<p> ,00  78,00  2,00  1,00 
</p>
<p> ,00  78,00  2,00  2,00 
</p>
<p> ,00  79,00  3,00  1,00 
</p>
<p> ,00  79,00  3,00  2,00 
</p>
<p> ,00  76,00  4,00  1,00 
</p>
<p> ,00  76,00  4,00  2,00 
</p>
<p> ,00  87,00  5,00  1,00 
</p>
<p> ,00  87,00  5,00  2,00 
</p>
<p> ,00  84,00  6,00  1,00 
</p>
<p> ,00  84,00  6,00  2,00 
</p>
<p> ,00  84,00  7,00  1,00 
</p>
<p> ,00  84,00  7,00  2,00 
</p>
<p> ,00  69,00  8,00  1,00 
</p>
<p> ,00  69,00  8,00  2,00 
</p>
<p> ,00  77,00  9,00  1,00 
</p>
<p> ,00  77,00  9,00  2,00 
</p>
<p> ,00  79,00  10,00  1,00 
</p>
<p> ,00  79,00  10,00  2,00 
</p>
<p>  Var 1 outcome (lifestyle advise given 
</p>
<p>0 = no, 1 = yes) 
</p>
<p> Var 2 physicians&rsquo; age 
</p>
<p> Var 3 physicians&rsquo; id 
</p>
<p> Var 4 prior postgraduate education regarding 
</p>
<p>lifestyle advise (1 = no, 2 = yes) 
</p>
<p>    Only the fi rst 10 physicians are given, the entire data fi le is entitled &ldquo;generalized-
</p>
<p>pairedbinary&rdquo; and is in extras.springer.com. All physicians are assessed twice, once 
</p>
<p>before lifestyle education and once after. The effect of lifestyle education on the 
</p>
<p>willingness to provide lifestyle advise was the main objective of the study. 
</p>
<p> SPSS 19.0 is used for analysis, with the help of an XML (eXtended Markup 
</p>
<p>Language) fi le. Start by opening the data fi le.
</p>
<p>  Command: 
</p>
<p>  Click Transform&hellip;.click Random Number Generators&hellip;.click Set Starting Point 
</p>
<p>&hellip;.click Fixed Value (2000000)&hellip;.click OK&hellip;.click Analyze&hellip;.Generalized Linear 
</p>
<p>Models&hellip;.Generalized Estimating Equations&hellip;.click Repeated&hellip;.in Subjects 
</p>
<p>20 Generalized Linear Models for Outcome Prediction with Paired Data&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>127
</p>
<p> variables enter physicianid&hellip;.in Within-subject variables enter lifestyle advise&hellip;.in 
</p>
<p>Structure enter Unstructured&hellip;.click Type of Model&hellip;.mark Binary logistic&hellip;.click 
</p>
<p>Response&hellip;.in Dependent Variable enter outcome&hellip;.click Reference Category&hellip;.
</p>
<p>mark First&hellip;.click Continue&hellip;.click Predictors&hellip;.in Factors enter lifestyleadvise&hellip;.
</p>
<p>in Covariates enter age&hellip;.click Model&hellip;.in Model enter lifestyle and age&hellip;.click 
</p>
<p>Save&hellip;.mark Predicted value of mean of response&hellip;.click Export &hellip;.mark Export 
</p>
<p>model in XML&hellip;.click Browse&hellip;. In File name: enter "exportpairedbinary"&hellip;.in 
</p>
<p>Look in: enter the appropriate map in your computer for storage&hellip;.click Save&hellip;.
</p>
<p>click Continue&hellip;.click OK.   
</p>
<p> Parameter estimates 
</p>
<p> Parameter  B  Std. Error 
</p>
<p> 95 % Wald 
</p>
<p>confi dence 
</p>
<p>interval  Hypothesis test 
</p>
<p> Lower  Upper 
</p>
<p> Wald 
</p>
<p>Chi- Square  df  Sig. 
</p>
<p> (Intercept)  2,469  ,7936  ,913  4,024  9,677  1  ,002 
</p>
<p> [lifestyleadvise = 1,00]  &minus;,522  ,2026  &minus;,919  &minus;,124  6,624  1  ,010 
</p>
<p> [lifestyleadvise = 2,00]  0 a  
</p>
<p> age  &minus;,042  ,0130  &minus;.068  &minus;,017  10,563  1  ,001 
</p>
<p> (Scale)  1 
</p>
<p>  Dependent variable: outcome 
</p>
<p> Model: (Intercept), lifestyleadvise, age 
</p>
<p>  a Set to zero because this parameter is redundant 
</p>
<p>    The output sheets show that both prior lifestyle education and physicians&rsquo; age are 
</p>
<p>very signifi cant predictors at p &lt; 0.01. Returning to the data fi le we will observe that 
</p>
<p>SPSS has computed predicted probabilities of lifestyle advise given or not by each 
</p>
<p>physician in the data fi le, and a novel variable is added to the data fi le for the pur-
</p>
<p>pose. It is given the name MeanPredicted. The saved XML fi le entitled &ldquo;export-
</p>
<p>pairedbinary&rdquo; will now be used to compute the predicted probability of receiving 
</p>
<p>lifestyle advise based on physicians&rsquo; age and the physicians&rsquo; prior lifestyle educa-
</p>
<p>tion in twelve novel physicians. For convenience the XML fi le is given in extras.
</p>
<p>springer.com. 
</p>
<p> Var 2  Var 3  Var 4 
</p>
<p> 64,00  1,00  2,00 
</p>
<p> 64,00  2,00  1,00 
</p>
<p> 65,00  3,00  1,00 
</p>
<p> 65,00  3,00  2,00 
</p>
<p> 52,00  4,00  1,00 
</p>
<p> 66,00  5,00  1,00 
</p>
<p> 79,00  6,00  1,00 
</p>
<p> 79,00  6,00  2,00 
</p>
<p>Generalized Estimation Equations, the Computer Teaches Itself to Make Predictions</p>
<p/>
</div>
<div class="page"><p/>
<p>128
</p>
<p> 53,00  7,00  1,00 
</p>
<p> 53,00  7,00  2,00 
</p>
<p> 55,00  8,00  1,00 
</p>
<p> 46,00  9,00  1,00 
</p>
<p>  Var 2 age 
</p>
<p> Var 3 physicianid 
</p>
<p> Var 4 lifestyleadvise (prior 
</p>
<p>postgraduate education regarding 
</p>
<p>lifestyle advise (1 = no, 2 = yes)) 
</p>
<p>    Enter the above data in a new SPSS data fi le.
</p>
<p>  Command: 
</p>
<p>  Utilities&hellip;.click Scoring Wizard&hellip;.click Browse&hellip;.click Select&hellip;.Folder: enter the 
</p>
<p>exportpairedbinary.xml fi le&hellip;.click Select&hellip;.in Scoring Wizard click Next&hellip;.mark 
</p>
<p>Probability of Predicted Category&hellip;.click Next&hellip;.click Finish.    
</p>
<p> The above data fi le now gives individually predicted probabilities of receiving 
</p>
<p>lifestyle advise as computed by the logistic model with the help of the XML fi le. 
</p>
<p> Var 2  Var 3  Var 4  Var 5 
</p>
<p> 64,00  1,00  2,00  ,56 
</p>
<p> 64,00  2,00  1,00  ,68 
</p>
<p> 65,00  3,00  1,00  ,69 
</p>
<p> 65,00  3,00  2,00  ,57 
</p>
<p> 52,00  4,00  1,00  ,56 
</p>
<p> 66,00  5,00  1,00  ,70 
</p>
<p> 79,00  6,00  1,00  ,80 
</p>
<p> 79,00  6,00  2,00  ,70 
</p>
<p> 53,00  7,00  1,00  ,57 
</p>
<p> 53,00  7,00  2,00  ,56 
</p>
<p> 55,00  8,00  1,00  ,59 
</p>
<p> 46,00  9,00  1,00  ,50 
</p>
<p>  Var 2 age 
</p>
<p> Var 3 physicianid 
</p>
<p> Var 4 lifestyleadvise 
</p>
<p> Var 5 probability of predicted category (between 0.0 and 1.0) 
</p>
<p>20 Generalized Linear Models for Outcome Prediction with Paired Data&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>129
</p>
<p>        Conclusion 
</p>
<p> The module generalized estimating equations can be readily trained to predict with 
</p>
<p>paired data the probability of physicians giving lifestyle advise as groups and, with 
</p>
<p>the help of an XML fi le, as individual physicians.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of paired analysis of 
</p>
<p>binary data is given in SPSS for starters part one, Chap. 13, entitled &ldquo;Paired binary 
</p>
<p>(McNemar test)&rdquo;, pp 47&ndash;49, Springer Heidelberg Germany, 2010, from the same 
</p>
<p>authors.    
</p>
<p> Note</p>
<p/>
</div>
<div class="page"><p/>
<p>131&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_21
</p>
<p>    Chapter 21   
</p>
<p> Generalized Linear Models Event-Rates 
</p>
<p>(50 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> To assess whether in a longitudinal study event rates, defi ned as numbers of events 
</p>
<p>per person per period, can be analyzed with the generalized linear model module.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Can generalized linear modeling be trained to predict rates of episodes of paroxysmal 
</p>
<p>atrial fi brillation both in groups and in individual future patients.  
</p>
<p>    Example 
</p>
<p> Fifty patients were followed for numbers of episodes of paroxysmal atrial 
</p>
<p>fibrillation (PAF), while on treated with two parallel treatment modalities. The data 
</p>
<p>fi le is below. 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 1&rdquo; as Chap. 6, 
</p>
<p>2013. </p>
<p/>
</div>
<div class="page"><p/>
<p>132
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5 
</p>
<p> 1  56,99  42,45  73  4 
</p>
<p> 1  37,09  46,82  73  4 
</p>
<p> 0  32,28  43,57  76  2 
</p>
<p> 0  29,06  43,57  74  3 
</p>
<p> 0  6,75  27,25  73  3 
</p>
<p> 0  61,65  48,41  62  13 
</p>
<p> 0  56,99  40,74  66  11 
</p>
<p> 1  10,39  15,36  72  7 
</p>
<p> 1  50,53  52,12  63  10 
</p>
<p> 1  49,47  42,45  68  9 
</p>
<p> 0  39,56  36,45  72  4 
</p>
<p> 1  33,74  13,13  74  5 
</p>
<p>  Var 1 = treatment modality 
</p>
<p> Var 2 = psychological score 
</p>
<p> Var 3 = social score 
</p>
<p> Var 4 = days of observation 
</p>
<p> Var 5 = number of episodes of paroxysmal atrial fi brillation (PAF) 
</p>
<p>    The fi rst 12 patients are shown only, the entire data fi le is entitled &ldquo;gene-
</p>
<p>ralizedlmeventrates&rdquo; and is in extras.springer.com.  
</p>
<p>    The Computer Teaches Itself to Make Predictions 
</p>
<p> SPSS 19.0 is used for training and outcome prediction. It uses XML (eXtended 
</p>
<p>Markup Language) fi les to store data. We will perform the analysis with a linear 
</p>
<p>regression analysis of variable 5 as outcome variable and the other 4 variables as 
</p>
<p>predictors. Start by opening the data fi le.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Regression&hellip;.Linear&hellip;.Dependent Variable: episodes of paroxysmal 
</p>
<p>atrial fi brillation&hellip;.Independent: treatment modality, psychological score, social 
</p>
<p>score, days of observation&hellip;.OK.   
</p>
<p> Coeffi cients a  
</p>
<p> Unstandardized coeffi cients 
</p>
<p> Standardized 
</p>
<p>coeffi cients 
</p>
<p> Model  B  Std. Error  Beta  t  Sig. 
</p>
<p> 1  (Constant)  49,059  5,447  9,006  ,000 
</p>
<p> Treat  &minus;2,914  1,385  &minus;,204  &minus;2,105  ,041 
</p>
<p> Psych  ,014  ,052  ,036  ,273  ,786 
</p>
<p>(continued)
</p>
<p>21 Generalized Linear Models Event-Rates (50 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>133
</p>
<p> Coeffi cients a  
</p>
<p> Unstandardized coeffi cients 
</p>
<p> Standardized 
</p>
<p>coeffi cients 
</p>
<p> Model  B  Std. Error  Beta  t  Sig. 
</p>
<p> Soc  &minus;,073  ,058  &minus;,169  &minus;1,266  ,212 
</p>
<p> Days  &minus;,557  ,074  &minus;,715  &minus;7,535  ,000 
</p>
<p>   a Dependent variable: paf 
</p>
<p>    The above table shows that treatment modality is weakly signifi cant, and psycho-
</p>
<p>logical and social scores are not. Furthermore, days of observation is very signifi -
</p>
<p>cant. However, it is not entirely appropriate to include this variable if your outcome 
</p>
<p>is the numbers of events per person per time unit. Therefore, we will perform a 
</p>
<p>linear regression, and adjust the outcome variable for the differences in days of 
</p>
<p>observation using weighted least square regression.
</p>
<p> Coeffi cients a,b  
</p>
<p> Model 
</p>
<p> Unstandardized coeffi cients 
</p>
<p> Standardized 
</p>
<p>coeffi cients 
</p>
<p> B  Std. Error  Beta  t  Sig. 
</p>
<p> 1  (Constant)  10,033  2,862  3,506  ,001 
</p>
<p> Treat  &minus;3,502  1,867  &minus;.269  &minus;1,876  ,067 
</p>
<p> Psych  ,033  ,069  ,093  ,472  ,639 
</p>
<p> Soc  &minus;,093  ,078  &minus;,237  &minus;1,194  ,238 
</p>
<p>   a Dependent variable: paf 
</p>
<p>  b Weighted least squares regression -Weighted by days 
</p>
<p>    Command: 
</p>
<p>  Analyze&hellip;.Regression&hellip;.Linear&hellip;.Dependent: episodes of paroxysmal atrial fi bril-
</p>
<p>lation&hellip;.Independent: treatment modality, psychological score, social score &hellip;.
</p>
<p>WLS Weight: days of observation&hellip;. OK.    
</p>
<p> The above table shows the results. A largely similar pattern is observed, but 
</p>
<p>treatment modality is no more statistically signifi cant. We will use the generalized 
</p>
<p>linear modeling module to perform a Poisson regression which is more appropriate 
</p>
<p>for rate data. The model applied will also be stored and reapplied for making predic-
</p>
<p>tions about event rates in individual future patients.
</p>
<p>  Command: 
</p>
<p>  Click Transform&hellip;.click Random Number Generators&hellip;.click Set Starting Point&hellip;. 
</p>
<p>click Fixed Value (2000000)&hellip;.click OK&hellip;.click Generalized Linear Models &hellip;.
</p>
<p>click again Generalized Linear Models&hellip;.mark: Custom&hellip;.Distribution: Poisson&hellip;..
</p>
<p>Link function: Log&hellip;.Response: Dependent variable: numbers of episodes of 
</p>
<p>PAF&hellip;.Scale Weight Variable: days of observation&hellip;.Predictors: Main Effect: treat-
</p>
<p>ment modality&hellip;.Covariates: psychological score, social score&hellip;. Model: main 
</p>
<p>The Computer Teaches Itself to Make Predictions</p>
<p/>
</div>
<div class="page"><p/>
<p>134
</p>
<p>effects: treatment modality, psychological score, social score&hellip;. Estimation: mark 
</p>
<p>Model-based Estimation &hellip;.click Save&hellip;.mark Predicted value of mean of 
</p>
<p>response&hellip;.click Export&hellip;.mark Export model in XML&hellip;.click Browse&hellip;. in File 
</p>
<p>name enter "exportrate"&hellip;.in Look in: enter the appropriate map in your computer 
</p>
<p>for storage&hellip;.click Save&hellip;.click OK.   
</p>
<p> Parameter estimates 
</p>
<p> Parameter  B 
</p>
<p> Std. 
</p>
<p>Error 
</p>
<p> 95 % Wald 
</p>
<p>confi dence interval  Hypothesis test 
</p>
<p> Lower  Upper  Wald Chi- Square  df  Sig. 
</p>
<p> (Intercept)  1,868  ,0206  1,828  1,909  8256,274  1  ,000 
</p>
<p> [treat = 0]  ,667  ,0153  ,637  ,697  1897,429  1  ,000 
</p>
<p> [treat = 1]  0 a  
</p>
<p> psych  ,006  ,0006  ,005  ,008  120,966  1  ,000 
</p>
<p> soc  &minus;,019  ,0006  &minus;.020  &minus;,017  830,264  1  ,000 
</p>
<p> (Scale)  1 b  
</p>
<p>  Dependent variable: paf 
</p>
<p> Model: (Intercept), treat, psych, soc 
</p>
<p>  a Set to zero because this parameter is redundant 
</p>
<p>  b Fixed at the displayed value 
</p>
<p>    The outcome sheets give the results. All of a sudden, all of the predictors including 
</p>
<p>treatment modality, psychological and social score are very signifi cant predictors of 
</p>
<p>the PAF rate. When minimizing the output sheets the data fi le returns and now 
</p>
<p>shows a novel variable entitled &ldquo;PredictedValues&rdquo; with the mean rates of PAF 
</p>
<p>episodes per patient (per day). The saved XML fi le will now be used to compute the 
</p>
<p>predicted PAF rate in 5 novel patients with the following characteristics. For con-
</p>
<p>venience the XML fi le is given in extras.springer.com. 
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5 
</p>
<p> 1,00  56,99  42,45  73,00  4,00 
</p>
<p> 1,00  30,09  46,82  34,00  4,00 
</p>
<p> ,00  32,28  32,00  76,00  2,00 
</p>
<p> ,00  29,06  40,00  36,00  3,00 
</p>
<p> ,00  6,75  27,25  73,00  3,00 
</p>
<p>  Var 1 = treatment modality 
</p>
<p> Var 2 = psychological score 
</p>
<p> Var 3 = social score 
</p>
<p> Var 4 = days of observation 
</p>
<p> Var 5 = number of episodes of paroxysmal atrial fi brillation (PAF) 
</p>
<p>    Enter the above data in a new SPSS data fi le.
</p>
<p>21 Generalized Linear Models Event-Rates (50 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>135
</p>
<p>  Command: 
</p>
<p>  Utilities&hellip;.click Scoring Wizard&hellip;.click Browse&hellip;.click Select&hellip;.Folder: enter the 
</p>
<p>exportrate.xml fi le&hellip;.click Select&hellip;.in Scoring Wizard click Next&hellip;.click Use value 
</p>
<p>substitution&hellip;.click Next&hellip;.click Finish.    
</p>
<p> The above data fi le now gives individually predicted rates of PAF as computed 
</p>
<p>by the linear model with the help of the XML fi le. Enter the above data in a new 
</p>
<p>SPSS data fi le. 
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5  Var 6 
</p>
<p> 1,00  56,99  42,45  73,00  4,00  4,23 
</p>
<p> 1,00  30,09  46,82  34,00  4,00  3,27 
</p>
<p> ,00  32,28  32,00  76,00  2,00  8,54 
</p>
<p> ,00  29,06  40,00  36,00  3,00  7,20 
</p>
<p> ,00  6,75  27,25  73,00  3,00  7,92 
</p>
<p>  Var 1 = treatment modality 
</p>
<p> Var 2 = psychological score 
</p>
<p> Var 3 = social score 
</p>
<p> Var 4 = days of observation 
</p>
<p> Var 5 = number of episodes of paroxysmal atrial fi brillation (PAF) 
</p>
<p> Var 6 = individually predicted mean rates of PAF (per day) 
</p>
<p>        Conclusion 
</p>
<p> The module generalized linear models can be readily trained to predict event rate of 
</p>
<p>PAF episodes both in groups, and, with the help of an XML fi le, in individual 
</p>
<p>patients.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of generalized linear 
</p>
<p>modeling is available in SPSS for Starters part two, Chap. 10, entitled &ldquo;Poisson 
</p>
<p>regression&rdquo;, pp 43&ndash;48, Springer Heidelberg Germany 2012, from the same authors.    
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>137&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_22
</p>
<p>    Chapter 22   
</p>
<p> Factor Analysis and Partial Least Squares 
(PLS) for Complex-Data Reduction 
(250 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> A few unmeasured factors, otherwise called latent factors, are identifi ed to explain 
</p>
<p>a much larger number of measured factors, e.g., highly expressed chromosome- 
</p>
<p>clustered genes. Unlike factor analysis, partial least squares (PLS) identifi es not 
</p>
<p>only exposure (x-value), but also outcome (y-value) variables.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Twelve highly expressed genes are used to predict drug effi cacy. Is factor analysis/ 
</p>
<p>PLS better than traditional analysis for regression data with multiple exposure and 
</p>
<p>outcome variables. 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 1&rdquo; as Chap. 7, 
</p>
<p>2013. </p>
<p/>
</div>
<div class="page"><p/>
<p>138
</p>
<p> G1  G2  G3  G4  G16  G17  G18  G19  G24  G25  G26  G27  O1  O2  O3  O4 
</p>
<p> 8  8  9  5  7  10  5  6  9  9  6  6  6  7  6  7 
</p>
<p> 9  9  10  9  8  8  7  8  8  9  8  8  8  7  8  7 
</p>
<p> 9  8  8  8  8  9  7  8  9  8  9  9  9  8  8  8 
</p>
<p> 8  9  8  9  6  7  6  4  6  6  5  5  7  7  7  6 
</p>
<p> 10  10  8  10  9  10  10  8  8  9  9  9  8  8  8  7 
</p>
<p> 7  8  8  8  8  7  6  5  7  8  8  7  7  6  6  7 
</p>
<p> 5  5  5  5  5  6  4  5  5  6  6  5  6  5  6  4 
</p>
<p> 9  9  9  9  8  8  8  8  9  8  3  8  8  8  8  8 
</p>
<p> 9  8  9  8  9  8  7  7  7  7  5  8  8  7  6  6 
</p>
<p> 10  10  10  10  10  10  10  10  10  8  8  10  10  10  9  10 
</p>
<p> 2  2  8  5  7  8  8  8  9  3  9  8  7  7  7  6 
</p>
<p> 7  8  8  7  8  6  6  7  8  8  8  7  8  7  8  8 
</p>
<p> 8  9  9  8  10  8  8  7  8  8  9  9  7  7  8  8 
</p>
<p>  Var G1-27 highly expressed genes estimated from their arrays&rsquo; normalized ratios 
</p>
<p> Var O1-4 drug effi cacy scores (the variables 20&ndash;23 from the initial data fi le) 
</p>
<p>    The data from the fi rst 13 patients are shown only (see extras.springer.com for 
</p>
<p>the entire data fi le entitled &ldquo;optscalingfactorplscanonical&rdquo;).  
</p>
<p>    Factor Analysis 
</p>
<p> First the reliability of the model was assessed by assessing the test-retest reliability 
</p>
<p>of the original predictor variables using the correlation coeffi cients after deletion of 
</p>
<p>one variable: all of the data fi les should produce at least by 80 % the same result as 
</p>
<p>that of the non-deleted data fi le (alphas &gt; 80 %). SPSS 19.0 is used. Start by opening 
</p>
<p>the data fi le.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Scale&hellip;.Reliability Analysis&hellip;.transfer original variables to Variables 
</p>
<p>box&hellip;.click Statistics&hellip;.mark Scale if item deleted&hellip;.mark Correlations &hellip;.
</p>
<p>Continue&hellip;.OK.   
</p>
<p> Item-total statistics 
</p>
<p> Scale mean 
</p>
<p>if item 
</p>
<p>deleted 
</p>
<p> Scale 
</p>
<p>variance if 
</p>
<p>item 
</p>
<p>deleted 
</p>
<p> Corrected 
</p>
<p>item-total 
</p>
<p>correlation 
</p>
<p> Squared 
</p>
<p>multiple 
</p>
<p>correlation 
</p>
<p> Cronbach&rsquo;s 
</p>
<p>alpha if item 
</p>
<p>deleted 
</p>
<p> Geneone  80,8680  276,195  ,540  ,485  ,902 
</p>
<p> Genetwo  80,8680  263,882  ,700  ,695  ,895 
</p>
<p> Genethree  80,7600  264,569  ,720  ,679  ,895 
</p>
<p> Genefour  80,7960  282,002  ,495  ,404  ,904 
</p>
<p>(continued)
</p>
<p>22 Factor Analysis and Partial Least Squares (PLS) for Complex-Data Reduction&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>139
</p>
<p> Item-total statistics 
</p>
<p> Scale mean 
</p>
<p>if item 
</p>
<p>deleted 
</p>
<p> Scale 
</p>
<p>variance if 
</p>
<p>item 
</p>
<p>deleted 
</p>
<p> Corrected 
</p>
<p>item-total 
</p>
<p>correlation 
</p>
<p> Squared 
</p>
<p>multiple 
</p>
<p>correlation 
</p>
<p> Cronbach&rsquo;s 
</p>
<p>alpha if item 
</p>
<p>deleted 
</p>
<p> Genesixteen  81,6200  258,004  ,679  ,611  ,896 
</p>
<p> Geneseventeen  80,9800  266,196  ,680  ,585  ,896 
</p>
<p> Geneeighteen  81,5560  263,260  ,606  ,487  ,899 
</p>
<p> Genenineteen  82,2040  255,079  ,696  ,546  ,895 
</p>
<p> Genetwentyfour  81,5280  243,126  ,735  ,632  ,893 
</p>
<p> Genetwentyfi ve  81,2680  269,305  ,538  ,359  ,902 
</p>
<p> Genetwentysix  81,8720  242,859  ,719  ,629  ,894 
</p>
<p> Genetwentyseven  81,0720  264,501  ,540  ,419  ,903 
</p>
<p>   None of the original variables after deletion reduce the test-retest reliability. The 
</p>
<p>data are reliable. We will now perform the principal components analysis with three 
</p>
<p>components, otherwise called latent variables.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Dimension Reduction&hellip;.Factor&hellip;.enter variables into Variables box&hellip;.
</p>
<p>click Extraction&hellip;.Method: click Principle Components&hellip;.mark Correlation Matrix, 
</p>
<p>Unrotated factor solution&hellip;.Fixed number of factors: enter 3&hellip;.Maximal Iterations 
</p>
<p>plot Convergence: enter 25&hellip;.Continue&hellip;.click Rotation&hellip;.Method: click Varimax&hellip;.
</p>
<p>mark Rotated solution&hellip;.mark Loading Plots&hellip;.Maximal Iterations: enter 25&hellip;.
</p>
<p>Continue&hellip;.click Scores&hellip;. mark Display factor score coeffi cient matrix &hellip;.OK.   
</p>
<p> Rotated component matrix a  
</p>
<p> Component 
</p>
<p> 1  2  3 
</p>
<p> Geneone  ,211  ,810  ,143 
</p>
<p> Genetwo  ,548  ,683  ,072 
</p>
<p> Genethree  ,624  ,614  ,064 
</p>
<p> Genefour  ,033  ,757  ,367 
</p>
<p> Genesixteen  ,857  ,161  ,090 
</p>
<p> Geneseventeen  ,650  ,216  ,338 
</p>
<p> Geneeighteen  ,526  ,297  ,318 
</p>
<p> Genenineteen  ,750  ,266  ,170 
</p>
<p> Genetwentyfour  ,657  ,100  ,539 
</p>
<p> Genetwentyfi ve  ,219  ,231  ,696 
</p>
<p> Genetwentysix  ,687  ,077  ,489 
</p>
<p> Genetwentyseven  ,188  ,159  ,825 
</p>
<p>  Extraction method: Principal component analysis 
</p>
<p> Rotation method: Varimax with Kaiser normalization 
</p>
<p>  a Rotation converged in eight iterations 
</p>
<p>Factor Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>140
</p>
<p>    The best fi t coeffi cients of the original variables constituting three new factors 
</p>
<p>(unmeasured, otherwise called latent, factors) are given. The latent factor 1 has a 
</p>
<p>very strong correlation with the genes 16&ndash;19, the latent factor 2 with the genes 1&ndash;4, 
</p>
<p>and the latent factor 3 with the genes 24&ndash;27. 
</p>
<p> When returning to the data fi le, we now observe, that, for each patient, the soft-
</p>
<p>ware program has produced the individual values of these novel predictors. 
</p>
<p> In order to fi t these novel predictors with the outcome variables, the drug effi cacy 
</p>
<p>scores (variables O1-4), multivariate analysis of variance (MANOVA) should be 
</p>
<p>appropriate. However, the large number of columns in the design matrix caused inte-
</p>
<p>ger overfl ow, and the command was not executed. Instead we will perform a univari-
</p>
<p>ate multiple linear regression with the add-up scores of the outcome variables (using 
</p>
<p>the Transform and Compute Variable command) as novel outcome variable.
</p>
<p>  Command: 
</p>
<p>  Transform&hellip;.Compute Variable&hellip;.transfer outcomeone to Numeric Expression 
</p>
<p>box&hellip;.click + &hellip;.outcometwo idem&hellip;.click + &hellip;.outcomethree idem&hellip;.click + &hellip;. 
</p>
<p>outcomefour idem&hellip;.Target Variable: enter "summaryoutcome"&hellip;.click OK.    
</p>
<p> In the data fi le the summaryoutcome values are displayed as a novel variable.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Regression&hellip;.Dependent: enter summaryoutcome&hellip;.Independent: enter 
</p>
<p>Fac 1, Fac 2, and Fac 3&hellip;.click OK.   
</p>
<p> Coeffi cients a  
</p>
<p> Model 
</p>
<p> Unstandardized 
</p>
<p>coeffi cients 
</p>
<p> Standardized 
</p>
<p>coeffi cients 
</p>
<p> B  Std. error  Beta  t  Sig. 
</p>
<p> 1  (Constant)  27,332  ,231  118,379  ,000 
</p>
<p> REGR factor score 1 for 
</p>
<p>analysis 1 
</p>
<p> 5,289  ,231  ,775  22,863  ,000 
</p>
<p> REGR factor score 2 for 
</p>
<p>analysis 1 
</p>
<p> 1,749  ,231  ,256  7,562  ,000 
</p>
<p> REGR factor score 3 for 
</p>
<p>analysis 1 
</p>
<p> 1,529  ,231  ,224  6,611  ,000 
</p>
<p>   a Dependent variable: summaryoutcome 
</p>
<p>    All of the three latent predictors were, obviously, very signifi cant predictors of 
</p>
<p>the summary outcome variable.  
</p>
<p>    Partial Least Squares Analysis (PLS) 
</p>
<p> Because PLS is not available in the basic and regression modules of SPSS, the soft-
</p>
<p>ware program R Partial Least Squares, a free statistics and forecasting software 
</p>
<p>available on the internet as a free online software calculator was used (  www.wessa.
</p>
<p>22 Factor Analysis and Partial Least Squares (PLS) for Complex-Data Reduction&hellip;</p>
<p/>
<div class="annotation"><a href="http://www.wessa.net/rwasp">http://www.wessa.net/rwasp</a></div>
</div>
<div class="page"><p/>
<p>141
</p>
<p>net/rwasp    ). The data fi le is imported directly from the SPSS fi le entitled &ldquo;optscal-
</p>
<p>ingfactorplscanonical&rdquo; (cut/past commands).
</p>
<p>  Command: 
</p>
<p>  List the selected clusters of variables: latent variable 2 (here G16-19), latent vari-
</p>
<p>able 1 (here G24-27), latent variable 4 (here G1-4), and latent outcome variable 3 
</p>
<p>(here O 1-4).    
</p>
<p> A square boolean matrix is constructed with &ldquo;0 or 1&rdquo; values if fi tted correlation 
</p>
<p>coeffi cients to be included in the model were &ldquo;no or yes&rdquo; according to the under-
</p>
<p>neath table. 
</p>
<p> Latent variable  1  2  3  4 
</p>
<p> Latent variable  1  0  0  0  0 
</p>
<p> 2  0  0  0  0 
</p>
<p> 3  1  1  0  0 
</p>
<p> 4  0  0  1  0 
</p>
<p>   Click &ldquo;compute&rdquo;. After 15 s of computing the program produces the results. 
</p>
<p>First, the data were validated using the GoF (goodness of fi t) criteria. GoF = 
</p>
<p>&radic; [mean of r-square values of comparisons in model * r-square overall model], 
where * is the sign of multiplication. A GoF value varies from 0 to 1 and values 
</p>
<p>larger than 0.8 indicate that the data are adequately reliable for modeling. 
</p>
<p> GoF value 
</p>
<p> Overall  0.9459 
</p>
<p> Outer model (including manifest variables)  0.9986 
</p>
<p> Inner model (including latent variables)  0.9466. 
</p>
<p>   The data are, thus, adequately reliable. The calculated best fi t r-values (correla-
</p>
<p>tion coeffi cients) are estimated from the model, and their standard errors would be 
</p>
<p>available from second derivatives. However, the problem with the second deriva-
</p>
<p>tives is that they require very large data fi les in order to be accurate. Instead, distri-
</p>
<p>bution free standard errors are calculated using bootstrap resampling. 
</p>
<p> Latent variables  Original r-value  Bootstrap r-value  Standard error  t-value 
</p>
<p> 1 versus 3  0.57654  0.57729  0.08466  6.8189 
</p>
<p> 2 versus 3  0.67322  0.67490  0.04152  16.2548 
</p>
<p> 4 versus 3  0.18322  0.18896  0.05373  3.5168 
</p>
<p>   All of the three correlation coeffi cients (r-values) are very signifi cant predictors 
</p>
<p>of the latent outcome variable.  
</p>
<p>Partial Least Squares Analysis (PLS)</p>
<p/>
<div class="annotation"><a href="http://www.wessa.net/rwasp">http://www.wessa.net/rwasp</a></div>
</div>
<div class="page"><p/>
<p>142
</p>
<p>    Traditional Linear Regression 
</p>
<p> When using the summary scores of the main components of the three latent vari-
</p>
<p>ables instead of the above modeled latent variables (using the above Transform and 
</p>
<p>Compute Variable commands), the effects remained statistically signifi cant, how-
</p>
<p>ever, at lower levels of signifi cance.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Regression&hellip;.Linear&hellip;.Dependent: enter summaryoutcome&hellip;. 
</p>
<p>Independent: enter the three summary factors 1-3&hellip;.click OK.   
</p>
<p> Coeffi cients a  
</p>
<p> Model 
</p>
<p> Unstandardized 
</p>
<p>coeffi cients 
</p>
<p> Standardized 
</p>
<p>coeffi cients 
</p>
<p> B  Std. Error  Beta  t  Sig. 
</p>
<p> 1  (Constant)  1,177  1,407  ,837  ,404 
</p>
<p> Summaryfac1  ,136  ,059  ,113  2,316  ,021 
</p>
<p> Summaryfac2  ,620  ,054  ,618  11,413  ,000 
</p>
<p> Summaryfac3  ,150  ,044  ,170  3,389  ,001 
</p>
<p>   a Dependent variable: summaryoutcome 
</p>
<p>    The partial least squares method produces smaller t-values than did factor analy-
</p>
<p>sis (t = 3.5&ndash;16.3 versus 6.6&ndash;22.9), but it is less biased, because it is a multivariate 
</p>
<p>analysis adjusting relationships between the outcome variables. Both methods pro-
</p>
<p>vided better t-values than did the above traditional regression analysis of summary 
</p>
<p>variables (t = 2.3&ndash;11.4).  
</p>
<p>    Conclusion 
</p>
<p> Factor analysis and PLS can handle many more variables than the standard meth-
</p>
<p>ods, and account the relative importance of the separate variables, their interactions 
</p>
<p>and differences in units. Partial least squares method is parsimonious to principal 
</p>
<p>components analysis, because it can separately include outcome variables in the 
</p>
<p>model.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of the three methods is 
</p>
<p>given in Machine learning in medicine part one, Chaps. 14 and 16, Factor analysis 
</p>
<p>pp 167&ndash;181, and Partial least squares, pp 197&ndash;212, Springer Heidelberg Germany 
</p>
<p>2013, from the same authors.    
</p>
<p>22 Factor Analysis and Partial Least Squares (PLS) for Complex-Data Reduction&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>143&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_23
</p>
<p>    Chapter 23   
</p>
<p> Optimal Scaling of High-Sensitivity Analysis 
of Health Predictors (250 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> In linear models of health predictors (x-values) and health outcomes (y-values), bet-
</p>
<p>ter power of testing can sometimes be obtained, if continuous predictor variables are 
</p>
<p>converted into the best fi t discretized ones.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Highly expressed genes were used to predict drug effi cacy. The example from chap. 
</p>
<p>22 was used once more. The gene expression levels were scored on a scale of 0&ndash;10, 
</p>
<p>but some scores were rarely observed. Can the strength of prediction be improved 
</p>
<p>by optimal scaling. 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 1&rdquo; as Chap. 8, 
</p>
<p>2013. </p>
<p/>
</div>
<div class="page"><p/>
<p>144
</p>
<p> G1  G2  G3  G4  G16  G17  G18  G19  G24  G25  G26  G27  O1  O2  O3  O4 
</p>
<p> 8  8  9  5  7  10  5  6  9  9  6  6  6  7  6  7 
</p>
<p> 9  9  10  9  8  8  7  8  8  9  8  8  8  7  8  7 
</p>
<p> 9  8  8  8  8  9  7  8  9  8  9  9  9  8  8  8 
</p>
<p> 8  9  8  9  6  7  6  4  6  6  5  5  7  7  7  6 
</p>
<p> 10  10  8  10  9  10  10  8  8  9  9  9  8  8  8  7 
</p>
<p> 7  8  8  8  8  7  6  5  7  8  8  7  7  6  6  7 
</p>
<p> 5  5  5  5  5  6  4  5  5  6  6  5  6  5  6  4 
</p>
<p> 9  9  9  9  8  8  8  8  9  8  3  8  8  8  8  8 
</p>
<p> 9  8  9  8  9  8  7  7  7  7  5  8  8  7  6  6 
</p>
<p> 10  10  10  10  10  10  10  10  10  8  8  10  10  10  9  10 
</p>
<p> 2  2  8  5  7  8  8  8  9  3  9  8  7  7  7  6 
</p>
<p> 7  8  8  7  8  6  6  7  8  8  8  7  8  7  8  8 
</p>
<p> 8  9  9  8  10  8  8  7  8  8  9  9  7  7  8  8 
</p>
<p>  Var G1-27 highly expressed genes estimated from their arrays&rsquo; normalized ratios 
</p>
<p> Var O1-4 drug effi cacy scores (sum of the scores is used as outcome) 
</p>
<p>    Only the data from the fi rst 13 patients are shown. The entire data fi le entitled 
</p>
<p>&ldquo;optscalingfactorplscanonical&rdquo; can be downloaded from extra.springer.com.  
</p>
<p>    Traditional Multiple Linear Regression 
</p>
<p> SPSS 19.0 is used for data analysis. Open the data fi le and command.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Regression&hellip;.Linear&hellip;.Dependent: enter the 12 highly expressed 
</p>
<p>genes&hellip;.Independent: enter the summary scores of the 4 outcome variables (use 
</p>
<p>Transform and Compute Variable command)&hellip;.click OK.   
</p>
<p> Coeffi cients a  
</p>
<p> Model 
</p>
<p> Unstandardized 
</p>
<p>coeffi cients 
</p>
<p> Standardized 
</p>
<p>coeffi cients 
</p>
<p> B  Std. Error  Beta  t  Sig. 
</p>
<p> 1  (Constant)  3,293  1,475  2,232  ,027 
</p>
<p> Geneone  &minus;,122  ,189  &minus;,030  &minus;.646  ,519 
</p>
<p> Genetwo  ,287  ,225  ,078  1,276  ,203 
</p>
<p> Genethree  ,370  ,228  ,097  1,625  ,105 
</p>
<p> Genefour  ,063  ,196  ,014  ,321  ,748 
</p>
<p> Genesixteen  ,764  ,172  ,241  4,450  ,000 
</p>
<p> Geneseventeen  ,835  ,198  ,221  4,220  ,000 
</p>
<p>(continued)
</p>
<p>23 Optimal Scaling of High-Sensitivity Analysis of Health Predictors (250 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>145
</p>
<p> Coeffi cients a  
</p>
<p> Model 
</p>
<p> Unstandardized 
</p>
<p>coeffi cients 
</p>
<p> Standardized 
</p>
<p>coeffi cients 
</p>
<p> B  Std. Error  Beta  t  Sig. 
</p>
<p> Geneeighteen  ,088  ,151  ,027  ,580  ,563 
</p>
<p> Genenineteen  ,576  ,154  ,188  3,751  ,000 
</p>
<p> Genetwentyfour  ,403  ,146  ,154  2,760  ,006 
</p>
<p> Genetwentyfi ve  ,028  ,141  ,008  ,198  ,843 
</p>
<p> Genetwentysix  ,320  ,142  ,125  2,250  ,025 
</p>
<p> Genetwentyseven  &minus;,275  ,133  &minus;,092  &minus;2,067  ,040 
</p>
<p>   a Dependent variable: summaryoutcome 
</p>
<p>    The number of statistically signifi cant p-values (indicated here with Sig.), 
</p>
<p>(&lt;0.10) was 6 out of 12. In order to improve this result the Optimal Scaling program 
</p>
<p>of SPSS is used. Continuous predictor variables are converted into best fi t dis-
</p>
<p>cretized ones.  
</p>
<p>    Optimal Scaling Without Regularization 
</p>
<p>   Command: 
</p>
<p>  Analyze&hellip;.Regression&hellip;.Optimal Scaling&hellip;.Dependent Variable: Var 28 (Defi ne 
</p>
<p>Scale: mark spline ordinal 2.2)&hellip;.Independent Variables: Var 1, 2, 3, 4, 16, 17, 18, 
</p>
<p>19, 24, 25, 26, 27 (all of them Defi ne Scale: mark spline ordinal 2.2)&hellip;.Discretize: 
</p>
<p>Method Grouping&hellip;.OK.   
</p>
<p> Coeffi cients 
</p>
<p> Standardized coeffi cients 
</p>
<p> df  F  Sig.  Beta 
</p>
<p> Bootstrap (1000) 
</p>
<p>estimate of Std. Error 
</p>
<p> Geneone  &minus;,109  ,110  2  ,988  ,374 
</p>
<p> Genetwo  ,193  ,107  3  3,250  ,023 
</p>
<p> Genethree  &minus;,092  ,119  2  ,591  ,555 
</p>
<p> Genefour  ,113  ,074  3  2,318  ,077 
</p>
<p> Genesixteen  ,263  ,087  4  9,065  ,000 
</p>
<p> Geneseventeen  ,301  ,114  2  6,935  ,001 
</p>
<p> Geneeighteen  ,113  ,136  1  ,687  ,408 
</p>
<p> Genenineteen  ,145  ,067  1  4,727  ,031 
</p>
<p> Genetwentyfour  ,220  ,097  2  5,166  ,007 
</p>
<p> Genetwentyfi ve  &minus;,039  ,094  1  ,170  ,681 
</p>
<p> Genetwentysix  ,058  ,107  2  ,293  ,746 
</p>
<p> Genetwentyseven  &minus;.127  ,104  2  1,490  ,228 
</p>
<p>  Dependent variable: summaryoutcome 
</p>
<p>Optimal Scaling Without Regularization</p>
<p/>
</div>
<div class="page"><p/>
<p>146
</p>
<p>    There is no intercept anymore and the t-tests have been replaced with F-tests. 
</p>
<p>The optimally scaled model without regularization shows similarly sized effects. 
</p>
<p> The number of p-values &lt; 0.10 is 6 out of 12. In order to fully benefi t from opti-
</p>
<p>mal scaling a regularization procedure for the purpose of correcting overdispersion 
</p>
<p>(more spread in the data than compatible with Gaussian data) is desirable. Ridge 
</p>
<p>regression minimizes the b-values such that b ridge  = b /(1 + shrinking factor). With 
</p>
<p>shrinking factor = 0, b ridge  = b, with &infin;, b ridge  = 0.  
</p>
<p>    Optimal Scaling With Ridge Regression 
</p>
<p>   Command: 
</p>
<p>  Analyze&hellip;.Regression&hellip;.Optimal Scaling&hellip;.Dependent Variable: Var 28 (Defi ne Scale: 
</p>
<p>mark spline ordinal 2.2)&hellip;.Independent Variables: Var 1, 2, 3, 4, 16, 17, 18, 19, 24, 25, 
</p>
<p>26, 27 (all of them Defi ne Scale: mark spline ordinal 2.2)&hellip;.Discretize: Method 
</p>
<p>Grouping, Number categories 7&hellip;.click Regularization&hellip;.mark Ridge&hellip;. OK.   
</p>
<p> Coeffi cients 
</p>
<p> Standardized coeffi cients 
</p>
<p> df  F  Sig.  Beta 
</p>
<p> Bootstrap (1000) 
</p>
<p>estimate of Std. Error 
</p>
<p> Geneone  ,032  ,033  2  ,946  ,390 
</p>
<p> Genetwo  ,068  ,021  3  10,842  ,000 
</p>
<p> Genethree  ,051  ,030  1  2,963  ,087 
</p>
<p> Genefour  ,064  ,020  3  10,098  ,000 
</p>
<p> Genesixteen  ,139  ,024  4  34,114  ,000 
</p>
<p> Geneseventeen  ,142  ,025  2  31,468  ,000 
</p>
<p> Geneeighteen  ,108  ,040  2  7,236  ,001 
</p>
<p> Genenineteen  ,109  ,020  2  30,181  ,000 
</p>
<p> Genetwentyfour  ,109  ,021  2  27,855  ,000 
</p>
<p> Genetwentyfi ve  ,041  ,038  3  1,178  ,319 
</p>
<p> Genetwentysix  ,098  ,023  2  17,515  ,000 
</p>
<p> Genetwentyseven  &minus;,017  ,047  1  ,132  ,716 
</p>
<p>  Dependent variable: 20&ndash;23 
</p>
<p>    The sensitivity of this model is better than the above two methods with 7 
</p>
<p>p- values &lt; 0.0001, and 9 p-values &lt; 0.10, while the traditional and unregularized 
</p>
<p>Optimal Scaling only produced 6 and 6 p-values &lt; 0.10. Also the lasso regulariza-
</p>
<p>tion model is possible (Var = variable). It shrinks the small b values to 0.  
</p>
<p>23 Optimal Scaling of High-Sensitivity Analysis of Health Predictors (250 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>147
</p>
<p>    Optimal Scaling With Lasso Regression 
</p>
<p>   Command: 
</p>
<p>  Analyze&hellip;.Regression&hellip;.Optimal Scaling&hellip;.Dependent Variable: Var 28 (Defi ne 
</p>
<p>Scale: mark spline ordinal 2.2)&hellip;.Independent Variables: Var 1, 2, 3, 4, 16, 17, 
</p>
<p>18, 19, 24, 25, 26, 27 (all of them Defi ne Scale: mark spline ordinal 2.2)&hellip;.
</p>
<p>Discretize: Method Grouping, Number categories 7&hellip;.click Regularization&hellip;.
</p>
<p>mark Lasso&hellip;. OK.   
</p>
<p> Coeffi cients 
</p>
<p> Standardized coeffi cients 
</p>
<p> df  F  Sid.  Beta 
</p>
<p> Bootstrap (1000) 
</p>
<p>estimate of Std. Error 
</p>
<p> Geneone  ,000  ,020  0  ,000 
</p>
<p> Genetwo  ,054  ,046  3  1,390  ,247 
</p>
<p> Genethree  ,000  ,026  0  ,000 
</p>
<p> Genefour  ,011  ,036  3  ,099  ,960 
</p>
<p> Genesixteen  ,182  ,084  4  4,684  ,001 
</p>
<p> Geneseventeen  ,219  ,095  3  5,334  ,001 
</p>
<p> Geneeighteen  ,086  ,079  2  1,159  ,316 
</p>
<p> Genenineteen  ,105  ,063  2  2,803  ,063 
</p>
<p> Genetwentyfour  ,124  ,078  2  2,532  ,082 
</p>
<p> Genetwentyfi ve  ,000  ,023  0  ,000 
</p>
<p> Genetwentysix  ,048  ,060  2  ,647  ,525 
</p>
<p> Genetwentyseven  ,000  ,022  0  ,000 
</p>
<p>  Dependent variable: 20&ndash;23 
</p>
<p>    The b-values of the genes 1, 3, 25 and 27 are now shrunk to zero, and eliminated 
</p>
<p>from the analysis. Lasso is particularly suitable if you are looking for a limited 
</p>
<p>number of predictors and improves prediction accuracy by leaving out weak predic-
</p>
<p>tors. Finally, the elastic net method is applied. Like lasso it shrinks the small b- values 
</p>
<p>to 0, but it performs better with many predictor variables.  
</p>
<p>    Optimal Scaling With Elastic Net Regression 
</p>
<p>   Command: 
</p>
<p>  Analyze&hellip;.Regression&hellip;.Optimal Scaling&hellip;.Dependent Variable: Var 28 (Defi ne 
</p>
<p>Scale: mark spline ordinal 2.2)&hellip;.Independent Variables: Var 1, 2, 3, 4, 16, 17, 18, 
</p>
<p>19, 24, 25, 26, 27 (all of them Defi ne Scale: mark spline ordinal 2.2)&hellip;.Discretize: 
</p>
<p>Method Grouping, Number categories 7&hellip;.click Regularization&hellip;.mark Elastic 
</p>
<p>Net&hellip;.OK.   
</p>
<p>Optimal Scaling With Elastic Net Regression</p>
<p/>
</div>
<div class="page"><p/>
<p>148
</p>
<p> Coeffi cients 
</p>
<p> Standardized coeffi cients 
</p>
<p> df  F  Sig.  Beta 
</p>
<p> Bootstrap (1000) estimate 
</p>
<p>of Std. Error 
</p>
<p> Geneone  ,000  ,016  0  ,000 
</p>
<p> Genetwo  ,029  ,039  3  ,553  ,647 
</p>
<p> Genethree  ,000  ,032  3  ,000  1,000 
</p>
<p> Genefour  ,000  ,015  0  ,000 
</p>
<p> Genesixteen  ,167  ,048  4  12,265  ,000 
</p>
<p> Geneseventeen  ,174  ,051  3  11,429  ,000 
</p>
<p> Geneeighteen  ,105  ,055  2  3,598  ,029 
</p>
<p> Genenineteen  ,089  ,048  3  3,420  ,018 
</p>
<p> Genetwentyfour  ,113  ,053  2  4,630  ,011 
</p>
<p> Genetwentyfi ve  ,000  ,012  0  ,000 
</p>
<p> Genetwentysix  ,062  ,046  2  1,786  ,170 
</p>
<p> Genetwentyseven  ,000  ,018  0  ,000 
</p>
<p>  Dependent variable: 20&ndash;23 
</p>
<p>    The results are pretty much the same, as it is with lasso. Elastic net does not 
</p>
<p>provide additional benefi t in this example but works better than lasso if the number 
</p>
<p>of predictors is larger than the number of observations.  
</p>
<p>    Conclusion 
</p>
<p> Optimal scaling of linear regression data provides little benefi t due to overdisper-
</p>
<p>sion. Regularized optimal scaling using ridge regression provides excellent results. 
</p>
<p>Lasso optimal scaling is suitable if you are looking for a limited number of strong 
</p>
<p>predictors. Elastic net optimal scaling works better than lasso if the number of pre-
</p>
<p>dictors is large.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of optimal scaling 
</p>
<p>with or without regularization is available in Machine learning in medicine part one, 
</p>
<p>Chaps. 3 and 4, entitled &ldquo;Optimal scaling: discretization&rdquo;, and &ldquo;Optimal scaling: 
</p>
<p>regularization including ridge, lasso, and elastic net regression&rdquo;, pp 25&ndash;37, and 
</p>
<p>pp 39&ndash;53, Springer Heidelberg Germany, 2013, from the same authors.    
</p>
<p>23 Optimal Scaling of High-Sensitivity Analysis of Health Predictors (250 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>149&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_24
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 1&rdquo; as Chap. 9, 
</p>
<p>2013. 
</p>
<p>    Chapter 24   
</p>
<p> Discriminant Analysis for Making a Diagnosis 
from Multiple Outcomes (45 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> To assess whether discriminant analysis can be used to make a diagnosis from mul-
</p>
<p>tiple outcomes both in groups and in individual patients.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Laboratory screenings were performed in patients with different types of sepsis 
</p>
<p>(urosepsis, bile duct sepsis, and airway sepsis). Can discriminant analysis of labora-
</p>
<p>tory screenings improve reliability of diagnostic processes. 
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5  Var 6  Var 7  Var 8  Var 9  Var 10  Var 11 
</p>
<p> 8,00  5,00  28,00  4,00  2,50  79,00  108,00  19,00  18,00  16,00  2,00 
</p>
<p> 11,00  10,00  29,00  7,00  2,10  94,00  89,00  18,00  15,00  15,00  2,00 
</p>
<p> 7,00  8,00  30,00  7,00  2,20  79,00  96,00  20,00  16,00  14,00  2,00 
</p>
<p> 4,00  6,00  16,00  6,00  2,60  80,00  120,00  17,00  17,00  19,00  2,00 
</p>
<p> 1,00  6,00  15,00  6,00  2,20  84,00  108,00  21,00  18,00  20,00  2,00 
</p>
<p> 23,00  5,00  14,00  6,00  2,10  78,00  120,00  18,00  17,00  21,00  3,00 
</p>
<p> 12,00  10,00  17,00  5,00  3,20  85,00  100,00  17,00  20,00  18,00  3,00 
</p>
<p> 31,00  8,00  27,00  5,00  ,20  68,00  113,00  19,00  15,00  18,00  3,00 
</p>
<p> 22,00  7,00  26,00  5,00  1,20  74,00  98,00  16,00  16,00  17,00  3,00 
</p>
<p> 30,00  6,00  25,00  4,00  2,40  69,00  90,00  20,00  18,00  16,00  3,00 
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>150
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5  Var 6  Var 7  Var 8  Var 9  Var 10  Var 11 
</p>
<p> 2,00  12,00  21,00  4,00  2,80  75,00  112,00  11,00  14,00  19,00  1,00 
</p>
<p> 10,00  21,00  20,00  4,00  2,90  70,00  100,00  12,00  15,00  20,00  1,00 
</p>
<p>  Var 1 gammagt 
</p>
<p> Var 2 asat 
</p>
<p> Var 3 alat 
</p>
<p> Var 4 bilirubine 
</p>
<p> Var 5 ureum 
</p>
<p> Var 6 creatinine 
</p>
<p> Var 7 creatinine clearance 
</p>
<p> Var 8 erythrocyte sedimentation rate 
</p>
<p> Var 9 c-reactive protein 
</p>
<p> Var 10 leucocyte count 
</p>
<p> Var 11 type of sepsis (1&ndash;3 as described above) 
</p>
<p>    The fi rst 12 patients are shown only, the entire data fi le is entitled &ldquo;optscalingfac-
</p>
<p>torplscanonical&rdquo; and is in extras.springer.com.  
</p>
<p>    The Computer Teaches Itself to Make Predictions 
</p>
<p> SPSS 19.0 is used for training and outcome prediction. It uses XML (eXtended 
</p>
<p>Markup Language) fi les to store data. Start by opening the data fi le.
</p>
<p>  Command: 
</p>
<p>  Click Transform&hellip;.click Random Number Generators&hellip;.click Set Starting Point&hellip;. 
</p>
<p>click Fixed Value (2000000)&hellip;.click OK&hellip;.click Analyze&hellip;.Classify&hellip;. 
</p>
<p>Discriminant Analysis&hellip;.Grouping Variable: enter diagnosisgroup&hellip;.Defi ne Range: 
</p>
<p>Minimum enter 1&hellip;Maximum enter 3&hellip;.click Continue&hellip;.Independents: enter all 
</p>
<p>of the 10 laboratory variables&hellip;.click Statistics&hellip;.mark Unstandardized &hellip;.mark 
</p>
<p>Separate-groups covariance&hellip;.click Continue&hellip;.click Classify&hellip;.mark All groups 
</p>
<p>equal&hellip;.mark Summary table&hellip;.mark Within-groups&hellip;.mark Combined groups&hellip;.
</p>
<p>click Continue&hellip;.click Save&hellip;.mark Predicted group memberships&hellip;.in Export 
</p>
<p>model information to XML fi le enter: exportdiscriminant&hellip;.click Browse and save 
</p>
<p>the XML fi le in your computer&hellip;.click Continue&hellip;.click OK.    
</p>
<p> The scientifi c question &ldquo;is the diagnosis group a signifi cant predictor of the out-
</p>
<p>come estimated with 10 lab values&rdquo; is hard to assess with traditional multivariate 
</p>
<p>methods due to interaction between the outcome variables. It is, therefore, assessed 
</p>
<p>with the question &ldquo;is the clinical outcome a signifi cant predictor of the odds of hav-
</p>
<p>ing had a particular prior diagnosis. This reasoning may seem incorrect, using an 
</p>
<p>24 Discriminant Analysis for Making a Diagnosis from Multiple Outcomes (45 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>151
</p>
<p>outcome for making predictions, but, mathematically, it is no problem. It is just a 
</p>
<p>matter of linear cause-effect relationships, but just the other way around, and it 
</p>
<p>works very conveniently with &ldquo;messy&rdquo; outcome variables like in the example given. 
</p>
<p>However, fi rst, the numbers of outcome variables have to be reduced. SPSS 
</p>
<p> accomplishes this by orthogonal modeling of the outcome variables, which  produces 
</p>
<p>novel composite outcome variables. They are the y-values of linear equations. The 
</p>
<p>x- values of these linear equations are the original outcome variables, and their 
</p>
<p>regression coeffi cients are given in the underneath table.
</p>
<p> Structure matrix 
</p>
<p> Function 
</p>
<p> 1  2 
</p>
<p> As at  ,574 *   ,184 
</p>
<p> Gammagt  ,460 *   ,203 
</p>
<p> C-reactive protein  &minus;.034  ,761 *  
</p>
<p> Leucos  ,193  ,537 *  
</p>
<p> Ureum  ,461  ,533 *  
</p>
<p> Creatinine  ,462  ,520 *  
</p>
<p> Alat  ,411  ,487 *  
</p>
<p> Bili  ,356  ,487 *  
</p>
<p> Esr  ,360  ,487 *  
</p>
<p> Creatinine clearance  &minus;,083  &minus;.374 *  
</p>
<p>  Pooled within-groups correlations between discriminating variables and standardized canonical 
</p>
<p>discriminant functions 
</p>
<p> Variables ordered by absolute size of correlation within function. 
</p>
<p>  * Largest absolute correlation between each variable and any discriminant function 
</p>
<p> Wilks&rsquo; Lambda 
</p>
<p> Test of function(s)  Wilks&rsquo; Lambda  Chi-square  df  Sig. 
</p>
<p> 1 through 2  ,420  32,500  20  ,038 
</p>
<p> 2  ,859  5,681  9  ,771 
</p>
<p>   The two novel outcome variables signifi cantly predict the odds of having had a 
</p>
<p>prior diagnosis with p = 0.038 as shown above. When minimizing the output sheets 
</p>
<p>we will return to the data fi le and observe that the novel outcome variables have been 
</p>
<p>added (the variables entitled Dis1_1 and Dis1_2), as well as the predicted diagnosis 
</p>
<p>group predicted from the discriminant model (the variable entitled Dis_1). For con-
</p>
<p>venience the XML fi le entitled &ldquo;exportdiscriminant&rdquo; is stored in extras.springer. com. 
</p>
<p> The saved XML fi le can now be used to predict the odds of having been in a 
</p>
<p>particular diagnosis group in fi ve novel patients whose lab values are known but 
</p>
<p>whose diagnoses are not yet obvious. 
</p>
<p>The Computer Teaches Itself to Make Predictions</p>
<p/>
</div>
<div class="page"><p/>
<p>152
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5  Var 6  Var 7  Var 8  Var 9  Var 10 
</p>
<p> 1049,00  466,00  301,00  268,00  59,80  213,00  &minus;2,00  109,00  121,00  42,00 
</p>
<p> 383,00  230,00  154,00  120,00  31,80  261,00  13,00  80,00  58,00  30,00 
</p>
<p> 9,00  9,00  31,00  204,00  34,80  222,00  10,00  60,00  57,00  34,00 
</p>
<p> 438,00  391,00  479,00  127,00  31,80  372,00  9,00  69,00  56,00  33,00 
</p>
<p> 481,00  348,00  478,00  139,00  21,80  329,00  15,00  49,00  47,00  32,00 
</p>
<p>  Var 1 gammagt 
</p>
<p> Var 2 asat 
</p>
<p> Var 3 alat 
</p>
<p> Var 4 bilirubine 
</p>
<p> Var 5 ureum 
</p>
<p> Var 6 creatinine 
</p>
<p> Var 7 creatinine clearance 
</p>
<p> Var 8 erythrocyte sedimentation rate 
</p>
<p> Var 9 c-reactive protein 
</p>
<p> Var 10 leucocyte count 
</p>
<p>    Enter the above data in a new SPSS data fi le.
</p>
<p>  Command: 
</p>
<p>  Utilities&hellip;.click Scoring Wizard&hellip;.click Browse&hellip;.click Select&hellip;.Folder: enter the 
</p>
<p>exportdiscriminant.xml fi le&hellip;.click Select&hellip;.in Scoring Wizard click Next&hellip;.click 
</p>
<p>Use value substitution&hellip;.click Next&hellip;.click Finish.    
</p>
<p> The above data fi le now gives predicted odds of having been in a particular diag-
</p>
<p>nosis group computed by the discriminant analysis module with the help of the xml 
</p>
<p>fi le. 
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5  Var 6  Var 7  Var 8  Var 9  Var 10  Var 11 
</p>
<p> 1049,00  466,00  301,00  268,00  59,80  213,00  &minus;2,00  109,00  121,00  42,00  2,00 
</p>
<p> 383,00  230,00  154,00  120,00  31,80  261,00  13,00  80,00  58,00  30,00  2,00 
</p>
<p> 9,00  9,00  31,00  204,00  34,80  222,00  10,00  60,00  57,00  34,00  1,00 
</p>
<p> 438,00  391,00  479,00  127,00  31,80  372,00  9,00  69,00  56,00  33,00  1,00 
</p>
<p> 481,00  348,00  478,00  139,00  21,80  329,00  15,00  49,00  47,00  32,00  2,00 
</p>
<p>  Var 1 gammagt 
</p>
<p> Var 2 asat 
</p>
<p> Var 3 alat 
</p>
<p> Var 4 bilirubine 
</p>
<p> Var 5 ureum 
</p>
<p> Var 6 creatinine 
</p>
<p> Var 7 creatinine clearance 
</p>
<p> Var 8 erythrocyte sedimentation rate 
</p>
<p> Var 9 c-reactive protein 
</p>
<p> Var 10 leucocyte count 
</p>
<p> Var 11 predicted odds of having been in a particular diagnosis group 
</p>
<p>24 Discriminant Analysis for Making a Diagnosis from Multiple Outcomes (45 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>153
</p>
<p>        Conclusion 
</p>
<p> The discriminant analysis module can be readily trained to provide from the labora-
</p>
<p>tory values of individual patients the best fi t odds of having been in a particular 
</p>
<p>diagnosis group. In this way discriminant analysis can support the hard work of 
</p>
<p>physicians trying to make a diagnosis.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of discriminant analy-
</p>
<p>sis is available in Machine learning part one, Chap. 17, entitled &ldquo;Discriminant anal-
</p>
<p>ysis for supervised data&rdquo;, pp 215&ndash;224, Springer Heidelberg Germany, 2013, from 
</p>
<p>the same authors.    
</p>
<p> Note</p>
<p/>
</div>
<div class="page"><p/>
<p>155&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_25
</p>
<p>    Chapter 25   
</p>
<p> Weighted Least Squares for Adjusting Effi cacy 
Data with Inconsistent Spread (78 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> Linear regression assumes that the spread of the outcome-values is homoscedastic: it 
</p>
<p>is the same for each predictor value. This assumption is, however, not warranted in 
</p>
<p>many real life situations. This chapter is to assess the advantages of  weighted  least 
</p>
<p>squares (WLS) instead of  ordinary  least squares (OLS) linear regression analysis.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> The effect of prednisone on peak expiratory fl ow was assumed to be more variable 
</p>
<p>with increasing dosages. Can it, therefore, be measured with more precision if linear 
</p>
<p>regression is replaced with weighted least squares procedure. 
</p>
<p> Var 1  Var 2  Var 3  Var 4 
</p>
<p> 1  29  1,40  174 
</p>
<p> 2  15  2,00  113 
</p>
<p> 3  38  0,00  281 
</p>
<p> 4  26  1,00  127 
</p>
<p> 5  47  1,00  267 
</p>
<p> 6  28  0,20  172 
</p>
<p> 7  20  2,00  118 
</p>
<p> 8  47  0,40  383 
</p>
<p> 9  39  0,40  97 
</p>
<p> 10  43  1,60  304 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 1&rdquo; as Chap. 
</p>
<p>10, 2013. 
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>156
</p>
<p> Var 1  Var 2  Var 3  Var 4 
</p>
<p> 11  16  0,40  85 
</p>
<p> 12  35  1,80  182 
</p>
<p> 13  47  2,00  140 
</p>
<p> 14  35  2,00  64 
</p>
<p> 15  38  0,20  153 
</p>
<p> 16  40  0,40  216 
</p>
<p>  Var 1 Patient no 
</p>
<p> Var 2 prednisone (mg/24 h) 
</p>
<p> Var 3 peak fl ow (ml/min) 
</p>
<p> Var 4 beta agonist (mg/24 h) 
</p>
<p>    Only the fi rst 16 patients are given, the entire data fi le is entitled &ldquo;weight-
</p>
<p>edleastsquares&rdquo; and is in extras.springer.com. SPSS 19.0 is used for data analysis. 
</p>
<p>We will fi rst make a graph of prednisone dosages and peak expiratory fl ows. Start 
</p>
<p>with opening the data fi le.  
</p>
<p>    Weighted Least Squares 
</p>
<p>   Command: 
</p>
<p>  click Graphs&hellip;.Legacy Dialogs&hellip;.Scatter/Dot&hellip;.click Simple Scatter&hellip;.click 
</p>
<p>Defi ne&hellip;.Y Axis enter peakfl ow&hellip;.X Axis enter prednisone&hellip;.click OK.   
</p>
<p>     
</p>
<p>25 Weighted Least Squares for Adjusting Effi cacy Data with Inconsistent Spread&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>157
</p>
<p>    The output sheet shows that the spread of the y-values is small with low dosages 
</p>
<p>and gradually increases. We will, therefore, perform both a traditional and a 
</p>
<p>weighted least squares analysis of these data.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Regression&hellip;.Linear&hellip;.Dependent: enter peakfl ow&hellip;.  
</p>
<p>  Independent: enter prednisone, betaagonist&hellip;.OK.   
</p>
<p> Model Summary a  
</p>
<p> Model  R  R square  Adjusted R square  Std. Error of the estimate 
</p>
<p> 1  ,763 b   ,582  ,571  65,304 
</p>
<p>   a Dependent variable: peak expiratory fl ow 
</p>
<p>  b Predictors: (Constant), beta agonist mg/24 h, prednisone mg/day 
</p>
<p> Coeffi cients a  
</p>
<p> Model 
</p>
<p> Unstandardized 
</p>
<p>coeffi cients 
</p>
<p> Standardized 
</p>
<p>coeffi cients 
</p>
<p> B  Std. Error  Beta  t  Sig. 
</p>
<p> 1  (Constant)  &minus;22,534  22,235  &minus;1,013  ,314 
</p>
<p> Prednisone mg/day  6,174  ,604  ,763  10,217  ,000 
</p>
<p> Beta agonist mg/24 h  6,744  11,299  ,045  ,597  ,552 
</p>
<p>   a Dependent variable: peak expiratory fl ow 
</p>
<p>    In the output sheets an R value of 0.763 is observed, and the linear effects of 
</p>
<p>prednisone dosages are a statistically signifi cant predictor of the peak expiratory 
</p>
<p>fl ow, but, surprisingly, the beta agonists dosages are not. 
</p>
<p> We will, subsequently, perform a WLS analysis.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Regression&hellip;.Weight Estimation&hellip;. select: Dependent: enter peakfl ow 
</p>
<p>&hellip;. Independent(s): enter prednisone, betaagonist&hellip;.select prednisone also as 
</p>
<p>Weight variable&hellip;.Power range: enter 0 through 5 by 0.5&hellip;.click Options&hellip;.select 
</p>
<p>Save best weights as new variable&hellip;.click Continue&hellip;.click OK.    
</p>
<p> In the output sheets it is observed that the software has calculated likelihoods for 
</p>
<p>different powers, and the best likelihood value is chosen for further analysis. When 
</p>
<p>returning to the data fi le again a novel variable is added, the WGT_1 variable (the 
</p>
<p>weights for the WLS analysis). The next step is to perform again a linear regression, 
</p>
<p>but now with the weight variable included.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Regression&hellip;.Linear&hellip;. select: Dependent: enter peakfl ow&hellip;. 
</p>
<p>Independent(s) : enter prednisone, betaagonist&hellip;.select the weights for the wls 
</p>
<p>analysis (the GGT_1) variable as WLS Weight&hellip;.click Save&hellip;.select Unstandardized 
</p>
<p>in Predicted Values&hellip;.deselect Standardized in Residuals&hellip;.click Continue&hellip;.click 
</p>
<p>OK.   
</p>
<p>Weighted Least Squares</p>
<p/>
</div>
<div class="page"><p/>
<p>158
</p>
<p> Model Summary a,b  
</p>
<p> Model  R  R Square  Adjusted R Square  Std. Error of the estimate 
</p>
<p> 1  ,846 c   ,716  ,709  ,125 
</p>
<p>   a Dependent Variable: peak expiratory fl ow 
</p>
<p>  b Weighted Least Squares Regression-Weighted by Weight for peakfl ow from WLS, MOD_6 
</p>
<p>PREDNISONE** -3,500 
</p>
<p>  c Predictors: (Constant), beta agonist mg/24 h, prednisone mg/day 
</p>
<p> Coeffi cients a,b  
</p>
<p> Model 
</p>
<p> Unstandardized 
</p>
<p>coeffi cients 
</p>
<p> Standardized 
</p>
<p> coeffi cients 
</p>
<p> t  Sig.  B  Std. Error  Beta 
</p>
<p> 1  (Constant)  5,029  7,544  ,667  ,507 
</p>
<p> Prednisone mg/day  5,064  ,369  ,880  13,740  ,000 
</p>
<p> Beta agonist mg/24 h  10,838  3,414  203  3,174  ,002 
</p>
<p>   a Dependent Variable: peak expiratoryfl ow 
</p>
<p>  b Weighted Least Squares Regression &ndash; Weighted by Weight for peakfl ow from WLS, M0D_6 
</p>
<p>PREDNISONE"-3,500 
</p>
<p>    The output table now shows an R value of 0.846. It has risen from 0.763, and 
</p>
<p>provides thus more statistical power. The above lower table shows the effects of the 
</p>
<p>two medicine dosages on the peak expiratory fl ows. The t-values of the medicine 
</p>
<p>predictors have increased from approximately 10 and 0.5 to 14 and 3.2. The p- values 
</p>
<p>correspondingly fell from 0.000 and 0.552 to respectively 0.000 and 0.002. Larger 
</p>
<p>prednisone dosages and larger beta agonist dosages signifi cantly and independently 
</p>
<p>increased peak expiratory fl ows. After adjustment for heteroscedasticity, the beta 
</p>
<p>agonist became a signifi cant independent determinant of peak fl ow.  
</p>
<p>    Conclusion 
</p>
<p> The current paper shows that, even with a sample of only 78 patients, WLS is able 
</p>
<p>to demonstrate statistically signifi cant linear effects that had been, previously, 
</p>
<p>obscured by heteroscedasticity of the y-value.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of weighted least 
</p>
<p>squares modeling is given in Machine learning in medicine part three, Chap. 10, 
</p>
<p>Weighted least squares, pp 107&ndash;116, Springer Heidelberg Germany, 2013, from the 
</p>
<p>same authors.    
</p>
<p>25 Weighted Least Squares for Adjusting Effi cacy Data with Inconsistent Spread&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>159&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_26
</p>
<p>    Chapter 26   
</p>
<p> Partial Correlations for Removing Interaction 
Effects from Effi cacy Data (64 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> The outcome of cardiovascular research is generally affected by many more factors 
</p>
<p>than a single one, and multiple regression assumes that these factors act indepen-
</p>
<p>dently of one another, but why should they not affect one another. This chapter is to 
</p>
<p>assess whether partial correlation can be used to remove interaction effects from 
</p>
<p>linear data.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Both calorie intake and exercise are signifi cant independent predictors of weight 
</p>
<p>loss. However, exercise makes you hungry and patients on weight training are 
</p>
<p>inclined to reduce (or increase) their calorie intake. Can partial correlations methods 
</p>
<p>adjust the interaction between the two predictors. 
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5 
</p>
<p> 1,00  0,00  1000,00  0,00  45,00 
</p>
<p> 29,00  0,00  1000,00  0,00  53,00 
</p>
<p> 2,00  0,00  3000,00  0,00  64,00 
</p>
<p> 1,00  0,00  3000,00  0,00  64,00 
</p>
<p> 28,00  6,00  3000,00  18000,00  34,00 
</p>
<p> 27,00  6,00  3000,00  18000,00  25,00 
</p>
<p> 30,00  6,00  3000,00  18000,00  34,00 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 1&rdquo; as Chap. 
</p>
<p>11, 2013. 
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>160
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5 
</p>
<p> 27,00  6,00  1000,00  6000,00  45,00 
</p>
<p> 29,00  0,00  2000,00  0,00  52,00 
</p>
<p> 31,00  3,00  2000,00  6000,00  59,00 
</p>
<p> 30,00  3,00  1000,00  3000,00  58,00 
</p>
<p> 29,00  3,00  1000,00  3000,00  47,00 
</p>
<p> 27,00  0,00  1000,00  0,00  45,00 
</p>
<p> 28,00  0,00  1000,00  0,00  66,00 
</p>
<p> 27,00  0,00  1000,00  0,00  67,00 
</p>
<p>  Var 1 weight loss (kg) 
</p>
<p> Var 2 exercise (times per week) 
</p>
<p> Var 3 calorie intake (cal) 
</p>
<p> Var 4 interaction 
</p>
<p> Var 5 age (years) 
</p>
<p>    Only the fi rst fi fteen patients are given, the entire fi le is entitled &ldquo;partialcorrela-
</p>
<p>tions&rdquo; and is in extras.springer.com.  
</p>
<p>    Partial Correlations 
</p>
<p> We will fi rst perform a linear regression of these data. SPSS 19.0 is used for the 
</p>
<p>purpose. Start by opening the data fi le.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Regression&hellip;.Linear&hellip;.Dependent variable: enter weightloss&hellip;. 
</p>
<p>Independent variables: enter exercise and calorieintake&hellip;.click OK.   
</p>
<p> Coeffi cients a  
</p>
<p> Model 
</p>
<p> Unstandardized coeffi cients 
</p>
<p> Standardized 
</p>
<p>coeffi cients 
</p>
<p> t  Sig.  B  Std. Error  Beta 
</p>
<p> 1  (Constant)  29,089  2,241  12,978  ,000 
</p>
<p> Exercise  2,548  ,439  ,617  5,802  ,000 
</p>
<p> Calorieintake  &minus;,006  ,001  &minus;,544  &minus;5,116  ,000 
</p>
<p>   a Dependent variable: weightloss 
</p>
<p>    The output sheets show that both calorie intake and exercise are signifi cant 
</p>
<p> independent predictors of weight loss. However, interaction between exercise and 
</p>
<p>calorie intake is not accounted. In order to check, an interaction variable (x 3  = calorie 
</p>
<p>intake * exercise, with * symbol of multiplication) is added to the model.
</p>
<p>  Command: 
</p>
<p>  Transform data&hellip;.Compute Variable&hellip;.in Target Variable enter the term "interac-
</p>
<p>tion"&hellip;.to Numeric Expression: transfer from Type &amp; Label "exercise" &hellip;.click * 
</p>
<p>&hellip;.transfer from Type &amp; Label calorieintake&hellip;.click OK.    
</p>
<p>26 Partial Correlations for Removing Interaction Effects from Effi cacy Data&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>161
</p>
<p> The interaction variable is added by SPSS to the data fi le and is entitled &ldquo;interac-
</p>
<p>tion&rdquo;. After the addition of the interaction variable to the regression model as third 
</p>
<p>independent variable, the analysis is repeated.
</p>
<p> Coeffi cients a  
</p>
<p> Model 
</p>
<p> Unstandardized 
</p>
<p>coeffi cients 
</p>
<p> Standardized 
</p>
<p>coeffi cients 
</p>
<p> t  Sia.  B  Std. Error  Beta 
</p>
<p> 1  (Constant)  34,279  2,651  12,930  ,000 
</p>
<p> Interaction  ,001  ,000  ,868  3,183  ,002 
</p>
<p> Exercise  &minus;,238  ,966  &minus;,058  &minus;,246  ,807 
</p>
<p> Calorieintake  &minus;,009  ,002  &minus;,813  &minus;6,240  ,000 
</p>
<p>   a Dependent variable: weightloss 
</p>
<p>    The output sheet now shows that exercise is no longer signifi cant and interaction 
</p>
<p>on the outcome is signifi cant at p = 0.002. There is, obviously, interaction in the 
</p>
<p>study, and the overall analysis of the data is, thus, no longer relevant. The best 
</p>
<p>method to fi nd the true effect of exercise would be to repeat the study with calorie 
</p>
<p>intake held constant. Instead of this laborious exercise, a partial correlation analysis 
</p>
<p>with calorie intake held artifi cially constant can be adequately performed, and 
</p>
<p>would provide virtually the same result. Partial correlation analysis is performed 
</p>
<p>using the SPSS module Correlations.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Correlate&hellip;.Partial&hellip;.Variables: enter weight loss and calorie intake 
</p>
<p>&hellip;.Controlling for: enter exercise&hellip;.OK.   
</p>
<p> Correlations 
</p>
<p> Control variables  Weightloss  Calorieintake 
</p>
<p> Exercise  Weightloss  Correlation  1,000  &minus;,548 
</p>
<p> Signifi cance (2-tailed)  ,000 
</p>
<p> df  0  61 
</p>
<p> Calorieintake  Correlation  &minus;,548  1,000 
</p>
<p> Signifi cance (2-tailed)  ,000 
</p>
<p> df  61  0 
</p>
<p> Correlations 
</p>
<p> Control variables  Weightloss  Exercise 
</p>
<p> Calorieintake  Weightloss  Correlation  1,000  ,596 
</p>
<p> Signifi cance (2-tailed)  ,000 
</p>
<p> df  0  61 
</p>
<p> Exercise  Correlation  ,596  1,000 
</p>
<p> Signifi cance (2-tailed)  ,000 
</p>
<p> df  61  0 
</p>
<p>Partial Correlations</p>
<p/>
</div>
<div class="page"><p/>
<p>162
</p>
<p>   The upper table shows, that, with exercise held constant, calorie intake is a 
</p>
<p>signifi cant negative predictor of weight loss with a correlation coeffi cient of &minus;0.548 
</p>
<p>and a p-value of 0.0001. Also partial correlation with exercise as independent and 
</p>
<p>calorie intake as controlling factor can be performed.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Correlate&hellip;.Partial&hellip;.Variables: enter weight loss and  exercise&hellip;.
</p>
<p>Controlling for: enter calorie intake&hellip;.OK.    
</p>
<p> The lower table shows that, with calorie intake held constant, exercise is a sig-
</p>
<p>nifi cant positive predictor of weight loss with a correlation coeffi cient of 0.596 and 
</p>
<p>a p-value of 0.0001. 
</p>
<p> Why do we no longer have to account interaction with partial correlations. This 
</p>
<p>is simply because, if you hold a predictor fi xed, this fi xed predictor can no longer 
</p>
<p>change and interact in a multiple regression model. 
</p>
<p> Also higher order partial correlation analyses are possible. E.g., age may affect 
</p>
<p>all of the three variables already in the model. The effect of exercise on weight loss 
</p>
<p>with calorie intake and age fi xed can be assessed.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Correlate&hellip;.Partial&hellip;.Variables: enter weight loss and exercise&hellip;. 
</p>
<p>Controlling for: enter calorie intake and age&hellip;.OK.   
</p>
<p> Correlations 
</p>
<p> Control variables  Weightloss  Exercise 
</p>
<p> Age &amp; calorieintake  Weightloss  Correlation  1,000  ,541 
</p>
<p> Signifi cance (2-tailed)  ,000 
</p>
<p> df  0  60 
</p>
<p> Exercise  Correlation  ,541  1,000 
</p>
<p> Signifi cance (2-tailed)  ,000 
</p>
<p> df  60  0 
</p>
<p>   In the above output sheet it can be observed that the correlation coeffi cient is still 
</p>
<p>very signifi cant.  
</p>
<p>    Conclusion 
</p>
<p> Without the partial correlation approach the conclusion from this study would have 
</p>
<p>been: no defi nitive conclusion about the effects of exercise and calorie intake is 
</p>
<p>possible, because of a signifi cant interaction between exercise and calorie intake. 
</p>
<p>The partial correlation analysis allows to conclude that both exercise and calorie 
</p>
<p>intake have a very signifi cant linear relationship with weight loss effect.  
</p>
<p>26 Partial Correlations for Removing Interaction Effects from Effi cacy Data&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>163
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of partial correlations 
</p>
<p>methods is given in Machine learning in medicine part one, Chap. 5, Partial correla-
</p>
<p>tions, pp 55&ndash;64, Springer Heidelberg Germany, 2013, from the same authors.    
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>165&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_27
</p>
<p>    Chapter 27   
</p>
<p> Canonical Regression for Overall Statistics 
of Multivariate Data (250 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> To assess in datasets with multiple predictor and outcome variables, whether canon-
</p>
<p>ical analysis, unlike traditional multivariate analysis of variance (MANOVA), can 
</p>
<p>provide overall statistics of combined effects.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> The example of the Chaps. 22 and 23 is used once again. Twelve highly expressed 
</p>
<p>genes are used to predict four measures of drug effi cacy. We are more interested in 
</p>
<p>the combined effect of the predictor variables on the outcome variables than we are 
</p>
<p>in the separate effects of the different variables. 
</p>
<p> G1  G2  G3  G4  G16  G17  G18  G19  G24  G25  G26  G27  O1  O2  O3  O4 
</p>
<p> 8  8  9  5  7  10  5  6  9  9  6  6  6  7  6  7 
</p>
<p> 9  9  10  9  8  8  7  8  8  9  8  8  8  7  8  7 
</p>
<p> 9  8  8  8  8  9  7  8  9  8  9  9  9  8  8  8 
</p>
<p> 8  9  8  9  6  7  6  4  6  6  5  5  7  7  7  6 
</p>
<p> 10  10  8  10  9  10  10  8  8  9  9  9  8  8  8  7 
</p>
<p> 7  8  8  8  8  7  6  5  7  8  8  7  7  6  6  7 
</p>
<p> 5  5  5  5  5  6  4  5  5  6  6  5  6  5  6  4 
</p>
<p> 9  9  9  9  8  8  8  8  9  8  3  8  8  8  8  8 
</p>
<p> 9  8  9  8  9  8  7  7  7  7  5  8  8  7  6  6 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 1&rdquo; as Chap. 
</p>
<p>12, 2013. 
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>166
</p>
<p> 10  10  10  10  10  10  10  10  10  8  8  10  10  10  9  10 
</p>
<p> 2  2  8  5  7  8  8  8  9  3  9  8  7  7  7  6 
</p>
<p> 7  8  8  7  8  6  6  7  8  8  8  7  8  7  8  8 
</p>
<p> 8  9  9  8  10  8  8  7  8  8  9  9  7  7  8  8 
</p>
<p>  Var G1-27 highly expressed genes estimated from their arrays&rsquo; normalized ratios 
</p>
<p> Var O1-4 drug effi cacy scores (the variables 20&ndash;23 from the initial data fi le) 
</p>
<p>    The data from the fi rst 13 patients are shown only (see extra.springer.com for the 
</p>
<p>entire data fi le entitled &ldquo;optscalingfactorplscanonical&rdquo;). First, MANOVA (multi-
</p>
<p>variate analysis of variance) was performed with the four drug effi cacy scores as 
</p>
<p>outcome variables and the twelve gene expression levels as covariates. We can now 
</p>
<p>use SPSS 19.0. Start by opening the data fi le.  
</p>
<p>    Canonical Regression 
</p>
<p>   Command: 
</p>
<p>  click Analyze&hellip;.click General Linear Model&hellip;.click Multivariate&hellip;.Dependent 
</p>
<p>Variables: enter the four drug effi cacy scores&hellip;.Covariates: enter the 12 genes&hellip;.
</p>
<p>OK.    
</p>
<p> Effect value  F  Hypothesis df  Error df  p-value 
</p>
<p> Intercept  0.043  2.657  4.0  234.0  0.034 
</p>
<p> Gene 1  0.006  0.362  4.0  234.0  0.835 
</p>
<p> Gene 2  0.27  1.595  4.0  234.0  0.176 
</p>
<p> Gene 3  0.042  2.584  4.0  234.0  0.038 
</p>
<p> Gene 4  0.013  0.744  4.0  234.0  0.563 
</p>
<p> Gene 16  0.109  7.192  4.0  234.0  0.0001 
</p>
<p> Gene 17  0.080  5.118  4.0  234.0  0.001 
</p>
<p> Gene 18  0.23  1.393  4.0  234.0  0.237 
</p>
<p> Gene 19  0.092  5.938  4.0  234.0  0.0001 
</p>
<p> Gene 24  0.045  2.745  4.0  234.0  0.029 
</p>
<p> Gene 25  0.017  1.037  4.0  234.0  0.389 
</p>
<p> Gene 26  0.027  1.602  4.0  234.0  0.174 
</p>
<p> Gene 27  0.045  2.751  4.0  234.0  0.029 
</p>
<p>   The MANOVA table is given (F = F-value, df = degrees of freedom). It shows that 
</p>
<p>MANOVA can be considered as another regression model with intercepts and 
</p>
<p>regression coeffi cients. We can conclude that the genes 3, 16, 17, 19, 24, and 27 are 
</p>
<p>signifi cant predictors of all four drug effi cacy outcome scores. Unlike ANOVA, 
</p>
<p>MANOVA does not give overall p-values, but rather separate p-values for separate 
</p>
<p>(continued)
</p>
<p>27 Canonical Regression for Overall Statistics of Multivariate Data (250 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>167
</p>
<p>covariates. However, we are, particularly, interested in the combined effect of the set 
</p>
<p>of predictors, otherwise called covariates, on the set of outcomes, rather than we are 
</p>
<p>in modeling the separate variables. In order to asses the overall effect of the cluster 
</p>
<p>of genes on the cluster of drug effi cacy scores canonical regression is performed.
</p>
<p>  Command: 
</p>
<p>  click File&hellip;.click New&hellip;.click Syntax&hellip;.the Syntax Editor dialog box is 
</p>
<p> displayed&hellip;.enter the following text: &ldquo;manova&rdquo; and subsequently enter all of the 
</p>
<p>outcome variables&hellip;.enter the text &ldquo;WITH&rdquo;&hellip;.then enter all of the gene- names&hellip;.
</p>
<p>then enter the following text: /discrim all alpha(1)/print = sig(eigen dim)&hellip;.click 
</p>
<p>Run.    
</p>
<p> Numbers variables (covariates v outcome variables) 
</p>
<p> Canon cor  Sq cor  Wilks L  F  Hypoth df  Error df  p 
</p>
<p> 12 v 4  0.87252  0.7613  0.19968  9.7773  48.0  903.4  0.0001 
</p>
<p> 7 v 4  0.87054  0.7578  0.21776  16.227  28.0  863.2  0.0001 
</p>
<p> 7 v 3  0.87009  0.7571  0.22043  22.767  21.0  689.0  0.0001 
</p>
<p>   The above table is given (cor = correlation coeffi cient, sq = squared, L = lambda, 
</p>
<p>hypoth = hypothesis, df = degree of freedom, p = p-value, v = versus). The upper row, 
</p>
<p>shows the result of the statistical analysis. The correlation coeffi cient between the 
</p>
<p>12 predictor and 4 outcome variables equals 0.87252. A squared correlation coeffi -
</p>
<p>cient of 0.7613 means that 76 % of the variability in the outcome variables is 
</p>
<p>explained by the 12 covariates. The cluster of predictors is a very signifi cant predic-
</p>
<p>tor of the cluster of outcomes, and can be used for making predictions about indi-
</p>
<p>vidual patients with similar gene profi les. Repeated testing after the removal of 
</p>
<p>separate variables gives an idea about relatively unimportant contributors as esti-
</p>
<p>mated by their coeffi cients, which are kind of canonical b-values (regression coef-
</p>
<p>fi cients). The larger they are, the more important they are. 
</p>
<p> Canon Cor 
</p>
<p> Raw Model  12 v 4  7 v 4  7 v 3 
</p>
<p> Outcome 1  &minus;0.24620  &minus;0.24603  0.25007 
</p>
<p> Outcome 2  &minus;0.20355  &minus;0.19683  0.20679 
</p>
<p> Outcome 3  &minus;0.02113  &minus;0.02532 
</p>
<p> Outcome 4  &minus;0.07993  &minus;0.08448  0.09037 
</p>
<p> Gene 1  0.01177 
</p>
<p> Gene 2  &minus;0.01727 
</p>
<p> Gene 3  &minus;0.05964  &minus;0.08344  0.08489 
</p>
<p> Gene 4  &minus;0.02865 
</p>
<p> Gene 16  &minus;0.14094  &minus;0.13883  0.13755 
</p>
<p> Gene 17  &minus;0.12897  &minus;0.14950  0.14845 
</p>
<p>(continued)
</p>
<p>Canonical Regression</p>
<p/>
</div>
<div class="page"><p/>
<p>168
</p>
<p> Canon Cor 
</p>
<p> Raw Model  12 v 4  7 v 4  7 v 3 
</p>
<p> Gene 18  &minus;0.03276 
</p>
<p> Gene 19  &minus;0.10626  &minus;0.11342  0.11296 
</p>
<p> Gene 24  &minus;0.07148  &minus;0.07024  0.07145 
</p>
<p> Gene 25  &minus;0.00164 
</p>
<p> Gene 26  &minus;0.05443  &minus;0.05326  0.05354 
</p>
<p> Gene 27  0.05589  0.04506  &minus;0.04527 
</p>
<p> Standardized 
</p>
<p> Outcome 1  &minus;0.49754  &minus;0.49720  0.50535 
</p>
<p> Outcome 2  &minus;0.40093  &minus;0.38771  0.40731 
</p>
<p> Outcome 3  &minus;0.03970  &minus;0.04758 
</p>
<p> Outcome 4  &minus;0.15649  &minus;0.16539  0.17693 
</p>
<p> Gene 1  0.02003 
</p>
<p> Gene 2  &minus;0.03211 
</p>
<p> Gene 3  &minus;0.10663  &minus;0.14919  0.15179 
</p>
<p> Gene 4  &minus;0.04363 
</p>
<p> Gene 16  &minus;0.30371  &minus;0.29918  0.29642 
</p>
<p> Gene 17  &minus;0.23337  &minus;0.27053  0.26862 
</p>
<p> Gene 18  &minus;0.06872 
</p>
<p> Gene 19  &minus;0.23696  &minus;0.25294  0.25189 
</p>
<p> Gene 24  &minus;0.18627  &minus;0.18302  0.18618 
</p>
<p> Gene 25  &minus;0.00335 
</p>
<p> Gene 26  &minus;0.14503  &minus;0.14191  0.14267 
</p>
<p> Gene 27  0.12711  0.10248  &minus;0.10229 
</p>
<p>   The above table left column gives an overview of raw and standardized (z trans-
</p>
<p>formed) canonical coeffi cients, otherwise called canonical weights (the multiple 
</p>
<p>b-values of canonical regression), (Canon Cor = canonical correlation coeffi cient, 
</p>
<p>v = versus, Model = analysis model after removal of one or more variables). The 
</p>
<p>outcome 3, and the genes 2, 4, 18 and 25 contributed little to the overall result. 
</p>
<p>When restricting the model by removing the variables with canonical coeffi cients 
</p>
<p>smaller than 0.05 or larger than &minus;0.05 (the middle and right columns of the table), 
</p>
<p>the results were largely unchanged. And so were the results of the overall tests (the 
</p>
<p>2nd and 3rd rows). Seven versus three variables produced virtually the same corre-
</p>
<p>lation coeffi cient but with much more power (lambda increased from 0.1997 to 
</p>
<p>0.2204, the F value from 9.7773 to 22.767, in spite of a considerable fall in the 
</p>
<p>degrees of freedom). It, therefore, does make sense to try and remove the weaker 
</p>
<p>variables from the model ultimately to be used. The weakest contributing covariates 
</p>
<p>of the MANOVA were virtually identical to the weakest canonical predictors, sug-
</p>
<p>(continued)
</p>
<p>27 Canonical Regression for Overall Statistics of Multivariate Data (250 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>169
</p>
<p>gesting that the two methods are closely related and one method confi rms the results 
</p>
<p>of the other.  
</p>
<p>    Conclusion 
</p>
<p> Canonical analysis is wonderful, because it can handle many more variables than 
</p>
<p>MANOVA, accounts for the relative importance of the separate variables and their 
</p>
<p>interactions, provides overall statistics. Unlike other methods for combining the 
</p>
<p>effects of multiple variables like factor analysis/partial least squares (chap. 8), 
</p>
<p>canonical analysis is scientifi cally entirely rigorous.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of canonical regres-
</p>
<p>sion is given in Machine learning in medicine part one, Chap. 18, Canonical regres-
</p>
<p>sion, pp 225&ndash;240, Springer Heidelberg Germany, 2013, from the same authors.    
</p>
<p> Note</p>
<p/>
</div>
<div class="page"><p/>
<p>171&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_28
</p>
<p>    Chapter 28   
</p>
<p> Multinomial Regression for Outcome 
Categories (55 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> To assess whether multinomial regression can be trained to make predictions about 
</p>
<p>(1) patients being in a category and (2) the probability of it.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Patients from different hospital departments and ages are assessed for falling out of 
</p>
<p>bed (0 = no, 1 = yes without injury, 2 = yes with injury). The falloutofbed categories 
</p>
<p>are the outcome, the department and ages are the predictors. Can a data fi le of such 
</p>
<p>patients be trained to make predictions in future patients about their best fi t category 
</p>
<p>and probability of being in it. 
</p>
<p> department  falloutofbed  age(years) 
</p>
<p> ,00  1  56,00 
</p>
<p> ,00  1  58,00 
</p>
<p> ,00  1  87,00 
</p>
<p> ,00  1  64,00 
</p>
<p> ,00  1  65,00 
</p>
<p> ,00  1  53,00 
</p>
<p> ,00  1  87,00 
</p>
<p> ,00  1  77,00 
</p>
<p> ,00  1  78,00 
</p>
<p> ,00  1  89,00 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 2&rdquo; as Chap. 4, 
</p>
<p>2014. </p>
<p/>
</div>
<div class="page"><p/>
<p>172
</p>
<p>   Only the fi rst 10 patients are given, the entire data fi le is entitled &ldquo;categoriesa-
</p>
<p>soutcome&rdquo; and is in extras.springer.com.  
</p>
<p>    The Computer Teaches Itself to Make Predictions 
</p>
<p> SPSS versions 18 and later can be used. SPSS will produce an XML (eXtended 
</p>
<p>Markup Language) fi le of the prediction model from the above data. We will start 
</p>
<p>by opening the above data fi le.
</p>
<p>  Command: 
</p>
<p>  click Transform&hellip;.click Random Number Generators&hellip;.click Set Starting Point&hellip;.
</p>
<p>click Fixed Value (2000000)&hellip;.click OK&hellip;.click Analyze&hellip;. Regression &hellip;.
</p>
<p>Multinomial Logistic Regression&hellip;.Dependent: falloutofbed&hellip;.     Factor: depart-
</p>
<p>ment&hellip;.Covariate: age&hellip;.click Save&hellip;.mark: Estimated response probability, 
</p>
<p>Predicted category, Predicted category probability, Actual category probability&hellip;.
</p>
<p>click Browse&hellip;.various folders in your personal computer come up&hellip;.in "File name" 
</p>
<p>of the appropriate folder enter "exportcategoriesasoutcome"&hellip;.click Save&hellip;.click 
</p>
<p>Continue&hellip;.click OK.   
</p>
<p> Parameter estimates 
</p>
<p> 95 % Confi dence 
</p>
<p>interval for Exp (B) 
</p>
<p> Fall with/out injury a   B 
</p>
<p> Std. 
</p>
<p>error  Wald  df  Sig.  Exp(B) 
</p>
<p> Lower 
</p>
<p>bound 
</p>
<p> Upper 
</p>
<p>bound 
</p>
<p> 0  Intercept  5,337  2,298  5,393  1  ,020 
</p>
<p> Age  &minus;,059  ,029  4,013  1  ,045  ,943  ,890  ,999 
</p>
<p> [department = ,00]  &minus;1,139  ,949  1,440  1  ,230  ,320  ,050  2,057 
</p>
<p> [department = 1,00]  0 b   0 
</p>
<p> 1  Intercept  3,493  2,333  2,241  1  ,134 
</p>
<p> Age  &minus;,022  ,029  ,560  1  ,454  ,978  ,924  1,036 
</p>
<p> [department = ,00]  &minus;1,945  ,894  4,735  1  ,030  ,143  ,025  ,824 
</p>
<p> [department = 1,00]  0 b   0 
</p>
<p>   a The reference category is: 2 
</p>
<p>  b This parameter is set to  ZSFO  because it is redundant 
</p>
<p>    The above table is in the output. The independent predictors of falloutofbed are 
</p>
<p>given. Per year of age there are 0,943 less &ldquo;no falloutofbeds&rdquo; versus &ldquo;falloutofbeds 
</p>
<p>with injury&rdquo;. The department 0,00 has 0,143 less falloutofbeds with versus without 
</p>
<p>injury. The respective p-values are 0,045 and 0,030. When returning to the main 
</p>
<p>data view, we will observe that SPSS has provided 6 novel variables for each patient.
</p>
<p>    1.    EST1_1 estimated response probability (probability of the category 0 for each 
</p>
<p>patient)   
</p>
<p>   2.    EST2_1 idem for category 1   
</p>
<p>28 Multinomial Regression for Outcome Categories (55 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>173
</p>
<p>   3.    EST3_1 idem for category 2   
</p>
<p>   4.    PRE_1 predicted category (category with highest probability score)   
</p>
<p>   5.    PCP_1 predicted category probability (the highest probability score predicted by model)   
</p>
<p>   6.    ACP_1 actual category probability (the highest probability computed from data)     
</p>
<p> With the Scoring Wizard and the exported XML fi le entitled "exportcategoriesa-
</p>
<p>soutcome" we can now try and predict from the department and age of future 
</p>
<p>patients (1) the most probable category they are in, and (2) the very probability of 
</p>
<p>it. The department and age of 12 novel patients are as follow. 
</p>
<p> department  age 
</p>
<p> ,00  73,00 
</p>
<p> ,00  38,00 
</p>
<p> 1,00  89,00 
</p>
<p> ,00  75,00 
</p>
<p> ,00  84,00 
</p>
<p> ,00  74,00 
</p>
<p> 1,00  90,00 
</p>
<p> 1,00  72,00 
</p>
<p> 1,00  62,00 
</p>
<p> 1,00  34,00 
</p>
<p> 1,00  85,00 
</p>
<p> 1,00  43,00 
</p>
<p>   Enter the above data in a novel data fi le and command:
</p>
<p>   Utilities&hellip;.click Scoring Wizard&hellip;.click Browse&hellip;.Open the appropriate folder 
</p>
<p>with the XML fi le entitled "exportcategoriesasoutcome"&hellip;.click on the latter and 
</p>
<p>click Select&hellip;.in Scoring Wizard double-click Next&hellip;.mark Predicted category and 
</p>
<p>Probability of it&hellip;.click Finish.    
</p>
<p> department  age  probability of being in predicted category  predicted category 
</p>
<p> ,00  73,00  ,48  1,00 
</p>
<p> ,00  38,00  ,48  1,00 
</p>
<p> 1,00  89,00  ,36  2,00 
</p>
<p> ,00  75,00  ,47  1,00 
</p>
<p> ,00  84,00  ,48  2,00 
</p>
<p> ,00  74,00  ,48  1,00 
</p>
<p> 1,00  90,00  ,37  2,00 
</p>
<p> 1,00  72,00  ,55  ,00 
</p>
<p> 1,00  62,00  ,65  ,00 
</p>
<p> 1,00  34,00  ,84  ,00 
</p>
<p>(continued)
</p>
<p>The Computer Teaches Itself to Make Predictions</p>
<p/>
</div>
<div class="page"><p/>
<p>174
</p>
<p> department  age  probability of being in predicted category  predicted category 
</p>
<p> 1,00  85,00  ,39  ,00 
</p>
<p> 1,00  43,00  ,79  ,00 
</p>
<p>  0 = no falloutofbed 
</p>
<p> 1 = falloutofbed without injury 
</p>
<p> 2 = falloutofbed with injury 
</p>
<p>    In the data fi le SPSS has provided two novel variables as requested. The fi rst 
</p>
<p>patient from department 0,00 and 73 years of age has a 48 % chance of being in the 
</p>
<p>&ldquo;falloutofbed without injury&rdquo;. His/her chance of being in the other two categories is 
</p>
<p>smaller than 48 %.  
</p>
<p>    Conclusion 
</p>
<p> Multinomial, otherwise called polytomous, logistic regression can be readily trained 
</p>
<p>to make predictions in future patients about their best fi t category and the probabil-
</p>
<p>ity of being in it.  
</p>
<p>    Note 
</p>
<p> More background theoretical and mathematical information of analyses using 
</p>
<p> categories as outcome is available in Machine learning in medicine part two, 
</p>
<p>Chap.10, Anomaly detection, pp 93&ndash;103, Springer Heidelberg Germany, 2013, 
</p>
<p>from the same authors.    
</p>
<p>28 Multinomial Regression for Outcome Categories (55 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>175&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_29
</p>
<p>    Chapter 29   
</p>
<p> Various Methods for Analyzing Predictor 
Categories (60 and 30 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> Categories unlike continuous data need not have stepping functions. In order to 
</p>
<p>apply regression analysis for their analysis we need to recode them into multiple 
</p>
<p>binary (dummy) variables. Particularly, if Gaussian distributions in the outcome are 
</p>
<p>uncertain, automatic non-parametric testing is an adequate and very convenient 
</p>
<p>modern alternative.  
</p>
<p>    Specifi c Scientifi c Questions 
</p>
<p>     1.    Does race have an effect on physical strength (the variable race has a categorical 
</p>
<p>rather than linear pattern).   
</p>
<p>   2.    Are the hours of sleep / levels of side effects different in categories treated with 
</p>
<p>different sleeping pills.      
</p>
<p>    Example 1 
</p>
<p> The effects on physical strength (scores 0&ndash;100) assessed in 60 subjects of different 
</p>
<p>races (hispanics (1), blacks (2), asians (3), and whites (4)), and ages (years), are in 
</p>
<p>the left three columns of the data fi le entitled &ldquo;categoriesaspredictor&rdquo;. 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 2&rdquo; as Chap. 5, 
</p>
<p>2014. </p>
<p/>
</div>
<div class="page"><p/>
<p>176
</p>
<p> Patient number  physical strength  race  age 
</p>
<p> 1  70,00  1,00  35,00  
</p>
<p> 2  77,00  1,00  55,00 
</p>
<p> 3  66,00  1,00  70,00 
</p>
<p> 4  59,00  1,00  55,00 
</p>
<p> 5  71,00  1,00  45,00 
</p>
<p> 6  72,00  1,00  47,00 
</p>
<p> 7  45,00  1,00  75,00 
</p>
<p> 8  85,00  1,00  83,00 
</p>
<p> 9  70,00  1,00  35,00 
</p>
<p> 10  77,00  1,00  49,00 
</p>
<p>   Only the fi rst 10 patients are displayed above. The entire data fi le in   www.
</p>
<p>springer.com    . For the analysis we will use multiple linear regression.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Regression&hellip;.Linear&hellip;.Dependent: physical strength score&hellip;. Independent: 
</p>
<p>race, age, &hellip;.OK.   
</p>
<p> Coeffi cients a  
</p>
<p> Unstandardized 
</p>
<p>coeffi cients 
</p>
<p> Standardized 
</p>
<p>coeffi cients 
</p>
<p> Model  B  Std. Error  Beta  t  Sig. 
</p>
<p> 1  (Constant)  92,920  7,640  12,162  ,000 
</p>
<p> Race  &minus;,330  1,505  &minus;,027  &minus;,219  ,827 
</p>
<p> Age  &minus;,356  ,116  &minus;,383  &minus;3,071  ,003 
</p>
<p>   a Dependent variable: strengthscore 
</p>
<p>    The above table shows that age is a signifi cant predictor but race is not. However, 
</p>
<p>the analysis is not adequate, because the variable race is analyzed as a stepwise 
</p>
<p>function from 1 to 4, and the linear regression model assumes that the outcome vari-
</p>
<p>able will rise (or fall) linearly, but, in the data given, this needs not be necessarily so. 
</p>
<p>It may, therefore, be more safe to recode the stepping variable into the form of a 
</p>
<p>categorical variable. The underneath data overview shows in the right 4 columns 
</p>
<p>how it is manually done. 
</p>
<p> patient number  physical strength  race  age  race 1  race 2  race 3  race 4 
</p>
<p> hispanics  blacks  asians  whites 
</p>
<p> 1  70,00  1,00  35,00  1,00  0,00  0,00  0,00 
</p>
<p> 2  77,00  1,00  55,00  1,00  0,00  0,00  0,00 
</p>
<p> 3  66,00  1,00  70,00  1,00  0,00  0,00  0,00 
</p>
<p> 4  59,00  1,00  55,00  1,00  0,00  0,00  0,00 
</p>
<p> 5  71,00  1,00  45,00  1,00  0,00  0,00  0,00 
</p>
<p> 6  72,00  1,00  47,00  1,00  0,00  0,00  0,00 
</p>
<p> 7  45,00  1,00  75,00  1,00  0,00  0,00  0,00 
</p>
<p> 8  85,00  1,00  83,00  1,00  0,00  0,00  0,00 
</p>
<p> 9  70,00  1,00  35,00  1,00  0,00  0,00  0,00 
</p>
<p> 10  77,00  1,00  49,00  1,00  0,00  0,00  0,00 
</p>
<p>29 Various Methods for Analyzing Predictor Categories (60 and 30 Patients)</p>
<p/>
<div class="annotation"><a href="http://www.springer.com/">http://www.springer.com/</a></div>
<div class="annotation"><a href="http://www.springer.com/">http://www.springer.com/</a></div>
</div>
<div class="page"><p/>
<p>177
</p>
<p>   We, subsequently, use again linear regression, but now for categorical analysis 
</p>
<p>of race.
</p>
<p>  Command: 
</p>
<p>  click Transform&hellip;.click Random Number Generators&hellip;.click Set Starting Point&hellip;.  
</p>
<p>  click Fixed Value (2000000)&hellip;.click OK&hellip;.click Analyze&hellip;.Regression &hellip;.Linear  
</p>
<p>  &hellip;.Dependent: physical strength score&hellip;.Independent: race 1, race 3, race 4, age&hellip;.  
</p>
<p>  click Save&hellip;.mark Unstandardized&hellip;.in Export model information to XML 
</p>
<p>(eXtended Markup Language) fi le: type "exportcategoriesaspredictor"&hellip;.click 
</p>
<p>Browse&hellip;.File name: enter "exportcategoriesaspredictor"&hellip;.click Continue&hellip;.click 
</p>
<p>OK.   
</p>
<p> Coeffi cients a  
</p>
<p> Unstandardized coeffi cients  Standardized coeffi cients 
</p>
<p> Model  B  Std. Error  Beta  t  Sig. 
</p>
<p> 1  (Constant)  97,270  4,509  21,572  ,000 
</p>
<p> Age  &minus;,200  ,081  &minus;,215  &minus;2,457  ,017 
</p>
<p> Race1  &minus;17,483  3,211  &minus;,560  &minus;5,445  ,000 
</p>
<p> Race3  &minus;25,670  3,224  &minus;,823  &minus;7,962  ,000 
</p>
<p> Race4  &minus;8,811  3,198  &minus;,282  &minus;2,755  ,008 
</p>
<p>   a Dependent variable: strengthscore 
</p>
<p>    The above table is in the output. It shows that race 1, 3, 4 are signifi cant predic-
</p>
<p>tors of physical strength compared to race 2. The results can be interpreted as 
</p>
<p>follows. 
</p>
<p> The underneath regression equation is used:
</p>
<p>  
y a b x b x b x b x    
</p>
<p>1 1 2 2 3 3 4 4    
</p>
<p>a = intercept 
</p>
<p> b 1  = regression coeffi cient for age 
</p>
<p> b 2  = hispanics 
</p>
<p> b 3  = asians 
</p>
<p> b 4  = white 
</p>
<p> If an individual is black (race 2), then  x  2 , x 3 , and x 4  will turn into 0, and the 
</p>
<p>regression equation becomes
</p>
<p>  
</p>
<p>y a b x
</p>
<p>If hispanic y a b x b x
</p>
<p>If asian y a b x b x
</p>
<p>If whi
</p>
<p> 
  
</p>
<p>  
</p>
<p>1 1
</p>
<p>1 1 2 2
</p>
<p>1 1 3 3
</p>
<p>,
</p>
<p>,
</p>
<p>tte y a b x b x, .  
1 1 4 4    
</p>
<p>So, e.g., the best predicted physical strength score of a white male of 25 years of age 
</p>
<p>would equal
</p>
<p>  
</p>
<p>y
</p>
<p>sign of multiplication
</p>
<p>   
 
97 270 0 20 25 8 811 1 93 459. . * . * . ,
</p>
<p>* .
   
</p>
<p> Example 1</p>
<p/>
</div>
<div class="page"><p/>
<p>178
</p>
<p>Obviously, all of the races are negative predictors of physical strength, but the 
</p>
<p>blacks scored highest and the asians lowest. All of these results are adjusted for age. 
</p>
<p> If we return to the data fi le page, we will observe that SPSS has added a new 
</p>
<p>variable entitled &ldquo;PRE_1&rdquo;. It represents the individual strengthscores as predicted 
</p>
<p>by the recoded linear model. They are pretty similar to the measured values. 
</p>
<p> We can now with the help of the Scoring Wizard and the exported XML 
</p>
<p>(eXtended Markup Language) fi le entitled &ldquo;exportcategoriesaspredictor&rdquo; try and 
</p>
<p>predict strength scores of future patients with known race and age. 
</p>
<p> race  age 
</p>
<p> 1,00  40,00 
</p>
<p> 2,00  70,00 
</p>
<p> 3,00  54,00 
</p>
<p> 4,00  45,00 
</p>
<p> 1,00  36,00 
</p>
<p> 2,00  46,00 
</p>
<p> 3,00  50,00 
</p>
<p> 4,00  36,00 
</p>
<p>   First, recode the stepping variable race into 4 categorical variables. 
</p>
<p> race  age  race1  race3  race4 
</p>
<p> 1,00  40,00  1,00  ,00  ,00 
</p>
<p> 2,00  70,00  ,00  ,00  ,00 
</p>
<p> 3,00  54,00  ,00  1,00  ,00 
</p>
<p> 4,00  45,00  ,00  ,00  1,00 
</p>
<p> 1,00  36,00  1,00  ,00  ,00 
</p>
<p> 2,00  46,00  ,00  ,00  ,00 
</p>
<p> 3,00  50,00  ,00  1,00  ,00 
</p>
<p> 4,00  36,00  ,00  ,00  1,00 
</p>
<p>   Then Command: 
</p>
<p>  click Utilities&hellip;.click Scoring Wizard&hellip;.click Browse&hellip;.click Select&hellip;.Folder: 
</p>
<p>enter the exportcategoriesaspredictor.xml fi le&hellip;.click Select&hellip;.in Scoring Wizard 
</p>
<p>click Next&hellip;.click Finish.    
</p>
<p> race  age  race1  race3  race4  predicted strength score 
</p>
<p> 1,00  40,00  1,00  ,00  ,00  71,81 
</p>
<p> 2,00  70,00  ,00  ,00  ,00  83,30 
</p>
<p> 3,00  54,00  ,00  1,00  ,00  60,83 
</p>
<p> 4,00  45,00  ,00  ,00  1,00  79,48 
</p>
<p> 1,00  36,00  1,00  ,00  ,00  72,60 
</p>
<p> 2,00  46,00  ,00  ,00  ,00  88,09 
</p>
<p> 3,00  50,00  ,00  1,00  ,00  61,62 
</p>
<p> 4,00  36,00  ,00  ,00  1,00  81,28 
</p>
<p>29 Various Methods for Analyzing Predictor Categories (60 and 30 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>179
</p>
<p>   The above data fi le now gives predicted strength scores of the 8 future patients as 
</p>
<p>computed with help of the XML fi le. 
</p>
<p> Also with a binary outcome variable categorical analysis of covariates is  possible. 
</p>
<p>Using logistic regression in SPSS is convenient for the purpose, we need not 
</p>
<p>  manually  transform the quantitative estimator into a categorical one. For the  analysis 
</p>
<p>we have to apply the usual commands.
</p>
<p>  Command: 
</p>
<p>  Analyze &hellip;.Regression&hellip;.Binary logistic&hellip;.Dependent variable&hellip;. Independent 
</p>
<p>variables&hellip;.then, open dialog box labeled Categorical Variables&hellip;. select the cate-
</p>
<p>gorical variable and transfer it to the box Categorical Variables&hellip;.then click 
</p>
<p>Continue&hellip;.OK.     
</p>
<p>    Example 2 
</p>
<p> Particularly, if Gaussian distributions in the outcome are uncertain, automatic 
</p>
<p> non- parametric testing is an adequate and very convenient modern alternative. 
</p>
<p>Three parallel-groups were treated with different sleeping pills. Both hours of sleep 
</p>
<p>and side effect score were assessed. 
</p>
<p> Group  effi cacy  gender  comorbidity  side effect score 
</p>
<p> 0  6,00  ,00  1,00  45,00 
</p>
<p> 0  7,10  ,00  1,00  35,00 
</p>
<p> 0  8,10  ,00  ,00  34,00 
</p>
<p> 0  7,50  ,00  ,00  29,00 
</p>
<p> 0  6,40  ,00  1,00  48,00 
</p>
<p> 0  7,90  1,00  1,00  23,00 
</p>
<p> 0  6,80  1,00  1,00  56,00 
</p>
<p> 0  6,60  1,00  ,00  54,00 
</p>
<p> 0  7,30  1,00  ,00  33,00 
</p>
<p> 0  5,60  ,00  ,00  75,00 
</p>
<p>   Only the fi rst ten patients are shown. The entire data fi le is in extras.springer.com 
</p>
<p>and is entitled &ldquo;categoriesaspredictor2&rdquo;. Automatic nonparametric tests is available 
</p>
<p>in SPSS 18 and up. Start by opening the above data fi le.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Nonparametric Tests&hellip;.Independent Samples&hellip;.click Objective&hellip;.mark 
</p>
<p>Automatically compare distributions across groups&hellip;.click Fields&hellip;.in Test fi elds: 
</p>
<p>enter "hours of sleep" and "side effect score"&hellip;.in Groups: enter "group"&hellip;.click 
</p>
<p>Settings&hellip;.Choose Tests&hellip;.mark "Automatically choose the tests based on the 
</p>
<p>data"&hellip;.click Run.    
</p>
<p> Example 2</p>
<p/>
</div>
<div class="page"><p/>
<p>180
</p>
<p> In the interactive output sheets the underneath table is given. Both the  distribution 
</p>
<p>of hours of sleep and side effect score are signifi cantly different across the three 
</p>
<p>categories of treatment. The traditional assessment of these data would have been a 
</p>
<p>multivariate analysis of variance (MANOVA) with treatment-category as predictor 
</p>
<p>and both hours of sleep and side effect score as outcome. However, normal 
</p>
<p> distributions are uncertain in this example, and the correlation between the two 
</p>
<p>outcome measures may not be zero, reducing the sensitivity of MANOVA. A nice 
</p>
<p>thing about the automatic nonparametric tests is that, like discriminant analysis 
</p>
<p>(Machine learning in medicine part one, Chap. 17, Discriminant analysis for 
</p>
<p> supervised data, pp 215&ndash;224, Springer Heidelberg Germany, 2013, from the same 
</p>
<p>authors), they assume orthogonality of the two outcomes, which means that the cor-
</p>
<p>relation level between the two does not have to be taken into account. By double-
</p>
<p>clicking the table you will obtain an interactive set of views of various details of the 
</p>
<p>analysis, entitled the Model Viewer.
</p>
<p> Hypothesis test summary 
</p>
<p> Null hypothesis  Test  Sig.  Decision 
</p>
<p> 1  The distribution of hours of sleep is the 
</p>
<p>same across categories of group. 
</p>
<p> Independent-Samples 
</p>
<p>Kruskal-Wallis Test 
</p>
<p> ,001  Reject the null 
</p>
<p>hypothesis. 
</p>
<p> 2  The distribution of side effect score is 
</p>
<p>the same across categories of group. 
</p>
<p> Independent-Samples 
</p>
<p>Kruskal-Wallis Test 
</p>
<p> ,036  Reject the null 
</p>
<p>hypothesis. 
</p>
<p>  Asymptotic signifi cances are displayed. The signifi cance level is,05 
</p>
<p>    One view provides the box and whiskers graphs (medians, quartiles, and ranges) 
</p>
<p>of hours of sleep of the three treatment groups. Group 0 seems to perform better 
</p>
<p>than the other two, but we don&rsquo;t know where the signifi cant differences are. 
</p>
<p>  
</p>
<p>8,00
</p>
<p>6,00
</p>
<p>4,00
</p>
<p>2,00
</p>
<p>0 1
</p>
<p>group
</p>
<p>h
o
</p>
<p>u
rs
</p>
<p> o
f 
</p>
<p>s
le
</p>
<p>e
p
</p>
<p>2
</p>
<p>  
</p>
<p>    Also the box and whiskers graph of side effect scores is given. Some groups 
</p>
<p>again seem to perform better than the other. However, we cannot see whether 0 vs 
</p>
<p>1, 1 vs 2, and/or 0 vs 2 are signifi cantly different. 
</p>
<p>29 Various Methods for Analyzing Predictor Categories (60 and 30 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>181
</p>
<p>  
</p>
<p>80,00
</p>
<p>60,00
</p>
<p>40,00
</p>
<p>20,00
</p>
<p>0
</p>
<p>group
</p>
<p>s
id
</p>
<p>e
 e
</p>
<p>ff
e
c
t 
</p>
<p>s
c
o
</p>
<p>re
</p>
<p>1 2
</p>
<p>  
</p>
<p>    In the view space at the bottom of the auxiliary view (right half of the Model 
</p>
<p>Viewer) several additional options are given. When clicking Pairwise Comparisons, 
</p>
<p>a distance network is displayed with yellow lines corresponding to statistically sig-
</p>
<p>nifi cant differences, and black ones to insignifi cant ones. Obviously, the differences 
</p>
<p>in hours of sleep of group 1 vs (versus) 0 and group 2 vs 0 are statistically signifi -
</p>
<p>cant, and 1 vs 2 is not. Group 0 had signifi cantly more hours of sleep than the other 
</p>
<p>two groups with p = 0.044 and 0.0001. 
</p>
<p>  
</p>
<p>Pairwise Comparisons of group
</p>
<p>23,75
</p>
<p>14,15
</p>
<p>8.60
2
</p>
<p>1
</p>
<p>0
</p>
<p>                       
</p>
<p> Each node shows the sample average rank of group 
</p>
<p> Sample1-  Sample2  Test statistic  Std. Error  Std. Test statistic  Sig.  Adj.Sig. 
</p>
<p> 2-  1  5,550  3,936  1,410  ,158  ,475 
</p>
<p> 2-  0  15,150  3,936  3,849  ,000  ,000 
</p>
<p> 1-  0  9,600  3,936  2,439  ,015  ,044 
</p>
<p>  Each row tests the null hypothesis that the Sample 1 and Sample 2 distributions are the same 
</p>
<p> Asymptotic signifi cances (2-sided tests) are displayed. The signifi cance level is ,05 
</p>
<p>Example 2</p>
<p/>
</div>
<div class="page"><p/>
<p>182
</p>
<p>    As shown below, the difference in side effect score of group 1 vs 0 is also 
</p>
<p> statistically signifi cant, and 1 vs 0, and 1 vs 2 are not. Group 0 has a signifi cantly 
</p>
<p>better side effect score than the 1 with p = 0.035, but group 0 vs 2 and 1 vs 2 are not 
</p>
<p>signifi cantly different.
</p>
<p>   
</p>
<p>19,85
</p>
<p>16,70
</p>
<p>9,95
0
</p>
<p>Pairwise Comparisons of group
</p>
<p>2
</p>
<p>1
</p>
<p>                       
</p>
<p> Each node shows the sample average rank of group 
</p>
<p> Sample1-  Sample2  Test statistic  Std. Error  Std. Test statistic  Sig.  Adj.Sig. 
</p>
<p> 0-  2  &minus;6,750  3,931  &minus;1,717  ,086  ,258 
</p>
<p> 0-  1  &minus;9,900  3,931  &minus;2,518  ,012  ,035 
</p>
<p> 2-  1  3,150  3,931  ,801  ,423  1,000 
</p>
<p>  Each row tests the null hypothesis that the Sample 1 and Sample 2 distributions are the same 
</p>
<p> Asymptotic signifi cances (2-sided tests) are displayed. The signifi cance level is ,05 
</p>
<p>        Conclusion 
</p>
<p> Predictor variables with a categorical rather than linear character should be recoded 
</p>
<p>into categorical variables before analysis in a regression model. An example is 
</p>
<p>given. Particularly if the Gaussian distributions in the outcome are uncertain, 
</p>
<p> automatic non-parametric testing is an adequate and very convenient alternative.  
</p>
<p>    Note 
</p>
<p> More background theoretical and mathematical information of categories as predic-
</p>
<p>tor is given in SPSS for starters part two, Chap. 5, Categorical data, pp 21&ndash;24, and 
</p>
<p>Statistics applied to clinical studies 5th edition, Chap. 21, Races as a categorical 
</p>
<p>variable, pp 244&ndash;252, both from the same authors and edited by Springer Heidelberg 
</p>
<p>Germany 2012.    
</p>
<p>29 Various Methods for Analyzing Predictor Categories (60 and 30 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>183&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_30
</p>
<p>    Chapter 30   
</p>
<p> Random Intercept Models for Both Outcome 
and Predictor Categories (55 patients) 
</p>
<p>                      General Purpose 
</p>
<p> Categories are very common in medical research. Examples include age classes, 
</p>
<p>income classes, education levels, drug dosages, diagnosis groups, disease severities, 
</p>
<p>etc. Statistics has generally diffi culty to assess categories, and traditional models 
</p>
<p>require either binary or continuous variables. If in the outcome, categories can be 
</p>
<p>assessed with multinomial regression (see the above Chap.   28    ), if as predictors, they 
</p>
<p>can be assessed with automatic nonparametric tests (see the above Chap.   29    ). 
</p>
<p>However, with multiple categories or with categories both in the outcome and as 
</p>
<p>predictors, random intercept models may provide better sensitivity of testing. The 
</p>
<p>latter models assume that for each predictor category or combination of categories 
</p>
<p>x 1 , x 2 ,&hellip;slightly different a-values can be computed with a better fi t for the outcome 
</p>
<p>category y than a single a-value.
</p>
<p>  
y a b x b x   
</p>
<p>1 1 2 2
.
   
</p>
<p>We should add that, instead of the above linear equation, even better results were 
</p>
<p>obtained with log-linear equations (log = natural logarithm).
</p>
<p>  
log .y a b x b x   
</p>
<p>1 1 2 2    
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 2&rdquo; as Chap. 6, 
</p>
<p>2014. </p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_28">http://dx.doi.org/10.1007/978-3-319-15195-3_28</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_29">http://dx.doi.org/10.1007/978-3-319-15195-3_29</a></div>
</div>
<div class="page"><p/>
<p>184
</p>
<p>      Specifi c Scientifi c Question 
</p>
<p> In a study three hospital departments (no surgery, little surgery, lot of surgery), and 
</p>
<p>three patient age classes (young, middle, old) were the predictors of the risk class of 
</p>
<p>falling out of bed (fall out of bed no, yes but no injury, yes and injury). Are the pre-
</p>
<p>dictor categories signifi cant determinants of the risk of falling out of bed with or 
</p>
<p>without injury. Does a random intercept provide better statistics.  
</p>
<p>    Example 
</p>
<p> department  falloutofbed  agecat  patient_id 
</p>
<p> 0  1  1,00  1,00 
</p>
<p> 0  1  1,00  2,00 
</p>
<p> 0  1  2,00  3,00 
</p>
<p> 0  1  1,00  4,00 
</p>
<p> 0  1  1,00  5,00 
</p>
<p> 0  1  ,00  6,00 
</p>
<p> 1  1  2,00  7,00 
</p>
<p> 0  1  2,00  8,00 
</p>
<p> 1  1  2,00  9,00 
</p>
<p> 0  1  ,00  10,00 
</p>
<p>  Variable 1: department = department class (0 = no 
</p>
<p>surgery, 1 = little surgery, 2 = lot of surgery) 
</p>
<p> Variable 2: falloutofbed = risk of falling out of bed 
</p>
<p>(0 = fall out of bed no, 1 = yes but no injury, 2 = yes 
</p>
<p>and injury) 
</p>
<p> Variable 3: agecat = patient age classes (young, middle, old) 
</p>
<p> Variable 4: patient_id = patient identifi cation 
</p>
<p>    Only the fi rst 10 patients of the 55 patient fi le is shown above. The entire data fi le is in 
</p>
<p>extras.springer.com and is entitled &ldquo;randomintercept.sav&rdquo;. SPSS version 20 and up 
</p>
<p>can be used for analysis. First, we will perform a fi xed intercept log-linear analysis.
</p>
<p>  Command: 
</p>
<p>  click Analyze&hellip;.Mixed Models....Generalized Linear Mixed Models....click Data 
</p>
<p>Structure&hellip;.click "patient_id" and drag to Subjects on the Canvas&hellip;.click Fields 
</p>
<p>and Effects&hellip;.click Target&hellip;.Target: select "fall with/out injury"&hellip;.click Fixed 
</p>
<p>Effects&hellip;.click "agecat"and "department" and drag to Effect Builder:&hellip;.mark 
</p>
<p>Include intercept&hellip;.click Run.    
</p>
<p> The underneath results show that both the various regression coeffi cients as well 
</p>
<p>as the overall correlation coeffi cients between the predictors and the outcome are, 
</p>
<p>generally, statistically signifi cant.
</p>
<p>30 Random Intercept Models for Both Outcome and Predictor Categories (55 patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>185
</p>
<p> Source  F  df1  df2  Sig. 
</p>
<p> Corrected Model  9,398  4  10  ,002 
</p>
<p> Agecat  6,853  2  10  ,013 
</p>
<p> Department  9,839  2  10  ,004 
</p>
<p>  Probability distribution: Multinomial 
</p>
<p> Link function: Cumulative logit 
</p>
<p> Model Term  Coeffi cient  Sig. 
</p>
<p> Threshold for falloutofbed= 
</p>
<p> 0  2,140  ,028 
</p>
<p> 1  7,229  ,000 
</p>
<p> Agecat=0  5,236  ,005 
</p>
<p> Agecat=1  &minus;0,002  ,998 
</p>
<p> Agecat=2  0,000 a  
</p>
<p> Department=0  3,660  ,008 
</p>
<p> Department=1  4,269  ,002 
</p>
<p> Department=2  0,000 a  
</p>
<p>  Probability distribution: Multinomial 
</p>
<p> Link function: Cumulative logit 
</p>
<p>  a This coeffi cient is set to zero because it is redundant 
</p>
<p>    Subsequently, a random intercept analysis is performed.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Mixed Models....Generalized Linear Mixed Models....click Data 
</p>
<p>Structure&hellip;.click "patient_id" and drag to Subjects on the Canvas&hellip;.click Fields and 
</p>
<p>Effects&hellip;.click Target&hellip;.Target: select "fall with/out injury"&hellip;.click Fixed Effects&hellip;.
</p>
<p>click "agecat"and "department" and drag to Effect Builder:&hellip;.mark Include inter-
</p>
<p>cept&hellip;.click Random Effects&hellip;.click Add Block&hellip;mark Include intercept&hellip;.Subject 
</p>
<p>combination: select patient_id&hellip;.click OK&hellip;.click Model Options&hellip;.click Save 
</p>
<p>Fields&hellip;mark PredictedValue&hellip;.mark PredictedProbability&hellip;.click Save....click Run.    
</p>
<p> The underneath results show the test statistics of the random intercept model. 
</p>
<p>The random intercept model shows better statistics:
</p>
<p>   p = 0.007 and 0.013 overall for age,  
</p>
<p>  p = 0.001 and 0.004 overall for department,  
</p>
<p>  p = 0.003 and 0.005 regression coeffi cients for age class 0 versus 2,  
</p>
<p>  p = 0.900 and 0.998 for age class 1 versus 2,  
</p>
<p>  p = 0.004 and 0.008 for department 0 versus 2, and  
</p>
<p>  p = 0.001 and 0.0002 for department 1 versus 2.   
</p>
<p> Source  F  df1  df2  Sig. 
</p>
<p> Corrected Model  7,935  4  49  ,000 
</p>
<p> Agecat  5,513  2  49  ,007 
</p>
<p> Department  7,602  2  49  ,001 
</p>
<p>  Probability distribution: Multinomial 
</p>
<p> Link function: Cumulative logit 
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>186
</p>
<p> Model term  Coeffi cient  Sig. 
</p>
<p> Threshold for falloutofbed=  0  2,082  ,015 
</p>
<p> 1  5,464  ,000 
</p>
<p> Agecat=0  3,869  ,003 
</p>
<p> Agecat=1  0,096  ,900 
</p>
<p> Agecat=2  0,000 a  
</p>
<p> Department=0  3,228  ,004 
</p>
<p> Department=1  3,566  ,000 
</p>
<p> Department=2  0,000 a  
</p>
<p>  Probability distribution: Multinomial 
</p>
<p> Link function: Cumulative logit 
</p>
<p>  a This coeffi cient is set to zero because it is redundant 
</p>
<p>    In the random intercept model we have also commanded predicted values (vari-
</p>
<p>able 7) and predicted probabilities of having the predicted values as computed by 
</p>
<p>the software (variables 5 and 6). 
</p>
<p> 1  2  3  4  5  6  7 (variables) 
</p>
<p> 0  1  1,00  1,00  ,224  ,895  1 
</p>
<p> 0  1  1,00  2,00  ,224  ,895  1 
</p>
<p> 0  1  2,00  3,00  ,241  ,903  1 
</p>
<p> 0  1  1,00  4,00  ,224  ,895  1 
</p>
<p> 0  1  1,00  5,00  ,224  ,895  1 
</p>
<p> 0  1  ,00  6,00  ,007  ,163  2 
</p>
<p> 1  1  2,00  7,00  ,185  ,870  1 
</p>
<p> 0  1  2,00  8,00  ,241  ,903  1 
</p>
<p> 1  1  2,00  9,00  ,185  ,870  1 
</p>
<p> 0  1  ,00  10,00  ,007  ,163  2 
</p>
<p>  Variable 1: department 
</p>
<p> Variable 2: falloutofbed 
</p>
<p> Variable 3: agecat 
</p>
<p> Variable 4: patient_id 
</p>
<p> Variable 5: predicted probability of predicted value of target accounting the department score only 
</p>
<p> Variable 6: predicted probability of predicted value of target accounting both department and age-
</p>
<p>cat scores 
</p>
<p> Variable 7: predicted value of target 
</p>
<p>    Like automatic linear regression (see Chap.   31    ) and other generalized mixed 
</p>
<p>linear models (see Chap.   33    ) random intercept models include the possibility to 
</p>
<p>make XML fi les from the analysis, that can subsequently be used for making pre-
</p>
<p>dictions about the chance of falling out of bed in future patients. However, SPSS 
</p>
<p>uses here slightly different software called winRAR ZIP fi les that are &ldquo;shareware&rdquo;. 
</p>
<p>This means that you pay a small fee and be registered if you wish to use it. Note that 
</p>
<p>winRAR ZIP fi les have an archive fi le format consistent of compressed data used by 
</p>
<p>30 Random Intercept Models for Both Outcome and Predictor Categories (55 patients)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_31">http://dx.doi.org/10.1007/978-3-319-15195-3_31</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_33">http://dx.doi.org/10.1007/978-3-319-15195-3_33</a></div>
</div>
<div class="page"><p/>
<p>187
</p>
<p>Microsoft since 2006 for the purpose of fi ling XML (eXtended Markup Language) 
</p>
<p>fi les. They are only employable for a limited period of time like e.g. 40 days.  
</p>
<p>    Conclusion 
</p>
<p> Generalized linear mixed models are suitable for analyzing data with multiple cat-
</p>
<p>egorical variables. Random intercept versions of these models provide better sensi-
</p>
<p>tivity of testing than fi xed intercept models.  
</p>
<p>    Note 
</p>
<p> More information on statistical methods for analyzing data with categories is in the 
</p>
<p>Chaps.   28     and   29     of this book.    
</p>
<p>Note</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_28">http://dx.doi.org/10.1007/978-3-319-15195-3_28</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_29">http://dx.doi.org/10.1007/978-3-319-15195-3_29</a></div>
</div>
<div class="page"><p/>
<p>189&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_31
</p>
<p>    Chapter 31   
</p>
<p> Automatic Regression for Maximizing Linear 
Relationships (55 patients) 
</p>
<p>                      General Purpose 
</p>
<p> Automatic linear regression is in the Statistics Base add-on module SPSS version 19 
</p>
<p>and up. X-variables are automatically transformed in order to provide an improved 
</p>
<p>data fi t, and SPSS uses rescaling of time and other measurement values, outlier 
</p>
<p>trimming, category merging and other methods for the purpose. This chapter is to 
</p>
<p>assess whether automatic linear regression is helpful to obtain an improved preci-
</p>
<p>sion of analysis of clinical trials.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> In a clinical crossover trial an old laxative is tested against a new one. Numbers of 
</p>
<p>stools per month is the outcome. The old laxative and the patients&rsquo; age are the pre-
</p>
<p>dictor variables. Does automatic linear regression provide better statistics of these 
</p>
<p>data than traditional multiple linear regression does.  
</p>
<p>    Data Example  
</p>
<p> Patno  newtreat  oldtreat  age categories 
</p>
<p> 1,00  24,00  8,00  2,00 
</p>
<p> 2,00  30,00  13,00  2,00 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 2&rdquo; as Chap. 7, 
</p>
<p>2014. 
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>190
</p>
<p> Patno  newtreat  oldtreat  age categories 
</p>
<p> 3,00  25,00  15,00  2,00 
</p>
<p> 4,00  35,00  10,00  3,00 
</p>
<p> 5,00  39,00  9,00  3,00 
</p>
<p> 6,00  30,00  10,00  3,00 
</p>
<p> 7,00  27,00  8,00  1,00 
</p>
<p> 8,00  14,00  5,00  1,00 
</p>
<p> 9,00  39,00  13,00  1,00 
</p>
<p> 10,00  42,00  15,00  1,00 
</p>
<p>  patno = patient number 
</p>
<p> newtreat = frequency of stools on a novel laxative 
</p>
<p> oldtreat = frequency of stools on an old laxative 
</p>
<p> agecategories = patients&rsquo; age categories (1 = young, 
</p>
<p>2 = middle-age, 3 = old) 
</p>
<p>    Only the fi rst 10 patients of the 55 patients are shown above. The entire fi le is in 
</p>
<p>extras.springer.com and is entitled &ldquo;automaticlinreg&rdquo;. We will fi rst perform a stan-
</p>
<p>dard multiple linear regression.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Regression&hellip;.Linear&hellip;.Dependent: enter newtreat&hellip;.Independent: 
</p>
<p>enter oldtreat and agecategories&hellip;.click OK.   
</p>
<p> Model summary 
</p>
<p> Model  R  R Square  Adjusted R Square  Std. error of the estimate 
</p>
<p> 1  ,429 a   ,184  ,133  9,28255 
</p>
<p>   a Predictors: (Constant), oldtreat, agecategories 
</p>
<p> ANOVA a  
</p>
<p> Model  Sum of squares  df  Mean square  F  Sig. 
</p>
<p> Regression  622,869  2  311,435  3,614  ,038 b  
</p>
<p> 1  Residual  2757,302  32  86,166 
</p>
<p> Total  3380,171  34 
</p>
<p>   a Dependent variable: newtreat 
</p>
<p>  b Predictors: (Constant), oldtreat, agecategories 
</p>
<p> Coeffi cients  a   
</p>
<p> Model 
</p>
<p> Unstandardized coeffi cients 
</p>
<p> Standardized 
</p>
<p>coeffi cients 
</p>
<p> t  Sig.  B  Std. Error  Beta 
</p>
<p> (Constant)  20,513  5,137  3,993  ,000 
</p>
<p> 1 agecategories  3,908  2,329  ,268  1,678  ,103 
</p>
<p> oldtreat  ,135  ,065  ,331  2,070  ,047 
</p>
<p>   a Dependent variable: newtreat 
</p>
<p>31 Automatic Regression for Maximizing Linear Relationships (55 patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>191
</p>
<p> Automatic Data Preparation 
</p>
<p> Target: newtreat 
</p>
<p> Field  Role  Actions taken 
</p>
<p> (agecategories_transformed)  Predictor  Merge categories to maximize association 
</p>
<p>with target 
</p>
<p> (oldtreat_transformed)  Predictor  Trim outliers 
</p>
<p>  If the original fi eld name is X, then the transformed fi eld is displayed as (X_transformed) The 
</p>
<p>original fi eld is excluded from the analysis and the transformed fi eld is included instead 
</p>
<p>    An interactive graph shows the predictors as lines with thicknesses correspond-
</p>
<p>ing to their predictive power and the outcome in the form of a histogram with its 
</p>
<p>best fi t Gaussian pattern. Both of the predictors are now statistically very signifi cant 
</p>
<p>with a correlation coeffi cient at p &lt; 0.0001, and regression coeffi cients at p-values of 
</p>
<p>respectively 0.001 and 0.007.
</p>
<p> 
</p>
<p>Intercept
</p>
<p>agecategories_...
</p>
<p>oldtreat_...
</p>
<p>newtreat
</p>
<p>Coefficient
</p>
<p>Estimate
</p>
<p>Coefficients
</p>
<p>Target: newtreat
</p>
<p>Positive
</p>
<p>Negative
</p>
<p>   
</p>
<p> Coeffi cients 
</p>
<p> Target: newtreat 
</p>
<p> Model term  Coeffi cient       Sig.  Importance 
</p>
<p> Intercept  35.926  .000 
</p>
<p> Agecategories_transformed=0  &ndash;11.187  .001  0.609 
</p>
<p> Agecategories_transformed=1  0.000 a   0.609 
</p>
<p> Oldtreat_transformed  0.209  .007  0.391 
</p>
<p>   a This coeffi cient is set to zero because it is redundant 
</p>
<p> Effects 
</p>
<p> Target: newtreat 
</p>
<p> Source  Sum of squares  df  Mean square  F  Sig. 
</p>
<p> Corrected model       1.289,960  2  644,980  9,874  ,000 
</p>
<p> Residual  2.090,212  32  65,319 
</p>
<p> Corrected total  3380,171  34 
</p>
<p>Data Example </p>
<p/>
</div>
<div class="page"><p/>
<p>192
</p>
<p>   Returning to the data view of the original data fi le, we now observe that SPSS 
</p>
<p>has provided a novel variables with values for the new treatment as predicted 
</p>
<p>from statistical model employed. They are pretty close to the real outcome 
</p>
<p>values. 
</p>
<p> Patno  newtreat  oldtreat  age categories  Predicted Values 
</p>
<p> 1,00  24,00  8,00  2,00  26,41 
</p>
<p> 2,00  30,00  13,00  2,00  27,46 
</p>
<p> 3,00  25,00  15,00  2,00  27,87 
</p>
<p> 4,00  35,00  10,00  3,00  38,02 
</p>
<p> 5,00  39,00  9,00  3,00  37,81 
</p>
<p> 6,00  30,00  10,00  3,00  38,02 
</p>
<p> 7,00  27,00  8,00  1,00  26,41 
</p>
<p> 8,00  14,00  5,00  1,00  25,78 
</p>
<p> 9,00  39,00  13,00  1,00  27,46 
</p>
<p> 10,00  42,00  15,00  1,00  27,87 
</p>
<p>  patno = patient number 
</p>
<p> newtreat = frequency of stools on a novel laxative 
</p>
<p> oldtreat = frequency of stools on an old laxative 
</p>
<p> agecategories = patients&rsquo; age categories (1 = young, 2 = middle-age, 3 = old) 
</p>
<p>        The Computer Teaches Itself to Make Predictions 
</p>
<p> The modeled regression coeffi cients are used to make predictions about future data 
</p>
<p>using the Scoring Wizard and an XML (eXtended Markup Language) fi le (winRAR 
</p>
<p>ZIP fi le) of the data fi le. Like random intercept models (see Chap. 6) and other 
</p>
<p>generalized mixed linear models (see Chap. 9) automatic linear regression includes 
</p>
<p>the possibility to make XML fi les from the analysis, that can subsequently be used 
</p>
<p>for making outcome predictions in future patients. SPSS uses here software called 
</p>
<p>winRAR ZIP fi les that are &ldquo;shareware&rdquo;. This means that you pay a small fee 
</p>
<p>and be registered if you wish to use it. Note that winRAR ZIP fi les have a archive 
</p>
<p>fi le format consistent of compressed data used by Microsoft since 2006 for the pur-
</p>
<p>pose of fi ling XML fi les. They are only employable for a limited period of time like 
</p>
<p>e.g. 40 days. Below the data of 9 future patients are given. 
</p>
<p> Newtreat  oldtreat  agecategory 
</p>
<p> 4,00  1,00 
</p>
<p> 13,00  1,00 
</p>
<p> 15,00  1,00 
</p>
<p> 15,00  1,00 
</p>
<p>(continued)
</p>
<p>31 Automatic Regression for Maximizing Linear Relationships (55 patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>193
</p>
<p> Newtreat  oldtreat  agecategory 
</p>
<p> 11,00  2,00 
</p>
<p> 80,00  2,00 
</p>
<p> 10,00  3,00 
</p>
<p> 18,00  2,00 
</p>
<p> 13,00  2,00 
</p>
<p>   Enter the above data in a novel data fi le and command:
</p>
<p>   Utilities&hellip;.click Scoring Wizard&hellip;.click Browse&hellip;.Open the appropriate folder 
</p>
<p>with the XML fi le entitled "exportautomaticlinreg"&hellip;.click on the latter and click 
</p>
<p>Select&hellip;.in Scoring Wizard double-click Next&hellip;.mark Predicted Value&hellip;.click 
</p>
<p>Finish.    
</p>
<p> Newtreat  oldtreat  agecategory  predictednewtreat 
</p>
<p> 4,00  1,00  25,58 
</p>
<p> 13,00  1,00  27,46 
</p>
<p> 15,00  1,00  27,87 
</p>
<p> 15,00  1,00  27,87 
</p>
<p> 11,00  2,00  27,04 
</p>
<p> 80,00  2,00  41,46 
</p>
<p> 10,00  3,00  38,02 
</p>
<p> 18,00  2,00  28,50 
</p>
<p> 13,00  2,00  27,46 
</p>
<p>   In the data fi le SPSS has provided the novel variable as requested. The fi rst 
</p>
<p>patient with only 4 stools per month on the old laxative and young of age will have 
</p>
<p>over 25 stools on the new laxative.  
</p>
<p>    Conclusion 
</p>
<p> SPSS&rsquo; automatic linear regression can be helpful to obtain an improved precision of 
</p>
<p>analysis of clinical trials and provided in the example given better statistics than 
</p>
<p>traditional multiple linear regression did.  
</p>
<p> Conclusion</p>
<p/>
</div>
<div class="page"><p/>
<p>194
</p>
<p>    Note 
</p>
<p> More background theoretical and mathematical information of linear regression is 
</p>
<p>available in Statistics applied to clinical studies 5th edition, Chap. 14, entitled 
</p>
<p>Linear regression basic approach, and Chap. 15, Linear regression for assessing 
</p>
<p>precision confounding interaction, Chap. 18, Regression modeling for improved 
</p>
<p>precision, pp 161&ndash;176, 177&ndash;185, 219&ndash;225, Springer Heidelberg Germany, 2013, 
</p>
<p>from the same authors.    
</p>
<p>31 Automatic Regression for Maximizing Linear Relationships (55 patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>195&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_32
</p>
<p>    Chapter 32   
</p>
<p> Simulation Models for Varying Predictors 
(9,000 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> In medicine predictors are often varying, like, e.g., the numbers of complications 
</p>
<p>and the days in hospital in patients with various conditions. This chapter is to assess, 
</p>
<p>whether Monte Carlo simulation of the varying predictors can improve the outcome 
</p>
<p>predictions.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> The hospital costs for patients with heart infarction are supposed to be dependent on 
</p>
<p>factors like patients&rsquo; age, intensive care hours (ichours), numbers of complications. 
</p>
<p>What percentage of patients will cost the hospital over 20,000 Euros, what percent-
</p>
<p>age over 10,000. How will costs develop if the numbers of complications are 
</p>
<p>reduced by 2 and the numbers of ichours by 20.  
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 2&rdquo; as Chap. 8, 
</p>
<p>2014. </p>
<p/>
</div>
<div class="page"><p/>
<p>196
</p>
<p>    Instead of Traditional Means and Standard Deviations, 
</p>
<p>Monte Carlo Simulations of the Input and Outcome Variables 
</p>
<p>are Used to Model the Data. This Enhances Precision, 
</p>
<p>Particularly, With non-Normal Data 
</p>
<p> Age Years  complication number  ic hours  costs Euros 
</p>
<p> 48  7  36  5488 
</p>
<p> 66  7  57  8346 
</p>
<p> 75  7  67  6976 
</p>
<p> 72  6  45  5691 
</p>
<p> 60  6  58  3637 
</p>
<p> 84  9  54  16369 
</p>
<p> 74  8  54  11349 
</p>
<p> 42  9  26  10213 
</p>
<p> 71  7  49  6474 
</p>
<p> 73  10  35  30018 
</p>
<p> 53  8  37  7632 
</p>
<p> 79  6  46  6538 
</p>
<p> 50  10  39  13797 
</p>
<p>   Only the fi rst 13 patients of this 9000 patient hypothesized data fi le is shown. The 
</p>
<p>entire data fi le is in extras.springer.com and is entitled "simulation1.sav". SPSS 21 or 
</p>
<p>22 can be used. Start by opening the data fi le. We will fi rst perform a traditional linear 
</p>
<p>regression with the fi rst three variables as input and the fourth variable as outcome.
</p>
<p>  Command: 
</p>
<p>  click Transform&hellip;.click Random Number Generators&hellip;.click Set Starting Point 
</p>
<p>&hellip;.click Fixed Value (2000000)&hellip;.click OK&hellip;.click Analyze &hellip;.Regression&hellip;.
</p>
<p>Linear&hellip;.Dependent: costs&hellip;Independent: age, complication, ichours&hellip;.click 
</p>
<p>Save&hellip;.click Browse&hellip;.Select the desired folder in your computer&hellip;.File name: 
</p>
<p>enter "exportsimulation"&hellip;.click Save&hellip;.click Continue&hellip;.click OK.    
</p>
<p> In the output sheets it is observed that all of the predictors are statistically very 
</p>
<p>signifi cant. Also a PMML (predictive model markup language) document, other-
</p>
<p>wise called XML (eXtended Markup Language) document has been produced and 
</p>
<p>fi led in your computer entitled "exportsimulation". 
</p>
<p> Coeffi cients a  
</p>
<p> Model 
</p>
<p> Unstandardized coeffi cients 
</p>
<p> Standardized 
</p>
<p>coeffi cients 
</p>
<p> t  Sig.  B  Std. Error  Beta 
</p>
<p> (Constant)  &minus;28570,977  254,044  &minus;112,465  ,000 
</p>
<p> age(years)  202,403  2,767  ,318  73,136  ,000 
</p>
<p> complications(n)  4022,405  21,661  ,807  185,696  ,000 
</p>
<p> ichours(hours)  &minus;111,241  2,124  &minus;,227  &minus;52,374  ,000 
</p>
<p>   a Dependent variable: cost (Euros)    
</p>
<p>32 Simulation Models for Varying Predictors (9,000 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>197
</p>
<p> We will now perform the Monte Carlo simulation.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Simulation&hellip;.click Select SPSS Model File&hellip;.click Continue&hellip;.in Look 
</p>
<p>in: select folder with "exportsimulation.xml" fi le&hellip;.click Open&hellip;.click Simulation 
</p>
<p>Fields&hellip;.click Fit All&hellip;.click Save&hellip;.mark Save the plan fi le for this simulation&hellip;.
</p>
<p>click Browse&hellip;.in Look in: select the appropriate folder for storage of a simulation 
</p>
<p>plan document and entitle it, e.g., "splan"&hellip;.click Save&hellip;.click Run.    
</p>
<p> In the output the underneath interactive probability density graph is exhibited. 
</p>
<p>After double-clicking the vertical lines can be moved and corresponding areas under 
</p>
<p>the curve percentages are shown. 
</p>
<p>    
</p>
<p>    Overall 90 % of the heart attacks patients will cost the hospital between 440 and 
</p>
<p>21.630 Euros. In the graph click Chart Options&hellip;.in View click Histogram&hellip;.click 
</p>
<p>Continue. 
</p>
<p> The histogram below is displayed. Again the vertical lines can be moved as 
</p>
<p>desired. It can, e.g., be observed that, around, 7.5 % of the heart attack patients will 
</p>
<p>cost the hospital over 20.000 Euros, around 50 % of them will cost over 10.000 
</p>
<p>Euros. 
</p>
<p>Instead of Traditional Means and Standard Deviations, Monte Carlo Simulations&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>198
</p>
<p>    
</p>
<p>    Monte Carlo can also be used to answer questions like " What will happen to 
</p>
<p>costs, if the numbers of complications are reduced by two or the ichours are reduced 
</p>
<p>by 20". For that purpose we will use the original data fi le entitled "chap8simula-
</p>
<p>tion1.sav" again. Also the document entitled "splan" which contains software syn-
</p>
<p>tax for performing a simulation is required.
</p>
<p>  Open "simulation1.sav" and command: 
</p>
<p>  Transform&hellip;.Compute Variable&hellip;.in Numeric Expression enter "complications" 
</p>
<p>from the panel below Numeric Expressions enter "-" and "2"&hellip;.in Target Variable 
</p>
<p>type complications&hellip;.click OK&hellip;.in Change existing variable click OK.    
</p>
<p> In the data fi le all of the values of the variable "complications" have now been 
</p>
<p>reduced by 2. This transformed data fi le is saved in the desired folder and entitled 
</p>
<p>e.g. "simulation2.sav". We will now perform a Monte Carlo simulation of this trans-
</p>
<p>formed data fi le using the simulation plan "splan".
</p>
<p>   In "simulation2.sav" command: Analyze&hellip;.Simulation&hellip;.click Open an Existing 
</p>
<p>Simulation Plan&hellip;.click Continue&hellip;.in Look in: fi nd the appropriate folder in your 
</p>
<p>computer&hellip;.click "splan.splan"&hellip;.click Open&hellip;.click Simulation&hellip;.click Fit All&hellip;.
</p>
<p>click Run.    
</p>
<p>32 Simulation Models for Varying Predictors (9,000 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>199
</p>
<p>    
</p>
<p>    The above graph shows that fewer complications reduces the costs, e.g., 5 % of 
</p>
<p>the patients cost over 13.875 Euros, while the same class costed over 21.633 Euros 
</p>
<p>before. 
</p>
<p> What about the effect of the hours in the ic unit. For that purpose, in "simula-
</p>
<p>tion1.sav" perform the same commands as shown directly above, and transform the 
</p>
<p>ichours variable by &minus;20 hours. The transformed document can be named "simula-
</p>
<p>tion3.sav" and saved. The subsequent simulation procedure in this data fi le using 
</p>
<p>again "splan.splan" produces the underneath output. 
</p>
<p>Instead of Traditional Means and Standard Deviations, Monte Carlo Simulations&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>200
</p>
<p>    
</p>
<p>    It is observed that the costs are now not reduced, but rather somewhat increased 
</p>
<p>with 5 % of the patients costing over 23.761 Euros instead of 21.633. This would 
</p>
<p>make sense, nonetheless, because it is sometimes assumed by hospital managers 
</p>
<p>that the reduction of stay-days in hospital is accompanied with more demanding 
</p>
<p>type of care (Statistics Applied to Clinical Studies 5th edition, Chap. 44, Clinical 
</p>
<p>data where variability is more important than averages, pp 487&ndash;498, Springer 
</p>
<p>Heidelberg Germany, 2012).  
</p>
<p>    Conclusion 
</p>
<p> Monte Carlo simulations of inputs where variability is more important than means 
</p>
<p>can model outcome distributions with increased precision. This is, particularly, so 
</p>
<p>with non-normal data. Also questions, like &ldquo;how will hospital costs develop, if the 
</p>
<p>numbers of complications are reduced by 2 or numbers of hours in the intensive care 
</p>
<p>unit reduced by 20&rdquo;, can be answered.  
</p>
<p>32 Simulation Models for Varying Predictors (9,000 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>201
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of Monte Carlo 
</p>
<p>simulation is provided in Statistics applied to clinical studies 5th edition, Chap. 44, 
</p>
<p>Clinical data where variability is more important than averages, pp 487&ndash;498, 
</p>
<p>Springer Heidelberg Germany, 2012, from the same authors as the current 
</p>
<p>publication.    
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>203&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_33
</p>
<p>    Chapter 33   
</p>
<p> Generalized Linear Mixed Models 
</p>
<p>for Outcome Prediction from Mixed Data 
</p>
<p>(20 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> To assess whether generalized linear mixed models can be used to train clinical 
</p>
<p>samples with both fi xed and random effects about individual future patients  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> In a parallel-group study of two treatments, each patient was measured weekly for 
</p>
<p>5 weeks. As repeated measures in one patient are more similar than unrepeated 
</p>
<p>ones, a random interaction effect between week and patient was assumed.  
</p>
<p>    Example 
</p>
<p> In a parallel-group study of two cholesterol reducing compounds, patients were 
</p>
<p>measured weekly for 5 weeks. As repeated measures in one patient are more similar 
</p>
<p>than unrepeated ones, we assumed that a random interaction variable between week 
</p>
<p>and patient would appropriately adjust this effect. 
</p>
<p> Patient_id  week  hdl-cholesterol (mmol/l)  treatment (0 or 1) 
</p>
<p> 1  1  1,66  0 
</p>
<p> 1  2  1,62  0 
</p>
<p> 1  3  1,57  0 
</p>
<p> 1  4  1,52  0 
</p>
<p> 1  5  1,50  0 
</p>
<p> 2  1  1,69  0 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 2&rdquo; as Chap. 9, 
</p>
<p>2014. 
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>204
</p>
<p> Patient_id  week  hdl-cholesterol (mmol/l)  treatment (0 or 1) 
</p>
<p> 2  2  1,71  0 
</p>
<p> 2  3  1,60  0 
</p>
<p> 2  4  1,55  0 
</p>
<p> 2  5  1,56  0 
</p>
<p>   Only the fi rst 2 patients of the data fi le is shown. The entire fi le entitled &ldquo;fi xedan-
</p>
<p>drandomeffects&rdquo; is in extras.springer.com. We will try and develop a mixed model 
</p>
<p>(mixed means a model with both fi xed and random predictors) for testing the data. 
</p>
<p>Also, SPSS will be requested to produce a ZIP (compressed fi le that can be unzipped) 
</p>
<p>fi le from the intervention study, which could then be used for making predictions 
</p>
<p>about cholesterol values in future patients treated similarly. We will start by opening 
</p>
<p>the intervention study&rsquo;s data fi le.
</p>
<p>  Command: 
</p>
<p>  click Transform&hellip;.click Random Number Generators&hellip;.click Set Starting Point&hellip;.click 
</p>
<p>Fixed Value (2000000)&hellip;.click OK&hellip;.click Analyze&hellip;.Mixed Linear&hellip;.Generalized 
</p>
<p>Mixed Linear Models&hellip;.click Data Structure&hellip;.click left mouse and drag patient_id to 
</p>
<p>Subjects part of the canvas&hellip;.click left mouse and drag week to Repeated Measures part 
</p>
<p>of the canvas&hellip;.click Fields and Effects&hellip;.click Target&hellip;.check that the variable out-
</p>
<p>come is already in the Target window&hellip;.check that Linear model is marked&hellip;.click 
</p>
<p>Fixed Effects&hellip;.drag treatment and week to Effect builder&hellip;.click Random Effects&hellip;.
</p>
<p>click Add Block&hellip;.click Add a custom term&hellip;.move week*treatment (* is symbol mul-
</p>
<p>tiplication and interaction) to the Custom term window&hellip;.click Add term&hellip;.click OK&hellip;.
</p>
<p>click Model Options&hellip;.click Save Fields&hellip;.mark Predicted Values&hellip;.click Export 
</p>
<p>model&hellip;. type exportfi xedandrandom&hellip;.click Browse&hellip;.in the appropriate folder enter 
</p>
<p>in File name: mixed&hellip;.click Run. 
</p>
<p>  
</p>
<p>Intercept
</p>
<p>treatment=0
</p>
<p>week=1
</p>
<p>week=2
</p>
<p>week=3
</p>
<p>week=4
</p>
<p>outcome
</p>
<p>Positive
</p>
<p>Coefficient
</p>
<p>Estimate
</p>
<p>Target:outcome
</p>
<p>Fixed Coefficients
</p>
<p>Negative
</p>
<p>  
</p>
<p>33 Generalized Linear Mixed Models for Outcome Prediction&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>205
</p>
<p> Source  F  df1  df2  Sig. 
</p>
<p> Corrected model  5,027  5  94  ,000 
</p>
<p> Treatment  23,722  1  94  ,000 
</p>
<p> Week  0,353  4  94  ,041 
</p>
<p>  Probability distribution:Normal 
</p>
<p> Link function:Identity 
</p>
<p>       In the output sheet a graph is observed with the mean and standard errors of the 
</p>
<p>outcome value displayed with the best fi t Gaussian curve. The F-value of 23.722 indi-
</p>
<p>cates that one treatment is very signifi cantly better than the other with p &lt;0.0001. The 
</p>
<p>thickness of the lines are a measure for level of signifi cance, and so the signifi cance of 
</p>
<p>the 5 week is very thin and thus very weak. Week 5 is not shown. It is redundant, 
</p>
<p>because it means absence of the other 4 weeks. If you click at the left bottom of the 
</p>
<p>graph panel, a table comes up providing similar information in written form. The 
</p>
<p>effect of the interaction variable is not shown, but implied in the analysis. 
</p>
<p> If we return to the data fi le page, we will observe that the software has produced 
</p>
<p>a predicted value for each actually measured cholesterol value. The predicted and 
</p>
<p>actual values are very much the same. 
</p>
<p> We will now use the ZIP fi le to make predictions about cholesterol values in 
</p>
<p>future patients treated similarly. 
</p>
<p> week  treatment  patient_id 
</p>
<p> 1  0  21 
</p>
<p> 2  0  21 
</p>
<p> 3  0  21 
</p>
<p> 4  0  21 
</p>
<p> 5  0  21 
</p>
<p> 1  1  22 
</p>
<p> 2  1  22 
</p>
<p> 3  1  22 
</p>
<p> 4  1  22 
</p>
<p> 5  1  22 
</p>
<p>   Command: 
</p>
<p>  click Utilities&hellip;.click Scoring Wizard&hellip;.click Browse&hellip;.click Select&hellip;.Folder: 
</p>
<p>enter the mixed ZIP fi le entitled "exportfi xedandrandom"&hellip;.click Select&hellip;.in 
</p>
<p>Scoring Wizard click Next&hellip;.click Finish.    
</p>
<p> In the data fi le now the predicted cholesterol values are given. 
</p>
<p> week  treatment  patient_id  predicted cholesterol 
</p>
<p> 1  0  21  1,88 
</p>
<p> 2  0  21  1,96 
</p>
<p> 3  0  21  1,94 
</p>
<p>(continued)
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>206
</p>
<p> week  treatment  patient_id  predicted cholesterol 
</p>
<p> 4  0  21  1,91 
</p>
<p> 5  0  21  1,89 
</p>
<p> 1  1  22  2,12 
</p>
<p> 2  1  22  2,20 
</p>
<p> 3  1  22  2,18 
</p>
<p> 4  1  22  2,15 
</p>
<p> 5  1  22  2,13 
</p>
<p>       Conclusion 
</p>
<p> The module Generalized mixed linear models provides the possibility to handle 
</p>
<p>both fi xed and random effects, and is, therefore appropriate to adjust data with 
</p>
<p>repeated measures and presumably a strong correlation between the repeated mea-
</p>
<p>sures. Also individual future patients treated similarly can be assessed for predicted 
</p>
<p>cholesterol values using a ZIP fi le.  
</p>
<p>    Note 
</p>
<p> More background theoretical and mathematical information of models with both 
</p>
<p>fi xed and random variables is given in: 
</p>
<p> 1. Machine learning in medicine part one, Chap. 6, Mixed linear models, pp 65&ndash;76, 
</p>
<p>2013, 
</p>
<p> 2. Statistics applied to clinical studies 5th edition, Chap. 56, Advanced analysis of 
</p>
<p>variance, random effects and mixed effects models, pp 607&ndash;618, 2012, 
</p>
<p> 3. SPSS for starters part one, Chap. 7, Mixed models, pp 25&ndash;29, 2010, and, 
</p>
<p> 4. Machine learning in medicine part three, Chap. 9, Random effects, pp 81&ndash;94, 
</p>
<p>2013. 
</p>
<p> All of these references are from the same authors and have been edited by 
</p>
<p>Springer Heidelberg Germany.    
</p>
<p>33 Generalized Linear Mixed Models for Outcome Prediction&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>207&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_34
</p>
<p>Chapter 34
</p>
<p>Two-Stage Least Squares (35 Patients)
</p>
<p> General Purpose
</p>
<p>The two stage least squares method assumes that the independent variable 
</p>
<p>(x- variable) is problematic, meaning that it is somewhat uncertain. An additional 
</p>
<p>variable can be argued to provide relevant information about the problematic vari-
</p>
<p>able, and is, therefore, called instrumental variable, and included in the analysis.
</p>
<p> Primary Scientific Question
</p>
<p>Non-compliance is a predictor of drug efficacy. Counseling causes improvement of 
</p>
<p>patients&rsquo; compliance and, therefore, indirectly improves the outcome drug 
</p>
<p>efficacy.
</p>
<p> 
</p>
<p>y outcome variable drugefficacy
</p>
<p>x problematic variable non co
</p>
<p>= ( )
= &minus; mmpliance
</p>
<p>z instrumental variable counseling
</p>
<p>( )
= ( )
</p>
<p> 
</p>
<p>With two stage least squares the underneath stages are assessed.
</p>
<p> 
</p>
<p>1st stage
</p>
<p>x intercept regression coefficient timesz= +
 
</p>
<p>This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 2&rdquo; as Chap. 
</p>
<p>10, 2014.</p>
<p/>
</div>
<div class="page"><p/>
<p>208
</p>
<p>With the help of the calculated intercept and regression coefficient from the above 
</p>
<p>simple linear regression analysis improved x-values are calculated, e.g., for patient 
</p>
<p>1:
</p>
<p> 
</p>
<p>1
</p>
<p>8 27
</p>
<p>st
</p>
<p>improved
</p>
<p>stage
</p>
<p>x intercept regression coefficient times= + = ..68
</p>
<p>2nd
</p>
<p>improve
</p>
<p>stage
</p>
<p>y intercept regression coefficient times x= +
dd  
</p>
<p> Example
</p>
<p>Patients&rsquo; non-compliance is a factor notoriously affecting the estimation of drug 
</p>
<p>efficacy. An example is given of a simple evaluation study that assesses the effect of 
</p>
<p>non-compliance (pills not used) on the outcome, the efficacy of a novel laxative 
</p>
<p>with numbers of stools per month as efficacy estimator (the y-variable). The data of 
</p>
<p>the first 10 of the 35 patients are in the table below. The entire data file is in extras.
</p>
<p>springer.com, and is entitled &ldquo;twostageleastsquares&rdquo;.
</p>
<p>Patient no Instrumental variable (z) Problematic predictor (x) Outcome (y)
</p>
<p>Frequency counseling
</p>
<p>Pills not used  
</p>
<p>(non-compliance)
</p>
<p>Efficacy estimator of new  
</p>
<p>laxative (stools/month)
</p>
<p>1. 8 25 24
</p>
<p>2. 13 30 30
</p>
<p>3. 15 25 25
</p>
<p>4. 14 31 35
</p>
<p>5. 9 36 39
</p>
<p>6. 10 33 30
</p>
<p>7. 8 22 27
</p>
<p>8. 5 18 14
</p>
<p>9. 13 14 39
</p>
<p>10. 15 30 42
</p>
<p>SPSS version 19 and up can be used for analysis. It uses the term explanatory 
</p>
<p>variable for the problematic variable. Start by opening the data file.
</p>
<p>Command:
</p>
<p>Analyze&hellip;.Regression&hellip;.2 Stage Least Squares&hellip;.Dependent: therapeutic effi-
</p>
<p>cacy&hellip;.Explanatory: non-compliance&hellip;. Instrumental: counseling &hellip;.OK.
</p>
<p>34 Two-Stage Least Squares (35 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>209
</p>
<p>Model description
</p>
<p>Type of variable
</p>
<p>Equation 1 y Dependent
</p>
<p>x Ppredictor
</p>
<p>z Instrumental
</p>
<p>ANOVA
</p>
<p>Sum of squares df Mean square F Sig.
</p>
<p>Equation 1 Regression 1408,040 1 1408,040 4,429 ,043
</p>
<p>Residual 10490,322 33 317,889
</p>
<p>Total 11898,362 34
</p>
<p>Coefficients
</p>
<p>Unstandardized coefficients
</p>
<p>Beta t Sig.B Std. Error
</p>
<p>Equation 1 (Constant) &minus;49,778 37,634 &minus;1,323 ,195
</p>
<p>x 2,675 1,271 1,753 2,105 ,043
</p>
<p>The result is shown above. The non-compliance adjusted for counseling is a 
</p>
<p> statistically significant predictor of laxative efficacy with p = 0.043. This p-value has 
</p>
<p>been automatically been adjusted for multiple testing. When we test the model with-
</p>
<p>out the help of the instrumental variable counseling the p-value is larger and the 
</p>
<p>effect is no more statistically significant as shown underneath.
</p>
<p>Command:
</p>
<p>Analyze&hellip;.Regression&hellip;.Linear&hellip;.Dependent: therapeutic efficacy &hellip;.Independent: 
</p>
<p>non-compliance&hellip;.OK.
</p>
<p>ANOVAa
</p>
<p>Model Sum of squares df Mean square F Sig.
</p>
<p>1 Regression 334,482 1 334,482 3,479 ,071b
</p>
<p>Residual 3172,489 33 96,136
</p>
<p>Total 3506,971 34
aDependent variable: drug efficacy
bPredictors: (Constant), non-compliance
</p>
<p>Coefficientsa
</p>
<p>Unstandardized 
</p>
<p>coefficients
</p>
<p>Standardized 
</p>
<p>coefficients
</p>
<p>Model B Std. Error Beta t Sig.
</p>
<p>1 (Constant) 15,266 7,637 1,999 ,054
</p>
<p>Non-compliance ,471 ,253 ,309 1,865 ,071
aDependent variable: drug efficacy
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>210
</p>
<p> Conclusion
</p>
<p>Two stage least squares with counseling as instrumental variable was more sensitive 
</p>
<p>than simple linear regression with laxative efficacy as outcome and non-compliance 
</p>
<p>as predictor. We should add that two stage least squares is at risk of overestimating 
</p>
<p>the precision of the outcome, if the analysis is not adequately adjusted for multiple 
</p>
<p>testing. However, in SPSS automatic adjustment for the purpose has been per-
</p>
<p>formed. The example is the simplest version of the procedure. And, multiple explan-
</p>
<p>atory and instrumental variables can be included in the models.
</p>
<p> Note
</p>
<p>More background theoretical and mathematical information of two stage least 
</p>
<p>squares analyses is given in Machine learning in medicine part two, Two-stage least 
</p>
<p>squares, pp 9&ndash;15, Springer Heidelberg Germany, 2013, from the same authors.
</p>
<p>34 Two-Stage Least Squares (35 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>211&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_35
</p>
<p>    Chapter 35   
</p>
<p> Autoregressive Models for Longitudinal Data 
(120 Mean Monthly Population Records) 
</p>
<p>                      General Purpose 
</p>
<p> Time series are encountered in every fi eld of medicine. Traditional tests are unable 
</p>
<p>to assess trends, seasonality, change points and the effects of multiple predictors 
</p>
<p>like treatment modalities simultaneously. To assess whether autoregressive inte-
</p>
<p>grated moving average (ARIMA) methods are able to do all of that.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Monthly HbA1c levels in patients with diabetes type II are a good estimator for 
</p>
<p>adequate diabetes control, and have been demonstrated to be seasonal with higher 
</p>
<p>levels in the winter. A large patient population was followed for 10 year. The mean 
</p>
<p>values are in the data. This chapter is to assess whether longitudinal summary sta-
</p>
<p>tistics of a population can be used for the effects of seasons and treatment changes 
</p>
<p>on populations with chronic diseases. 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 2&rdquo; as Chap. 
</p>
<p>11, 2014. </p>
<p/>
</div>
<div class="page"><p/>
<p>212
</p>
<p> Note: 
</p>
<p> No conclusion can here be drawn about individual patients. Autoregressive 
</p>
<p> models can also be applied with data sets of individual patients, and with multiple 
</p>
<p>outcome variables like various health outcomes.  
</p>
<p>    Example 
</p>
<p> The underneath data are from the fi rst year&rsquo;s observation data of the above diabetic 
</p>
<p>patient data. The entire data fi le is in extras.springer.com, and is entitled 
</p>
<p>&ldquo;arimafi le&rdquo;. 
</p>
<p> Date  HbA1  nurse  doctor  phone  self  meeting 
</p>
<p> 01/01/1989  11,00  8,00  7,00  3  22  2 
</p>
<p> 02/01/1989  10,00  8,00  9,00  3  27  2 
</p>
<p> 03/01/1989  17,00  8,00  7,00  2  30  3 
</p>
<p> 04/01/1989  7,00  8,00  9,00  2  29  2 
</p>
<p> 05/01/1989  7,00  9,00  7,00  2  23  2 
</p>
<p> 06/01/1989  10,00  8,00  9,00  3  27  2 
</p>
<p> 07/01/1989  9,00  8,00  8,00  3  27  2 
</p>
<p> 08/01/1989  10,00  8,00  7,00  3  30  2 
</p>
<p> 09/01/1989  12,00  8,00  8,00  4  27  2 
</p>
<p> 10/01/1989  13,00  9,00  11,00  3  32  2 
</p>
<p> 11/01/1989  14,00  9,00  7,00  3  29  2 
</p>
<p> 12/01/1989  23,00  10,00  11,00  5  39  3 
</p>
<p> 01/01/1990  12,00  8,00  7,00  4  23  2 
</p>
<p> 02/01/1990  8,00  8,00  6,00  2  25  3 
</p>
<p>  Date = date of observation, 
</p>
<p> HbA1 = mean HbA1c of diabetes population, 
</p>
<p> nurse = mean number of diabetes nurse visits, 
</p>
<p> doctor = mean number of doctor visits, 
</p>
<p> phone = mean number of phone visits, 
</p>
<p> self = mean number of self-controls, 
</p>
<p> meeting = mean number of patient educational meetings 
</p>
<p>    We will fi rst assess the observed values along the time line. The analysis is per-
</p>
<p>formed using SPSS statistical software.
</p>
<p>  Command: 
</p>
<p>  analyze&hellip;.Forecast&hellip;.Sequence Charts&hellip;.Variables: enter HbA1c&hellip;.Time Axis 
</p>
<p>Labels: enter Date&hellip;.OK.    
</p>
<p>35 Autoregressive Models for Longitudinal Data (120 Mean Monthly Population&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>213
</p>
<p>    
</p>
<p>    The above output sheets show the observed data. There are (1) numerous peaks, 
</p>
<p>which are (2) approximately equally sized, and (3) there is an upward trend: (2) 
</p>
<p>suggests periodicity which was expected from the seasonal pattern of HbA1c val-
</p>
<p>ues, (3) is also expected, it suggests increasing HbA1c after several years due to 
</p>
<p>beta-cell failure. Finally (4), there are several peaks that are not part of the seasonal 
</p>
<p>pattern, and could be due to outliers. 
</p>
<p> ARIMA (autoregressive integrated moving average methodology) is used for 
</p>
<p>modeling this complex data pattern. It uses the Export Modeler for outlier detection, 
</p>
<p>and produces for the purpose XML (eXtended Markup Language) fi les for predic-
</p>
<p>tion modeling of future data.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Forecast&hellip;.Time Series Modeler&hellip;.Dependent Variables: enter 
</p>
<p>HbA1c&hellip;.Independent Variables: enter nurse, doctor, phone, self control, and 
</p>
<p>patient meeting&hellip;.click Methods: Expert Modeler&hellip;.click Criteria&hellip;.Click Outlier 
</p>
<p>Table&hellip;.Select automatically &hellip;.Click Statistics Table&hellip;.Select Parameter 
</p>
<p>Estimates&hellip;.mark Display forecasts&hellip;.click Plots table&hellip;.click Series, Observed 
</p>
<p>values, Fit values&hellip;.click Save&hellip;.Predicted Values: mark Save&hellip;.Export XML File: 
</p>
<p>click Browse&hellip;.various folders in your PC come up&hellip;.in "File Name"of the appro-
</p>
<p>priate folder enter "exportarima"&hellip;.click Save&hellip;.click Continue&hellip;.click OK.    
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>214
</p>
<p>    
</p>
<p>    The above graph shows that a good fi t of the observed data is given by the 
</p>
<p>ARIMA model, and that an adequate predictive model is provided. The upward 
</p>
<p>trend is in agreement with beta-cell failure after several years. 
</p>
<p> The underneath table shows that 3 signifi cant predictors have been identifi ed. Also the 
</p>
<p>goodness of fi t of the ARIMA (p, d, q) model is given, where p = number of lags, d = the 
</p>
<p>trend (one upward trend means d = 1), and q = number of moving averages (=0 here). Both 
</p>
<p>Stationary R square, and Ljung-Box tests are insignifi cant. A signifi cant test would have 
</p>
<p>meant poor fi t. In our example, there is an adequate fi t, but the model has identifi ed no 
</p>
<p>less than 7 outliers. Phone visits, nurse visits, and doctor visits were signifi cant predictors 
</p>
<p>at p &lt; 0.0001, while self control and educational patient meetings were not so. All of the 
</p>
<p>outliers are signifi cantly more distant from the ARIMA model than could happen by 
</p>
<p>chance. All of the p-values were very signifi cant with p &lt; 0.001 and &lt; 0.0001.
</p>
<p> Model statistics 
</p>
<p> Model 
</p>
<p> Number of 
</p>
<p>predictors 
</p>
<p> Model Fit statistics  Ljung-BoxQ(18)  Number 
</p>
<p>of outliers  Stationary R-squared  Statistics  DF  Sig. 
</p>
<p> men-Model_1  3  ,898  17,761  18  ,471  7 
</p>
<p> ARIMA model parameters 
</p>
<p> Estimate  SE  t  Sig. 
</p>
<p> men- Model_1   men  Natural log  Constant  &minus;2,828  ,456  &minus;6,207  ,000 
</p>
<p> phone  Natural log  Numerator  Lag 0  ,569  ,064  8,909  ,000 
</p>
<p> nurse  Natural log  Numerator  Lag 0  1,244  ,118  10,585  ,000 
</p>
<p> doctor  Natural log  Numerator  Lag 0  ,310  ,077  4,046  ,000 
</p>
<p> Lag 1  &minus;,257  ,116  &minus;2,210  ,029 
</p>
<p> Lag 2  &minus;,196  ,121  &minus;1,616  ,109 
</p>
<p> Denominator  Lag 1  ,190  ,304  ,623  ,535 
</p>
<p>35 Autoregressive Models for Longitudinal Data (120 Mean Monthly Population&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>215
</p>
<p> Outliers 
</p>
<p> Estimate  SE  t  Sig. 
</p>
<p> men-Model_1  3  Additive  ,769  ,137  5,620  ,000 
</p>
<p> 30  Additive  ,578  ,138  4,198  ,000 
</p>
<p> 53  Additive  ,439  ,135  3,266  ,001 
</p>
<p> 69  Additive  ,463  ,135  3,439  ,001 
</p>
<p> 78  Additive  &minus;,799  ,138  &minus;5,782  ,000 
</p>
<p> 88  Additive  ,591  ,134  4,409  ,000 
</p>
<p> 105  Additive  &minus;1,771  ,134  &minus;13,190  ,000 
</p>
<p>   When returning to the data view screen, we will observe that SPSS has added 
</p>
<p>HbA1 values (except for the fi rst two dates due to lack of information) as a novel 
</p>
<p>variable. The predicted values are pretty similar to the measured values, supporting 
</p>
<p>the adequacy of the model. 
</p>
<p> We will now apply the XML fi le and the Apply Models modus for making pre-
</p>
<p>dictions about HbA1 values in the next 6 months, assuming that the signifi cant 
</p>
<p>variables nurse, doctor, phone are kept constant at their overall means. 
</p>
<p> First add the underneath data to the original data fi le and rename the fi le, e.g., 
</p>
<p>&ldquo;arimafi le2&rdquo;, and store it at an appropriate folder in your computer. 
</p>
<p> Date  HbA1  nurse  doctor  phone  self  meeting 
</p>
<p> 01/01/1999  10,00  8,00  4,00 
</p>
<p> 01/02/1999  10,00  8,00  4,00 
</p>
<p> 01/03/1999  10,00  8,00  4,00 
</p>
<p> 01/04/1999  10,00  8,00  4,00 
</p>
<p> 01/05/1999  10,00  8,00  4,00 
</p>
<p> 01/06/1999  10,00  8,00  4,00 
</p>
<p>   Then open &ldquo;arimafi le2.sav&rdquo; and command: 
</p>
<p>     Analyze&hellip;.click Apply Models&hellip;.click Reestimate from data&hellip;.click First case 
</p>
<p>after end of estimation period through a specifi ed date&hellip;.Observation: enter 
</p>
<p>01/06/1999&hellip;.click Statistics: click Display Forecasts&hellip;.click Save: Predicted 
</p>
<p>Values mark Save&hellip;.click OK.    
</p>
<p> The underneath table shows the predicted HbA1 values for the next 6 months and 
</p>
<p>their upper and lower confi dence limits (UCL and LCL).
</p>
<p> Forecast 
</p>
<p> Model  121  122  123  124  125  126 
</p>
<p> HbA1-Model_1  Forecast  17,69  17,30  16,49  16,34  16,31  16,30 
</p>
<p> UCL  22,79  22,28  21,24  21,05  21,01  21,00 
</p>
<p> LCL  13,49  13,19  12,58  12,46  12,44  12,44 
</p>
<p>  For each model, forecasts start after the last non-missing in the range of the requested estimation 
</p>
<p>period, and end at the last period for which non-missing values of all the predictors are available 
</p>
<p>or at the end date of the requested forecast period, whichever is earlier 
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>216
</p>
<p>     
</p>
<p>40,00
</p>
<p>Observed
</p>
<p>Fit
Forecast
</p>
<p>30,00
</p>
<p>20,00
</p>
<p>10,00
</p>
<p>,00
</p>
<p>Date
</p>
<p>H
b
A
1
-M
o
d
e
l_
1
</p>
<p>N
u
m
b
e
r
</p>
<p>1 5 9 1
3
</p>
<p>1
7
</p>
<p>2
1
2
5
</p>
<p>2
9
</p>
<p>3
3
3
7
</p>
<p>4
1
</p>
<p>4
5
</p>
<p>4
9
</p>
<p>5
3
</p>
<p>5
7
6
1
</p>
<p>6
5
6
9
</p>
<p>7
3
</p>
<p>7
7
</p>
<p>8
1
</p>
<p>8
5
</p>
<p>8
9
</p>
<p>9
3
9
7
</p>
<p>1
0
1
</p>
<p>1
0
5
</p>
<p>1
0
9
</p>
<p>1
1
3
</p>
<p>1
1
7
</p>
<p>1
2
1
</p>
<p>1
2
5
</p>
<p>1
2
9
</p>
<p>  
</p>
<p>    Also a graph of the HbA1 pattern after the estimation period is given as shown in 
</p>
<p>the above graph. When returning to the data view of the arimafi le2, we will observe 
</p>
<p>that SPSS has added the predicted values as a novel variable. 
</p>
<p> Date  HbA1  nurse  doctor  phone  self  meeting 
</p>
<p> modeled 
</p>
<p>HbA1 
</p>
<p> predicted 
</p>
<p>HbA1 
</p>
<p> 07/01/1998  19,00  11,00  8,00  5,00  28,00  4,00  21,35  21,35 
</p>
<p> 08/01/1998  30,00  12,00  9,00  4,00  27,00  5,00  21,31  21,31 
</p>
<p> 09/01/1998  24,00  13,00  8,00  5,00  30,00  5,00  26,65  26,65 
</p>
<p> 10/01/1998  24,00  12,00  10,00  4,00  28,00  6,00  22,59  22,59 
</p>
<p> 11/01/1998  24,00  11,00  8,00  5,00  26,00  5,00  22,49  22,49 
</p>
<p> 12/01/1998  39,00  15,00  10,00  5,00  37,00  7,00  34,81  34,81 
</p>
<p> 01/01/1999  10,00  8,00  4,00  17,69 
</p>
<p> 01/02/1999  10,00  8,00  4,00  17,30 
</p>
<p> 01/03/1999  10,00  8,00  4,00  16,49 
</p>
<p> 01/04/1999  10,00  8,00  4,00  16,34 
</p>
<p> 01/05/1999  10,00  8,00  4,00  16,31 
</p>
<p> 01/06/1999  10,00  8,00  4,00  16,30 
</p>
<p>  modeled HbA1 = calculated HbA1 values from the above arima model 
</p>
<p> Predicted HbA1 = the predicted HbA1 values using the XML fi le for future dates. 
</p>
<p>        Conclusion 
</p>
<p> Autoregressive integrated moving average methods are appropriate for assessing 
</p>
<p>trends, seasonality, and change points in a time series. In the example given no 
</p>
<p>conclusion can be drawn about individual patients. Autoregressive models can, 
</p>
<p>35 Autoregressive Models for Longitudinal Data (120 Mean Monthly Population&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>217
</p>
<p>however, also be applied for data sets of individual patients. Also as a multivariate 
</p>
<p>methodology it is appropriate for multiple instead of a single outcome variable like 
</p>
<p>various health outcomes.  
</p>
<p>    Note 
</p>
<p> More background theoretical and mathematical information of autoregressive mod-
</p>
<p>els for longitudinal data is in Machine learning in medicine part two, Multivariate 
</p>
<p>analysis of time series, pp 139&ndash;154, Springer Heidelberg Germany, 2013, from the 
</p>
<p>same authors.    
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>219&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_36
</p>
<p>Chapter 36
</p>
<p>Variance Components for Assessing 
the Magnitude of Random Effects  
(40 Patients)
</p>
<p> General Purpose
</p>
<p>If we have reasons to believe that in a study certain patients due to co-mobidity, co- 
</p>
<p>medication and other factors will respond differently from others, then the spread in 
</p>
<p>the data is caused not only by residual effect, but also by some subgroup property, 
</p>
<p>otherwise called some random effect. Variance components analysis is able to 
</p>
<p>assess the magnitudes of random effects as compared to that of the residual error of 
</p>
<p>a study.
</p>
<p> Primary Scientific Question
</p>
<p>Can a variance components analysis by including the random effect in the analysis 
</p>
<p>reduce the unexplained variance in a study, and, thus, increase the accuracy of the 
</p>
<p>analysis model as used.
</p>
<p>This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 3&rdquo; as Chap. 3, 
</p>
<p>2014.</p>
<p/>
</div>
<div class="page"><p/>
<p>220
</p>
<p> Example
</p>
<p>Variables
</p>
<p>PAT treat gender cad
</p>
<p>52,00 ,00 ,00 2,00
</p>
<p>48,00 ,00 ,00 2,00
</p>
<p>43,00 ,00 ,00 1,00
</p>
<p>50,00 ,00 ,00 2,00
</p>
<p>43,00 ,00 ,00 2,00
</p>
<p>44,00 ,00 ,00 1,00
</p>
<p>46,00 ,00 ,00 2,00
</p>
<p>46,00 ,00 ,00 2,00
</p>
<p>43,00 ,00 ,00 1,00
</p>
<p>49,00 ,00 ,00 2,00
</p>
<p>28,00 1,00 ,00 1,00
</p>
<p>35,00 1,00 ,00 2,00
</p>
<p>PAT = episodes of paroxysmal atrial tachycardias
</p>
<p>treat = treatment modality (0 = placebo treatment, 1 = active treatment)
</p>
<p>gender = gender (0 = female)
</p>
<p>cad = presence of coronary artery disease (1 no, 2 = yes)
</p>
<p>The first 12 of a 40 patient parallel-group study of the treatment of paroxysmal 
</p>
<p>tachycardia with numbers of episodes of PAT as outcome is given above. The entire 
</p>
<p>data file is in &ldquo;variancecomponents&rdquo;, and is available at extras.springer.com. We had 
</p>
<p>reason to believe that the presence of coronary artery disease would affect the out-
</p>
<p>come, and, therefore, used this variable as a random rather than fixed variable. SPSS 
</p>
<p>statistical software was used for data analysis. Start by opening the data file in SPSS.
</p>
<p>Command:
</p>
<p>Analyze....General Linear Model....Variance Components....Dependent Variable: 
</p>
<p>enter "paroxtachyc"....Fixed Factor(s): enter "treat, gender"....Random Factor(s): 
</p>
<p>enter "corartdisease"....Model: mark Custom....Model: enter "treat, gender, cad"&hellip;.
</p>
<p>click Continue....click Options....mark ANOVA....mark Type III....mark Sums of 
</p>
<p>squares....mark Expected mean squares....click Continue....click OK.
</p>
<p>The output sheets are given underneath. The Variance Estimate table gives the 
</p>
<p>magnitude of the Variance due to cad, and that due to residual error (unexplained 
</p>
<p>variance, otherwise called Error). The ratio of the Var (cad)/[Var (Error) + Var (cad)] 
</p>
<p>gives the proportion of variance in the data due to the random cad effect (5.844/(28
</p>
<p>.426 + 5.844) = 0.206 = 20.6 %). This means that 79.4 % instead of 100 % of the 
</p>
<p>error is now unexplained.
</p>
<p>36 Variance Components for Assessing the Magnitude of Random Effects (40 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>221
</p>
<p>Variance estimates
</p>
<p>Component Estimate
</p>
<p>Var(cad) 5,844
</p>
<p>Var(Error) 28,426
</p>
<p>Dependent variable: paroxtach
</p>
<p>Method: ANOVA (Type III sum of squares)
</p>
<p>The underneath ANOVA table gives the sums of squares and mean squares of 
</p>
<p>different effects. E.g. the mean square of cad = 139.469, and that of residual 
</p>
<p>effect = 28.426.
</p>
<p>ANOVA
</p>
<p>Source Type III sum of squares df Mean square
</p>
<p>Corrected model 727,069 3 242,356
</p>
<p>Intercept 57153,600 1 57153,600
</p>
<p>treat 515,403 1 515,403
</p>
<p>gender ,524 1 ,524
</p>
<p>cad 139,469 1 139,469
</p>
<p>Error 1023,331 36 28,426
</p>
<p>Total 58904,000 40
</p>
<p>Corrected total 1750,400 39
</p>
<p>Dependent variable: paroxtach
</p>
<p>The underneath Expected Mean Squares table gives the results of a special pro-
</p>
<p>cedure, whereby variances of best fit quadratic functions of the variables are mini-
</p>
<p>mized to obtain the best unbiased estimate of the variance components. A little 
</p>
<p>mental arithmetic is now required.
</p>
<p>Expected mean squares
</p>
<p>Variance component
</p>
<p>Source Var(cad) Var(Error) Quadratic term
</p>
<p>Intercept 20,000 1,000 Intercept, treat, gender
</p>
<p>treat ,000 1,000 treat
</p>
<p>gender ,000 1,000 gender
</p>
<p>cad 19,000 1,000
</p>
<p>Error ,000 1,000
</p>
<p>Dependent variable: paroxtach
</p>
<p>Expected mean squares are based on Type III sums of squares
</p>
<p>For each source, the expected mean square equals the sum of the coefficients in the cells times the 
</p>
<p>variance components, plus a quadratic term involving effects in the Quadratic Term cell
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>222
</p>
<p> 
</p>
<p>EMS expected mean square of cad the random effect
</p>
<p>Variance c
</p>
<p>( ) ( )
= &times;19 aad Variance Error
</p>
<p>EMSof Error the residual effect
</p>
<p>( )+ ( )
=
</p>
<p>( )
</p>
<p>139 469.
</p>
<p>== + ( )
=
</p>
<p>( )
= &times;
</p>
<p>0
</p>
<p>28 426
</p>
<p>19
</p>
<p>Variance Error
</p>
<p>EMSof cad Variance Error
</p>
<p>Vari
</p>
<p>.
</p>
<p> 
aance cad
</p>
<p>Variance cad
</p>
<p>( )
= &minus;
</p>
<p>=
</p>
<p>( )
=
</p>
<p>=
</p>
<p>139 469 28 426
</p>
<p>110 043
</p>
<p>110 043 19
</p>
<p>. .
</p>
<p>.
</p>
<p>. /
</p>
<p>55 844. compare with the resultsof theabove Variance Estimates tablee( )
 
</p>
<p>It can, thus, be concluded that around 20 % of the uncertainty is in the data is 
</p>
<p>caused by the random effect.
</p>
<p> Conclusion
</p>
<p>If we have reasons to believe that in a study certain patients due to co-mobidity, co- 
</p>
<p>medication and other factors will respond differently from others, then the spread in 
</p>
<p>the data will be caused, not only by the residual effect, but also by the subgroup 
</p>
<p>property, otherwise called the random effect. Variance components analysis, by 
</p>
<p>including the random effect in the analysis, reduces the unexplained variance in a 
</p>
<p>study, and, thus, increases the accuracy of the analysis model used.
</p>
<p> Note
</p>
<p>More background, theoretical and mathematical information of random effects 
</p>
<p>models are given in Machine learning in medicine part three, Chap. 9, Random 
</p>
<p>effects, pp 81&ndash;94, 2013, Springer Heidelberg Germany, from the same authors.
</p>
<p>36 Variance Components for Assessing the Magnitude of Random Effects (40 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>223&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_37
</p>
<p>    Chapter 37   
</p>
<p> Ordinal Scaling for Clinical Scores 
with Inconsistent Intervals (900 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> Clinical studies often have categories as outcome, like various levels of health or 
</p>
<p>disease. Multinomial regression is suitable for analysis (see Chap.   28    ). However, if 
</p>
<p>one or two outcome categories in a study are severely underpresented, multinomial 
</p>
<p>regression is fl awed, and ordinal regression including specifi c link functions may 
</p>
<p>provide a better fi t for the data.  
</p>
<p>    Primary Scientifi c Questions 
</p>
<p> This chapter is to assess how ordinal regression performs in studies where clinical 
</p>
<p>scores have inconsistent intervals.  
</p>
<p>    Example 
</p>
<p> In 900 patients the independent predictors for different degrees of feeling healthy 
</p>
<p>were assessed. The predictors included were: 
</p>
<p> Variable  2  fruit consumption (times per week) 
</p>
<p> 3  unhealthy snacks (times per week) 
</p>
<p> 4  fastfood consumption (times per week) 
</p>
<p> 5  physical activities (times per week) 
</p>
<p> 6  age (number of years). 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 3&rdquo;as Chap. 4, 
</p>
<p>2014. </p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_28">http://dx.doi.org/10.1007/978-3-319-15195-3_28</a></div>
</div>
<div class="page"><p/>
<p>224
</p>
<p>   Feeling healthy (Variable 1) was assessed as mutually elusive categories:
</p>
<p>   1 very much so  
</p>
<p>  2 much so  
</p>
<p>  3 not entirely so  
</p>
<p>  4 not so  
</p>
<p>  5 not so at all.    
</p>
<p> Underneath are the fi rst 10 patients of the data fi le. The entire data fi le is in 
</p>
<p>extras.springer.com, and is entitled &ldquo;ordinalscaling&rdquo;. 
</p>
<p> Variables 
</p>
<p> 1  2  3  4  5  6 
</p>
<p> 4  6  9  12  6  34 
</p>
<p> 4  7  24  3  6  35 
</p>
<p> 4  3  5  9  6  30 
</p>
<p> 4  5  14  6  3  36 
</p>
<p> 4  9  9  12  12  62 
</p>
<p> 2  2  3  3  6  31 
</p>
<p> 3  3  26  6  3  57 
</p>
<p> 5  9  38  6  6  36 
</p>
<p> 4  5  8  9  6  28 
</p>
<p> 5  9  25  12  12  28 
</p>
<p>   First, we will perform a multinomial regression analysis using SPSS statistical 
</p>
<p>software. Open the data fi le in SPSS.
</p>
<p>  Command: 
</p>
<p>  Analyze....Regression....Multinomial Logistic Regression....Dependent: enter feeling 
</p>
<p>healthy....Covariates: enter fruitt/week, snacks.week, fastfood/week, physicalactivi-
</p>
<p>ties/week, age in years....click OK.   
</p>
<p> Parameter estimates 
</p>
<p> 95 % Confi dence 
</p>
<p>interval for Exp (B) 
</p>
<p> feeling 
</p>
<p>healthy a   B 
</p>
<p> Std. 
</p>
<p>Error  Wald  df  Sig.  Exp(B) 
</p>
<p> Lower 
</p>
<p>bound 
</p>
<p> Upper 
</p>
<p>bound 
</p>
<p> very 
</p>
<p>much so 
</p>
<p> Intercept  &minus;1,252  ,906  1,912  1  ,167 
</p>
<p> fruit  ,149  ,069  4,592  1  ,032  1,161  1,013  1,330 
</p>
<p> snacks  ,020  ,017  1,415  1  ,234  1,020  ,987  1,055 
</p>
<p> fastfood  &minus;,079  ,057  1,904  1  ,168  ,924  ,827  1,034 
</p>
<p> physical  &minus;,013  ,056  ,059  1  ,809  ,987  ,885  1,100 
</p>
<p> age  &minus;,027  ,017  2,489  1  ,115  ,974  ,942  1,007 
</p>
<p>(continued)
</p>
<p>37 Ordinal Scaling for Clinical Scores with Inconsistent Intervals (900 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>225
</p>
<p> Parameter estimates 
</p>
<p> 95 % Confi dence 
</p>
<p>interval for Exp (B) 
</p>
<p> feeling 
</p>
<p>healthy a   B 
</p>
<p> Std. 
</p>
<p>Error  Wald  df  Sig.  Exp(B) 
</p>
<p> Lower 
</p>
<p>bound 
</p>
<p> Upper 
</p>
<p>bound 
</p>
<p> much so  Intercept  &minus;2,087  ,863  5,853  1  ,016 
</p>
<p> fruit  ,108  ,071  2,302  1  ,129  1,114  ,969  1,280 
</p>
<p> snacks  &minus;.001  ,019  ,004  1  ,950  ,999  ,962  1,037 
</p>
<p> fastfood  ,026  ,057  ,212  1  ,645  1,026  ,919  1,147 
</p>
<p> physical  &minus;,005  ,051  ,009  1  ,925  ,995  ,900  1,101 
</p>
<p> age  &minus;.010  ,014  ,522  1  ,470  ,990  ,962  1,018 
</p>
<p> not 
</p>
<p>entirely so 
</p>
<p> Intercept  2,161  ,418  26,735  1  ,000 
</p>
<p> fruit  ,045  ,039  1,345  1  ,246  1,046  ,969  1,130 
</p>
<p> snacks  &minus;,012  ,011  1,310  1  ,252  ,988  ,968  1,009 
</p>
<p> fastfood  &minus;,037  ,027  1,863  1  ,172  ,964  ,914  1,016 
</p>
<p> physical  &minus;,040  ,025  2,518  1  ,113  ,961  ,914  1,010 
</p>
<p> age  &minus;.028  ,007  14,738  1  ,000  ,972  ,959  ,986 
</p>
<p> no so  Intercept  ,781  ,529  2,181  1  ,140 
</p>
<p> fruit  ,100  ,046  4,600  1  ,032  1,105  1,009  1,210 
</p>
<p> snacks  &minus;,001  ,012  ,006  1  ,939  ,999  ,975  1,024 
</p>
<p> fastfood  &minus;,038  ,034  1,225  1  ,268  ,963  ,901  1,029 
</p>
<p> physical  &minus;,037  ,032  1,359  1  ,244  ,963  ,905  1,026 
</p>
<p> age  &minus;,028  ,010  8,651  1  ,003  ,972  ,954  ,991 
</p>
<p>   a The reference category is: not so at all 
</p>
<p>    The above table gives the analysis results. Twenty-four p-values are produced, 
</p>
<p>and a few of them are statistically signifi cant at p &lt; 0.05. For example, per fruit unit 
</p>
<p>you may have 1.161 times more chance of feeling very healthy versus not healthy at 
</p>
<p>all at p = 0.032. And per year of age you may have 0.972 times less chance of feeling 
</p>
<p>not entirely healthy versus not healthy at all at p = 0.0001. We should add that the 
</p>
<p>few signifi cant p-values among the many insignifi cant ones could easily be due to 
</p>
<p>type I errors (due to multiple testing). Also a fl awed analysis due to inconsistent 
</p>
<p>intervals has not yet been excluded. To assess this point a graph will be drawn.
</p>
<p>  Command: 
</p>
<p>  Graphs....Legacy Dialogs....Bar....click Simple....mark Summary for groups of 
</p>
<p>cases....click Defi ne....Category Axis: enter "feeling healthy"....click OK.    
</p>
<p> The underneath graph is in the output sheet. It shows that, particularly the cate-
</p>
<p>gories 1 and 2 are severily underpresented. Ordinal regression analysis with a com-
</p>
<p>plimentary log-log function gives little weight to small counts, and more weight to 
</p>
<p>large counts, and may, therefore, better fi t these data. 
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>226
</p>
<p>    
</p>
<p>    Command: 
</p>
<p>  Analyze....Regression....Ordinal Regression....Dependent: enter feeling healthy.... 
</p>
<p>Covariates: enter fruit/week, snacks.week, fastfood/week, physicalactivities/week, 
</p>
<p>age in years....click Options....Link: click Complementary Log-log....click 
</p>
<p>Continue....click OK.   
</p>
<p> Model fi tting information 
</p>
<p> Model  &minus;2 Log Likelihood  Chi-Square  df  Sig. 
</p>
<p> Intercept only  2349,631 
</p>
<p> Final  2321,863  27,768  5  ,000 
</p>
<p>  Link function: Complementary Log-log 
</p>
<p>    In the output sheets the model fi tting table shows that the ordinal model provides 
</p>
<p>an excellent fi t for the data.
</p>
<p>37 Ordinal Scaling for Clinical Scores with Inconsistent Intervals (900 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>227
</p>
<p> Parameter estimates 
</p>
<p> Estimate 
</p>
<p> Std. 
</p>
<p>Error  Wald  df  Sig. 
</p>
<p> 95 % Confi dence 
</p>
<p>interval 
</p>
<p> Lower 
</p>
<p>bound 
</p>
<p> Upper 
</p>
<p>bound 
</p>
<p> Threshold  [feelinghealthy = 1]  &minus;2,427  ,259  87,865  1  ,000  &minus;2,935  &minus;1,920 
</p>
<p> [feelinghealthy = 2]  &minus;1,605  ,229  49,229  1  ,000  &minus;2,053  &minus;1,156 
</p>
<p> [feelinghealthy = 3]  ,483  ,208  5,414  1  ,020  ,076  ,890 
</p>
<p> [feelinghealthy = 4]  ,971  ,208  21,821  1  ,000  ,564  1,379 
</p>
<p> Location  fruit  &minus;,036  ,018  3,907  1  ,048  &minus;,072  ,000 
</p>
<p> snacks  ,004  ,005  ,494  1  ,482  &minus;,006  ,013 
</p>
<p> fastfood  ,017  ,013  1,576  1  ,209  &minus;,009  ,042 
</p>
<p> physical  ,017  ,012  1,772  1  ,183  &minus;,008  ,041 
</p>
<p> age  ,015  ,004  15,393  1  ,000  ,008  ,023 
</p>
<p>  Link function: Complementary Log-log 
</p>
<p>    The above table is also shown, and indicates that fruit and age are signifi cant 
</p>
<p>predictors of levels of feeling healthy. The less fruit/week, the more chance of feel-
</p>
<p>ing healthy versus not health at all (p = 0.048), the higher the age the more chance of 
</p>
<p>feeling healthy versus not healthy at all (p = 0.0001).  
</p>
<p>    Conclusion 
</p>
<p> Clinical studies often have categories as outcome, like various levels of health or 
</p>
<p>disease. Multinomial regression is suitable for analysis, but, if one or two outcome 
</p>
<p>categories in a study are severely underpresented, ordinal regression including 
</p>
<p>specifi c link functions may better fi t the data. The current chapter also shows that, 
</p>
<p>unlike multinomial regression, ordinal regression tests the outcome categories as an 
</p>
<p>overall function.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of multinomial 
</p>
<p>regression is given in the Chap.   28    .    
</p>
<p>Note</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_28">http://dx.doi.org/10.1007/978-3-319-15195-3_28</a></div>
</div>
<div class="page"><p/>
<p>229&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_38
</p>
<p>    Chapter 38   
</p>
<p> Loglinear Models for Assessing Incident Rates 
with Varying Incident Risks (12 Populations) 
</p>
<p>                      General Purpose 
</p>
<p> Data files that assess the effect of various predictors on frequency counts of 
</p>
<p>morbidities/mortalities can be classifi ed into multiple cells with varying incident 
</p>
<p>risks (like, e.g., the incident risk of infarction). The underneath table gives an 
</p>
<p>example: 
</p>
<p> In patients at risk of infarction with little soft drink consumption, and consump-
</p>
<p>tion of wine and other alcoholic beverages the incident risk of infarction equals 
</p>
<p>240/930 = 24.2 %, in those with lots of soft drinks, no wine, and no alcohol otherwise 
</p>
<p>it is 285/1043 = 27.3 %. 
</p>
<p> soft drink (1 = little)  wine (0 = no)  alc beverages (0 = no)  infarcts number  Population number 
</p>
<p> 1,00  1,00  1,00  240  993 
</p>
<p> 1,00  1,00  ,00  237  998 
</p>
<p> 2,00  1,00  1,00  236  1016 
</p>
<p> 2,00  1,00  ,00  236  1011 
</p>
<p> 3,00  1,00  1,00  221  1004 
</p>
<p> 3,00  1,00  ,00  221  1003 
</p>
<p> 1,00  ,00  1,00  270  939 
</p>
<p> 1,00  ,00  ,00  269  940 
</p>
<p> 2,00  ,00  1,00  274  979 
</p>
<p> 2,00  ,00  ,00  273  966 
</p>
<p> 3,00  ,00  1,00  284  1041 
</p>
<p> 3,00  ,00  ,00  285  1043 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 3&rdquo; as Chap. 5, 
</p>
<p>2014. </p>
<p/>
</div>
<div class="page"><p/>
<p>230
</p>
<p>   The general loglinear model using Poisson distributions (see Statistics applied to 
</p>
<p>clinical studies 5th edition, Chap. 23, Poisson regression, pp 267&ndash;275, Springer 
</p>
<p>Heidelberg Germany, 2012, from the same authors) is an appropriate method for 
</p>
<p>statistical testing. This chapter is to assess this method, frequently used by banks 
</p>
<p>and insurance companies but little by clinicians so far.  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> Can general loglinear modeling identify subgroups with signifi cantly larger inci-
</p>
<p>dent risks than other subgroups.  
</p>
<p>    Example 
</p>
<p> The example in the above table will be applied. We wish to investigate the effect of 
</p>
<p>soft drink, wine, and other alcoholic beverages on the risk of infarction. The data fi le 
</p>
<p>is in extras.springer.com, and is entitled &ldquo;loglinear&rdquo;. Start by opening the fi le in 
</p>
<p>SPSS statistical software.
</p>
<p>  Command: 
</p>
<p>  Analyze....Loglinear ....General Loglinear Analysis....Factor(s): enter softdrink, 
</p>
<p>wine, other alc beverages....click &ldquo;Data&rdquo; in the upper textrow of your screen....click 
</p>
<p>Weigh Cases....mark Weight cases by....Frequency Variable: enter &ldquo;infarcts&rdquo;....click 
</p>
<p>OK....return to General Loglinear Analysis....Cell structure: enter &ldquo;population&rdquo;.... 
</p>
<p>Options ....mark Estimates....click Continue....Distribution of Cell Counts: mark 
</p>
<p>Poisson....click OK.   
</p>
<p> Parameter estimates a,b  
</p>
<p> 95 % Confi dence 
</p>
<p>interval 
</p>
<p> Parameter  Estimate 
</p>
<p> Std. 
</p>
<p>Error  Z  Sig. 
</p>
<p> Lower 
</p>
<p>bound 
</p>
<p> Upper 
</p>
<p>bound 
</p>
<p> Constant  &minus;1,513  ,067  &minus;22,496  ,000  &minus;1,645  &minus;1,381 
</p>
<p> [softdrink = 1,00]  ,095  ,093  1,021  ,307  &minus;,088  ,278 
</p>
<p> [softdrink = 2,00]  ,053  ,094  ,569  ,569  &minus;,130  ,237 
</p>
<p> [softdrink = 3,00]  0 c  
</p>
<p> [wine = ,00]  ,215  ,090  2,403  ,016  ,040  ,391 
</p>
<p> [wine = 1,00]  0 c  
</p>
<p> [alcbeverages = ,00]  ,003  ,095  ,029  ,977  &minus;,184  ,189 
</p>
<p> [alcbeverages = 1,00]  0 c  
</p>
<p> [softdrink = 1,00] * [wine = ,00]  &minus;,043  ,126  &minus;,345  ,730  &minus;,291  ,204 
</p>
<p> [softdrink = 1,00] * [wine = 1,00]  0 c  
</p>
<p> [softdrink = 2,00] * [wine = ,00]  &minus;,026  ,126  &minus;,209  ,834  &minus;,274  ,221 
</p>
<p> [softdrink = 2,00] * [wine = 1,00]  0 c  
</p>
<p>(continued)
</p>
<p>38 Loglinear Models for Assessing Incident Rates with Varying Incident Risks&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>231
</p>
<p> Parameter estimates a,b  
</p>
<p> 95 % Confi dence 
</p>
<p>interval 
</p>
<p> Parameter  Estimate 
</p>
<p> Std. 
</p>
<p>Error  Z  Sig. 
</p>
<p> Lower 
</p>
<p>bound 
</p>
<p> Upper 
</p>
<p>bound 
</p>
<p> [softdrink = 3,00] * [wine = ,00]  0 c  
</p>
<p> [softdrink = 3,00] * [wine = 1,00]  0 c  
</p>
<p> [softdrink = 1,00] * [alcbeverages = ,00]  &minus;,021  ,132  &minus;,161  ,872  &minus;,280  ,237 
</p>
<p> [softdrink = 1,00] * 
</p>
<p>[alcbeverages = 1,00] 
</p>
<p> 0 c  
</p>
<p> [softdrink = 2,00] * [alcbeverages = ,00]  ,003  ,132  ,024  ,981  &minus;,256  ,262 
</p>
<p> [softdrink = 2,00] * 
</p>
<p>[alcbeverages = 1,00] 
</p>
<p> 0 c  
</p>
<p> [softdrink = 3,00] * [alcbeverages = ,00]  0 c  
</p>
<p> [softdrink = 3,00] * 
</p>
<p>[alcbeverages = 1,00] 
</p>
<p> 0 c  
</p>
<p> [wine = ,00] * [alcbeverages = ,00]  &minus;,002  ,127  &minus;,018  ,986  &minus;,251  ,246 
</p>
<p> [wine = ,00] * [alcbeverages = 1,00]  0 c  
</p>
<p> [wine = 1,00] * [alcbeverages = ,00]  0 c  
</p>
<p> [wine = 1,00] * [alcbeverages = 1,00]  0 c  
</p>
<p> [softdrink = 1,00] * [wine = ,00] * 
</p>
<p>[alcbeverages = ,00] 
</p>
<p> ,016  ,178  ,089  ,929  &minus;,334  ,366 
</p>
<p> [softdrink = 1,00] * [wine = ,00] * 
</p>
<p>[alcbeverages = 1,00] 
</p>
<p> 0 c  
</p>
<p> [softdrink = 1,00] * [wine = 1,00] * 
</p>
<p>[alcbeverages = ,00] 
</p>
<p> 0 c  
</p>
<p> [softdrink = 1,00] * [wine = 1,0] * 
</p>
<p>[alcbeverages = 1,00] 
</p>
<p> 0 c  
</p>
<p> [softdrink = 2,00] * [wine = ,00] * 
</p>
<p>[alcbeverages = ,00] 
</p>
<p> ,006  ,178  ,036  ,971  &minus;,343  ,356 
</p>
<p> [softdrink = 2,00] * [wine = ,00] * 
</p>
<p>[alcbeverages = 1,00] 
</p>
<p> 0 c  
</p>
<p> [softdrink = 2,00] * [wine = 1,00] * 
</p>
<p>[alcbeverages = ,00] 
</p>
<p> 0 c  
</p>
<p> [softdrink = 2,00] * [wine = 1,0]* 
</p>
<p>[alcbeverages = 1,00] 
</p>
<p> 0 c  
</p>
<p> [softdrink = 3,00] * [wine = ,00] * 
</p>
<p>[alcbeverages = ,00] 
</p>
<p> 0 c  
</p>
<p> [softdrink = 3,00] * [wine = ,00] * 
</p>
<p>[alcbeverages = 1,00] 
</p>
<p> 0 c  
</p>
<p> [softdrink = 3,00] * [wine = 1,00] * 
</p>
<p>[alcbeverages = ,00] 
</p>
<p> 0 c  
</p>
<p> [softdrink = 3,00] * [wine = 1,0] * 
</p>
<p>[alcbeverages = 1,00] 
</p>
<p> 0 c  
</p>
<p>   a Model: Poisson 
</p>
<p>  b Design: Constant + softdrink + wine + alcbeverages + softdrink * wine + softdrink * alcbever-
</p>
<p>ages + wine * alcbeverages + softdrink * wine * alcbeverages 
</p>
<p>  c This parameter is set to zero because it is redundant 
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>232
</p>
<p>    The above pretty dull table gives some wonderful information. The soft drink 
</p>
<p>classes 1 and 2 are not signifi cantly different from zero. These classes have, thus, no 
</p>
<p>greater risk of infarction than class 3. However, the regression coeffi cient of no wine 
</p>
<p>is greater than zero at p = 0.016. No wine drinkers have a signifi cantly greater risk of 
</p>
<p>infarction than the wine drinkers have. No &ldquo;other alcoholic beverages&rdquo; did not pro-
</p>
<p>tect from infarction better than the consumption of it. The three predictors did not 
</p>
<p>display any interaction effects. This result would be in agreement with the famous 
</p>
<p>French paradox.  
</p>
<p>    Conclusion 
</p>
<p> Data fi les that assess the effect of various predictors on frequency counts of mor-
</p>
<p>bidities/mortalities can be classifi ed into multiple cells with varying incident risks 
</p>
<p>(like, e.g., the incident risk of infarction). The general loglinear model using Poisson 
</p>
<p>distributions is an appropriate method for statistical testing. It can identify sub-
</p>
<p>groups with signifi cantly larger incident risks than other subgroups.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information Poisson regression is 
</p>
<p>given in Statistics applied to clinical studies 5th edition, Chap. 23, Poisson regres-
</p>
<p>sion, pp 267&ndash;275, Springer Heidelberg Germany, 2012, from the same authors.    
</p>
<p>38 Loglinear Models for Assessing Incident Rates with Varying Incident Risks&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>233&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_39
</p>
<p>    Chapter 39   
</p>
<p> Loglinear Modeling for Outcome Categories 
(445 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> Multinomial regression is adequate for identifying the main predictors of certain 
</p>
<p>outcome categories, like different levels of injury or quality of life (QOL) (see also 
</p>
<p>Chap. 28). An alternative approach is logit loglinear modeling. The latter method 
</p>
<p>does not use continuous predictors on a case by case basis, but rather the weighted 
</p>
<p>means of these predictors. This approach may allow for relevant additional conclu-
</p>
<p>sions from your data.  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> Does logit loglinear modeling allow for relevant additional conclusions from your 
</p>
<p>categorical data as compared to polytomous / multinomial regression?  
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 3&rdquo; as Chap. 6, 
</p>
<p>2014. </p>
<p/>
</div>
<div class="page"><p/>
<p>234
</p>
<p>    Example 
</p>
<p> age  gender  married  lifestyle  qol 
</p>
<p> 55  1  0  0  2 
</p>
<p> 32  1  1  1  2 
</p>
<p> 27  1  1  0  1 
</p>
<p> 77  0  1  0  3 
</p>
<p> 34  1  1  0  1 
</p>
<p> 35  1  0  1  1 
</p>
<p> 57  1  1  1  2 
</p>
<p> 57  1  1  1  2 
</p>
<p> 35  0  0  0  1 
</p>
<p> 42  1  1  0  2 
</p>
<p> 30  0  1  0  3 
</p>
<p> 34  0  1  1  1 
</p>
<p>  Variable 
</p>
<p> 1 age (years) 
</p>
<p> 2 gender (0 = female) 
</p>
<p> 3 married (0 = no) 
</p>
<p> 4 lifestyle (0 = poor) 
</p>
<p> 5 qol (quality of life levels 0 = low, 2 = high) 
</p>
<p>    The above table show the data of the fi rst 12 patiens of a 445 patient data fi le of 
</p>
<p>qol (quality of life) levels and patient characteristics. The characteristics are the 
</p>
<p>predictor variables of the qol levels (the outcome variable). The entire 
</p>
<p>data fi le is in extras.springer.com, and is entitled &ldquo;logitloglinear&rdquo;. We will 
</p>
<p>fi rst perform a traditional polynomial regression and then the logit loglinear 
</p>
<p>model. SPSS statistical is used for analysis. Start by opening SPSS, and entering 
</p>
<p>the data fi le.
</p>
<p>  Command: 
</p>
<p>  Analyze....Regression....Multinomial Logistic Regression....Dependent: 
</p>
<p>enter "qol"....Factor(s): enter "gender, married, lifestyle"....Covariate(s): enter 
</p>
<p>"age"....OK.    
</p>
<p> The underneath table shows the main results. The following conclusions are 
</p>
<p>appropriate.
</p>
<p>39 Loglinear Modeling for Outcome Categories (445 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>235
</p>
<p> Parameter estimates 
</p>
<p> qol a   B 
</p>
<p> Std. 
</p>
<p>Error  Wald  df  Sig.  Exp(B) 
</p>
<p> 95 % Confi dence 
</p>
<p>interval for Exp (B) 
</p>
<p> Lower 
</p>
<p>bound 
</p>
<p> Upper 
</p>
<p>bound 
</p>
<p> Low  Intercept  28,027  2,539  121,826  1  ,000 
</p>
<p> age  &minus;,559  ,047  143,158  1  ,000  ,572  ,522  ,626 
</p>
<p> [gender = 0]  ,080  ,508  ,025  1  ,875  1,083  ,400  2,930 
</p>
<p> [gender = 1]  0 b   0 
</p>
<p> [married = 0]  2,081  ,541  14,784  1  ,000  8,011  2,774  23,140 
</p>
<p> [married = 1]  0 b   0 
</p>
<p> [lifestyle = 0]  &minus;,801  ,513  2,432  1  ,119  ,449  ,164  1,228 
</p>
<p> [lifestyle = 1]  0 b   0 
</p>
<p> Medium  Intercept  20,133  2,329  74,743  1  ,000 
</p>
<p> age  &minus;,355  ,040  79,904  1  ,000  ,701  ,649  ,758 
</p>
<p> [gender = 0]  ,306  ,372  ,674  1  ,412  1,358  ,654  2,817 
</p>
<p> [gender = 1]  0 b   0 
</p>
<p> [married = 0]  ,612  ,394  2,406  1  ,121  1,843  ,851  3,992 
</p>
<p> [married = 1]  0 b   0 
</p>
<p> [lifestyle = 0]  &minus;,014  ,382  ,001  1  ,972  ,987  ,466  2,088 
</p>
<p> [lifestyle = 1]  0 b   0 
</p>
<p>   a The reference category is: high 
</p>
<p>  b This parameter is set to zero because it is redundant 
</p>
<p>      1.    The unmarried subjects have a greater chance of QOL level 0 than the married 
</p>
<p>ones (the b-value is positive here).   
</p>
<p>   2.    The higher the age, the less chance of QOL levels 0 and 1 (the b-values are nega-
</p>
<p>tive here). If you wish, you may also report the odds ratios (Exp (B).    
</p>
<p>  We will now perform a logit loglinear analysis.
</p>
<p>  Command: 
</p>
<p>  Analyze.... Loglinear....Logit....Dependent: enter "qol"....Factor(s): enter "gender, 
</p>
<p>married, lifestyle"....Cell Covariate(s): enter: "age"....Model: Terms in Model: enter: 
</p>
<p>"gender, married, lifestyle, age"....click Continue....click Options....mark 
</p>
<p>Estimates....mark Adjusted residuals....mark normal probabilities for adjusted resid-
</p>
<p>uals....click Continue....click OK.    
</p>
<p> The underneath table shows the observed frequencies per cell, and the frequen-
</p>
<p>cies to be expected, if the predictors had no effect on the outcome.
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>236
</p>
<p> C
el
</p>
<p>l 
co
</p>
<p>u
n
ts
</p>
<p> a
n
d
 r
</p>
<p>es
id
</p>
<p>u
al
</p>
<p>s a
,b
  
</p>
<p> O
b
se
</p>
<p>rv
ed
</p>
<p> 
 E
</p>
<p>x
p
ec
</p>
<p>te
d
 
</p>
<p> G
en
</p>
<p>d
er
</p>
<p> 
 M
</p>
<p>ar
ri
</p>
<p>ed
 
</p>
<p> L
if
</p>
<p>es
ty
</p>
<p>le
 
</p>
<p> q
o
l 
</p>
<p> C
o
u
n
t 
</p>
<p> %
 
</p>
<p> C
o
u
n
t 
</p>
<p> %
 
</p>
<p> R
es
</p>
<p>id
u
al
</p>
<p> 
</p>
<p> S
ta
</p>
<p>n
d
ar
</p>
<p>d
iz
</p>
<p>ed
 
</p>
<p>re
si
</p>
<p>d
u
al
</p>
<p> 
</p>
<p> A
d
ju
</p>
<p>st
ed
</p>
<p> 
</p>
<p>re
si
</p>
<p>d
u
al
</p>
<p> 
 D
</p>
<p>ev
ia
</p>
<p>n
ce
</p>
<p> 
</p>
<p> M
al
</p>
<p>e 
 U
</p>
<p>n
m
</p>
<p>ar
ri
</p>
<p>ed
 
</p>
<p> In
ac
</p>
<p>ti
v
e 
</p>
<p> L
o
w
</p>
<p> 
 7
 
</p>
<p> 2
3
,3
</p>
<p> %
 
</p>
<p> 9
,1
</p>
<p>1
1
 
</p>
<p> 3
0
,4
</p>
<p> %
 
</p>
<p> &minus;
2
,1
</p>
<p>1
1
 
</p>
<p> &minus;
,8
</p>
<p>3
8
 
</p>
<p> &minus;
1
,1
</p>
<p>2
5
 
</p>
<p> &minus;
1
,9
</p>
<p>2
1
 
</p>
<p> M
ed
</p>
<p>iu
m
</p>
<p> 
 1
6
 
</p>
<p> 5
3
,3
</p>
<p> %
 
</p>
<p> 1
4
,1
</p>
<p>2
4
 
</p>
<p> 4
7
,1
</p>
<p> %
 
</p>
<p> 1
,8
</p>
<p>7
6
 
</p>
<p> ,6
8
6
 
</p>
<p> ,8
8
8
 
</p>
<p> 1
,9
</p>
<p>9
8
 
</p>
<p> H
ig
</p>
<p>h
 
</p>
<p> 7
 
</p>
<p> 2
3
,3
</p>
<p> %
 
</p>
<p> 6
,7
</p>
<p>6
5
 
</p>
<p> 2
2
,6
</p>
<p> %
 
</p>
<p> ,2
3
5
 
</p>
<p> ,1
0
3
 
</p>
<p> ,1
2
7
 
</p>
<p> ,6
9
1
 
</p>
<p> A
ct
</p>
<p>iv
e 
</p>
<p> L
o
w
</p>
<p> 
 2
9
 
</p>
<p> 6
1
,7
</p>
<p> %
 
</p>
<p> 2
5
,8
</p>
<p>4
0
 
</p>
<p> 5
5
,0
</p>
<p> %
 
</p>
<p> 3
,1
</p>
<p>6
0
 
</p>
<p> ,9
2
7
 
</p>
<p> 2
,0
</p>
<p>1
8
 
</p>
<p> 2
,5
</p>
<p>8
7
 
</p>
<p> M
ed
</p>
<p>iu
m
</p>
<p> 
 5
 
</p>
<p> 1
0
,6
</p>
<p> %
 
</p>
<p> 1
0
,0
</p>
<p>8
7
 
</p>
<p> 2
1
,5
</p>
<p> %
 
</p>
<p> &minus;
5
,0
</p>
<p>8
7
 
</p>
<p> &minus;
1
,8
</p>
<p>0
7
 
</p>
<p> &minus;
2
,9
</p>
<p>3
3
 
</p>
<p> &minus;
2
,6
</p>
<p>4
9
 
</p>
<p> H
ig
</p>
<p>h
 
</p>
<p> 1
3
 
</p>
<p> 2
7
,7
</p>
<p> %
 
</p>
<p> 1
1
,0
</p>
<p>7
4
 
</p>
<p> 2
3
,6
</p>
<p> %
 
</p>
<p> 1
,9
</p>
<p>2
6
 
</p>
<p> ,6
6
2
 
</p>
<p> 2
,0
</p>
<p>1
9
 
</p>
<p> 2
,0
</p>
<p>4
2
 
</p>
<p> M
ar
</p>
<p>ri
ed
</p>
<p> 
 In
</p>
<p>ac
ti
</p>
<p>v
e 
</p>
<p> L
o
w
</p>
<p> 
 9
 
</p>
<p> 1
1
,0
</p>
<p> %
 
</p>
<p> 1
0
,6
</p>
<p>3
6
 
</p>
<p> 1
3
,0
</p>
<p> %
 
</p>
<p> &minus;
1
,6
</p>
<p>3
6
 
</p>
<p> &minus;
,5
</p>
<p>3
8
 
</p>
<p> &minus;
,8
</p>
<p>2
6
 
</p>
<p> &minus;
1
,7
</p>
<p>3
4
 
</p>
<p> M
ed
</p>
<p>iu
m
</p>
<p> 
 4
1
 
</p>
<p> 5
0
,0
</p>
<p> %
 
</p>
<p> 4
3
,4
</p>
<p>5
4
 
</p>
<p> 5
3
,0
</p>
<p> %
 
</p>
<p> &minus;
2
,4
</p>
<p>5
4
 
</p>
<p> &minus;
,5
</p>
<p>4
3
 
</p>
<p> &minus;
1
,0
</p>
<p>6
2
 
</p>
<p> &minus;
2
,1
</p>
<p>8
3
 
</p>
<p> H
ig
</p>
<p>h
 
</p>
<p> 3
2
 
</p>
<p> 3
9
,0
</p>
<p> %
 
</p>
<p> 2
7
,9
</p>
<p>1
0
 
</p>
<p> 3
4
,0
</p>
<p> %
 
</p>
<p> 4
,0
</p>
<p>9
0
 
</p>
<p> ,9
5
3
 
</p>
<p> 2
,0
</p>
<p>0
6
 
</p>
<p> 2
,9
</p>
<p>5
8
 
</p>
<p> A
ct
</p>
<p>iv
e 
</p>
<p> L
o
w
</p>
<p> 
 1
5
 
</p>
<p> 2
3
,8
</p>
<p> %
 
</p>
<p> 1
4
,4
</p>
<p>1
3
 
</p>
<p> 2
2
,9
</p>
<p> %
 
</p>
<p> ,5
8
7
 
</p>
<p> ,1
7
6
 
</p>
<p> ,7
5
4
 
</p>
<p> 1
,0
</p>
<p>9
4
 
</p>
<p> M
ed
</p>
<p>iu
m
</p>
<p> 
 2
7
 
</p>
<p> 4
2
,9
</p>
<p> %
 
</p>
<p> 2
1
,3
</p>
<p>3
6
 
</p>
<p> 3
3
,9
</p>
<p> %
 
</p>
<p> 5
,6
</p>
<p>6
4
 
</p>
<p> 1
,5
</p>
<p>0
8
 
</p>
<p> 2
,7
</p>
<p>6
1
 
</p>
<p> 3
,5
</p>
<p>6
6
 
</p>
<p> H
ig
</p>
<p>h
 
</p>
<p> 2
1
 
</p>
<p> 3
3
,3
</p>
<p> %
 
</p>
<p> 2
7
,2
</p>
<p>5
1
 
</p>
<p> 4
3
,3
</p>
<p> %
 
</p>
<p> &minus;
6
,2
</p>
<p>5
1
 
</p>
<p> &minus;
1
,5
</p>
<p>9
0
 
</p>
<p> &minus;
2
,8
</p>
<p>6
8
 
</p>
<p> &minus;
3
,3
</p>
<p>0
8
 
</p>
<p> F
em
</p>
<p>al
e 
</p>
<p> U
n
m
</p>
<p>ar
ri
</p>
<p>ed
 
</p>
<p> In
ac
</p>
<p>ti
v
e 
</p>
<p> L
o
w
</p>
<p> 
 1
2
 
</p>
<p> 2
6
,1
</p>
<p> %
 
</p>
<p> 1
1
,1
</p>
<p>1
9
 
</p>
<p> 2
4
,2
</p>
<p> %
 
</p>
<p> ,8
8
1
 
</p>
<p> ,3
0
3
 
</p>
<p> ,6
2
7
 
</p>
<p> 1
,3
</p>
<p>5
3
 
</p>
<p> M
ed
</p>
<p>iu
m
</p>
<p> 
 2
6
 
</p>
<p> 5
6
,5
</p>
<p> %
 
</p>
<p> 2
2
,9
</p>
<p>9
1
 
</p>
<p> 5
0
,0
</p>
<p> %
 
</p>
<p> 3
,0
</p>
<p>0
9
 
</p>
<p> ,8
8
7
 
</p>
<p> 1
,6
</p>
<p>0
1
 
</p>
<p> 2
,5
</p>
<p>2
9
 
</p>
<p> H
ig
</p>
<p>h
 
</p>
<p> 8
 
</p>
<p> 1
7
,4
</p>
<p> %
 
</p>
<p> 1
1
,8
</p>
<p>9
0
 
</p>
<p> 2
5
,8
</p>
<p> %
 
</p>
<p> &minus;
3
,6
</p>
<p>9
0
 
</p>
<p> &minus;
1
,3
</p>
<p>1
0
 
</p>
<p> &minus;
1
,9
</p>
<p>9
4
 
</p>
<p> &minus;
2
,5
</p>
<p>1
8
 
</p>
<p> A
ct
</p>
<p>iv
e 
</p>
<p> L
o
w
</p>
<p> 
 1
8
 
</p>
<p> 5
4
,5
</p>
<p> %
 
</p>
<p> 1
9
,9
</p>
<p>3
0
 
</p>
<p> 6
0
,4
</p>
<p> %
 
</p>
<p> &minus;
1
,9
</p>
<p>3
0
 
</p>
<p> &minus;
,6
</p>
<p>8
7
 
</p>
<p> &minus;
,9
</p>
<p>7
8
 
</p>
<p> &minus;
1
,9
</p>
<p>1
5
 
</p>
<p> M
ed
</p>
<p>iu
m
</p>
<p> 
 6
 
</p>
<p> 1
8
,2
</p>
<p> %
 
</p>
<p> 5
,7
</p>
<p>9
9
 
</p>
<p> 1
7
,6
</p>
<p> %
 
</p>
<p> ,2
0
1
 
</p>
<p> ,0
9
2
 
</p>
<p> ,1
3
8
 
</p>
<p> ,6
3
9
 
</p>
<p> H
ig
</p>
<p>h
 
</p>
<p> 9
 
</p>
<p> 2
7
,3
</p>
<p> %
 
</p>
<p> 7
,2
</p>
<p>7
1
 
</p>
<p> 2
2
,0
</p>
<p> %
 
</p>
<p> 1
,7
</p>
<p>2
9
 
</p>
<p> ,7
2
6
 
</p>
<p> 1
,0
</p>
<p>6
4
 
</p>
<p> 1
,9
</p>
<p>5
9
 
</p>
<p> M
ar
</p>
<p>ri
ed
</p>
<p> 
 In
</p>
<p>ac
ti
</p>
<p>v
e 
</p>
<p> L
o
w
</p>
<p> 
 1
5
 
</p>
<p> 1
8
,5
</p>
<p> %
 
</p>
<p> 1
2
,1
</p>
<p>3
4
 
</p>
<p> 1
5
,0
</p>
<p> %
 
</p>
<p> 2
,8
</p>
<p>6
6
 
</p>
<p> ,8
9
2
 
</p>
<p> 1
,6
</p>
<p>7
0
 
</p>
<p> 2
,5
</p>
<p>2
2
 
</p>
<p> M
ed
</p>
<p>iu
m
</p>
<p> 
 2
7
 
</p>
<p> 3
3
,3
</p>
<p> %
 
</p>
<p> 2
9
,4
</p>
<p>3
2
 
</p>
<p> 3
6
,3
</p>
<p> %
 
</p>
<p> &minus;
2
,4
</p>
<p>3
2
 
</p>
<p> &minus;
,5
</p>
<p>6
2
 
</p>
<p> &minus;
1
,7
</p>
<p>8
1
 
</p>
<p> &minus;
2
,1
</p>
<p>5
8
 
</p>
<p> H
ig
</p>
<p>h
 
</p>
<p> 3
9
 
</p>
<p> 4
8
,1
</p>
<p> %
 
</p>
<p> 3
9
,4
</p>
<p>3
4
 
</p>
<p> 4
8
,7
</p>
<p> %
 
</p>
<p> &minus;
,4
</p>
<p>3
4
 
</p>
<p> &minus;
,0
</p>
<p>9
7
 
</p>
<p> &minus;
,3
</p>
<p>5
8
 
</p>
<p> &minus;
,9
</p>
<p>2
9
 
</p>
<p> A
ct
</p>
<p>iv
e 
</p>
<p> L
o
w
</p>
<p> 
 1
6
 
</p>
<p> 2
5
,4
</p>
<p> %
 
</p>
<p> 1
7
,8
</p>
<p>1
7
 
</p>
<p> 2
8
,3
</p>
<p> %
 
</p>
<p> &minus;
1
,8
</p>
<p>1
7
 
</p>
<p> &minus;
,5
</p>
<p>0
8
 
</p>
<p> &minus;
1
,1
</p>
<p>2
3
 
</p>
<p> &minus;
1
,8
</p>
<p>5
5
 
</p>
<p> M
ed
</p>
<p>iu
m
</p>
<p> 
 2
4
 
</p>
<p> 3
8
,1
</p>
<p> %
 
</p>
<p> 2
4
,7
</p>
<p>7
9
 
</p>
<p> 3
9
,3
</p>
<p> %
 
</p>
<p> &minus;
,7
</p>
<p>7
9
 
</p>
<p> &minus;
,2
</p>
<p>0
1
 
</p>
<p> &minus;
,8
</p>
<p>8
2
 
</p>
<p> &minus;
1
,2
</p>
<p>3
8
 
</p>
<p> H
ig
</p>
<p>h
 
</p>
<p> 2
3
 
</p>
<p> 3
6
,5
</p>
<p> %
 
</p>
<p> 2
0
,4
</p>
<p>0
4
 
</p>
<p> 3
2
,4
</p>
<p> %
 
</p>
<p> 2
,5
</p>
<p>9
6
 
</p>
<p> ,6
9
9
 
</p>
<p> 1
,4
</p>
<p>0
7
 
</p>
<p> 2
,3
</p>
<p>4
7
 
</p>
<p>  
 
</p>
<p>a  M
o
d
el
</p>
<p>: 
M
</p>
<p>u
lt
</p>
<p>in
o
m
</p>
<p>ia
l 
</p>
<p>lo
g
it
</p>
<p> 
</p>
<p>  b  D
es
</p>
<p>ig
n
: 
</p>
<p>C
o
n
st
</p>
<p>an
t +
</p>
<p> q
o
l +
</p>
<p> q
o
l 
</p>
<p>*
 g
</p>
<p>en
d
er
</p>
<p> +
 q
</p>
<p>o
l 
</p>
<p>*
 m
</p>
<p>ar
ri
</p>
<p>ed
 +
</p>
<p> q
o
l 
</p>
<p>*
 l
</p>
<p>if
es
</p>
<p>ty
le
</p>
<p> +
 q
</p>
<p>o
l 
</p>
<p>*
 a
</p>
<p>g
e 
</p>
<p>39 Loglinear Modeling for Outcome Categories (445 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>237
</p>
<p>    The two graphs below show the goodnesses of fi t of the model, which are 
</p>
<p>obviously pretty good, as both expected versus observed counts (fi rst graph below) 
</p>
<p>and q-q plot (second graph below) show excellent linear relationships (q-q plots are 
</p>
<p>further explained in Chap. 42). 
</p>
<p>    
</p>
<p>       
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>238
</p>
<p>    Note: the Q-Q plot (Q stands for quantile) shows here that the differences 
</p>
<p>between observed and expected counts follow a normal distribution. 
</p>
<p> The next page table shows the results of the statistical tests of the data.
</p>
<p>    1.    The unmarried subjects have a greater chance of QOL 1 (low QOL) than their 
</p>
<p>married counterparts.   
</p>
<p>   2.    The poor lifestyle subjects have a greater chance of QOL 1 (low QOL) than their 
</p>
<p>adequate-lifestyle counterparts.   
</p>
<p>   3.    The higher the age the more chance of QOL 2 (medium level QOL), which is 
</p>
<p>neither very good nor very bad, nut rather in between (as you would expect).     
</p>
<p> We may conclude that the two procedures produce similar results, but the latter 
</p>
<p>method provides some additional and relevant information about the lifestyle and 
</p>
<p>age data.
</p>
<p> Parameter estimates c,d  
</p>
<p> 95 % Confi dence 
</p>
<p>interval 
</p>
<p> Parameter  Estimate 
</p>
<p> Std. 
</p>
<p>Error  Z  Slg. 
</p>
<p> Lower 
</p>
<p>bound 
</p>
<p> Upper 
</p>
<p>bound 
</p>
<p> Constant  [gender = 0] * [married = 0] * 
</p>
<p>[lifestyle = 0] 
</p>
<p> &minus;7,402 a  
</p>
<p> [gender = 0] * [married = 0] * 
</p>
<p>[lifestyle = 1] 
</p>
<p> &minus;7,409 a  
</p>
<p> [gender = 0] * [married = 1] * 
</p>
<p>[lifestyle = 0] 
</p>
<p> &minus;6,088 a  
</p>
<p> [gender = 0] * [married = 1]* 
</p>
<p>[lifestyle = 1] 
</p>
<p> &minus;6,349 a  
</p>
<p> [gender = 1] * [married = 0] * 
</p>
<p>[lifestyle = 0] 
</p>
<p> &minus;6,825 a  
</p>
<p> [gender = 1]* [married = 0]* 
</p>
<p>[lifestyle = 1] 
</p>
<p> &minus;7,406 a  
</p>
<p> [gender = 1] * [married = 1]* 
</p>
<p>[lifestyle = 0] 
</p>
<p> &minus;5,960 a  
</p>
<p> [gender = 1]* [married = 1]* 
</p>
<p>[lifestyle = 1] 
</p>
<p> &minus;6,567 a  
</p>
<p> [qol = 1]  5,332  8,845  ,603  ,547  &minus;12,004  22,667 
</p>
<p> [qol = 2]  4,280  10,073  ,425  ,671  &minus;15,463  24,022 
</p>
<p> [qol = 3]  0 b  
</p>
<p> [qol = 1]* [gender = 0]  ,389  ,360  1,079  ,280  &minus;,317  1,095 
</p>
<p> [qol = 1]* [gender = 1]  0 b  
</p>
<p> [qol = 2]* [gender = 0]  &minus;,140  ,265  &minus;,528  ,597  &minus;,660  ,380 
</p>
<p> [qol = 2]* [gender = 1]  0 b  
</p>
<p> [qol = 3]* [gender = 0]  0 b  
</p>
<p> [qol = 3]* [gender = 1]  0 b  
</p>
<p> [qol = 1]* [married = 0]  1,132  ,283  4,001  ,000  ,578  1,687 
</p>
<p>(continued)
</p>
<p>39 Loglinear Modeling for Outcome Categories (445 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>239
</p>
<p> Parameter estimates c,d  
</p>
<p> 95 % Confi dence 
</p>
<p>interval 
</p>
<p> Parameter  Estimate 
</p>
<p> Std. 
</p>
<p>Error  Z  Slg. 
</p>
<p> Lower 
</p>
<p>bound 
</p>
<p> Upper 
</p>
<p>bound 
</p>
<p> [qol = 1]* [married = 1]  0 b  
</p>
<p> [qol = 2] * [married = 0]  &minus;,078  ,294  &minus;,267  ,790  &minus;,655  ,498 
</p>
<p> [qol = 2] * [married = 1]  0 b  
</p>
<p> [qol = 3] * [married = 0]  0 b  
</p>
<p> [qol = 3] * [married = 1]  0 b  
</p>
<p> [qol = 1]* [lifestyle = 0]  &minus;1,004  ,311  &minus;3,229  ,001  &minus;1,613  &minus;,394 
</p>
<p> [qol = 1] * [lifestyle = 1]  0 b  
</p>
<p> [qol = 2]* [lifestyle = 0]  ,016  ,271  ,059  ,953  &minus;,515  ,547 
</p>
<p> [qol = 2]* [lifestyle = 1]  0 b  
</p>
<p> [qol = 3]* [lifestyle = 0]  0 b  
</p>
<p> [qol = 3]* [lifestyle = 1]  0 b  
</p>
<p> [qol = 1] * age  ,116  ,074  1,561  ,119  &minus;,030  ,261 
</p>
<p> [qol = 2]* age  ,114  ,054  2,115  ,034  ,008  ,219 
</p>
<p> [qol = 3]* age  ,149  ,138  1,075  ,282  &minus;,122  ,419 
</p>
<p>   a Constants are not parameters under the multinomial assumption. Therefore, their standard errors 
</p>
<p>are not calculated 
</p>
<p>  b This parameter is set to zero because it is redundant 
</p>
<p>  c Model: Multinomial logit 
</p>
<p>  d Design: Constant + qol + qol * gender + qol * married + qol * lifestyle + qol * age 
</p>
<p>        Conclusion 
</p>
<p> Multinomial regression is adequate for identifying the main predictors of certain 
</p>
<p>outcome categories, like different levels of injury or quality of life An alternative 
</p>
<p>approach is logit loglinear modeling. The latter method does not use continuous 
</p>
<p>predictors on a case by case basis, but rather the weighted means. This approach 
</p>
<p>allowed for relevant additional conclusions in the example given.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of polytomous/
</p>
<p>multinomial regression is given in the Chap. 28. More information of loglinear 
</p>
<p>modeling is in the Chap. 38, entitled &ldquo;Loglinear models for assessing incident rates 
</p>
<p>with varying incident risks&rdquo;.    
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>241&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_40
</p>
<p>    Chapter 40   
</p>
<p> Heterogeneity in Clinical Research: 
Mechanisms Responsible (20 Studies) 
</p>
<p>                      General Purpose 
</p>
<p> In clinical research similar studies often have different results. This may be due to 
</p>
<p>differences in patient-characteristics and trial-quality-characteristics such as the use 
</p>
<p>of blinding, randomization, and placebo-controls. This chapter is to assess whether 
</p>
<p>3-dimensional scatter plots and regression analyses with the treatment results as 
</p>
<p>outcome and the predictors of heterogeneity as exposure are able to identify mecha-
</p>
<p>nisms responsible.  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> Are scatter plots and regression models able to identify the mechanisms responsible 
</p>
<p>for heterogeneity in clinical research.  
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 3&rdquo; as 
</p>
<p>Chap. 7, 2014. </p>
<p/>
</div>
<div class="page"><p/>
<p>242
</p>
<p>    Example 
</p>
<p> Variables 
</p>
<p> 1  2  3  4 
</p>
<p> % ADEs  study size  age  investigator type 
</p>
<p> 21,00  106  1  1 
</p>
<p> 14,40  578  1  1 
</p>
<p> 30,40  240  1  1 
</p>
<p> 6,10  671  0  0 
</p>
<p> 12,00  681  0  0 
</p>
<p> 3,40  28411  1  0 
</p>
<p> 6,60  347  0  0 
</p>
<p> 3,30  8601  0  0 
</p>
<p> 4,90  915  0  0 
</p>
<p> 9,60  156  0  0 
</p>
<p> 6,50  4093  0  0 
</p>
<p> 6,50  18820  0  0 
</p>
<p> 4,10  6383  0  0 
</p>
<p> 4,30  2933  0  0 
</p>
<p> 3,50  480  0  0 
</p>
<p> 4,30  19070  1  0 
</p>
<p> 12,60  2169  1  0 
</p>
<p> 33,20  2261  0  1 
</p>
<p> 5,60  12793  0  0 
</p>
<p> 5,10  355  0  0 
</p>
<p>  ADEs = adverse drug effects 
</p>
<p> age 0 = young, 1 = elderly 
</p>
<p> investigator type, 0 = pharmacists, 1 = clinicians 
</p>
<p>    In the above 20 studies the % of admissions to hospital due to adverse drug effects 
</p>
<p>were assessed. The studies were very heterogeneous, because the percentages 
</p>
<p>admissions due to adverse drug effects varied from 3.3 to 33.2. In order to identify 
</p>
<p>possible mechanisms responsible, a scatter plot was fi rst drawn. The data fi le is in 
</p>
<p>extras.springer.com and is entitled &ldquo;heterogeneity&rdquo;. 
</p>
<p> Start by opening the data fi le in SPSS statistical software.
</p>
<p>  Command: 
</p>
<p>  click Graphs....click Legacy Dialogs....click Scatter/Dot....click 3-D Scatter....click 
</p>
<p>Defi ne....Y-Axis: enter percentage (ADEs)....X Axis: enter study-magnitude....Z 
</p>
<p>Axis: enter clinicians =1....Set Markers by: enter elderly = 1....click OK.    
</p>
<p> The underneath fi gure is displayed, and it gives a 3-dimensional graph of the 
</p>
<p>outcome (% adverse drug effects) versus study size versus investigator type 
</p>
<p>(1 = clinician, 0 = pharmacist). A 4th dimension is obtained by coloring the circles 
</p>
<p>40 Heterogeneity in Clinical Research: Mechanisms Responsible (20 Studies)</p>
<p/>
</div>
<div class="page"><p/>
<p>243
</p>
<p>(green = elderly, blue = young). Small studies tended to have larger results. Also 
</p>
<p>clinician studies (clinicians = 1) tended to have larger results, while studies in elderly 
</p>
<p>had both large and small effects. 
</p>
<p>  
</p>
<p>40,00
</p>
<p>elderly=1
</p>
<p>0
1
</p>
<p>30,00
</p>
<p>20,00
</p>
<p>10,00
</p>
<p>,00
</p>
<p>p
e
rc
e
n
ta
g
e
A
D
E
s
</p>
<p>clin
icia
</p>
<p>ns=
1
</p>
<p>study-magnitude
</p>
<p>1,0 ,
8 ,6
</p>
<p>,4 ,2
,0
</p>
<p>0 10000 20000
</p>
<p>  
</p>
<p>    In order to test whether the observed trends were statistically signifi cant, a linear 
</p>
<p>regression is performed.
</p>
<p>  Command: 
</p>
<p>  Analyze....Regression....Linear....Dependent: enter "percentage ADEs".... 
</p>
<p>Independent(s): enter "study-magnitude, elderly = 1, clinicians = 1"....click OK.     
</p>
<p> Coeffi cients a  
</p>
<p> Unstandardized 
</p>
<p>coeffi cients 
</p>
<p> Standardized 
</p>
<p>coeffi cients 
</p>
<p> Model  B  Std. Error  Beta  t  Sig. 
</p>
<p> 1  (Constant)  6,924  1,454  4,762  ,000 
</p>
<p> Study- magnitude   &minus;7,674E-5  ,000  &minus;,071  &minus;,500  ,624 
</p>
<p> Elderly = 1  &minus;1,393  2,885  &minus;,075  &minus;,483  ,636 
</p>
<p> Clinicians = 1  18,932  3,359  ,887  5,636  ,000 
</p>
<p>   a Dependent variable: percentage ADEs  
</p>
<p> The output sheets show the above table. The investigator type is the only 
</p>
<p>statistically signifi cant predictor of % of ADEs. Clinicians observed signifi cantly 
</p>
<p>more ADE admissions than did pharmacists at p &lt; 0.0001. This is in agreement with 
</p>
<p>the above graph  
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>244
</p>
<p>    Conclusion 
</p>
<p> In clinical research similar studies often have different results. This may be due to 
</p>
<p>differences in patient-characteristics and trial-quality-characteristics such as the use 
</p>
<p>of blinding, randomization, and placebo-controls. This chapter shows that 
</p>
<p>3- dimensional scatter plot are able to identify the mechanisms responsible. Linear 
</p>
<p>regression analyses with the treatment results as outcome and the predictors of 
</p>
<p>heterogeneity as exposure are able to rule out heterogeneity due to chance. This is 
</p>
<p>particularly important, when no clinical explanation is found or when heterogeneity 
</p>
<p>seems to be clinically irrelevant.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of heterogeneous 
</p>
<p>studies and meta-regression is in Statistics applied to clinical studies 5th edition, 
</p>
<p>Chap. 33, Meta-analysis, review and update of methodologies, pp 379&ndash;390, and 
</p>
<p>Chap. 34, Meta-regression, pp 391&ndash;397, Springer Heidelberg Germany, both from 
</p>
<p>the same authors as the current work.   
</p>
<p>40 Heterogeneity in Clinical Research: Mechanisms Responsible (20 Studies)</p>
<p/>
</div>
<div class="page"><p/>
<p>245&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_41
</p>
<p>Chapter 41
</p>
<p>Performance Evaluation of Novel Diagnostic 
Tests (650 and 588 Patients)
</p>
<p> General Purpose
</p>
<p>Both logistic regression and c-statistics can be used to evaluate the performance of 
</p>
<p>novel diagnostic tests (see also Machine learning in medicine part two, Chap. 6, 
</p>
<p>pp 45&ndash;52, Logistic regression for assessment of novel diagnostic tests against con-
</p>
<p>trols, Springer Heidelberg Germany, 2013, from the same authors). This chapter is 
</p>
<p>to assess whether one method can outperform the other.
</p>
<p> Primary Scientific Question
</p>
<p>Is logistic regression with the odds of disease as outcome and test scores as covari-
</p>
<p>ate a better alternative for concordance (c)-statistics using the area under the curve 
</p>
<p>of ROC (receiver operated characteristic) curves.
</p>
<p> Example
</p>
<p>In 650 patients with peripheral vascular disease a noninvasive vascular lab test was 
</p>
<p>performed. The results of the first 10 patients are underneath.
</p>
<p>test score presence of peripheral vascular disease (0 = no, 1 = yes)
</p>
<p>1,00 ,00
</p>
<p>1,00 ,00
</p>
<p>This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 3&rdquo; as Chap. 8, 
</p>
<p>2014.
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>246
</p>
<p>test score presence of peripheral vascular disease (0 = no, 1 = yes)
</p>
<p>2,00 ,00
</p>
<p>2,00 ,00
</p>
<p>3,00 ,00
</p>
<p>3,00 ,00
</p>
<p>3,00 ,00
</p>
<p>4,00 ,00
</p>
<p>4,00 ,00
</p>
<p>4,00 ,00
</p>
<p>The entire data file is in extras.springer.com, and is entitled &ldquo;vascdisease1&rdquo;. Start 
</p>
<p>by opening the data file in SPSS.
</p>
<p>Then Command:
</p>
<p>Graphs....Legacy Dialogs....Histogram....Variable(s): enter "score"....Row(s): enter 
</p>
<p>"disease"....click OK.
</p>
<p>The underneath figure shows the output sheet. On the x-axis we have the vascular 
</p>
<p>lab scores, on the y-axis &ldquo;how often&rdquo;. The scores in patients with (1) and without (0) 
</p>
<p>the presence of disease according to the gold standard (angiography) are respec-
</p>
<p>tively in the lower and upper graph.
</p>
<p> 
</p>
<p>41 Performance Evaluation of Novel Diagnostic Tests (650 and 588 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>247
</p>
<p>The second data file is obtained from a parallel-group population of 588 patients 
</p>
<p>after the noninvasive vascular test has been improved. The first 10 patients are 
</p>
<p>underneath.
</p>
<p>test score presence of peripheral vascular disease (0 = no, 1 = yes)
</p>
<p>1,00 ,00
</p>
<p>2,00 ,00
</p>
<p>2,00 ,00
</p>
<p>3,00 ,00
</p>
<p>3,00 ,00
</p>
<p>3,00 ,00
</p>
<p>4,00 ,00
</p>
<p>4,00 ,00
</p>
<p>4,00 ,00
</p>
<p>4,00 ,00
</p>
<p>The entire data file is in extras.springer.com, and is entitled "vascdisease2". Start 
</p>
<p>by opening the data file in SPSS.
</p>
<p>Then Command:
</p>
<p>Graphs....Legacy Dialogs....Histogram....Variable(s): enter "score"....Row(s): enter 
</p>
<p>"disease"....click OK.
</p>
<p> 
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>248
</p>
<p>The above figure is in the output sheet.
</p>
<p>The first test (upper figure) seems to perform less well than the second test (lower 
</p>
<p>figure), because there may be more risk of false positives (the 0 disease curve is 
</p>
<p>more skewed to the right in the upper than in the lower figure).
</p>
<p> Binary Logistic Regression
</p>
<p>Binary logistic regression is used for assessing this question. The following 
</p>
<p> reasoning is used. If we move the threshold for a positive test to the right, then the 
</p>
<p>proportion of false positive will decrease. The steeper the logistic regression line the 
</p>
<p>faster this will happen. In contrast, if we move the threshold to the left, the propor-
</p>
<p>tion of false negatives will decrease. Again, the steeper the logistic regression line, 
</p>
<p>the faster it will happen. And so, the steeper the logistic regression line, the fewer 
</p>
<p>false negatives and false positives and thus the better the diagnostic test.
</p>
<p>For both data files the above analysis is performed.
</p>
<p>Command:
</p>
<p>Analyze&hellip;. Regression.&hellip;Binary logistic&hellip;. Dependent variable: disease&hellip;.
</p>
<p>Covariate: score.&hellip;OK.
</p>
<p>The output sheets show the best fit regression equations.
</p>
<p>Variables in the equation
</p>
<p>B S.E. Wald df Sig. Exp(B)
</p>
<p>Step 1a VAR00001 ,398 ,032 155,804 1 ,000 1,488
</p>
<p>Constant &minus;8,003 ,671 142,414 1 ,000 ,000
aVariable(s) entered on step 1: VAR00001
</p>
<p>Variables in the equation
</p>
<p>B S.E. Wald df Sig. Exp(B)
</p>
<p>Step 1a VAR00001 ,581 ,051 130,715 1 ,000 1,789
</p>
<p>Constant &minus;10,297 ,915 126,604 1 ,000 ,000
aVariable(s) entered on step 1: VAR00001
</p>
<p>Data file 1: log odds of having the disease = &minus;8.003 + 0.398 times the score
</p>
<p>Data file 2: log odds of having the disease = &minus;10.297 + 0.581 times the score.
</p>
<p>The regression coefficient of data file 2 is much steeper than that of data file 1, 
</p>
<p>0.581 and 0.398.
</p>
<p>Both regression equations produce highly significant regression coefficients with 
</p>
<p>standard errors of respectively 0.032 and 0.051 and p-values of &lt; 0.0001. The two 
</p>
<p>regression coefficients are tested for significance of difference using the z &ndash; test (the 
</p>
<p>z-test is in Chap. 2 of Statistics on a Pocket Calculator part 2, pp 3&ndash;5, Springer 
</p>
<p>Heidelberg Germany, 2012, from the same authors):
</p>
<p>41 Performance Evaluation of Novel Diagnostic Tests (650 and 588 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>249
</p>
<p> 
</p>
<p>z
</p>
<p>which c
</p>
<p>= &minus;( ) &radic; +( )= &minus; = &minus;0 398 0 581 0 032 0 051 0 183 0 060 3 052 2. . / . . . / . . ,
oorresponds witha p -value of &lt; 0 01. .
</p>
<p> 
</p>
<p>Obviously, test 2 produces a significantly steeper regression model, which means 
</p>
<p>that it is a better predictor of the risk of disease than test 1. We can, additionally, 
</p>
<p>calculate the odds ratios of successfully testing with test 2 versus test 1. The odds of 
</p>
<p>disease with test 1 equals e0.398 = 1.488, and with test 2 it equals e0.581 = 1.789. The 
</p>
<p>odds ratio = 1.789/1.488 = 1.202, meaning that the second test produces a 1.202 
</p>
<p>times better chance of rightly predicting the disease than test 1 does.
</p>
<p> C-Statistics
</p>
<p>C-statistics is used as a contrast test. Open data file 1 again.
</p>
<p>Command:
</p>
<p>Analyze....ROC Curve....Test Variable: enter "score"....State Variable: enter "dis-
</p>
<p>ease"....Value of State Variable: type "1"....mark ROC Curve....mark Standard Error 
</p>
<p>and Confidence Intervals....click OK.
</p>
<p>1,0
</p>
<p>0,8
</p>
<p>0,6
</p>
<p>0,4
</p>
<p>0,2
</p>
<p>0,0
0,0 0,2 0,4
</p>
<p>Diagonal segments are produced by ties.
</p>
<p>1 - Specificity
</p>
<p>S
e
</p>
<p>n
s
</p>
<p>it
iv
</p>
<p>it
y
</p>
<p>ROC Curve
</p>
<p>0,6 0,8 1,0
</p>
<p> 
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>250
</p>
<p>Area under the curve
</p>
<p>Test result variable(s): score
</p>
<p>Area Std. errora Asymptotic Sig.b
Asymptotic 95 % confidence interval
</p>
<p>Lower bound Upper bound
</p>
<p>,945 ,009 ,000 ,928 ,961
</p>
<p>The test result variable(s): score has at least one tie between the positive actual state group and the 
</p>
<p>negative actual state group. Statistics may be biased
aUnder the nonparametric assumption
bNull hypothesis: true area = 0.5
</p>
<p>Subsequently the same procedure is followed for data file 2.
</p>
<p>1,0
</p>
<p>ROC Curve
</p>
<p>1 - Specificity
</p>
<p>S
e
n
</p>
<p>s
it
</p>
<p>iv
it
</p>
<p>y
</p>
<p>0,8
</p>
<p>0,6
</p>
<p>0,4
</p>
<p>0,2
</p>
<p>0,0
</p>
<p>0,0 0,2 0,4 0,6 0,8 1,0
</p>
<p>Diagonal segments are produced by ties.  
</p>
<p>Area under the curve
</p>
<p>Test result variable(s): score
</p>
<p>Area Std. errora Asymptotic Sig.b
Asymptotic 95 % confidence interval
</p>
<p>Lower bound Upper bound
</p>
<p>,974 ,005 ,000 ,965 ,983
</p>
<p>The test result variable(s): score has at least one tie between the positive actual state group and the 
</p>
<p>negative actual state group. Statistics may be biased
aUnder the nonparametric assumption
bNull hypothesis: true area = 0.5
</p>
<p>41 Performance Evaluation of Novel Diagnostic Tests (650 and 588 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>251
</p>
<p>The Area under curve of data file 2 is larger than that of data file 1. The test 2 
</p>
<p>seems to perform better. The z-test can again be used to test for significance of 
</p>
<p>difference.
</p>
<p> 
</p>
<p>z
</p>
<p>p
</p>
<p>= &minus;( ) &radic; +( )=
= &lt;
</p>
<p>0 974 0 945 0 009 0 005 2 90
</p>
<p>0 01
</p>
<p>2 2. . / . . .
</p>
<p>. .
 
</p>
<p> Conclusion
</p>
<p>Both logistic regression with the presence of disease as outcome and test scores of 
</p>
<p>as predictor and c-statistics can be used for comparing the performance of qualita-
</p>
<p>tive diagnostic tests. However, c-statistics may perform less well with very large 
</p>
<p>areas under the curve, and it assesses relative risks while in practice absolute risk 
</p>
<p>levels may be more important
</p>
<p> Note
</p>
<p>More background, theoretical and mathematical information of logistic regression 
</p>
<p>and c-statistics is in Machine learning in medicine part two, Chap. 6, pp 45&ndash;52, 
</p>
<p>Logistic regression for assessment of novel diagnostic tests against controls, 
</p>
<p>Springer Heidelberg Germany, 2013, from the same authors.
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>253&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_42
</p>
<p>    Chapter 42   
</p>
<p> Quantile-Quantile Plots, a Good Start 
for Looking at Your Medical Data (50 
Cholesterol Measurements and 58 Patients) 
</p>
<p>                       General Purpose 
</p>
<p> A good place to start looking at your data before analysis is a data plot, e.g., a scatter plot 
</p>
<p>or histogram. It can help you decide whether the data are normal (bell shape, Gaussian), 
</p>
<p>and give you a notion of outlier data and skewness. Another approach is using a normal-
</p>
<p>ity test like the chi-square goodness of fi t, the Shapiro-Wilkens, or the Kolmogorov 
</p>
<p>Smirnov tests (Cleophas, Zwinderman, Chap. 42, pp 469&ndash;478, Testing clinical trials for 
</p>
<p>randomness, in: Statistics applied to clinical studies 5th edition, Springer Heidelberg 
</p>
<p>Germany, 2012), but these tests often have little power, and, therefore, do not adequately 
</p>
<p>identify departures from normality. This chapter is to assess the performance of another 
</p>
<p>and probably better method, the Q-Q (quantile- quantile) plot.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Are Q-Q plots of medical records capable of identifying normality and departures 
</p>
<p>from normality. Random samples of hdl cholesterol and ages are used for 
</p>
<p>examples.  
</p>
<p>    Q-Q Plots for Assessing Departures from Normality 
</p>
<p> hdl cholesterol values (mmol/l) 
</p>
<p> 3,80 
</p>
<p> 4,20 
</p>
<p> 4,27 
</p>
<p> 3,70 
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>254
</p>
<p> hdl cholesterol values (mmol/l) 
</p>
<p> 3,76 
</p>
<p> 4,11 
</p>
<p> 4,24 
</p>
<p> 4,20 
</p>
<p> 4,24 
</p>
<p> 3,63 
</p>
<p>   The above table gives the fi rst 10 values of a 50 value data fi le of hdl cholesterol 
</p>
<p>measurements. The entire fi le is in the SPSS fi le entitled &ldquo;q-q plot&rdquo;, and is available 
</p>
<p>on the internet at extras.springer.com. SPSS statistical software is applied. Start by 
</p>
<p>opening the data fi le in SPSS.
</p>
<p>  Command: 
</p>
<p>  click Graphs....Legacy Dialogs....Histogram....Variable: enter hdlcholesterol....click 
</p>
<p>OK.    
</p>
<p> A histogram with individual hdl cholesterol values on the x-axis and &ldquo;how often&rdquo; 
</p>
<p>on the y-axis is given in the output sheet: 50 hdl cholesterol values are classifi ed in 
</p>
<p>percentages (%) or quantiles (= frequencies = numbers of observations/50 here). 
</p>
<p>E.g., one value is between 2,5 and 3,0, two values are between 3,0 and 3,5, etc. The 
</p>
<p>pattern tends to be somewhat bell shape, but there is obvious outlier frequencies 
</p>
<p>close to 3 mmol/l and close to 4 mmol/l. Also some skewness to the right is observed, 
</p>
<p>and the values around 4 mmol/l look a little bit like Manhattan rather than Gaussian. 
</p>
<p>  
</p>
<p>12,5
</p>
<p>Mean = 4,01
</p>
<p>Std. Dev.= ,33
</p>
<p>N = 50
</p>
<p>10,0
</p>
<p>7,5
</p>
<p>5,0
</p>
<p>2,5
</p>
<p>0,0
</p>
<p>2,50 3,00 3,50
</p>
<p>mmol/l
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>e
n
</p>
<p>c
y
</p>
<p>4,00 4,50 5,00
</p>
<p>  
</p>
<p>42 Quantile-Quantile Plots, a Good Start for Looking at Your Medical Data&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>255
</p>
<p>    A Q-Q plot (quantile-quantile plot) can be helpful do decide what type of data we 
</p>
<p>have here. First, the best fi t normal curve is construed, e.g., based on the mean and 
</p>
<p>standard deviation of the data. A graph of it is easy to produce in SPSS.
</p>
<p>  Command: 
</p>
<p>  click Graphs....Legacy Dialogs....Histogram....Variable: enter hdlcholesterol ....
</p>
<p>mark: Display normal curve....click OK.    
</p>
<p>  
</p>
<p>12,5
</p>
<p>Mean = 4,01
</p>
<p>Std. Dev. = ,33
</p>
<p>N = 50
</p>
<p>10,0
</p>
<p>7,5
</p>
<p>5,0
</p>
<p>2,5
</p>
<p>0,0
2,50 3,00 3,50 4,00 4,50
</p>
<p>mmol/l
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>e
n
</p>
<p>c
y
</p>
<p>5,00
</p>
<p>  
</p>
<p>    SPSS uses the curve to calculate the values for a Q-Q plot.
</p>
<p>  Command: 
</p>
<p>  Analyze....Descriptive Statistics....Q-Q plots....Variables: enter hdlcholesterol.... 
</p>
<p>click OK.    
</p>
<p> The underneath plot is construed of the observed x-values versus the same 
</p>
<p>x- values taken from the above best fi t normal curve. If our data perfectly matched 
</p>
<p>the best fi t Gaussian curve, then all of the x-values would be on the 45&deg; diagonal 
</p>
<p>line. However, we have outliers. The x-value close to 3 mmol/l is considerably left 
</p>
<p>from the diagonal, and thus smaller than expected. The value close to 4 mmol/l is 
</p>
<p>obviously on the right side of the diagonal, and thus larger than expected. 
</p>
<p>Nonetheless, The remainder of the observed values vary well fi t the diagonal, and it 
</p>
<p>seems adequate to conclude that normal statistical test for analysis of these data will 
</p>
<p>be appropriate. 
</p>
<p>Q-Q Plots for Assessing Departures from Normality</p>
<p/>
</div>
<div class="page"><p/>
<p>256
</p>
<p>  
</p>
<p>5,0
</p>
<p>4,5
</p>
<p>4,0
</p>
<p>3,5
</p>
<p>3,0
</p>
<p>2,5 3,0 3,5
</p>
<p>Observed Value
</p>
<p>Normal Q-Q Plot of mmol/l
</p>
<p>E
x
p
</p>
<p>e
c
te
</p>
<p>d
 N
</p>
<p>o
rm
</p>
<p>a
l 
</p>
<p>V
a
lu
</p>
<p>e
</p>
<p>4,0 4,5 5,0
</p>
<p>  
</p>
<p>        Q-Q Plots as Diagnostics for Fitting Data to Normal 
</p>
<p>(and Other Theoretical) Distributions 
</p>
<p> Age (years) 
</p>
<p> 85,00 
</p>
<p> 89,00 
</p>
<p> 50,00 
</p>
<p> 63,00 
</p>
<p> 76,00 
</p>
<p> 57,00 
</p>
<p> 86,00 
</p>
<p> 56,00 
</p>
<p> 76,00 
</p>
<p> 66,00 
</p>
<p>   The above table gives the fi rst 10 values of a 58 value data fi le of patients with 
   different ages. The entire fi le is in the SPSS fi le entitled &ldquo;q-q plot&rdquo;, and is available 
</p>
<p>42 Quantile-Quantile Plots, a Good Start for Looking at Your Medical Data&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>257
</p>
<p>on the internet at extras.springer.com. SPSS statistical software is applied. Start by 
</p>
<p>opening the data fi le in SPSS.
</p>
<p>  Command: 
</p>
<p>  Analyze....Descriptive Statistics....Q-Q plots....Variables: enter age....click OK.    
</p>
<p>  
</p>
<p>120
</p>
<p>100
</p>
<p>80
</p>
<p>60
</p>
<p>40
</p>
<p>20
</p>
<p>20 40 60
</p>
<p>Observed Value
</p>
<p>Normal Q-Q Plot of years
</p>
<p>E
x
p
</p>
<p>e
c
te
</p>
<p>d
 N
</p>
<p>o
rm
</p>
<p>a
l 
</p>
<p>V
a
lu
</p>
<p>e
</p>
<p>80 100 120
</p>
<p>  
</p>
<p>    In the output sheets is the above graph. It shows a pattern with the left end below 
</p>
<p>the diagonal line and the right end above it. Also the overall pattern seems to be 
</p>
<p>somewhat undulating with the initially an increasing slope, and then a decreasing 
</p>
<p>slope. The possible interpretations of these patterns are the following.
</p>
<p>    1.    Left end below and right end above the diagonal may indicate a bells shape with 
</p>
<p>long tails (overdispersion).   
</p>
<p>   2.    In contrast, left end above and right end below indicates short tails.   
</p>
<p>   3.    An increasing slope from left to right may indicate skewness to the right.   
</p>
<p>   4.    In contrast, a decreasing slope suggests skewness to the left.   
</p>
<p>   5.    The few cases with largest departures from the diagonal may of course also be 
</p>
<p>interpreted as outliers.     
</p>
<p> The above Q-Q plot can hardly be assumed to indicate Gaussian data. The 
</p>
<p> histogram confi rms this.
</p>
<p>Q-Q Plots as Diagnostics for Fitting Data to Normal (and Other Theoretical)&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>258
</p>
<p>  Command: 
</p>
<p>  click Graphs....Legacy Dialogs....Histograms....Variables: enter age....click OK.    
</p>
<p>  
</p>
<p>12 Mean = 71,24
Std. Dev. = 15, 96
N = 58
</p>
<p>10
</p>
<p>8
</p>
<p>6
</p>
<p>4
</p>
<p>2
</p>
<p>0
</p>
<p>40,00 50,00 60,00 70,00
</p>
<p>years
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>e
n
</p>
<p>c
y
</p>
<p>80,00 90,00 100,00
</p>
<p>  
</p>
<p>    The histogram given in the output sheets seems to confi rm that this is so. The 
</p>
<p>Q-Q plot method is somewhat subjective, but an excellent alternative to underpow-
</p>
<p>ered goodness of fi t tests, and provides better information regarding normality than 
</p>
<p>simple data plots or histograms do, because each datum assessed against its best fi t 
</p>
<p>normal distribution counterpart. We should add that SPSS and other software also 
</p>
<p>offer the construction of Q-Q plots using other than normal distributions.  
</p>
<p>    Conclusion 
</p>
<p> Q-Q plots are adequate assess whether your data have a Gaussian-like pattern. Non- 
</p>
<p>Gaussian patterns and outliers are visualized, and often an interpretation can be 
</p>
<p>given of them. The Q-Q plot method is similar to the less popular P-P (probability- 
</p>
<p>probability) plot method, which has cumulative probabilities (= areas under curve 
</p>
<p>left from the x-value), instead of the x-values on the x-axis and their expected 
</p>
<p> counterparts on the y-axis. They are a little bit harder to understand.  
</p>
<p>42 Quantile-Quantile Plots, a Good Start for Looking at Your Medical Data&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>259
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of frequency distribu-
</p>
<p>tions and goodness of fi t testing is in the Chap. 42, pp 469&ndash;478, Testing clinical 
</p>
<p>trials for randomness, in: Statistics applied to clinical studies 5th edition, Springer 
</p>
<p>Heidelberg Germany, 2012, from the same authors as the current work.    
</p>
<p> Note</p>
<p/>
</div>
<div class="page"><p/>
<p>261&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_43
</p>
<p>    Chapter 43   
</p>
<p> Rate Analysis of Medical Data Better than 
</p>
<p>Risk Analysis (52 Patients) 
</p>
<p>                       General Purpose 
</p>
<p> For the assessment of medical treatments clinical event analysis with logistic regres-
</p>
<p>sion is often performed. Treatment modalities are used as predictor and the odds of 
</p>
<p>the event as outcome. However, instead of the odds of event, counted rates of events 
</p>
<p>can be computed and statistically tested. This may produce better sensitivity of test-
</p>
<p>ing, because their standard errors are smaller.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Does rate analysis of medical events provide better sensitivity of testing than tradi-
</p>
<p>tional risk analysis.  
</p>
<p>    Example 
</p>
<p> We will use an example also used in the Chap. 10 of SPSS for starters part two, 
</p>
<p>pp 43&ndash;48, Poisson regression, Springer Heidelberg Germany, 2012, from the same 
</p>
<p>authors. In a parallel-group study of 52 patients the presence of torsade de pointes 
</p>
<p>was measured during two treatment modalities. </p>
<p/>
</div>
<div class="page"><p/>
<p>262
</p>
<p> treatment modality  Presence torsade de pointes 
</p>
<p> ,00  1,00 
</p>
<p> ,00  1,00 
</p>
<p> ,00  1,00 
</p>
<p> ,00  1,00 
</p>
<p> ,00  1,00 
</p>
<p> ,00  1,00 
</p>
<p> ,00  1,00 
</p>
<p> ,00  1,00 
</p>
<p> ,00  1,00 
</p>
<p> ,00  1,00 
</p>
<p>   The fi rst 10 patients are above. The entire data fi le is in extras.springer.com, and 
</p>
<p>is entitled &ldquo;rates&rdquo;. SPSS statistical software will be used for analysis. First, we will 
</p>
<p>perform a traditional binary logistic regression with torsade de pointes as outcome 
</p>
<p>and treatment modality as predictor.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Regression&hellip;.Binary Logistic&hellip;.Dependent: torsade&hellip;.. Covariates: 
</p>
<p>treatment&hellip;.OK.   
</p>
<p> Variables in the Equation 
</p>
<p> B  S.E.  Wald  df  Sid.  Exp(B) 
</p>
<p> Step 1 a   VAR00001  1,224  ,626  3,819  1  ,051  3,400 
</p>
<p> Constant  -,125  ,354  ,125  1  ,724  ,882 
</p>
<p>   a Variable(s) entered on step 1: VAR00001 
</p>
<p>    The above table shows that the treatment modality does not signifi cantly predict 
</p>
<p>the presence of torsades de pointes. The numbers of torsades in one group is not 
</p>
<p>signifi cantly different from the other group. 
</p>
<p> A rate analysis is performed subsequently.
</p>
<p>  Command: 
</p>
<p>  Generalized Linear Models &hellip;.mark Custom&hellip;.Distribution: Poisson &hellip;.Link 
</p>
<p>Function: Log&hellip;.Response: Dependent Variable: torsade&hellip;. Predictors: Main 
</p>
<p>Effect: treatment&hellip;..Estimation: mark Robust Tests&hellip;.OK.   
</p>
<p>43 Rate Analysis of Medical Data Better than Risk Analysis (52 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>263
</p>
<p> Parameter estimates 
</p>
<p> Parameter  B  Std. Error 
</p>
<p> 95 % Wald 
</p>
<p>confi dence 
</p>
<p>interval  Hypothesis test 
</p>
<p> Lower  Upper  Wald Chi- Square  df  Sig. 
</p>
<p> (Intercept)  -,288  ,1291  -,541  -,035  4,966  1  ,026 
</p>
<p> [VAR00001=,00]  -,470  ,2282  -.917  -,023  4,241  1  ,039 
</p>
<p> [VAR00001 = 1,00]  0 a  
</p>
<p> (Scale)  1 b  
</p>
<p>  Dependent Variable: torsade 
</p>
<p> Model: (Intercept), VAR00001 
</p>
<p>  a Set to zero because this parameter is redundant 
</p>
<p>  b Fixed at the displayed value 
</p>
<p>    The predictor treatment modality is now statistically signifi cant at p = 0.039. And 
</p>
<p>so, using the Poisson distribution in Generalized Linear Models, we found that 
</p>
<p>treatment one performed signifi cantly better in predicting numbers of torsades de 
</p>
<p>pointe than did treatment zero at 0.039. We will check with a 3-dimensional graph 
</p>
<p>of the data if this result is in agreement with the data as observed.
</p>
<p>  Command: 
</p>
<p>  Graphs&hellip;.Legacy Dialog&hellip;.3-D Bar: X-Axis mark: Groups of Cases, Z-Axis mark: 
</p>
<p>Groups of Cases&hellip;Defi ne 3-D Bar: X Category Axis: treatment, Z Category Axis: 
</p>
<p>torsade&hellip;.OK.    
</p>
<p>    
</p>
<p> Example</p>
<p/>
</div>
<div class="page"><p/>
<p>264
</p>
<p>    The above graph shows that in the 0-treatment (placebo) group the number of 
</p>
<p>patients with torsades de pointe is virtually equal to that of the patients without. 
</p>
<p>However, in the 1-treatment group it is smaller. The treatment seems to be 
</p>
<p>effi cacious.  
</p>
<p>    Conclusion 
</p>
<p> Rate analysis using Poisson regression is different from logistic regression, because 
</p>
<p>it uses a log transformed dependent variable. For the analysis of rates Poisson 
</p>
<p>regression is very sensitive and, thus, better than standard logistic regression.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of rate analysis is 
</p>
<p>given in Chap. 10 of SPSS for starters part two, pp 43&ndash;48, Poisson regression, 
</p>
<p>Springer Heidelberg Germany, 2012, from the same authors.    
</p>
<p>43 Rate Analysis of Medical Data Better than Risk Analysis (52 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>265&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_44
</p>
<p>    Chapter 44   
</p>
<p> Trend Tests Will Be Statistically Signifi cant 
if Traditional Tests Are Not 
(30 and 106 Patients) 
</p>
<p>                       General Purpose 
</p>
<p> Incremental dosages of medicines usually cause incremental treatment effi cacies. 
</p>
<p>This chapter is to assess whether trend tests are more sensitive than traditional 
</p>
<p>ANOVAs for continuous outcome data (analyses of variance) and chi-square tests 
</p>
<p>for binary outcome data to demonstrate the incremental effi cacies.  
</p>
<p>    Specifi c Scientifi c Questions 
</p>
<p> In patients with hypertension do incremental treatment dosages cause incremental 
</p>
<p>benefi cial effect on blood pressure? We will use the examples previously used in the 
</p>
<p>Chaps. 9 and 12 of SPSS for starters part one, pp 33&ndash;34, and 43&ndash;46, entitled &ldquo;Trend 
</p>
<p>test for continuous data&rdquo; and &ldquo;trend tests for binary data&rdquo;, Springer Heidelberg 
</p>
<p>Germany, 2010, from the same authors.  
</p>
<p>    Example 1 
</p>
<p> In a parallel group study of 30 patients with hypertension 3 incremental antihyper-
</p>
<p>tensive treatment dosages are assessed. The fi rst 13 patients of the data fi le is given 
</p>
<p>underneath. The entire data fi le is in extras.springer.com, and is entitled &ldquo;trend.sav&rdquo;. 
</p>
<p> Variable 
</p>
<p> 1  2 
</p>
<p> 1,00  113,00 
</p>
<p> 1,00  131,00 
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>266
</p>
<p> Variable 
</p>
<p> 1,00  112,00 
</p>
<p> 1,00  132,00 
</p>
<p> 1,00  114,00 
</p>
<p> 1,00  130,00 
</p>
<p> 1,00  115,00 
</p>
<p> 1,00  129,00 
</p>
<p> 1,00  122,00 
</p>
<p> 2,00  118,00 
</p>
<p> 2,00  109,00 
</p>
<p> 2,00  127,00 
</p>
<p> 2,00  110,00 
</p>
<p>  Var 1 = treatment dosage (Var = variable) 
</p>
<p> Var 2 = treatment response (mean blood pressure after treatment) 
</p>
<p>    We will fi rst perform a one-way ANOVA (see also Chap. 8, SPSS for starters part 
</p>
<p>one, entitled &ldquo;One way ANOVA, Kruskall-Wallis&rdquo;, pp 29&ndash;31, Springer Heidelberg 
</p>
<p>Germany, 2012, from the same authors) to see, if there are any signifi cant differ-
</p>
<p>ences in the data. If not, we will perform a trend test using simple linear 
</p>
<p>regression.
</p>
<p>  Command: 
</p>
<p>  Analyze....Compare Means....One-way ANOVA....dependent list: mean blood 
</p>
<p>pressure after treatment - factor: treatment dosage....OK   
</p>
<p> ANOVA 
</p>
<p> VAR00002 
</p>
<p> Sum of squares  df  Mean square  F  Sig. 
</p>
<p> Between groups  246,667  2  123,333  2,035  ,150 
</p>
<p> Within groups  1636,000  27  60,593 
</p>
<p> Total  1882,667  29 
</p>
<p>   The output table shows that there is no signifi cant difference in effi cacy between 
</p>
<p>the treatment dosages, and so, sadly, this is a negative study. However, a trend test 
</p>
<p>having just 1 degree of freedom has more sensitivity than a usual one-way ANOVA, 
</p>
<p>and it could, therefore, be statistically signifi cant even so.
</p>
<p>  Command: 
</p>
<p>  Analyze....regression....linear....dependent = mean blood pressure after treatment.... 
</p>
<p>independent = treatment dosage....OK   
</p>
<p>44 Trend Tests Will Be Statistically Signifi cant if Traditional Tests Are Not&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>267
</p>
<p> ANOVA b  
</p>
<p> Model  Sum of squares  df  Mean square  F  Sig. 
</p>
<p> 1 Regression  245,000  1  245,000  4,189  ,050 a  
</p>
<p> Residual  1637,667  28  58,488 
</p>
<p> Total  1882,667  29 
</p>
<p>   a Predictors: (Constant), VAR00001 
</p>
<p>  b Dependent Variable: VAR00002 
</p>
<p>    The above output table shows that treatment dosage is a signifi cant predictor of 
</p>
<p>treatment response wit a p-value of 0.050. There is, thus, a signifi cantly incremental 
</p>
<p>response with incremental dosages.  
</p>
<p>    Example 2 
</p>
<p> In a parallel group study of 106 patients with hypertension 3 incremental antihyper-
</p>
<p>tensive treatment dosages are assessed. The fi rst 13 patients of the data fi le is given 
</p>
<p>underneath. The entire data fi le is in extras.springer.com, and is entitled &ldquo;trend.sav&rdquo;.
</p>
<p> responder (1 = yes, 0 = no)  Treatment (1 = low, 2 = medium, 3 = high dosage) 
</p>
<p> 1,00  1,00 
</p>
<p> 1,00  1,00 
</p>
<p> 1,00  1,00 
</p>
<p> 1,00  1,00 
</p>
<p> 1,00  1,00 
</p>
<p> 1,00  1,00 
</p>
<p> 1,00  1,00 
</p>
<p> 1,00  1,00 
</p>
<p> 1,00  1,00 
</p>
<p> 1,00  1,00 
</p>
<p> 1,00  2,00 
</p>
<p> 1,00  2,00 
</p>
<p> 1,00  2,00 
</p>
<p>   Command: 
</p>
<p>  Analyze....Descriptive Statistics....Crosstabs....Row(s): enter responders.... 
</p>
<p>Column(s): enter treatment....click Cell(s)....Counts: mark Observed..... Percentage: 
</p>
<p>mark Columns....click continue....click OK    
</p>
<p> The underneath contingency table shows that with incremental dosages the % of 
</p>
<p>responders incrementally rises from 40 % to 51.3 % and then to 64.3 %.
</p>
<p>Example 2</p>
<p/>
</div>
<div class="page"><p/>
<p>268
</p>
<p> Treatment 
</p>
<p> Total  1,00  2,00  3,00 
</p>
<p> Responder  ,00  Count  15  19  15  49 
</p>
<p> % within treatment  60,0 %  48,7 %  35,7 %  46,2 % 
</p>
<p> 1,00  Count  10  20  27  57 
</p>
<p> % within treatment  40,0 %  51,3 %  64,3 %  53,8 % 
</p>
<p> Total  Count  25  39  42  106 
</p>
<p> % within treatment  100,0 %  100,0 %  100,0 %  100,0 % 
</p>
<p>   Subsequently, a chi-square test will be performed to assess whether the cells are 
</p>
<p>signifi cantly different from one another.
</p>
<p>  Command: 
</p>
<p>  Analyze....Descriptive Statistics....Crosstabs.... Row(s): enter responders.... 
</p>
<p>Column(s): enter treatment....click Statistics....Chi-square....OK   
</p>
<p> Chi-square tests 
</p>
<p> Value  df  Asy mp. Sig. (2-sided) 
</p>
<p> Pearson chi-square  3,872 a   2  ,144 
</p>
<p> Likelihood ratio  3,905  2  ,142 
</p>
<p> Linear-by-linear association  3,829  1  ,050 
</p>
<p> N of valid cases  106 
</p>
<p>   a 0 cells (,0 %) have expected court less than 5. The minimum expected count is 11,56 
</p>
<p>    The output table shows that the Pearson chi-square value for multiple groups 
</p>
<p>testing is not signifi cant with a value of 3.872 and a p-value of 0.144, and we need 
</p>
<p>to conclude that there is no signifi cant difference between the cells. Subsequently, a 
</p>
<p>chi-square test for trends is required for that purpose. Actually, the &ldquo;linear-by-linear 
</p>
<p>association&rdquo; from the same table is appropriate. It has approximately the same chi- 
</p>
<p>square value, but it has only 1 degree of freedom, and, therefore it reaches statistical 
</p>
<p>signifi cance with a p-value of 0.050. There is, thus, a signifi cant incremental trend 
</p>
<p>of responding with incremental dosages. As an alternative the trend in this example 
</p>
<p>can also be tested using logistic regression with responding as outcome variable and 
</p>
<p>treatment as independent variable.
</p>
<p>  Command: 
</p>
<p>  Analyze....Regression....Binary Logistic Regression....Dependent: enter responder....
</p>
<p>Covariates: enter treatment....click OK   
</p>
<p> Variables in the equation 
</p>
<p> B  S.E.  Wald  df  Sig.  Exp(B) 
</p>
<p> Step 1 a   Treatment  ,500  ,257  3,783  1  ,052  1,649 
</p>
<p> Constant  -,925  ,587  2,489  1  ,115  ,396 
</p>
<p>   a Variable(s) entered on step 1: treatment. 
</p>
<p>44 Trend Tests Will Be Statistically Signifi cant if Traditional Tests Are Not&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>269
</p>
<p>    The output sheet shows that the p-value of the logistic model is virtually identical 
</p>
<p>to the p-value of chi-square test for trends, 0.052 and 0.050.  
</p>
<p>    Conclusion 
</p>
<p> The examples in this chapter show that both with continuous and binary outcome 
</p>
<p>variables trend tests are more sensitive to demonstrate signifi cant effects in dose 
</p>
<p>response studies than traditional statistical tests.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of trend tests are given 
</p>
<p>in the Chap. 27, Trend-testing, pp 313&ndash;318, in: Statistics applied to clinical studies 
</p>
<p>5th edition, Springer Heidelberg Germany, 2012, from the same authors.    
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>271&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_45
</p>
<p>    Chapter 45   
</p>
<p> Doubly Multivariate Analysis of Variance 
</p>
<p>for Multiple Observations from Multiple 
</p>
<p>Outcome Variables (16 Patients) 
</p>
<p>                       General Purpose 
</p>
<p> One way analysis of variance (ANOVA) is for analysis of studies with multiple 
</p>
<p>unpaired observations (i.e. 1 subject is observed once) and a single outcome  variable 
</p>
<p>(see Chap. 8, One way anova and Kruskall-Wallis, pp 29&ndash;31, in: SPSS for starters 
</p>
<p>part one, Springer Heidelberg Germany, 2010, from the same authors). 
</p>
<p> Repeated measures ANOVA is for studies with multiple paired observations (i.e. 
</p>
<p>more than a single observation per subject) and also with a single outcome variable 
</p>
<p>(see Chap. 6, Repeated measures anova, pp 21&ndash;24, in: SPSS for starters part one, 
</p>
<p>Springer Heidelberg Germany, 2010, from the same authors). 
</p>
<p> Multivariate ANOVA is for studies with multiple unpaired observations and 
</p>
<p>more than a single outcome variable (see Chap. 4, Multivariate anova, pp 13&ndash;20, in: 
</p>
<p>SPSS for starters part two, Springer Heidelberg Germany, 2012, from the same 
</p>
<p>authors). 
</p>
<p> Finally, doubly multivariate ANOVA is for studies with multiple paired observa-
</p>
<p>tions and more than a single outcome variable. 
</p>
<p> An example of the latter is given in the SPSS tutorial case studies: in a diet study 
</p>
<p>of overweight patients the triglyceride and weight values were the outcome  variables 
</p>
<p>and they were measured repeatedly during several months of follow up.  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> Can doubly multivariate analysis be used to simultaneously assess the effects of 
</p>
<p>three different sleeping pills on two outcome variables, (1) hours of sleep and (2) 
</p>
<p>morning body temperatures (in patients with sleep deprivation morning body 
</p>
<p> temperature is higher than in those without sleep deprivation).  </p>
<p/>
</div>
<div class="page"><p/>
<p>272
</p>
<p>    Example 
</p>
<p> In 16 patients a three period crossover study of three sleeping pills (treatment levels) 
</p>
<p>were studied. The underneath table give the data of the fi rst 8 patients. The entire 
</p>
<p>data fi le is entitled &ldquo;doubly.sav&rdquo;, and is in extras.springer.com. Two outcome vari-
</p>
<p>ables are measured at three levels each. This study would qualify for a doubly mul-
</p>
<p>tivariate analysis, because we have multiple paired outcomes and multiple measures 
</p>
<p>of each of the outcomes. 
</p>
<p> hours  age  gen  temp 
</p>
<p> a  b  c  a  b  c 
</p>
<p> 6,10  6,80  5,20  55,00  0,00  35,90  35,30  36,80 
</p>
<p> 7,00  7,00  7,90  65,00  0,00  37,10  37,80  37,00 
</p>
<p> 8,20  9,00  3,90  74,00  0,00  38,30  34,00  39,10 
</p>
<p> 7,60  7,80  4,70  56,00  1,00  37,50  34,60  37,70 
</p>
<p> 6,50  6,60  5,30  44,00  1,00  36,40  35,30  36,70 
</p>
<p> 8,40  8,00  5,40  49,00  1,00  38,30  35,50  38,00 
</p>
<p> 6,90  7,30  4,20  53,00  0,00  37,00  34,10  37,40 
</p>
<p> 6,70  7,00  6,10  76,00  0,00  36,80  36,10  36,90 
</p>
<p>  hours = hours of sleep on sleeping pill 
</p>
<p> a, b, c = different sleeping pills (levels of treatment) 
</p>
<p> age = patient age 
</p>
<p> gen = gender 
</p>
<p> temp = different morning body temperatures on sleeping pill 
</p>
<p>    SPSS statistical software will be used for data analysis. We will start by opening 
</p>
<p>the data fi le in SPSS.
</p>
<p>  Then Command: 
</p>
<p>  Analyze....General Linear Models....Repeated Measures....Within-Subject Factor 
</p>
<p>Name: type treatment....Number of Levels: type 3....click Add....Measure Name: 
</p>
<p>type hours....click Add....Measure Name: type temp....click Add....click Defi ne....
</p>
<p>Within-Subjects Variables(treatment): enter hours a, b, c, and temp a, b, c....
</p>
<p>Between-Subjects Factor(s): enter gender....click Contrast....Change Contrast....
</p>
<p>Contrast....select Repeated....click Change....click Continue....click Plots....
</p>
<p>Horizontal Axis: enter treatment....Separate Lines: enter gender....click Add....click 
</p>
<p>Continue....click Options....Display Means for: enter gender*treatment....mark 
</p>
<p>Estimates of effect size....mark SSCP matrices....click Continue....click OK.    
</p>
<p>45 Doubly Multivariate Analysis of Variance for Multiple Observations&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>273
</p>
<p> The underneath table is in the output sheets.
</p>
<p> Multivariate tests b  
</p>
<p> Effect  Value  F 
</p>
<p> Hypothesis 
</p>
<p>df 
</p>
<p> Error 
</p>
<p>df  Sig. 
</p>
<p> Partial 
</p>
<p>Eta 
</p>
<p>Squared 
</p>
<p> Between 
</p>
<p>subjects 
</p>
<p> Intercept  Pillai&rsquo;s 
</p>
<p>Trace 
</p>
<p> 1,000  3,271 E6  2,000  13,000  ,000  1,000 
</p>
<p> Wilks&rsquo; 
</p>
<p>Lambda 
</p>
<p> ,000  3,271 E6  2,000  13,000  ,000  1,000 
</p>
<p> Hotelling&rsquo;s 
</p>
<p>Trace 
</p>
<p> 503211,785  3,271 E6  2,000  13,000  ,000  1,000 
</p>
<p> Roy&rsquo;s 
</p>
<p>Largest 
</p>
<p>Root 
</p>
<p> 503211,785  3,271 E6  2,000  13,000  ,000  1,000 
</p>
<p> Gender  Pillai&rsquo;s 
</p>
<p>Trace 
</p>
<p> ,197  1,595 a   2,000  13,000  ,240  ,197 
</p>
<p> Wilks&rsquo; 
</p>
<p>Lambda 
</p>
<p> ,803  1,595 a   2,000  13,000  ,240  ,197 
</p>
<p> Hotelling&rsquo;s 
</p>
<p>Trace 
</p>
<p> ,245  1,595 a   2,000  13,000  ,240  ,197 
</p>
<p> Roy&rsquo;s 
</p>
<p>Largest 
</p>
<p>Root 
</p>
<p> ,245  1,595 a   2,000  13,000  ,240  ,197 
</p>
<p> Within 
</p>
<p>subjects 
</p>
<p> Treatment  Pillai&rsquo;s 
</p>
<p>Trace 
</p>
<p> ,562  3,525 a   4,000  11,000  ,044  ,562 
</p>
<p> Wilks&rsquo; 
</p>
<p>Lambda 
</p>
<p> ,438  3,525 a   4,000  11,000  ,044  ,562 
</p>
<p> Hotelling&rsquo;s 
</p>
<p>Trace 
</p>
<p> 1,282  3,525 a   4,000  11,000  ,044  ,562 
</p>
<p> Roy&rsquo;s 
</p>
<p>Largest 
</p>
<p>Root 
</p>
<p> 1,282  3,525 a   4,000  11,000  ,044  ,562 
</p>
<p> Treatment 
</p>
<p>* gender 
</p>
<p> Pillai&rsquo;s 
</p>
<p>Trace 
</p>
<p> ,762  8,822 a   4,000  11,000  ,002  ,762 
</p>
<p> Wilks&rsquo; 
</p>
<p>Lambda 
</p>
<p> ,238  8,822 a   4,000  11,000  ,002  ,762 
</p>
<p> Hotelling&rsquo;s 
</p>
<p>Trace 
</p>
<p> 3,208  8,822 a   4,000  11,000  ,002  ,762 
</p>
<p> Roy&rsquo;s 
</p>
<p>Largest 
</p>
<p>Root 
</p>
<p> 3,208  8,822 a   4,000  11,000  ,002  ,762 
</p>
<p>   a Exactstatistic 
</p>
<p>  b Design: Intercept + gender 
</p>
<p> Within Subjects Design: treatment 
</p>
<p>    Doubly multivariate analysis has multiple paired outcomes and multiple  measures 
</p>
<p>of these outcomes. For analysis of such data both between and within subjects tests 
</p>
<p>are performed. We are mostly interested in the within subject effects of the  treatment 
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>274
</p>
<p>levels, but the above table starts by showing the not so interesting gender effect on 
</p>
<p>hours of sleep and morning temperatures. They are not signifi cantly different 
</p>
<p>between the genders. More important is the treatment effects. The hours of sleep 
</p>
<p>and the morning temperature are signifi cantly different between the different 
</p>
<p> treatment levels at p = 0.044. Also these signifi cant effects are different between 
</p>
<p>males and females at p = 0.002.
</p>
<p> Tests of within-subjects contrasts 
</p>
<p> Source  Measure  Treatment 
</p>
<p> Type III 
</p>
<p>sum of 
</p>
<p>squares  df 
</p>
<p> Mean 
</p>
<p>square  F  Sia. 
</p>
<p> Partial 
</p>
<p>eta 
</p>
<p>squared 
</p>
<p> Treatment  Hours  Level 1 vs. 
</p>
<p>Level 2 
</p>
<p> ,523  1  ,523  6,215  ,026  ,307 
</p>
<p> Level 2 w. 
</p>
<p>Level 3 
</p>
<p> 62,833  1  62,833  16,712  ,001  ,544 
</p>
<p> Temp  Level 1 vs. 
</p>
<p>Level 2 
</p>
<p> 49,323  1  49,323  15,788  ,001  ,530 
</p>
<p> Level 2 vs. 
</p>
<p>Level 3 
</p>
<p> 62,424  1  62,424  16,912  ,001  ,547 
</p>
<p> Treatment* 
</p>
<p>gender 
</p>
<p> Hours  Level 1 vs. 
</p>
<p>Level 2 
</p>
<p> ,963  1  ,963  11,447  ,004  ,450 
</p>
<p> Level 2 vs. 
</p>
<p>Level 3 
</p>
<p> ,113  1  ,113  ,030  ,865  ,002 
</p>
<p> Temp  Level 1 vs. 
</p>
<p>Level 2 
</p>
<p> ,963  1  ,963  ,308  ,588  ,022 
</p>
<p> Level 2 w. 
</p>
<p>Level 3 
</p>
<p> ,054  1  ,054  ,015  ,905  ,001 
</p>
<p> Error(treatment)  Hours  Level 1 vs. 
</p>
<p>Level 2 
</p>
<p> 1,177  14  ,084 
</p>
<p> Level 2 vs. 
</p>
<p>Level 3 
</p>
<p> 52,637  14  3,760 
</p>
<p> Temp  Level 1 w. 
</p>
<p>Level 2 
</p>
<p> 43,737  14  3,124 
</p>
<p> Level 2 vs. 
</p>
<p>Level 3 
</p>
<p> 51,676  14  3,691 
</p>
<p>   The above table shows, whether differences between levels of treatment were 
</p>
<p>signifi cantly different from one another by comparison with the subsequent levels 
</p>
<p>(contrast tests). The effects of treatment levels 1 versus (vs) 2 on hours of sleep were 
</p>
<p>different at p = 0.026, levels 2 vs 3 at p = 0.001. The effects of treatments levels 1 vs 
</p>
<p>2 on morning temperatures were different at p = 0.001, levels 2 vs 3 on morning 
</p>
<p>temperatures were also different at p = 0.001. The effects on hours of sleep of treat-
</p>
<p>ment levels 1 vs 2 accounted for the differences in gender remained very signifi cant 
</p>
<p>at p = 0.004.
</p>
<p>45 Doubly Multivariate Analysis of Variance for Multiple Observations&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>275
</p>
<p> Gender * treatment 
</p>
<p> Measure  Gender  Treatment  Mean  Std. Error 
</p>
<p> 95 % Confi dence interval 
</p>
<p> Lower bound  Upper bound 
</p>
<p> Hours  ,00  1  6,980  ,268  6,404  7,556 
</p>
<p> 2  7,420  ,274  6,833  8,007 
</p>
<p> 3  5,460  ,417  4,565  6,355 
</p>
<p> 1,00  1  7,350  ,347  6,607  8,093 
</p>
<p> 2  7,283  ,354  6,525  8,042 
</p>
<p> 3  5,150  ,539  3,994  6,306 
</p>
<p> Temp  ,00  1  37,020  ,284  36,411  37,629 
</p>
<p> 2  35,460  ,407  34,586  36,334 
</p>
<p> 3  37,440  ,277  36,845  38,035 
</p>
<p> 1,00  1  37,250  ,367  36,464  38,036 
</p>
<p> 2  35,183  ,526  34,055  36,311 
</p>
<p> 3  37,283  ,358  36,515  38,051 
</p>
<p>   The above table shows the mean hours of sleep and mean morning temperatures 
</p>
<p>for the different subsets of observations. Particularly, we observe the few hours of 
</p>
<p>sleep on treatment level 3, and the highest morning temperatures at the same level. 
</p>
<p>The treatment level 2, in contrast, causes pretty many hours of sleep and, at the same 
</p>
<p>time, the lowest morning temperatures (consistent with longer periods of sleep). 
</p>
<p>The underneath fi gures show the same. 
</p>
<p>   
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>276
</p>
<p>      
</p>
<p>        Conclusion 
</p>
<p> Doubly multivariate ANOVA is for studies with multiple paired observations and 
</p>
<p>more than a single outcome variable. For example, in a study with two or more dif-
</p>
<p>ferent outcome variables the outcome values are measured repeatedly during a 
</p>
<p>period of follow up or in a study with two or more outcome variables the outcome 
</p>
<p>values are measured at different levels, e.g., different treatment dosages or different 
</p>
<p>compounds. The multivariate approach prevents the type I errors from being 
</p>
<p>infl ated, because we only have one test and, so, the p-values need not be adjusted for 
</p>
<p>multiple testing (see Chap. 3, Multiple treatments,, pp 19&ndash;27, and Chap. 4, Multiple 
</p>
<p>endpoints, pp 29&ndash;36, both in: Machine learning in medicine part three, Springer 
</p>
<p>Heidelberg Germany, from the same authors). Also, the multivariate test battery 
</p>
<p>accounts for multiple effects simultaneously.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of data fi les with mul-
</p>
<p>tiple variables are the following. One way analysis of variance (anova) analysis of 
</p>
<p>studies with multiple unpaired observations (i.e. 1 subject is observed once) and a 
</p>
<p>45 Doubly Multivariate Analysis of Variance for Multiple Observations&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>277
</p>
<p>single outcome variable (see Chap. 8, One way anova and Kruskall-Wallis, 
</p>
<p>pp 29&ndash;31, in: SPSS for starters part one, Springer Heidelberg Germany, 2010, from 
</p>
<p>the same authors), repeated measures ANOVA for studies with multiple paired 
</p>
<p>observations (i.e. more than a single observation per subject) and also with a single 
</p>
<p>outcome variable (see Chap. 6, Repeated measures anova, pp 21&ndash;24, in: SPSS for 
</p>
<p>starters part one, Springer Heidelberg Germany, 2010, from the same authors), and 
</p>
<p>multivariate ANOVA is for studies with multiple unpaired observations and more 
</p>
<p>than a single outcome variable (see Chap. 4, Multivariate anova, pp 13&ndash;20, in: SPSS 
</p>
<p>for starters part two, Springer Heidelberg Germany, 2012, from the same authors). 
</p>
<p>The advantages of multivariate analyses as compared to univariate analyses are dis-
</p>
<p>cussed in the Chap. 3, Multiple treatments, pp 19&ndash;27, and the Chap. 4, Multiple 
</p>
<p>endpoints, pp 29&ndash;36, both in: Machine learning in medicine part three, Springer 
</p>
<p>Heidelberg Germany, 2013, from the same authors.    
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>279&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_46
</p>
<p>    Chapter 46   
</p>
<p> Probit Models for Estimating Effective 
Pharmacological Treatment Dosages (14 Tests) 
</p>
<p>                       General Purpose 
</p>
<p> Probit regression is, just like logistic regression, for estimating the effect of predic-
</p>
<p>tors on yes/no outcomes. If your predictor is multiple pharmacological treatment 
</p>
<p>dosages, then probit regression may be more convenient than logistic regression, 
</p>
<p>because your results will be reported in the form of response rates instead of odds 
</p>
<p>ratios. The dependent variable of the two methods log odds (otherwise called logit) 
</p>
<p>and log prob (otherwise called probit) are closely related to one another. Log prob 
</p>
<p>(probability), is the z-value corresponding to its area under the curve value of the 
</p>
<p>normal distribution. It can be shown that the log odds of responding &asymp; (/&radic;3) x log 
prob of responding (see Chap. 7, Machine learning in medicine part three, Probit 
</p>
<p>regression, pp 63&ndash;68, 2013, Springer Heidelberg Germany, from the same authors).  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> This chapter will assess whether probit regression is able to fi nd response rates of 
</p>
<p>different dosages of mosquito repellents.  
</p>
<p>    Example 
</p>
<p>    Simple Probit Regression 
</p>
<p> repellent nonchem  repellent chem  mosquitos gone  n mosquitos 
</p>
<p> 1  ,02  1000  18000 
</p>
<p> 1  ,03  1000  18500 
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>280
</p>
<p> repellent nonchem  repellent chem  mosquitos gone  n mosquitos 
</p>
<p> 1  ,03  3500  19500 
</p>
<p> 1  ,04  4500  18000 
</p>
<p> 1  ,07  9500  16500 
</p>
<p> 1  ,09  17000  22500 
</p>
<p> 1  ,10  20500  24000 
</p>
<p>   In 14 test sessions the effect measured as the numbers of mosquitos gone after 
</p>
<p>administration of different dosages of a chemical repellent was assessed. The fi rst 7 
</p>
<p>sessions are in the above table. The entire data fi le is entitled probit.sav, and is in 
</p>
<p>extras.springer.com. Start by opening the data fi le in SPSS statistical software.
</p>
<p>  Command: 
</p>
<p>  Analyze....Regression....Probit Regression....Response Frequency: enter "mosqui-
</p>
<p>tos gone"....Total Observed: enter "n mosquitos"....Covariate(s): enter "chemical"....
</p>
<p>Transform: select "natural log"....click OK.   
</p>
<p> Chi-Square tests 
</p>
<p> Chi-Square  df a   Sig. 
</p>
<p> PROBIT  Pearson Goodness-of-Fit Test  7706,816  12  ,000 b  
</p>
<p>   a Statistics based on individual cases differ from statistics based on aggregated cases 
</p>
<p>  b Since the signifi cance level is less than, 150, a heterogeneity factor is used in the calculation of 
</p>
<p>confi dence limits 
</p>
<p>    In the output sheets the above table shows that the goodness of fi t tests of the data 
</p>
<p>is signifi cant, and, thus, the data do not fi t the probit model very well. However, 
</p>
<p>SPSS is going to produce a heterogeneity correction factor and we can proceed. The 
</p>
<p>underneath shows that chemical dilution levels are a very signifi cant predictor of 
</p>
<p>proportions of mosquitos gone.
</p>
<p> Parameter estimates 
</p>
<p> Parameter  Estimate 
</p>
<p> Std. 
</p>
<p>Error  Z  Sig. 
</p>
<p> 95 % Confi dence 
</p>
<p>interval 
</p>
<p> Lower 
</p>
<p>bound 
</p>
<p> Upper 
</p>
<p>bound 
</p>
<p> PROBIT a   Chemical 
</p>
<p>(dilution) 
</p>
<p> 1,649  ,006  286,098  ,000  1,638  1,660 
</p>
<p> Intercept  4,489  ,017  267,094  ,000  4,472  4,506 
</p>
<p>   a PROBIT model: PROBIT(p) = Intercept + BX (Covariates X are transformed using the base 2.718 
</p>
<p>logarithm.) 
</p>
<p>46 Probit Models for Estimating Effective Pharmacological Treatment Dosages&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>281
</p>
<p> Cell counts and residuals 
</p>
<p> Number 
</p>
<p> Chemical 
</p>
<p>(dilution) 
</p>
<p> Number of 
</p>
<p>subjects 
</p>
<p> Observed 
</p>
<p>responses 
</p>
<p> Expected 
</p>
<p>responses  Residual  Probability 
</p>
<p> PROBIT  1  &minus;3,912  18000  1000  448,194  551,806  ,025 
</p>
<p> 2  &minus;3,624  18500  1000  1266,672  &minus;266,672  ,068 
</p>
<p> 3  &minus;3,401  19500  3500  2564,259  935,741  ,132 
</p>
<p> 4  &minus;3,124  18000  4500  4574,575  &minus;74,575  ,254 
</p>
<p> 5  &minus;2,708  16500  9500  8405,866  1094,134  ,509 
</p>
<p> 6  &minus;2,430  22500  17000  15410,676  1589,324  ,685 
</p>
<p> 7  &minus;2,303  24000  20500  18134,992  2365,008  ,756 
</p>
<p> 8  &minus;3,912  22500  500  560,243  &minus;60,243  ,025 
</p>
<p> 9  &minus;3,624  18500  1500  1266,672  233,328  ,068 
</p>
<p> 10  &minus;3,401  19000  1000  2498,508  &minus;1498,508  ,132 
</p>
<p> 11  &minus;3,124  20000  5000  5082,861  &minus;82,861  ,254 
</p>
<p> 12  &minus;2,708  22000  10000  11207,821  &minus;1207,821  ,509 
</p>
<p> 13  &minus;2,430  16500  8000  11301,162  &minus;3301,162  ,685 
</p>
<p> 14  &minus;2,303  18500  13500  13979,056  &minus;479,056  ,756 
</p>
<p>   The above table shows that according to chi-square tests the differences between 
</p>
<p>observed and expected proportions of mosquitos gone is several times statistically 
</p>
<p>signifi cant. 
</p>
<p> It does, therefore, make sense to make some inferences using the underneath 
</p>
<p>confi dence limits table.
</p>
<p> Confi dence limits 
</p>
<p> Probability 
</p>
<p> 95 % Confi dence limits 
</p>
<p>for chemical (dilution) 
</p>
<p> 95 % Confi dence limits for log 
</p>
<p>(chemical (dilution) b  
</p>
<p> Estimate 
</p>
<p> Lower 
</p>
<p>bound 
</p>
<p> Upper 
</p>
<p>bound  Estimate 
</p>
<p> Lower 
</p>
<p>bound 
</p>
<p> Upper 
</p>
<p>bound 
</p>
<p> PROBIT a   ,010  ,016  ,012  ,020  &minus;4,133  &minus;4,453  &minus;3,911 
</p>
<p> ,020  ,019  ,014  ,023  &minus;3,968  &minus;4,250  &minus;3,770 
</p>
<p> ,030  ,021  ,016  ,025  &minus;3,863  &minus;4,122  &minus;3,680 
</p>
<p> ,040  ,023  ,018  ,027  &minus;3,784  &minus;4,026  &minus;3,612 
</p>
<p> ,050  ,024  ,019  ,029  &minus;3,720  &minus;3,949  &minus;3,557 
</p>
<p> ,060  ,026  ,021  ,030  &minus;3,665  &minus;3,882  &minus;3,509 
</p>
<p> ,070  ,027  ,022  ,031  &minus;3,617  &minus;3,825  &minus;3,468 
</p>
<p> ,080  ,028  ,023  ,032  &minus;3,574  &minus;3,773  &minus;3,430 
</p>
<p> ,090  ,029  ,024  ,034  &minus;3,535  &minus;3,726  &minus;3,396 
</p>
<p> ,100  ,030  ,025  ,035  &minus;3,500  &minus;3,683  &minus;3,365 
</p>
<p> ,150  ,035  ,030  ,039  &minus;3,351  &minus;3,506  &minus;3,232 
</p>
<p> ,200  ,039  ,034  ,044  &minus;3,233  &minus;3,368  &minus;3,125 
</p>
<p> ,250  ,044  ,039  ,048  &minus;3,131  &minus;3,252  &minus;3,031 
</p>
<p>(continued)
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>282
</p>
<p> Confi dence limits 
</p>
<p> Probability 
</p>
<p> 95 % Confi dence limits 
</p>
<p>for chemical (dilution) 
</p>
<p> 95 % Confi dence limits for log 
</p>
<p>(chemical (dilution) b  
</p>
<p> Estimate 
</p>
<p> Lower 
</p>
<p>bound 
</p>
<p> Upper 
</p>
<p>bound  Estimate 
</p>
<p> Lower 
</p>
<p>bound 
</p>
<p> Upper 
</p>
<p>bound 
</p>
<p>    ,300  ,048  ,043  ,053  &minus;3,040  &minus;3,150  &minus;2,943 
</p>
<p> ,350  ,052  ,047  ,057  &minus;2,956  &minus;3,059  &minus;2,860 
</p>
<p> ,400  ,056  ,051  ,062  &minus;2,876  &minus;2,974  &minus;2,778 
</p>
<p> ,450  ,061  ,055  ,067  &minus;2,799  &minus;2,895  &minus;2,697 
</p>
<p> ,500  ,066  ,060  ,073  &minus;2,722  &minus;2,819  &minus;2,614 
</p>
<p> ,550  ,071  ,064  ,080  &minus;2,646  &minus;2,745  &minus;2,529 
</p>
<p> ,600  ,077  ,069  ,087  &minus;2,569  &minus;2,672  &minus;2/442 
</p>
<p> ,650  ,083  ,074  ,095  &minus;2,489  &minus;2,598  &minus;2,349 
</p>
<p> ,700  ,090  ,080  ,105  &minus;2,404  &minus;2,522  &minus;2,251 
</p>
<p> ,750  ,099  ,087  ,117  &minus;2,313  &minus;2/441  &minus;2,143 
</p>
<p> ,800  ,109  ,095  ,132  &minus;2,212  &minus;2,351  &minus;2,022 
</p>
<p> ,850  ,123  ,106  ,153  &minus;2,094  &minus;2,248  &minus;1,879 
</p>
<p> ,900  ,143  ,120  ,183  &minus;1,945  &minus;2,120  &minus;1,699 
</p>
<p> ,910  ,148  ,124  ,191  &minus;1,909  &minus;2,089  &minus;1,655 
</p>
<p> ,920  ,154  ,128  ,200  &minus;1,870  &minus;2,055  &minus;1,608 
</p>
<p> ,930  ,161  ,133  ,211  &minus;1,827  &minus;2,018  &minus;1,556 
</p>
<p> ,940  ,169  ,138  ,224  &minus;1,780  &minus;1,977  &minus;1/497 
</p>
<p> ,950  ,178  ,145  ,239  &minus;1,725  &minus;1,931  &minus;1/430 
</p>
<p> ,960  ,190  ,153  ,259  &minus;1,661  &minus;1,876  &minus;1,352 
</p>
<p> ,970  ,206  ,164  ,285  &minus;1,582  &minus;1,809  &minus;1,255 
</p>
<p> ,980  ,228  ,179  ,324  &minus;1,477  &minus;1,719  &minus;1,126 
</p>
<p> ,990  ,269  ,206  ,397  &minus;1,312  &minus;1,579  &minus;,923 
</p>
<p>   a A heterogeneity factor is used 
</p>
<p>  b Logarithm base = 2.718 
</p>
<p>    E.g., one might conclude that a 0,143 dilution of the chemical repellent causes 
</p>
<p>0,900 (=90 %) of the mosquitos to have gone. And 0,066 dilution would mean that 
</p>
<p>0,500 (=50 %) of the mosquitos disappeared.  
</p>
<p>    Multiple Probit Regression 
</p>
<p> Like multiple logistic regression using multiple predictors, probit regression can 
</p>
<p>also be applied with multiple predictors. We will add as second predictor to the 
</p>
<p>above example the nonchemical repellents ultrasound (=1) and burning candles 
</p>
<p>(=2) (see uppermost table of this chapter).
</p>
<p>  Command: 
</p>
<p>  Analyze....Regression....Probit Regression....Response Frequency: enter "mosqui-
</p>
<p>tos gone"....Total Observed: enter "n mosquitos"....Covariate(s): enter "chemical"....
</p>
<p>Transform: select "natural log"....click OK.   
</p>
<p>46 Probit Models for Estimating Effective Pharmacological Treatment Dosages&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>283
</p>
<p> Chi-Square tests 
</p>
<p> Chi-Square  df a   Sig. 
</p>
<p> PROBIT  Pearson Goodness-of-Fit Test  3863,489  11  ,000 b  
</p>
<p>   a Statistics based on individual cases differ from statistics based on aggregated cases 
</p>
<p>  b Since the signifi cance level is less than, 150, a heterogeneity factor is used in the calculation of 
</p>
<p>confi dence limits 
</p>
<p>    Again, the goodness of fi t is not what it should be, but SPSS adds a correction factor 
</p>
<p>for heterogeneity. The underneath shows the regression coeffi cients for the multiple 
</p>
<p>model. The no chemical repellents have signifi cantly different effects on the outcome.
</p>
<p> Parameter estimates 
</p>
<p> Parameter  Estimate 
</p>
<p> Std. 
</p>
<p>Error  Z  Sig. 
</p>
<p> 95 % 
</p>
<p>Confi dence 
</p>
<p>interval 
</p>
<p> Lower 
</p>
<p>bound 
</p>
<p> Upper 
</p>
<p>bound 
</p>
<p> PROBIT a   Chemical (dilution)  1,654  ,006  284,386  ,000  1,643  1,665 
</p>
<p> Intercept b   Ultrasound  4,678  ,017  269,650  ,000  4,661  4,696 
</p>
<p> Burning 
</p>
<p>candles 
</p>
<p> 4,321  ,017  253,076  ,000  4,304  4,338 
</p>
<p>   a PROBIT model: PROBIT(p) = Intercept + BX(Covariates X are transformed using the base 2.718 
</p>
<p>logarithm.) 
</p>
<p>  b Corresponds to the grouping variable repellentnonchemical 
</p>
<p> Cell counts and residuals 
</p>
<p> Number 
</p>
<p> Repellent 
</p>
<p>non 
</p>
<p>chemical 
</p>
<p> chemical 
</p>
<p>(dilution) 
</p>
<p> Number 
</p>
<p>of subjects 
</p>
<p> Observed 
</p>
<p>responses 
</p>
<p> Expected 
</p>
<p>responses  Residual  Probability 
</p>
<p> PROBIT  1  1  &minus;3,912  18000  1000  658,233  341,767  ,037 
</p>
<p> 2  1  &minus;3,624  18500  1000  1740,139  &minus;740,139  ,094 
</p>
<p> 3  1  &minus;3,401  19500  3500  3350,108  149,892  ,172 
</p>
<p> 4  1  &minus;3,124  18000  4500  5630,750  &minus;1130,750  ,313 
</p>
<p> 5  1  &minus;2,708  16500  9500  9553,811  &minus;53,811  ,579 
</p>
<p> 6  1  &minus;2,430  22500  17000  16760,668  239,332  ,745 
</p>
<p> 7  1  &minus;2,303  24000  20500  19388,521  1111,479  ,808 
</p>
<p> 8  2  &minus;3,912  22500  500  355,534  144,466  ,016 
</p>
<p> 9  2  &minus;3,624  18500  1500  871,485  628,515  ,047 
</p>
<p> 10  2  &minus;3,401  19000  1000  1824,614  &minus;824,614  ,096 
</p>
<p> 11  2  &minus;3,124  20000  5000  3979,458  1020,542  ,199 
</p>
<p> 12  2  &minus;2,708  22000  10000  9618,701  381,299  ,437 
</p>
<p> 13  2  &minus;2,430  16500  8000  10202,854  &minus;2202,654  ,618 
</p>
<p> 14  2  &minus;2,303  18500  13500  12873,848  626,152  ,696 
</p>
<p>   In the above Cell Counts table, it is shown that according to the chi-square tests 
</p>
<p>the differences of observed and expected proportions of mosquitos gone were statis-
</p>
<p>tically signifi cant several times. The next page table gives interesting results. E.g., a 
</p>
<p>0,128 dilution of the chemical repellent causes 0,900 (=90 %) of the mosquitos to 
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>284
</p>
<p>have gone in the ultrasound tests. And 0,059 dilution would mean that 0,500 (=50 %) 
</p>
<p>of the mosquitos disappeared. The results of burning candles were less impressive. 
</p>
<p>0,159 dilution caused 90 % of the mosquitos to disappear, 0,073 dilution 50 %.
</p>
<p> Confi dence limits 
</p>
<p> Nonchemical  Probability 
</p>
<p> 95 % Confi dence limits 
</p>
<p>for chemical (dilution) 
</p>
<p> 95 % Confi dence limits for 
</p>
<p>log (chemical (dilution) b  
</p>
<p> Estimate 
</p>
<p> Lower 
</p>
<p>bound 
</p>
<p> Upper 
</p>
<p>bound  Estimate 
</p>
<p> Lower 
</p>
<p>bound 
</p>
<p> Upper 
</p>
<p>bound 
</p>
<p> PROBIT a   Ultrasound  ,010  ,014  ,011  ,018  &minus;4,235  &minus;4,486  &minus;4,042 
</p>
<p> ,020  ,017  ,014  ,020  &minus;4,070  &minus;4,296  &minus;3,895 
</p>
<p> ,030  ,019  ,015  ,022  &minus;3,966  &minus;4,176  &minus;3,801 
</p>
<p> ,040  ,021  ,017  ,024  &minus;3,887  &minus;4,086  &minus;3,731 
</p>
<p> ,050  ,022  ,018  ,025  &minus;3,823  &minus;4,013  &minus;3,673 
</p>
<p> ,060  ,023  ,019  ,027  &minus;3,769  &minus;3,951  &minus;3,624 
</p>
<p> ,070  ,024  ,020  ,028  &minus;3,721  &minus;3,896  &minus;3,581 
</p>
<p> ,080  ,025  ,021  ,029  &minus;3,678  &minus;3,848  &minus;3,542 
</p>
<p> ,090  ,026  ,022  ,030  &minus;3,639  &minus;3,804  &minus;3,506 
</p>
<p> ,100  ,027  ,023  ,031  &minus;3,603  &minus;3,763  &minus;3,473 
</p>
<p> ,150  ,032  ,027  ,036  &minus;3,455  &minus;3,597  &minus;3,337 
</p>
<p> ,200  ,036  ,031  ,040  &minus;3,337  &minus;3,467  &minus;3,227 
</p>
<p> ,250  ,039  ,035  ,044  &minus;3,236  &minus;3,356  &minus;3,131 
</p>
<p> ,300  ,043  ,038  ,048  &minus;3,146  &minus;3,258  &minus;3,043 
</p>
<p> ,350  ,047  ,042  ,052  &minus;3,062  &minus;3,169  &minus;2,961 
</p>
<p> ,400  ,051  ,046  ,056  &minus;2,982  &minus;3,085  &minus;2,882 
</p>
<p> ,450  ,055  ,049  ,061  &minus;2,905  &minus;3,006  &minus;2,803 
</p>
<p> ,500  ,059  ,053  ,066  &minus;2,829  &minus;2,929  &minus;2,725 
</p>
<p> ,550  ,064  ,058  ,071  &minus;2,753  &minus;2,853  &minus;2,646 
</p>
<p> ,600  ,069  ,062  ,077  &minus;2,675  &minus;2,777  &minus;2,564 
</p>
<p> ,650  ,075  ,067  ,084  &minus;2,596  &minus;2,700  &minus;2,478 
</p>
<p> ,700  ,081  ,073  ,092  &minus;2,512  &minus;2,620  &minus;2,387 
</p>
<p> ,750  ,089  ,079  ,102  &minus;2,421  &minus;2,534  &minus;2,287 
</p>
<p> ,800  ,098  ,087  ,114  &minus;2,320  &minus;2,440  &minus;2,174 
</p>
<p> ,850  ,111  ,097  ,130  &minus;2,202  &minus;2,332  &minus;2,042 
</p>
<p> ,900  ,128  ,111  ,153  &minus;2,054  &minus;2,197  &minus;1,874 
</p>
<p> ,910  ,133  ,115  ,160  &minus;2,018  &minus;2,165  &minus;1,833 
</p>
<p> ,920  ,138  ,119  ,167  &minus;1,979  &minus;2,129  &minus;1,789 
</p>
<p> ,930  ,144  ,124  ,175  &minus;1,936  &minus;2,091  &minus;1,740 
</p>
<p> ,940  ,151  ,129  ,185  &minus;1,889  &minus;2,048  &minus;1,686 
</p>
<p> ,950  ,160  ,135  ,197  &minus;1,834  &minus;1,999  &minus;1,623 
</p>
<p> ,960  ,170  ,143  ,212  &minus;1,770  &minus;1,942  &minus;1,550 
</p>
<p> ,970  ,184  ,154  ,232  &minus;1,691  &minus;1,871  &minus;1,459 
</p>
<p> ,980  ,205  ,169  ,262  &minus;1,587  &minus;1,778  &minus;1,339 
</p>
<p> ,990  ,241  ,196  ,317  &minus;1,422  &minus;1,632  &minus;1,149 
</p>
<p>(continued)
</p>
<p>46 Probit Models for Estimating Effective Pharmacological Treatment Dosages&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>285
</p>
<p> Confi dence limits 
</p>
<p> Nonchemical  Probability 
</p>
<p> 95 % Confi dence limits 
</p>
<p>for chemical (dilution) 
</p>
<p> 95 % Confi dence limits for 
</p>
<p>log (chemical (dilution) b  
</p>
<p> Estimate 
</p>
<p> Lower 
</p>
<p>bound 
</p>
<p> Upper 
</p>
<p>bound  Estimate 
</p>
<p> Lower 
</p>
<p>bound 
</p>
<p> Upper 
</p>
<p>bound 
</p>
<p>    Burning 
</p>
<p>candles 
</p>
<p> ,010  ,018  ,014  ,021  &minus;4,019  &minus;4,247  &minus;3,841 
</p>
<p> ,020  ,021  ,017  ,025  &minus;3,854  &minus;4,058  &minus;3,693 
</p>
<p> ,030  ,024  ,019  ,027  &minus;3,750  &minus;3,939  &minus;3,599 
</p>
<p> ,040  ,025  ,021  ,029  &minus;3,671  &minus;3,850  &minus;3,528 
</p>
<p> ,050  ,027  ,023  ,031  &minus;3,607  &minus;3,777  &minus;3,469 
</p>
<p> ,060  ,029  ,024  ,033  &minus;3,553  &minus;3,716  &minus;3,420 
</p>
<p> ,070  ,030  ,026  ,034  &minus;3,505  &minus;3,662  &minus;3,376 
</p>
<p> ,080  ,031  ,027  ,036  &minus;3,462  &minus;3,614  &minus;3,336 
</p>
<p> ,090  ,033  ,028  ,037  &minus;3,423  &minus;3,571  &minus;3,300 
</p>
<p> ,100  ,034  ,029  ,038  &minus;3,387  &minus;3,531  &minus;3,267 
</p>
<p> ,150  ,039  ,034  ,044  &minus;3,239  &minus;3,367  &minus;3,128 
</p>
<p> ,200  ,044  ,039  ,049  &minus;3,121  &minus;3,240  &minus;3,015 
</p>
<p> ,250  ,049  ,044  ,054  &minus;3,020  &minus;3,132  &minus;2,916 
</p>
<p> ,300  ,053  ,048  ,059  &minus;2,930  &minus;3,037  &minus;2,826 
</p>
<p> ,350  ,058  ,052  ,065  &minus;2,845  &minus;2,950  &minus;2,741 
</p>
<p> ,400  ,063  ,057  ,070  &minus;2,766  &minus;2,869  &minus;2,658 
</p>
<p> ,450  ,068  ,061  ,076  &minus;2,688  &minus;2,793  &minus;2,578 
</p>
<p> ,500  ,073  ,066  ,082  &minus;2,613  &minus;2,718  &minus;2,497 
</p>
<p> ,550  ,079  ,071  ,089  &minus;2,537  &minus;2,644  &minus;2,415 
</p>
<p> ,600  ,085  ,076  ,097  &minus;2,459  &minus;2,571  &minus;2,331 
</p>
<p> ,650  ,093  ,082  ,106  &minus;2,380  &minus;2,495  &minus;2,244 
</p>
<p> ,700  ,101  ,089  ,116  &minus;2,295  &minus;2,417  &minus;2,151 
</p>
<p> ,750  ,110  ,097  ,129  &minus;2,205  &minus;2,333  &minus;2,049 
</p>
<p> ,800  ,122  ,106  ,144  &minus;2,104  &minus;2,240  &minus;1,936 
</p>
<p> ,850  ,137  ,119  ,165  &minus;1,986  &minus;2,133  &minus;1,802 
</p>
<p> ,900  ,159  ,136  ,195  &minus;1,838  &minus;1,999  &minus;1,633 
</p>
<p> ,910  ,165  ,140  ,203  &minus;1,802  &minus;1,966  &minus;1,592 
</p>
<p> ,920  ,172  ,145  ,213  &minus;1,763  &minus;1,932  &minus;1,548 
</p>
<p> ,930  ,179  ,151  ,223  &minus;1,720  &minus;1,893  &minus;1,499 
</p>
<p> ,940  ,188  ,157  ,236  &minus;1,672  &minus;1,850  &minus;1,444 
</p>
<p> ,950  ,198  ,165  ,251  &minus;1,618  &minus;1,802  &minus;1,381 
</p>
<p> ,960  ,211  ,175  ,270  &minus;1,554  &minus;1,745  &minus;1,308 
</p>
<p> ,970  ,229  ,187  ,296  &minus;1,475  &minus;1,675  &minus;1,217 
</p>
<p> ,980  ,254  ,206  ,334  &minus;1,371  &minus;1,582  &minus;1,096 
</p>
<p> ,990  ,299  ,238  ,404  &minus;1,206  &minus;1,436  &minus;,906 
</p>
<p>   a A heterogenelty factor is used 
</p>
<p>  b Logarithm base = 2.718 
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>286
</p>
<p>       
</p>
<p>    The above fi gure supports the adequacy of the multiple variables probit model, 
</p>
<p>with two similarly sloped linear patterns (the blue and the green one) of "chemical 
</p>
<p>repellent levels" versus "mosquitos gone levels" regressions.   
</p>
<p>    Conclusion 
</p>
<p> Probit regression is, just like logistic regression, for estimating the effect of predic-
</p>
<p>tors on yes/no outcomes. If your predictor is multiple pharmacological treatment 
</p>
<p>dosages, then probit regression may be more convenient than logistic regression, 
</p>
<p>because your results will be reported in the form of response rates instead of odds 
</p>
<p>ratios. 
</p>
<p> This chapter shows that probit regression is able to fi nd response rates of differ-
</p>
<p>ent dosages of mosquito repellents.  
</p>
<p>46 Probit Models for Estimating Effective Pharmacological Treatment Dosages&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>287
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of probit regression is 
</p>
<p>given in the Chap. 7, Machine learning in medicine part three, Probit regression, 
</p>
<p>pp 63&ndash;68, 2013, Springer Heidelberg Germany, (from the same authors).    
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>289&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_47
</p>
<p>    Chapter 47   
</p>
<p> Interval Censored Data Analysis for Assessing 
Mean Time to Cancer Relapse (51 Patients) 
</p>
<p>                       General Purpose 
</p>
<p> In survival studies often time to fi rst outpatient clinic check instead of time to event 
</p>
<p>is measured. Somewhere in the interval between the last and current visit an event 
</p>
<p>may have taken place. For simplicity such data are often analyzed using the propor-
</p>
<p>tional hazard model of Cox (Chap. 17, Cox regression, pp. 209&ndash;212, in: Statistics 
</p>
<p>applied to clinical studies 5th edition, Springer Heidelberg Germany, 2012, from the 
</p>
<p>same authors). However, this analysis is not entirely appropriate. It assumes that 
</p>
<p>time to fi rst outpatient check is equal to time to relapse. However, instead of a time 
</p>
<p>to relapse an interval is given, in which the relapse has occurred, and so this variable 
</p>
<p>is somewhat more loose than the usual variable time to event. An appropriate statis-
</p>
<p>tic for the current variable would be the mean time to relapse inferenced from a 
</p>
<p>generalized linear model with an interval censored link function, rather than the 
</p>
<p>proportional hazard method of Cox.  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> This chapter is to assess whether an appropriate statistic for the variable &ldquo;time to 
</p>
<p>fi rst check&rdquo; in survival studies would be the mean time to relapse, as inferenced 
</p>
<p>from a generalized linear model with an interval censored link function.  </p>
<p/>
</div>
<div class="page"><p/>
<p>290
</p>
<p>    Example 
</p>
<p> In 51 patients in remission their status at the time-to-fi rst-outpatient-clinic-control 
</p>
<p>was checked (mths = months). 
</p>
<p> treatment (0 and 1)  time to fi rst check (mths)  result (0 = remission 1 = relapse) 
</p>
<p> 1  11  0 
</p>
<p> 0  12  1 
</p>
<p> 0  9  1 
</p>
<p> 1  12  0 
</p>
<p> 0  12  0 
</p>
<p> 1  12  0 
</p>
<p> 1  5  1 
</p>
<p> 1  12  0 
</p>
<p> 1  12  0 
</p>
<p> 0  12  0 
</p>
<p>   The fi rst 10 patients are above. The entire data fi le is entitled &ldquo;intervalcensored.
</p>
<p>sav&rdquo;, and is in extras.springer.com. Cox regression was applied. Start by opening 
</p>
<p>the data fi le in SPSS statistical software.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Survival&hellip;.Cox Regression&hellip;.Time : time to fi rst check&hellip;.Status : 
</p>
<p>result&hellip;.Defi ne Event&hellip;.Single value: type 1&hellip;.click Continue&hellip;.Covariates: enter 
</p>
<p>treatment&hellip;.click Categorical&hellip;.Categorical Covariates: enter treatment&hellip;.click 
</p>
<p>Continue&hellip;.click Plots&hellip;.mark Survival&hellip;.Separate Lines for: enter treatment&hellip;.
</p>
<p>click Continue&hellip;.click OK.   
</p>
<p> Variables in the Equation 
</p>
<p> B  SE  Wald  df  Sig.  Exp(B) 
</p>
<p> Treatment  .919  .477  3.720  1  .054  2.507 
</p>
<p>47 Interval Censored Data Analysis for Assessing Mean Time to Cancer Relapse</p>
<p/>
</div>
<div class="page"><p/>
<p>291
</p>
<p>      
</p>
<p>    The above table is in the output. It shows that treatment is not a signifi cant 
</p>
<p> predictor for relapse. In spite of the above Kaplan-Meier curves, suggesting the 
</p>
<p>opposite, the treatments are not signifi cantly different from one another because 
</p>
<p>p &gt; 0.05. However, the analysis so far is not entirely appropriate. It assumes that 
</p>
<p>time to fi rst outpatient check is equal to time to relapse. However, instead of a time 
</p>
<p>to relapse an interval is given between 2 and 12 months in which the relapse has 
</p>
<p>occurred, and so this variables is somewhat more loose than the usual variable time 
</p>
<p>to event. An appropriate statistic for the current variable would be the mean time to 
</p>
<p>relapse inferenced from a generalized linear model with an interval censored link 
</p>
<p>function, rather than the proportional hazard method of Cox.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.click Generalized Linear Models&hellip;.click once again Generalized Linear 
</p>
<p>Models&hellip;.Type of Model&hellip;.mark Interval censored survival&hellip;.click Response&hellip;. 
</p>
<p>Dependent Variable: enter Result&hellip;.Scale Weight Variable: enter &ldquo;time to fi rst 
</p>
<p>check&rdquo;&hellip;.click Predictors&hellip;.Factors: enter &ldquo;treatment&rdquo;&hellip;.click Model&hellip;.click once 
</p>
<p>again Model: enter once again &ldquo;treatment&rdquo;&hellip;.click Save&hellip;.mark Predicted value of 
</p>
<p>mean of response&hellip;.click OK.   
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>292
</p>
<p> Parameter estimates 
</p>
<p> Parameter  B  Std. Error 
</p>
<p> 95 % Wald 
</p>
<p>confi dence interval  Hypothesis test 
</p>
<p> Lower  Upper 
</p>
<p> Wald 
</p>
<p>Chi-Square  df  Sig. 
</p>
<p> (Intercept)  .467  .0735  .323  .611  40.431  1  .000 
</p>
<p> [treatments]  &minus;.728  .1230  &minus;.969  &minus;.487  35.006  1  .000 
</p>
<p> [treatments]  0 a  
</p>
<p> (Scale)  1 b  
</p>
<p>  Dependent Variable: Result 
</p>
<p> Model: (Intercept), treatment 
</p>
<p>  a Set to zero because this parameter Is redundant 
</p>
<p>  b Fixed at the displayed value 
</p>
<p>    The generalized linear model shows, that, after censoring the intervals, the treat-
</p>
<p>ment 0 is, compared to treat 1, a very signifi cant better maintainer of remission. 
</p>
<p>When we return to the data, we will observe as a novel variable, the mean predicted 
</p>
<p>probabilities of persistent remission for each patient. This is shown underneath for 
</p>
<p>the fi rst 10 patients. For the patients on treatment 1 it equals 79,7 %, for the patients 
</p>
<p>on treatment 0 it is only 53,7 %. And so, treatment 1 performs, indeed, a lot better 
</p>
<p>than does treatment 0 (mths = months). 
</p>
<p> treatment (0 and 1)  time to fi rst check (mths)  result (0 = remission) 
</p>
<p>    Mean Predicted_
</p>
<p>1 1 = relapse) 
</p>
<p> 1  11  0  .797 
</p>
<p> 0  12  1  .537 
</p>
<p> 0  9  1  .537 
</p>
<p> 1  12  0  .797 
</p>
<p> 0  12  0  .537 
</p>
<p> 1  12  0  .797 
</p>
<p> 1  5  1  .797 
</p>
<p> 1  12  0  .797 
</p>
<p> 1  12  0  .797 
</p>
<p> 0  12  0  .537 
</p>
<p>       Conclusion 
</p>
<p> This chapter assesses whether an appropriate statistic for the variable &ldquo;time to fi rst 
</p>
<p>check&rdquo; in survival studies is the mean time to relapse, as inferenced from a general-
</p>
<p>ized linear model with an interval censored link function. The current example 
</p>
<p>shows that, in addition, more sensitivity of testing is obtained with p-values of 0.054 
</p>
<p>47 Interval Censored Data Analysis for Assessing Mean Time to Cancer Relapse</p>
<p/>
</div>
<div class="page"><p/>
<p>293
</p>
<p>versus 0.0001. Also, predicted probabilities of persistent remission or risk of relapse 
</p>
<p>for different treatment modalities are given. This method is an important tool for 
</p>
<p>analyzing such data.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of survival analyses is 
</p>
<p>given in Chap. 17, Cox regression, pp. 209&ndash;212, in: Statistics applied to clinical 
</p>
<p>studies 5th edition, Springer Heidelberg Germany, 2012, from the same authors.    
</p>
<p> Note</p>
<p/>
</div>
<div class="page"><p/>
<p>295&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_48
</p>
<p>    Chapter 48   
</p>
<p> Structural Equation Modeling (SEM) 
with SPSS Analysis of Moment Structures 
(Amos) for Cause Effect Relationships I 
(35 Patients) 
</p>
<p>                       General Purpose 
</p>
<p> In clinical effi cacy studies the outcome is often infl uenced by multiple causal fac-
</p>
<p>tors, like drug - noncompliance, frequency of counseling, and many more factors. 
</p>
<p>Structural equation modeling (SEM) was only recently formally defi ned by Pearl 
</p>
<p>(In: Causality, reason, and inference, Cambridge University Press, Cambridge UK 
</p>
<p>2000). This statistical methodology includes
</p>
<p>    1.    factor analysis (see also Chap.14, Factor analysis, pp 167&ndash;181, in: Machine 
</p>
<p>learning in medicine part one, Springer Heidelberg Germany, 2013, from the 
</p>
<p>same authors),   
</p>
<p>   2.    path analysis (see also Chap. 2, Multistage regression, in: SPSS for starters part 
</p>
<p>two, pp 3&ndash;6, Springer Heidelberg Germany, 2012 from the same authors),   
</p>
<p>   3.    regression analysis (see also Chap. 14, Linear regression, basic approach, in: 
</p>
<p>Statistics applied to clinical studies 5th edition, pp 161&ndash;176, Springer Heidelberg 
</p>
<p>Germany, 2012, from the same authors).     
</p>
<p> An SEM model looks like a complex regression model, but it is more. It extends 
</p>
<p>the prior hypothesis of correlation to that of causality, and this is accomplished by a 
</p>
<p>network of variables tested versus one another with standardized rather than unstan-
</p>
<p>dardized regression coeffi cients. 
</p>
<p> The network is commonly named a Bayesian network, otherwise called a DAG 
</p>
<p>(directed acyclic graph), (see also Chap. 16, Bayesian networks, pp 163&ndash;170, in: 
</p>
<p>Machine learning in medicine part 2, Springer Heidelberg Germany, 2013, from the 
</p>
<p>same authors), which is a probabilistic graphical model of nodes (the variables) and 
</p>
<p>connecting arrows presenting the conditional dependencies of the nodes. 
</p>
<p> This chapter is to assess whether the Amos (analysis of moment structures) add-
</p>
<p> on module of SPSS statistical software, frequently used in econo-/sociometry, but 
</p>
<p>little used in medicine, is able to perform an SEM analysis of pharmacodynamic 
</p>
<p>data.  </p>
<p/>
</div>
<div class="page"><p/>
<p>296
</p>
<p>    Primary Scientifi c Question 
</p>
<p> Can SEM modeling in Amos (Analysis of Moment Structures) demonstrate direct 
</p>
<p>and indirect effects of non-compliance and counseling on treatment effi cacy.  
</p>
<p>    Example 
</p>
<p> We will use the same example as the one used in Chap. 2, Multistage regression, in: 
</p>
<p>SPSS for starters part two, pp 3&ndash;6, Springer Heidelberg Germany, 2012 from the 
</p>
<p>same authors. 
</p>
<p> stool  counseling  noncompliance 
</p>
<p> stools/month  counselings/month  drug noncompliances/month 
</p>
<p> 24,00  8,00  25,00 
</p>
<p> 30,00  13,00  30,00 
</p>
<p> 25,00  15,00  25,00 
</p>
<p> 35,00  10,00  31,00 
</p>
<p> 39,00  9,00  36,00 
</p>
<p> 30,00  10,00  33,00 
</p>
<p> 27,00  8,00  22,00 
</p>
<p> 14,00  5,00  18,00 
</p>
<p> 39,00  13,00  14,00 
</p>
<p> 42,00  15,00  30,00 
</p>
<p>   The fi rst 10 patients of the 35 patient data fi le is above. The entire data fi le is in 
</p>
<p>extras.springer.com, and is entitled &ldquo;amos1.sav&rdquo;. We will fi rst perform traditional 
</p>
<p>linear regressions. Start by opening the data fi le.
</p>
<p>  Command: 
</p>
<p>  Analyze....Regression....Linear....Dependent: enter "stool"....Independent(s): enter 
</p>
<p>"counseling and non-compliance"....click OK.   
</p>
<p> Coeffi cients a  
</p>
<p> Model 
</p>
<p> Unstandardized 
</p>
<p>coeffi cients 
</p>
<p> Standardized 
</p>
<p>coeffi cients 
</p>
<p> t  Sig.  B  Std. Error  Beta 
</p>
<p> 1  (Constant)  2,270  4,823  ,471  ,641 
</p>
<p> Counseling  1,876  ,290  ,721  6,469  ,000 
</p>
<p> Non-compliance  ,285  ,167  ,190  1,705  ,098 
</p>
<p>   a Dependent variable: ther eff 
</p>
<p>    The above table is given on the output sheet, and shows that, with p = 0.10 as cut- 
</p>
<p>off for statistical signifi cance both variables are signifi cant.
</p>
<p>48 Structural Equation Modeling (SEM) with SPSS Analysis of Moment Structures&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>297
</p>
<p>  Command: 
</p>
<p>  Analyze....Regression....Linear....Dependent: enter "counseling&rdquo;....Independent(s): 
</p>
<p>enter "non-compliance"....click OK.   
</p>
<p> Coeffi cients a  
</p>
<p> Model 
</p>
<p> Unstandardized 
</p>
<p>coeffi cients 
</p>
<p> Standardized 
</p>
<p>coeffi cients 
</p>
<p> t  Sig.  B  Std. Error  Beta 
</p>
<p> 1  (Constant)  4,228  2,800  1,510  ,141 
</p>
<p> Non-compliance  ,220  ,093  ,382  2,373  ,024 
</p>
<p>   a Dependent variable: counseling 
</p>
<p>    The above table shows that non-compliance is also a signifi cant predictor of 
</p>
<p>counseling. This would mean that non-compliance works two ways: it predicts ther-
</p>
<p>apeutic effi cacy directly and indirectly through counseling. However, the indirect 
</p>
<p>way is not taken into account in the one step linear regression. We will now use the 
</p>
<p>Amos add-on module for further analysis.
</p>
<p>  Command: 
</p>
<p>  Analyze....click IBM SPSS Amos    
</p>
<p> The work area of Amos appears. The menu is in the second upper row. The tool-
</p>
<p>bar is on the left. In the empty area on the right you can draw your networks.
</p>
<p>   click File....click Save as....Browse the folder you selected in your personal com-
</p>
<p>puter, and enter Amos1....click Save.    
</p>
<p> In the fi rst upper row the title Amos1 has appeared, in the bottom rectangle left 
</p>
<p>from the empty area the title Amos1 has also appeared.
</p>
<p>   click Diagram....left click "Draw Observed" and drag to empty area....click the green 
</p>
<p>rectangle and a colorless rectangle appears....left click it and a red rectangle appears....
</p>
<p>do this 2 more times and have the rectangles at different places....click Diagram again....
</p>
<p>left click "Draw Unobserved" and drag to right part of empty area....click the green 
</p>
<p>ellipse.....a colorless ellipse appears, and later a red and fi nally black ellipse.    
</p>
<p> The underneath fi gure shows how your screen will look by now. There are three 
</p>
<p>rectangle nodes for observed variables, and one oval node for an unobserved, other-
</p>
<p>wise called latent, variable. 
</p>
<p>    
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>298
</p>
<p>    Next we will have to enter the names of the variables.
</p>
<p>  Command: 
</p>
<p>  right click in the left upper rectangle....click Object Properties....Variable name: 
</p>
<p>type "noncompliance"....close dialog box....the name is now in the rectangle....do 
</p>
<p>the same for the other two rectangles and type in the ellipse the term others (it indi-
</p>
<p>cates the remainder of variables not taken into account in the current model, together 
</p>
<p>with the variables present the outcome is explained by 100 %).    
</p>
<p> Next arrows have to be added to the diagram.
</p>
<p>  Command: 
</p>
<p>  click Diagram....click Draw Path for arrows....click Draw Covariance for double- 
</p>
<p>headed arrow.    
</p>
<p>    
</p>
<p>    The above fi gure is now in the empty area. In order to match the &ldquo;others&rdquo; vari-
</p>
<p>ables in the ellipse with the three other variables of the model, they a regression 
</p>
<p>weight, like the value 1, has to be added.
</p>
<p>   right click at the arrow of the ellipse....click Object Properties....click Parameters....
</p>
<p>Regression weights: enter 1....close dialog box.    
</p>
<p>    
</p>
<p>    The empty area now has the value 1.
</p>
<p>   click from menu View....Analysis Properties....Output....mark "Minimization his-
</p>
<p>tory, Standardized estimates, and Squared multiple correlations"....close dialog 
</p>
<p>box....click Analyze....Calculate Estimates.    
</p>
<p>48 Structural Equation Modeling (SEM) with SPSS Analysis of Moment Structures&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>299
</p>
<p> A new path diagram button has appeared in the upper white rectangle left from 
</p>
<p>the empty area. Click it 
</p>
<p>    
</p>
<p>    Unstandardized regression coeffi cients are now in the model, that of counseling 
</p>
<p>versus stool equals 1.88, that of noncompliance versus stool equals 0.28. These 
</p>
<p>values are identical to the values obtained by ordinary multiple linear regression as 
</p>
<p>shown in the previous tabs. However, for path analysis standardized values are more 
</p>
<p>important. 
</p>
<p> In order to view the standardized values, click in the third white rectangle left 
</p>
<p>from the empty area &ldquo;Standardized estimates&rdquo;. 
</p>
<p>    
</p>
<p>    Now we observe the standardized regression coeffi cients. They are identical to 
</p>
<p>the standardized regression coeffi cients as computed by the ordinary linear regres-
</p>
<p>sion models as shown in the previous tabs:
</p>
<p>   0.19 noncompliance versus stool  
</p>
<p>  0.72 counseling versus stool  
</p>
<p>  0.38 noncompliance versus counseling.    
</p>
<p> What advantage does this path analysis give us as compared to traditional regres-
</p>
<p>sion modeling. The advantage is that multiple regression coeffi cients, as they are 
</p>
<p>standardized, can be simply added up after weighting in order to estimate the entire 
</p>
<p>strength of prediction. Single path analysis gives a standardized regression coeffi -
</p>
<p>cient of 0.19. This underestimates the real effect of non-compliance. Two step path 
</p>
<p>analysis is more realistic and shows that the add-up path statistic is larger and equals
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>300
</p>
<p>  0 19 0 38 0 72 0 46. . . .+ &acute; =    
</p>
<p>The two-path statistic of 0.46 is a lot better than the single path statistic of 0.19 with 
</p>
<p>an increase of 60 %.  
</p>
<p>    Conclusion 
</p>
<p> SEM is adequate for cause effect assessments in pharmacology, and, in addition, it 
</p>
<p>is very easy to use. Arbuckle, the author of IBM SPSS Amos 19 User&rsquo;s Guide noted, 
</p>
<p>that it may open the way to data analysis to nonstatisticians, because it avoids math-
</p>
<p>ematics, and, like other machine learning software, e.g., SPSS modeler (see the 
</p>
<p>Chaps. 61, 64, 65), Knime (see the Chaps. 7, 8, 70, 71, 74), and Weka (see the Chap. 
</p>
<p>70), it makes extensively use of beautiful graphs to visualize procedures and results 
</p>
<p>instead. 
</p>
<p> The current chapter gives only the simplest applications of SEM modeling. SEM 
</p>
<p>modeling can also handle binary data using chi-square tests, include multiple mod-
</p>
<p>els in a single analysis, replace more complex multivariate analysis of variance with 
</p>
<p>similar if not better power.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of structural equation 
</p>
<p>models like path analysis, factor analysis, and regression models are in (1) the 
</p>
<p>Chap. 14, Factor analysis, pp 167&ndash;181, in: Machine learning in medicine part 1, 
</p>
<p>Springer Heidelberg Germany, 2013, (2) the Chap. 2, Multistage regression, in: 
</p>
<p>SPSS for starters part 2, pp 3&ndash;6, Springer Heidelberg Germany, 2012, (3) the Chap. 
</p>
<p>14, Linear regression, basic approach, in: Statistics applied to clinical studies 5th 
</p>
<p>edition, pp 161&ndash;176, Springer Heidelberg Germany, 2012, all from the same authors.    
</p>
<p>48 Structural Equation Modeling (SEM) with SPSS Analysis of Moment Structures&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>301&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_49
</p>
<p>    Chapter 49   
</p>
<p> Structural Equation Modeling (SEM) 
with SPSS Analysis of Moment Structures 
(Amos) for Cause Effect Relationships 
in Pharmacodynamic Studies II (35 Patients) 
</p>
<p>                       General Purpose 
</p>
<p> In clinical effi cacy studies the outcome is often infl uenced by multiple causal fac-
</p>
<p>tors, like drug - noncompliance, frequency of counseling, and many more factors. 
</p>
<p>Structural equation modeling (SEM) was only recently formally defi ned by Pearl 
</p>
<p>(In: Causality, reason, and inference, Cambridge University Press, Cambridge UK 
</p>
<p>2000). This statistical methodology includes
</p>
<p>    1.    factor analysis (see also Chap. 14, Factor analysis, pp 167&ndash;181, in: Machine 
</p>
<p>learning in medicine part one, Springer Heidelberg Germany, 2013, from the 
</p>
<p>same authors),   
</p>
<p>   2.    path analysis (see also Chap. 2, Multistage regression, in: SPSS for starters part 
</p>
<p>two, pp 3&ndash;6, Springer Heidelberg Germany, 2012 from the same authors),   
</p>
<p>   3.    regression analysis (see also Chap. 14, Linear regression, basic approach, in: 
</p>
<p>Statistics applied to clinical studies 5th edition, pp 161&ndash;176, Springer Heidelberg 
</p>
<p>Germany, 2012, from the same authors).     
</p>
<p> An SEM model looks like a complex regression model, but it is more. It extends 
</p>
<p>the prior hypothesis of correlation to that of causality, and this is accomplished by a 
</p>
<p>network of variables tested versus one another with standardized rather than unstan-
</p>
<p>dardized regression coeffi cients. 
</p>
<p> The network is commonly named a Bayesian network, otherwise called a DAG 
</p>
<p>(directed acyclic graph), (see also Chap.16, Bayesian networks, pp 163&ndash;170, in: 
</p>
<p>Machine learning in medicine part 2, Springer Heidelberg Germany, 2013, from the 
</p>
<p>same authors), which is a probabilistic graphical model of nodes (the variables) and 
</p>
<p>connecting arrows presenting the conditional dependencies of the nodes. 
</p>
<p> This chapter is to assess whether the Amos (analysis of moment structures) add-
</p>
<p> on module of SPSS statistical software, frequently used in econo-/sociometry but 
</p>
<p>little used in medicine, is able to perform an SEM analysis of pharmacodynamic 
</p>
<p>data.  </p>
<p/>
</div>
<div class="page"><p/>
<p>302
</p>
<p>    Primary Scientifi c Question 
</p>
<p> Can SEM modeling in Amos (analysis of moment structures) demonstrate direct 
</p>
<p>and indirect effects of non-compliance and counseling on treatment effi cacy and 
</p>
<p>quality of life.  
</p>
<p>    Example 
</p>
<p> We will use the same example as the one used in Chap.3, Multivariate analysis using 
</p>
<p>path statistics, in: SPSS for starters part 2, pp 7&ndash;11, Springer Heidelberg Germany, 
</p>
<p>2012 from the same authors. 
</p>
<p> stool  counseling  noncompliance  qol 
</p>
<p> stools/month  counselings/month  drug noncompliances/month  quality of life score 
</p>
<p> 24,00  8,00  25,00  69,00 
</p>
<p> 30,00  13,00  30,00  110,00 
</p>
<p> 25,00  15,00  25,00  78,00 
</p>
<p> 35,00  10,00  31,00  103,00 
</p>
<p> 39,00  9,00  36,00  103,00 
</p>
<p> 30,00  10,00  33,00  102,00 
</p>
<p> 27,00  8,00  22,00  76,00 
</p>
<p> 14,00  5,00  18,00  75,00 
</p>
<p> 39,00  13,00  14,00  99,00 
</p>
<p> 42,00  15,00  30,00  107,00 
</p>
<p>   The fi rst 10 patients of the 35 patient data fi le is above. The entire data fi le is in 
</p>
<p>extras.springer.com, and is entitled &ldquo;amos2.sav&rdquo;. 
</p>
<p> We will use SEM modeling for estimating variances and covariances in these 
</p>
<p>data. 
</p>
<p>    
</p>
<p>    It is a measure for the spread of the data of the variable x. 
</p>
<p>    
</p>
<p>    It is a measure for the strength of association between the two variables x 1  and x 2 . 
</p>
<p>If the covariances are signifi cantly larger than zero, this would mean that there is a 
</p>
<p>signifi cant association between them.
</p>
<p>49 Structural Equation Modeling (SEM) with SPSS Analysis of Moment Structures&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>303
</p>
<p>  Command: 
</p>
<p>  Analyze....click IBM SPSS Amos    
</p>
<p> The work area of Amos appears. The menu is in the second upper row. The 
</p>
<p> toolbar is on the left. In the empty area on the right you can draw your networks.
</p>
<p>   click File....click Save as....Browse the folder you selected in your personal com-
</p>
<p>puter, and enter amos2....click Save.    
</p>
<p> In the fi rst upper row the title amos2 has appeared, in the bottom rectangle left 
</p>
<p>from the empty area the title amos2 has also appeared.
</p>
<p>   click Diagram....left click "Draw Observed" and drag to empty area....click the 
</p>
<p>green rectangle and a colorless rectangle appears....left click it and a red rectangle 
</p>
<p>appears....do this 3 more times and have the rectangles at different places    
</p>
<p> The underneath fi gure shows how your screen will look by now. There are four 
</p>
<p>rectangle nodes for observed variables. 
</p>
<p>    
</p>
<p>    Next we will have to enter the names of the variables.
</p>
<p>  Command: 
</p>
<p>  right click in the left upper rectangle....click Object Properties....Variable name: 
</p>
<p>type "noncompliance"....close dialog box....the name is now in the rectangle....do 
</p>
<p>the same for the other three rectangles.    
</p>
<p>    
</p>
<p>    Next arrows have to be added to the diagram.
</p>
<p>  Command: 
</p>
<p>  click Diagram....click Draw Covariances.    
</p>
<p>    
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>304
</p>
<p>    The above fi gure is now in the empty area. We will subsequently perform the 
</p>
<p>analysis.
</p>
<p>  Command: 
</p>
<p>  Analyze....Calculate Estimates....click File....click Save as....Browse for the folder 
</p>
<p>of your choice and enter a name....click Save....click the new path diagram button 
</p>
<p>that has appeared in the upper white rectangle left from the empty area.    
</p>
<p>  99,11
</p>
<p>30,00
</p>
<p>29,45 9,46
46,45
</p>
<p>28,98
</p>
<p>96,58
</p>
<p>stool
counseling noncompliance
</p>
<p>qol
</p>
<p>14,26
43,05
</p>
<p>252,45
</p>
<p>  
</p>
<p>    Unstandardized covariances of the variables are now in the graph and variances 
</p>
<p>of the variables are in the right upper corner of the nodes. 
</p>
<p> We will also view the text output.
</p>
<p>  Command: 
</p>
<p>  click View....click Text output.    
</p>
<p>    
</p>
<p>    The above table shows the same values as the graph did, but p-values are added 
</p>
<p>to the covariances. All of them except stool versus counseling were statistically 
</p>
<p>signifi cant with p-values from 0.002 to 0.038, meaning that all of these variables 
</p>
<p>were closer associated with one another than could happen by chance. 
</p>
<p>49 Structural Equation Modeling (SEM) with SPSS Analysis of Moment Structures&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>305
</p>
<p> Unstandardized covariances of variables with different units are not appropriate, 
</p>
<p>and, therefore, Amos also produces standardized values (= unstandardized divided 
</p>
<p>by their own standard errors).
</p>
<p>   click View....click Analysis Properties....click Output tab....mark Standardized esti-
</p>
<p>mates....close dialog box....choose Analyze....click Calculate Estimates....click the 
</p>
<p>path diagram button....click standardized estimates in third white rectangle left from 
</p>
<p>the empty area.    
</p>
<p>  ,63
</p>
<p>,47
,48
</p>
<p>,38 ,45,79
</p>
<p>stool
counseling noncompliance
</p>
<p>qol
</p>
<p>  
</p>
<p>    The standardized covariances is given. 
</p>
<p> Finally, we will view the standardized results as table.
</p>
<p>   click View....click Text Output.    
</p>
<p>   
</p>
<p>    The standardized covariances are in the column entitled Correlation.  
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>306
</p>
<p>    Conclusion 
</p>
<p> SEM modeling can estimate covariances with their standard error and p-values. 
</p>
<p>Signifi cant p-values mean that the association of the variables is statistically signifi -
</p>
<p>cant and that the paired data are thus closer to one another than could happen by 
</p>
<p>chance. The analyses of covariances is a basic methodology of SEM modeling used 
</p>
<p>for testing and making clinical inferences like the presence of meaningful cause 
</p>
<p>effect relationships like pharmacodynamic relationships.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of structural equation 
</p>
<p>models like path analysis, factor analysis, and regression models are in (1) Chap. 
</p>
<p>14, Factor analysis, pp 167&ndash;181, in: Machine learning in medicine part 1, Springer 
</p>
<p>Heidelberg Germany, 2013, in (2) the Chap. 2, Multistage regression, in: SPSS for 
</p>
<p>starters part two, pp 3&ndash;6, Springer Heidelberg Germany, 2012, and in (3) the Chap. 
</p>
<p>14, Linear regression, basic approach, in: Statistics applied to clinical studies 5th 
</p>
<p>edition, pp 161&ndash;176, Springer Heidelberg Germany, 2012, all from the same authors.    
</p>
<p>49 Structural Equation Modeling (SEM) with SPSS Analysis of Moment Structures&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>   Part III 
</p>
<p>   Rules Models        </p>
<p/>
</div>
<div class="page"><p/>
<p>309&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_50
</p>
<p>    Chapter 50   
</p>
<p> Neural Networks for Assessing Relationships 
That Are Typically Nonlinear (90 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> Unlike regression analysis which uses algebraic functions for data fi tting, neural 
</p>
<p>networks uses a stepwise method called the steepest decent method for the purpose. 
</p>
<p>To asses whether typically nonlinear relationships can be adequately fi t by this 
</p>
<p>method.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Body surface is a better indicator for drug dosage than body weight. The relation-
</p>
<p>ship between body weight, length and surface are typically nonlinear. Can a neural 
</p>
<p>network be trained to predict body surface of individual patients. 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 1&rdquo; as Chap. 
</p>
<p>13, 2013. </p>
<p/>
</div>
<div class="page"><p/>
<p>310
</p>
<p> Var 1  Var 2  Var 3 
</p>
<p> 30,50  138,50  10072,90 
</p>
<p> 15,00  101,00  6189,00 
</p>
<p> 2,50  51,50  1906,20 
</p>
<p> 30,00  141,00  10290,60 
</p>
<p> 40,50  154,00  13221,60 
</p>
<p> 27,00  136,00  9654,50 
</p>
<p> 15,00  106,00  6768,20 
</p>
<p> 15,00  103,00  6194,10 
</p>
<p> 13,50  96,00  5830,20 
</p>
<p> 36,00  150,00  11759,00 
</p>
<p> 12,00  92,00  5299,40 
</p>
<p> 2,50  51,00  2094,50 
</p>
<p> 19,00  121,00  7490,80 
</p>
<p> 28,00  130,50  9521,70 
</p>
<p>  Var 1 weight (kg) 
</p>
<p> Var 2 height (m) 
</p>
<p> Var 3 body surface measured photometrically (cm 2 ) 
</p>
<p>    The fi rst 14 patients are shown only, the entire data fi le is entitled &ldquo;neuralnetworks&rdquo; 
</p>
<p>and is in extras.springer.com.  
</p>
<p>    The Computer Teaches Itself to Make Predictions 
</p>
<p> SPSS 19.0 is used for training and outcome prediction. It uses XML (eXtended 
</p>
<p>Markup Language) fi les to store data. MLP stands for multilayer perceptron, and 
</p>
<p>indicates the neural network methodology used.
</p>
<p>  Command: 
</p>
<p>  Click Transform&hellip;.click Random Number Generators&hellip;.click Set Starting 
</p>
<p>Point&hellip;.click Fixed Value (2,000,000)&hellip;.click OK&hellip;.click Analyze&hellip;.Neural 
</p>
<p>Networks&hellip;. Multilayer Perceptron&hellip;.Dependent Variable: select body surface 
</p>
<p>&hellip;.Factors: select weight and height&hellip;.click Partitioning: set the training sam-
</p>
<p>ple (7), test sample (3), hold out sample (0)&hellip;.click Architecture: click Custom 
</p>
<p>Architecture&hellip;.set the numbers of hidden layers (2)&hellip;.click Activation Function: 
</p>
<p>click hyperbolic tangens&hellip;.click Save: click Save predicted values or category 
</p>
<p>for each dependent variable&hellip;.click Export: click Export synaptic weight esti-
</p>
<p>mates to XML fi le&hellip;.click Browse&hellip;.File name: enter &ldquo;exportnn&rdquo;&hellip;.click 
</p>
<p>Save&hellip;.Options: click Maximum training time Minutes (15)&hellip;.click OK.    
</p>
<p>50 Neural Networks for Assessing Relationships That Are Typically Nonlinear&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>311
</p>
<p> The output warns that in the testing sample some cases have been excluded from 
</p>
<p>analysis because of values not occurring in the training sample. Minimizing the 
</p>
<p>output sheets shows the data fi le with predicted values (MLP_PredictedValue). 
</p>
<p> They are pretty much similar to the measured body surface values. We will use 
</p>
<p>linear regression to estimate the association between the two.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Regresssion&hellip;.Linear&hellip;.Dependent: bodysurface &hellip;.Independent: 
</p>
<p>MLP_PredictedValue&hellip;.OK.    
</p>
<p> The output sheets show that the r-value is 0.998, r-square 0.995, p &lt; 0.0001. The 
</p>
<p>saved XML fi le will now be used to compute the body surface in fi ve individual 
</p>
<p>patients. 
</p>
<p> patient no  weight  height 
</p>
<p> 1  36,00  130,50 
</p>
<p> 2  28,00  150,00 
</p>
<p> 3  12,00  121,00 
</p>
<p> 4  19,00  92,00 
</p>
<p> 5  2,50  51,00 
</p>
<p>   Enter the above data in a new SPSS data fi le.
</p>
<p>  Command: 
</p>
<p>  Utilities&hellip;.click Scoring Wizard&hellip;.click Browse&hellip;.click Select&hellip;.Folder: enter the 
</p>
<p>exportnn.xml fi le&hellip;.click Select&hellip;.in Scoring Wizard click Next&hellip;.click Use value 
</p>
<p>substitution&hellip;.click Next&hellip;.click Finish.    
</p>
<p> The above data fi le now gives the body surfaces computed by the neural network 
</p>
<p>with the help of the XML fi le. 
</p>
<p> Patient no  weight  height  computed body surfaces 
</p>
<p> 1  36,00  130,50  10290,23 
</p>
<p> 2  28,00  150,00  11754,33 
</p>
<p> 3  12,00  121,00  7635,97 
</p>
<p> 4  19,00  92,00  4733,40 
</p>
<p> 5  2,50  51,00  2109,32 
</p>
<p>       Conclusion 
</p>
<p> Multilayer perceptron neural networks can be readily trained to provide accurate 
</p>
<p>body surface values of individual patients, and other nonlinear clinical outcomes.  
</p>
<p> Conclusion</p>
<p/>
</div>
<div class="page"><p/>
<p>312
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of neural networks is 
</p>
<p>available in Machine learning in medicine part one, Chaps. 12 and 13, entitled &ldquo;Artifi cial 
</p>
<p>intelligence, multilayer perceptron&rdquo; and &ldquo;Artifi cial intelligence, radial basis functions&rdquo;, 
</p>
<p>pp 145&ndash;156 and 157&ndash;166, Springer Heidelberg Germany 2013, and the Chap. 63.    
</p>
<p>50 Neural Networks for Assessing Relationships That Are Typically Nonlinear&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>313&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_51
</p>
<p>    Chapter 51   
</p>
<p> Complex Samples Methodologies for Unbiased 
Sampling (9,678 Persons) 
</p>
<p>                      General Purpose 
</p>
<p> The research of entire populations is costly and obtaining information from selected 
</p>
<p>samples instead is generally biased by selection bias. Complex sampling produces 
</p>
<p>weighted, and, therefore, unbiased population estimates. This chapter is to assess 
</p>
<p>whether this method can be trained for predicting health outcomes.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Can complex samples be trained to predict unbiased current health outcomes from 
</p>
<p>previous health outcomes in individual members of an entire population. 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 1&rdquo; as Chap. 
</p>
<p>14, 2013. </p>
<p/>
</div>
<div class="page"><p/>
<p>314
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5  Var 6 
</p>
<p> 1  1  1  9  19,26  3,00 
</p>
<p> 1  1  1  7  21,11  6,00 
</p>
<p> 1  1  1  9  22,42  9,00 
</p>
<p> 1  1  1  7  20,13  12,00 
</p>
<p> 1  1  1  5  16,37  15,00 
</p>
<p> 1  1  1  8  20,49  18,00 
</p>
<p> 1  1  1  7  20,79  21,00 
</p>
<p> 1  1  1  7  17,52  24,00 
</p>
<p> 1  1  1  7  18,12  27,00 
</p>
<p> 1  1  1  6  18,60  30,00 
</p>
<p>  Var 1 neighborhood 
</p>
<p> Var 2 town 
</p>
<p> Var 3 county 
</p>
<p> Var 4 time (years) 
</p>
<p> Var 5 last health score 
</p>
<p> Var 6 case identity number (defi ned as property ID) 
</p>
<p>    Prior health scores of a 9,768 member population recorded some 5&ndash;10 years ago 
</p>
<p>were available as well as topographical information (the data fi le is entitled &ldquo;com-
</p>
<p>plexsamples&rdquo; and is in extras.springer.com). We wish to obtain information of individ-
</p>
<p>ual current health scores. For that purpose the information of the entire data plus 
</p>
<p>additional information on the current health scores from a random sample of 1,000 from 
</p>
<p>this population were used. First, a  sampling plan  was designed with different counties, 
</p>
<p>townships and neighborhoods weighted differently. A  random sample  of 1,000 was 
</p>
<p>taken, and additional information was obtained from this random sample, and included. 
</p>
<p> The latter data fi le plus the  sampling plan  were, then, used for analysis. The 
</p>
<p>SPSS modules complex samples (cs) &ldquo;general linear model&rdquo; and &ldquo;ratios&rdquo; modules 
</p>
<p>were applied for analyses. A  sampling plan  of the above population data was 
</p>
<p>designed using SPSS. Open in extras.springer.com the database entitled 
</p>
<p>&ldquo;complexsamples&rdquo;.
</p>
<p>  Command: 
</p>
<p>  click Analyze&hellip;.Complex Samples&hellip;. Select a sample&hellip;. click Design a sample, 
</p>
<p>click Browse: select a map and enter a name, e.g., complexsamplesplan&hellip;.click 
</p>
<p>Next&hellip;.Stratify by: select county&hellip;.Clusters: select township&hellip;.click Next&hellip;Type: 
</p>
<p>Simple Random Sampling&hellip;.click Without replacement&hellip;.click Next&hellip;.Units: 
</p>
<p>enter Counts&hellip;.click Value: enter 4&hellip;.click Next&hellip;.click Next&hellip;.click (Yes, add 
</p>
<p>stage 2 now)&hellip;.click Next&hellip;Stratify by: enter neighbourhood&hellip;.next&hellip;Type: 
</p>
<p>Simple random sampling&hellip;.click Without replacement&hellip;.click Next&hellip;.Units: enter 
</p>
<p>proportions&hellip;.click Value: enter 0,25&hellip;.click Next&hellip;.click Next&hellip;.click (No, do 
</p>
<p>not add another stage now)&hellip;.click Next&hellip;Do you want to draw a sample: click 
</p>
<p>Yes&hellip;.Click Custom value&hellip;.enter 123&hellip;.click Next&hellip;.click External fi le, click 
</p>
<p>Browse: select a map and enter a name, e.g., complexsamplessample &hellip;.click 
</p>
<p>Save&hellip;.click Next&hellip;.click Finish.    
</p>
<p>51 Complex Samples Methodologies for Unbiased Sampling (9,678 Persons)</p>
<p/>
</div>
<div class="page"><p/>
<p>315
</p>
<p> In the original data fi le the weights of 1,006 randomly sampled individuals are 
</p>
<p>now given. In the maps selected above we fi nd two new fi les,
</p>
<p>    1.    entitled &ldquo;complexsamplesplan&rdquo; (this map can not be opened, but it can in closed 
</p>
<p>form be entered whenever needed during further complex samples analyses of 
</p>
<p>these data), and   
</p>
<p>   2.    entitled &ldquo;complexsamplessample&rdquo; containing 1,006 randomly selected individuals 
</p>
<p>from the main data fi le.     
</p>
<p> The latter data fi le is fi rst completed with current health scores before the defi ni-
</p>
<p>tive analysis. Only of 974 individuals the current information could be obtained, and 
</p>
<p>these data were added as a new variable (see &ldquo;complexsamplessample&rdquo; at extras.
</p>
<p>springer.com). Also &ldquo;complexsamplesplan&rdquo; has for convenience been made avail-
</p>
<p>able at extras.springer.com.  
</p>
<p>    The Computer Teaches Itself to Predict Current Health Scores 
</p>
<p>from Previous Health Scores 
</p>
<p> We now use the above data fi les &ldquo;complexsamplessample&rdquo; and &ldquo;complexsamples-
</p>
<p>plan&rdquo; for predicting individual current health scores and odds ratios of current 
</p>
<p>versus previous health scores. Also, an XML (eXtended Markup Language) fi le will 
</p>
<p>be designed for analyzing future data. First, open &ldquo;complexsamplessample&rdquo;.
</p>
<p>  Command: 
</p>
<p>  Click Transform&hellip;.click Random Number Generators&hellip;.click Set Starting Point&hellip;.
</p>
<p>click Fixed Value (2000000)&hellip;.click OK&hellip;.click Analyze&hellip;.Complex Samples&hellip;.
</p>
<p>General Linear Model&hellip;.click Browse: select the appropriate map and enter com-
</p>
<p>plexsamplesplan&hellip;.click Continue&hellip;Dependent variable: enter curhealthscore &hellip;.
</p>
<p>Covariates: enter last healthscores&hellip;.click Statistics: mark Estimates, 95 % 
</p>
<p>Confi dence interval, t-test&hellip;.click Save&hellip;.mark Predicted Values&hellip;.in Export 
</p>
<p>Model as XML click Browse&hellip;.in appropriate folder enter File name: "exportcs-
</p>
<p>lin"&hellip;.click Save&hellip;.click Continue&hellip;.click OK.    
</p>
<p> The underneath table gives the correlation coeffi cient and the 95 % confi dence 
</p>
<p>intervals. The lower part gives the data obtained through the usual commands 
</p>
<p>(Analyze, Regression, Linear, Dependent (curhealthscore), Independent (s) (last 
</p>
<p>healthscore), OK). It is remarkable to observe the differences between the two anal-
</p>
<p>yses. The correlation coeffi cients are largely the same but their standard errors are 
</p>
<p>respectively 0.158 and 0.044. The t-value of the complex sampling analysis equals 
</p>
<p>5.315, while that of the traditional analysis equals no less than 19.635. Nonetheless, 
</p>
<p>the reduced precision of the complex sampling analysis did not produce a statisti-
</p>
<p>cally insignifi cant result, and, in addition, it was, of course, again adjusted for inap-
</p>
<p>propriate probability estimates.
</p>
<p>The Computer Teaches Itself to Predict Current Health Scores from Previous Health&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>316
</p>
<p> Parameter estimates a  
</p>
<p> Parameter  Estimate  Std. error 
</p>
<p> 95 % confi dence 
</p>
<p>interval  Hypothesis test 
</p>
<p> Lower  Upper  t  df  Sig. 
</p>
<p> (Intercept)  8,151  2,262  3,222  13,079  3,603  12,000  ,004 
</p>
<p> Lasthealthscore  ,838  ,158  ,494  1,182  5,315  12,000  ,000 
</p>
<p>   a Model: curhealthscore = (Intercept) + lasthealthscore 
</p>
<p> Coeffi cients a  
</p>
<p> Model 
</p>
<p> Unstandardized 
</p>
<p>coeffi cients 
</p>
<p> Standardized 
</p>
<p>coeffi cients 
</p>
<p> B  Std. error  Beta  t  Sig. 
</p>
<p> 1  (Constant)  7,353  ,677  10,856  ,000 
</p>
<p> Last healthscore  ,864  ,044  ,533  19,635  ,000 
</p>
<p>   a Dependent Variable: curhealthscore 
</p>
<p>    The saved XML fi le will now be used to compute the predicted current health 
</p>
<p>score in fi ve individual patients from this population. 
</p>
<p> Var 5 
</p>
<p> 1  19,46 
</p>
<p> 2  19,77 
</p>
<p> 3  16,75 
</p>
<p> 4  16,37 
</p>
<p> 5  18,35 
</p>
<p>  Var 5 Last health score 
</p>
<p>    Enter the above data in a new SPSS data fi le.
</p>
<p>  Command: 
</p>
<p>  Utilities&hellip;.click Scoring Wizard&hellip;.click Browse&hellip;.click Select&hellip;.Folder: enter the 
</p>
<p>exportcslin.xml fi le&hellip;.click Select&hellip;.in Scoring Wizard click Next&hellip;.mark 
</p>
<p>Predicted Value&hellip;.click Next&hellip;.click Finish.    
</p>
<p> The above data fi le now gives the predicted current health scores with the help of 
</p>
<p>the XML fi le. 
</p>
<p> Var 5  Var 6 
</p>
<p> 1  19,46  24,46 
</p>
<p> 2  19,77  24,72 
</p>
<p> 3  16,75  22,19 
</p>
<p> 4  16,37  21,87 
</p>
<p> 5  18,35  23,53 
</p>
<p>  Var 5 last health score 
</p>
<p> Var 6 predicted value of current health score 
</p>
<p>51 Complex Samples Methodologies for Unbiased Sampling (9,678 Persons)</p>
<p/>
</div>
<div class="page"><p/>
<p>317
</p>
<p>        The Computer Teaches Itself to Predict Individual 
</p>
<p>Odds Ratios of Current Health Scores Versus Previous 
</p>
<p>Health Scores 
</p>
<p> Open again the data fi le &ldquo;complexsamplessample&rdquo;.
</p>
<p>  Command: 
</p>
<p>  Click Transform&hellip;.click Random Number Generators&hellip;.click Set Starting Point&hellip;. 
</p>
<p>click Fixed Value (2000000)&hellip;.click OK&hellip;.click Analyze&hellip;.Complex Samples 
</p>
<p>&hellip;.Ratios&hellip;.click Browse: select the appropriate map and enter "complexsamples-
</p>
<p>plan"&hellip;.click Continue&hellip;Numerators: enter curhealthscore&hellip;. Denominator: enter 
</p>
<p>last healthscore &hellip;.Subpopulations: enter County&hellip;.click Statistics: mark Standard 
</p>
<p>error, Confi dence interval (enter 95 %), Design effect&hellip;.click Continue&hellip;.click OK.    
</p>
<p> The underneath table (upper part) gives the overall ratio and the ratios per county 
</p>
<p>plus 95 % confi dence intervals. The design effects are the ratios of the variances of 
</p>
<p>the complex sampling method versus that of the traditional, otherwise called simple 
</p>
<p>random sampling (srs), method. In the given example the ratios are mostly 3&ndash;4, 
</p>
<p>which means that the uncertainty of the complex samples methodology is 3&ndash;4 times 
</p>
<p>larger than that of the traditional method. However, this reduction in precision is 
</p>
<p>compensated for by the removal of biases due to the use of inappropriate probabili-
</p>
<p>ties used in the srs method. 
</p>
<p> The lower part of the table gives the srs data obtained through the usual com-
</p>
<p>mands (Analyze, Descriptive Statistics, Ratio, Numerator (curhealthscore), 
</p>
<p>Denominator (lasthealthscore), Group Variable (County), Statistics (means, confi -
</p>
<p>dence intervals etc)). Again the ratios of the complex samples and traditional analy-
</p>
<p>ses are rather similar, but the confi dence intervals are very different. E.g., the 95 % 
</p>
<p>confi dence intervals of the Northern County went from 1.172 to 1.914 in the com-
</p>
<p>plex samples, and from 1.525 to 1.702 in the traditional analysis, and was thus over 
</p>
<p>3 times wider.
</p>
<p> Ratios 1 
</p>
<p> Numerator  Denominator 
</p>
<p> Ratio 
</p>
<p>estimate 
</p>
<p> Standard 
</p>
<p>error 
</p>
<p> 95 % confi dence 
</p>
<p>interval  Design 
</p>
<p>effect  Lower  Upper 
</p>
<p> Curhealthscore  Last 
</p>
<p>healthscore 
</p>
<p> 1,371  ,059  1,244  1,499  17,566 
</p>
<p>The Computer Teaches Itself to Predict Individual Odds Ratios of Current Health&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>318
</p>
<p> Ratios 1 
</p>
<p> Country  Numerator  Denominator 
</p>
<p> Ratio 
</p>
<p>estimate 
</p>
<p> Standard 
</p>
<p> error 
</p>
<p> 95 % 
</p>
<p>confi dence 
</p>
<p>interval 
</p>
<p> Lower  Upper 
</p>
<p> Design 
</p>
<p>effect 
</p>
<p> Eastern  Curhealthscore  Last 
</p>
<p>healthscore 
</p>
<p> 1,273  ,076  1,107  1,438  12,338 
</p>
<p> "Southern  Curhealthscore  Last 
</p>
<p>healthscore 
</p>
<p> 1,391  ,100  1,174  1,608  21,895 
</p>
<p> "Western  Curhealthscore  Last 
</p>
<p>healthscore 
</p>
<p> 1,278  ,039  1,194  1,362  1,518 
</p>
<p> Northern  Curhealthscore  Last 
</p>
<p>healthscore 
</p>
<p> 1,543  ,170  1,172  1,914  15,806 
</p>
<p> Ratio statistics for curhealthscore/last healthscore 
</p>
<p> Group  Mean 
</p>
<p> 95 % confi dence 
</p>
<p>interval tor mean 
</p>
<p> Price related 
</p>
<p>differential 
</p>
<p> Coeffi cient 
</p>
<p>of dispersion 
</p>
<p> Coeffi cient 
</p>
<p>of variation 
</p>
<p> Lower 
</p>
<p>bound 
</p>
<p> Upper 
</p>
<p>bound 
</p>
<p> Median 
</p>
<p>centered 
</p>
<p> Eastern  1,282  1,241  1,323  1,007  ,184  24,3 % 
</p>
<p> "Southern  1,436  1,380  1,492  1,031  ,266  33,4 % 
</p>
<p> "Western  1,342  1,279  1,406  1,051  ,271  37,7 % 
</p>
<p> Northern  1,613  1,525  1,702  1,044  ,374  55,7 % 
</p>
<p> Overall  1,429  1,395  1,463  1,047  ,285  41,8 % 
</p>
<p>  The confi dence intends are constructed by assuming a Normal distribution for the ratios 
</p>
<p>    In addition to the statistics given above, other complex samples statistics are 
</p>
<p>possible, and they can be equally well executed in SPSS, that is if the data are 
</p>
<p>appropriate. If you have a binary outcome variable (dichotomous) available, then 
</p>
<p>logistic regression modeling is possible, if an ordinal outcome variable is available, 
</p>
<p>complex samples ordinal regression, if time to event information is in the data, 
</p>
<p>complex samples Cox regression can be performed.  
</p>
<p>    Conclusion 
</p>
<p> Complex samples is a cost-effi cient method for analyzing target populations that are 
</p>
<p>large and heterogeneously distributed. Also it is time-effi cient, and offers greater 
</p>
<p>scope and deeper insight, because specialized equipments are feasible. 
</p>
<p> Traditional analysis of limited samples from heterogeneous target populations is 
</p>
<p>a biased methodology, because each individual selected is given the same probabil-
</p>
<p>ity, and the spread in the data is, therefore, generally underestimated. In complex 
</p>
<p>sampling this bias is adjusted for by assigning appropriate weights to each individ-
</p>
<p>ual included.  
</p>
<p>51 Complex Samples Methodologies for Unbiased Sampling (9,678 Persons)</p>
<p/>
</div>
<div class="page"><p/>
<p>319
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of complex samples 
</p>
<p>methodologies is given in Machine learning in medicine part three, Chap. 12, 
</p>
<p>Complex samples, pp 127&ndash;139, Springer Heidelberg Germany 2013.    
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>321&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_52
</p>
<p>Chapter 52
</p>
<p>Correspondence Analysis for Identifying 
the Best of Multiple Treatments in Multiple 
Groups (217 Patients)
</p>
<p> General Purpose
</p>
<p>Multiple treatments for one condition are increasingly available, and a systematic 
</p>
<p>assessment would serve optimal care. Research in this field to date is problematic.
</p>
<p>This chapter is to propose a novel method based on cross-tables, correspondence 
</p>
<p>analysis.
</p>
<p> Specific Scientific Question
</p>
<p>Can correspondence analysis avoid the bias of multiple testing, and identify the best 
</p>
<p>of multiple treatments in multiple groups
</p>
<p>This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 1&rdquo; as Chap. 
</p>
<p>15, 2013.</p>
<p/>
</div>
<div class="page"><p/>
<p>322
</p>
<p>Var 1 Var 2
</p>
<p>1 1
</p>
<p>1 1
</p>
<p>1 1
</p>
<p>1 1
</p>
<p>1 1
</p>
<p>1 1
</p>
<p>1 1
</p>
<p>1 1
</p>
<p>1 1
</p>
<p>1 1
</p>
<p>1 1
</p>
<p>1 1
</p>
<p>Var 1 treatment modality (1&ndash;3)
</p>
<p>Var 2 response (1 = complete remission, 2 = par-
</p>
<p>tial remission, 3 = no response) 
</p>
<p>Only the first 12 patients are given, the entire 
</p>
<p>data file entitled &ldquo;correspondenceanalysis&rdquo; is in 
</p>
<p>extras.springer.com. 217 patients were ran-
</p>
<p>domly treated with one of three treatments 
</p>
<p>(treat = treatment) and produced one of three 
</p>
<p>responses (1 = complete remission, 2 = partial 
</p>
<p>remission, 3 = no response). We will use SPSS 
</p>
<p>statistical software 19.0
</p>
<p> Correspondence Analysis
</p>
<p>First, a multiple groups chi-square test is performed. Start by opening the data file.
</p>
<p>Command:
</p>
<p>Analyze....Descriptive Statistics&hellip;.Crosstabs&hellip;.Row(s): enter treatment&hellip;. 
</p>
<p>Column(s): enter remission, partial, no [Var 2]&hellip;.click Statistics&hellip;.mark Chi- 
</p>
<p>square&hellip;.click Continue&hellip;.click Cell Display &hellip;.mark Observed&hellip;.mark Expected 
</p>
<p>&hellip;.click Continue&hellip;.OK.
</p>
<p>52 Correspondence Analysis for Identifying the Best of Multiple Treatments&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>323
</p>
<p>Treatment * remission, partial, no Crosstabulation
</p>
<p>Remission, partial, no
</p>
<p>Total1,00 2,00 3,00
</p>
<p>Treatment 1,00 Count 19 21 18 58
</p>
<p>Expected count 21,6 10,7 25,7 58,0
</p>
<p>2,00 Count 41 9 39 89
</p>
<p>Expected count 33,2 16,4 39,4 89,0
</p>
<p>3,00 Count 21 10 39 70
</p>
<p>Expected count 26,1 12,9 31,0 70,0
</p>
<p>Total Count 81 40 96 217
</p>
<p>Expected count 81,0 40,0 96,0 217,0
</p>
<p>The output file compares the observed counts (patients) per cell with the expected 
</p>
<p>count, if no significant difference existed. Also, a chi-square value is given, 21.462 with 
</p>
<p>4&deg; of freedom, p-value &lt; 0.0001. There is a significantly different pattern in numbers of 
</p>
<p>responders between the different treatment groups. To find out what treatment is best a 
</p>
<p>correspondence analysis is performed. For that purpose the individual chi-square values 
</p>
<p>are calculated from the values of the above table according to the underneath equation.
</p>
<p> 
observed count expected count expected count&ndash; /( )&eacute;&euml;
</p>
<p>&ugrave;
&ucirc;
</p>
<p>2
</p>
<p> 
</p>
<p>Then, the individual chi-square values are converted to similarity measures. With these 
</p>
<p>values the software program creates a two-dimensional quantitative distance measure 
</p>
<p>that is used to interpret the level of nearness between the treatment groups and response 
</p>
<p>groups. We will use again SPSS 19.0 statistical software for the analysis.
</p>
<p>Command:
</p>
<p>Analyze....Dimension Reduction....Correspondence Analysis ....Row: enter treat-
</p>
<p>ment&hellip;.click Define Range&hellip;.Minimum value: enter1&hellip;.Maximum value: enter 
</p>
<p>3&hellip;.click Update&hellip;.Column: enter remission, partial, no [Var 2] &hellip;.click Define 
</p>
<p>Range&hellip;.Minimum value: enter1&hellip;.Maximum value: enter 3&hellip;.click Update&hellip;.
</p>
<p>click Continue &hellip;.click Model&hellip;.Distance Measure: click Chi square&hellip;.click 
</p>
<p>Continue&hellip;.click Plots&hellip;.mark Biplot&hellip;.OK.
</p>
<p>Remission
</p>
<p>Treatment yes partial no
</p>
<p>1 residual &minus;2,6 10,3 &minus;7,7
</p>
<p>(o-e)2/e 0,31 9,91 2,31
</p>
<p>similarity &minus;0,31 9,91 &minus;2,31
</p>
<p>2 residual 7,9 &minus;7,4 &minus;0,4
</p>
<p>(o-e)2/e 1,88 3,34 0,004
</p>
<p>similarity 1,88 &minus;3,34 &minus;0.004
</p>
<p>3 residual &minus;4,1 &minus;2,9 8,0
</p>
<p>(o-e)2/e 0,64 0.65 2,65
</p>
<p>similarity &minus;0,64 &minus;0,65 2,65
</p>
<p>Correspondence Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>324
</p>
<p>The above table of similarity values is given in the output. Also the underneath 
</p>
<p>plot of the coordinates of both the treatment groups and the response groups in a one 
</p>
<p>two-dimensional plane is shown in the output. This plot is meaningful. As treatment 
</p>
<p>group 2 and response group 1 tend to join, and treatment group 1 and response 
</p>
<p>group 2 do, equally, so, we have reason to believe that treatment group 2 has the best 
</p>
<p>treatment and treatment group 1 the second best. This is, because response group 1 
</p>
<p>has a complete remission, and response group 2 has a partial remission. If a 2 &times; 2 
</p>
<p>table of the treatment groups 1 and 2 versus the response groups 1 and 2 shows a 
</p>
<p>significant difference between the treatments, then we can argue, that the best treat-
</p>
<p>ment is, indeed, significantly better than the second best treatment.
</p>
<p> 
</p>
<p>For statistical testing response 1 and 2 versus treatment 1 and 2 recoding of the 
</p>
<p>variables is required, but a simpler solution is to use a pocket calculator method for 
</p>
<p>computing the chi-square value.
</p>
<p>response
</p>
<p>treatment 1 2 total
</p>
<p>1 19 21 40
</p>
<p>2 41 9 50
</p>
<p>60 30 90
</p>
<p> 
</p>
<p>Chi -square with degr=
&times;( ) &minus; &times;( )  &times;
</p>
<p>&times; &times; &times;
=
</p>
<p>9 19 21 41 90
</p>
<p>60 30 50 40
11 9 1
</p>
<p>2
</p>
<p>. eeeof freedom p, .&lt; 0 0001
</p>
<p>52 Correspondence Analysis for Identifying the Best of Multiple Treatments&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>325
</p>
<p>Treatment 2, indeed, produced significantly more complete remissions than did 
</p>
<p>treatment 1, as compared to the partial remissions.
</p>
<p> Conclusion
</p>
<p>In our example correspondence analysis was able to demonstrate which one of three 
</p>
<p>treatments was best, and it needed, instead of multiple 2 &times; 2 tables, only a single 
</p>
<p>2 &times; 2 table for that purpose. The advantage of this procedure will be even more obvi-
</p>
<p>ous, if larger sets of categorical data have to be assessed. A nine cells data file would 
</p>
<p>require only nine 2 &times; 2 tables to be tested, a sixteen cells data file would require 
</p>
<p>thirty-six of them. This procedure will almost certainly produce significant effects 
</p>
<p>by chance rather than true effects, and is, therefore, rather meaningless. In contrast, 
</p>
<p>very few tests are needed, when a correspondence analysis is used to identify the 
</p>
<p>proximities in the data, and the risk of type I errors is virtually negligible.
</p>
<p> Note
</p>
<p>We should add that, instead of a two-dimensional analysis as used in the current 
</p>
<p>chapter, correspondence analysis can also be applied for multidimensional analyses. 
</p>
<p>More background, theoretical and mathematical information of correspondence 
</p>
<p>analysis is given in Machine learning in medicine part two, Chap. 13, Correspondence 
</p>
<p>analysis, pp 129&ndash;137, Springer Heidelberg Germany 2013.
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>327&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_53
</p>
<p>    Chapter 53   
</p>
<p> Decision Trees for Decision Analysis (1,004 
</p>
<p>and 953 Patients)      
</p>
<p>                 General Purpose 
</p>
<p> Decision trees are, so-called, non-metric or non-algorithmic methods adequate for 
</p>
<p>fi tting nominal and interval data (the latter either categorical or continuous). Better 
</p>
<p>accuracy from decision trees is sometimes obtained by the use of a training sample 
</p>
<p>(Chap.   8    ). This chapter is to assess whether decision trees can be appropriately 
</p>
<p>applied to predict health risks and improvements.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Can decision trees be trained to predict in individual future patients risk of infarction 
</p>
<p>and ldl (low density lipoprotein) cholesterol decrease.  
</p>
<p>    Decision Trees with a Binary Outcome 
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5  Var 6 
</p>
<p> ,00  44,86  1,00  ,00  1,00  2,00 
</p>
<p> ,00  42,71  2,00  ,00  1,00  2,00 
</p>
<p> ,00  43,34  3,00  ,00  2,00  2,00 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 1&rdquo; as Chap. 16, 
</p>
<p>2013. 
</p>
<p>(continued)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_8">http://dx.doi.org/10.1007/978-3-319-15195-3_8</a></div>
</div>
<div class="page"><p/>
<p>328
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5  Var 6 
</p>
<p> ,00  44,02  3,00  ,00  1,00  2,00 
</p>
<p> ,00  67,97  1,00  ,00  2,00  2,00 
</p>
<p> ,00  40,31  2,00  ,00  2,00  2,00 
</p>
<p> ,00  66,56  1,00  ,00  2,00  2,00 
</p>
<p> ,00  45,95  1,00  ,00  2,00  2,00 
</p>
<p> ,00  52,27  1,00  ,00  1,00  2,00 
</p>
<p> ,00  43,86  1,00  ,00  1,00  2,00 
</p>
<p> ,00  46,58  3,00  ,00  2,00  1,00 
</p>
<p> ,00  53,83  2,00  ,00  2,00  2,00 
</p>
<p> ,00  49,48  1,00  ,00  2,00  1,00 
</p>
<p>  Var 1 infarct_rating (,00 no, 1,00 yes) 
</p>
<p> Var 2 age (years) 
</p>
<p> Var 3 cholesterol_level (1,00-3,00) 
</p>
<p> Var 4 smoking (,00 no, 1,00 yes) 
</p>
<p> Var 5 education (levels 1,00 and 2,00) 
</p>
<p> Var 6 weight_level (levels 1,00 and 2,00) 
</p>
<p>    The data from the fi rst 13 patients are shown only. See extra.springer.com for the 
</p>
<p>entire data fi le entitled &ldquo;decisiontreebinary&rdquo;: in a 1,004 patient data fi le of risk fac-
</p>
<p>tors for myocardial infarct a so-called chi-squared automatic interaction (CHAID) 
</p>
<p>model is used for analysis. Also an XML (eXtended Markup Language) will be 
</p>
<p>exported for the analysis of future data. Start by opening the data fi le.
</p>
<p>  Command: 
</p>
<p>  Click Transform&hellip;.click Random Number Generators&hellip;.click Set Starting 
</p>
<p>Point&hellip;. click Fixed Value (2000000)&hellip;.click OK&hellip;.click Classify&hellip;.Tree&hellip;.
</p>
<p>Dependent Variable: enter infarct rating&hellip;.Independent Variables: enter age, 
</p>
<p>cholesterol level, smoking, education, weight level&hellip;.Growing Method: select 
</p>
<p>CHAID&hellip;.click Categories: Target mark yes&hellip;.Continue&hellip;.click Output: mark 
</p>
<p>Tree in table format&hellip;.Criteria: Parent Node type 200, Child Node type 100&hellip;.
</p>
<p>click Continue&hellip;. click Save: mark Terminal node number, Predicted probabili-
</p>
<p>ties&hellip;. in Export Tree Model as XML mark Training sample&hellip;.click Browse&hellip;
</p>
<p>.&hellip;.in File name enter "exportdecisiontreebinary" &hellip;.in Look in: enter the 
</p>
<p>appropriate map in your computer for storage&hellip;.click Save&hellip;.click OK.    
</p>
<p>53 Decision Trees for Decision Analysis (1,004 and 953 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>329
</p>
<p>  
</p>
<p>infarct rating
</p>
<p>Node 0
</p>
<p>Category
</p>
<p>Category
</p>
<p>%
</p>
<p>%
</p>
<p>n
</p>
<p>n
</p>
<p>no
</p>
<p>no
</p>
<p>no
yes
</p>
<p>yes
</p>
<p>yes
</p>
<p>&lt;= Low
</p>
<p>&lt;= 51,850 &lt;= 57,188 &gt; 57,188&gt; 51,850
</p>
<p>smoking
Adj. P-value=0,000, Chi.square=19,
</p>
<p>910, df=1 746, df=1
</p>
<p>weight level
Adi. P-value=0,009, Chi-square=6,
</p>
<p>no yes high normal
</p>
<p>(Low, Medium) &gt; Medium
</p>
<p>Age
</p>
<p>7,7 32
92,3 383
</p>
<p>41541,3
</p>
<p>264, df=1
</p>
<p>Adj. P-value=0,001, Chi-square=15,
</p>
<p>cholesterol level
Adj. P-value=0,000, Chi-square=205,
</p>
<p>214, df=2
</p>
<p>313, df=1
</p>
<p>Age
Adi. P-value=0,000, Chi-square=65,
</p>
<p>Node  1
</p>
<p>Total
</p>
<p>Total
</p>
<p>Category % n
</p>
<p>no
</p>
<p>yes
</p>
<p>Node  2
</p>
<p>Total
</p>
<p>Category % n
</p>
<p>no
</p>
<p>12,4
87,6
</p>
<p>22,5
</p>
<p>28 2,1 4
</p>
<p>97,9
</p>
<p>18,8
</p>
<p>185
</p>
<p>189
</p>
<p>198
</p>
<p>226
</p>
<p>yes
</p>
<p>Node  3
</p>
<p>Total
</p>
<p>Category % n
</p>
<p>no
</p>
<p>yes
</p>
<p> Node 6
</p>
<p>Total
</p>
<p>Category % n
</p>
<p>no
</p>
<p>yes
</p>
<p> Node 7
</p>
<p>Total
</p>
<p>Category % n
</p>
<p>no
yes
</p>
<p>Node  11
</p>
<p>Total
</p>
<p>Category % n
</p>
<p>no
yes
</p>
<p>Node  10
</p>
<p>Total
</p>
<p>Category % n
</p>
<p>no
yes
</p>
<p>Node  9
</p>
<p>Total
</p>
<p>Category % n
</p>
<p>no
yes
</p>
<p>Node  8
</p>
<p>Total
</p>
<p>Category % n
</p>
<p>no
yes
</p>
<p>Node  4
</p>
<p>Total
</p>
<p>Category % n
</p>
<p>no
yes
</p>
<p>Node  5
</p>
<p>Total
</p>
<p>100,0
</p>
<p>20,6
</p>
<p>64,5 19,1 86
365
</p>
<p>451
</p>
<p>80,9
</p>
<p>44,9
</p>
<p>89
49
</p>
<p>138
</p>
<p>35,5
</p>
<p>37,9 7,2 20
92,8 257
</p>
<p>27727,6
</p>
<p>62,1
</p>
<p>17,3
</p>
<p>13,9 0,0 17,5 6,0
</p>
<p>94,0
</p>
<p>10,0 100
</p>
<p>94
6
</p>
<p>82,5
</p>
<p>12,5
</p>
<p>22
</p>
<p>104
</p>
<p>126
</p>
<p>100,0
</p>
<p>13,2
</p>
<p>0
</p>
<p>133
</p>
<p>13386,1
</p>
<p>14,3 144
</p>
<p>20
124
</p>
<p>66
</p>
<p>108
</p>
<p>174
</p>
<p>13,7
</p>
<p>79,4 797
207
</p>
<p>1004
</p>
<p>  
</p>
<p>    The output sheets show the decision tree and various tables. The Cholesterol 
</p>
<p>level is the best predictor of the infarct rating. For low cholesterol the cholesterol 
</p>
<p>level is the only signifi cant predictor of infarction: only 35.5 % will have an infarc-
</p>
<p>tion. In the medium and high cholesterol groups age is the next best predictor. In the 
</p>
<p>elderly with medium cholesterol smoking contributes considerably to the risk of 
</p>
<p>infarction. In contrast, in the younger with high cholesterol those with normal 
</p>
<p>weight are slightly more at risk of infarction than those with high weights. For each 
</p>
<p>node (subgroup) the number of cases, the chi-square value, and level of signifi cance 
</p>
<p>is given. A p-value &lt; 0.05 indicates that the difference between the 2 &times; 2 or 3 &times; 2 
</p>
<p>tables of the paired nodes are signifi cantly different from one another. All of the 
</p>
<p>p-values were very signifi cant. 
</p>
<p> The risk and classifi cation tables indicate that the category infarction predicted 
</p>
<p>by the model is wrong in 0.166 = 16.6 % of the cases (underneath table). A correct 
</p>
<p>prediction of 83.4 % is fi ne. However, in those without an infarction no infarction is 
</p>
<p>predicted in only 43.0 % of the cases (underneath table).
</p>
<p> Risk 
</p>
<p> Estimate  Std. error 
</p>
<p> ,166  ,012 
</p>
<p>  Growing Method: CHAID 
</p>
<p> Dependent Variable: infarct rating 
</p>
<p>Decision Trees with a Binary Outcome</p>
<p/>
</div>
<div class="page"><p/>
<p>330
</p>
<p> Classifi cation 
</p>
<p> Observed 
</p>
<p> Predicted 
</p>
<p> No  Yes  Percent correct 
</p>
<p> No  89  118  43,0 % 
</p>
<p> Yes  49  748  93,9 % 
</p>
<p> Overall percentage  13,7 %  86,3 %  83,4 % 
</p>
<p>  Growing Method: CHAID 
</p>
<p> Dependent Variable: infarct rating 
</p>
<p>    When returning to the original data fi le we will observe 3 new variables, (1) the 
</p>
<p>terminal node number, (2) the predicted probabilities of no infarction for each case, 
</p>
<p>(3) the predicted probabilities of yes infarction for each case. In a binary logistic 
</p>
<p>regression it can be tested that the later variables are much better predictors of the 
</p>
<p>probability of infarction than each of the original variables are. The saved XML fi le 
</p>
<p>will now be used to compute the predicted probability of infarct in 6 novel patients 
</p>
<p>with the following characteristics. For convenience the XML fi le is given in extras.
</p>
<p>springer. com. 
</p>
<p> Var 2  Var 3  Var 4  Var 5  Var 6 
</p>
<p> 59,16  2,00  ,00  1,00  2,00 
</p>
<p> 53,42  1,00  ,00  2,00  2,00 
</p>
<p> 43,02  2,00  ,00  2,00  2,00 
</p>
<p> 76,91  3,00  1,00  1,00  1,00 
</p>
<p> 70,53  2,00  ,00  1,00  2,00 
</p>
<p> 47,02  3,00  1,00  1,00  1,00 
</p>
<p>  Var 2 age (years) 
</p>
<p> Var 3 cholesterol_level (1,00-3,00) 
</p>
<p> Var 4 smoking (,00 no, 1,00 yes) 
</p>
<p> Var 5 education (level 1,00 and 2,00) 
</p>
<p> Var 6 weight_level (1,00 and 2,00) 
</p>
<p>    Enter the above data in a new SPSS data fi le.
</p>
<p>  Command: 
</p>
<p>  Utilities&hellip;.click Scoring Wizard&hellip;.click Browse&hellip;.click Select&hellip;.Folder: enter the 
</p>
<p>exportdecisiontreebinary.xml fi le&hellip;.click Select&hellip;.in Scoring Wizard click Next&hellip;.
</p>
<p>mark Node Number&hellip;.mark Probability of Predicted Category&hellip;.click Next&hellip;.
</p>
<p>click Finish.    
</p>
<p> The above data fi le now gives the individual predicted nodes numbers and prob-
</p>
<p>abilities of infarct for the six novel patients as computed by the linear model with 
</p>
<p>the help of the XML fi le. Enter the above data in a new SPSS data fi le. 
</p>
<p>53 Decision Trees for Decision Analysis (1,004 and 953 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>331
</p>
<p> Var 2  Var 3  Var 4  Var 5  Var 6  Var 7  Var 8 
</p>
<p> 59,16  2,00  ,00  1,00  2,00  8,00  ,86 
</p>
<p> 53,42  1,00  ,00  2,00  2,00  1,00  ,64 
</p>
<p> 43,02  2,00  ,00  2,00  2,00  4,00  ,62 
</p>
<p> 76,91  3,00  1,00  1,00  1,00  7,00  ,98 
</p>
<p> 70,53  2,00  ,00  1,00  2,00  8,00  ,86 
</p>
<p> 47,02  3,00  1,00  1,00  1,00  11,00  ,94 
</p>
<p>  Var 2 age 
</p>
<p> Var 3 cholesterol_level 
</p>
<p> Var 4 smoking 
</p>
<p> Var 5 education 
</p>
<p> Var 6 weight_level 
</p>
<p> Var 7 predicted node number 
</p>
<p> Var 8 predicted probability of infarct 
</p>
<p>        Decision Trees with a Continuous Outcome 
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5  Var 6 
</p>
<p> 3,41  0  1  3,00  3  0 
</p>
<p> 1,86  &minus;1  1  2,00  3  1 
</p>
<p> ,85  &minus;2  1  1,00  4  1 
</p>
<p> 1,63  &minus;1  1  2,00  3  1 
</p>
<p> 6,84  4  0  4,00  2  0 
</p>
<p> 1,00  &minus;2  0  1,00  3  0 
</p>
<p> 1,14  &minus;2  1  1,00  3  1 
</p>
<p> 2,97  0  1  3,00  4  0 
</p>
<p> 1,05  &minus;2  1  1,00  4  1 
</p>
<p> ,63  &minus;2  0  1,00  3  0 
</p>
<p> 1,18  &minus;2  0  1,00  2  0 
</p>
<p> ,96  &minus;2  1  1,00  2  0 
</p>
<p> 8,28  5  0  4,00  2  1 
</p>
<p>  Var 1 ldl_reduction 
</p>
<p> Var 2 weight_redcution 
</p>
<p> Var 3 gender 
</p>
<p> Var 4 sport 
</p>
<p> Var 5 treatment_level 
</p>
<p> Var 6 diet 
</p>
<p>    For the decision tree with continuous outcome the classifi cation and regression tree 
</p>
<p>(CRT) model is applied. A 953 patient data fi le is used of various predictors of ldl 
</p>
<p>(low-density-lipoprotein)-cholesterol reduction including weight reduction, gender, 
</p>
<p>sport, treatment level, diet. The fi le is in extras.springer.com and is entitled &ldquo;deci-
</p>
<p>siontreecontinuous&rdquo;. The fi le is opened.
</p>
<p>  Command: 
</p>
<p>  Click Transform&hellip;.click Random Number Generators&hellip;.click Set Starting Point&hellip;. 
</p>
<p>click Fixed Value (2000000)&hellip;.click OK&hellip;.click Analyze&hellip;.Classify&hellip;Tree&hellip;. 
</p>
<p>Decision Trees with a Continuous Outcome</p>
<p/>
</div>
<div class="page"><p/>
<p>332
</p>
<p>Dependent Variable: enter ldl_reduction&hellip;. Independent Variables: enter weight reduc-
</p>
<p>tion, gender, sport, treatment level, diet&hellip;.Growing Methods: select CRT &hellip;.click 
</p>
<p>Criteria: enter Parent Node 300, Child Node 100&hellip;.click Output: Tree mark Tree in table 
</p>
<p>format&hellip;.click Continue&hellip;.click Save&hellip;.mark Terminal node number&hellip;.mark Predicted 
</p>
<p>value&hellip;.in Export Tree Model as XML mark Training sample&hellip;.click Browse&hellip;.&hellip;.in 
</p>
<p>File name enter "exportdecisiontreecontinuous" &hellip;.in Look in: enter the appropriate map 
</p>
<p>in your computer for storage&hellip;.click Save&hellip;.click OK.    
</p>
<p>  
</p>
<p>Idl reduction
</p>
<p>Node 0
</p>
<p>Mean
</p>
<p>Std. Dev.
</p>
<p>n
</p>
<p>%
</p>
<p>Predicted
</p>
<p>Mean
</p>
<p>Std. Dev.
</p>
<p>n
</p>
<p>%
</p>
<p>Predicted
</p>
<p>Mean
</p>
<p>Std. Dev.
</p>
<p>n
</p>
<p>%
</p>
<p>Predicted
</p>
<p>Mean
</p>
<p>Std. Dev.
</p>
<p>n
</p>
<p>%
</p>
<p>Predicted
</p>
<p>Mean 1,522
0,499
</p>
<p>526
</p>
<p>55,2
</p>
<p>1,522
</p>
<p>3,255
</p>
<p>0,479
211
</p>
<p>22,1
</p>
<p>3,255
</p>
<p>Std. Dev.
</p>
<p>n
</p>
<p>%
</p>
<p>Predicted
</p>
<p>Mean
</p>
<p>Std. Dev.
</p>
<p>n
</p>
<p>%
</p>
<p>Predicted
</p>
<p>Mean 1,114 1,938
</p>
<p>0,314
260
</p>
<p>27,3
1,938
</p>
<p>0,247
266
</p>
<p>27,9
</p>
<p>1,114
</p>
<p>Std. Dev.
</p>
<p>n
</p>
<p>%
</p>
<p>Predicted
</p>
<p>3, 038
</p>
<p>2, 018 6, 518
</p>
<p>1, 425
216
</p>
<p>22,7
</p>
<p>6,518
</p>
<p>0, 926
</p>
<p>737
</p>
<p>77,3
2, 018
</p>
<p>2, 162
953
</p>
<p>100.0
3,038
</p>
<p>weight reduction
</p>
<p>Improvement=3,550
</p>
<p>&lt;= 1,3
</p>
<p>&lt;= low level
</p>
<p>&lt;= -1,5 &gt; -1,5
</p>
<p>&gt; low level
</p>
<p>sport
</p>
<p>Improvement=0.475
</p>
<p>Node 1
</p>
<p>Node 3
</p>
<p>weight reduction
</p>
<p>Improvement=0,094
</p>
<p>Node 4
</p>
<p>Node 6Node 5
</p>
<p>Node 2
</p>
<p>&gt; 1,3
</p>
<p>  
</p>
<p>53 Decision Trees for Decision Analysis (1,004 and 953 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>333
</p>
<p>    The output sheets show the classifi cation tree. Only weight reduction and sport 
</p>
<p>signifi cantly contributed to the model, with the overall mean and standard deviation 
</p>
<p>dependent variable ldl cholesterol in the parent (root) node. Weight reduction with 
</p>
<p>a cut-off level of 1.3 units is the best predictor of ldl reduction. In the little weight 
</p>
<p>reduction group sport is the best predictor. In the low sport level subgroup again 
</p>
<p>weight reduction is a predictor, but here there is a large difference between weight 
</p>
<p>gain (&le;1.5 units) and weight loss (&ge;1.5 units). Minimizing the output shows the 
</p>
<p>original data fi le. It now contains two novel variables, the npde classifi cation and the 
</p>
<p>predicted value of ldl cholesterol reduction. They are entitled NodeId and 
</p>
<p>PredictedValue. The saved XML (eXtended Markup Language) fi le will now be 
</p>
<p>used to compute the predicted node classifi cation and value of ldl cholesterol reduc-
</p>
<p>tion in 5 novel patients with the following characteristics. For convenience the XML 
</p>
<p>fi le is given in extras.springer.com. 
</p>
<p> Var 2  Var 3  Var 4  Var 5  Var 6 
</p>
<p> &minus;,63  1,00  2,00  1,00  ,00 
</p>
<p> 2,10  ,00  4,00  4,00  1,00 
</p>
<p> &minus;1,16  1,00  2,00  1,00  1,00 
</p>
<p> 4,22  ,00  4,00  1,00  ,00 
</p>
<p> &minus;,59  ,00  3,00  4,00  1,00 
</p>
<p>  Var 2 weight_reduction 
</p>
<p> Var 3 gender 
</p>
<p> Var 4 sport 
</p>
<p> Var 5 treatment_level 
</p>
<p> Var 6 diet 
</p>
<p>    Enter the above data in a new SPSS data fi le.
</p>
<p>  Command: 
</p>
<p>  Utilities&hellip;.click Scoring Wizard&hellip;.click Browse&hellip;.click Select&hellip;.Folder: enter the 
</p>
<p>exportdecisiontreecontinuous.xml fi le&hellip;.click Select&hellip;.in Scoring Wizard click 
</p>
<p>Next&hellip;.mark Node Number&hellip;.mark Predicted Value&hellip;.click Next&hellip;.click Finish.    
</p>
<p> The above data fi le now gives individually predicted node classifi cations and 
</p>
<p>predicted ldl cholesterol reductions as computed by the linear model with the help 
</p>
<p>of the XML fi le. 
</p>
<p>Decision Trees with a Continuous Outcome</p>
<p/>
</div>
<div class="page"><p/>
<p>334
</p>
<p> Var 2  Var 3  Var 4  Var 5  Var 6  Var 7  Var 8 
</p>
<p> &minus;,63  1,00  2,00  1,00  ,00  6,00  1,94 
</p>
<p> 2,10  ,00  4,00  4,00  1,00  2,00  6,52 
</p>
<p> &minus;1,16  1,00  2,00  1,00  1,00  6,00  1,94 
</p>
<p> 4,22  ,00  4,00  1,00  ,00  2,00  6,52 
</p>
<p> &minus;,59  ,00  3,00  4,00  1,00  4,00  3,25 
</p>
<p>  Var 2 weight_reduction 
</p>
<p> Var 3 gender 
</p>
<p> Var 4 sport 
</p>
<p> Var 5 treatment_level 
</p>
<p> Var 6 diet 
</p>
<p> Var 7 predicted node classifi cation 
</p>
<p> Var 8 predicted ldl cholesterol reduction 
</p>
<p>        Conclusion 
</p>
<p> The module decision trees can be readily trained to predict in individual future 
</p>
<p>patients risk of infarction and ldl (low density lipoprotein) cholesterol decrease. 
</p>
<p>Instead of trained XML fi les for predicting about future patients, also syntax fi les 
</p>
<p>are possible for the purpose. They perform better if predictions from multiple 
</p>
<p>instead of single future patients are requested.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of decision trees as 
</p>
<p>well as the steps for utilizing syntax fi les is available in Machine learning in medi-
</p>
<p>cine part three, Chap. 14, entitled &ldquo;Decision trees&rdquo;, pp 153&ndash;168, Springer Heidelberg, 
</p>
<p>Germany 2013. Better accuracy from decision trees is sometimes obtained by the 
</p>
<p>use of a training sample (Chap.   8    ).   
</p>
<p>53 Decision Trees for Decision Analysis (1,004 and 953 Patients)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_8">http://dx.doi.org/10.1007/978-3-319-15195-3_8</a></div>
</div>
<div class="page"><p/>
<p>335&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_54
</p>
<p>    Chapter 54   
</p>
<p> Multidimensional Scaling for Visualizing 
Experienced Drug Effi cacies (14 Pain-Killers 
and 42 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> To individual patients, objective criteria of drug effi cacy, like pharmaco-dynamic/-
</p>
<p>kinetic and safety measures may not mean too much, and patients&rsquo; personal opin-
</p>
<p>ions are important too. This chapter is to assess whether multidimensional scaling 
</p>
<p>can visualize subgroup differences in experienced drug effi cacies, and whether 
</p>
<p>data-based dimensions can be used to match dimensions as expected from pharma-
</p>
<p>cological properties.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Can proximity and preference scores of pain-killers as judged by patient samples be 
</p>
<p>used for obtaining insight in the real priorities both in populations and in individual 
</p>
<p>patients. Can the data-based dimensions as obtained by this procedure be used to 
</p>
<p>match dimensions as expected from pharmacological properties.  
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 1&rdquo; as Chap. 
</p>
<p>17, 2013. </p>
<p/>
</div>
<div class="page"><p/>
<p>336
</p>
<p>    Proximity Scaling 
</p>
<p> Var 
</p>
<p> Var 
</p>
<p>1 
</p>
<p> Var 
</p>
<p>2 
</p>
<p> Var 
</p>
<p>3 
</p>
<p> Var 
</p>
<p>4 
</p>
<p> Var 
</p>
<p>5 
</p>
<p> Var 
</p>
<p>6 
</p>
<p> Var 
</p>
<p>7 
</p>
<p> Var 
</p>
<p>8 
</p>
<p> Var 
</p>
<p>9 
</p>
<p> Var 
</p>
<p>10 
</p>
<p> Var 
</p>
<p>11 
</p>
<p> Var 
</p>
<p>12 
</p>
<p> Var 
</p>
<p>13 
</p>
<p> Var 
</p>
<p>14 
</p>
<p> 1  0 
</p>
<p> 2  8  0 
</p>
<p> 3  7  2  0 
</p>
<p> 4  5  4  5  0 
</p>
<p> 5  8  5  4  6  0 
</p>
<p> 6  7  5  6  6  8  0 
</p>
<p> 7  4  5  6  3  7  4  0 
</p>
<p> 8  8  5  4  6  3  8  7  0 
</p>
<p> 9  3  7  9  4  8  7  5  8  0 
</p>
<p> 10  5  6  7  6  9  4  4  9  6  0 
</p>
<p> 11  9  5  4  6  3  8  7  3  8  9  0 
</p>
<p> 12  9  4  3  7  5  7  7  5  8  9  5  0 
</p>
<p> 13  4  6  6  3  7  5  4  8  4  5  7  7  0 
</p>
<p> 14  6  6  7  6  8  2  4  9  7  3  9  7  5  0 
</p>
<p>  Var 1&ndash;14 one by one distance scores of the pain-killers 1&ndash;14, mean estimates of 20 patients (scale 
</p>
<p>0&ndash;10). The 14 pain-killers are also given in the fi rst column. The data fi le is entitled &ldquo;proxscal&rdquo; and 
</p>
<p>is in extras.springer.com 
</p>
<p>    The above matrix mean scores can be considered as one by one distances between 
</p>
<p>all of the medicines connected with one another by straight lines in 14 different 
</p>
<p>ways. Along an x- and y-axis they are subsequently modeled using the equation: the 
</p>
<p>distance between drug i and drug j = &radic; [(x i  &minus; x j ) 2  + (y i  &minus; y j ) 2 ]. SPSS statistical 
 software 19.0 will be used for analysis. Start by opening the data fi le.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Scale&hellip;.Multidimensional scaling (PROXSCAL)&hellip;.Data Format: click 
</p>
<p>The data are proximities&hellip;.Number of Sources: click One matrix source&hellip;.One 
</p>
<p>Source: click The proximities are in a matrix across columns&hellip;.click Defi ne&hellip;. 
</p>
<p>enter all variables (medicines) into &ldquo;Proximities&rdquo;&hellip;.Model: Shape: click Lower- 
</p>
<p>triangular matrix&hellip;.Proximity Transformation: click Interval&hellip;.Dimensions: 
</p>
<p>Minimum: enter 2&hellip;.Maximum: enter 2&hellip;.click Continue&hellip;.click Plots&hellip;.mark 
</p>
<p>Common space&hellip;.mark Transformed proximities vs distances&hellip;.click Continue 
</p>
<p>&hellip;.click: Output&hellip;.mark Common space coordinates&hellip;.mark Multiple stress mea-
</p>
<p>sures&hellip;.click Continue&hellip;.click OK.   
</p>
<p> Stress and fi t measures 
</p>
<p> Normalized raw stress  ,00819 
</p>
<p> Stress-I  ,09051 a  
</p>
<p> Stress-II  ,21640 a  
</p>
<p> S-stress  ,02301 b  
</p>
<p> Dispersion accounted for (DAF.)  ,99181 
</p>
<p> Tucker&rsquo;s coeffi cient of congruence  ,99590 
</p>
<p>  PROXSCAL minimizes Normalized Raw Stress 
</p>
<p>  a Optimal scaling factor = 1,008 
</p>
<p>  b Optimal scaling factor = ,995 
</p>
<p>54 Multidimensional Scaling for Visualizing Experienced Drug Effi cacies&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>337
</p>
<p>    The output sheets gives the uncertainty of the model (stress = standard error) and 
</p>
<p>dispersion values. The model is assumed to appropriately describe the data if they 
</p>
<p>are respectively &lt; 0.20 and approximately 1.0. 
</p>
<p>    
</p>
<p>    Also, a plot of the actual distances as observed versus the distances fi tted by the 
</p>
<p>statistical program is given. A perfect fi t should produce a straight line, a poor fi t 
</p>
<p>produces a lot of spread around a line or even no line at all. The fi gure is not perfect 
</p>
<p>but it shows a very good fi t as expected from the stress and fi t measures. 
</p>
<p>  
</p>
<p>0,8
</p>
<p>D
im
</p>
<p>e
n
</p>
<p>s
io
</p>
<p>n
 2
</p>
<p>Dimension 1
</p>
<p>drugl
</p>
<p>drugA
</p>
<p>drugN
drugD
</p>
<p>drugG
</p>
<p>drugK
</p>
<p>drugO
drugF
</p>
<p>drugB
drugC
</p>
<p>drugM
</p>
<p>drugE
</p>
<p>drugH
</p>
<p>drugL
</p>
<p>0,6
</p>
<p>0,4
</p>
<p>0,2
</p>
<p>0,0
</p>
<p>-0,2
</p>
<p>-0,4
</p>
<p>-0,6
-1,0 -0,5 -0,0 -0,5 1,0
</p>
<p>  
</p>
<p>Proximity Scaling</p>
<p/>
</div>
<div class="page"><p/>
<p>338
</p>
<p>    Finally, the above fi gure shows the most important part of the outcome. The 
</p>
<p>standardized x- and y-axes values give some insight in the relative position of the 
</p>
<p>medicines according to perception of our study population. Four clusters are identi-
</p>
<p>fi ed. Using Microsoft&rsquo;s drawing commands we can encircle the clusters as identi-
</p>
<p>fi ed. The cluster at the upper right quadrant comprises high priorities of the patients 
</p>
<p>along both the x- an the y-axis. The cluster at the lower left quadrant comprises low 
</p>
<p>priorities of the patients along both axes. If, pharmacologically, the drugs in the 
</p>
<p>right upper quadrant were highly potent with little side effects, then the patients&rsquo; 
</p>
<p>priorities would fairly match the pharmacological properties of the medicines.  
</p>
<p>    Preference Scaling 
</p>
<p> Var 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15 
</p>
<p> 12  13  7  4  5  2  8  10  11  14  3  1  6  9  15 
</p>
<p> 14  11  6  3  10  4  15  8  9  12  7  1  5  2  13 
</p>
<p> 13  10  12  14  3  2  9  8  7  11  1  6  4  5  15 
</p>
<p> 7  14  11  3  6  8  12  10  9  15  4  1  2  5  13 
</p>
<p> 14  9  6  15  13  2  11  8  7  10  12  1  3  4  5 
</p>
<p> 9  11  15  4  7  6  14  10  8  12  5  2  3  1  13 
</p>
<p> 9  14  5  6  8  4  13  11  12  15  7  2  1  3  10 
</p>
<p> 15  10  12  6  8  2  13  9  7  11  3  1  5  4  14 
</p>
<p> 13  12  2  4  5  8  10  11  3  15  7  9  6  1  14 
</p>
<p> 15  13  10  7  6  4  9  11  12  14  5  2  8  1  3 
</p>
<p> 9  2  4  13  8  5  1  10  6  7  11  15  14  12  3 
</p>
<p>  Var 1&ndash;15 preference scores (1 = most prefered, 15 = least prefered) 
</p>
<p> Only the fi rst 11 patients are given. The entire data fi le is entitled &ldquo;prefscal&rdquo; and is in extras.
</p>
<p>springer.com. 
</p>
<p>    To 42 patients 15 different pain-killers are administered, and the patients are 
</p>
<p>requested to rank them in order of preference from 1 &ldquo;most prefered&rdquo; to 15 &ldquo;least 
</p>
<p>prefered&rdquo;. First will try and draw a three dimensional view of the individually 
</p>
<p>assigned preferences. We will use SPSS 19.0. Start by opening the data fi le.
</p>
<p>  Command: 
</p>
<p>  Graphs&hellip;.Legacy Dialogs&hellip;.3-D Bar&hellip;.X-axis represents: click Separate vari-
</p>
<p>ables&hellip;.Z-axis represents: click Individual cases&hellip;.Defi ne&hellip;.Bars Represent: enter 
</p>
<p>pain-killers 1-15&hellip;.Show Cases on: click Y-axis&hellip;.Show Cases with: click Case 
</p>
<p>number&hellip;.click OK.    
</p>
<p>54 Multidimensional Scaling for Visualizing Experienced Drug Effi cacies&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>339
</p>
<p>    
</p>
<p>    The above fi gure shows the result: a very irregular pattern consisting of multiple 
</p>
<p>areas with either high or low preference is observed. We will now perform a prefer-
</p>
<p>ence scaling analysis. Like with proximity scaling, preference assessments is 
</p>
<p>mapped in a 2 dimensional plane with the rank orders of the medicines as measures 
</p>
<p>of distance between the medicines. Two types of maps are constructed: an aggregate 
</p>
<p>map giving average distances of the entire population or individual maps of single 
</p>
<p>patients, and an ideal point map where ideal points have to be interpreted as a map 
</p>
<p>with ideal medicines, one for each patient. SPSS 19.0 is used once more.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Scale&hellip;.Multidimensional Unfolding (PREFSCAL)&hellip;.enter all vari-
</p>
<p>ables (medicines) into &ldquo;Proximities&rdquo;&hellip;.click Model&hellip;.click Dissimilarities&hellip;. 
</p>
<p>Dimensions: Minimum enter 2 &hellip;.Maximum enter 2&hellip;.Proximity Transformations: 
</p>
<p>click Ordinal &hellip;.click Within each row separately&hellip;.click Continue&hellip;.click 
</p>
<p>Options: imputation by: enter Spearman&hellip;.click Continue&hellip;.click Plots: mark Final 
</p>
<p>common space&hellip;.click Continue&hellip;.click Output: mark Fit measures &hellip;.mark Final 
</p>
<p>common space&hellip;.click Continue&hellip;.click OK.   
</p>
<p> Preference Scaling</p>
<p/>
</div>
<div class="page"><p/>
<p>340
</p>
<p> Measures 
</p>
<p> Iterations  115 
</p>
<p> Final function value  ,7104127 
</p>
<p> Function value parts  Stress part  ,2563298 
</p>
<p> Penalty part  1,9688939 
</p>
<p> Badness of fi t  Normalized stress  ,0651568 
</p>
<p> Kruskal&rsquo;s stress-I  ,2552582 
</p>
<p> Kruskal&rsquo;s stress-II  ,6430926 
</p>
<p> Young&rsquo;s S-stress-I  ,3653360 
</p>
<p> Young&rsquo;s S-stress-II  ,5405226 
</p>
<p> Goodness of fi t  Dispersion accounted for  ,9348432 
</p>
<p> Variance accounted for  ,7375011 
</p>
<p> Recovered preference orders  ,7804989 
</p>
<p> Spearman&rsquo;s Rho  ,8109694 
</p>
<p> Kendall&rsquo;s Tau-b  ,6816390 
</p>
<p> Variation coeffi cients  Variation proximities  ,5690984 
</p>
<p> Variation transformed proximities  ,5995274 
</p>
<p> Variation distances  ,4674236 
</p>
<p> Degeneracy indices  Sum-of-squares of DeSarbo&rsquo;s intermixedness indices  ,2677061 
</p>
<p> Shepard&rsquo;s rough nondegeneracy index  ,7859410 
</p>
<p>   The above table gives the stress (standard error) and fi t measures. The best fi t 
</p>
<p>distances as estimated by the model are adequate: measures of stress including nor-
</p>
<p>malized stress and Kruskal&rsquo;s stress-I are close to 0.20 or less, the value of dispersion 
</p>
<p>measures (Dispersion Accounted For) is close to 1.0. The table also shows whether 
</p>
<p>there is a risk of a  degenerate  solution, otherwise called loss function. The individ-
</p>
<p>ual proximities have a tendency to form circles, and when averaged for obtaining 
</p>
<p>average proximities, there is a tendency for the average treatment places to center in 
</p>
<p>the middle of the map. The solution is a penalty term, but in our example we need 
</p>
<p>not worry. The DeSarbo&rsquo;s and Shepard criteria are close to respectively 0 and 80 %, 
</p>
<p>and no penalty adjustment is required. 
</p>
<p>54 Multidimensional Scaling for Visualizing Experienced Drug Effi cacies&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>341
</p>
<p>    
</p>
<p>       
</p>
<p> Preference Scaling</p>
<p/>
</div>
<div class="page"><p/>
<p>342
</p>
<p>    The above fi gure (upper graph) gives the most important part of the output. The 
</p>
<p>standardized x- and y-axes values of the upper graph give some insight in the rela-
</p>
<p>tive position of the medicines according to our study population. The results can be 
</p>
<p>understood as the relative position of the medicines according to the perception of 
</p>
<p>our study population. Both the horizontal and the vertical dimension appears to 
</p>
<p>discriminate between different preferences. The lower graph gives the patients&rsquo; 
</p>
<p> ideal points . The patients seem to be split into two clusters with different prefer-
</p>
<p>ences, although with much variation along the y-axis. The dense cluster in the right 
</p>
<p>lower quadrant represented patients with preferences both along the x- and y-axis. 
</p>
<p>Instead of two-dimensions, multidimensional scaling enables to assess multiple 
</p>
<p>dimensions each of which can be assigned to one particular cause for proximity. 
</p>
<p>This may sound speculative, but if the pharmacological properties of the drugs 
</p>
<p>match the place of the medicines in a particular dimension, then we will be more 
</p>
<p>convinced that the multi-dimensional display gives, indeed, an important insight in 
</p>
<p>the real priorities of the patients. In order to address this issue, we will now perform 
</p>
<p>a multidimensional scaling procedure of the above data including three 
</p>
<p>dimensions.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Scale&hellip;.Multidimensional Unfolding (PREFSCAL)&hellip;.enter all vari-
</p>
<p>ables (medicines) into &ldquo;Proximities&rdquo;&hellip;.click Model&hellip;.click Dissimilarities&hellip;. 
</p>
<p>Dimensions: Minimum enter 3 &hellip;.Maximum enter 3&hellip;.Proximity Transformations: 
</p>
<p>click Ordinal &hellip;.click Within each row separately&hellip;.click Continue&hellip;.click 
</p>
<p>Options: imputation by: enter Spearman&hellip;.click Continue&hellip;.click Plots: mark Final 
</p>
<p>common space&hellip;.click Continue&hellip;.click Output: mark Fit measures &hellip;.mark Final 
</p>
<p>common space&hellip;.click Continue&hellip;.click OK.    
</p>
<p> Final Column Coordinates 
</p>
<p> Dimension 
</p>
<p> Painkiller no.  1  2  3 
</p>
<p> 1  &minus;2.49  &minus;9.08  &minus;4.55 
</p>
<p> 2  &minus;7.08  &minus;1.81  1.43 
</p>
<p> 3  &minus;3.46  3.46  &minus;2.81 
</p>
<p> 4  5.41  &minus;4.24  1.67 
</p>
<p> 5  &minus;.36  6.21  5.25 
</p>
<p> 6  .17  1.88  &minus;3.27 
</p>
<p> 7  &minus;7.80  &minus;2.07  &minus;1.59 
</p>
<p> 8  &minus;5.17  &minus;4.18  2.91 
</p>
<p> 9  4.75  &minus;.59  4.33 
</p>
<p> 10  &minus;6.80  &minus;4.83  .27 
</p>
<p> 11  6.22  2.50  .88 
</p>
<p> 12  3.71  &minus;1.27  &minus;.49 
</p>
<p> 13  5.30  &minus;2.95  1.51 
</p>
<p> 14  2.82  1.66  &minus;2.09 
</p>
<p> 15  &minus;4.35  2.76  &minus;6.72 
</p>
<p>54 Multidimensional Scaling for Visualizing Experienced Drug Effi cacies&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>343
</p>
<p>   The output sheets shows the standardized mean preference values of the different 
</p>
<p>pain-killers as x- y-, and z-axis coordinates. The best fi t outcome of the three- 
</p>
<p>dimensional (3-D) model can be visualized in a 3-D fi gure. SPSS 19.0 is used. First 
</p>
<p>cut and paste the data from the above table to the preference scaling fi le or another 
</p>
<p>fi le. Then proceed.
</p>
<p>  Command: 
</p>
<p>  Graphs&hellip;.Legacy Dialogs&hellip;.Scatter/Dot&hellip;.click 3-D Scatter&hellip;.click 
</p>
<p>Defi ne&hellip;.Y-Axis: enter dimension 1&hellip;.X-Axis: enter dimension 2&hellip;.Z-Axis: enter 
</p>
<p>dimension 3&hellip;.click OK.    
</p>
<p>    
</p>
<p>    The above fi gure gives the best fi t outcome of a 3-dimensional scaling model. 
</p>
<p>Three clusters were identifi ed, consistent with patients&rsquo; preferences along an x-, y-, 
</p>
<p>and z-axis. Using Microsoft&rsquo;s drawing commands we can encircle the clusters as 
</p>
<p>identifi ed. In the fi gure an example is given of how pharmacological properties 
</p>
<p>could be used to explain the cluster pattern.  
</p>
<p>    Conclusion 
</p>
<p> Multidimensional scaling is helpful both to underscore the pharmacological proper-
</p>
<p>ties of the medicines under studies, and to identify what effects are really important 
</p>
<p>to patients, and uses for these purposes estimated proximities as surrogates for 
</p>
<p>counted estimates of patients&rsquo; opinions. Multidimensional scaling can, like 
</p>
<p> Conclusion</p>
<p/>
</div>
<div class="page"><p/>
<p>344
</p>
<p>regression analysis, be used two ways, (1) for estimating preferences of treatment 
</p>
<p>modalities in a population, (2) for assessing the preferred treatment modalities in 
</p>
<p>individual patients.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of multidimensional 
</p>
<p>scaling is given in Machine learning in medicine part two, Chap. 12, Multidimensional 
</p>
<p>scaling, pp 115&ndash;127, Springer Heidelberg Germany 2013.    
</p>
<p>54 Multidimensional Scaling for Visualizing Experienced Drug Effi cacies&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>345&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_55
</p>
<p>Chapter 55
</p>
<p>Stochastic Processes for Long Term 
</p>
<p>Predictions from Short Term Observations
</p>
<p> General Purpose
</p>
<p>Markov modeling, otherwise called stochastic processes, assumes that per time unit 
</p>
<p>the same % of a population will have an event, and it is used for long term predic-
</p>
<p>tions from short term observations. This chapter is to assess whether the method can 
</p>
<p>be applied by non-mathematicians using an online matrix-calculator.
</p>
<p> Specific Scientific Questions
</p>
<p>If per time unit the same % of patients will have an event like surgery, medical treat-
</p>
<p>ment, a complication like a co-morbidity or death, what will be the average time 
</p>
<p>before such events take place.
</p>
<p> Example 1
</p>
<p>Patients with three states of treatment for a disease are checked every 4 months. 
</p>
<p>The underneath matrix is a so-called transition matrix. The states 1&ndash;3 indicate the 
</p>
<p>chances of treatment: 1 = no treatment, 2 = surgery, 3 = medicine. If you are in state 
</p>
<p>1 today, there will be a 0.3 = 30 % chance that you will receive no treatment in 
</p>
<p>the next 4 months, a 0.2 = 20 % chance of surgery, and a 0.5 = 50 % chance of 
</p>
<p> medicine treatment. If you are still in state 1 (no treatment) after 4 months, there 
</p>
<p>This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 1&rdquo; as Chap. 
</p>
<p>18, 2013.</p>
<p/>
</div>
<div class="page"><p/>
<p>346
</p>
<p>will again be a 0.3 chance that this will be the same in the second 4 month period 
</p>
<p>etc. So, after 5 periods the chance of being in state 1 equals 0.3 &times; 0.3 &times; 0.3 &times; 0.3 &times; 
</p>
<p>0.3 = 0.00243. The chance that you will be in the states 2 or 3 is much larger, and 
</p>
<p>there is something special about these states. Once you are in these states you will 
</p>
<p>never leave them anymore, because the patients who were treated with either sur-
</p>
<p>gery or medicine are no longer followed in this study. That this happens can be 
</p>
<p>observed from the matrix: if you are in state 2, you will have a chance of 1 = 100 % 
</p>
<p>to stay in state 2 and a chance of 0 = 0 % not to do so. The same is true for the state 3.
</p>
<p>State in next period  
</p>
<p>(4 months)
</p>
<p>1 2 3
</p>
<p>State in current time
</p>
<p>1 0.3 0.2 0.5
</p>
<p>2 0 1 0
</p>
<p>3 0 0 1
</p>
<p>Now we will compute what will happen with the chances of a patient in the state 
</p>
<p>1 after several 4 month periods.
</p>
<p>chances of being in state:
</p>
<p>state 1 state 2 state 3
</p>
<p>4 month period
</p>
<p>1st 30 % 20 % 50 %
</p>
<p>2nd 30 &times; 0.3 = 9 % 20 + 0.3 &times; 20 = 26 % 50 + 0.3 &times; 50 = 65 %
</p>
<p>3rd 9 &times; 0.3 = 3 % 26 + 9 &times; 0.2 = 27.8 % 65 + 9 &times; 0.5 = 69.5 %
</p>
<p>4th 3 &times; 0.3 = 0.9 % 27.8 + 3 &times; 0.2 = 28.4 % 69.5 + 3 &times; 0.5 = 71.0 %
</p>
<p>5th 0.9 &times; 0.3 = 0.27 % 28.4 + 0.9 &times; 0.2 = 28.6 % 71.0 + 0.9 &times; 0.5 = 71.5
</p>
<p>Obviously, the chances of being in the states 2 or 3 will increase, though 
</p>
<p> increasingly slowly, and the chance of being in state 1 is, ultimately, going to 
</p>
<p>approximate zero. In clinical terms: postponing the treatment does not make much 
</p>
<p>sense, because everyone in the no treatment group will eventually receive a  treatment 
</p>
<p>and the ultimate chances of surgery and medicine treatment are approximately 29 
</p>
<p>and 71 %. With larger matrices this method for calculating the ultimate chances is 
</p>
<p>rather laborious. Matrix algebra offers a rapid method.
</p>
<p>State in next period (4 months)
</p>
<p>1 2 3
</p>
<p>State in current time
</p>
<p>1 [0.3] [0.2 0.5] matrix Q matrix R
</p>
<p>2 [0] [1 0] matrix O matrix I
</p>
<p>3 [0] [0 1]
</p>
<p>The states are called transient, if they can change (the state 1), and absorbing if 
</p>
<p>not (the states 2 and 3). The original matrix is partitioned into four submatrices, 
</p>
<p>otherwise called the canonical form:
</p>
<p>55 Stochastic Processes for Long Term Predictions from Short Term Observations</p>
<p/>
</div>
<div class="page"><p/>
<p>347
</p>
<p>[0.3] Upper left corner:
</p>
<p>This square matrix Q can be sometimes very large with rows and columns 
</p>
<p>respectively presenting the transient states.
</p>
<p>[0.2 0.5] Upper right corner:
</p>
<p>This R matrix presents in rows the chance of being absorbed from the transient state.
</p>
<p>[1 0] Lower right corner:
</p>
<p>[0 1] This identity matrix I presents rows and columns with chances of
</p>
<p>being in the absorbing states, the I matrix must be adjusted to the
</p>
<p>size of the Q matrix (here it will look like [1] instead of [1 0]
</p>
<p>[0] Lower left corner.
</p>
<p>[0] This is a matrix of zeros (0 matrix).
</p>
<p>From the above matrices a fundamental matrix (F) is constructed.
</p>
<p> 
matrix I matrix R( )&minus; ( )  =[ ] =
</p>
<p>&minus; &minus;1 1
0 7 10 7. /
</p>
<p> 
</p>
<p>With larger matrices a matrix calculator, like the Bluebit Online Matrix Calculator 
</p>
<p>can be used to compute the matrix to the &minus;1 power by clicking &ldquo;Inverse&rdquo;.
</p>
<p>The fundamental matrix F equals 10/7. It can be interpreted as the average time, 
</p>
<p>before someone goes into the absorbing state (10/7 &times; 4 months = 5.714 months). The 
</p>
<p>product of the fundamental matrix F and the R matrix gives more exact chances of 
</p>
<p>a person in state 1 ending up in the states 2 and 3.
</p>
<p> 
F R&times; = ( )&times; =] [ =] [ 10 7 0 2 0 5 2 7 5 7 0 285714 0 714286/ . . / / . . .  
</p>
<p>The two latter values add up to 1.00, which indicates a combined chance of end-
</p>
<p>ing up in an absorbing state equal to 100 %.
</p>
<p> Example 2
</p>
<p>Patients with three states of treatment for a chronic disease are checked every 4 
</p>
<p>months.
</p>
<p>State in next period (4 months)
</p>
<p>1 2 3
</p>
<p>State in current time
</p>
<p>1 0.3 0.6 0.1
</p>
<p>2 0.45 0.5 0.05
</p>
<p>3 0 0 1
</p>
<p>The above matrix of three states and second periods of time gives again the 
</p>
<p>chances of different treatment for a particular disease, but it is slightly different 
</p>
<p>Example 2</p>
<p/>
</div>
<div class="page"><p/>
<p>348
</p>
<p>from the first example. Here state 1 = no treatment state, state 2 = medicine  treatment, 
</p>
<p>state 3 = surgery state. We assume that medicine can be stopped while surgery is 
</p>
<p>irretrievable, and, thus, an absorbing state. We first partition the matrix.
</p>
<p>State in next period (4 months)
</p>
<p>1 2 3
</p>
<p>State in current time
</p>
<p>1 [0.3 0.6] [0.1] matrix Q matrix R
</p>
<p>2 [0.45 0.5] [0.05]
</p>
<p>3 [0 0] [1] matrix O matrix I
</p>
<p>The R matrix [0.1] is in the upper right corner.
</p>
<p>[0.05]
</p>
<p>The Q matrix [0.3 0.6] is in the left upper corner.
</p>
<p>[0.45 0.5]
</p>
<p>The I matrix [1] is in the lower right corner, and must be adjusted,
</p>
<p>   before it can be subtracted from the Q matrix according
</p>
<p>       
</p>
<p>to 1 0
</p>
<p>0 1
</p>
<p>
</p>
<p>

</p>
<p>
</p>
<p>The 0 matrix [0 0] is in the lower left corner
</p>
<p> 
</p>
<p>I Q&minus; =

</p>
<p>

</p>
<p>
</p>
<p>
 &minus;
</p>
<p>
</p>
<p>

</p>
<p>
</p>
<p>
 =
</p>
<p>&minus;
</p>
<p>&minus;
</p>
<p>
</p>
<p>

</p>
<p>
</p>
<p>

</p>
<p>1 0
</p>
<p>0 1
</p>
<p>0 3 0 6
</p>
<p>0 45 0 5
</p>
<p>0 7 0 6
</p>
<p>0 45 0 5
</p>
<p>. .
</p>
<p>. .
</p>
<p>. .
</p>
<p>. .
.
</p>
<p> 
</p>
<p>The inverse of [I &ndash; Q] is obtained by marking &ldquo;Inverse&rdquo; at the online Bluebit Matrix 
</p>
<p>Calculator and equals
</p>
<p> 
</p>
<p>I Q fundamentalmatrix F&minus;[ ] =

</p>
<p>

</p>
<p>
</p>
<p>
 =
</p>
<p>&minus;1 6 25 7 5
</p>
<p>5 625 8 75
</p>
<p>. .
</p>
<p>. .
.
</p>
<p> 
</p>
<p>It is interpreted as the average periods of time before some transient state goes 
</p>
<p>into the absorbing state:
</p>
<p>(6.25 + 7.5 = 13.75) &times; 4 months for the patients in state 1 first and state 2 second,
</p>
<p>(5.625 + 8.75 = 14.375) &times; 4 months for the patients in state 2 first and state 1 
</p>
<p>second.
</p>
<p>Finally, the product of matrix F times matrix R is calculated. It gives the chances 
</p>
<p>of ending up in the absorbing state for those starting in the states 1 and 2.
</p>
<p> 
</p>
<p>6 25 7 5
</p>
<p>5 625 8 75
</p>
<p>0 1
</p>
<p>0 05
</p>
<p>1 00
</p>
<p>1 00
</p>
<p>. .
</p>
<p>. .
</p>
<p>.
</p>
<p>.
</p>
<p>.
</p>
<p>.
.
</p>
<p>
</p>
<p>

</p>
<p>
</p>
<p>
 &times;
</p>
<p>
</p>
<p>

</p>
<p>
</p>
<p>
 =
</p>
<p>
</p>
<p>

</p>
<p>
</p>
<p>

</p>
<p> 
</p>
<p>Obviously the chance of both the transient states for ending up in the absorbing state 
</p>
<p>is 1.00 = 100 %.
</p>
<p>55 Stochastic Processes for Long Term Predictions from Short Term Observations</p>
<p/>
</div>
<div class="page"><p/>
<p>349
</p>
<p> Example 3
</p>
<p>State 1 = stable coronary artery disease (CAD),
</p>
<p>state 2 = complications,
</p>
<p>state 3 = recovery state,
</p>
<p>state 4 = death state.
</p>
<p>State in next period (4 months)
</p>
<p>1 2 3 4
</p>
<p>State in current time
</p>
<p>1 0.95 0.04 0 0.01
</p>
<p>2 0 0 0.9 0.1
</p>
<p>3 0 0.3 0.3 0.4
</p>
<p>4 0 0 0 1
</p>
<p>If you take higher powers of this transition matrix (P), you will observe long-
</p>
<p>term trends of this model. For that purpose use the matrix calculator and square the 
</p>
<p>transition matrix (P2 gives the chances in the 2nd 4 month period etc) and compute 
</p>
<p>also higher powers (P3,P4,P5, etc).
</p>
<p> 
</p>
<p>P2
</p>
<p>0 903 0 038 0 036 0 024
</p>
<p>0 000 0 270 0 270 0 460
</p>
<p>0 000 0 090 0 360 0
</p>
<p>. . . .
</p>
<p>. . . .
</p>
<p>. . . ..
</p>
<p>. . . .
</p>
<p>. . . .
</p>
<p>. .
</p>
<p>550
</p>
<p>0 000 0 000 0 000 1 000
</p>
<p>0 698 0 048 0 063 0 191
</p>
<p>0 000 0 026 0
</p>
<p>6P
</p>
<p>.. .
</p>
<p>. . . .
</p>
<p>. . . .
</p>
<p>064 0 910
</p>
<p>0 000 0 021 0 047 0 931
</p>
<p>0 000 0 000 0 000 1 000  
</p>
<p>The above higher order transition matrices suggest that with rising powers, and, 
</p>
<p>thus, after multiple 4 month periods, there is a general trend towards the absorbing 
</p>
<p>state: in each row the state 4 value continually rises. In the end we all will die, but 
</p>
<p>in order to be more specific about the time, a special matrix like the one described 
</p>
<p>in the previous examples is required. In order to calculate the precise time before the 
</p>
<p>transient states go into the absorbing state, we need to partition the initial transition 
</p>
<p>matrix.
</p>
<p>Example 3</p>
<p/>
</div>
<div class="page"><p/>
<p>350
</p>
<p>State in next period (4 months)
</p>
<p>1 2 3 4
</p>
<p>State in current time
</p>
<p>1 0 95 0 04 0 0
</p>
<p>0 0 0 0 0 9
</p>
<p>0 0 0 3 0 3
</p>
<p>. . .
</p>
<p>. . .
</p>
<p>. . .
</p>
<p>
</p>
<p>
</p>
<p>

</p>
<p>
</p>
<p>
</p>
<p>

</p>
<p>0 01
</p>
<p>0 1
</p>
<p>0 4
</p>
<p>.
</p>
<p>.
</p>
<p>.
</p>
<p>
</p>
<p>
</p>
<p>

</p>
<p>
</p>
<p>
</p>
<p>

</p>
<p>2 matrix Q matrix R
</p>
<p>3
</p>
<p>4 [ 0 0 0] [1] matrix O matrix I
</p>
<p> 
F I Q= &minus;( )
</p>
<p>&minus;1
</p>
<p> 
</p>
<p> 
</p>
<p>I Q&minus; =
</p>
<p>
</p>
<p>
</p>
<p>


</p>
<p>
</p>
<p>
</p>
<p>


</p>
<p>&minus;
</p>
<p>
</p>
<p>
</p>
<p>


</p>
<p>1 0 0
</p>
<p>0 1 0
</p>
<p>0 0 1
</p>
<p>0 95 0 04 0 0
</p>
<p>0 0 0 0 0 9
</p>
<p>0 0 0 3 0 3
</p>
<p>. . .
</p>
<p>. . .
</p>
<p>. . .
</p>
<p>
</p>
<p>
</p>
<p>


</p>
<p> 
</p>
<p> 
</p>
<p>F =
</p>
<p>&minus;
</p>
<p>&minus;
</p>
<p>&minus;
</p>
<p>
</p>
<p>
</p>
<p>


</p>
<p>
</p>
<p>
</p>
<p>


</p>
<p>&minus;
0 5 0 04 0
</p>
<p>0 0 1 0 0 9
</p>
<p>0 0 0 3 0 7
</p>
<p>1
. .
</p>
<p>. . .
</p>
<p>. . .
 
</p>
<p>The online Bluebit Matrix calculator (mark inverse) produces the underneath result.
</p>
<p> 
</p>
<p>F =
</p>
<p>
</p>
<p>
</p>
<p>


</p>
<p>
</p>
<p>
</p>
<p>


</p>
<p>20 0 1 302 1 674
</p>
<p>0 0 1 628 2 093
</p>
<p>0 0 0 698 2 326
</p>
<p>. . .
</p>
<p>. . .
</p>
<p>. . .
 
</p>
<p>The average time before various transient states turn into the absorbing state (dying 
</p>
<p>in this example) is given.
</p>
<p> 
</p>
<p>State months months
</p>
<p>State
</p>
<p>1 20 1 302 1 674 4 91 904
</p>
<p>2 0 0 1
</p>
<p>: . . . .
</p>
<p>: . .
</p>
<p>+ +( )&times; =
+ 6628 2 093 4 14 884
</p>
<p>3 0 0 0 698 2 326
</p>
<p>+( )&times; =
+ +( )
</p>
<p>. . .
</p>
<p>: . . .
</p>
<p>months months
</p>
<p>State &times;&times; =4 12 098months months. .
 
</p>
<p>The chance of dying for each state is computed from matrix F times matrix R 
</p>
<p>(click multiplication, enter the data in the appropriate fields and click calculate).
</p>
<p> 
</p>
<p>F R.
</p>
<p>. . .
</p>
<p>. . .
</p>
<p>. . .
</p>
<p>.
</p>
<p>=
</p>
<p>
</p>
<p>
</p>
<p>


</p>
<p>
</p>
<p>
</p>
<p>


</p>
<p>&times;
</p>
<p>20 0 1 302 1 672
</p>
<p>0 0 1 628 2 093
</p>
<p>0 0 0 698 2 326
</p>
<p>0 011
</p>
<p>0 1
</p>
<p>0 4
</p>
<p>1 0
</p>
<p>1 0
</p>
<p>1 0
</p>
<p>.
</p>
<p>.
</p>
<p>.
</p>
<p>.
</p>
<p>.
</p>
<p>.
</p>
<p>
</p>
<p>
</p>
<p>


</p>
<p>
</p>
<p>
</p>
<p>


</p>
<p>=
</p>
<p>
</p>
<p>
</p>
<p>


</p>
<p>
</p>
<p>
</p>
<p>


</p>
<p> 
</p>
<p>Like in the previous examples again the products of the matrices F and R show that 
</p>
<p>all of the states end up with death. However, in the state 1 this takes more time than 
</p>
<p>it does in the other states.
</p>
<p>55 Stochastic Processes for Long Term Predictions from Short Term Observations</p>
<p/>
</div>
<div class="page"><p/>
<p>351
</p>
<p> Conclusion
</p>
<p>Markov chains are used to analyze the long-term risks of reversible and irreversible 
</p>
<p>complications including death. The future is not shown, but it is shown, what will 
</p>
<p>happen, if everything remains the same. Markov chains assume, that the chance of 
</p>
<p>an event is not independent, but depends on events in the past.
</p>
<p> Note
</p>
<p>More background, theoretical and mathematical information of Markov chains (sto-
</p>
<p>chastic modeling) is given in Machine learning in medicine part three, Chaps. 17 
</p>
<p>and 18, &ldquo;Stochastic processes: stationary Markov chains&rdquo; and &ldquo;Stochastic pro-
</p>
<p>cesses: absorbing Markov chains&rdquo;, pp 195&ndash;204 and 205&ndash;216, Springer Heidelberg 
</p>
<p>Germany 2013.
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>353&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_56
</p>
<p>    Chapter 56   
</p>
<p> Optimal Binning for Finding High Risk 
Cut- offs (1,445 Families) 
</p>
<p>                      General Purpose 
</p>
<p> Optimal binning is a so-called non-metric method for describing a continuous 
</p>
<p>predictor variable in the form of best fi t categories for making predictions. Like 
</p>
<p>binary partitioning (Machine Learning in Medicine Part One, Chap. 7, Binary 
</p>
<p>partitioning, pp 79&ndash;86, Springer Heidelberg Germany, 2013) it uses an exact test 
</p>
<p>called the entropy method, which is based on log likelihoods. It may, therefore, 
</p>
<p>produce better statistics than traditional tests. In addition, unnecessary noise due to 
</p>
<p>continuous scaling is deleted, and categories for identifying patients at high risk of 
</p>
<p>particular outcomes can be identifi ed. This chapter is to assess its effi ciency in 
</p>
<p>medical research.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Unhealthy lifestyles cause increasingly high risks of overweight children. 
</p>
<p>We are, particularly, interested in the best fi t cut-off values of unhealthy lifestyle 
</p>
<p>estimators to maximize the difference between low and high risk. 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 1&rdquo; as Chap. 
</p>
<p>19, 2013. </p>
<p/>
</div>
<div class="page"><p/>
<p>354
</p>
<p> Var 1  Var 2  Var 3  Var 4  Var 5 
</p>
<p> 0  11  1  8  0 
</p>
<p> 0  7  1  9  0 
</p>
<p> 1  25  7  0  1 
</p>
<p> 0  11  4  5  0 
</p>
<p> 1  5  1  8  1 
</p>
<p> 0  10  2  8  0 
</p>
<p> 0  11  1  6  0 
</p>
<p> 0  7  1  8  0 
</p>
<p> 0  7  0  9  0 
</p>
<p> 0  15  3  0  0 
</p>
<p>  Var 1fruitvegetables (0 = no, 1 = yes) 
</p>
<p> Var 2 unhealthysnacks (times per week) 
</p>
<p> Var 3 fastfoodmeal (times per week) 
</p>
<p> Var 4 physicalactivities (times per week) 
</p>
<p> Var 5 overweightchildren (0 = no, 1 = yes) 
</p>
<p>    Only the fi rst 10 families are given, the entire data fi le is entitled &ldquo;optimalbin-
</p>
<p>ning&rdquo; and is in extras.springer.com.  
</p>
<p>    Optimal Binning 
</p>
<p> SPSS 19.0 is used for analysis. Start by opening the data fi le.
</p>
<p>  Command: 
</p>
<p>  Transform&hellip;.Optimal Binning&hellip;.Variables into Bins: enter fruitvegetables, 
</p>
<p>unhealthysnacks, fastfoodmeal, physicalactivities&hellip;.Optimize Bins with Respect 
</p>
<p>to: enter "overweightchildren"&hellip;.click Output&hellip;.Display: mark Endpoints&hellip;.mark 
</p>
<p>Descriptive statistics&hellip;.mark Model Entropy&hellip;.click Save: mark Create variables 
</p>
<p>that contain binned data&hellip;.click OK.   
</p>
<p> Descriptive statistics 
</p>
<p> N  Minimum  Maximum 
</p>
<p> Number of 
</p>
<p>distinct values 
</p>
<p> Number 
</p>
<p>of bins 
</p>
<p> Fruitvegetables/wk  1,445  0  34  33  2 
</p>
<p> Unhealthysnacks/wk  1,445  0  42  1,050  3 
</p>
<p> Fastfoodmeal/wk  1,445  0  21  1,445  2 
</p>
<p> Physicalactivities/wk  1,445  0  10  1,385  2 
</p>
<p>   In the output the above table is given. N = the number of adults in the analysis, 
</p>
<p>Minimum/Maximum = the range of the original continuous variables, Number of 
</p>
<p>Distinct Values = the separate values of the continuous variables as used in the 
</p>
<p>binning process, Number of Bins = the number of bins (= categories) generated and 
</p>
<p>is smaller than the initials separate values of the same variables.
</p>
<p>56 Optimal Binning for Finding High Risk Cut-offs (1,445 Families)</p>
<p/>
</div>
<div class="page"><p/>
<p>355
</p>
<p> Model entropy 
</p>
<p> Model entropy 
</p>
<p> Fruitvegetables/wk  ,790 
</p>
<p> Unhealthysnacks/wk  ,720 
</p>
<p> Fastfoodmeal/wk  ,786 
</p>
<p> Physicalactivities/wk  ,805 
</p>
<p>  Smaller model entropy Indicates higher predictive 
</p>
<p>accuracy of the binned variable on guide variable 
</p>
<p>overweight children 
</p>
<p>    Model Entropy gives estimates of the usefulness of the bin models as pre-
</p>
<p>dictor models for probability of overweight: the smaller the entropy, the better 
</p>
<p>the model. 
</p>
<p> Values under 0,820 indicate adequate usefulness.
</p>
<p> Fruitvegetables/wk 
</p>
<p> Bin 
</p>
<p> End point 
</p>
<p> Number of cases by level of 
</p>
<p>overweight children 
</p>
<p> Lower  Upper  No  Yes  Total 
</p>
<p> 1   a   14  802  340  1,142 
</p>
<p> 2  14   a   274  29  303 
</p>
<p> Total  1,076  369  1,445 
</p>
<p> Unhealthysnacks/wk 
</p>
<p> Bin 
</p>
<p> End point 
</p>
<p> Number of cases by level of 
</p>
<p>overweight children 
</p>
<p> Lower  Upper  No  Yes  Total 
</p>
<p> 1   a   12  830  143  973 
</p>
<p> 2  12  19  188  126  314 
</p>
<p> 3  19   a   58  100  158 
</p>
<p> Total  1,076  369  1,445 
</p>
<p> Fastfoodmeal/wk 
</p>
<p> Bin 
</p>
<p> End point 
</p>
<p> Number of cases by level of 
</p>
<p>overweight children 
</p>
<p> Lower  Upper  No  Yes  Total 
</p>
<p> 1   a   2  896  229  1,125 
</p>
<p> 2  2   a   180  140  320 
</p>
<p> Total  1,076  369  1,445 
</p>
<p>Optimal Binning</p>
<p/>
</div>
<div class="page"><p/>
<p>356
</p>
<p> Physicalactivities/wk 
</p>
<p> Bin 
</p>
<p> End point 
</p>
<p> Number of cases by level of 
</p>
<p>overweight children 
</p>
<p> Lower  Upper  No  Yes  Total 
</p>
<p> 1   a   8  469  221  690 
</p>
<p> 2  8   a   607  148  755 
</p>
<p> Total  1,076  369  1,445 
</p>
<p>  Each bin is computed as Lower &lt; = physicalactivities/wk &lt; Upper 
</p>
<p>  a Unbounded 
</p>
<p>    The above tables show the high risk cut-offs for overweight children of the four 
</p>
<p>predicting factors. E.g., in 1,142 adults scoring under 14 units of fruit/vegetable per 
</p>
<p>week, are put into bin 1 and 303 scoring over 14 units per week, are put into bin 2. 
</p>
<p>The proportion of overweight children in bin 1 is much larger than it is in bin 2: 
</p>
<p>340/1,142 = 0.298 (30 %) and 29/303 = 0.096 (10 %). Similarly high risk cut-offs are 
</p>
<p>found for
</p>
<p>   unhealthy snacks less than 12, 12&ndash;19, and over 19 per week  
</p>
<p>  fastfood meals less than 2, and over 2 per week  
</p>
<p>  physical activities less than 8 and over 8 per week.    
</p>
<p> These cut-offs can be used as meaningful recommendation limits to future 
</p>
<p>families. 
</p>
<p> When we return to the dataview page, we will observe that the four variables 
</p>
<p>have been added in the form of bin variables (with suffi x _bin). They can be used 
</p>
<p>as outcome variables for making predictions from other variables like personal 
</p>
<p>characteristics of parents. Also they can be used, instead of the original variable, as 
</p>
<p>predictors in regression modeling. A binary logistic regression with overweight 
</p>
<p>children as dependent variable will be performed to assess their predictive strength 
</p>
<p>as compared to that of the original variables. SPSS 19.0 will again be used.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Regression&hellip;.Binary Logistic&hellip;.Dependent: enter overweight children 
</p>
<p>&hellip;.Covariates: enter fruitvegetables, unhealthysnack, fastfoodmeal, physicalactivi-
</p>
<p>ties&hellip;.click OK.   
</p>
<p> Variables in the equation 
</p>
<p> B  S.E.  Wald  df  Sig.  Exp(B) 
</p>
<p> Step 1 a   Fruitvegetables  &minus;,092  ,012  58,775  1  ,000  ,912 
</p>
<p> Unhealthysnacks  ,161  ,014  127,319  1  ,000  1,175 
</p>
<p> Fastfoodmeal  ,194  ,041  22,632  1  ,000  1,214 
</p>
<p> Physicalactivities  ,199  ,041  23,361  1  ,000  1,221 
</p>
<p> Constant  &minus;4,008  ,446  80,734  1  ,000  ,018 
</p>
<p>   a Variable(s) entered on step 1:fruitvegetables, unhealthysnacks, fastfoodmeal, physical activities 
</p>
<p>56 Optimal Binning for Finding High Risk Cut-offs (1,445 Families)</p>
<p/>
</div>
<div class="page"><p/>
<p>357
</p>
<p>    The output shows that the predictors are very signifi cant independent predictors 
</p>
<p>of overweight children. Next the bin variable will be used.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Regression&hellip;.Binary Logistic&hellip;.Dependent: enter overweight children 
</p>
<p>&hellip;.Covariates: enter fruitvegetables_bin, unhealthysnack_bin, fastfoodmeal_bin, 
</p>
<p>physicalactivities_bin&hellip;.click OK.   
</p>
<p> Variables in the equation 
</p>
<p> B  S.E.  Wald  df  Sig.  Exp(B) 
</p>
<p> Step 1 a   Fruitvegetables_bin  &minus;1,694  ,228  55,240  1  ,000  ,184 
</p>
<p> Unhealthysnacks_bin  1,264  ,118  113,886  1  ,000  3,540 
</p>
<p> Fastfbodmeal_bin  ,530  ,169  9,827  1  ,002  1,698 
</p>
<p> Physicalactivities_bin  ,294  ,167  3,086  1  ,079  1,341 
</p>
<p> Constant  &minus;2,176  ,489  19,803  1  ,000  ,114 
</p>
<p>   a Variable(s) entered on step 1: fruitvegetables bin, unhealthysnacks bin, fastfoodmeal bin, physica-
</p>
<p>lactivities_bin 
</p>
<p>    If p &lt; 0.10 is used to indicate statistical signifi cance, all of the bin variables are 
</p>
<p>independent predictors, though at a somewhat lower level of signifi cance than the 
</p>
<p>original variables. Obviously, in the current example some precision is lost by the 
</p>
<p>binning procedure. This is, because information may be lost if you replace a con-
</p>
<p>tinuous variable with a binary or nominal one. Nonetheless, the method is precious 
</p>
<p>for identifying high risk cut-offs for recommendation purposes.  
</p>
<p>    Conclusion 
</p>
<p> Optimal binning variables instead of the original continuous variables may either 
</p>
<p>produce (1) better statistics, because unnecessary noise due to the continuous 
</p>
<p>scaling may be deleted, (2) worse statistics, because information may be lost if your 
</p>
<p>replace a continuous variable with a binary one. It is more adequate than traditional 
</p>
<p>analyses, if categories are considered clinically more relevant  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of optimal binning is 
</p>
<p>given in Machine learning in medicine part three, Chap. 5, Optimal binning, 
</p>
<p>pp 37&ndash;48, Springer Heidelberg Germany 2013. See also the Chap.   5     of this book for 
</p>
<p>bin membership assessment in future families.    
</p>
<p>Note</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_5">http://dx.doi.org/10.1007/978-3-319-15195-3_5</a></div>
</div>
<div class="page"><p/>
<p>359&copy; Springer International Publishing Switzerland 2015 
T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
Overview, DOI 10.1007/978-3-319-15195-3_57
</p>
<p>    Chapter 57   
</p>
<p> Conjoint Analysis for Determining the Most 
Appreciated Properties of Medicines 
to Be Developed (15 Physicians) 
</p>
<p>                      General Purpose 
</p>
<p> Products like articles of use, food products, or medicines have multiple characteristics. 
</p>
<p>Each characteristic can be measured in several levels, and too many combinations 
</p>
<p>are possible for a single person to distinguish. Conjoint analysis models a limited, 
</p>
<p>but representative and meaningful subset of combinations, which can, subsequently, 
</p>
<p>be presented to persons for preference scaling. The chapter is to assess whether this 
</p>
<p>method is effi cient for the development of new medicines.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Can conjoint analysis be helpful to pharmaceutical institutions for determining the 
</p>
<p>most appreciated properties of medicines they will develop.  
</p>
<p>    Constructing an Analysis Plan 
</p>
<p> A novel medicine is judged by fi ve characteristics:
</p>
<p>    1.    safety expressed in 3 levels,   
</p>
<p>   2.    effi cacy in 3,   
</p>
<p>   3.    price in 3,   
</p>
<p>   4.    pill size in 2,   
</p>
<p>   5.    prolonged activity in 2 levels.     
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 1&rdquo; as Chap. 
20, 2013. </p>
<p/>
</div>
<div class="page"><p/>
<p>360
</p>
<p> From the levels 3 &times; 3 &times; 3 &times; 2 &times; 2 = 108 combinations can be formed, which is too 
</p>
<p>large a number for physicians to distinguish. In addition, some combinations, e.g., 
</p>
<p>high price and low effi cacy will never be prefered and could be skipped from the 
</p>
<p>listing. Instead, a limited but representative number of profi les is selected. SPSS 
</p>
<p>statistical software 19.0 is used for the purpose.
</p>
<p>  Command: 
</p>
<p>  Data&hellip;.Orthogonal Design&hellip;.Generate&hellip;.Factor Name: enter safety&hellip;.Factor 
</p>
<p>Label: enter safety design&hellip;.click Add&hellip;.click ?....click Defi ne Values: enter 1,2,3 
</p>
<p>on the left, and A,B,C on the right side&hellip;.Do the same for all of the characteristics 
</p>
<p>(here called factors)&hellip;.click Create a new dataset&hellip;.Dataset name: enter medicine_
</p>
<p>plan&hellip;.click Options: Minimum number of cases: enter 18&hellip;.mark Number of hold-
</p>
<p>out cases: enter 4&hellip;.Continue&hellip;.OK.    
</p>
<p> The output sheets show a listing of 22, instead of 108, combinations with two new 
</p>
<p>variables (status_ and card_) added. The variable Status_ gives a &ldquo;0&rdquo; to the fi rst 18 
</p>
<p>combinations used for subsequent analyses, and &ldquo;1&rdquo; to holdout combinations to be 
</p>
<p>used by the computer for checking the validity of the program. The variable Card_ 
</p>
<p>gives identifi cation numbers to each combination. For further use of the model 
</p>
<p>designed so far, we will fi rst need to perform the Display Design commands.
</p>
<p>  Command: 
</p>
<p>  Data&hellip;.Orthogonal Design&hellip;.Display&hellip;.Factors: transfer all of the characteristics 
</p>
<p>to this window&hellip;.click Listing for experimenter&hellip;.click OK.    
</p>
<p> The output sheet now shows a plan card, which looks virtually identical to the 
</p>
<p>above 22 profi le listing. It must be saved. We will use the name medicine_plan for 
</p>
<p>the fi le. For convenience the design fi le is given on the internet at extras.springer.
</p>
<p>com. The next thing is to use SPSS&rsquo; syntax program to complete the preparation for 
</p>
<p>real data analysis.
</p>
<p>  Command: 
</p>
<p>  click File&hellip;.move to Open&hellip;.move to Syntax&hellip;.enter the following text&hellip;.  
</p>
<p>  CONJOINT PLAN = 'g:medicine_plan.sav'  
</p>
<p>  /DATA = 'g:medicine_prefs.sav'  
</p>
<p>  /SEQUENCE = PREF1 TO PREF22  
</p>
<p>  /SUBJECT = ID  
</p>
<p>  /FACTORS = SAFETY EFFICACY (DISCRETE)  
</p>
<p>  PRICE (LINEAR LESS)  
</p>
<p>  PILLSIZE PROLONGEDACTIVITY (LINEAR MORE)  
</p>
<p>  /PRINT = SUMMARYONLY.    
</p>
<p> Save this syntax fi le at the directory of your choice. Note: the conjoint fi le 
</p>
<p>entitled &ldquo;conjoint&rdquo; only works, if both the plan fi le and the data fi le to be analyzed 
</p>
<p>are correctly entered in the above text. In our example we saved both fi les at a USB 
</p>
<p>stick (recognised by our computer under the directory &ldquo;g:&rdquo;). For convenience the 
</p>
<p>conjoint fi le entitled &ldquo;conjoint&rdquo; is also given at extras.springer.com. Prior to use it 
</p>
<p>should also be saved at the USB-stick. 
</p>
<p>57 Conjoint Analysis for Determining the Most Appreciated Properties of Medicines&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>361
</p>
<p> The 22 combinations including the 4 holdouts, can now be used to perform a 
</p>
<p>conjoint analysis with real data. For that purpose 15 physicians are requested to 
</p>
<p>express their preferences of the 22 different combinations. 
</p>
<p> The preference scores are entered in the data fi le with the IDs of the physicians 
</p>
<p>as a separate variable in addition to the 22 combinations (the columns). For conve-
</p>
<p>nience the data fi le entitled &ldquo;medicine_prefs&rdquo; is given at extras.springer.com, but, if 
</p>
<p>you want to use it, it should fi rst be saved at the USB stick. The conjoint analysis 
</p>
<p>can now be successfully performed.  
</p>
<p>    Performing the Final Analysis 
</p>
<p>   Command: 
</p>
<p>  Open the USB stick&hellip;.click conjoint&hellip;.the above syntax text is shown&hellip;.click 
</p>
<p>Run&hellip;select All.   
</p>
<p> Model description 
</p>
<p> N of levels  Relation to ranks or scores 
</p>
<p> Safety  3  Linear (more) 
</p>
<p> Effi cacy  3  Linear (more) 
</p>
<p> Price  3  Linear (less) 
</p>
<p> Pillsize  2  Discrete 
</p>
<p> Prolongedactivity  2  Discrete 
</p>
<p>  All factors are orthogonal 
</p>
<p>    The above table gives an overview of the different characteristics (here called fac-
</p>
<p>tors), and their levels used to construct an analysis plan of the data from our data fi le.
</p>
<p> Utilities 
</p>
<p> Utility estimate  Std. error 
</p>
<p> Pillsize  Large  &minus;1,250  ,426 
</p>
<p> Small  1,250  ,426 
</p>
<p> Prolongedactivity  No  &minus;,733  ,426 
</p>
<p> Yes  ,733  ,426 
</p>
<p> Safety  A*  1,283  ,491 
</p>
<p> B*  2,567  ,983 
</p>
<p> C*  3,850  1,474 
</p>
<p> Effi cacy  High  &minus;,178  ,491 
</p>
<p> Medium  &minus;,356  ,983 
</p>
<p> Low  &minus;,533  1,474 
</p>
<p> Price  $4  &minus;1,189  ,491 
</p>
<p> $6  &minus;2,378  ,983 
</p>
<p> $8  &minus;3,567  1,474 
</p>
<p> (Constant)  10,328  1,761 
</p>
<p>Performing the Final Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>362
</p>
<p>   The above table gives the utility scores, which are the overall levels of the 
</p>
<p>preferences expressed by the physicians. The meaning of the levels are given:
</p>
<p>   safety level C: best safety  
</p>
<p>  effi cacy level high: best effi cacy  
</p>
<p>  pill size 2: smallest pill  
</p>
<p>  prolonged activity 2: prolonged activity present  
</p>
<p>  price $8: most expensive pill.    
</p>
<p> Generally, higher scores mean greater preference. There is an inverse relationship 
</p>
<p>between pill size and preference, and between pill costs and preference. The safest 
</p>
<p>pill and the most effi caceous pill were given the best preferences. 
</p>
<p> However, the regression coeffi cients for effi cacy were, statistically, not very 
</p>
<p>signifi cant. Nonetheless, they were included in the overall analysis by the software 
</p>
<p>program. As the utility scores are simply linear regression coeffi cients, the scores 
</p>
<p>can be used to compute total utilities (add-up preference scores) for a medicine with 
</p>
<p>known characteristic levels. An interesting thing about the methodology is that, like 
</p>
<p>with linear regression modeling, the characteristic levels can be used to calculate an 
</p>
<p>individual add-up utility score (preference score) for a pill with e.g., the underneath 
</p>
<p>characteristics:
</p>
<p>   (1) pill size (small) + (2) prolonged activity (yes) + safety (C) + effi cacy (high) + price 
</p>
<p>($4) = 1.250 + 0.733 + 3.850&minus;0.178&minus;1.189 + constant (10.328) = 14.974.    
</p>
<p> For the underneath pill the add-up utility score is, as expected, considerably 
</p>
<p>lower.
</p>
<p>   (1) pill size (large) + (2) prolonged activity (no) + safety (A) + effi cacy (low) + price 
</p>
<p>($8) = &minus;1.250-0.733 + 1.283&minus;0.533&minus;3.567 + constant (10.328) = 5.528.    
</p>
<p> The above procedure is the real power of conjoint analysis. It enables to predict 
</p>
<p>preferences for combinations that were not rated by the physicians. In this way you 
</p>
<p>will obtain an idea about the preference to be received by a medicine with known 
</p>
<p>characteristics.
</p>
<p> Importance values 
</p>
<p> Pillsize  15,675 
</p>
<p> Prolongedactivity  12,541 
</p>
<p> Safety  28,338 
</p>
<p> Effi cacy  12,852 
</p>
<p> Price  30,594 
</p>
<p>  Averaged Importance Score 
</p>
<p>    The range of the utility (preference) scores for each characteristic is an indication 
</p>
<p>of how important the characteristic is. Characteristics with greater ranges play a 
</p>
<p>larger role than the others. As observed the safety and price are the most important 
</p>
<p>preference producing characteristics, while prolonged activity, effi cacy, and pill size 
</p>
<p>appear to play a minor role according to the respondents&rsquo; judgments. The ranges are 
</p>
<p>computed such that they add-up to 100 (%).
</p>
<p>57 Conjoint Analysis for Determining the Most Appreciated Properties of Medicines&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>363
</p>
<p> Coeffi cients 
</p>
<p> B coeffi cient 
</p>
<p> Estimate 
</p>
<p> Safety  1,283 
</p>
<p> Effi cacy  &minus;,178 
</p>
<p> Price  &minus;1,189 
</p>
<p>   The above table gives the linear regression coeffi cients for the factors that are 
</p>
<p>specifi ed as linear. The interpretation of the utility (preference) score for the cheapest 
</p>
<p>pill equals $4 &times; (&minus;1.189) = &minus;4.756
</p>
<p> Correlations a  
</p>
<p> Value  Sig. 
</p>
<p> Pearson&rsquo;s R  ,819  ,000 
</p>
<p> Kendall&rsquo;s tau  ,643  ,000 
</p>
<p> Kendall&rsquo;s tau for Holdouts  ,333  ,248 
</p>
<p>   a Correlations between observed and estimated preferences 
</p>
<p>    The correlation coeffi cients between the observed preferences and the preferences 
</p>
<p>calculated from conjoint model shows that the correlations by Pearson and Kendall&rsquo;s 
</p>
<p>method are pretty good, indicating that the conjoint methodology produced a sensitive 
</p>
<p>prediction model. The regression analysis of the holdout cases is intended as a 
</p>
<p>validity check, and produced a pretty large p-value of 24.8 %. Still it means that we 
</p>
<p>have about 75 % to fi nd no type I error in this procedure.
</p>
<p> Number of reversals 
</p>
<p> Factor  Effi cacy  9 
</p>
<p> Price  5 
</p>
<p> Safety  4 
</p>
<p> Prolongedactivity  0 
</p>
<p> Pillsize  0 
</p>
<p> Subject  1  Subject 1  1 
</p>
<p> 2  Subject 2  0 
</p>
<p> 3  Subject 3  0 
</p>
<p> 4  Subject 4  1 
</p>
<p> 5  Subject5  3 
</p>
<p> 6  Subject 6  1 
</p>
<p> 7  Subject 7  3 
</p>
<p> 8  Subject 8  2 
</p>
<p> 9  Subject 9  1 
</p>
<p> 10  Subject 10  0 
</p>
<p> 11  Subject 11  1 
</p>
<p> 12  Subject 12  1 
</p>
<p> 13  Subject 13  0 
</p>
<p> 14  Subject 14  1 
</p>
<p> 15  Subject 15  3 
</p>
<p>Performing the Final Analysis</p>
<p/>
</div>
<div class="page"><p/>
<p>364
</p>
<p>   Finally, the conjoint program reports the number of physicians whose preference 
</p>
<p>was different from what was expected. Particularly in the effi cacy characteristic 
</p>
<p>there were 9 of the 15 physicians who chose differently from expected, underlining 
</p>
<p>the limited role of this characteristic.  
</p>
<p>    Conclusion 
</p>
<p> Conjoint analysis is helpful to pharmaceutical institutions for determining the most 
</p>
<p>appreciated properties of medicines they will develop. Disadvantage include: (1) it 
</p>
<p>is pretty complex; (2) it may be hard to respondents to express preferences; (3) other 
</p>
<p>characteristics not selected may be important too, e.g., physical and pharmacological 
</p>
<p>factors.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of conjoint modeling 
</p>
<p>is given in Machine learning in medicine part three, Chap. 19, Conjoint analysis, 
</p>
<p>pp 217&ndash;230, Springer Heidelberg Germany 2013.    
</p>
<p>57 Conjoint Analysis for Determining the Most Appreciated Properties of Medicines&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>365&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_58
</p>
<p>    Chapter 58   
</p>
<p> Item Response Modeling for Analyzing 
Quality of Life with Better Precision 
(1,000 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> Item response tests are goodness of fi t tests for analyzing the item scores of 
</p>
<p> intelligence tests, and they perform better for the purpose than traditional tests, 
</p>
<p>based on reproducibility measures, do. Like intelligence, quality of life is a 
</p>
<p> multidimensional construct, and may, therefore, be equally suitable for item 
</p>
<p>response modeling.  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> Can quality of life data be analyzed through item response modeling, and provide 
</p>
<p>more sensitivity than classical linear models do?  
</p>
<p>    Example 
</p>
<p> As an example we will analyze the 5-items of a mobility-domain of a quality of life 
</p>
<p>(QOL) battery for patients with coronary artery disease in a group of 1,000 patients. 
</p>
<p>Instead of fi ve many more items can be included. However, for the purpose of simplic-
</p>
<p>ity we will use only fi ve items: the domain mobility in a quality of life battery was 
</p>
<p>assessed by answering &ldquo;yes or no&rdquo; to experienced diffi culty (1) while climbing stair, 
</p>
<p>(2) on short distances, (3) on long distances, (4) on light household work, 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 2&rdquo; as Chap. 
</p>
<p>12, 2014. </p>
<p/>
</div>
<div class="page"><p/>
<p>366
</p>
<p>(5) on heavy household work. In the underneath table the data of 1,000 patients are 
</p>
<p>summarized. These data can be fi tted into a standard normal Gaussian frequency distri-
</p>
<p>bution curve (see underneath fi gure). From it, it can be seen that the items used here are 
</p>
<p>more adequate for demonstrating low quality of life than they are for demonstrating 
</p>
<p>high quality of life, but, nonetheless, an entire Gaussian distribution can be extrapo-
</p>
<p>lated from the data given. The lack of histogram bars on the right side of the Gaussian 
</p>
<p>curve suggests that more high quality of life items in the questionnaire would be wel-
</p>
<p>come in order to improve the fi t of the histogram into the Gaussian curve. Yet it is 
</p>
<p>interesting to observe that, even with a limited set of items, already a fairly accurate 
</p>
<p>frequency distribution pattern of all quality of life levels of the population is obtained. 
</p>
<p> No. response 
</p>
<p>pattern 
</p>
<p> Response pattern (1 = yes, 2 = no) 
</p>
<p>to items 1 to 5 
</p>
<p> Observed 
</p>
<p>Frequencies 
</p>
<p> 1  11111  4 
</p>
<p> 2  11112  7 
</p>
<p> 3  11121  3 
</p>
<p> 4  11122  12 
</p>
<p> 5  11211  2 
</p>
<p> 6  11212  2 
</p>
<p> 7  11221  4 
</p>
<p> 8  11222  5 
</p>
<p> 9  12111  2 
</p>
<p> 10  12112  9 
</p>
<p> 11  12121  1 
</p>
<p> 12  12122  17 
</p>
<p> 13  12211  1 
</p>
<p> 14  12212  4 
</p>
<p> 15  12221  3 
</p>
<p> 16  12222  16 
</p>
<p> 17  21111  11 
</p>
<p> 18  21112  30 
</p>
<p> 19  21121  15 
</p>
<p> 20  21122  21 
</p>
<p> 21  21211  4 
</p>
<p> 22  21212  29 
</p>
<p> 23  21221  16 
</p>
<p> 24  21222  81 
</p>
<p> 25  22111  17 
</p>
<p> 26  22112  57 
</p>
<p> 27  22121  22 
</p>
<p> 28  22122  174 
</p>
<p> 29  22211  12 
</p>
<p> 30  22212  62 
</p>
<p> 31  22221  29 
</p>
<p> 32  22222  263 
</p>
<p>58 Item Response Modeling for Analyzing Quality of Life with Better Precision&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>367
</p>
<p>    
</p>
<p>0
</p>
<p>20
</p>
<p>40
</p>
<p>60
</p>
<p>80
</p>
<p>100
</p>
<p>120
</p>
<p>140
</p>
<p>160
</p>
<p>180
</p>
<p>200
</p>
<p>220
</p>
<p>240
</p>
<p>260
</p>
<p>1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43
</p>
<p>Response pattern
</p>
<p>F
re
</p>
<p>q
u
</p>
<p>e
n
</p>
<p>c
y
</p>
<p>  
</p>
<p>    The LTA-2 (Latent Trait Analysis &ndash; 2) free software program is used (Uebersax 
</p>
<p>J. Free Software LTA (latent trait analysis) -2 (with binary items), 2006,   www.john- 
</p>
<p>uebersax.com/stat/Ital.htm    ). The data fi le entitled &ldquo;itemresponsemodeling&rdquo; is 
</p>
<p> available in extras.springer.com. We enter the data fi le by the traditional copy and 
</p>
<p>paste commands.
</p>
<p>  Command: 
</p>
<p>  Gaussian error model for IRF (Instrument Response Function) shape&hellip;.chi-square 
</p>
<p>goodness of fi t for Fit Statistics&hellip;. Frequency table&hellip;.EAP score table.    
</p>
<p> The software program calculates the quality of life scores of the different 
</p>
<p>response patterns as EAP (Expected Ability a Posteriori) scores. These scores can 
</p>
<p>be considered as the z-values of a normal Gaussian curve, meaning that the associ-
</p>
<p>ated area under curve (AUC) of the Gaussian curve is an estimate of the level of 
</p>
<p>quality of life. 
</p>
<p> There is, approximately,
</p>
<p>   a 50 % quality of life level with an EAP score of 0,  
</p>
<p>  a 35 % QOL level with an EAP score of &minus;1 (standard deviations),  
</p>
<p>  a 2.5 %  &ldquo;   &rdquo;   of &minus;2  
</p>
<p>  a 85 %    &ldquo;   &rdquo;   of +1  
</p>
<p>  a 97.5 %  &ldquo;   &rdquo;   of +2    
</p>
<p>Example</p>
<p/>
<div class="annotation"><a href="http://www.john-uebersax.com/stat/Ital.htm">http://www.john-uebersax.com/stat/Ital.htm</a></div>
<div class="annotation"><a href="http://www.john-uebersax.com/stat/Ital.htm">http://www.john-uebersax.com/stat/Ital.htm</a></div>
</div>
<div class="page"><p/>
<p>368
</p>
<p> No. response 
</p>
<p>Pattern 
</p>
<p> Response pattern (1 = yes, 2 = no) 
</p>
<p>to items 1 to 5 
</p>
<p> EAP scores 
</p>
<p>(SDs) 
</p>
<p> AUCs 
</p>
<p>(QOL levels) (%) 
</p>
<p> Classical 
</p>
<p>Scores (0&ndash;5) 
</p>
<p> 1.  11111  &ndash;1.8315  3.4  0 
</p>
<p> 2.  11112  &ndash;1.4425  7.5  1 
</p>
<p> 3.  11121  &ndash;1.4153  7.8  1 
</p>
<p> 4.  11122  &ndash;1.0916  15.4  2 
</p>
<p> 5.  11211  &ndash;1.2578  10.4  1 
</p>
<p> 6.  11212  &ndash;0.8784  18.9  2 
</p>
<p> 7.  11221  &ndash;0.8600  19.4  2 
</p>
<p> 8.  11222  &ndash;0.4596  32.3  3 
</p>
<p> 9.  12111  &ndash;1.3872  8.2  1 
</p>
<p> 10.  12112  &ndash;0.9946  16.1  2 
</p>
<p> 11.  12121  &ndash;0.9740  16.6  2 
</p>
<p> 12.  12122  &ndash;0.5642  28.8  3 
</p>
<p> 13.  12211  &ndash;0.8377  20.1  2 
</p>
<p> 14.  12212  &ndash;0.4389  33.0  3 
</p>
<p> 15.  12221  &ndash;0.4247  33.4  3 
</p>
<p> 16.  12222  0.0074  50.4  4 
</p>
<p> 17.  21111  &ndash;1.3501  8.9  1 
</p>
<p> 18.  21112  &ndash;0.9381  17.4  2 
</p>
<p> 19.  21121  &ndash;0.9172  17.9  2 
</p>
<p> 20.  21122  &ndash;0.4866  31.2  3 
</p>
<p> 21.  21211  &ndash;0.7771  21.8  2 
</p>
<p> 22.  21212  &ndash;0.3581  35.9  3 
</p>
<p> 23.  21221  &ndash;0.3439  36.7  3 
</p>
<p> 24.  21222  0.1120  54.4  4 
</p>
<p> 25.  22111  &ndash;0.8925  18.7  2 
</p>
<p> 26.  22112  &ndash;0.4641  32.3  3 
</p>
<p> 27.  22121  &ndash;0.4484  32.6  3 
</p>
<p> 28.  22122  0.0122  50.4  4 
</p>
<p> 29.  22211  &ndash;0.3231  37.5  3 
</p>
<p> 30.  22212  0.1322  55.2  4 
</p>
<p> 31.  22221  0.1433  55.6  4 
</p>
<p> 32.  22222  0.6568  74.5  5 
</p>
<p>   EAP  expected ability a posteriori,  QOL  quality of life 
</p>
<p>    In the above table the EAP scores per response pattern is given as well as the 
</p>
<p>AUC (= quality of life level) values as calculated by the software program are given. 
</p>
<p>In the fourth column the classical score is given ranging from 0 (no yes answers) to 
</p>
<p>5 (5 yes answers). Unlike the classical scores, running from 0 to 100 %, the item 
</p>
<p>scores are more precise and vary from 3.4 to 74.5 % with an overall mean score, by 
</p>
<p>defi nition, of 50 %. The item response model produce an adequate fi t for the data as 
</p>
<p>demonstrated by chi-square goodness of fi t values/degrees of freedom of 0.86. What 
</p>
<p>is even more important, is, that we have 32 different QOL scores instead of no more 
</p>
<p>58 Item Response Modeling for Analyzing Quality of Life with Better Precision&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>369
</p>
<p>than fi ve as observed with the classical score method. With six items the numbers of 
</p>
<p>scores would even rise to 64. The interpretation is: the higher the score, the better 
</p>
<p>the quality of life.  
</p>
<p>    Conclusion 
</p>
<p> Quality of life assessments can be analyzed through item response modeling, and 
</p>
<p>provide more sensitivity than classical linear models do.  
</p>
<p>    Note 
</p>
<p> More background theoretical and mathematical information of item response mod-
</p>
<p>eling is given in Machine learning in medicine part one, Chap. 8, Item response 
</p>
<p>modeling, pp 87&ndash;98, edited by Springer Heidelberg Germany, 2012, from the same 
</p>
<p>authors. In the current chapter the LTA-2 the free software program is used (Uebersax 
</p>
<p>J. Free Software LTA (latent trait analysis) -2 (with binary items), 2006,   www.john- 
</p>
<p>uebersax.com/stat/Ital.htm    ).    
</p>
<p>Note</p>
<p/>
<div class="annotation"><a href="http://www.john-uebersax.com/stat/Ital.htm">http://www.john-uebersax.com/stat/Ital.htm</a></div>
<div class="annotation"><a href="http://www.john-uebersax.com/stat/Ital.htm">http://www.john-uebersax.com/stat/Ital.htm</a></div>
</div>
<div class="page"><p/>
<p>371&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_59
</p>
<p>    Chapter 59   
</p>
<p> Survival Studies with Varying 
Risks of Dying (50 and 60 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> Patients&rsquo; predictors of survival may change across time, because people may change 
</p>
<p>their lifestyles. Standard statistical methods do not allow adjustments for time- 
</p>
<p>dependent predictors. Time-dependent Cox regression has been introduced as a 
</p>
<p>method adequate for the purpose.  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> Predictors of survival may change across time, e.g., the effect of smoking, cholesterol, 
</p>
<p>and increased blood pressure on cardiovascular disease, and patients&rsquo; frailty in 
</p>
<p>oncology research.  
</p>
<p>    Examples 
</p>
<p>    Cox Regression with a Time-Dependent Predictor 
</p>
<p> The level of LDL cholesterol is a strong predictor of cardiovascular survival. 
</p>
<p>However, in a survival study virtually no one will die from elevated values in the 
</p>
<p>fi rst decade of observation. LDL cholesterol may be, particularly, a killer in the 
</p>
<p>second decade of observation. The Cox regression model is not appropriate for ana-
</p>
<p>lyzing the effect of LDL cholesterol on survival, because it assumes that the relative 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 2&rdquo; as Chap. 13, 
</p>
<p>2014. </p>
<p/>
</div>
<div class="page"><p/>
<p>372
</p>
<p>hazard of dying is the same in the fi rst, second and third decade. If you want to 
</p>
<p>analyze such data, an extended Cox regression model allowing for non-proportional 
</p>
<p>hazards can be applied, and is available in SPSS statistical software. In the under-
</p>
<p>neath example the fi rst 10 of 60 patients are given. They were followed for 30 years 
</p>
<p>for the occurrence of a cardiovascular event. Each row represents a patient, the 
</p>
<p>columns are the patient characteristics, otherwise called the variables. 
</p>
<p> Variable (Var) 
</p>
<p> 1  2  3  4  5  6 
</p>
<p> 1,00  1  0  65,00  0,00  2,00 
</p>
<p> 1,00  1  0  66,00  0,00  2,00 
</p>
<p> 2,00  1  0  73,00  0,00  2,00 
</p>
<p> 2,00  1  0  54,00  0,00  2,00 
</p>
<p> 2,00  1  0  46,00  0,00  2,00 
</p>
<p> 2,00  1  0  37,00  0,00  2,00 
</p>
<p> 2,00  1  0  54,00  0,00  2,00 
</p>
<p> 2,00  1  0  66,00  0,00  2,00 
</p>
<p> 2,00  1  0  44,00  0,00  2,00 
</p>
<p> 3,00  0  0  62,00  0,00  2,00 
</p>
<p>  Var 00001 = follow-up period (years) (Var = variable) 
</p>
<p> Var 00002 = event (0 or 1, event or lost for follow-up = censored) 
</p>
<p> Var 00003 = treatment modality (0 = treatment-1, 1 = treatment-2) 
</p>
<p> Var 00004 = age (years) 
</p>
<p> Var 00005 = gender (0 or 1, male or female) 
</p>
<p> Var 00006 = LDL-cholesterol (0 or 1, &lt; 3.9 or &gt; = 3.9 mmol/l) 
</p>
<p>    The entire data fi le is in extras.springer.com, and is entitled &ldquo;survivalvaryingrisks&rdquo;. 
</p>
<p>Start by opening the fi le. First, a usual Cox regression is performed with LDL-
</p>
<p>cholesterol as predictor of survival (var = variable).
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.survival&hellip;.Cox regression&hellip;.time: follow months&hellip;. status: var 2&hellip;.
</p>
<p>defi ne event (1)&hellip;.Covariates&hellip;.categorical: elevated LDL-cholesterol (Var 00006) 
</p>
<p>= &gt; categorical variables&hellip;.continue&hellip;.plots&hellip;. survival = &gt; &hellip;.hazard&hellip;.continue&hellip;.
</p>
<p>OK.   
</p>
<p> Variables in the equation 
</p>
<p> B  SE  Wald  df  Sig.  Exp(B) 
</p>
<p> VAR00006  &minus;,482  ,307  2,462  1  ,117  ,618 
</p>
<p> Variables in the equation 
</p>
<p> B  SE  Wald  df  Siq.  Exp(B) 
</p>
<p> T_COV_  &minus;,131  ,033  15,904  1  ,000  ,877 
</p>
<p>59 Survival Studies with Varying Risks of Dying (50 and 60 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>373
</p>
<p>   The upper table shows that elevated LDL-cholesterol is not a signifi cant predic-
</p>
<p>tor of survival with a p-value as large as 0.117 and a hazard ratio of 0.618. In order 
</p>
<p>to assess, whether elevated LDL-cholesterol adjusted for time has an effect on sur-
</p>
<p>vival, a time-dependent Cox regression will be performed as shown in the above 
</p>
<p>lower table. For that purpose the time-dependent covariate is defi ned as a function 
</p>
<p>of both the variable time (called &ldquo;T_&rdquo; in SPSS) and the LDL-cholesterol-variable, 
</p>
<p>while using the product of the two. This product is applied as the &ldquo;time-dependent 
</p>
<p>predictor of survival&rdquo;, and a usual Cox model is, subsequently, performed 
</p>
<p>(Cov = covariate).
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.survival&hellip;.Cox w/Time-Dep Cov&hellip;.Compute Time-Dep Cov&hellip;.Time 
</p>
<p>(T_) = &gt; in box Expression for T_Cov&hellip;.add the sign * &hellip;.add the LDL- cholesterol 
</p>
<p>variable&hellip;.model&hellip;.time: follow months&hellip;.status: var 00002&hellip;.?: defi ne event:1&hellip;.
</p>
<p>continue&hellip;.T_Cov = &gt; in box covariates&hellip;.OK.    
</p>
<p> The above lower table shows that elevated LDL-cholesterol after adjustment for 
</p>
<p>differences in time is a highly signifi cant predictor of survival. If we look at the 
</p>
<p>actual data of the fi le, we will observe that, overall, the LDL-cholesterol variable is 
</p>
<p>not an important factor. But, if we look at the blood pressures of the three decades 
</p>
<p>separately, then it is observed that something very special is going on: in the fi rst 
</p>
<p>decade virtually no one with elevated LDL-cholesterol dies. In the second decade 
</p>
<p>virtually everyone with an elevated LDL-cholesterol does: LDL cholesterol seems 
</p>
<p>to be particularly a killer in the second decade. Then, in the third decade other reasons 
</p>
<p>for dying seem to have occurred.  
</p>
<p>    Cox Regression with a Segmented Time-Dependent Predictor 
</p>
<p> Some variables may have different values at different time periods. For example, 
</p>
<p>elevated blood pressure may be, particularly, harmful not after decades but at the 
</p>
<p>very time-point it is highest. The blood pressure is highest in the fi rst and third 
</p>
<p>decade of the study. However, in the second decade it is mostly low, because the 
</p>
<p>patients were adequately treated at that time. For the analysis we have to use the 
</p>
<p>socalled logical expressions. They take the value 1, if the time is true, and 0, if false. 
</p>
<p>Using a series of logical expressions, we can create our time-dependent predictor, 
</p>
<p>that can, then, be analyzed by the usual Cox model. In the underneath example 11 
</p>
<p>of 60 patients are given. The entire data fi le is in extras.springer.com, and is entitled 
</p>
<p>&ldquo;survivalvaryingrisks2&rdquo; The patients were followed for 30 years for the occurrence 
</p>
<p>of a cardiovascular event. Each row represents again a patient, the columns are the 
</p>
<p>patient characteristics. 
</p>
<p>Examples</p>
<p/>
</div>
<div class="page"><p/>
<p>374
</p>
<p> Var 1  2  3  4  5  6  7 
</p>
<p> 7,00  1  76  ,00  133,00  .  . 
</p>
<p> 9,00  1  76  ,00  134,00  .  . 
</p>
<p> 9,00  1  65  ,00  143,00  .  . 
</p>
<p> 11,00  1  54  ,00  134,00  110,00  . 
</p>
<p> 12,00  1  34  ,00  143,00  111,00  . 
</p>
<p> 14,00  1  45  ,00  135,00  110,00  . 
</p>
<p> 16,00  1  56  1,00  123,00  103,00  . 
</p>
<p> 17,00  1  67  1,00  133,00  107,00  . 
</p>
<p> 18,00  1  86  1,00  134,00  108,00  . 
</p>
<p> 30,00  1  75  1,00  134,00  102,00  134,00 
</p>
<p> 30,00  1  65  1,00  132,00  121,00  126,00 
</p>
<p>  Var 00001 = follow-up period years (Var = variable) 
</p>
<p> Var 00002 = event (0 or 1, event or lost for follow-up = censored) 
</p>
<p> Var 00003 = age (years) 
</p>
<p> Var 00004 = gender 
</p>
<p> Var 00005 = mean blood pressure in the fi rst decade 
</p>
<p> Var 00006 = mean blood pressure in the second decade 
</p>
<p> Var 00007 = mean blood pressure in the third decade 
</p>
<p>    In the second and third decade an increasing number of patients have been lost. 
</p>
<p>The following time-dependent covariate must be constructed for the analysis of 
</p>
<p>these data (* = sign of multiplication) using the click Transform and click Compute 
</p>
<p>Variable commands:
</p>
<p>   (T_ &gt; =1&amp;T_ &lt; 11)*Var 5 + (T_ &gt; =11&amp;T_ &lt; 21)*Var 6 + (T_ &gt; =21&amp;T_ &lt; 31)*Var 7    
</p>
<p> This novel predictor variable is entered in the usual way with the commands 
</p>
<p>(Cov = covariate):
</p>
<p>   Model&hellip;.time: follow months&hellip;.status: var 00002&hellip;.?: defi ne event:1 &ndash; continue&hellip;.
</p>
<p>T_Cov = &gt; in box covariates&hellip;.OK.    
</p>
<p> The underneath table shows that, indeed, a mean blood pressure after adjustment 
</p>
<p>for difference in decades is a signifi cant predictor of survival at p = 0.040, and with 
</p>
<p>a hazard ratio of 0.936 per mm Hg. In spite of the better blood pressures in the second 
</p>
<p>decade, blood pressure is a signifi cant killer in the overall analysis.
</p>
<p> Variables in the equation 
</p>
<p> B  SE  Wald  df  Sig.  Exp(B) 
</p>
<p> T_COV_  &minus;,066  ,032  4,238  1  ,040  ,936 
</p>
<p>        Conclusion 
</p>
<p> Many predictors of survival change across time, e.g., the effect of smoking, choles-
</p>
<p>terol, and increased blood pressure in cardiovascular research, and patients&rsquo; frailty 
</p>
<p>in oncology research.  
</p>
<p>59 Survival Studies with Varying Risks of Dying (50 and 60 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>375
</p>
<p>    Note 
</p>
<p> More background theoretical and mathematical information is given in Machine 
</p>
<p>learning in medicine part one, Chap. 9, Time-dependent predictor modeling, 
</p>
<p>pp 99&ndash;111, Springer Heidelberg Germany, 2012, from the same authors.    
</p>
<p> Note</p>
<p/>
</div>
<div class="page"><p/>
<p>377&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_60
</p>
<p>    Chapter 60   
</p>
<p> Fuzzy Logic for Improved Precision 
of Dose- Response Data (8 Induction Dosages) 
</p>
<p>                      General Purpose 
</p>
<p> Fuzzy logic can handle questions to which the answers may be &ldquo;yes&rdquo; at one time 
</p>
<p>and &ldquo;no&rdquo; at the other, or may be partially true and untrue. Pharmacodynamic data 
</p>
<p>deal with questions like &ldquo;does a patient respond to a particular drug dose or not&rdquo;, or 
</p>
<p>&ldquo;does a drug cause the same effects at the same time in the same subject or not&rdquo;. 
</p>
<p>Such questions are typically of a fuzzy nature, and might, therefore, benefi t from an 
</p>
<p>analysis based on fuzzy logic.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> This chapter is to study whether fuzzy logic can improve the precision of predictive 
</p>
<p>models for pharmacodynamic data.  
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 2&rdquo; as Chap. 
</p>
<p>14, 2014. </p>
<p/>
</div>
<div class="page"><p/>
<p>378
</p>
<p>    Example 
</p>
<p> Imput values  output values  fuzzy-modeled output 
</p>
<p> induction dosage of thiopental (mg/kg)  numbers of responders (n)  numbers of responders(n) 
</p>
<p> 1  4  4 
</p>
<p> 1.5  5  5 
</p>
<p> 2  6  8 
</p>
<p> 2.5  9  10 
</p>
<p> 3  12  12 
</p>
<p> 3.5  17  14 
</p>
<p> 4  17  16 
</p>
<p> 4.5  12  14 
</p>
<p> 5  9  1 
</p>
<p>   We will use as an example the quantal pharmacodynamic effects of different 
</p>
<p> induction dosages of thiopental on numbers of responding subjects. It is usually not 
</p>
<p>possible to know what type of statistical distribution the experiment is likely to 
</p>
<p> follow, sometimes Gaussian, sometimes very skewed. A pleasant aspect of fuzzy 
</p>
<p>modeling is that it can be applied with any type of statistical distribution and that it 
</p>
<p>is particularly suitable for uncommon and unexpected non linear relationships. 
</p>
<p> Quantal response data are often presented in the literature as S-shape dose- 
</p>
<p>cumulative response curves with the dose plotted on a logarithmic scale, where the 
</p>
<p>log transformation has an empirical basis. We will, therefore, use a logarithmic 
</p>
<p>regression model. SPSS Statistical Software is used for analysis.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;regression&hellip;curve estimation&hellip;dependent variable: data second col-
</p>
<p>umn&hellip;independent variable: data fi rst column&hellip;logarithmic&hellip;OK.    
</p>
<p>    
</p>
<p>60 Fuzzy Logic for Improved Precision of Dose-Response Data&hellip; </p>
<p/>
</div>
<div class="page"><p/>
<p>379
</p>
<p>       
</p>
<p>    The analysis produces a moderate fi t of the data (upper curve) with an r-square 
</p>
<p>value of 0.555 (F-value 8.74, p-value 0.024). 
</p>
<p> We, subsequently, fuzzy-model the imput and output relationships (underneath 
</p>
<p>fi gure). First of all, we create linguistic rules for the imput and output data. 
</p>
<p> For that purpose we divide the universal space of the imput variable into fuzzy 
</p>
<p>memberships with linguistic membership names:
</p>
<p>   imput -zero, -small, -medium, -big, -superbig .    
</p>
<p> Then we do the same for the output variable:
</p>
<p>   output- zero, -small, -medium, -big .    
</p>
<p> Subsequently, we create linguistic rules. 
</p>
<p>  
</p>
<p>0
</p>
<p>0,2
</p>
<p>0,4
</p>
<p>0,6
</p>
<p>0,8
</p>
<p>1
</p>
<p>1 2 3 4 5 6
</p>
<p>Imput variable
</p>
<p>M
e
</p>
<p>m
b
</p>
<p>e
rs
</p>
<p>h
ip
</p>
<p> G
ra
</p>
<p>d
e
</p>
<p>zero small medium big superbig
</p>
<p>  
</p>
<p> Example</p>
<p/>
</div>
<div class="page"><p/>
<p>380
</p>
<p>     
</p>
<p>0
</p>
<p>0,2
</p>
<p>0,4
</p>
<p>0,6
</p>
<p>0,8
</p>
<p>1
</p>
<p>4 6 8 10 12 14 16 18 20
</p>
<p>Output variable
</p>
<p>M
e
</p>
<p>m
b
</p>
<p>e
rs
</p>
<p>h
ip
</p>
<p> G
ra
</p>
<p>d
e
</p>
<p>zero small medium big
</p>
<p>  
</p>
<p>    The fi gure shows that imput- zero  consists of the values 1 and 1.5.
</p>
<p>   The value 1 (100 % membership) has 4 as outcome value (100 % membership of 
</p>
<p>output- zero ).  
</p>
<p>  The value 1.5 (50 % membership) has 5 as outcome value (75 % membership of 
</p>
<p>output- zero , 25 % of output -small ).    
</p>
<p> The imput- zero  produces 100 % &times; 100 % + 50 % &times; 75 % = 137.5 % membership to 
</p>
<p>output- zero , and 50 % &times; 25 % = 12.5 % membership to output- small , and so, output- 
</p>
<p>zero    is the most important output contributor here, and we forget about the small 
</p>
<p>contribution of output- small . 
</p>
<p> Imput- small  is more complex, it consists of the values 1.5, and 2.0, and 2.5.
</p>
<p>   The value 1.5 (50 % membership) has 5 as outcome value (75 % membership of 
</p>
<p>output- zero , 25 % membership of output -small ).  
</p>
<p>  The value 2.0 (100 % membership) has 6 as outcome value (50 % membership of 
</p>
<p>outcome- zero , and 50 % membership of output- small ).  
</p>
<p>  The value 2.5 (50 % membership) has 9 as outcome value (75 % membership of 
</p>
<p>output- small  and 25 % of output- medium ).    
</p>
<p> The imput- small  produces 50 % &times; 75 % + 100 % &times; 50 % = 87.5 % membership to 
</p>
<p>output- zero , 50 % &times; 25 % + 100 % &times; 50 % + 50 % &times; 75% = 100 % membership to 
</p>
<p>output- small   , and 50 % &times; 25 % = 12.5 % membership to output- medium . And so, the 
</p>
<p>output- small  is the most important contributor here, and we forget about the 
</p>
<p>other two. 
</p>
<p> For the other imput memberships similar linguistic rules are determined:
</p>
<p>   Imput- medium  &rarr; output- medium   
</p>
<p>  Imput- big  &rarr; output- big   
</p>
<p>  Imput- superbig  &rarr; output- medium     
</p>
<p> We are, particularly interested in the modeling capacity of fuzzy logic in order to 
</p>
<p>improve the precision of pharmacodynamic modeling. 
</p>
<p> The modeled output value of imput value 1 is found as follows.
</p>
<p>60 Fuzzy Logic for Improved Precision of Dose-Response Data&hellip; </p>
<p/>
</div>
<div class="page"><p/>
<p>381
</p>
<p>   Value 1 is 100 % member of imput- zero , meaning that according to the above lin-
</p>
<p>guistic rules it is also associated with a 100 % membership of output- zero  cor-
</p>
<p>responding with a value of 4.  
</p>
<p>  Value 1.5 is 50 % member of imput- zero  and 50 % imput- small . This means it is 
</p>
<p>50 % associated with the output- zero  and &ndash; small  corresponding with values of 
</p>
<p>50 % &times; (4 + 8) = 6.    
</p>
<p> For all of the imput values modeled output values can be found in this way. The 
</p>
<p>table on page 378, right column shows the results. We perform a logarithmic regres-
</p>
<p>sion on the fuzzy-modeled outcome data similar to that for the un-modeled output 
</p>
<p>values. The fuzzy-modeld output data provided a much better fi t than did the un-
</p>
<p>modeled output values (lower curve) with an r-square value of 0.852 (F-value = 40.34) 
</p>
<p>as compared to 0.555 (F-value 8.74) for the un-modeled output data.  
</p>
<p>    Conclusion 
</p>
<p> Fuzzy logic can handle questions to which the answers may be &ldquo;yes&rdquo; at one time 
</p>
<p>and &ldquo;no&rdquo; at the other, or may be partially true and untrue. Dose response data deal 
</p>
<p>with questions like &ldquo;does a patient respond to a particular drug dose or not&rdquo;, or 
</p>
<p>&ldquo;does a drug cause the same effects at the same time in the same subject or not&rdquo;. 
</p>
<p>Such questions are typically of a fuzzy nature, and might, therefore, benefi t from an 
</p>
<p>analysis based on fuzzy logic.  
</p>
<p>    Note 
</p>
<p> More background theoretical and mathematical information of analyses using fuzzy 
</p>
<p>logic is given in Machine learning in medicine part one, Chap. 19, pp 241&ndash;253, 
</p>
<p>Springer Heidelberg Germany, 2012, from the same authors.    
</p>
<p> Note</p>
<p/>
</div>
<div class="page"><p/>
<p>383&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_61
</p>
<p>    Chapter 61  
</p>
<p> Automatic Data Mining for the Best Treatment 
of a Disease (90 Patients) 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 2&rdquo; as Chap. 
</p>
<p>15, 2014. 
</p>
<p>                      General Purpose 
</p>
<p> SPSS modeler is a work bench for automatic data mining (current chapter) and data 
</p>
<p>modeling (Chaps.   64     and   65    ). So far it is virtually unused in medicine, and mainly 
</p>
<p>applied by econo-/sociometrists. We will assess whether it can also be used for 
</p>
<p>multiple outcome analysis of clinical data.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Patients with sepsis have been given one of three treatments. Various outcome variables 
</p>
<p>are used to assess which one of the treatments performs best.  
</p>
<p>    Example 
</p>
<p> In data mining the question &ldquo;is a treatment a predictor of clinical improvement&rdquo; is 
</p>
<p>assessed by the question &ldquo;is the outcome, clinical improvement, a predictor of the 
</p>
<p>chance of having had a treatment&rdquo;. This approach may seem incorrect, but is also 
</p>
<p>used with discriminant analysis, and works fi ne, because it does not suffer from 
</p>
<p>strong correlations between outcome variables (Machine Learning in Medicine Part 
</p>
<p>One, Chap. 17, Discriminant analysis of supervised data, pp 215&ndash;224, Springer </p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_64">http://dx.doi.org/10.1007/978-3-319-15195-3_64</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_65">http://dx.doi.org/10.1007/978-3-319-15195-3_65</a></div>
</div>
<div class="page"><p/>
<p>384
</p>
<p>Heidelberg Germany, 2013). In this example, 90 patients with sepsis are treated 
</p>
<p>with three different treatments. Various outcome values are used as predictors of the 
</p>
<p>output treatment. 
</p>
<p> asat  alat  ureum  creat  crp  leucos  treat  low bp  death 
</p>
<p> 5,00  29,00  2,40  79,00  18,00  16,00  1,00  1  0 
</p>
<p> 10,00  30,00  2,10  94,00  15,00  15,00  1,00  1  0 
</p>
<p> 8,00  31,00  2,30  79,00  16,00  14,00  1,00  1  0 
</p>
<p> 6,00  16,00  2,70  80,00  17,00  19,00  1,00  1  0 
</p>
<p> 6,00  16,00  2,20  84,00  18,00  20,00  1,00  1  0 
</p>
<p> 5,00  13,00  2,10  78,00  17,00  21,00  1,00  1  0 
</p>
<p> 10,00  16,00  3,10  85,00  20,00  18,00  1,00  1  0 
</p>
<p> 8,00  28,00  8,00  68,00  15,00  18,00  1,00  1  0 
</p>
<p> 7,00  27,00  7,80  74,00  16,00  17,00  1,00  1  0 
</p>
<p> 6,00  26,00  8,40  69,00  18,00  16,00  1,00  1  0 
</p>
<p> 12,00  22,00  2,70  75,00  14,00  19,00  1,00  1  0 
</p>
<p> 21,00  21,00  3,00  70,00  15,00  20,00  1,00  1  0 
</p>
<p> 10,00  20,00  23,00  74,00  15,00  18,00  1,00  1  0 
</p>
<p> 19,00  19,00  2,10  75,00  16,00  16,00  1,00  1  0 
</p>
<p> 8,00  32,00  2,00  85,00  18,00  19,00  1,00  2  0 
</p>
<p> 20,00  11,00  2,90  63,00  18,00  18,00  1,00  1  0 
</p>
<p> 7,00  30,00  6,80  72,00  17,00  18,00  1,00  1  0 
</p>
<p> 1973,00  846,00  73,80  563,00  18,00  38,00  3,00  2  0 
</p>
<p> 1863,00  757,00  41,70  574,00  15,00  34,00  3,00  2  1 
</p>
<p> 1973,00  646,00  38,90  861,00  16,00  38,00  3,00  2  1 
</p>
<p>  asat = aspartate aminotransferase 
</p>
<p> alat = alanine aminotransferase 
</p>
<p> creat = creatinine 
</p>
<p> crp = c-reactive protein 
</p>
<p> treat = treatments 1&ndash;3 
</p>
<p> low bp = low blood pressure (1 no, 2 slight, 3 severe) 
</p>
<p> death = death (0 no, 1 yes) 
</p>
<p>    Only the fi rst 20 patients are above, the entire data fi le is in extra.springer.com 
</p>
<p>and is entitled &ldquo;spssmodeler.sav&rdquo;. SPSS modeler version 14.2 is used for the analy-
</p>
<p>sis. Start by opening SPSS modeler.  
</p>
<p>61 Automatic Data Mining for the Best Treatment of a Disease (90 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>385
</p>
<p>    Step 1 Open SPSS Modeler 
</p>
<p>      
</p>
<p>    In the palettes at the bottom of the screen full of nodes, look and fi nd the  Statistics 
</p>
<p>File node , and drag it to the canvas. Double-click on it&hellip;.Import fi le: browse and 
</p>
<p>enter the fi le &ldquo;spssmodeler.sav&rdquo;&hellip;.click OK&hellip;.in the palette fi nd  Distribution node  
</p>
<p>and drag to canvas&hellip;.right-click on the Statistics File node&hellip;.a Connect symbol 
</p>
<p>comes up&hellip;.click on the Distribution node&hellip;.an arrow is displayed&hellip;.double-click 
</p>
<p>on the Distribution Node&hellip;.after a second or two the underneath graph with infor-
</p>
<p>mation from the Distribution node is observed.  
</p>
<p>    Step 2 The Distribution Node 
</p>
<p>      
</p>
<p>    It gives the frequency distribution of the three treatments in the 90 patient data fi le. 
</p>
<p>All of the treatments are substantially present. 
</p>
<p> Next remove the Distribution node by clicking on it and press delete on the key 
</p>
<p>board of your computer. Continue by dragging the Data audit node to the canvas&hellip;.
</p>
<p>perform the connecting manoeuvres as above&hellip;.double-click it again.  
</p>
<p>Step 2 The Distribution Node</p>
<p/>
</div>
<div class="page"><p/>
<p>386
</p>
<p>    Step 3 The Data Audit Node 
</p>
<p>      
</p>
<p>    The Data audit will be edited. Select &ldquo;treatment&rdquo; as target fi eld (fi eld is variable 
</p>
<p>here)....click Run. The information from this node is now given in the form of a 
</p>
<p>Data audit plot, showing that due to the treatment low values are frequently more 
</p>
<p>often observed than the high values. Particularly, the treatments 1 and 2 (light blue 
</p>
<p>and red) are often associated with low values, these are probably the best treatments. 
</p>
<p>Next remove the Data audit node by clicking on it and press delete on the key board 
</p>
<p>of your computer. Continue by dragging the Plot node to the canvas&hellip;.perform the 
</p>
<p>connecting manoeuvres as above&hellip;.double-click it again.  
</p>
<p>61 Automatic Data Mining for the Best Treatment of a Disease (90 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>387
</p>
<p>    Step 4 The Plot Node 
</p>
<p>    
</p>
<p>1000
</p>
<p>800
</p>
<p>600
</p>
<p>400
</p>
<p>200
</p>
<p>0
</p>
<p>0 200 400
</p>
<p>alat
</p>
<p>c
r
e
a
ti
n
in
e
</p>
<p>600 800 1000
</p>
<p>  
</p>
<p>    The Plot node will be edited. On the Plot tab select creatinine as y-variable and alat 
</p>
<p>as x-variable, and treatment in the Overlay fi eld at Color&hellip;.click Run. The informa-
</p>
<p>tion from this node is now given in the form of a scatter plot of patients. This scatter 
</p>
<p>plot of alat versus creatinine values shows that the three treatments are somewhat 
</p>
<p>separately clustered. Treatment 1 (blue) in the left lower part, 2 (green) in the mid-
</p>
<p>dle, and 3 in the right upper part. Low values means adequate effect of treatment. So 
</p>
<p>treatment 1 (and also some patients with treatment 2) again perform pretty well. 
</p>
<p>Next remove the Plot node by clicking on it and press delete on the key board of 
</p>
<p>your computer. Continue by dragging the Web node to the canvas&hellip;.perform the 
</p>
<p>connecting manoeuvres as above&hellip;.double-click it again.  
</p>
<p>Step 4 The Plot Node</p>
<p/>
</div>
<div class="page"><p/>
<p>388
</p>
<p>    Step 5 The Web Node 
</p>
<p>    
</p>
<p>1.000000
</p>
<p>death lowbloodpressure treatment
</p>
<p>2.000000
</p>
<p>3.000000
</p>
<p>1
</p>
<p>3
</p>
<p>2
</p>
<p>1 0
</p>
<p>  
</p>
<p>    The Web node will be edited. In the Web note dialog box click Select All&hellip;.click 
</p>
<p>Run. The web graph that comes up, shows that treatment 1 (indicated here as 
</p>
<p>1.000000) is strongly associated with no death and no low blood pressure (thick 
</p>
<p>line), which is very good. However, the treatments 2 (2.000000) and 3 (3.000000) 
</p>
<p>are strongly associated with death and treatment 2 (2.000000) is also associated 
</p>
<p>with the severest form of low blood pressure. Next remove the Web node by click-
</p>
<p>ing on it and press delete on the key board of your computer. Continue by dragging 
</p>
<p>both the Type and C5.0 nodes to the canvas&hellip;.perform the connecting manoeuvres 
</p>
<p>respectively as indicated in the fi rst graph of this chapter&hellip;.double-click it again&hellip;a 
</p>
<p>gold nugget is placed as shown above&hellip;.click the gold nugget.  
</p>
<p>61 Automatic Data Mining for the Best Treatment of a Disease (90 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>389
</p>
<p>    Step 6 The Type and c5.0 Nodes 
</p>
<p>    
</p>
<p>Category
Node 0
</p>
<p>treatment
</p>
<p>1.000
2.000
</p>
<p>2.000
1.000 89.474
</p>
<p>3.000
</p>
<p>3.000
</p>
<p>&lt;= 32.000
</p>
<p>&lt;= 372.000 &gt; 372.000
</p>
<p>&gt; 32.000
</p>
<p>21.111 19
</p>
<p>42.222  38
</p>
<p>7.895
</p>
<p>1.000
2.000 96.774
</p>
<p>1.000 0.000
14.286
85.714
</p>
<p>23.33334.444 31 21
</p>
<p>0
3
</p>
<p>183.000 3.000
2.000
</p>
<p>3.226 1
30
</p>
<p>0.000 0
</p>
<p>1.000
2.000
3.000
</p>
<p>1.923
63.462
34.615
</p>
<p>57.778 52
</p>
<p>18
33
</p>
<p>1
3
</p>
<p>34
</p>
<p>12.632
</p>
<p>38.889 35
40.000 36
</p>
<p>100.000 90Total
</p>
<p>alat
</p>
<p>creatinine
</p>
<p>Total Total
</p>
<p>Total Total
</p>
<p>% n
</p>
<p>Category
Node 1
</p>
<p>% n Category
Node 2
</p>
<p>% n
</p>
<p>Category
Node 3
</p>
<p>% n Category
Node 4
</p>
<p>% n
</p>
<p>  
</p>
<p>    The output sheets give various interactive graphs and tables. One of them is the 
</p>
<p>above C5.0 decision tree. C5.0 decision trees are an improved version of the tradi-
</p>
<p>tional Quinlan decision trees with less, but more-relevant information. 
</p>
<p> The C5.0 classifi er underscores the previous fi ndings. The variable alat is the 
</p>
<p>best classifi er of the treatments with alat &lt;32 over 89 % of the patients having had 
</p>
<p>treatment 1, and with alat &gt; 32 over 63 % of the patients having had treatment 2. 
</p>
<p>Furthermore, in the high alat class patients with a creatinine over 372 around 86 % 
</p>
<p>has treatment 3. And so all in all, the treatment 1 would seem the best treatment and 
</p>
<p>treatment 3 the worst one.  
</p>
<p>Step 6 The Type and c5.0 Nodes</p>
<p/>
</div>
<div class="page"><p/>
<p>390
</p>
<p>    Step 7 The Output Node 
</p>
<p>      
</p>
<p>    In order to assess the accuracy of the C5.0 classifi er output an Output node is 
</p>
<p>attached to the gold nugget. Find Output node and drag it to the canvas&hellip;.perform 
</p>
<p>connecting manoeuvres with the gold nugget&hellip;.double-click the Output node 
</p>
<p>again&hellip;.click Run. The output sheet shows an accuracy (true positives and true 
</p>
<p>negatives) of 91,11 %, which is pretty good.  
</p>
<p>    Conclusion 
</p>
<p> SPSS modeler can be adequately used for multiple outcomes analysis of clinical 
</p>
<p>data. Finding the most appropriate treatment for a disease might be one of the goals 
</p>
<p>of this kind of research.  
</p>
<p>    Note 
</p>
<p> SPSS modeler is a software program entirely distinct from SPSS statistical soft-
</p>
<p>ware, though it uses most if not all of the calculus methods of it. It is a standard 
</p>
<p>software package particularly used by market analysts, but as shown can, perfectly, 
</p>
<p>well be applied for exploratory purposes in medical research. SPSS modeler is also 
</p>
<p>applied in the Chaps.   64     and   65    .   
</p>
<p>61 Automatic Data Mining for the Best Treatment of a Disease (90 Patients)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_64">http://dx.doi.org/10.1007/978-3-319-15195-3_64</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_65">http://dx.doi.org/10.1007/978-3-319-15195-3_65</a></div>
</div>
<div class="page"><p/>
<p>391&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_62
</p>
<p>    Chapter 62   
</p>
<p> Pareto Charts for Identifying the Main 
Factors of Multifactorial Outcomes 
(2,000 Admissions to Hospital) 
</p>
<p>                      General Purpose 
</p>
<p> In 1906 the Italian economist Pareto observed that 20 % of the Italian population 
</p>
<p>possessed 80 % of the land, and, looking at other countries, virtually the same 
</p>
<p>seemed to be true. The Pareto principle is currently used to identify the main factors 
</p>
<p>of multifactorial outcomes. Pareto charts is available in SPSS, and this chapter is to 
</p>
<p>assess whether it is useful, not only in marketing science, but also in medicine.  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> To assess whether pareto charts can be applied to identify in a study of hospital 
</p>
<p>admissions the main causes of iatrogenic admissions.  
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 2&rdquo; as Chap. 
</p>
<p>16, 2014. </p>
<p/>
</div>
<div class="page"><p/>
<p>392
</p>
<p>    Example 
</p>
<p> Two thousand subsequent admissions to a general hospital in the Netherlands were 
</p>
<p>classifi ed. 
</p>
<p> Indications for admission  Numbers  %  confi dence intervals (95 %) 
</p>
<p> 1. Cardiac condition and hypertension  810  40.5  38.0&ndash;42.1 
</p>
<p> 2. Gastrointestinal condition  254  12.7  11.9&ndash;14.2 
</p>
<p> 3. Infectious disease  200  10.0  9.2&ndash;12.0 
</p>
<p> 4. Pulmonary disease  137  6.9  6.5&ndash;7.7 
</p>
<p> 5. Hematological condition  109  5.5  4.0&ndash;6.2 
</p>
<p> 6. Malignancy  74  3.7  2.7&ndash;4.9 
</p>
<p> 7. Mental disease  54  2.7  1.9&ndash;3.8 
</p>
<p> 8. Endocrine condition  49  2.5  1.7&ndash;3.5 
</p>
<p> 9. Bleedings with acetyl salicyl/NSAIDS  47  2.4  1.6&ndash;3.4 
</p>
<p> 10. Other  41  2.1  1.4&ndash;3.1 
</p>
<p> 11. Unintentional overdose  31  1.6  1.0&ndash;2.5 
</p>
<p> 12. Bleeding with acenocoumarol/dalteparin  28  1.4  0.8&ndash;2.2 
</p>
<p> 13. Fever after chemotherapy  26  1.3  0.7&ndash;2.1 
</p>
<p> 14. Electrolyte disturbance  26  1.3  0.7&ndash;2.1 
</p>
<p> 15. Dehydration  23  1.2  0.7&ndash;2.0 
</p>
<p> 16. Other problems after chemotherapy  20  1.0  0.5&ndash;1.8 
</p>
<p> 17. Allergic reaction  17  0.9  0.4&ndash;1.7 
</p>
<p> 18. Renal disease  16  0.8  0.3&ndash;1.5 
</p>
<p> 19. Pain syndrome  8  0.4  0.1&ndash;1.0 
</p>
<p> 20. Hypotension  8  0.4  0.1&ndash;1.0 
</p>
<p> 21. Neurological disease  7  0.4  0.1&ndash;1.0 
</p>
<p> 22. Vascular disease  6  0.3  0.06&ndash;0.7 
</p>
<p> 23. Rheumatoid arthritis/arthrosis/osteoporosis  6  0.3  0.06&ndash;0.7 
</p>
<p> 24. Dermatological condition  3  0.2  0.02&ndash;0.7 
</p>
<p> 2,000  100 
</p>
<p>   NSAIDS  non-steroidal anti-infl ammatory drugs 
</p>
<p>    The data fi le is in extras.springer.com and is entitled &ldquo;paretocharts.sav&rdquo;. Open it.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Quality Control&hellip;.Pareto Charts&hellip;.click Simple&hellip;.mark Value of indi-
</p>
<p>vidual cases&hellip;.click Defi ne&hellip;.Values: enter "alladmissions"&hellip;.mark Variable: enter 
</p>
<p>"diagnosisgroups"&hellip;.click OK.    
</p>
<p> The underneath graph shows that over 50 % of the admissions is in the fi rst two 
</p>
<p>diagnosis groups. A general rule as postulated by Pareto says: when analyzing 
</p>
<p>observational studies with multifactorial effects, usually less than 20 % of the 
</p>
<p> factors determines over 80 % of the effect. This postulate seems to be true in this 
</p>
<p>example. The graph shows that the fi rst fi ve diagnosis groups out of 24 % determine 
</p>
<p>around 80 % of the effect (admission). When launching a program to reduce  hospital 
</p>
<p>62 Pareto Charts for Identifying the Main Factors of Multifactorial Outcomes&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>393
</p>
<p>admissions in general, it would make sense to prioritize these fi ve diagnosis groups, 
</p>
<p>and to neglect the other diagnosis groups. 
</p>
<p>    
</p>
<p>    In order to fi nd out how diagnosis groups contributed to the numbers of iatro-
</p>
<p>genic admissions, a pareto chart was constructed. The data are underneath, and are 
</p>
<p>the variables 4 and 5 in &ldquo;paretocharts.sav&rdquo;. 
</p>
<p> Numbers  %  95 % CIs 
</p>
<p> 1. Cardiac condition and hypertension  202  35.1  31.1&ndash;38.9 
</p>
<p> 2. Gastrointestinal condition  89  15.5  12.2&ndash;18.1 
</p>
<p> 3. Bleedings with acetyl salicyl/NSAIDS  46  8.0  5.9&ndash;10.4 
</p>
<p> 4. Infectious disease  31  5.4  3.6&ndash;7.4 
</p>
<p> 5. Bleeding with acenocoumarol/dalteparin  28  4.9  3.1&ndash;6.8 
</p>
<p> 6. Fever after chemotherapy  26  4.5  2.9&ndash;6.4 
</p>
<p> 7. Hematological condition  24  4.2  2.7&ndash;6.1 
</p>
<p> 8. Other problems after chemotherapy  20  3.5  2.1&ndash;5.3 
</p>
<p> 9. Endocrine condition  19  3.3  2.0&ndash;5.1 
</p>
<p> 10. Dehydration  18  3.1  1.9&ndash;4.9 
</p>
<p> 11. Electrolyte disturbance  14  2.4  1.3&ndash;3.8 
</p>
<p> 12. Pulmonary disease  9  1.6  0.8&ndash;3.0 
</p>
<p> 13. Allergic reaction  8  1.4  0.6&ndash;2.8 
</p>
<p>(continued)
</p>
<p> Example</p>
<p/>
</div>
<div class="page"><p/>
<p>394
</p>
<p> Numbers  %  95 % CIs 
</p>
<p> 14. Hypotension not due to antihypertensives  8  1.4  0.6&ndash;2.8 
</p>
<p> 15. Other  7  1.2  0.5&ndash;2.4 
</p>
<p> 16. Unintentional overdose  6  1.0  0.4&ndash;2.1 
</p>
<p> 17. Malignancy  6  1.0  0.4&ndash;2.1 
</p>
<p> 18. Neurological disease  4  0.7  0.2&ndash;1.7 
</p>
<p> 19. Mental disease  4  0.7  0.2&ndash;1.7 
</p>
<p> 20. Renal disease  2  0.3  0.04&ndash;1.2 
</p>
<p> 21. Vascular disease  2  0.3  0.04&ndash;1.2 
</p>
<p> 22. Dermatological condition  2  0.3  0.04&ndash;1.2 
</p>
<p> 23. Rheumatoid arthritis/arthrosis/osteoporosis  1  0.2  0.0&ndash;0.9 
</p>
<p> Total  576  100 
</p>
<p>   NSAIDS  non-steroidal anti-infl ammatory drugs,  ns  not signifi cant 
</p>
<p>    Command: 
</p>
<p>  Analyze&hellip;.Quality Control&hellip;.Pareto Charts&hellip;.click Simple&hellip;.mark Value of indi-
</p>
<p>vidual cases&hellip;.click Defi ne&hellip;.Values: enter "iatrogenicadmissions"&hellip;.mark 
</p>
<p>Variable: enter "diagnosisgroups"&hellip;.click OK.   
</p>
<p>     
</p>
<p>62 Pareto Charts for Identifying the Main Factors of Multifactorial Outcomes&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>395
</p>
<p>    The above pareto chart has a breakpoint at 50 %. Generally, a breakpoint is 
</p>
<p>observed at around 50 % of the effect with around 10 % of the factors before the 
</p>
<p>breakpoint. The breakpoint would be helpful for setting priorities, when addressing 
</p>
<p>the problem of iatrogenic admissions. The diagnosis groups, cardiac condition and 
</p>
<p>gastrointestinal condition, cause over 50 % of all of the iatrogenic admissions. 
</p>
<p> In order to fi nd which medicines were responsible for the iatrogenic admissions, 
</p>
<p>again a pareto chart was constructed. The variables 1 and 2 of the data fi le 
</p>
<p> &ldquo;paretocharts.sav&rdquo; will be used.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Quality Control&hellip;.Pareto Charts&hellip;.click Simple&hellip;.mark Value of indi-
</p>
<p>vidual cases&hellip;.click Defi ne&hellip;.Values: enter "iatrogenicad"&hellip;.mark Variable: enter 
</p>
<p>"medicinecat"&hellip;.click OK.    
</p>
<p>    
</p>
<p>    No breakpoint is observed, but the fi rst two medicine categories were responsible 
</p>
<p>for 50 % of the entire number of iatrogenic admissions. We can conclude, that over 
</p>
<p>50 % of the iatrogenic admissions were in two diagnosis groups, and over 50 % of 
</p>
<p>the medicines responsible were also in two main medicine categories.  
</p>
<p> Example</p>
<p/>
</div>
<div class="page"><p/>
<p>396
</p>
<p>    Conclusion 
</p>
<p> Pareto charts are useful for identifying the main factors of multifactorial outcomes, 
</p>
<p>not only in marketing science but also in medicine.  
</p>
<p>    Note 
</p>
<p> In addition to fl ow charts, scattergrams, histograms, control charts, cause effects 
</p>
<p>diagrams, and checklists, pareto charts are basic graphical tools of data analysis. All 
</p>
<p>of them require little training in statistics.    
</p>
<p>62 Pareto Charts for Identifying the Main Factors of Multifactorial Outcomes&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>397&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_63
</p>
<p>    Chapter 63   
</p>
<p> Radial Basis Neural Networks 
</p>
<p>for Multidimensional Gaussian Data 
</p>
<p>(90 Persons) 
</p>
<p>                      General Purpose 
</p>
<p> Radial basis functions may better than multilayer neural network (Chap.   50    ), predict 
</p>
<p>medical data, because it uses a Gaussian activation function, but it is rarely used. 
</p>
<p>This chapter is to assess its performance in clinical research.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Body surface area is an indicator for metabolic body mass, and is used for adjusting 
</p>
<p>oxygen, CO 2  transport parameters, blood volumes, urine creatinine clearance, protein/
</p>
<p>creatinine ratios and other parameters. Can a radial basis neural network be applied 
</p>
<p>to accurately predict the body surface from gender, age, weight and height?  
</p>
<p>    Example 
</p>
<p> The body surfaces of 90 persons were calculated using direct photometric 
</p>
<p>measurements. These previously measured outcome data will be used as the 
</p>
<p>socalled learning sample, and the computer will be commanded to teach itself 
</p>
<p>making predictions about the body surface from the predictor variables gender, 
</p>
<p>age, weight and height. The fi rst 20 patients are underneath. The entire data fi le is 
</p>
<p>in &ldquo;radialbasisnn&rdquo;. 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 2&rdquo; as Chap. 
</p>
<p>17, 2013. </p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_50">http://dx.doi.org/10.1007/978-3-319-15195-3_50</a></div>
</div>
<div class="page"><p/>
<p>398
</p>
<p> 1,00  13,00  30,50  138,50  10072,90 
</p>
<p> 0,00  5,00  15,00  101,00  6189,00 
</p>
<p> 0,00  0,00  2,50  51,50  1906,20 
</p>
<p> 1,00  11,00  30,00  141,00  10290,60 
</p>
<p> 1,00  15,00  40,50  154,00  13221,60 
</p>
<p> 0,00  11,00  27,00  136,00  9654,50 
</p>
<p> 0,00  5,00  15,00  106,00  6768,20 
</p>
<p> 1,00  5,00  15,00  103,00  6194,10 
</p>
<p> 1,00  3,00  13,50  96,00  5830,20 
</p>
<p> 0,00  13,00  36,00  150,00  11759,00 
</p>
<p> 0,00  3,00  12,00  92,00  5299,40 
</p>
<p> 1,00  0,00  2,50  51,00  2094,50 
</p>
<p> 0,00  7,00  19,00  121,00  7490,80 
</p>
<p> 1,00  13,00  28,00  130,50  9521,70 
</p>
<p> 1,00  0,00  3,00  54,00  2446,20 
</p>
<p> 0,00  0,00  3,00  51,00  1632,50 
</p>
<p> 0,00  7,00  21,00  123,00  7958,80 
</p>
<p> 1,00  11,00  31,00  139,00  10580,80 
</p>
<p> 1,00  7,00  24,50  122,50  8756,10 
</p>
<p> 1,00  11,00  26,00  133,00  9573,00 
</p>
<p>  Var 1 gender 
</p>
<p> Var 2 age 
</p>
<p> Var 3 weight (kg) 
</p>
<p> Var 4 height (m) 
</p>
<p> Var 5 body surface measured (cm 2 ) 
</p>
<p>        The Computer Teaches Itself to Make Predictions 
</p>
<p> The SPSS module Neural Networks is used for training and outcome prediction. It 
</p>
<p>uses XML (exTended Markup Language) fi les to store the neural network. Start by 
</p>
<p>opening the data fi le.
</p>
<p>  Command: 
</p>
<p>  click Transform&hellip;.click Random Number Generators&hellip;.click Set Starting Point&hellip;.
</p>
<p>click Fixed Value (2000000)&hellip;.click OK&hellip;.click Analyze&hellip;. Neural Networks&hellip;.
</p>
<p>Radial Basis Function&hellip;.Dependent Variables: enter Body surface measured&hellip;.
</p>
<p>Factors: enter gender, age, weight, and height&hellip;.Partitions: Training 7&hellip;.Test 3&hellip;.
</p>
<p>Holdout 0&hellip;.click Output: mark Description&hellip;.Diagram&hellip;. Model summary&hellip;.
</p>
<p>Predicted by observed chart&hellip;.Case processing summary &hellip;.click Save: mark Save 
</p>
<p>predicted value of category for each dependent variable&hellip;.automatically generate 
</p>
<p>unique names&hellip;.click Export&hellip;.mark Export synaptic weights estimates to XML 
</p>
<p>fi le&hellip;.click Browse&hellip;.File Name: enter "exportradialbasisnn" and save in the appro-
</p>
<p>priate folder of your computer&hellip;.click OK.    
</p>
<p>63 Radial Basis Neural Networks for Multidimensional Gaussian Data (90 Persons)</p>
<p/>
</div>
<div class="page"><p/>
<p>399
</p>
<p> The output warns that in the testing sample some cases have been excluded from 
</p>
<p>analysis, because of values not occurring in the training sample. Minimizing the 
</p>
<p>output sheets shows the data fi le with predicted values. They are pretty much the 
</p>
<p>same as the measured body surface values. We will use linear regression to estimate 
</p>
<p>the association between the two.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Regression&hellip;.Linear&hellip;.Dependent: bodysurface &hellip;.Independent: RBF_
</p>
<p>PredictedValue&hellip;.OK.    
</p>
<p> The output sheets show that the r-value is 0.931, p &lt; 0.0001. The saved XML fi le 
</p>
<p>will now be used to compute the body surface in six individual patients. 
</p>
<p> gender  age  weight  height 
</p>
<p> 1,00  9,00  29,00  138,00 
</p>
<p> 1,00  1,00  8,00  76,00 
</p>
<p> ,00  15,00  42,00  165,00 
</p>
<p> 1,00  15,00  40,00  151,00 
</p>
<p> 1,00  1,00  9,00  80,00 
</p>
<p> 1,00  7,00  22,00  123,00 
</p>
<p>  gender 
</p>
<p> age (years) 
</p>
<p> weight (kg) 
</p>
<p> height (m) 
</p>
<p>    Enter the above data in a new SPSS data fi le.
</p>
<p>  Command: 
</p>
<p>  Utilities&hellip;.click Scoring Wizard&hellip;.click Browse&hellip;.click Select&hellip;.Folder: enter the 
</p>
<p>exportradialbasisnn.xml fi le&hellip;.click Select&hellip;.in Scoring Wizard click Next&hellip;.click 
</p>
<p>Use value substitution&hellip;.click Next&hellip;.click Finish.    
</p>
<p> The underneath data fi le now gives the body surfaces computed by the neural 
</p>
<p>network with the help of the XML fi le. 
</p>
<p> gender  age  weight  height  predicted body surface 
</p>
<p> 1,00  9,00  29,00  138,00  9219,71 
</p>
<p> 1,00  1,00  8,00  76,00  5307,81 
</p>
<p> ,00  15,00  42,00  165,00  13520,13 
</p>
<p> 1,00  15,00  40,00  151,00  13300,79 
</p>
<p> 1,00  1,00  9,00  80,00  5170,13 
</p>
<p> 1,00  7,00  22,00  123,00  8460,05 
</p>
<p>  gender 
</p>
<p> age (years) 
</p>
<p> weight (kg) 
</p>
<p> height (m) 
</p>
<p> predicted body surface (cm 2 ) 
</p>
<p>The Computer Teaches Itself to Make Predictions</p>
<p/>
</div>
<div class="page"><p/>
<p>400
</p>
<p>        Conclusion 
</p>
<p> Radial basis neural networks can be readily trained to provide accurate body surface 
</p>
<p>values of individual patients.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of neural networks is 
</p>
<p>available in Machine learning in medicine part one, Chaps. 12 and 13, entitled 
</p>
<p>&ldquo;Artifi cial intelligence, multilayer perceptron&rdquo; and &ldquo;Artifi cial intelligence, radial 
</p>
<p>basis functions&rdquo;, pp 145&ndash;156 and 157&ndash;166, Springer Heidelberg Germany 2013, 
</p>
<p>and in the Chap.   50     of the current book.    
</p>
<p>63 Radial Basis Neural Networks for Multidimensional Gaussian Data (90 Persons)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_50">http://dx.doi.org/10.1007/978-3-319-15195-3_50</a></div>
</div>
<div class="page"><p/>
<p>401&copy; Springer International Publishing Switzerland 2015 
T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
Overview, DOI 10.1007/978-3-319-15195-3_64
</p>
<p>    Chapter 64   
</p>
<p> Automatic Modeling of Drug Effi cacy 
Prediction (250 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> SPSS modeler is a work bench for automatic data mining (Chap.   61    ) and modeling 
</p>
<p>(see also the Chap.   65    ). So far it is virtually unused in medicine, and mainly applied 
</p>
<p>by econo-/sociometrics. Automatic modeling of continuous outcomes computes the 
</p>
<p>ensembled result of a number of best fi t models for a particular data set, and pro-
</p>
<p>vides better sensitivity than the separate models do. This chapter is to demonstrate 
</p>
<p>its performance with drug effi cacy prediction.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> The expression of a cluster of genes can be used as a functional unit to predict the 
</p>
<p>effi cacy of cytostatic treatment. Can ensembled modeling with three best fi t statisti-
</p>
<p>cal models provide better precision than the separate analysis with single statistical 
</p>
<p>models does.  
</p>
<p>    Example 
</p>
<p> A 250 patients&rsquo; data fi le includes 28 variables consistent of patients&rsquo; gene expres-
</p>
<p>sion levels and their drug effi cacy scores. Only the fi rst 12 patients are shown under-
</p>
<p>neath. The entire data fi le is in extras.springer.com, and is entitled 
</p>
<p>&ldquo;ensembledmodelcontinuous&rdquo;. All of the variables were standardized by scoring 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 2&rdquo; as Chap. 
18, 2014. </p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_61">http://dx.doi.org/10.1007/978-3-319-15195-3_61</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_65">http://dx.doi.org/10.1007/978-3-319-15195-3_65</a></div>
</div>
<div class="page"><p/>
<p>402
</p>
<p>them on 11 points linear scales. The following genes were highly expressed: the 
</p>
<p>genes 1&ndash;4, 16&ndash;19, and 24&ndash;27. 
</p>
<p> G1  G2  G3  G4  G16  G17  G18  G19  G24  G25  G26  G27  O 
</p>
<p> 8,00  8,00  9,00  5,00  7,00  10,00  5,00  6,00  9,00  9,00  6,00  6,00  7,00 
</p>
<p> 9,00  9,00  10,00  9,00  8,00  8,00  7,00  8,00  8,00  9,00  8,00  8,00  7,00 
</p>
<p> 9,00  8,00  8,00  8,00  8,00  9,00  7,00  8,00  9,00  8,00  9,00  9,00  8,00 
</p>
<p> 8,00  9,00  8,00  9,00  6,00  7,00  6,00  4,00  6,00  6,00  5,00  5,00  7,00 
</p>
<p> 10,00  10,00  8,00  10,00  9,00  10,00  10,00  8,00  8,00  9,00  9,00  9,00  8,00 
</p>
<p> 7,00  8,00  8,00  8,00  8,00  7,00  6,00  5,00  7,00  8,00  8,00  7,00  6,00 
</p>
<p> 5,00  5,00  5,00  5,00  5,00  6,00  4,00  5,00  5,00  6,00  6,00  5,00  5,00 
</p>
<p> 9,00  9,00  9,00  9,00  8,00  8,00  8,00  8,00  9,00  8,00  3,00  8,00  8,00 
</p>
<p> 9,00  8,00  9,00  8,00  9,00  8,00  7,00  7,00  7,00  7,00  5,00  8,00  7,00 
</p>
<p> 10,00  10,00  10,00  10,00  10,00  10,00  10,00  10,00  10,00  8,00  8,00  10,00  10,00 
</p>
<p> 2,00  2,00  8,00  5,00  7,00  8,00  8,00  8,00  9,00  3,00  9,00  8,00  7,00 
</p>
<p> 7,00  8,00  8,00  7,00  8,00  6,00  6,00  7,00  8,00  8,00  8,00  7,00  7,00 
</p>
<p>   G  gene (gene expression levels),  O  outcome (score) 
</p>
<p>        Step 1 Open SPSS Modeler (14.2) 
</p>
<p>      
</p>
<p>64 Automatic Modeling of Drug Effi cacy Prediction (250 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>403
</p>
<p>        Step 2 The Statistics File Node 
</p>
<p> The canvas is, initially, blank, and above a screen view is of the fi nal &ldquo;completed 
</p>
<p>ensemble&rdquo; model, otherwise called stream of nodes, which we are going to build. 
</p>
<p>First, in the palettes at the bottom of the screen full of nodes, look and fi nd the 
</p>
<p> Statistics File node , and drag it to the canvas. Double-click on it&hellip;.Import fi le: 
</p>
<p>browse and enter the fi le &ldquo;ensembledmodelcontinuous&rdquo; &hellip;.click OK. The graph 
</p>
<p>below shows that the data fi le is open for analysis. 
</p>
<p>    
</p>
<p>        Step 3 The Type Node 
</p>
<p> In the palette at the bottom of screen fi nd Type node and drag to the canvas&hellip;.right- 
</p>
<p>click on the Statistics File node&hellip;.a Connect symbol comes up&hellip;.click on the Type 
</p>
<p>node&hellip;.an arrow is displayed&hellip;.double-click on the Type Node&hellip;.after a second or 
</p>
<p>two the underneath graph with information from the Type node is observed. Type 
</p>
<p>Step 3 The Type Node</p>
<p/>
</div>
<div class="page"><p/>
<p>404
</p>
<p>nodes are used to access the properties of the variables (often called fi elds here) like 
</p>
<p>type, role, unit etc. in the data fi le. As shown below, the variables are appropriately 
</p>
<p>set: 14 predictor variables, 1 outcome (= target) variable, all of them continuous. 
</p>
<p>    
</p>
<p>        Step 4 The Auto Numeric Node 
</p>
<p> Now, click the Auto Numeric node and drag to canvas and connect with the Type 
</p>
<p>node using the above connect-procedure. Click the Auto Numeric node, and the 
</p>
<p>underneath graph comes up&hellip;.now click Model&hellip;.select Correlation as metric to 
</p>
<p>rank quality of the various analysis methods used&hellip;. the additional manoeuvres are 
</p>
<p>as indicated below&hellip;.in Numbers of models to use: type the number 3. 
</p>
<p>64 Automatic Modeling of Drug Effi cacy Prediction (250 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>405
</p>
<p>    
</p>
<p>        Step 5 The Expert Node 
</p>
<p> Then click the Expert tab. It is shown below. Out of seven statistical models the 
</p>
<p>three best fi t ones are used by SPSS modeler for the ensembled model. 
</p>
<p>Step 5 The Expert Node</p>
<p/>
</div>
<div class="page"><p/>
<p>406
</p>
<p>    
</p>
<p>    The seven statistical models include:
</p>
<p>    1.    Linear regression (Regression)   
</p>
<p>   2.    Generalized linear model (Generalized&hellip;.)   
</p>
<p>   3.    K nearest neighbor clustering (KNN Algorithm)   
</p>
<p>   4.    Support vector machine (SVM)   
</p>
<p>   5.    Classifi cation and regression tree (C&amp;R Tree)   
</p>
<p>   6.    Chi square automatic interaction detection (CHAID Tree)   
</p>
<p>   7.    Neural network (Neural Net)     
</p>
<p> More background information of the above methods are available at
</p>
<p>    1.    SPSS for Starters Part One, Chap. 5, Linear regression, pp 15&ndash;18, Springer 
</p>
<p>Heidelberg Germany 2010   
</p>
<p>   2.    The Chaps.   20     and   21     of current book.   
</p>
<p>   3.    Chapter   1     of current work.   
</p>
<p>   4.    Machine Learning in Medicine Part Two, Chap. 15, Support vector machines, 
</p>
<p>pp 155&ndash;161, Springer Heidelberg Germany, 2013.   
</p>
<p>   5.    Chapter   53     of current book.   
</p>
<p>   6.    Machine Learning in Medicine Part Three, Chap. 14, Decision trees, pp 137&ndash;
</p>
<p>150, Springer Heidelberg Germany 2013.   
</p>
<p>64 Automatic Modeling of Drug Effi cacy Prediction (250 Patients)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_20">http://dx.doi.org/10.1007/978-3-319-15195-3_20</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_21">http://dx.doi.org/10.1007/978-3-319-15195-3_21</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_1">http://dx.doi.org/10.1007/978-3-319-15195-3_1</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_53">http://dx.doi.org/10.1007/978-3-319-15195-3_53</a></div>
</div>
<div class="page"><p/>
<p>407
</p>
<p>   7.    Machine Learning in Medicine Part One, Chap. 12, Artifi cial intelligence, 
</p>
<p> multilayer perceptron modeling, pp 145&ndash;154, Springer Heidelberg Germany 
</p>
<p>2013.     
</p>
<p> All of the seven above references are from the same authors as the current work.  
</p>
<p>    Step 6 The Settings Tab 
</p>
<p> In the above graph click the Settings tab&hellip;.click the Run button&hellip;.now a gold nug-
</p>
<p>get is placed on the canvas&hellip;.click the gold nugget&hellip;.the model created is shown 
</p>
<p>below. 
</p>
<p>    
</p>
<p>    The correlation coeffi cients of the three best models are close to 0.8, and, thus, 
</p>
<p>pretty good. We will now perform the ensembled procedure.  
</p>
<p>    Step 7 The Analysis Node 
</p>
<p> Find in the palettes below the screen the Analysis node and drag it to the canvas. 
</p>
<p>With the above connect procedure connect it with the gold nugget&hellip;.click the 
</p>
<p>Analysis node. 
</p>
<p>    
</p>
<p>Step 7 The Analysis Node</p>
<p/>
</div>
<div class="page"><p/>
<p>408
</p>
<p>    The above table is shown and gives the statistics of the ensembled model created. 
</p>
<p>The ensembled outcome is the average score of the scores from the three best fi t 
</p>
<p>statistical models. Adjustment for multiple testing and for variance stabilization 
</p>
<p>with Fisher transformation is automatically carried out. The ensembled outcome 
</p>
<p>(named the $XR-outcome) is compared with the outcomes of the three best fi t sta-
</p>
<p>tistical models, namely, CHAID (chi square automatic interaction detector), SVM 
</p>
<p>(support vector machine), and Regression (linear regression). The ensembled cor-
</p>
<p>relation coeffi cient is larger (0.859) than the correlation coeffi cients from the three 
</p>
<p>best fi t models (0.854, 0.836, 0.821), and so ensembled procedures make sense, 
</p>
<p>because they can provide increased precision in the analysis. The ensembled model 
</p>
<p>can now be stored as an SPSS Modeler Stream fi le for future use in the appropriate 
</p>
<p>folder of your computer. For the readers&rsquo; convenience it is in extras.springer.com, 
</p>
<p>and it is entitled &ldquo;ensembledmodelcontinuous&rdquo;.  
</p>
<p>    Conclusion 
</p>
<p> In the example given in this chapter, the ensembled correlation coeffi cient is larger 
</p>
<p>(0.859) than the correlation coeffi cients from the three best fi t models (0.854, 0.836, 
</p>
<p>0.821), and, so, ensembled procedures do make sense, because they can provide 
</p>
<p>increased precision in the analysis.  
</p>
<p>    Note 
</p>
<p> SPSS modeler is a software program entirely distinct from SPSS statistical soft-
</p>
<p>ware, though it uses most if not all of the calculus methods of it. It is a standard 
</p>
<p>software package particularly used by market analysts, but, as shown, can, perfectly, 
</p>
<p>well be applied for exploratory purposes in medical research. It is also applied in the 
</p>
<p>Chaps.   61     and   65    .    
</p>
<p>64 Automatic Modeling of Drug Effi cacy Prediction (250 Patients)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_61">http://dx.doi.org/10.1007/978-3-319-15195-3_61</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_65">http://dx.doi.org/10.1007/978-3-319-15195-3_65</a></div>
</div>
<div class="page"><p/>
<p>409&copy; Springer International Publishing Switzerland 2015 
T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
Overview, DOI 10.1007/978-3-319-15195-3_65
</p>
<p>    Chapter 65   
</p>
<p> Automatic Modeling for Clinical Event 
Prediction (200 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> SPSS modeler is a work bench for automatic data mining (Chap.   61    ) and modeling 
</p>
<p>(see also the Chap.   64    ). So far it is virtually unused in medicine, and mainly applied 
</p>
<p>by econo-/sociometrists. Automatic modeling of binary outcomes computes the 
</p>
<p>ensembled result of a number of best fi t models for a particular data set, and pro-
</p>
<p>vides better sensitivity than the separate models do. This chapter is to demonstrate 
</p>
<p>its performance with clinical event prediction.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Multiple laboratory values can predict events like health, death, morbidities etc. Can 
</p>
<p>ensembled modeling with four best fi t statistical models provide better precision 
</p>
<p>than the separate analysis with single statistical models does.  
</p>
<p>    Example 
</p>
<p> A 200 patients&rsquo; data fi le includes 11 variables consistent of patients&rsquo; laboratory 
</p>
<p>values and their subsequent outcome (death or alive). Only the fi rst 12 patients are 
</p>
<p>shown underneath. The entire data fi le is in extras.springer.com, and is entitled 
</p>
<p>&ldquo;ensembledmodelbinary&rdquo;. 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 2&rdquo; as Chap. 
19, 2014. </p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_61">http://dx.doi.org/10.1007/978-3-319-15195-3_61</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_64">http://dx.doi.org/10.1007/978-3-319-15195-3_64</a></div>
</div>
<div class="page"><p/>
<p>410
</p>
<p> Death  ggt  asat  alat  bili  ureum  creat  c-clear  esr  crp  leucos 
</p>
<p> ,00  20,00  23,00  34,00  2,00  3,40  89,00  &minus;111,00  2,00  2,00  5,00 
</p>
<p> ,00  14,00  21,00  33,00  3,00  2,00  67,00  &minus;112,00  7,00  3,00  6,00 
</p>
<p> ,00  30,00  35,00  32,00  4,00  5,60  58,00  &minus;116,00  8,00  4,00  4,00 
</p>
<p> ,00  35,00  34,00  40,00  4,00  6,00  76,00  &minus;110,00  6,00  5,00  7,00 
</p>
<p> ,00  23,00  33,00  22,00  4,00  6,10  95,00  &minus;120,00  9,00  6,00  6,00 
</p>
<p> ,00  26,00  31,00  24,00  3,00  5,40  78,00  &minus;132,00  8,00  4,00  8,00 
</p>
<p> ,00  15,00  29,00  26,00  2,00  5,30  47,00  &minus;120,00  12,00  5,00  5,00 
</p>
<p> ,00  13,00  26,00  24,00  1,00  6,30  65,00  &minus;132,00  13,00  6,00  6,00 
</p>
<p> ,00  26,00  27,00  27,00  4,00  6,00  97,00  &minus;112,00  14,00  6,00  7,00 
</p>
<p> ,00  34,00  25,00  13,00  3,00  4,00  67,00  &minus;125,00  15,00  7,00  6,00 
</p>
<p> ,00  32,00  26,00  24,00  3,00  3,60  58,00  &minus;110,00  13,00  8,00  6,00 
</p>
<p> ,00  21,00  13,00  15,00  3,00  3,60  69,00  &minus;102,00  12,00  2,00  4,00 
</p>
<p>  death = death yes no (0 = no) 
 ggt = gamma glutamyl transferase (u/l) 
 asat = aspartate aminotransferase (u/l) 
 alat = alanine aminotransferase (u/l) 
 bili = bilirubine (micromol/l) 
 ureum = ureum (mmol/l) 
 creat = creatinine (mmicromol/l) 
 c-clear = creatinine clearance (ml/min) 
 esr = erythrocyte sedimentation rate (mm) 
 crp = c-reactive protein (mg/l) 
 leucos = leucocyte count (.10 9 /l) 
</p>
<p>        Step 1 Open SPSS Modeler (14.2) 
</p>
<p>      
</p>
<p>65 Automatic Modeling for Clinical Event Prediction (200 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>411
</p>
<p>        Step 2 The Statistics File Node 
</p>
<p> The canvas is, initially, blank, and above is given a screen view of the completed 
</p>
<p>ensembled model, otherwise called stream of nodes, which we are going to build. 
</p>
<p>First, in the palettes at the bottom of the screen full of nodes, look and fi nd the 
</p>
<p> Statistics File node , and drag it to the canvas, pressing the mouse left side. Double- 
</p>
<p>click on this node&hellip;.Import fi le: browse and enter the fi le &ldquo;ensembledmodelbinary&rdquo; 
</p>
<p>&hellip;.click OK. The graph below shows, that the data fi le is open for analysis. 
</p>
<p>    
</p>
<p>        Step 3 The Type Node 
</p>
<p> In the palette at the bottom of screen fi nd Type node and drag to the canvas&hellip;.right- 
</p>
<p>click on the Statistics File node&hellip;.a Connect symbol comes up&hellip;.click on the Type 
</p>
<p>node&hellip;.an arrow is displayed&hellip;.double-click on the Type Node&hellip;.after a second or 
</p>
<p>two the underneath graph with information from the Type node is observed. Type 
</p>
<p>nodes are used to access the properties of the variables (often called fi elds here) 
</p>
<p>like type, role, unit etc. in the data fi le. As shown below, 10 predictor variables 
</p>
<p>(all of them continuous) are appropriately set. However, VAR 00001 (death) is the 
</p>
<p>Step 3 The Type Node</p>
<p/>
</div>
<div class="page"><p/>
<p>412
</p>
<p>outcome (= target) variable, and is binary. Click in the row of variable VAR00001 
</p>
<p>on the measurement column and replace &ldquo;Continuous&rdquo; with &ldquo;Flag&rdquo;. Click Apply 
</p>
<p>and OK. The underneath fi gure is removed and the canvas is displayed again. 
</p>
<p>    
</p>
<p>        Step 4 The Auto Classifi er Node 
</p>
<p> Now, click the Auto Classifi er node and drag to the canvas, and connect with the 
</p>
<p>Type node using the above connect-procedure. Click the Auto Classifi er node, and 
</p>
<p>the underneath graph comes up&hellip;.now click Model&hellip;.select Lift as Rank model of 
</p>
<p>the various analysis models used&hellip;. the additional manoeuvres are as indicated 
</p>
<p>below&hellip;.in Numbers of models to use: type the number 4. 
</p>
<p>65 Automatic Modeling for Clinical Event Prediction (200 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>413
</p>
<p>    
</p>
<p>        Step 5 The Expert Tab 
</p>
<p> Then click the Expert tab. It is shown below. Out of 11 statistical models the four 
</p>
<p>best fi t ones are selected by SPSS modeler for constructing an ensembled model. 
</p>
<p>    
</p>
<p>Step 5 The Expert Tab</p>
<p/>
</div>
<div class="page"><p/>
<p>414
</p>
<p>    The 11 statistical analysis methods for a fl ag target (= binary outcome) include:
</p>
<p>    1.    C5.0 decision tree (C5.0)   
</p>
<p>   2.    Logistic regression (Logist r&hellip;)   
</p>
<p>   3.    Decision list (Decision&hellip;.)   
</p>
<p>   4.    Bayesian network (Bayesian&hellip;.)   
</p>
<p>   5.    Discriminant analysis (Discriminant)   
</p>
<p>   6.    K nearest neighbors algorithm (KNN Alg&hellip;)   
</p>
<p>   7.    Support vector machine (SVM)   
</p>
<p>   8.    Classifi cation and regression tree (C&amp;R Tree)   
</p>
<p>   9.    Quest decision tree (Quest Tr&hellip;.)   
</p>
<p>   10.    Chi square automatic interaction detection (CHAID Tree)   
</p>
<p>   11.    Neural network (Neural Net)     
</p>
<p> More background information of the above methods are available at.
</p>
<p>    1.    Chapter   15     of current work, Automatic data mining for the best treatment of a 
</p>
<p>Disease.   
</p>
<p>   2.    SPSS for Starters Part One, Chap. 11, Logistic regression, pp 39&ndash;42, Springer 
</p>
<p>Heidelberg Germany 2010.   
</p>
<p>   3.    Decision list models identify high and low performing segments in a data fi le,   
</p>
<p>   4.    Machine Learning in Medicine Part Two, Chap. 16, Bayesian networks, 
</p>
<p>pp 163&ndash;170, Springer Heidelberg Germany, 2013.   
</p>
<p>   5.    Machine Learning in Medicine Part One, Chap. 17, Discriminant analysis for 
</p>
<p>supervised data, pp 215&ndash;224, Springer Heidelberg Germany 2013.   
</p>
<p>   6.    Chapter   4     of current work, Nearest neighbors for classifying new medicines.   
</p>
<p>   7.    Machine Learning in Medicine Part Two, Chap. 15, Support vector machines, 
</p>
<p>pp 155&ndash;161, Springer Heidelberg Germany, 2013.   
</p>
<p>   8.    Chapter   53     of current work.   
</p>
<p>   9.    QUEST (Quick Unbiased Effi cient Statistical Trees) are improved decision 
</p>
<p>trees for binary outcomes.   
</p>
<p>   10.    Machine Learning in Medicine Part Three, Chap. 14, Decision trees, pp 137&ndash;
</p>
<p>150, Springer Heidelberg Germany 2013.   
</p>
<p>   11.    Machine Learning in Medicine Part One, Chap. 12, Artifi cial intelligence, mul-
</p>
<p>tilayer perceptron modeling, pp 145&ndash;154, Springer Heidelberg Germany 2013.     
</p>
<p> All of the above references are from the same authors as the current work.  
</p>
<p>    Step 6 The Settings Tab 
</p>
<p> In the above graph click the Settings tab&hellip;.click the Run button&hellip;.now a gold nugget 
</p>
<p>is placed on the canvas&hellip;.click the gold nugget&hellip;.the model created is shown below. 
</p>
<p>65 Automatic Modeling for Clinical Event Prediction (200 Patients)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_15">http://dx.doi.org/10.1007/978-3-319-15195-3_15</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_4">http://dx.doi.org/10.1007/978-3-319-15195-3_4</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_53">http://dx.doi.org/10.1007/978-3-319-15195-3_53</a></div>
</div>
<div class="page"><p/>
<p>415
</p>
<p>    
</p>
<p>    The overall accuracies (%) of the four best fi t models are close to 0.8, and are, 
</p>
<p>thus, pretty good. We will now perform the ensembled procedure.  
</p>
<p>    Step 7 The Analysis Node 
</p>
<p> Find in the palettes at the bottom of the screen the Analysis node and drag it to the 
</p>
<p>canvas. With above connect procedure connect it with the gold nugget&hellip;.click the 
</p>
<p>Analysis node. 
</p>
<p>    
</p>
<p>Step 7 The Analysis Node</p>
<p/>
</div>
<div class="page"><p/>
<p>416
</p>
<p>    The above table is shown and gives the statistics of the ensembled model created. 
</p>
<p>The ensembled outcome is the average accuracy of the accuracies from the four best 
</p>
<p>fi t statistical models. In order to prevent overstated certainty due to overfi tting, boot-
</p>
<p>strap aggregating (&ldquo;bagging&rdquo;) is used. The ensembled outcome (named the 
</p>
<p>$XR-outcome) is compared with the outcomes of the four best fi t statistical models, 
</p>
<p>namely, Bayesian network, k Nearest Neighbor clustering, Logistic regression, and 
</p>
<p>Neural network. The ensembled accuracy (97.97 %) is much larger than the accura-
</p>
<p>cies of the four best fi t models (76.423, 80,081, 76,829, and 78,862 %), and, so, 
</p>
<p>ensembled procedures make sense, because they provide increased precision in the 
</p>
<p>analysis. The computed ensembled model can now be stored in your computer in 
</p>
<p>the form of an SPSS Modeler Stream fi le for future use. For the readers&rsquo; conve-
</p>
<p>nience it is in extras.springer.com, and entitled &ldquo;ensembledmodelbinary&rdquo;.  
</p>
<p>    Conclusion 
</p>
<p> In the example given in this chapter, the ensembled accuracy is larger (97,97 %) 
</p>
<p>than the accuracies from the four best fi t models (76.423, 80,081, 76,829, and 
</p>
<p>78,862 %), and so ensembled procedures make sense, because they can provide 
</p>
<p>increased precision in the analysis.  
</p>
<p>    Note 
</p>
<p> SPSS modeler is a software program entirely distinct from SPSS statistical soft-
</p>
<p>ware, though it uses most if not all of the calculus methods of it. It is a standard 
</p>
<p>software package particularly used by market analysts, but, as shown, can perfectly 
</p>
<p>well be applied for exploratory purposes in medical research. It is also applied in the 
</p>
<p>Chaps.   61     and   64    .    
</p>
<p>65 Automatic Modeling for Clinical Event Prediction (200 Patients)</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_61">http://dx.doi.org/10.1007/978-3-319-15195-3_61</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_64">http://dx.doi.org/10.1007/978-3-319-15195-3_64</a></div>
</div>
<div class="page"><p/>
<p>417&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_66
</p>
<p>Chapter 66
</p>
<p>Automatic Newton Modeling in Clinical 
Pharmacology (15 Alfentanil Dosages, 15 
Quinidine Time-Concentration Relationships)
</p>
<p> General Purpose
</p>
<p>Traditional regression analysis selects a mathematical function, and, then, uses the 
</p>
<p>data to find the best fit parameters. For example, the parameters a and b for a linear 
</p>
<p>regression function with the equation y = a + bx have to be calculated according to
</p>
<p> 
</p>
<p>b regression coefficient
x x y y
</p>
<p>x x
</p>
<p>a intercept y bx
</p>
<p>= =
&minus;( ) &minus;( )
</p>
<p>&minus;( )
= = &minus;
</p>
<p>&sum;
</p>
<p>&sum;
2
</p>
<p> 
</p>
<p>With a quadratic function, y = a + b1x + b2x
2 (and other functions) the calculations are 
</p>
<p>similar, but more complex. Newton&rsquo;s method works differently. Instead of selecting 
</p>
<p>a mathematical function and using the data for finding the best fit parameter-values, 
</p>
<p>it uses arbitrary parameter-values for a, b1, b2, and, then, iteratively measures the 
</p>
<p>distance between the data and the modeled curve until the shortest distance is 
</p>
<p>obtained. Calculations are much more easy than those of traditional regression anal-
</p>
<p>ysis, making the method, particularly, interesting for comparing multiple functions 
</p>
<p>to one data set. Newton&rsquo;s method is mainly used for computer solutions of engineer-
</p>
<p>ing problems, but is little used in clinical research. This chapter is to assess whether 
</p>
<p>it is also suitable for the latter purpose.
</p>
<p>This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 2&rdquo; as Chap. 
</p>
<p>20, 2014.</p>
<p/>
</div>
<div class="page"><p/>
<p>418
</p>
<p> Specific Scientific Question
</p>
<p>Can Newton&rsquo;s methods provide appropriate mathematical functions for dose- 
</p>
<p>effectiveness and time-concentration studies?
</p>
<p> Examples
</p>
<p> Dose-Effectiveness Study
</p>
<p>Alfentanil dose x-axis mg/m2 effectiveness y-axis [1- pain scale]
</p>
<p>0,10 0,1701
</p>
<p>0,20 0,2009
</p>
<p>0,30 0,2709
</p>
<p>0,40 0,2648
</p>
<p>0,50 0,3013
</p>
<p>0,60 0,4278
</p>
<p>0,70 0,3466
</p>
<p>0,80 0,2663
</p>
<p>0,90 0,3201
</p>
<p>1,00 0,4140
</p>
<p>1,10 0,3677
</p>
<p>1,20 0,3476
</p>
<p>1,30 0,3656
</p>
<p>1,40 0,3879
</p>
<p>1,50 0,3649
</p>
<p>The above table gives the data of a dose-effectiveness study. Newton&rsquo;s algorithm is 
</p>
<p>performed. We will apply the online Nonlinear Regression Calculator of Xuru&rsquo;s 
</p>
<p>website. This website is made available by Xuru, the world largest business network 
</p>
<p>based in Auckland CA, USA. We simply copy or paste the data of the above table 
</p>
<p>into the spreadsheet given be the website, then click &ldquo;allow comma as decimal sepa-
</p>
<p>rator&rdquo; and click &ldquo;calculate&rdquo;. Alternatively the SPSS file available at extras.springer.
</p>
<p>com entitled &ldquo;newtonmethod&rdquo; can be opened if SPSS is installed in your computer 
</p>
<p>and the copy and paste commands are similarly given.
</p>
<p>Since Newton&rsquo;s method can be applied to (almost) any function, most computer 
</p>
<p>programs fit a given dataset to over 100 functions including Gaussians, sigmoids, 
</p>
<p>ratios, sinusoids etc. For the data given 18 significantly (P &lt; 0.05) fitting non-linear 
</p>
<p>functions were found, the first six of them are shown underneath.
</p>
<p>66 Automatic Newton Modeling in Clinical Pharmacology&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>419
</p>
<p>Non-linear function residual sum of squares P value
</p>
<p>1. y = 0.42x/(x + 0.17) 0.023 0.003
</p>
<p>2. y = &minus;1/(38.4x + 1)0.12 + 1 0.024 0.003
</p>
<p>3. y = 0.08 ln x + 0.36 0.025 0.004
</p>
<p>4. y = 0.40e&minus;0.11/x 0.025 0.004
</p>
<p>5. y = 0.36x0.26 0.027 0.004
</p>
<p>6. y = &minus;0.024/x + 0.37 0.029 0.005
</p>
<p>The first one gives the best fit. Its measure of certainty, given as residual sum of 
</p>
<p>squares, is 0.023. It is the function of a hyperbola:
</p>
<p> 
y x x= +( )0 42 0 17. / . .
</p>
<p> 
</p>
<p>This is convenient, because, dose-effectiveness curves are, often, successfully 
</p>
<p>assessed with hyperbolas mimicking the Michaelis-Menten equation. The parame-
</p>
<p>ters of the equation can be readily interpreted as effectivenessmaximum = 0.42, and 
</p>
<p> dissociation constant = 0.17. It is usually very laborious to obtain these parameters 
</p>
<p>from traditional regression modeling of the quantal effect histograms and cumula-
</p>
<p>tive histograms requiring data samples of at least 100 or so to be meaningful. The 
</p>
<p>underneath figure shows an Excel graph of the fitted non-linear function for the 
</p>
<p>data, using Newton&rsquo;s method (the best fit curve is here a hyperbola). A cubic spline 
</p>
<p>goes smoothly through every point, and does this by ensuring that the first and 
</p>
<p> second derivatives of the segments match those that are adjacent.
</p>
<p>The Newton&rsquo;s equation better fits the data than traditional modeling with linear, 
</p>
<p>logistic, quadratic, and polynomial modeling does as shown underneath.
</p>
<p>Examples</p>
<p/>
</div>
<div class="page"><p/>
<p>420
</p>
<p> Time-Concentration Study
</p>
<p>Time x-axis hours quinidine concentration g/ml
</p>
<p>0,10 0,41
</p>
<p>0,20 0,38
</p>
<p>0,30 0,36
</p>
<p>0,40 0,34
</p>
<p>0,50 0,36
</p>
<p>0,60 0,23
</p>
<p>0,70 0,28
</p>
<p>0,80 0,26
</p>
<p>0,90 0,17
</p>
<p>1,00 0,30
</p>
<p>1,10 0,30
</p>
<p>1,20 0,26
</p>
<p>1,30 0,27
</p>
<p>1,40 0,20
</p>
<p>1,50 0,17
</p>
<p>The above table gives the data of a time-concentration study. Again a non-linear 
</p>
<p>regression using Newton&rsquo;s algorithm is performed. We use the online Nonlinear 
</p>
<p>Regression Calculator of Xuru&rsquo;s website. We copy or paste the data of the above 
</p>
<p>table into the spreadsheet, then click &ldquo;allow comma as decimal separator&rdquo; and click 
</p>
<p>&ldquo;calculate&rdquo;. Alternatively the SPSS file available at extras.springer.com entitled 
</p>
<p>66 Automatic Newton Modeling in Clinical Pharmacology&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>421
</p>
<p>&ldquo;newtonmethod&rdquo; can be opened if SPSS is installed in your computer and the copy 
</p>
<p>and paste commands are similarly given. For the data given 10 statistically signifi-
</p>
<p>cantly (P &lt; 0.05) fitting non-linear functions were found and shown. For further 
</p>
<p>assessment of the data an exponential function, which is among the first five shown 
</p>
<p>by the software, is chosen, because relevant pharmacokinetic parameters can be 
</p>
<p>conveniently calculated from it:
</p>
<p> 
y e
</p>
<p>x
=
</p>
<p>&minus;
</p>
<p>0 41
0 48
</p>
<p>. .
.
</p>
<p> 
</p>
<p>This function&rsquo;s measure of uncertainty (residual sums of squares) value is 0.027, 
</p>
<p>(with a p-value of 0.003). The following pharmacokinetic parameters are derived:
</p>
<p> 
</p>
<p>0 41
</p>
<p>0
</p>
<p>0
. /
</p>
<p>.
</p>
<p>= = ( ) ( )
&minus;
</p>
<p>C administration dosage drug distribution volume
</p>
<p>448 = elimination constant.
 
</p>
<p>Below an Excel graph of the exponential function fitted to the data is given. Also, a 
</p>
<p>cubic spline curve going smoothly through every point and to be considered as a 
</p>
<p>perfect fit curve is again given. It can be observed from the figure that the exponen-
</p>
<p>tial function curve matches the cubic spline curve well.
</p>
<p>0
</p>
<p>0,05
</p>
<p>0,1
</p>
<p>0,15
</p>
<p>0,2
</p>
<p>0,25
</p>
<p>0,3
</p>
<p>0,35
</p>
<p>0,4
</p>
<p>0,45
</p>
<p>0 0,2 0,4 0,6 0,8 1 1,2 1,4 1,6
</p>
<p>time(hours)
</p>
<p>d
ru
</p>
<p>g
 c
</p>
<p>o
n
</p>
<p>c
e
n
</p>
<p>tr
a
ti
</p>
<p>o
n
</p>
<p>The Newton&rsquo;s equation fits the data approximately equally well as do traditional 
</p>
<p>best fit models with linear, logistic, quadratic, and polynomial modeling shown 
</p>
<p>underneath. However, traditional models do not allow for the computation of phar-
</p>
<p>macokinetic parameters.
</p>
<p>Examples</p>
<p/>
</div>
<div class="page"><p/>
<p>422
</p>
<p> Conclusion
</p>
<p>Newton&rsquo;s methods provide appropriate mathematical functions for dose- 
</p>
<p>effectiveness and time-concentration studies.
</p>
<p> Note
</p>
<p>More background theoretical and mathematical information of Newton&rsquo;s methods 
</p>
<p>are in Machine learning in medicine part three, Chap. 16, Newton&rsquo;s methods, 
</p>
<p>pp 161&ndash;172, Springer Heidelberg Germany, 2013, from the same authors.
</p>
<p>66 Automatic Newton Modeling in Clinical Pharmacology&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>423&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_67
</p>
<p>    Chapter 67  
</p>
<p> Spectral Plots for High Sensitivity Assessment 
of Periodicity (6 Years&rsquo; Monthly C Reactive 
Protein Levels)      
</p>
<p>                 General Purpose 
</p>
<p> In clinical research times series often show many peaks and irregular spaces. 
</p>
<p> Spectral plots is based on traditional Fourier analyses, and may be more sensitive 
</p>
<p>than traditional autocorrelation analysis in this situation.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> To assess whether, in monthly C reactive Protein (CRP) levels with inconclusive 
</p>
<p>scattergrams and autocorrelation analysis, spectral plot methodology is able to dem-
</p>
<p>onstrate periodicity even so.  
</p>
<p>    Example 
</p>
<p> A data fi le of 6 years&rsquo; mean monthly CRP levels from a target population was 
</p>
<p>assessed for seasonality. The fi rst 2 years&rsquo; values are given underneath. The entire 
</p>
<p>data fi le is in &ldquo;spectralanalysis&rdquo; as available on the internet at extras.springer.com. 
</p>
<p> First day of month  CRP level (mg/l) 
</p>
<p> 1993/07/01  1.29 
</p>
<p> 1993/08/01  1.43 
</p>
<p> 1993/09/01  1.54 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 3&rdquo; as Chap. 9, 
</p>
<p>2014. 
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>424
</p>
<p> First day of month  CRP level (mg/l) 
</p>
<p> 1993/10/01  1.68 
</p>
<p> 1993/11/01  1.54 
</p>
<p> 1993/12/01  2.78 
</p>
<p> 1994/01/01  1.27 
</p>
<p> 1994/02/01  1.26 
</p>
<p> 1994/03/01  1.26 
</p>
<p> 1994/04/01  1.54 
</p>
<p> 1994/05/01  1.13 
</p>
<p> 1994/06/01  1.60 
</p>
<p> 1994/07/01  1.47 
</p>
<p> 1994/08/01  1.78 
</p>
<p> 1994/09/01  2.69 
</p>
<p> 1994/10/01  1.91 
</p>
<p> 1994/11/01  1.74 
</p>
<p> 1994/12/01  3.11 
</p>
<p>   Start by opening the data fi le in SPSS.
</p>
<p>  Command: 
</p>
<p>  Click Graphs&hellip;.click Legacy Dialogs&hellip;.click Scatter/Dot&hellip;. Click Simple 
</p>
<p>Scatter&hellip;..click Defi ne&hellip;.y-axis: enter &ldquo;mean crp mg/l&rdquo;&hellip;.x-axis: enter date&hellip;.
</p>
<p>click OK.    
</p>
<p>    
</p>
<p>67 Spectral Plots for High Sensitivity Assessment of Periodicity (6 Years&rsquo; Monthly&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>425
</p>
<p>    In the output the above fi gure is displayed. Many peaks and irregularities are 
</p>
<p>observed, and the presence of periodicity is not unequivocal. 
</p>
<p> Subsequently, autocorrelation coeffi cients are computed.
</p>
<p>  Command: 
</p>
<p>  click Analyze&hellip;.click Forecast&hellip;.click Autocorrelations&hellip;.Variables: enter &ldquo;mean 
</p>
<p>crp mg/l&rdquo;&hellip;.click OK.    
</p>
<p>    
</p>
<p>    In the output the above autocorrelation coeffi cients are given. It suggests the 
</p>
<p>presence of periodicity. However, this conclusion is based on a single value, i.e., the 
</p>
<p>12th month value, and, for concluding unequivocal periodicity not only autocorrela-
</p>
<p>tion coeffi cients signifi cantly larger than 0 but also signifi cantly smaller than 0 
</p>
<p>should have been observed. 
</p>
<p> Spectral plots may be helpful for support.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Forecasting&hellip;.Spectral Analysis&hellip;.select CRP and enter into 
</p>
<p>Variable(s)&hellip;.select Spectral density in Plot&hellip;.click Paste&hellip;.change in syntax text: 
</p>
<p>TSET PRINT-DEFAULT into TSET PRINT-DETAILED&hellip;. click Run&hellip;.click All.    
</p>
<p> In the output sheets underneath the  periodogram  is observed (upper part) with 
</p>
<p>mean CRP values on the y-axis and frequencies on the x-axis. Of the peaks CRP- 
</p>
<p>values observed the fi rst one has a frequency of slightly less than 0.1. We assumed 
</p>
<p>that CRP had an annual periodicity. Twelve months are in a year, months is the unit 
</p>
<p>applied. As period is the inverted value of frequency a period of 12 months would 
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>426
</p>
<p>equal a frequency of 1/12 = 0.0833. An annual periodicity would produce a peak 
</p>
<p>CRP-value with a frequency of 1/12 = 0.0833. Indeed, the table underneath shows 
</p>
<p>that at a frequency of 0.0833 the highest CRP value is observed. However, many 
</p>
<p>more peaks are observed, and how to interpret them. For that purpose we use  spec-
</p>
<p>tral density analysis  (lower fi gure underneath). 
</p>
<p>    
</p>
<p> Univariate statistics 
</p>
<p> Series name:mean crp mg/l 
</p>
<p> Frequency  Period 
</p>
<p> Sine 
</p>
<p>transform 
</p>
<p> Cosine 
</p>
<p>transform  Periodogram 
</p>
<p> Spectral 
</p>
<p>density 
</p>
<p>estimate 
</p>
<p> 1  ,00000  ,000  1,852  ,000  8,767 
</p>
<p> 2  ,01389  &minus;,197  ,020  1,416  12,285 
</p>
<p> 3  ,02778  &minus;,123  ,012  ,552  9,223 
</p>
<p> 4  ,04167  &minus;,231  ,078  2,144  10,429 
</p>
<p> 5  ,05556  ,019  ,010  ,016  23,564 
</p>
<p> 6  ,06944  &minus;,040  &minus;,117  ,552  22,985 
</p>
<p> 7  ,08333  &minus;,365  ,267  7,355  19,519 
</p>
<p> 8  ,09722  &minus;,057  &minus;,060  ,243  20,068 
</p>
<p> 9  ,11111  &minus;,101  &minus;,072  ,556  20,505 
</p>
<p> 10  ,12500  &minus;,004  &minus;,089  ,286  5,815 
</p>
<p> 11  ,13889  ,065  &minus;,135  ,811  10,653 
</p>
<p> 12  ,15278  &minus;,024  ,139  ,715  10,559 
</p>
<p>67 Spectral Plots for High Sensitivity Assessment of Periodicity (6 Years&rsquo; Monthly&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>427
</p>
<p>      
</p>
<p>    The spectral density curve is a fi ltered, otherwise called smoothed, version of the 
</p>
<p>usual periodogram with irregularities beyond a given threshold (noise) fi ltered out. 
</p>
<p>The above spectral density curve shows fi ve distinct peaks with a rather regular pat-
</p>
<p>tern. The lowest frequency simply displays the yearly peak at a frequency of 0.0833. 
</p>
<p>The other peaks at higher frequencies are the result of the Fourier model consistent 
</p>
<p>of sine and cosine functions, and do not indicate additional periodicities. Even so 
</p>
<p>much so that they demonstrate the absence of further periodicities.  
</p>
<p>    Conclusion 
</p>
<p> Seasonal patterns are assumed in many fi elds of medicine. Usually, the mean differ-
</p>
<p>ences between the data of different seasons or months are used. E.g., the number of 
</p>
<p>hospital admissions in the month of January may be roughly twice that of July. 
</p>
<p>However, biological processes are full of variations and the possibility of chance 
</p>
<p>fi ndings can not be fully ruled out. Autocorrelations can be adequately used for the 
</p>
<p>purpose. It is a technique that cuts time curves into pieces. These pieces are, subse-
</p>
<p>quently, compared with the original data-curve using linear regression analysis. 
</p>
<p>Autocorrelation coeffi cients signifi cantly larger and smaller than 0 must be observed 
</p>
<p>in order to conclude periodicity. If not, spectral analysis is often helpful. 
</p>
<p> It displays a peak outcome at the frequency of the expected periodicity (months, 
</p>
<p>years, weeks etc.). The current chapter shows that spectral analysis can be ade-
</p>
<p>quately used with very irregular patterns and inconclusive autocorrelation analysis, 
</p>
<p>and is able to demonstrate unequivocal periodicities where visual methods like 
</p>
<p>scatter-grams and traditional methods like autocorrelations are inconclusive. 
</p>
<p>Conclusion</p>
<p/>
</div>
<div class="page"><p/>
<p>428
</p>
<p> A limitation of spectral analysis is the variance problem. The periodogram&rsquo;s 
</p>
<p>variance does not decrease with increased sample sizes. However, smoothing using 
</p>
<p>the spectral density function, is sample size dependent, and therefore, reduces the 
</p>
<p>variance problem.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of spectral analysis 
</p>
<p>and autocorrelations is given in Machine learning in medicine part three, Chap. 15, 
</p>
<p>Spectral plots, pp 151&ndash;160, Springer Heidelberg Germany 2013, and in Machine 
</p>
<p>learning in medicine part one, Chap. 10, Seasonality assessments, pp 113&ndash;126, 
</p>
<p>Springer Heidelberg Germany, 2013, both from the same authors.   
</p>
<p>67 Spectral Plots for High Sensitivity Assessment of Periodicity (6 Years&rsquo; Monthly&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>429&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_68
</p>
<p>    Chapter 68   
</p>
<p> Runs Test for Identifying Best Regression 
Models (21 Estimates of Quantity and Quality 
of Patient Care) 
</p>
<p>                      General Purpose 
</p>
<p> R-square values are often used to test the appropriateness of diagnostic models. 
</p>
<p> However, in practice, pretty large r-square values (squared correlation coeffi -
</p>
<p>cients) may be observed even if data do not fi t the model very well. This chapter 
</p>
<p>assesses whether the runs test is a better alternative to the traditional r-square test for 
</p>
<p>addressing the differences between the data and the best fi t regression models.  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> A real data example was given comparing quantity of care with quality of care 
</p>
<p>scores.  
</p>
<p>    Example 
</p>
<p> Doctors were assessed for the relationship between their quantity and quality of 
</p>
<p>care. The quantity of care was estimated with the numbers of daily interventions 
</p>
<p>like endoscopies and small operations per doctor, the quality of care with quality of 
</p>
<p>care scores. The data fi le is given below, and is also available in &ldquo;runstest&rdquo; on the 
</p>
<p>internet at extras.springer.com. 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 3&rdquo; as Chap. 
</p>
<p>10, 2014. </p>
<p/>
</div>
<div class="page"><p/>
<p>430
</p>
<p> Quantity of care  Quality of care 
</p>
<p> 19,00  2,00 
</p>
<p> 20,00  3,00 
</p>
<p> 23,00  4,00 
</p>
<p> 24,00  5,00 
</p>
<p> 26,00  6,00 
</p>
<p> 27,00  7,00 
</p>
<p> 28,00  8,00 
</p>
<p> 29,00  9,00 
</p>
<p> 29,00  10,00 
</p>
<p> 29,00  11,00 
</p>
<p> 28,00  12,00 
</p>
<p> 27,00  13,00 
</p>
<p> 27,00  14,00 
</p>
<p> 26,00  15,00 
</p>
<p> 25,00  16,00 
</p>
<p> 24,00  17,00 
</p>
<p> 23,00  18,00 
</p>
<p> 22,00  19,00 
</p>
<p> 22,00  20,00 
</p>
<p> 21,00  21,00 
</p>
<p> 21,00  22,00 
</p>
<p>  Quantity of care = numbers of daily interventions 
</p>
<p> per doctor; Quality of care = quality of care scores 
</p>
<p>    The relationship seemed not to be linear, and curvilinear regression in SPSS was 
</p>
<p>used to fi nd the best fi t curve to describe the data and eventually use them as predic-
</p>
<p>tion model. First, we will make a graph of the data.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Graphs&hellip;.Chart builder&hellip;.click: Scatter/Dot&hellip;.Click quality of care 
</p>
<p>and drag to the Y-Axis&hellip;.Click Intervention per doctor and drag to the  X-Axis&hellip;.
</p>
<p>OK.        
</p>
<p>68 Runs Test for Identifying Best Regression Models (21 Estimates of Quantity&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>431
</p>
<p>    
</p>
<p>    The above fi gure shows the scattergram of the data. A non-linear relationship is 
</p>
<p>indeed suggested, and the curvilinear regression option in SPSS was helpful to fi nd 
</p>
<p>the best fi t model.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Regression&hellip;.Curve Estimation&hellip;.mark: Quadratic, Cubic&hellip;.mark: 
</p>
<p>Display ANOVA Table&hellip;.OK.    
</p>
<p> The quadratic (best fi t second order, parabolic, relationship) and cubic (best fi t 
</p>
<p>third order, hyperbolic, relationship) were the best options, with very good r-squares 
</p>
<p>and p-values &lt; 0.0001 as shown in the table given by the software.
</p>
<p> Model summary and parameter estimates 
</p>
<p> Dependent variable:qual care score 
</p>
<p> Equation 
</p>
<p> Model summary  Parameter estimates 
</p>
<p> R square  F  df1  df2  Sig.  Constant  b1  b2  b3 
</p>
<p> Quadratic  ,866  58,321  2  18  ,000  16,259  2,017  &minus;,087 
</p>
<p> Cubic  ,977  236,005  3  17  ,000  10,679  4,195  &minus;,301  ,006 
</p>
<p>  The independent variable is interventions/doctor 
</p>
<p>    The runs test requires the residues from respectively the best fi t quadratic and the 
</p>
<p>cubic models of the data (instead of &minus; and + distances from the modeled curves (the 
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>432
</p>
<p>residues) to be read from the above fi gure, the values 0 and 1 have to be added as 
</p>
<p>separate variables used in SPSS). 
</p>
<p> Quantity of care  Quality of care  Residues quadratic model  Residues cubic model 
</p>
<p> 19,00  2,00  0,00  1,00 
</p>
<p> 20,00  3,00  0,00  0,00 
</p>
<p> 23,00  4,00  1,00  1,00 
</p>
<p> 24,00  5,00  0,00  0,00 
</p>
<p> 26,00  6,00  1,00  0,00 
</p>
<p> 27,00  7,00  1,00  0,00 
</p>
<p> 28,00  8,00  1,00  0,00 
</p>
<p> 29,00  9,00  1,00  1,00 
</p>
<p> 29,00  10,00  1,00  1,00 
</p>
<p> 29,00  11,00  1,00  1,00 
</p>
<p> 28,00  12,00  1,00  1,00 
</p>
<p> 27,00  13,00  0,00  0,00 
</p>
<p> 27,00  14,00  0,00  1,00 
</p>
<p> 26,00  15,00  0,00  1,00 
</p>
<p> 25,00  16,00  0,00  0,00 
</p>
<p> 24,00  17,00  0,00  0,00 
</p>
<p> 23,00  18,00  0,00  0,00 
</p>
<p> 22,00  19,00  0,00  0,00 
</p>
<p> 22,00  20,00  1,00  1,00 
</p>
<p> 21,00  21,00  1,00  0,00 
</p>
<p> 21,00  22,00  1,00  1,00 
</p>
<p>   Command: 
</p>
<p>  Analyze&hellip;.Nonparametric tests&hellip;.Runs Test&hellip;.move the runsquadratic model resi-
</p>
<p>dues variable to Test Variable List&hellip;.click Options&hellip;.click Descriptives&hellip;.click 
</p>
<p>Continue&hellip;.click Cut Point&hellip;.mark Median&hellip;.click OK.    
</p>
<p> The output table shows that in the runs test the quadratic model differs from the 
</p>
<p>actual data with p = 0.02. It means that the quadratic model is systematically differ-
</p>
<p>ent from the data.
</p>
<p> Runs test 
</p>
<p> Runsquadraticmodel 
</p>
<p> Test value a   1,00 
</p>
<p> Cases &lt; test value  10 
</p>
<p> Cases &gt; = test value  11 
</p>
<p> Total cases  21 
</p>
<p> Number of runs  6 
</p>
<p> Z  &minus;2,234 
</p>
<p>(continued)
</p>
<p>68 Runs Test for Identifying Best Regression Models (21 Estimates of Quantity&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>433
</p>
<p> Runs test 
</p>
<p> Runsquadraticmodel 
</p>
<p> Asymp. sig. (2-tailed)  ,026 
</p>
<p> Exact sig. (2-tailed)  ,022 
</p>
<p> Point probability  ,009 
</p>
<p>   a Median 
</p>
<p>    When the similar procedure is followed for the best fi t cubic model, the result is 
</p>
<p>very insignifi cant with a p-value of 1.00. The cubic model was, thus, a much better 
</p>
<p>predicting model for the data than the quadratic model.
</p>
<p> Runs test 2 
</p>
<p> Runscubicmodel 
</p>
<p> Test value a   ,4762 
</p>
<p> Cases &lt; test value  11 
</p>
<p> Cases &gt; = test value  10 
</p>
<p> Total cases  21 
</p>
<p> Number of runs  11 
</p>
<p> Z  ,000 
</p>
<p> Asymp. sig. (2-tailed)  1,000 
</p>
<p> Exact sig. (2-tailed)  1,000 
</p>
<p> Point probability  ,165 
</p>
<p>   a Mean 
</p>
<p>        Conclusion 
</p>
<p> The runs test is appropriate both for testing whether fi tted theoretical curves are 
</p>
<p>systematically different or not from a given data set. The fi t of regression models is 
</p>
<p>traditionally assessed with r-square tests. However, the runs test is more appropriate 
</p>
<p>for the purpose, because large r-square value do not exclude poor systematic data fi t, 
</p>
<p>and because the runs test assesses the entire pattern in the data, rather than mean 
</p>
<p>distances between data and model.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of the runs test is given 
</p>
<p>in Machine learning in medicine part three, Chap. 13, Runs test, pp 127&ndash;135, 
</p>
<p>Springer Heidelberg Germany 2013, from the same authors.    
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>435&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_69
</p>
<p>    Chapter 69   
</p>
<p> Evolutionary Operations for Process 
Improvement (8 Operation Room Air 
Condition Settings) 
</p>
<p>                      General Purpose 
</p>
<p> Evolutionary operations (evops) try and fi nd improved processes by exploring the 
</p>
<p>effect of small changes in an experimental setting. It stems from evolutionary 
</p>
<p>algorithms (see Machine learning in medicine part three, Chap. 2, Evolutionary 
</p>
<p>operations, pp 11&ndash;18, Springer Heidelberg Germany, 2013, from the same authors), 
</p>
<p>which uses rules based on biological evolution mechanisms where each next 
</p>
<p>generation is slightly different and generally somewhat improved as compared to its 
</p>
<p>ancestors. It is widely used not only in genetic research, but also in chemical and 
</p>
<p>technical processes. So much so that the internet nowadays offers free evop calcula-
</p>
<p>tors suitable not only for the optimization of the above processes, but also for the 
</p>
<p>optimization of your pet&rsquo;s food, your car costs, and many other daily life standard 
</p>
<p>issues. This chapter is to assess how evops can be helpful to optimize the air quality 
</p>
<p>of operation rooms.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> The air quality of operation rooms is important for infection prevention. Particularly, 
</p>
<p>the factors (1) humidity (30&ndash;60 %), (2) fi lter capacity (70&ndash;90 %), and (3) air volume 
</p>
<p>change (20&ndash;30 % per hour) are supposed to be important determinants. Can an evo-
</p>
<p>lutionary operation be used for process improvement.  
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 3&rdquo; as Chap. 
</p>
<p>11, 2014. </p>
<p/>
</div>
<div class="page"><p/>
<p>436
</p>
<p>    Example 
</p>
<p> Eight operation room air condition settings were investigated, and the results are 
</p>
<p>underneath. 
</p>
<p> Operation Setting 
</p>
<p> humidity 
</p>
<p>(30 % = 1, 60 % = 4) 
</p>
<p> fi lter capacity 
</p>
<p>(70 % = 1, 90 % = 3) 
</p>
<p> air volume change 
</p>
<p>(20 % = 1, 30 % = 3) 
</p>
<p> infections 
</p>
<p>number of 
</p>
<p> 1  1  1  1  99 
</p>
<p> 2  2  1  1  90 
</p>
<p> 3  1  2  1  75 
</p>
<p> 4  2  2  1  73 
</p>
<p> 5  1  1  2  99 
</p>
<p> 6  2  1  2  99 
</p>
<p> 7  1  2  2  61 
</p>
<p> 8  2  2  2  52 
</p>
<p>   We will use multiple linear regression in SPSS with the number of infections as 
</p>
<p>outcome and the three factors as predictors to identify the signifi cant predictors. 
</p>
<p> First, the data fi le available as &ldquo;evops&rdquo; in extras.springer.com is opened in SPSS.
</p>
<p>  Command: 
</p>
<p>     Analyze&hellip;.Regression&hellip;.Linear&hellip;.Dependent: enter "Var00004"&hellip;. Independent(s): 
</p>
<p>enter "Var00001-00003"&hellip;.click OK.    
</p>
<p> The underneath table in the output shows that all of the determinants are statisti-
</p>
<p>cally signifi cant at p &lt; 0.10. A higher humidity, fi ltering level, and air volume change 
</p>
<p>better prevents infections.
</p>
<p> Coeffi cients a  
</p>
<p> Model 
</p>
<p> Unstandardized 
</p>
<p>coeffi cients 
</p>
<p> Standardized 
</p>
<p>coeffi cients 
</p>
<p> B  Std. error  Beta  t  Sig. 
</p>
<p> 1  (Constant)  103,250  18,243  5,660  ,005 
</p>
<p> Humidity1  &minus;12,250  3,649  &minus;,408  &minus;3,357  ,028 
</p>
<p> Filter capacity1  &minus;21,250  3,649  &minus;,707  &minus;5,824  ,004 
</p>
<p> Air volume change1  15,750  3,649  ,524  4,317  ,012 
</p>
<p>   a Dependent Variable: infections1 
</p>
<p>    In the next eight operation settings higher determinant levels were assessed. 
</p>
<p> Operation Setting 
</p>
<p> humidity 
</p>
<p>(30 % = 1, 60 % = 4) 
</p>
<p> fi lter capacity 
</p>
<p>(70 % = 1, 90 % = 3) 
</p>
<p> air volume change  
</p>
<p>(20 % = 1, 30 % = 3) 
</p>
<p> infections 
</p>
<p>number of 
</p>
<p> 1  3  2  2  51 
</p>
<p> 2  4  2  2  45 
</p>
<p>(continued)
</p>
<p>69 Evolutionary Operations for Process Improvement (8 Operation Room Air&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>437
</p>
<p> Operation Setting 
</p>
<p> humidity 
</p>
<p>(30 % = 1, 60 % = 4) 
</p>
<p> fi lter capacity 
</p>
<p>(70 % = 1, 90 % = 3) 
</p>
<p> air volume change  
</p>
<p>(20 % = 1, 30 % = 3) 
</p>
<p> infections 
</p>
<p>number of 
</p>
<p> 3  3  3  2  33 
</p>
<p> 4  4  3  2  26 
</p>
<p> 5  3  2  3  73 
</p>
<p> 6  4  2  3  60 
</p>
<p> 7  3  3  3  54 
</p>
<p> 8  4  3  3  31 
</p>
<p>   We will use again multiple linear regression in SPSS with the number of infec-
</p>
<p>tions as outcome and the three factors as predictors to identify the signifi cant 
</p>
<p>predictors.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Regression&hellip;.Linear&hellip;.Dependent: enter "Var00008"&hellip;. Independent(s): 
</p>
<p>enter "Var00005-00007"&hellip;.click OK.   
</p>
<p> Coeffi cients a  
</p>
<p> Model 
</p>
<p> Unstandardized 
</p>
<p>coeffi cients 
</p>
<p> Standardized 
</p>
<p>coeffi cients 
</p>
<p> B  Std. error  Beta  t  Sig. 
</p>
<p> 1  (Constant)  145,500  15,512  9,380  ,001 
</p>
<p> Humidity2  &minus;5,000  5,863  &minus;,145  &minus;,853  ,442 
</p>
<p> Filter capacity2  &minus;31,500  5,863  &minus;,910  &minus;5,373  ,006 
</p>
<p> Air volume change2  &minus;6,500  5,863  &minus;,188  &minus;1,109  ,330 
</p>
<p>   a Dependent Variable: infections2 
</p>
<p>    The underneath table in the output shows that only Var 00006 (the fi lter capacity) 
</p>
<p>is still statistically signifi cant. Filter capacity 3 performs better than 2, while humid-
</p>
<p>ity levels and air volume changes were not signifi cantly different. We could go one 
</p>
<p>step further to fi nd out how higher levels would perform, but for now we will con-
</p>
<p>clude that humidity level 2&ndash;4, fi lter capacity level 3, and air fl ow change level 2&ndash;4 
</p>
<p>are effi cacious level combinations. Higher levels of humidity and air fl ow change is 
</p>
<p>not meaningful. An additional benefi t of a higher level of fi lter capacity cannot be 
</p>
<p>excluded, but requires additional testing.  
</p>
<p>    Conclusion 
</p>
<p> Evolutionary operations can be used to improve the process of air quality mainte-
</p>
<p>nance in operation rooms. This methodology can similarly be applied for fi nding the 
</p>
<p>best settings for numerous clinical, and laboratory settings. We have to add that 
</p>
<p>interaction between the predictors was not taken into account in the current 
</p>
<p>Conclusion</p>
<p/>
</div>
<div class="page"><p/>
<p>438
</p>
<p>example. For a meaningful assessment of 2- and 3-factor interactions larger samples 
</p>
<p>would be required, however. Moreover, we have clinical arguments that no impor-
</p>
<p>tant interactions are to be expected.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of evops is given in 
</p>
<p>Machine learning in medicine part three, Chap. 2, Evolutionary operations, 
</p>
<p>pp 11&ndash;18, Springer Heidelberg Germany, 2013, from the same authors.    
</p>
<p>69 Evolutionary Operations for Process Improvement (8 Operation Room Air&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>439&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_70
</p>
<p>    Chapter 70   
</p>
<p> Bayesian Networks for Cause Effect Modeling 
(600 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> Bayesian networks are probabilistic graphical models using nodes and arrows, 
</p>
<p>respectively representing variables, and probabilistic dependencies between two 
</p>
<p>variables. Computations in a Bayesian network are performed using weighted like-
</p>
<p>lihood methodology and marginalization, meaning that irrelevant variables are inte-
</p>
<p>grated or summed out. Additional theoretical information is given in Machine 
</p>
<p>Learning in medicine part two, Chap. 16, Bayesian networks, pp 163&ndash;170, Springer 
</p>
<p>Heidelberg Germany, 2013 (from the same authors). This chapter is to assess if 
</p>
<p>Bayesian networks is able to determine direct and indirect predictors of binary out-
</p>
<p>comes like morbidity/mortality outcomes.  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> Longevity is multifactorial, and logistic regression is adequate to assess the chance 
</p>
<p>of longevity in patients with various predictor scores like physical, psychological, 
</p>
<p>and family scores. However, some factors may have both direct and indirect effects. 
</p>
<p>Can a best fi t Bayesian network demonstrate not only direct but also indirect effects 
</p>
<p>of factors on the outcome?  
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 3&rdquo; as Chap. 
</p>
<p>12, 2014. </p>
<p/>
</div>
<div class="page"><p/>
<p>440
</p>
<p>    Example 
</p>
<p> In 600 patients, 70 years of age, a score sampling of factors predicting longevity 
</p>
<p>was performed. The outcome was death after 10 years of follow-up. The fi rst 12 
</p>
<p>patients are underneath, the entire data fi le is in &ldquo;longevity&rdquo;, and is available at 
</p>
<p>extras.springer.com. We will fi rst perform a logistic regression of these data using 
</p>
<p>SPSS statistical software. Start by opening SPSS. Enter the above data fi le. 
</p>
<p> Variables 
</p>
<p> 1  2  3  4  5  6 
</p>
<p> death  econ  psychol  physic  family  educ 
</p>
<p> 0  70  117  76  77  120 
</p>
<p> 0  70  68  76  56  114 
</p>
<p> 0  70  74  71  57  109 
</p>
<p> 0  90  114  82  79  125 
</p>
<p> 0  90  117  100  68  123 
</p>
<p> 0  70  74  100  57  121 
</p>
<p> 1  70  77  103  62  145 
</p>
<p> 0  70  62  71  56  100 
</p>
<p> 0  90  86  88  65  114 
</p>
<p> 0  90  77  88  61  111 
</p>
<p> 0  110  56  65  59  130 
</p>
<p> 0  70  68  50  60  118 
</p>
<p>  death (0 = no) 
</p>
<p> econ = economy score 
</p>
<p> psychol = psychological score 
</p>
<p> physic = physical score 
</p>
<p> family = familial risk score of longevity 
</p>
<p> educ = educational score 
</p>
<p>        Binary Logistic Regression in SPSS 
</p>
<p>   Command: 
</p>
<p>  Analyze....Regression....Binary Logistic....Dependent: enter "death"....Covariates: 
</p>
<p>enter "econ, psychol, physical, family, educ"....OK.    
</p>
<p> The underneath output table shows the results. With p &lt; 0.10 as cut-off for statis-
</p>
<p>tical signifi cance, all of the covariates, except economical score, were signifi cant 
</p>
<p>predictors of longevity (death), although both negative and positive b-values were 
</p>
<p>observed.
</p>
<p>70 Bayesian Networks for Cause Effect Modeling (600 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>441
</p>
<p> Variables in the equation 
</p>
<p> B  S.E.  Wald  df  Sig.  Exp(B) 
</p>
<p> Step 1 a   ecom  ,003  ,006  ,306  1  ,580  1,003 
</p>
<p> psychol  &minus;,056  ,009  43,047  1  ,000  ,946 
</p>
<p> physical  &minus;,019  ,007  8,589  1  ,003  ,981 
</p>
<p> family  ,045  ,017  7,297  1  ,007  1,046 
</p>
<p> educ  ,017  ,009  3,593  1  ,058  1,018 
</p>
<p> Constant  &minus;,563  ,922  ,373  1  ,541  ,569 
</p>
<p>   a Variable(s) entered on step 1: ecom, psychol, physical, family, educ 
</p>
<p>    For these data we hypothesized that all of these scores would independently 
</p>
<p>affect longevity. However, indirect effects were not taken into account, like the 
</p>
<p>effect of psychological on physical scores, and the effect of family on educational 
</p>
<p>scores etc. In order to assess both direct and indirect effects, a Bayesian network 
</p>
<p>DAG (directed acyclic graph) was fi tted to the data. The Konstanz information 
</p>
<p>miner (Knime) was used for the analysis. In order to enter the SPSS data fi le in 
</p>
<p>Knime, an excel version of the data fi le is required. For that purpose open the fi le in 
</p>
<p>SPSS and follow the commands.
</p>
<p>  Command in SPSS: 
</p>
<p>  click File....click Save as....in "Save as&rdquo; type: enter Comma Delimited (*.csv)....
</p>
<p>click Save.    
</p>
<p> For convenience the excel fi le has been added to extras.springer.com, and is, just 
</p>
<p>like the SPSS fi le, entitled &ldquo;longevity&rdquo;.  
</p>
<p>    Konstanz Information Miner (Knime) 
</p>
<p> In Google enter the term &ldquo;knime&rdquo;. Click Download and follow instructions. After 
</p>
<p>completing the pretty easy download procedure, open the knime workbench by 
</p>
<p>clicking the knime welcome screen. The center of the screen displays the workfl ow 
</p>
<p>editor like the canvas in SPSS modeler. It is empty, and can be used to build a stream 
</p>
<p>of nodes, called workfl ow in knime. The node repository is in the left lower angle of 
</p>
<p>the screen, and the nodes can be dragged to the workfl ow editor simply by left- 
</p>
<p>clicking. The nodes are computer tools for data analysis like visualization and sta-
</p>
<p>tistical processes. Node description is in the right upper angle of the screen. Before 
</p>
<p>the nodes can be used, they have to be connected with the fi le reader and with one 
</p>
<p>another by arrows drawn again simply by left clicking the small triangles attached 
</p>
<p>to the nodes. Right clicking on the fi le reader enables to confi gure from your com-
</p>
<p>puter a requested data fi le....click Browse....and download from the appropriate 
</p>
<p>folder a csv type Excel fi le. You are almost set for analysis now, but in order to 
</p>
<p>perform a Bayesian analysis Weka software 3.6 for windows (statistical software 
</p>
<p>from the University of Waikato (New Zealand)) is required. Simply type the term 
</p>
<p>Weka software, and fi nd the site. The software can be freely downloaded from the 
</p>
<p>Konstanz Information Miner (Knime)</p>
<p/>
</div>
<div class="page"><p/>
<p>442
</p>
<p>internet, following a few simple instructions, and it can, subsequently, be readily 
</p>
<p>opened in Knime. Once it has been opened, it is stored in your Knime node reposi-
</p>
<p>tory, and you will be able to routinely use it.  
</p>
<p>    Knime Workfl ow 
</p>
<p> A knime workfl ow for the analysis of the above data example is built, and the fi nal 
</p>
<p>result is shown in the underneath fi gure, by dragging and connecting as explained 
</p>
<p>above. 
</p>
<p>    
</p>
<p>    In the node repository click and type File Reader and drag to workfl ow editor in 
</p>
<p>the node repository click again File reader....click the ESCbutton of your com-
</p>
<p>puter....in the node repository click again and type Number to String....the node is 
</p>
<p>displayed....drag it to the workfl ow editor....perform the same kind of actions for all 
</p>
<p>of the nodes as shown in the above fi gure....connect, by left clicking, all of the nodes 
</p>
<p>with arrows as indicated above....click File Reader....click Browse....and type the 
</p>
<p>requested data fi le (&ldquo;longevity.csv&rdquo;)....click OK....the data fi le is given....right click 
</p>
<p>all of the nodes and then right click Confi gurate and execute all of the nodes by right 
</p>
<p>clicking the nodes and then the texts &ldquo;Confi gurate&rdquo; and &ldquo;Execute&rdquo;....the red lights 
</p>
<p>will successively turn orange and then green....right click the Weka Predictor node....
</p>
<p>right click the Weka Node View....right click Graph. 
</p>
<p>  
</p>
<p>ecom psychol family
</p>
<p>physical educ
</p>
<p>death
</p>
<p>  
</p>
<p>70 Bayesian Networks for Cause Effect Modeling (600 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>443
</p>
<p>    The above graph, a socalled directed acyclic graph (DAG) shows the Bayesian 
</p>
<p>network obtained from the analysis. This best fi tting DAG was, obviously, more 
</p>
<p>complex than expected from the logistic model. Longevity was directly determined 
</p>
<p>by all of the fi ve predictors, but additional indirect effects were between physical 
</p>
<p>and psychological scores, and between educational and family scores. In order to 
</p>
<p>assess the validity of the Bayesian model, a confusion matrix and accuracy statistics 
</p>
<p>were computed. 
</p>
<p> Right click the Scorer node....right click Confusion matrix
</p>
<p>    Confusion matrix     
</p>
<p>    
</p>
<p>    The observed and predicted values are summarized. Subsequently, right click 
</p>
<p>Accuracy statistics.
</p>
<p>    Accuracy statistics     
</p>
<p>    
</p>
<p>    The sensitivity of the Bayesian model to predict longevity was pretty good, 
</p>
<p>80.8 %. However, the specifi city was pretty bad. &ldquo;No deaths&rdquo; were rightly predicted 
</p>
<p>in 80.8 % of the patients, &ldquo;deaths&rdquo;, however, were rightly predicted in only 32.6 % 
</p>
<p>of the patients.  
</p>
<p>    Conclusion 
</p>
<p> Bayesian networks are probabilistic graphical models for assessing cause effect 
</p>
<p>relationships. This chapter is to assess if Bayesian networks is able to determine 
</p>
<p>direct and indirect predictors of binary outcomes like morbidity/mortality outcomes. 
</p>
<p>As an example a longevity study is used. Longevity is multifactorial, and logistic 
</p>
<p>regression is adequate to assess the chance of longevity in patients with various 
</p>
<p>Conclusion</p>
<p/>
</div>
<div class="page"><p/>
<p>444
</p>
<p>predictor scores like physical, psychological, and family scores. However, factors 
</p>
<p>may have both direct and indirect effects. A best fi t Bayesian network demonstrated 
</p>
<p>not only direct but also indirect effects of the factors on the outcome.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of Bayesian networks 
</p>
<p>is in Machine learning in medicine part two, Chap. 16, Bayesian networks, pp 163&ndash;
</p>
<p>170, Springer Heidelberg Germany, 2013, from the same authors.    
</p>
<p>70 Bayesian Networks for Cause Effect Modeling (600 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>445&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_71
</p>
<p>    Chapter 71   
</p>
<p> Support Vector Machines for Imperfect 
Nonlinear Data (200 Patients with Sepsis) 
</p>
<p>                      General Purpose 
</p>
<p> The basic aim of support vector machines is to construct the best fi t separation line 
</p>
<p>(or with three dimensional data separation plane), separating cases and controls as 
</p>
<p>good as possible. Discriminant analysis, classifi cation trees, and neural networks 
</p>
<p>(see Machine Learning in medicine part one, Chap. 17, Discriminant analysis for 
</p>
<p>supervised data, pp 215&ndash;224, Chap. 13, Artifi cial intelligence, Chaps. 12 and 13, 
</p>
<p>pp 145&ndash;165, 2013, and Machine Learning in medicine part three, Chap. 14, Decision 
</p>
<p>trees, pp 137&ndash;150, 2013, Springer Heidelberg Germany, by the same authors as the 
</p>
<p>current chapter) are alternative methods for the purpose, but support vector machines 
</p>
<p>are generally more stable and sensitive, although heuristic studies to indicate when 
</p>
<p>they perform better are missing. Support vector machines are also often used in 
</p>
<p>automatic modeling that computes the ensembled results of several best fi t models 
</p>
<p>(see the Chaps.   64     and   65    )   . 
</p>
<p> This chapter uses the Konstanz information miner (Knime), a free data mining 
</p>
<p>software package developed at the University of Konstanz, and also used in the 
</p>
<p>chaps. 7 and 8.  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> Is support vector machines adequate to classify cases and controls in a cohort of 
</p>
<p>admitted because of sepsis?  
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 3&rdquo; as Chap. 
</p>
<p>13, 2014. </p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_64">http://dx.doi.org/10.1007/978-3-319-15195-3_64</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_65">http://dx.doi.org/10.1007/978-3-319-15195-3_65</a></div>
</div>
<div class="page"><p/>
<p>446
</p>
<p>    Example 
</p>
<p> Two hundred patients were admitted because of sepsis. The laboratory values and 
</p>
<p>the outcome death or alive were registered. We wish to use support vector machines to 
</p>
<p>predict from the laboratory values the outcome, death or alive, including informa-
</p>
<p>tion on the error rate. The data of the fi rst 12 patients are underneath. The entire data 
</p>
<p>fi le is in extras.springer.com. Konstanz information miner (Knime) does not use 
</p>
<p>SPSS fi les, and, so, the fi le has to be transformed into a csv excel fi le (click Save 
</p>
<p>As....in &ldquo;Save as&rdquo; type: replace SPSS Statistics(*sav) with SPSS Statistics(*csv)). 
</p>
<p>For convenience the csv fi le is in extras.springer.com and is entitled &ldquo;svm&rdquo;. 
</p>
<p> Death 1 = yes  Ggt  asat  alat  bili  ureum  creat  c-clear  esr  crp  leucos 
</p>
<p> var1  var2  var3  var4  var5  var6  var7  var8  var9  Var10  var11 
</p>
<p> 0  20  23  34  2  3,4  89  &minus;111  2  2  5 
</p>
<p> 0  14  21  33  3  2  67  &minus;112  7  3  6 
</p>
<p> 0  30  35  32  4  5,6  58  &minus;116  8  4  4 
</p>
<p> 0  35  34  40  4  6  76  &minus;110  6  5  7 
</p>
<p> 0  23  33  22  4  6,1  95  &minus;120  9  6  6 
</p>
<p> 0  26  31  24  3  5,4  78  &minus;132  8  4  8 
</p>
<p> 0  15  29  26  2  5,3  47  &minus;120  12  5  5 
</p>
<p> 0  13  26  24  1  6,3  65  &minus;132  13  6  6 
</p>
<p> 0  26  27  27  4  6  97  &minus;112  14  6  7 
</p>
<p> 0  34  25  13  3  4  67  &minus;125  15  7  6 
</p>
<p> 0  32  26  24  3  3,6  58  &minus;110  13  8  6 
</p>
<p> 0  21  13  15  3  3,6  69  &minus;102  12  2  4 
</p>
<p>  Var 1 death 1 = yes 
</p>
<p> Var 2 gammagt (Var = variable) (U/l) 
</p>
<p> Var 3 asat (U/l) 
</p>
<p> Var 4 alat (U/l) 
</p>
<p> Var 5 bili (mumol/l) 
</p>
<p> Var 6 ureum (mmol/l) 
</p>
<p> Var 7 creatinine (mumol/l) 
</p>
<p> Var 8 creatinine clearance (ml/min) 
</p>
<p> Var 9 esr (erythrocyte sedimentation rate) (mm) 
</p>
<p> Var 10 c-reactive protein (mg/l) 
</p>
<p> Var 11 leucos (&times;10 9 /l) 
</p>
<p>        Knime Data Miner 
</p>
<p> In Google enter the term &ldquo;knime&rdquo;. Click Download and follow instructions. After 
</p>
<p>completing the pretty easy download procedure, open the knime workbench by 
</p>
<p>clicking the knime welcome screen. The center of the screen displays the workfl ow 
</p>
<p>71 Support Vector Machines for Imperfect Nonlinear Data (200 Patients with Sepsis)</p>
<p/>
</div>
<div class="page"><p/>
<p>447
</p>
<p>editor like the canvas in SPSS modeler. It is empty, and can be used to build a stream 
</p>
<p>of nodes, called workfl ow in knime. The node repository is in the left lower angle of 
</p>
<p>the screen, and the nodes can be dragged to the workfl ow editor simply by left- 
</p>
<p>clicking. The nodes are computer tools for data analysis like visualization and sta-
</p>
<p>tistical processes. Node description is in the right upper angle of the screen. Before 
</p>
<p>the nodes can be used they have to be connected with the fi le reader and with one 
</p>
<p>another by arrows drawn again simply by left clicking the small triangles attached 
</p>
<p>to the nodes. Right clicking on the fi le reader enables to confi gure from your com-
</p>
<p>puter a requested data fi le.  
</p>
<p>    Knime Workfl ow 
</p>
<p> A knime workfl ow for the analysis of the above data example will be built, and the 
</p>
<p>fi nal result is shown in the underneath fi gure 
</p>
<p>    
</p>
<p>        File Reader Node 
</p>
<p> In the node repository fi nd the node File Reader. Drag the node to the workfl ow edi-
</p>
<p>tor by left clicking....click Browse....and download from extras.springer.com the csv 
</p>
<p>type Excel fi le entitled &ldquo;svm&rdquo;. You are set for analysis now. By left clicking the 
</p>
<p>node the fi le is displayed. The File Reader has chosen Var 0006 (ureum) as S vari-
</p>
<p>able (dependent). However, we wish to replace it with Var 0001 (death yes = 1)....
</p>
<p>click the column header of Var 0006....mark &ldquo;Don&rsquo;t include column in output&rdquo;....
</p>
<p>click OK....in the column header of Var 0001 leave unmarked &ldquo;Don&rsquo;t include col-
</p>
<p>umn in output&rdquo; click OK. 
</p>
<p> The outcome variable is now rightly the Var 0001 and is indicated with S, the Var 
</p>
<p>0006 has obtained the term &ldquo;SKIP&rdquo; between brackets.  
</p>
<p>File Reader Node</p>
<p/>
</div>
<div class="page"><p/>
<p>448
</p>
<p>    The Nodes X-Partitioner, svm Learner, 
</p>
<p>svm Predictor, X-Aggregator 
</p>
<p> Find the above nodes in the node repository and drag them to the workfl ow editor 
</p>
<p>and connect them with one another according to the above fi gure. Confi gurate and 
</p>
<p>execute all them by right clicking the nodes and the texts &ldquo;Confi gurate&rdquo; and 
</p>
<p>&ldquo;Execute&rdquo;. The red lights under the nodes get, subsequently, yellow and, then, 
</p>
<p>green. The miner has accomplished its task.  
</p>
<p>    Error Rates 
</p>
<p> Right click the X-Aggregator node once more, and then right click Error rates. The 
</p>
<p>underneath table is shown. The svm model is used to make predictions about death 
</p>
<p>or not from the other variables of your fi le. Nine random samples of 25 patients are 
</p>
<p>shown. The error rates are pretty small, and vary from 0 to 12.5 %. We should add 
</p>
<p>that other measures of uncertainty like sensitivity or specifi city are not provided by 
</p>
<p>knime. 
</p>
<p>    
</p>
<p>        Prediction Table 
</p>
<p> Right click the x-aggregator node once more, and then right click Prediction Table. 
</p>
<p>The underneath table is shown. The svm model is used to make predictions about 
</p>
<p>death or not from the other variables of your fi le. 
</p>
<p> The left column gives the outcome values (death yes = 1), the right one gives the 
</p>
<p>predicted values. It can be observed that the two results very well match one another. 
</p>
<p>71 Support Vector Machines for Imperfect Nonlinear Data (200 Patients with Sepsis)</p>
<p/>
</div>
<div class="page"><p/>
<p>449
</p>
<p>    
</p>
<p>        Conclusion 
</p>
<p> The basic aim of support vector machines is to construct the best fi t separation line 
</p>
<p>(or with three dimensional data separation plane), separating cases and controls as 
</p>
<p>good as possible. This chapter uses the Konstanz information miner, a free data 
</p>
<p>mining software package developed at the University of Konstanz, and also used in 
</p>
<p>the chaps. 1 and 2. The example shows that support vector machines is adequate to 
</p>
<p>predict the presence of a disease or not in a cohort of patients at risk of a disease.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of support vector 
</p>
<p>machines is given in Machine in medicine part two, Chap. 14, Support vector 
</p>
<p>machines, pp 155&ndash;162, Springer Heidelberg Germany, from the same authors.    
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>451&copy; Springer International Publishing Switzerland 2015 
T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
Overview, DOI 10.1007/978-3-319-15195-3_72
</p>
<p>    Chapter 72   
</p>
<p> Multiple Response Sets for Visualizing Clinical 
Data Trends (811 Patient Visits) 
</p>
<p>                      General Purpose 
</p>
<p> Multiple response methodology answers multiple qualitative questions about a sin-
</p>
<p>gle group of patients, and uses for the purpose summary tables. The method visual-
</p>
<p>izes trends and similarities in the data, but no statistical test is given.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Can multiple response sets better than traditional frequency tables demonstrate 
</p>
<p>results that could be selected for formal trend tests.  
</p>
<p>    Example 
</p>
<p> An 811 person health questionnaire addressed the reasons for visiting general practi-
</p>
<p>tioners (gps) in 1 month. Nine qualitative questions addressed various aspects of health 
</p>
<p>as primary reasons for visits. SPSS statistical software was used to analyze the data. 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 3&rdquo; as Chap. 
14, 2014. </p>
<p/>
</div>
<div class="page"><p/>
<p>452
</p>
<p> ill  alcohol  weight  tired  cold  family  mental  physical  social  no 
</p>
<p> 0  0  1  1  0  0  0  0  1  0 
</p>
<p> 1  1  1  1  1  1  1  1  1  0 
</p>
<p> 0  0  0  0  0  0  0  0  0  1 
</p>
<p> 1  0  0  1  0  0  0  0  0  0 
</p>
<p> 1  0  0  0  1  1  1  1  0  0 
</p>
<p> 0  0  0  0  0  1  1  1  0  0 
</p>
<p> 0  0  0  0  0  0  0  0  0  1 
</p>
<p> 1  1  0  1  1  1  1  1  1  0 
</p>
<p> 0  0  0  0  0  0  1  0  0  0 
</p>
<p> 1  1  0  1  0  1  1  1  1  0 
</p>
<p> 0  0  0  0  0  0  0  0  0  1 
</p>
<p> 0  0  0  0  0  1  1  1  0  0 
</p>
<p> 1  1  0  1  1  1  0  1  1  0 
</p>
<p> 1  0  0  0  1  0  1  0  0  0 
</p>
<p> 1  0  0  1  0  0  0  1  1  0 
</p>
<p>  ill = ill feeling 
 alcohol = alcohol abuse 
 weight = weight problems 
 tired + tiredness 
 cold = common cold 
 family = family problem 
 mental = mental problem 
 physical = physical problem 
 social = social problem 
 no = no answer 
</p>
<p>    The fi rst 15 patient data are given. The entire data fi le is entitled &ldquo;multiplere-
</p>
<p>sponse&rdquo;, and can be downloaded from extras.springer.com. SPSS statistical soft-
</p>
<p>ware is used for analysis. We will start by the descriptive statistics.
</p>
<p>  Command: 
</p>
<p>  Descriptive Statistics&hellip;.Frequencies&hellip;.Variables: enter the variables between 
</p>
<p>"illfeeling" to "no answer"&hellip;.click Statistics&hellip;.click Sum&hellip;.click Continue&hellip;.click 
</p>
<p>OK.    
</p>
<p> The output is in the underneath 10 tables. It is pretty hard to observe trends 
</p>
<p>across the tables. Also redundant information as given is not helpful for overall 
</p>
<p>conclusion about the relationships between the different questions.
</p>
<p> Illfeeling 
</p>
<p> Frequency  Percent  Valid percent  Cumulative percent 
</p>
<p> Valid  No  420  43,3  51,8  51,8 
</p>
<p> Yes  391  40,3  48,2  100,0 
</p>
<p> Total  811  83,6  100,0 
</p>
<p> Missing  System  159  16,4 
</p>
<p> Total  970  100,0 
</p>
<p>72 Multiple Response Sets for Visualizing Clinical Data Trends (811 Patient Visits)</p>
<p/>
</div>
<div class="page"><p/>
<p>453
</p>
<p> Alcohol 
</p>
<p> Frequency  Percent  Valid percent  Cumulative percent 
</p>
<p> Valid  No  569  58,7  70,2  70,2 
</p>
<p> Yes  242  24,9  29,8  100,0 
</p>
<p> Total  811  83,6  100,0 
</p>
<p> Missing  System  159  16,4 
</p>
<p> Total  970  100,0 
</p>
<p> Weight problem 
</p>
<p> Frequency  Percent  Valid percent  Cumulative percent 
</p>
<p> Valid  No  597  61,5  73,6  73,6 
</p>
<p> Yes  214  22,1  26,4  100,0 
</p>
<p> Total  811  83,6  100,0 
</p>
<p> Missing  System  159  16,4 
</p>
<p> Total  970  100,0 
</p>
<p> Tiredness 
</p>
<p> Frequency  Percent  Valid percent  Cumulative percent 
</p>
<p> Valid  No  511  52,7  63,0  63,0 
</p>
<p> Yes  300  30,9  37,0  100,0 
</p>
<p> Total  811  83,6  100,0 
</p>
<p> Missing  System  159  16,4 
</p>
<p> Total  970  100,0 
</p>
<p> Cold 
</p>
<p> Frequency  Percent  Valid percent  Cumulative percent 
</p>
<p> Valid  No  422  43,5  52,0  52,0 
</p>
<p> Yes  389  40,1  48,0  100,0 
</p>
<p> Total  811  83,6  100,0 
</p>
<p> Missing  System  159  16,4 
</p>
<p> Total  970  100,0 
</p>
<p> Family problem 
</p>
<p> Frequency  Percent  Valid percent  Cumulative percent 
</p>
<p> Valid  No  416  42,9  51,3  51,3 
</p>
<p> Yes  395  40,7  48,7  100,0 
</p>
<p> Total  811  83,6  100,0 
</p>
<p> Missing  System  159  16,4 
</p>
<p> Total  970  100,0 
</p>
<p> Mental problem 
</p>
<p> Frequency  Percent  Valid percent  Cumulative percent 
</p>
<p> Valid  No  410  42,3  50,6  50,6 
</p>
<p> Yes  401  41,3  49,4  100,0 
</p>
<p> Total  811  83,6  100,0 
</p>
<p> Missing  System  159  16,4 
</p>
<p> Total  970  100,0 
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>454
</p>
<p> Physical problem 
</p>
<p> Frequency  Percent  Valid percent  Cumulative percent 
</p>
<p> Valid  No  402  41,4  49,6  49,6 
</p>
<p> Yes  409  42,2  50,4  100,0 
</p>
<p> Total  811  83,6  100,0 
</p>
<p> Missing  System  159  16,4 
</p>
<p> Total  970  100,0 
</p>
<p> Social problem 
</p>
<p> Frequency  Percent  Valid percent  Cumulative percent 
</p>
<p> Valid  No  518  53,4  63,9  63,9 
</p>
<p> Yes  293  30,2  36,1  100,0 
</p>
<p> Total  811  83,6  100,0 
</p>
<p> Missing  System  159  16,4 
</p>
<p> Total  970  100,0 
</p>
<p> No answer 
</p>
<p> Frequency  Percent  Valid percent  Cumulative percent 
</p>
<p> V&auml;lid  ,00  722  74,4  89,0  89,0 
</p>
<p> 1,00  89  9,2  11,0  100,0 
</p>
<p> Total  811  83,6  100,0 
</p>
<p> Missing  System  159  16,4 
</p>
<p> Total  970  100,0 
</p>
<p>   In order to fi nd out more about trends in de data a multiple response analysis will 
</p>
<p>be performed next.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Multiple Response&hellip;.Defi ne Variable Sets&hellip;.move "ill feeling, alcohol, 
</p>
<p>tiredness, cold, family problem, mental problem, physical problem, social problem" 
</p>
<p>from Set Defi nition to Variables in Set&hellip;.Counted Values enter 1&hellip;.Name enter 
</p>
<p>"health"&hellip;.Label enter "health"&hellip;.Multiple Response Set: click Add&hellip;.click 
</p>
<p>Close&hellip;.click Analyze&hellip;.Multiple Response&hellip;.click Frequencies&hellip;.move $health 
</p>
<p>from Multiple Response Sets to Table(s)&hellip;.click OK.    
</p>
<p> The underneath Case Summary table show that of all visitants 25.6 % did not 
</p>
<p>answer any question, here called the missing cases.
</p>
<p> Case summary 
</p>
<p> Cases 
</p>
<p> Valid  Missing  Total 
</p>
<p> N  Percent  N  Percent  N  Percent 
</p>
<p> $health a   722  74,4 %  248  25,6 %  970  100,0 % 
</p>
<p>   a Dichotomy group tabulated at value 1 
</p>
<p>72 Multiple Response Sets for Visualizing Clinical Data Trends (811 Patient Visits)</p>
<p/>
</div>
<div class="page"><p/>
<p>455
</p>
<p> $health frequencies 
</p>
<p> Responses 
</p>
<p> Percent of cases  N  Percent 
</p>
<p> Health a   Illfeeling  391  12,9 %  54,2 % 
</p>
<p> Alcohol  242  8,0 %  33,5 % 
</p>
<p> Weight problem  214  7,1 %  29,6 % 
</p>
<p> Tiredness  300  9,9 %  41,6 % 
</p>
<p> Cold  389  12,8 %  53,9 % 
</p>
<p> Family problem  395  13,0 %  54,7 % 
</p>
<p> Mental problem  401  13,2 %  55,5 % 
</p>
<p> Physical problem  409  13,5 %  56,6 % 
</p>
<p> Social problem  293  9,7 %  40,6 % 
</p>
<p> Total  3,034  100,0 %  420,2 % 
</p>
<p>   a Dichotomy group tabulated at value 1 
</p>
<p>    The letter N gives the numbers of yes-answers per question, &ldquo;Percent of Cases&rdquo; 
</p>
<p>gives the yes-answers per question in those who answered at least once (missing 
</p>
<p>data not taken into account), and Percent gives the percentages of these yes-answers 
</p>
<p>per question 
</p>
<p> The above output shows the number of patients who answered yes to at least one 
</p>
<p>question. Of all visitants 25.6 % did not answer any question, here called the miss-
</p>
<p>ing cases. In the second table the letter N gives the numbers of yes-answers per 
</p>
<p>question, &ldquo;Percent of Cases&rdquo; gives the yes-answers per question in those who 
</p>
<p>answered at least once (missing data not taken into account), and &ldquo;Percent&rdquo; gives 
</p>
<p>the percentages of these yes-answers per question. The gp consultation burden of 
</p>
<p>mental and physical problems was about twice the size of that of alcohol and weight 
</p>
<p>problems. Tiredness and social problems were in-between. In order to assess these 
</p>
<p>data against all visitants, the missing cases have to be analyzed fi rst.
</p>
<p>  Command: 
</p>
<p>  Transform&hellip;.Compute Variable&hellip;.Target Variable: type "none"&hellip;.Numeric 
</p>
<p>Expression: enter "1-max(illfeeling, &hellip;&hellip;.., social problem)" &hellip;.click Type and 
</p>
<p>Label&hellip;.LabelL enter "no answer"&hellip;.click Continue&hellip;.click OK&hellip;.Analyze &hellip;.
</p>
<p>Multiple Response &hellip;.Defi ne Variable sets&hellip;.click Defi ne Multiple Response 
</p>
<p>Sets&hellip;.click $health&hellip;.move "no answer" to Variables in Set&hellip;.click Change&hellip;.
</p>
<p>click Close.    
</p>
<p> The data fi le now contains the novel &ldquo;no answer&rdquo; variable and a novel multiple 
</p>
<p>response variable including the missing cases but the latter is not shown. It is now 
</p>
<p>also possible to produce crosstabs with the different questions as rows and other 
</p>
<p>variables like personal characteristics as columns. In this way the interaction with 
</p>
<p>the personal characteristics can be assessed.
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>456
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Multiple Response&hellip;.Multiple Response Crosstabs&hellip;.Rows: enter 
</p>
<p>$health&hellip;.Columns: enter ed (= level of education)&hellip;.click Defi ne Range&hellip;. 
</p>
<p>Minimum: enter 1&hellip;.Maximum: enter 5&hellip;.Continue&hellip;.Click Options&hellip;.Cell 
</p>
<p>Percentages: click Columns&hellip;. click Continue&hellip;.click OK.   
</p>
<p> $heatth*ed crosstabulatlon 
</p>
<p> Level of education 
</p>
<p> Total 
 No high 
school 
</p>
<p> No high 
school  College  University 
</p>
<p> Completed 
university 
</p>
<p> Health a   Ill feeling  Count  45  101  87  115  43  391 
</p>
<p> % 
within ed 
</p>
<p> 27,6 %  43,3 %  50,9 %  60,2 %  81,1 % 
</p>
<p> Alcohol  Count  18  55  52  83  34  242 
</p>
<p> % 
within ed 
</p>
<p> 11,0 %  23,6 %  30,4 %  43,5 %  64,2 % 
</p>
<p> Weight 
problem 
</p>
<p> Count  13  51  43  82  25  214 
</p>
<p> % 
within ed 
</p>
<p> 8,0 %  21,9 %  25,1 %  42,9 %  47,2 % 
</p>
<p> Tiredness  Count  10  55  71  122  42  300 
</p>
<p> % 
within ed 
</p>
<p> 6,1 %  23,6 %  41,5 %  63,9 %  79,2 % 
</p>
<p> Cold  Count  71  116  85  96  21  389 
</p>
<p> % 
within ed 
</p>
<p> 43,6 %  49,8 %  49,7 %  50,3 %  39,6 % 
</p>
<p> Family 
problem 
</p>
<p> Count  75  118  84  93  25  395 
</p>
<p> % 
within ed 
</p>
<p> 46,0 %  50,6 %  49,1 %  48,7 %  47,2 % 
</p>
<p> Mental 
problem 
</p>
<p> Count  78  112  91  94  26  401 
</p>
<p> % 
within ed 
</p>
<p> 47,9 %  48,1 %  53,2 %  49,2 %  49,1 % 
</p>
<p> Physical 
problem 
</p>
<p> Count  78  123  87  95  26  409 
</p>
<p> % 
within ed 
</p>
<p> 47,9 %  52,8 %  50,9 %  49,7 %  49,1 % 
</p>
<p> Social 
problem 
</p>
<p> Count  11  67  68  111  36  293 
</p>
<p> % 
within ed 
</p>
<p> 6,7 %  28,8 %  39,8 %  58,1 %  67,9 % 
</p>
<p> No 
answer 
</p>
<p> Count  39  29  13  6  2  89 
</p>
<p> % 
within ed 
</p>
<p> 23,9 %  12,4 %  7,6 %  3,1 %  3,8 % 
</p>
<p> Total  Count  163  233  171  191  53  811 
</p>
<p>  Percentages and totals are based on respondents 
  a Dichotomy group tabulated at value 1 
</p>
<p>    The output table gives the results. Various trends are observed. E.g., there is a 
</p>
<p>decreasing trend of patients not answering any question with increased levels of 
</p>
<p>education. Also there is an increasing trend of ill feeling, alcohol problems, weight 
</p>
<p>72 Multiple Response Sets for Visualizing Clinical Data Trends (811 Patient Visits)</p>
<p/>
</div>
<div class="page"><p/>
<p>457
</p>
<p>problems, tiredness and social problems with increased levels of education. If we 
</p>
<p>wish to test whether the increasing trend of tiredness with increased level of educa-
</p>
<p>tion is statistically signifi cant, a formal trend test can be performed.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Descriptive Statistics&hellip;.Crosstabs&hellip;.Rows: enter tiredness&hellip;.Columns: 
</p>
<p>enter level of education&hellip;.click Statistics&hellip;.mark Chi-square&hellip;.click Continue&hellip;.
</p>
<p>click OK.    
</p>
<p> Underneath a formal trend test is given. It tests whether an increasing trend of 
</p>
<p>tiredness is associated with increased levels of education.
</p>
<p> Chi-square tests 
</p>
<p> Value  df  Asymp. sig. (2-sided) 
</p>
<p> Pearson Chi-square  185,824 a   4  ,000 
</p>
<p> Likelihood ratio  202,764  4  ,000 
</p>
<p> Linear-by-linear association  184,979  1  ,000 
</p>
<p> N of valid cases  811 
</p>
<p>   a 0 cells (,0 %) have expected countless than 5. The minimum expected count is 19,61 
</p>
<p>    In the output chi-square tests are given. The linear-by-linear association data 
</p>
<p>show a chi-square value of 184.979 and 1 degree of freedom. This means that a 
</p>
<p>statistically very signifi cant linear trend with p &lt; 0.0001 is in these data. 
</p>
<p> Also interactions and trends of any other health problems with all of the other 
</p>
<p>variables including gender, age, marriage, income, period of constant address or 
</p>
<p>employment can be similarly analyzed.  
</p>
<p>    Conclusion 
</p>
<p> The answers to a set of multiple questions about a single underlying disease / condi-
</p>
<p>tion can be assessed as multiple dimensions of a complex variable. Multiple response 
</p>
<p>methodology is adequate for the purpose. The most important advantage of the 
</p>
<p>multiple response methodology versus traditional frequency table analysis is that it 
</p>
<p>is possible to observe relevant trends and similarities directly from data tables. A 
</p>
<p>disadvantage is that only summaries but no statistical tests are given, but observed 
</p>
<p>trends can, of course, be, additionally, tested statistically with formal trend tests.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of multiple response 
</p>
<p>sets are in Machine Learning in medicine part three, Chap. 11, pp 105&ndash;115, Multiple 
</p>
<p>response sets, Springer Heidelberg Germany, 2013, from the same authors.    
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>459&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_73
</p>
<p>    Chapter 73   
</p>
<p> Protein and DNA Sequence Mining 
</p>
<p>                      General Purpose 
</p>
<p> Sequence similarity searching is a method that can be applied by almost anybody 
</p>
<p>for fi nding similarities between his/her query sequences of amino acids and DNA 
</p>
<p>and the sequences known to be associated with different clinical effects. The latter 
</p>
<p>have been included in database systems like the BLAST (Basic Local Alignment 
</p>
<p>Search Tool) database system from the US National Center of Biotechnology 
</p>
<p>Information (NCBI), and the MOTIF data base system, a joint website from differ-
</p>
<p>ent European and American institutions, and they are available through the internet 
</p>
<p>for the benefi t of individual researchers trying and fi nding a match for novel 
</p>
<p>sequences from their own research. This chapter is to demonstrate that sequence 
</p>
<p>similarity searching is a method that can be applied by almost anybody for fi nding 
</p>
<p>similarities between his/her sequences and the sequences known to be associated 
</p>
<p>with different clinical effects.  
</p>
<p>    Specifi c Scientifi c Question 
</p>
<p> Amino acid  Three-letter abbreviation  One-letter symbol 
</p>
<p> Alanine  Ala  A 
</p>
<p> Arginine  Arg  R 
</p>
<p> Asparagine  Asn  N 
</p>
<p> Aspartic acid  Asp  D 
</p>
<p> Asparagine or aspartic acid  Asx  B 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 3&rdquo; as Chap. 
</p>
<p>15, 2014. 
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>460
</p>
<p> Amino acid  Three-letter abbreviation  One-letter symbol 
</p>
<p> Cysteine  Cys  C 
</p>
<p> Glutamine  Gln  Q 
</p>
<p> Glutamic acid  Glu  E 
</p>
<p> Glutamine or glutamic acid  Glx  Z 
</p>
<p> Glycine  Gly  G 
</p>
<p> Histidine  His  H 
</p>
<p> Isoleucine  Ile  I 
</p>
<p> Leucine  Leu  L 
</p>
<p> Lysine  Lys  K 
</p>
<p> Methionine  Met  M 
</p>
<p> Phenylalanine  Phe  F 
</p>
<p> Proline  Pro  P 
</p>
<p> Serine  Ser  S 
</p>
<p> Threonine  Thr  T 
</p>
<p> Tryptophan  Trp  W 
</p>
<p> Tyrosine  Tyr  Y 
</p>
<p> Valine  Val  V 
</p>
<p>   In this chapter amino acid sequences are analyzed, but nucleic acids sequences can 
</p>
<p>similarly be assessed. The above table gives the one letter abbreviations of amino 
</p>
<p>acids. The specifi c scientifi c question is: can sequence similarity search be applied 
</p>
<p>for fi nding similarities between the sequences found in your own research and the 
</p>
<p>sequences known to be associated with different clinical effects.  
</p>
<p>    Data Base Systems on the Internet 
</p>
<p> The BLAST (  http://blast.ncbi.nlm.nih.gov/Blast.cgi    ) program reports several terms:
</p>
<p>    1.    Max score = best bit score between query sequence and database sequence (the 
</p>
<p>bit score = the standardized score, i.e. the score that is independent of any unit).   
</p>
<p>   2.    Total score = best bit score if some amino acid pairs in the data have been used 
</p>
<p>more often than just once.   
</p>
<p>   3.    Query coverage = percentage of amino acids used in the analysis.   
</p>
<p>   4.    E-value = expected number of large similarity alignment scores.    
</p>
<p>  If the E-value is very small for the score observed, then a chance fi nding can be 
</p>
<p>rejected. The sequences are then really related. An E-value = p-value adjusted for 
</p>
<p>multiple testing = the chance that the association found is a chance fi nding. It indi-
</p>
<p>cates that the match between a novel and already known sequence is closer than 
</p>
<p>73 Protein and DNA Sequence Mining</p>
<p/>
<div class="annotation"><a href="http://blast.ncbi.nlm.nih.gov/Blast.cgi">http://blast.ncbi.nlm.nih.gov/Blast.cgi</a></div>
</div>
<div class="page"><p/>
<p>461
</p>
<p>could happen by chance, and that the novel and known sequence are thus homolo-
</p>
<p>gous (philogenetically from the same ancestor, whatever that means).  
</p>
<p>    Example 1 
</p>
<p> We isolated the following amino acid sequence: serine, isoleucine, lysine, leucine, tryp-
</p>
<p>tophan, proline, proline. The one letter abbreviation code for this sequence is 
</p>
<p>SIKLWPP. The BLAST Search site is explored, while giving the following commands.
</p>
<p>   Open BLAST Search site at appropriate address (Reference 1).  
</p>
<p>  Choose Protein BLAST  
</p>
<p>  Click Enter Sequences and enter the amino acid sequence SIKLWPP  
</p>
<p>  Click BLAST    
</p>
<p> The output tables use the term blast hit which means here a database sequence 
</p>
<p>selected by the provider&rsquo;s software to be largely similar to the unknown sequence, 
</p>
<p>and the term query, which means here an unknown sequence that the investigator 
</p>
<p>has entered for sequence testing against known sequences from the database. The 
</p>
<p>output tables report
</p>
<p>    1.    No putative conserved domains have been detected.   
</p>
<p>   2.    In the Distribution of 100 Blast Hits on the Query sequence all of the Blast Hits 
</p>
<p>have a very low alignment score (&lt;40).   
</p>
<p>   3.    In spite of the low scores their precise alignment values are given next, e.g. the 
</p>
<p>best one has
</p>
<p>   a max score of 21.8,  
</p>
<p>  total score of 21.8,  
</p>
<p>  query coverage of 100 %, and  
</p>
<p>  adjusted p-value of 1956 (not signifi cant).        
</p>
<p> As a contrast search the MOTIF Search site is explored. We command.
</p>
<p>   Open MOTIF Search site at appropriate address (MOTIF Search.   http://www.
</p>
<p>genome.jp/tools/motif    ).  
</p>
<p>  Choose: Searching Protein Sequence Motifs  
</p>
<p>  Click: Enter your query sequence and enter the amino acid sequence SIKLWPP  
</p>
<p>  Select motif libraries: click various databases given  
</p>
<p>  Then click Search.    
</p>
<p> The output table reports: 1 motif found in PROSITE database (found motif 
</p>
<p>PKC_PHOSPHO_SITE; description: protein kinase C phosphorylation site). 
</p>
<p>Obviously, it is worthwhile to search other databases if one does not provide any 
</p>
<p>hits.  
</p>
<p>Example 1</p>
<p/>
<div class="annotation"><a href="http://www.genome.jp/tools/motif">http://www.genome.jp/tools/motif</a></div>
<div class="annotation"><a href="http://www.genome.jp/tools/motif">http://www.genome.jp/tools/motif</a></div>
</div>
<div class="page"><p/>
<p>462
</p>
<p>    Example 2 
</p>
<p> We wish to examine a 12 amino acid sequence that we isolated at our laboratory, use 
</p>
<p>again BLAST. We command.
</p>
<p>   Open BLAST Search site at appropriate address (Reference 1).  
</p>
<p>  Choose Protein BLAST  
</p>
<p>  Click Enter Sequences and enter the amino acid sequence ILVFMCWLVFQC  
</p>
<p>  Click BLAST    
</p>
<p> The output tables report
</p>
<p>    1.    No putative conserved domains have been detected.   
</p>
<p>   2.    In the Distribution of 100 Blast Hits on the Query sequence all of the Blast Hits 
</p>
<p>have a very low alignment score (&lt;40).   
</p>
<p>   3.    In spite of the low scores their precise alignment values are given next. Three of 
</p>
<p>them have a signifi cant alignment score at p &lt; 0.05 with
</p>
<p>   max scores of 31.2,  
</p>
<p>  total scores 31.2,  
</p>
<p>  query cover of around 60 %, and  
</p>
<p>  E-values (adjusted p-values) of 4.1, 4.1, and 4.5.        
</p>
<p> Parts of the novel sequence have been aligned to known sequences of proteins 
</p>
<p>from a streptococcus and a nocardia bacteria and from caenorhabditis, a small soil- 
</p>
<p>dwelling nematode. These fi ndings may not seem clinically very relevant, and may 
</p>
<p>be due to type I errors, with low levels of statistical signifi cance, or material 
</p>
<p>contamination.  
</p>
<p>    Example 3 
</p>
<p> A somewhat larger amino acid sequence (25 letters) is examined using BLAST. We 
</p>
<p>command.
</p>
<p>   Open BLAST Search site at appropriate address (Reference 1).  
</p>
<p>  Choose Protein BLAST  
</p>
<p>  Click Enter Sequences and enter the amino acid sequence 
</p>
<p>SIKLWPPSQTTRLLLVERMANNLST  
</p>
<p>  Click BLAST    
</p>
<p> The output tables report the following.
</p>
<p>    1.    Putative domains have been detected. Specifi c hits regard the WPP superfamily. 
</p>
<p>The WPP domain is a 90 amino acid protein that serves as a transporter protein 
</p>
<p>for other protein in the plant cell from the cell plasma to the nucleus.   
</p>
<p>   2.    In the Distribution of 100 Blast Hits on the Query sequence all of the Blast Hits 
</p>
<p>have a very high alignment score (80&ndash;200 for the fi rst 5 hits, over 50 for the 
</p>
<p>remainder, all of them statistically very signifi cant).   
</p>
<p>73 Protein and DNA Sequence Mining</p>
<p/>
</div>
<div class="page"><p/>
<p>463
</p>
<p>   3.    Precise alignment values are given next. The fi rst 5 hits have the highest 
</p>
<p>scores: with
</p>
<p>   max scores of 83.8,  
</p>
<p>  total scores of 83.8,  
</p>
<p>  Cover queries of 100 %,  
</p>
<p>  p-values of 4 e &minus;17 , which is much smaller than 0.05 (5 %).    
</p>
<p> All of them relate to the WPP superfamily sequence. 
</p>
<p> The next 95 hits produced Max scores and Total scores from 68.9 to 62.1, 
</p>
<p>query coverages from 100 to 96 %. and adjusted p-values from 5 e &minus;12  to 1 e &minus;9 , 
</p>
<p>which is again much smaller than 0.05 (5 %).   
</p>
<p>   4.     We can subsequently browse through the 95 hits to see if anything of interest for 
</p>
<p>our purposes can be found. All of the alignments as found regarded plant pro-
</p>
<p>teins like those of grasses, maize, nightshade and other plants, no alignments 
</p>
<p>with human or veterinary proteins were established.      
</p>
<p>    Example 4 
</p>
<p> A 27 amino acid sequence from a laboratory culture of pseudomonas is examined 
</p>
<p>using BLAST. We command.
</p>
<p>   Open BLAST Search site at appropriate address (Reference 1).  
</p>
<p>  Choose Protein BLAST  
</p>
<p>  Click Enter Sequences and enter the amino acid sequence 
</p>
<p>MTDLNIPHTHAHLVDAFQALGIRAQAL  
</p>
<p>  Click BLAST    
</p>
<p> The output tables report
</p>
<p>    1.    No putative domains have been detected.   
</p>
<p>   2.    The 100 blast hit table shows, however, a very high alignment score for gentami-
</p>
<p>cin acetyl transferase enzyme, recently recognized as being responsible for resis-
</p>
<p>tance of pseudomonas to gentamicin. The ailments values were
</p>
<p>   max score  
</p>
<p>  total score of 85.5,  
</p>
<p>  query coverage of 100 %,  
</p>
<p>  adjusted p-value of 1 e &minus;17 , and so statistically very signifi cant.      
</p>
<p>   3.    In the Distribution of the 99 remaining Blast Hits only 5 other signifi cant align-
</p>
<p>ment were detected with
</p>
<p>   max score and total scores from 38.5 to 32.9,  
</p>
<p>  query coverages 55&ndash;92 %,  
</p>
<p>  adjusted p-values between 0.08 and 4.5 (all of them 5 %).        
</p>
<p> The signifi cant alignments regarded bacterial proteins including the gram nega-
</p>
<p>tive bacterias, rhizobium, xanthomonas, and morganella, and a mite protein. This 
</p>
<p>Example 4</p>
<p/>
</div>
<div class="page"><p/>
<p>464
</p>
<p>may not clinically be very relevant, but our novel sequence was derived from a 
</p>
<p>pseudomonas culture, and we know now that this particular culture contains patho-
</p>
<p>gens very resistant to gentamicin.  
</p>
<p>    Conclusion 
</p>
<p> Sequence similarity searching is a method that can be applied by almost anybody 
</p>
<p>for fi nding similarities between his/her query sequences and the sequences known 
</p>
<p>to be associated with different clinical effects. 
</p>
<p> With sequence similarity searching the use of p-values to distinguish between 
</p>
<p>high and low similarity is relevant. Unlike the BLAST interactive website, the 
</p>
<p>MOTIF interactive website does not give them, which hampers inferences from the 
</p>
<p>alignments to be made.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of protein and DNA 
</p>
<p>sequence mining is given in Machine learning in medicine part two, Chap. 17, 
</p>
<p>pp 171&ndash;185, Protein and DNA sequence mining, Springer Heidelberg Germany 
</p>
<p>2013, from the same authors.    
</p>
<p>73 Protein and DNA Sequence Mining</p>
<p/>
</div>
<div class="page"><p/>
<p>465&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_74
</p>
<p>    Chapter 74   
</p>
<p> Iteration Methods for Crossvalidations 
(150 Patients with Pneumonia)       
</p>
<p>                General Purpose 
</p>
<p> In the Chap.   8     of this book validation of a decision tree model is performed splitting 
</p>
<p>a data fi le into a training and a testing sample. This method performed pretty well 
</p>
<p>with a sensitivity of 90&ndash;100 % and an overall accuracy of 94 %. However, measures 
</p>
<p>of error of predictive models like the above one are based on residual methods, 
</p>
<p>assuming a priori defi ned data distributions, particularly normal distributions. 
</p>
<p>Machine learning data fi le may not meet such assumptions, and distribution free 
</p>
<p>methods of validation, like crossvalidations may be more safe.  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> How does crossvalidation of the data from Chap.   8     perform as compared to the 
</p>
<p>residual method used in the scorer node of the Konstanz information miner (Knime)?  
</p>
<p>    Example 
</p>
<p> The data fi le from Chap.   8     is used once more. Four infl ammatory markers (CRP 
</p>
<p>(C-reactive protein), ESR (erythrocyte sedimentation rate), leucocyte count (leu-
</p>
<p>cos), and fi brinogen) were measured In 150 patients. Based on x-ray chest clinical 
</p>
<p>severity was classifi ed as A (mild infection), B (medium severity), C (severe 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 3&rdquo; as Chap. 
</p>
<p>16, 2014. </p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_8">http://dx.doi.org/10.1007/978-3-319-15195-3_8</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_8">http://dx.doi.org/10.1007/978-3-319-15195-3_8</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_8">http://dx.doi.org/10.1007/978-3-319-15195-3_8</a></div>
</div>
<div class="page"><p/>
<p>466
</p>
<p>infection). A major scientifi c question was to assess what markers were the best 
</p>
<p> predictors of the severity of infection. 
</p>
<p> CRP  leucos  fi brinogen  ESR  x-ray severity 
</p>
<p> 120,00  5,00  11,00  60,00  A 
</p>
<p> 100,00  5,00  11,00  56,00  A 
</p>
<p> 94,00  4,00  11,00  60,00  A 
</p>
<p> 92,00  5,00  11,00  58,00  A 
</p>
<p> 100,00  5,00  11,00  52,00  A 
</p>
<p> 108,00  6,00  17,00  48,00  A 
</p>
<p> 92,00  5,00  14,00  48,00  A 
</p>
<p> 100,00  5,00  11,00  54,00  A 
</p>
<p> 88,00  5,00  11,00  54,00  A 
</p>
<p> 98,00  5,00  8,00  60,00  A 
</p>
<p> 108,00  5,00  11,00  68,00  A 
</p>
<p> 96,00  5,00  11,00  62,00  A 
</p>
<p> 96,00  5,00  8,00  46,00  A 
</p>
<p> 86,00  4,00  8,00  60,00  A 
</p>
<p> 116,00  4,00  11,00  50,00  A 
</p>
<p> 114,00  5,00  17,00  52,00  A 
</p>
<p>  CRP = C-reactive protein (mg/l) 
</p>
<p> leucos = leucyte count (*10 9 /l) 
</p>
<p> fi brinogen = fi brinogen level (mg/l) 
</p>
<p> ESR = erythrocyte sedimentation rate (mm) 
</p>
<p> x-ray severity = x-chest severity pneumonia score (A &ndash; C = mild to severe) 
</p>
<p>    The fi rst 16 patients are in the above table, the entire data fi le is in &ldquo;decisiontree&rdquo; 
</p>
<p>and can be obtained from &ldquo;extras.springer.com&rdquo; on the internet.  
</p>
<p>    Downloading the Knime Data Miner 
</p>
<p> In Google enter the term &ldquo;knime&rdquo;. Click Download and follow instructions. After 
</p>
<p>completing the pretty easy download procedure, open the knime workbench by 
</p>
<p>clicking the knime welcome screen. The center of the screen displays the workfl ow 
</p>
<p>editor like the canvas in SPSS modeler. It is empty, and can be used to build a stream 
</p>
<p>of nodes, called workfl ow in knime. The node repository is in the left lower angle of 
</p>
<p>the screen, and the nodes can be dragged to the workfl ow editor simply by left- 
</p>
<p>clicking. Start by dragging the fi le reader node to the workfl ow. The nodes are com-
</p>
<p>puter tools for data analysis like visualization and statistical processes. Node 
</p>
<p>description is in the right upper angle of the screen. Before the nodes can be used, 
</p>
<p>they have to be connected with the fi le reader node and with one another by arrows 
</p>
<p>drawn again simply by left clicking the small triangles attached to the nodes. Right 
</p>
<p>clicking on the fi le reader node enables to confi gure from your computer a requested 
</p>
<p>data fi le....click Browse....and download from the appropriate folder a csv type 
</p>
<p>Excel fi le. You are set for analysis now. 
</p>
<p>74 Iteration Methods for Crossvalidations (150 Patients with Pneumonia)</p>
<p/>
</div>
<div class="page"><p/>
<p>467
</p>
<p> Note: the above data fi le cannot be read by the fi le reader node as it is an SPSS 
</p>
<p>fi le, and must fi rst be saved as an csv type Excel fi le. For that purpose command in 
</p>
<p>SPSS: click File....click Save as....in &ldquo;Save as&rdquo; type: enter Comma Delimited (*.
</p>
<p>csv)....click Save. For your convenience it is available in extras.springer.com, and is 
</p>
<p>also entitled &ldquo;decisiontree&rdquo;.  
</p>
<p>    Knime Workfl ow 
</p>
<p> A knime workfl ow for the analysis of the above data example is built, and the fi nal 
</p>
<p>result is shown in the underneath fi gure 
</p>
<p>    
</p>
<p>    In the node repository click X-Partitioner, Decision Tree Learner, Decision Tree 
</p>
<p>Predictor and X-Aggregator and drag them to the workfl ow editor. If you have dif-
</p>
<p>fi culty fi nding the nodes (the repository contains hundreds of nodes), you may type 
</p>
<p>their names in the small window at the top of the node repository box, and its icon 
</p>
<p>and name will immediately appear. Connect, by left clicking, all of the nodes with 
</p>
<p>arrows as indicated above....Confi gurate and execute all of the nodes by right click-
</p>
<p>ing the nodes and then the texts &ldquo;Confi gurate&rdquo; and &ldquo;Execute&rdquo;....the red lights will 
</p>
<p>successively turn orange and then green....right click the Decision Tree Predictor 
</p>
<p>again....right click the text &ldquo;View: Decision Tree View&rdquo;. The decision tree comes 
</p>
<p>up, and it is, obviously, identical to the one of Chap.   2    .  
</p>
<p>    Crossvalidation 
</p>
<p> If you, subsequently, right click the Decision Tree Predictor, and then click Classifi ed 
</p>
<p>Data, a table turns up of 15 randomly selected subjects from your test sample. The 
</p>
<p>predicted values are identical to the measured ones. And, so, for this selection the 
</p>
<p>Decision Tree Predictor node performed well. 
</p>
<p>Crossvalidation</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_2">http://dx.doi.org/10.1007/978-3-319-15195-3_2</a></div>
</div>
<div class="page"><p/>
<p>468
</p>
<p>    
</p>
<p>    Next, right click the x-aggregator node, and then click Prediction table. The 
</p>
<p>results of 10 iterative random samples of 15 subjects from your test sample are 
</p>
<p>simultaneously displayed. Obviously, virtually all of the predictions were in agree-
</p>
<p>ment with the measured values. Subsequently, right click the node again, and then 
</p>
<p>click Error rates. 
</p>
<p>    
</p>
<p>    The above table comes up. It shows the error rates of the above 10 iterative ran-
</p>
<p>dom samples. The result is pretty good. Virtually, all of them have 0 or 1 erroneous 
</p>
<p>value. 
</p>
<p> The crossvalidation can also be performed with a novel validation set. For that 
</p>
<p>purpose you need a novel fi le reader node, and the novel validation set has to be 
</p>
<p>confi gured and executed. Furthermore, you need to copy and paste the above 
</p>
<p>74 Iteration Methods for Crossvalidations (150 Patients with Pneumonia)</p>
<p/>
</div>
<div class="page"><p/>
<p>469
</p>
<p>Aggregator node, and you need to connect the output port of the above Decision 
</p>
<p>Tree Predictor node to the input port of Aggregator node.  
</p>
<p>    Conclusion 
</p>
<p> In Chap.   8     of this volume validation of a decision tree model was performed split-
</p>
<p>ting a data fi le into a training and testing sample. This method performed pretty well 
</p>
<p>with an overall accuracy of 94 %. However, the measure of error is based on the 
</p>
<p>normal distribution assumption, and data may not meet this assumption. 
</p>
<p>Crossvalidation is a distribution free method, and may here be a more safe, and less 
</p>
<p>biased approach to validation. 
</p>
<p> It performed very well, with errors mostly 0 and 1 out of 15 cases. We should add 
</p>
<p>that Knime does not provide sensitivity and specifi city measures here.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of validations and 
</p>
<p>crossvalidations is given in:
</p>
<p>  Statistics applied to clinical studies 5th edition, Springer Heidelberg 
</p>
<p>Germany. 
</p>
<p>  Chap. 46, Validating qualitative diagnostic tests, pp 509&ndash;517, 2012,  
</p>
<p>  Chap. 47 Uncertainty of qualitative diagnostic tests, pp 519&ndash;525, 2012,  
</p>
<p>  Chap. 50 Validating quantitative diagnostic tests, pp 545&ndash;552, 2012,  
</p>
<p>  Chap. 51 Summary of validation procedures for diagnostic tests, pp 555&ndash;568, 2012.   
</p>
<p>  Machine learning in medicine part one, Springer Heidelberg Germany. 
</p>
<p>  Chap. 1 Introduction to machine learning, p 5, 2012,  
</p>
<p>  Chap. 3 Optimal scaling: discretization, p 28, 2012,  
</p>
<p>  Chap. 4 Optimal scaling, regularization including ridge, lasso, and elastic net regres-
</p>
<p>sion, p 41, 2012.    
</p>
<p> All of the above publications are from the same authors as the current work.   
</p>
<p>Note</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-3-319-15195-3_8">http://dx.doi.org/10.1007/978-3-319-15195-3_8</a></div>
</div>
<div class="page"><p/>
<p>471&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_75
</p>
<p>Chapter 75
</p>
<p>Testing Parallel-Groups with Different Sample 
Sizes and Variances (5 Parallel-Group Studies)
</p>
<p> General Purpose
</p>
<p>Unpaired t-tests are traditionally used for testing the significance of difference 
</p>
<p>between parallel-groups according to
</p>
<p> 
t value mean mean SD N SD N = -( ) &Ouml; +( )1 2 1 1 2 2/ / /  
</p>
<p>where mean, SD, N are respectively the mean, the standard deviation and the sam-
</p>
<p>ple size of the parallel groups.
</p>
<p>Many calculators on the internet (e.g., the P value calculator-GraphPad) can tell 
</p>
<p>you whether the t-value is significantly smaller than 0.05, and, thus, whether there 
</p>
<p>is a statistically significant difference between the parallel groups.
</p>
<p>E.g., open Google and type p-value calculator for t-test....click Enter....click P 
</p>
<p>value calculator -GraphPad....select P from t....t: enter computed t-value....DF: com-
</p>
<p>pute N1 + N2 -2 and enter the result....click Compute P.
</p>
<p>This procedure assumes that the two parallel groups have equal variances. 
</p>
<p>However in practice this is virtually never entirely true. This chapter is to assess 
</p>
<p>tests accounting the effect of different variances on the estimated p-values.
</p>
<p> Primary Scientific Question
</p>
<p>Two methods for adjustment of different variances and different sample sizes are 
</p>
<p>available, the pooled t-test which assumes that the differences in variances are just 
</p>
<p>residual, and that the two variances are equal, and the Welch&rsquo;s test which assumes 
</p>
<p>This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 3&rdquo; as Chap. 
</p>
<p>17, 2014.</p>
<p/>
</div>
<div class="page"><p/>
<p>472 75 Testing Parallel-Groups with Different Sample Sizes and Variances&hellip;
</p>
<p>that they are due to a real effect, like a difference in treatment effect with comcomi-
</p>
<p>tant difference in spread of the data. How are the results of the two adjustment 
</p>
<p>procedures.
</p>
<p> Examples
</p>
<p>In the underneath table the t- test statistics and p-values of 5 parallel-group studies 
</p>
<p>with differences in the means, standard deviations (SDs) and sample sizes (Ns) are 
</p>
<p>given. In the examples 2, 3, 4, and 5 respectively the Ns, SDs, means, and SDs have 
</p>
<p>been changed as compared to example 1.
</p>
<p>means SDs Ns
</p>
<p>unadjusted adjusted (pooled) Welch&rsquo;s adjust
</p>
<p>t value p value t value p value t value p value
</p>
<p>1. 50/40 5/3 100/200 1.715/0.087 1.811/0.071 1.715/0.088
</p>
<p>2. 10/20 1.715/0.092 1.814/0.080 1.715/0.100
</p>
<p>3. 10/3 0.958/0.339 1.214/0.226 0.958/0.340
</p>
<p>4. 60/40 3.430/0.007 3.662/0.000 3.430/0.001
</p>
<p>5. 6/2 1.581/0.115 1.963/0.051 1.581/0.117
</p>
<p>Open Google and type GraphPad Software QuickCalcs t test calculator....mark: 
</p>
<p>Enter mean, SEM, N....mark: Unpaired test....label: type Group 1....mean: type 
</p>
<p>50....SEM: type 5....N: type 100....label: type Group 2.... mean: type 40....SEM: 
</p>
<p>type 3....N: type 200....click Calculate now.
</p>
<p>In the output an adjusted t-value of 1.811 is given and a p-value of 0.071, slightly 
</p>
<p>better than the unadjusted p-value of 0.087. Next a Welch&rsquo;s t-test will be performed 
</p>
<p>using the same procedure as above, but with Welch&rsquo;s Unpaired t-test marked instead 
</p>
<p>of just Unpaired t-test. The output sheet shows that the p-value is now worse than 
</p>
<p>the unadjusted p-value instead of better.
</p>
<p>In the examples 2&ndash;5 slightly different means, SDs, and Ns were used but, other-
</p>
<p>wise, the data were the same. After computations it can be observed that in all of the 
</p>
<p>examples the adjusted test using pooled variances produced the best p-values. This 
</p>
<p>sometimes lead to a statistically significant effect while the other two test are non- 
</p>
<p>significant, for example with data 5 (p-value = 0.05). The Welch&rsquo;s adjustment pro-
</p>
<p>duced the worst p-value, while the unadjusted produced the best statistics.</p>
<p/>
</div>
<div class="page"><p/>
<p>473Note
</p>
<p> Conclusion
</p>
<p>Two methods for adjustment of different variances and different sample sizes are 
</p>
<p>available, the pooled t-test which assumes that the differences in variances are just 
</p>
<p>residual, and the Welch&rsquo;s test which assumes that they are real differences. From 
</p>
<p>five examples it can be observed that the t-tests using pooled variances consistently 
</p>
<p>produced the best p-values sometimes leading to a statistically significant result in 
</p>
<p>otherwise statistically insignificant data. In contrast, the Welch&rsquo;s adjustment consis-
</p>
<p>tently produced the worst result. The pooled t-test is probably the best option if we 
</p>
<p>have clinical arguments for residual differences in variances, while the Welch&rsquo;s test 
</p>
<p>would be a scientifically better option, if it can be argued that differences in variance 
</p>
<p>were due to real clinical effects. Moreover, the Welch&rsquo;s test would be more in agree-
</p>
<p>ment with the general feature of advanced statistical analyses: tests taking special 
</p>
<p>effects in the data into account are associated with larger p-values (more 
</p>
<p>uncertainties).
</p>
<p> Note
</p>
<p>More background, theoretical and mathematical information of improved t-tests are 
</p>
<p>in Statistics applied to clinical studies, Chap. 2, The analysis of efficacy data, 
</p>
<p>pp 15&ndash;40, Springer Heidelberg Germany 5th edition, 2012, from the same authors.</p>
<p/>
</div>
<div class="page"><p/>
<p>475&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_76
</p>
<p>    Chapter 76   
</p>
<p> Association Rules Between Exposure 
</p>
<p>and Outcome (50 and 60 Patients) 
</p>
<p>                      General Purpose 
</p>
<p>    Traditional analysis of exposure outcome relationships is only sensitive with strong 
</p>
<p>relationships. This chapter is to assess whether association rules, based on condi-
</p>
<p>tional probabilities, may be more sensitive in case of weak relationships.  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> Is association rule analysis better sensitive than regression analysis and paired chi- 
</p>
<p>square tests are for demonstrating signifi cant exposure effects.  
</p>
<p>    Example 
</p>
<p> The proportions observed in a sample are equal to chances or probabilities. If you 
</p>
<p>observe a 40 % proportion of healthy patients, then the chance or probability (P) of 
</p>
<p>being healthy in this group is 40 %. With two variables, e.g., healthy and happy, the 
</p>
<p>symbol &cap; is often used to indicate &ldquo;and&rdquo; (both are present). Underneath a hypothe-
</p>
<p>sized example of 5 patients, with 3 of them having overweight and 2 of them coro-
</p>
<p>nary artery disease (CAD), is given. 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 3&rdquo; as Chap. 
</p>
<p>18, 2014. </p>
<p/>
</div>
<div class="page"><p/>
<p>476
</p>
<p> Patient  overweight (predictor)  coronary artery disease 
</p>
<p> X  Y 
</p>
<p> 1  1  0 
</p>
<p> 2  0  1 
</p>
<p> 3  0  0 
</p>
<p> 4  1  1 
</p>
<p> 5  1  0 
</p>
<p>   Support rule 
</p>
<p>    
Support PX Y= = =&cap; 1 5 0 2/ .
</p>
<p>   
</p>
<p>     Confi dence rule 
</p>
<p>    
Confidence PX Y PY= = [ ] [ ]=&cap; / / / / .1 5 2 5 0 5
</p>
<p>   
</p>
<p>     Lift rule (or lift-up rule) 
</p>
<p>    
Lift PX Y PX PY= &times;[ ] = [ ] &times;[ ]=&cap; / / / / / .1 5 2 5 2 5 1 25
</p>
<p>   
</p>
<p>     Conviction rule 
</p>
<p>    
Conviction PY PX Y PY= &minus;[ ] &minus; =] [ &minus;  [ ] =1 1 1 2 5 1 0 5 1 20/ / / / . .&cap;    
</p>
<p>       I.    The support gives the proportion of patients with both overweight and CAD in 
</p>
<p>the entire population. A support of 0.0 would mean that overweight and CAD 
</p>
<p>are mutually elusive, a support of x.y would mean that the two factors are inde-
</p>
<p>pendent of one another.   
</p>
<p>   II.    The confi dence gives the fraction of patients with both CAD and overweight in 
</p>
<p>those with CAD. This fraction is obviously larger than that in the entire 
</p>
<p> population, because it rose from 0.2 to 0.5.   
</p>
<p>   III.    The lift compares the observed proportion of patients with both overweight and 
</p>
<p>CAD with the expected proportion if CAD and overweight would have occurred 
</p>
<p>independently of one another. Obviously, the observed value is larger than 
</p>
<p>expected, 1.25 versus 1.00, suggesting that overweight does contribute to the 
</p>
<p>risk of CAD.   
</p>
<p>   IV.    Finally, the conviction compares the patients with no-CAD in the entire popula-
</p>
<p>tion with those with both no-CAD and the presence of overweight. The ratio is 
</p>
<p>larger than 1.00, namely 1.20. Obviously, the benefi t of no-CAD is better for 
</p>
<p>the entire population than it is for the subgroup with overweight.    
</p>
<p>76 Association Rules Between Exposure and Outcome (50 and 60 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>477
</p>
<p>  In order to assess whether the computed values, like 0.2 and 1.25, are signifi cantly 
</p>
<p>different from 0.0 to 1.0 confi dence intervals have to be calculated. We will use the 
</p>
<p>McCallum-Layton calculator for proportions, freely available from the Internet 
</p>
<p>(Confi dence interval calculator for proportions.   www.mccallum-layton.co.uk/    ). 
</p>
<p> The calculations will somewhat overestimate the true confi dence intervals, 
</p>
<p>because the true confi dence intervals are here mostly composed of two or more 
</p>
<p>proportions, and this is not taken into account. Therefore, doubling the p-values 
</p>
<p>may be more adequate (Bonferroni adjustment), but with very small p-values we 
</p>
<p>need not worry. 
</p>
<p>    Example One 
</p>
<p> A data set of 50 patients with coronary artery disease or not (1 = yes) and overweight 
</p>
<p>as predictor (1 = yes) is given underneath 
</p>
<p> Patient  Overweight  Coronary artery disease 
</p>
<p> 1  1,00  0,00 
</p>
<p> 2  0,00  1,00 
</p>
<p> 3  0,00  0,00 
</p>
<p> 4  1,00  1,00 
</p>
<p> 5  0,00  0,00 
</p>
<p> 6  1,00  0,00 
</p>
<p> 7  0,00  1,00 
</p>
<p> 8  0,00  0,00 
</p>
<p> 9  1,00  1,00 
</p>
<p> 10  0,00  0,00 
</p>
<p> 11  1,00  0,00 
</p>
<p> 12  0,00  1,00 
</p>
<p> 13  0,00  0,00 
</p>
<p> 14  1,00  1,00 
</p>
<p> 15  0,00  0,00 
</p>
<p>   The fi rst 15 patients are given. The entire data fi le are the Variables A and B of 
</p>
<p>the data fi le entitled &ldquo;associationrule&rdquo;, and are in extras.springer.com. 
</p>
<p> 20/50 of the patients have overweight (predictor), 20/50 have CAD. A paired 
</p>
<p>binary test (McNemar&rsquo;s test) shows no signifi cant difference between the two col-
</p>
<p>umns (p = 1.0). Binary logistic regression with the predictor as independent variable 
</p>
<p>is equally insignifi cant (b = 0.69, p = 0.241). 
</p>
<p> Applying association rules we fi nd a support of 0.2 and confi dence of 0.5. The 
</p>
<p>lift is 1.25 and the conviction is 1.20. The McCallum calculator gives the confi dence 
</p>
<p>Example</p>
<p/>
<div class="annotation"><a href="http://www.mccallum-layton.co.uk/">http://www.mccallum-layton.co.uk/</a></div>
</div>
<div class="page"><p/>
<p>478
</p>
<p>intervals, respectively 10&ndash;34, 36&ndash;64, 110&ndash;145, and 107&ndash;137 %. All of these 95 % 
</p>
<p>confi dence intervals indicate a very signifi cant difference from respectively 0 % 
</p>
<p>(support and confi dence) and 100 % (lift and conviction) with p-values &lt; 0.001 
</p>
<p>(Bonferroni adjusted p &lt; 0.002) Indeed, the predictor overweight had a very signifi -
</p>
<p>cant positive effect on the risk of CAD.  
</p>
<p>    Example Two 
</p>
<p> A data set of 60 patients with coronary artery disease or not (1 = yes) with over-
</p>
<p>weight and &ldquo;being manager&rdquo; as predictors (1 = yes) 
</p>
<p> Patient  Overweight  Manager  Coronary artery disease 
</p>
<p> 1  1,00  1,00  0,00 
</p>
<p> 2  0,00  0,00  1,00 
</p>
<p> 3  1,00  1,00  0,00 
</p>
<p> 4  0,00  1,00  1,00 
</p>
<p> 5  0,00  0,00  0,00 
</p>
<p> 6  1,00  1,00  1,00 
</p>
<p> 7  1,00  1,00  0,00 
</p>
<p> 8  0,00  0,00  1,00 
</p>
<p> 9  1,00  1,00  0,00 
</p>
<p> 10  0,00  1,00  1,00 
</p>
<p> 11  0,00  0,00  0,00 
</p>
<p> 12  1,00  1,00  1,00 
</p>
<p> 13  1,00  1,00  0,00 
</p>
<p> 14  0,00  0,00  1,00 
</p>
<p> 15  1,00  1,00  0,00 
</p>
<p>   The fi rst 15 patients are given. The entire data fi le are the variables C, D, and E 
</p>
<p>of the data fi le entitled &ldquo;associationrule&rdquo;, and is in extras.springer.com. 
</p>
<p> Instead of a single x &ndash;variable now two of them are included. 30/60 of the patients 
</p>
<p>have overweight, 40/60 are manager, and 30/60 have CAD. A paired binary test 
</p>
<p>(Cochran&rsquo;s test, see note section) shows no signifi cant difference between the three 
</p>
<p>columns (p = 0.082). Binary logistic regression with the two predictors as indepen-
</p>
<p>dent variables is equally insignifi cant (b-values are &ndash; 21.9 and 21.2, p-values are 
</p>
<p>0.99 and 0.99). 
</p>
<p> Applying association rules we fi nd a support of 0.1666 and confi dence of 0.333. 
</p>
<p>The lift is 2.0 and the conviction is 1.25. The McCallum calculator gives the confi -
</p>
<p>dence intervals. Expressed as percentages they are respectively, 8&ndash;29, 22&ndash;47, and 
</p>
<p>159&ndash;270 and 108&ndash;136 %. All of these 95 % confi dence intervals indicate a very 
</p>
<p>signifi cant difference from respectively 0 % (support and confi dence) and 100 % 
</p>
<p>76 Association Rules Between Exposure and Outcome (50 and 60 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>479
</p>
<p>(lift and conviction) with p-values &lt;0.001 (Bonferroni adjusted p &lt; 0.002 or &lt; 0.003). 
</p>
<p>Indeed, the predictors overweight and being manager had a statistically very signifi -
</p>
<p>cant effect on the risk of CAD.   
</p>
<p>    Conclusion 
</p>
<p> Association rule analysis is more sensitive than regression analysis and paired chi- 
</p>
<p>square tests, and is able to demonstrate signifi cant predictor effects, when the other 
</p>
<p>methods are not. It can also include multiple variables and very large datasets and is 
</p>
<p>a welcome methodology for clinical predictor research.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of association rules are 
</p>
<p>in Machine learning in medicine part two, Chap. 11, pp 105&ndash;113, Springer 
</p>
<p>Heidelberg Germany, 2013, from the same authors. Cochran&rsquo;s test is explained in 
</p>
<p>SPSS for starters part one, Chap. 14, pp 51&ndash;53, Springer Heidelberg Germany, 
</p>
<p>2010, from the same authors.    
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>481&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_77
</p>
<p>    Chapter 77   
</p>
<p> Confi dence Intervals for Proportions 
</p>
<p>and Differences in Proportions 
</p>
<p>(100 and 75 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> Proportions, fractions, percentages, risks, hazards are all synonymous terms to indi-
</p>
<p>cate what part of a population had events like death, illness, complications etc. 
</p>
<p>Instead of p-values, confi dence intervals are often calculated. If you obtained many 
</p>
<p>samples from the same population, 95 % of them would have their mean results 
</p>
<p>between the 95 % confi dence intervals. And, likewise, samples from the same popu-
</p>
<p>lation with their proportions outside the 95 % confi dence intervals means that they 
</p>
<p>are signifi cantly different from the population with a probability of 5 % (p &lt; 0.05). 
</p>
<p>This chapter is to assess how confi dence intervals can be computed.  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> P-values give the type I error, otherwise called the chance of fi nding a difference 
</p>
<p>where there is none. Confi dence intervals tell you the same, but, in addition, they 
</p>
<p>give the range in which the true outcome value lies, and the direction and strength 
</p>
<p>of it. Are confi dence intervals more relevant for exploratory studies than p-values, 
</p>
<p>because of the additional information provided.  
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 3&rdquo; as Chap. 
</p>
<p>19, 2014. </p>
<p/>
</div>
<div class="page"><p/>
<p>482
</p>
<p>    Example 
</p>
<p> If in two parallel groups of respectively 100 and 75 patients the numbers of patients 
</p>
<p>with an event are 75 and 50, according to a z-test or chi-square test (see Statistics 
</p>
<p>Applied to Clinical Studies 5th edition, Chap. 3, The analysis of safety data, 
</p>
<p>pp 41&ndash;60, 2012, Springer Heidelberg Germany, from the same authors), then the 
</p>
<p>p-value of difference will be 0.23. This means that we have 23 % chance of a type 
</p>
<p>one error, and that this chance is far too large to be statistically signifi cant (p &gt; 0.05). 
</p>
<p> In the two above groups the proportions are respectively 75/100 = 0.750 and 
</p>
<p>50/75 = 0.667. The standard errors of these proportions can be calculated from the 
</p>
<p>equation
</p>
<p>  
standard errors p p n= &plusmn;&radic; &minus;( ) &radic;( )1 /
</p>
<p>   
</p>
<p>where p = proportion and n = sample size.
</p>
<p>  
95 1 96 1% . /confidence intervals p p n=&plusmn; &radic; &minus;( ) &radic;( )
</p>
<p>   
</p>
<p>If you have little affi nity with computations, then plenty of calculators on the inter-
</p>
<p>net are helpful. 
</p>
<p>    Confi dence Intervals of Proportions 
</p>
<p> We will use the free &ldquo;Matrix Software&rdquo;. Open Google and type Standard Error (SE) 
</p>
<p>of Sample Proportion Calculator-Binomial Standard Deviation....click Enter....click 
</p>
<p>Matrix Software....in Calculate SE Sample Proportion of Standard deviation type 
</p>
<p>0.75 for Proportion of successes (p)....type 100 for Number of Observations (n)....
</p>
<p>click Calculate....
</p>
<p>  
</p>
<p>The binomialSE of theSample proportion
</p>
<p>The co
</p>
<p>= &plusmn; 0 04330127
95
</p>
<p>. ....
</p>
<p>% nnfidence interval of this proportion = &plusmn; &times;
= &plusmn;
</p>
<p>1 96 0 04330127
</p>
<p>0 0848
</p>
<p>. .
</p>
<p>. 77
</p>
<p>0 66513 0 83487= between and. .    
</p>
<p>Similarly the 95 % confi dence interval of the data from group 2 can be calculated.  
</p>
<p>77 Confi dence Intervals for Proportions and Differences in Proportions&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>483
</p>
<p>    Confi dence Intervals of Differences in Proportions 
</p>
<p> In order to calculate the confi dence interval of the differences between the above 
</p>
<p>two proportions, we will use the free Vassarstats. Open Google en type   http://vas-
</p>
<p>sarstats.net/prop2_ind.html....click     enter....select The Confi dence Interval for the 
</p>
<p>Difference Between Two Independent Proportions....Larger Proportion: k a  (number 
</p>
<p>of observations with event) = : type 75....n a  (total number of observations) = : type 
</p>
<p>100....click Calculate. 
</p>
<p>    
</p>
<p>    The output is given above. p a  an p b  are the proportions, p a  &ndash; p b  the difference. The 
</p>
<p>95 % confi dence interval is between &minus;0.0505 and 0.2183. 
</p>
<p> Proportions are yes/no data, e.g., a proportion of 75 subjects out of 100 had an 
</p>
<p>event. The normal distribution is used for the calculation of the p-values and confi -
</p>
<p>dence intervals. In order to test yes/no data with a normal distribution, a continuity 
</p>
<p>correction can be used to improve the quality of the analysis. In the example given 
</p>
<p>the 75/100 in your sample indicates that the real event rate in your entire population 
</p>
<p>of, e.g., 1,000 may be between 745 and 755/1,000. Because 745/1,000 is, of course, 
</p>
<p>smaller than 750/1,000, it would make sense to use the proportion 745/1,000 for the 
</p>
<p>calculation of the confi dence interval instead of 750/1,000. This procedure is called 
</p>
<p>the continuity correction, and as shown above it produces somewhat wider confi -
</p>
<p>dence intervals, and, thus, more uncertainty in your data. Unfortunately, higher 
</p>
<p>quality is often associated with larger levels of uncertainty.   
</p>
<p> Example</p>
<p/>
<div class="annotation"><a href="http://vassarstats.net/prop2_ind.html....click">http://vassarstats.net/prop2_ind.html....click</a></div>
<div class="annotation"><a href="http://vassarstats.net/prop2_ind.html....click">http://vassarstats.net/prop2_ind.html....click</a></div>
</div>
<div class="page"><p/>
<p>484
</p>
<p>    Conclusion 
</p>
<p> Proportions are used to indicate what part of a population had events. Instead of 
</p>
<p>p-values to tell you whether your observed proportion is statistically signifi cantly 
</p>
<p>different from a proportion of 0.0, 95 % confi dence intervals are often calculated. If 
</p>
<p>you obtained many samples from the same population, 95 % of them would have 
</p>
<p>their result between the 95 % confi dence intervals. And, likewise, samples from the 
</p>
<p>same population having their proportions outside the 95 % confi dence intervals 
</p>
<p>means that they are signifi cantly different from the population with a probability of 
</p>
<p>5 % (p &lt; 0.05). P-values give the type I error, otherwise called the chance of fi nding 
</p>
<p>a difference where there is none, or the chance of erroneously rejecting the null- 
</p>
<p>hypothesis. Confi dence intervals tell you the same, but, in addition, they give you 
</p>
<p>the range in which the true outcome value lies, and the direction and strength of it. 
</p>
<p>Particularly, for data mining of exploratory studies the issue of null-hypothesis test-
</p>
<p>ing with p-values is generally less important than information on the range in which 
</p>
<p>the true outcome value lies, and the direction and strength of it.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of proportions and 
</p>
<p>their confi dence intervals is given in Statistics applied to clinical studies 5th edition, 
</p>
<p>Chap. 3, The analysis of safety data, pp 41&ndash;60, 2012, Springer Heidelberg Germany, 
</p>
<p>from the same authors.    
</p>
<p>77 Confi dence Intervals for Proportions and Differences in Proportions&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>485&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_78
</p>
<p>    Chapter 78   
</p>
<p> Ratio Statistics for Effi cacy Analysis of New 
Drugs (50 Patients) 
</p>
<p>                      General Purpose 
</p>
<p> Treatment effi cacies are often assessed as differences from baseline. However, bet-
</p>
<p>ter treatment effi cacies may be observed in patients with high baseline-values than 
</p>
<p>in those with low ones. This was, e.g., the case in the Progress study, a parallel- 
</p>
<p>group study of pravastatin versus placebo (see Statistics applied to clinical studies 
</p>
<p>5th edition, Chap. 17, Logistic and Cox regression, Markov models, and Laplace 
</p>
<p>transformations, pp 199&ndash;218, Springer Heidelberg Germany, 2012, from the same 
</p>
<p>authors). This chapter assesses the performance of ratio statistics for that purpose.  
</p>
<p>    Primary Scientifi c Question 
</p>
<p> The differences of treatment effi cacy and baseline may be the best fi t test statistic, if 
</p>
<p>the treatment effi cacies are independent of baseline. However, if not, then ratios of 
</p>
<p>the two may fi t the data better.  
</p>
<p>    Example 
</p>
<p> A 50-patient 5-group parallel-group study was performed with fi ve different 
</p>
<p>cholesterol- lowering compounds. The fi rst 12 patients of the data fi le is underneath. 
</p>
<p>The entire data fi le is entitled &ldquo;ratiostatistics&rdquo; and is in extra.springer.com. 
</p>
<p> This chapter was previously published in &ldquo;Machine learning in medicine-cookbook 2&rdquo; as Chap. 
</p>
<p>20, 2014. </p>
<p/>
</div>
<div class="page"><p/>
<p>486
</p>
<p> Variable 
</p>
<p> 1  2  3  4 
</p>
<p> Baseline cholesterol 
</p>
<p>(mmol/l) 
</p>
<p> Treatment cholesterol 
</p>
<p>(mmol/l) 
</p>
<p> Treatment 
</p>
<p>group no. 
</p>
<p> Baseline minus treatment 
</p>
<p>cholesterol level (mmol/l) 
</p>
<p> 6.10  5.20  1.00  .90 
</p>
<p> 7.00  7.90  1.00  &minus;.90 
</p>
<p> 8.20  3.90  1.00  4.30 
</p>
<p> 7.60  4.70  1.00  2.90 
</p>
<p> 6.50  5.30  1.00  1.20 
</p>
<p> 8.40  5.40  1.00  3.00 
</p>
<p> 6.90  4.20  1.00  2.70 
</p>
<p> 6.70  6.10  1.00  .60 
</p>
<p> 7.40  3.80  1.00  3.60 
</p>
<p> 5.80  6.30  1.00  &minus;.50 
</p>
<p> 6.20  4.30  2.00  1.90 
</p>
<p> 7.10  6.80  2.00  .30 
</p>
<p>   Start by opening the above data fi le in SPSS statistical software.
</p>
<p>  Command: 
</p>
<p>  Graphs&hellip;.Legacy Dialogs&hellip;..Error Bar&hellip;.mark Summaries of groups of cases&hellip;.
</p>
<p>click Defi ne&hellip;.Variable: enter &ldquo;baseline minus treatment&rdquo;&hellip;.Category Axis: enter 
</p>
<p>Treatment group&hellip;.Confi dence interval for mean: Level enter 95 %....click OK.    
</p>
<p> The underneath graph shows that all of the treatments were excellent and signifi -
</p>
<p>cantly lowered cholesterol levels as shown by the 95 % confi dence intervals. T-tests 
</p>
<p>are not needed here. 
</p>
<p>78 Ratio Statistics for Effi cacy Analysis of New Drugs (50 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>487
</p>
<p>  
</p>
<p>5,00
</p>
<p>4,00
</p>
<p>3,00
</p>
<p>2,00
</p>
<p>1,00
</p>
<p>,00
</p>
<p>1,00 2,00 3,00
</p>
<p>treatmentmodality
</p>
<p>9
5
</p>
<p>%
 C
</p>
<p>l 
b
</p>
<p>a
s
</p>
<p>e
li
</p>
<p>n
e
</p>
<p>m
in
</p>
<p>tr
e
</p>
<p>a
t
</p>
<p>4,00 5,00
</p>
<p>  
</p>
<p>    A one-way ANOVA (treatment modality as predictor and &ldquo;baseline minus treat-
</p>
<p>ment&rdquo; as outcome) will be performed to assess whether any of the treatments sig-
</p>
<p>nifi cantly outperformed the others.
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Compare means&hellip;.One-Way ANOVA&hellip;.Dependent List: enter &ldquo;base-
</p>
<p>line minus treatment&rdquo;&hellip;.Factor: enter Treatment group&hellip;.click OK.   
</p>
<p> ANOVA 
</p>
<p> Baselinemintreat 
</p>
<p> Sum of squares  df  Mean square  F  Sig. 
</p>
<p> Between groups  10,603  4  2,651  ,886  ,480 
</p>
<p> Within groups  134,681  45  2,993 
</p>
<p> Total  145,284  49 
</p>
<p>   According the above table the differences between the different treatment were 
</p>
<p>statistically insignifi cant. And, so, according to the above analysis all treatments 
</p>
<p>were excellent and no signifi cance difference between any of the groups were 
</p>
<p>observed. Next, we will try and fi nd out whether ratio statistics can make additional 
</p>
<p>observations.
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>488
</p>
<p>  Command: 
</p>
<p>  Analyze&hellip;.Descriptive Statistics&hellip;.Ratio&hellip;.Numerator: enter &ldquo;treatment&rdquo;&hellip;. 
</p>
<p>Denominator: enter &ldquo;baseline&rdquo;&hellip;.Group Variable: enter &ldquo;treatmentmodality 
</p>
<p> (treatment group)&rdquo;&hellip;.click Statistics&hellip;.mark Median&hellip;.mark COD (coeffi cient of 
</p>
<p>dispersion)&hellip;.Concentration Index: Low Proportion: type 0.8&hellip;.High Proportion: 
</p>
<p>type 1.2&hellip;.click Add&hellip;.Percent of Median: enter 20&hellip;.click Add&hellip;.click 
</p>
<p>Continue&hellip;.click OK.    
</p>
<p> The underneath table is shown.
</p>
<p> Ratio statistics for treatment/baseline 
</p>
<p> Group  Median 
</p>
<p> Coeffi cient 
</p>
<p>of dispersion 
</p>
<p> Coeffi cient of concentration 
</p>
<p> Percent between 0.8 and 
</p>
<p>1.2 inclusive (%) 
</p>
<p> Within 20 % of median 
</p>
<p>inclusive (%) 
</p>
<p> 1.00  .729  .265  50.0  50.0 
</p>
<p> 2.00  .597  .264  22.2  44.4 
</p>
<p> 3.00  .663  .269  36.4  54.5 
</p>
<p> 4.00  .741  .263  50.0  50.0 
</p>
<p> 5.00  .733  .267  50.0  50.0 
</p>
<p> Overall  .657  .282  42.0  38.0 
</p>
<p>   A problem with ratios is, that they usually suffer from overdispersion, and, there-
</p>
<p>fore, the spread in the data must be assessed differently from that of normal distribu-
</p>
<p>tions. First medians are applied, which is not the mean value but the values in the 
</p>
<p>middle of all values. Assessment of spread is then estimated with
</p>
<p>    (1)    the coeffi cient of dispersion,   
</p>
<p>   (2)    the percentual coeffi cient of concentration (all ratios within 20 % of the median 
</p>
<p>are included),   
</p>
<p>   (3)    the interval coeffi cient of concentration (all ratios between the ratio 0.8*median 
</p>
<p>and 1.2*median are included (* = symbol of multiplication)).     
</p>
<p> The coeffi cients (2) and (3) are not the same, if the distribution of the ratios are 
</p>
<p>very skewed. 
</p>
<p> The above table shows the following. 
</p>
<p> Treatment 2 (Group 2) performs best with 60 % reduction of cholesterol after 
</p>
<p>treatment, treatment 4 performs worst with only 74 % reduction of cholesterol after 
</p>
<p>treatment. The coeffi cient (1) is a general measure of variability of the ratios and the 
</p>
<p>coeffi cient (3) shows the same but is more easy to interpret: around 50 % of the 
</p>
<p>individual ratios are within 20 % distance from the median ratio. The coeffi cient (2) 
</p>
<p>gives the % of individual ratios between the interval of 0.8 and 1.2 * median ratio. 
</p>
<p>Particularly, groups 2 and 3 have small coeffi cients indicating little concentration of 
</p>
<p>the individual ratios here. Group 2 may produce the best median ratio, but is also 
</p>
<p>least concentrated, and is thus more uncertain than, e.g., groups 1, 4, 5. 
</p>
<p> It would make sense to conclude from these observations that treatment group 1 
</p>
<p>with more certainty is a better treatment choice than treatment group 2.  
</p>
<p>78 Ratio Statistics for Effi cacy Analysis of New Drugs (50 Patients)</p>
<p/>
</div>
<div class="page"><p/>
<p>489
</p>
<p>    Conclusion 
</p>
<p> Treatment effi cacies are often assessed as differences from baseline. However, bet-
</p>
<p>ter treatment effi cacies may be observed in patients with high baseline-values than 
</p>
<p>in those with low ones. The differences of treatment effi cacy and baseline may be 
</p>
<p>the best fi t test statistic, if the treatment effi cacies are independent of baseline. 
</p>
<p>However, if not, then ratios of the two may fi t the data better, and allow for relevant 
</p>
<p>additional conclusions.  
</p>
<p>    Note 
</p>
<p> More background, theoretical and mathematical information of treatment effi ca-
</p>
<p>cies that are not independent of baseline is given in Statistics applied to clinical 
</p>
<p>studies 5th edition, Chap. 17, Logistic and Cox regression, Markov models, and 
</p>
<p>Laplace transformations, pp 199&ndash;218, Springer Heidelberg Germany, 2012, from 
</p>
<p>the same authors.    
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>491&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_79
</p>
<p>Chapter 79
</p>
<p>Fifth Order Polynomes of Circadian Rhythms 
</p>
<p>(1 Patient with Hypertension)
</p>
<p> General Purpose
</p>
<p>Ambulatory blood pressure measurements and other circadian phenomena are  
</p>
<p>traditionally analyzed using mean values of arbitrarily separated daytime hours.  
</p>
<p>The poor reproducibility of these mean values undermines the validity of this  
</p>
<p>diagnostic tool. In 1998 our group demonstrated that polynomial regression lines of 
</p>
<p>the 4th to 7th order generally provided adequate reliability to describe the best fit 
</p>
<p>circadian sinusoidal patterns of ambulatory blood pressure measurements (Van de 
</p>
<p>Luit et al., Eur J Intern Med 1998; 9: 99&ndash;103 and 251&ndash;256).
</p>
<p>We should add that the terms multinomial and polynomial are synonymous. 
</p>
<p>However, in statistics terminology is notoriously confusing, and multinomial analyses 
</p>
<p>are often, though not always, used to indicate logistic regression models with multiple 
</p>
<p>outcome categories. In contrast, polynomial regression analyses are often used to 
</p>
<p>name the extensions of simple linear regression models with multiple order instead 
</p>
<p>of first order relationships between the x and y values (Chap. 16, Curvilinear regres-
</p>
<p>sion, pp 187&ndash;198, in: Statistics applied to clinical studies 5th edition, Springer 
</p>
<p>Heidelberg Germany 2012, from the same authors as the current work). Underneath 
</p>
<p>polynomial regression equations of the first to fifth order are given with y as depen-
</p>
<p>dent and x as independent variables.
</p>
<p>y = a + bx first order (linear) relationship
</p>
<p>y = a + bx + cx2 second order (parabolic) relationship
</p>
<p>y = a + bx + cx2 + dx3 third order (hyperbolic) relationship
</p>
<p>y = a + bx + cx2 + dx3 + ex4 fourth order (sinusoidal) relationship
</p>
<p>y = a + bx + cx2 + dx3 + ex4 + fx5 fifth order relationship
</p>
<p>This chapter is to assess whether this method can readily visualize circadian  
</p>
<p>patterns of blood pressure in individual patients with hypertension, and, thus, be 
</p>
<p>helpful for making a precise diagnosis of the type of hypertension, like borderline, 
</p>
<p>diastolic, systolic, white coat, no dipper hypertension.</p>
<p/>
</div>
<div class="page"><p/>
<p>492
</p>
<p> Primary Scientific Question
</p>
<p>Can 5th order polynomes readily visualize the ambulatory blood pressure pattern of 
</p>
<p>individual patients?
</p>
<p> Example
</p>
<p>In an untreated patient with mild hypertension ambulatory blood pressure measure-
</p>
<p>ment was performed using a light weight portable equipment (Space Lab Medical 
</p>
<p>Inc, Redmond WA) every 30 min for 24 h. The first 10 measurements are under-
</p>
<p>neath, the entire data file is entitled polynomials and is in extras.springer.com.
</p>
<p>Blood pressure mm Hg Time (30 min intervals)
</p>
<p>205,00 1,00
</p>
<p>185,00 2,00
</p>
<p>191,00 3,00
</p>
<p>158,00 4,00
</p>
<p>198,00 5,00
</p>
<p>135,00 6,00
</p>
<p>221,00 7,00
</p>
<p>170,00 8,00
</p>
<p>197,00 9,00
</p>
<p>172,00 10,00
</p>
<p>188,00 11,00
</p>
<p>173,00 12,00
</p>
<p>SPSS statistical software will be used for polynomial modeling of these data. 
</p>
<p>Open the data file in SPSS.
</p>
<p>Command:
</p>
<p>Analyze....General Linear Model....Univariate....Dependent: enter y (mm Hg).... 
</p>
<p>Covariate(s): enter x (min)....click: Options....mark: Parameter Estimates....click 
</p>
<p>Continue....click Paste....in "/Design = x."replace x with a 5th order polynomial 
</p>
<p>equation tail (* is sign of multiplication)
</p>
<p>x x*x x*x*x x*x*x*x x*x*x*x*x
</p>
<p>....then click the green triangle in the upper graph row of your screen.
</p>
<p>79 Fifth Order Polynomes of Circadian Rhythms (1 Patient with Hypertension)</p>
<p/>
</div>
<div class="page"><p/>
<p>493
</p>
<p>The underneath table is in the output sheets, and gives you the partial regression 
</p>
<p>coefficients (B values) of the 5th order polynomial with blood pressure as outcome 
</p>
<p>and with time as independent variable (&minus;7,135E-6 indicates &minus;0.000007135, which 
</p>
<p>is a pretty small B value). However, in the equation it will have to be multiplied with 
</p>
<p>x5, and a large term will result even so.
</p>
<p>Parameter estimates
</p>
<p>Dependent variable: y
</p>
<p>Parameter B Std. error t Sig.
</p>
<p>95 % confidence interval
</p>
<p>Lower bound Upper bound
</p>
<p>Intercept 206,653 17,511 11,801 ,000 171,426 241,881
</p>
<p>x &minus;9,112 6,336 &minus;1,438 ,157 &minus;21,858 3,634
</p>
<p>x*x ,966 ,710 1,359 ,181 &minus;,463 2,395
</p>
<p>x*x*x &minus;,047 ,033 &minus;1,437 ,157 &minus;,114 ,019
</p>
<p>x*x*x*x ,001 ,001 1,471 ,148 ,000 ,002
</p>
<p>x*x*x*x*x &minus;7,135E-6 4,948E-6 &minus;1,442 ,156 &minus;1,709E-5 2,819E-6
</p>
<p>Parameter estimates
</p>
<p>Dependent variable:yy
</p>
<p>Parameter B Std. error t Sig.
</p>
<p>95 % confidence interval
</p>
<p>Lower bound Upper bound
</p>
<p>Intercept 170,284 11,120 15,314 ,000 147,915 192,654
</p>
<p>x &minus;7,034 4,023 &minus;1,748 ,087 &minus;15,127 1,060
</p>
<p>x*x ,624 ,451 1,384 ,173 &minus;,283 1,532
</p>
<p>x*x*x &minus;,027 ,021 &minus;1,293 ,202 &minus;,069 ,015
</p>
<p>x*x*x*x ,001 ,000 1,274 ,209 ,000 ,001
</p>
<p>x*x*x*x*x &minus;3,951E-6 3,142E-6 &minus;1,257 ,215 &minus;1,027E-5 2,370E-6
</p>
<p>The entire equations can be written from the above B values:
</p>
<p> 
y x x x x x= &minus; + &minus; + +206 653 9 112 0 966 0 47 0 001 0 0000071352 3 4 5. , . . . .
</p>
<p> 
</p>
<p>This equation is entered in the polynomial grapher of David Wees available on the 
</p>
<p>internet at &ldquo;davidwees.com/polygrapher/&rdquo;, and the underneath graph is drawn. This 
</p>
<p>graph is speculative as none of the x terms is statistically significant. Yet, the actual 
</p>
<p>data have definite patterns with higher values at daytime and lower ones at night. 
</p>
<p>Sometimes even better fit curve are obtained by taking higher order polynomes like 
</p>
<p>5th order polynomes as previously tested by us (see the above section General 
</p>
<p>Purpose). We should add that in spite of the insignificant p-values in the above 
</p>
<p>tables the two polynomes are not meaningless. The first one suggests some white 
</p>
<p> Example</p>
<p/>
</div>
<div class="page"><p/>
<p>494
</p>
<p>coat effect, the second one suggests normotension and a normal dipping pattern. 
</p>
<p>With machine learning meaningful visualizations can sometimes be produced of 
</p>
<p>your data, even if statistics are pretty meaningless.
</p>
<p>240,00
</p>
<p>220,00
</p>
<p>200,00
</p>
<p>y
</p>
<p>x
</p>
<p>180,00
</p>
<p>160,00
</p>
<p>140,00
</p>
<p>120,00
</p>
<p>,00 10,00 20,00 30,00 40,00 50,00 60,00
</p>
<p> 
</p>
<p> 
</p>
<p>Twenty-four hour ABPM recording (30 min measures) of untreated subject with 
</p>
<p>hypertension and 5th order polynome (suggesting some white coat effect)
</p>
<p>79 Fifth Order Polynomes of Circadian Rhythms (1 Patient with Hypertension)</p>
<p/>
</div>
<div class="page"><p/>
<p>495
</p>
<p>180,00
</p>
<p>170,00
</p>
<p>160,00
</p>
<p>150,00
</p>
<p>140,00
</p>
<p>130,00
</p>
<p>120,00
</p>
<p>110,00
</p>
<p>,00 10,00
</p>
<p>x
</p>
<p>y
y
</p>
<p>20,00 30,00 40,00 50,00 60,00
</p>
<p> 
</p>
<p> 
</p>
<p>Twenty-four hour ABPM recording (30 min measures) of the above subject 
</p>
<p>treated and 5th order polynome (suggesting normotension and a normal dipping 
</p>
<p>pattern).
</p>
<p> Conclusion
</p>
<p>Polynomes of ambulatory blood pressure measurements can be applied for visual-
</p>
<p>izing not only hypertension types but also treatment effects, see underneath graphs 
</p>
<p>of circadian patterns in individual patients (upper row) and groups of patients on 
</p>
<p>Conclusion</p>
<p/>
</div>
<div class="page"><p/>
<p>496
</p>
<p>different treatments (Figure from Cleophas et al., Chap. 16, Curvilinear regression, 
</p>
<p>pp 187&ndash;198, in: Statistics applied to clinical studies 5th edition, Springer Heidelberg 
</p>
<p>Germany 2012, with permission from the editor).
</p>
<p>180
</p>
<p>(mm Hg)
</p>
<p>80
</p>
<p>d
ia
</p>
<p>s
to
</p>
<p>lic
 a
</p>
<p>n
d
 s
</p>
<p>y
s
to
</p>
<p>lic
 b
</p>
<p>lo
o
d
 p
</p>
<p>re
s
s
u
re
</p>
<p>bolderline diastolic systolic white coat no dipper 8 - 8hours AM
</p>
<p> 
</p>
<p>180
</p>
<p>(mm Hg)
</p>
<p>d
ia
</p>
<p>s
to
</p>
<p>lic
 a
</p>
<p>n
d
</p>
<p> s
y
s
to
</p>
<p>lic
 b
</p>
<p>lo
o
</p>
<p>d
 p
</p>
<p>re
s
s
u
</p>
<p>re
</p>
<p>140
</p>
<p>80
</p>
<p>placebo enalapril amlodipine carvedilol celiprolol
8- 8 hours AM 
</p>
<p> 
</p>
<p> Note
</p>
<p>More background, theoretical and mathematical information of polynomes is given 
</p>
<p>in Chap. 16, Curvilinear regression, pp 187&ndash;198, in: Statistics applied to clinical 
</p>
<p>studies 5th edition, Springer Heidelberg Germany 2012, from the same authors.
</p>
<p>79 Fifth Order Polynomes of Circadian Rhythms (1 Patient with Hypertension)</p>
<p/>
</div>
<div class="page"><p/>
<p>497&copy; Springer International Publishing Switzerland 2015 
</p>
<p>T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
</p>
<p>Overview, DOI 10.1007/978-3-319-15195-3_80
</p>
<p>Chapter 80
</p>
<p>Gamma Distribution for Estimating 
the Predictors of Medical Outcome 
Scores (110 Patients)
</p>
<p> General Purpose
</p>
<p>The gamma frequency distribution is suitable for statistical testing of nonnegative 
</p>
<p>data with a continuous outcome variable and fits such data often better than does the 
</p>
<p>normal frequency distribution, particularly when magnitudes of benefits or risks is 
</p>
<p>the outcome, like costs. It is often used in marketing research.
</p>
<p>By readers not fond of maths the next few lines can be skipped.
</p>
<p>The gamma frequency distribution ranges, like the Poisson distribution for rate 
</p>
<p>assessments, from 0 to &infin;. It is bell-shaped, like the normal distribution, but not 
</p>
<p>as symmetric, looking a little like the chi-square distribution. Its algebraic 
</p>
<p>approximation is given underneath.
</p>
<p> 
</p>
<p>y e x standardizednormaldistribution
</p>
<p>y x e x g
r
</p>
<p>= &minus; ( )
</p>
<p>= ( ) &minus;
</p>
<p>^ /
</p>
<p>/ * ^
</p>
<p>1 2 2
</p>
<p>l g l aammadistribution( )
 
</p>
<p>where
</p>
<p> = scale parameter
</p>
<p>r = shape parameter
</p>
<p> = correction constant.
</p>
<p>This chapter is to assess whether gamma distributions are also helpful for the 
</p>
<p>analysis of medical data, particularly those with outcome scores.</p>
<p/>
</div>
<div class="page"><p/>
<p>498
</p>
<p> Primary Scientific Question
</p>
<p>Is gamma regression a worthwhile analysis model complementary to traditional 
</p>
<p>linear regression, can it elucidate effects unobserved in the linear models.
</p>
<p> Example
</p>
<p>In 110 patients the effects of age, psychological and social score on health scores 
</p>
<p>was assessed. The first 10 patients are underneath. The entire data file is entitled 
</p>
<p>&ldquo;gamma.sav&rdquo;, and is in extras.springer.com.
</p>
<p>age psychologic score social score health score
</p>
<p>3 5 4 8
</p>
<p>1 4 8 7
</p>
<p>1 5 13 4
</p>
<p>1 4 15 6
</p>
<p>1 7 4 10
</p>
<p>1 8 8 6
</p>
<p>1 9 12 8
</p>
<p>1 8 16 2
</p>
<p>1 12 4 6
</p>
<p>1 13 1 8
</p>
<p>age = age class 1&ndash;7
</p>
<p>psychologicscore = psychological score 1&ndash;20
</p>
<p>socialscore = social score 1&ndash;20
</p>
<p>healthscore = health score 1&ndash;20
</p>
<p>Start by opening the data file in SPSS statistical software. We will first perform 
</p>
<p>linear regressions.
</p>
<p>Command:
</p>
<p>Analyze&hellip;.Regression&hellip;.Linear&hellip;.Dependent: enter healthscore&hellip;.Independent(s): 
</p>
<p>enter socialscore&hellip;.click OK.
</p>
<p>The underneath table gives the result. Social score seems to be a very significant 
</p>
<p>predictor of health score.
</p>
<p>Coefficientsa
</p>
<p>Model
</p>
<p>Unstandardized 
</p>
<p>coefficients Standardized coefficients
</p>
<p>t Sig.B Std. error Beta
</p>
<p>1 (Constant) 9.833 .535 18.388 .000
</p>
<p>Social score &minus;.334 .050 &minus;.541 &minus;6.690 .000
aDependent Variable: health score
</p>
<p>80 Gamma Distribution for Estimating the Predictors of Medical Outcome Scores&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>499
</p>
<p>Similarly psychological score and age class are tested.
</p>
<p>Coefficientsa
</p>
<p>Model
</p>
<p>Unstandardized 
</p>
<p>coefficients
</p>
<p>Standardized 
</p>
<p>coefficients
</p>
<p>t Sig.B Std. error Beta
</p>
<p>1 (Constant) 5.152 .607 8.484 .000
</p>
<p>Psychological score .140 .054 .241 2.575 .011
aDependent Variable: health score
</p>
<p>Coefficientsa
</p>
<p>Model
</p>
<p>Unstandardized 
</p>
<p>coefficients Standardized coefficients
</p>
<p>t Sig.B Std. error Beta
</p>
<p>1 (Constant) 7.162 .588 12.183 .000
</p>
<p>Age class &minus;.149 .133 &minus;.107 &minus;1.118 .266
aDependent Variable: health score
</p>
<p>Linear regression with the 3 predictors as independent variables and health 
</p>
<p>scores as outcome suggests that both psychological and social scores are significant 
</p>
<p>predictors of health but age is not. In order to assess confounding and interaction a 
</p>
<p>multiple linear regression is performed.
</p>
<p>Command:
</p>
<p>Analyze&hellip;.Regression&hellip;.Linear&hellip;.Dependent: enter healthscore&hellip;.Independent(s): 
</p>
<p>enter socialscore, psychologicscore, age&hellip;.click OK.
</p>
<p>Coefficientsa
</p>
<p>Model
</p>
<p>Unstandardized 
</p>
<p>coefficients
</p>
<p>Standardized 
</p>
<p>coefficients
</p>
<p>t Sig.B Std. error Beta
</p>
<p>1 (Constant) 9.388 .870 10.788 .000
</p>
<p>Social score &minus;.329 .049 &minus;.533 &minus;6.764 .000
</p>
<p>Psychological score .111 .046 .190 2.418 .017
</p>
<p>Age class &minus;.184 .109 &minus;.132 &minus;1.681 .096
aDependent Variable: health score
</p>
<p>The above table is shown. Social score is again very significant. Psychological 
</p>
<p>score also, but after Bonferroni adjustment (rejection p-value = 0.05/4 = 0.0125) it 
</p>
<p>would be no more so, because p = 0.017 is larger than 0.0125. Age is again not sig-
</p>
<p>nificant. Health score is here a continuous variable of nonnegative values, and per-
</p>
<p>haps better fit of these data could be obtained by a gamma regression. We will use 
</p>
<p>SPSS statistical software again.
</p>
<p>Command:
</p>
<p>Analyze&hellip;.click Generalized Linear Models&hellip;.click once again Generalized Linear 
</p>
<p>Models&hellip;.mark Custom&hellip;.Distribution: select Gamma&hellip;.Link function: select 
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>500
</p>
<p>Power&hellip;.Power: type &minus;1&hellip;.click Response&hellip;.Dependent Variable: enter health-
</p>
<p>score click Predictors&hellip;.Factors: enter socialscore, psychologicscore, age&hellip;.Model: 
</p>
<p>enter socialscore, psychologicscore, age&hellip;.Estimation: Scale Parameter Method: 
</p>
<p>select Pearson chi-square&hellip;.click EM Means: Displays Means for: enter age, psy-
</p>
<p>chologicscore, socialscore&hellip;.click Save&hellip;.mark Predict value of linear predictor&hellip;.
</p>
<p>Standardize deviance residual&hellip;.click OK.
</p>
<p>Tests of model effects
</p>
<p>Source
</p>
<p>Type III
</p>
<p>Wald Chi-square df Sig.
</p>
<p>(Intercept) 216.725 1 .000
</p>
<p>Ageclass 8.838 6 .183
</p>
<p>Psychologicscore 18.542 13 .138
</p>
<p>Socialscore 61.207 13 .000
</p>
<p>Dependent Variable: health score
</p>
<p>Model: (Intercept), ageclass, psychologicscore, socialscore
</p>
<p>The above table give the overall result: is similar to that of the multiple linear 
</p>
<p>regression with only social class as significant independent predictor.
</p>
<p>Parameter estimates
</p>
<p>Parameter B Std. error
</p>
<p>95 % Wald 
</p>
<p>confidence interval Hypothesis test
</p>
<p>Lower Upper
</p>
<p>Wald 
</p>
<p>Chi-square df Sig.
</p>
<p>(Intercept) .188 .0796 .032 .344 5.566 1 .018
</p>
<p>[ageclass = 1] &minus;.017 .0166 &minus;.050 .015 1.105 1 .293
</p>
<p>[ageclass = 2] &minus;.002 .0175 &minus;.036 .032 .010 1 .919
</p>
<p>[ageclass = 3] &minus;.015 .0162 &minus;.047 .017 .839 1 .360
</p>
<p>[ageclass = 4] .014 .0176 &minus;.020 .049 .658 1 .417
</p>
<p>[ageclass = 5] .025 .0190 &minus;.012 .062 1.723 1 .189
</p>
<p>[ageclass = 6] .005 .0173 &minus;.029 .039 .087 1 .767
</p>
<p>[ageclass = 7] 0a . . . . . .
</p>
<p>[psychologicscore = 3] .057 .0409 &minus;.023 .137 1.930 1 .165
</p>
<p>[psychologicscore = 4] .057 .0220 .014 .100 6.754 1 .009
</p>
<p>[psychologicscore = 5] .066 .0263 .015 .118 6.352 1 .012
</p>
<p>[psychologicscore = 7] .060 .0311 &minus;.001 .121 3.684 1 .055
</p>
<p>[psychologicscore = 8] .061 .0213 .019 .102 8.119 1 .004
</p>
<p>[psychologicscore = 9] .035 .0301 &minus;.024 .094 1.381 1 .240
</p>
<p>[psychologicscore = 11] .057 .0325 &minus;.007 .120 3.059 1 .080
</p>
<p>[psychologicscore = 12] .060 .0219 .017 .103 7.492 1 .006
</p>
<p>[psychologicscore = 13] .040 .0266 &minus;.012 .092 2.267 1 .132
</p>
<p>[psychologicscore = 14] .090 .0986 &minus;.103 .283 .835 1 .361
</p>
<p>[psychologicscore = 15] .121 .0639 &minus;.004 .247 3.610 1 .057
</p>
<p>[psychologicscore = 16] .041 .0212 &minus;.001 .082 3.698 1 .054
</p>
<p>[psychologicscore = 17] .022 .0241 &minus;.025 .069 .841 1 .359
</p>
<p>(continued)
</p>
<p>80 Gamma Distribution for Estimating the Predictors of Medical Outcome Scores&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>501
</p>
<p>Parameter estimates
</p>
<p>Parameter B Std. error
</p>
<p>95 % Wald 
</p>
<p>confidence interval Hypothesis test
</p>
<p>Lower Upper
</p>
<p>Wald 
</p>
<p>Chi-square df Sig.
</p>
<p>[psychologicscore = 18] 0a . . . . . .
</p>
<p>[socialscore = 4] &minus;.120 .0761 &minus;.269 .029 2.492 1 .114
</p>
<p>[socialscore = 6] &minus;.028 .0986 &minus;.221 .165 .079 1 .778
</p>
<p>[socialscore = 8] &minus;.100 .0761 &minus;.249 .050 1.712 1 .191
</p>
<p>[socialscore = 9] .002 .1076 &minus;.209 .213 .000 1 .988
</p>
<p>[socialscore = 10] &minus;.123 .0864 &minus;.293 .046 2.042 1 .153
</p>
<p>[socialscore = 11] .015 .0870 &minus;.156 .185 .029 1 .865
</p>
<p>[socialscore = 12] &minus;.064 .0772 &minus;.215 .088 .682 1 .409
</p>
<p>[socialscore = 13] &minus;.065 .0773 &minus;.216 .087 .703 1 .402
</p>
<p>[socialscore = 14] .008 .0875 &minus;.163 .180 .009 1 .925
</p>
<p>[socialscore = 15] &minus;.051 .0793 &minus;.207 .104 .420 1 .517
</p>
<p>[socialscore = 16] .026 .0796 &minus;.130 .182 .107 1 .744
</p>
<p>[socialscore = 17] &minus;.109 .0862 &minus;.277 .060 1.587 1 .208
</p>
<p>[socialscore = 18] &minus;.053 .0986 &minus;.246 .141 .285 1 .593
</p>
<p>[socialscore = 19] 0a . . . . . .
</p>
<p>(Scale) .088b
</p>
<p>Dependent Variable: health score
</p>
<p>Model: (Intercept), ageclass, psychologicscore, socialscore
aSet to zero because this parameter is redundant
bComputed based on the Pearson chi-square
</p>
<p>However, as shown in the above large table, gamma regression enables to test 
</p>
<p>various levels of the predictors separately. Age classes were not significant predic-
</p>
<p>tors. Of the psychological scores, however, no less than 8 scores produced pretty 
</p>
<p>small p-values, even as small as 0.004 and 0.009. Of the social scores now no one is 
</p>
<p>significant.
</p>
<p>In order to better understand what is going on SPSS provides marginal means 
</p>
<p>analysis here.
</p>
<p>Estimates
</p>
<p>Age class Mean Std. error
</p>
<p>95 % Wald confidence interval
</p>
<p>Lower Upper
</p>
<p>1 5.62 .531 4.58 6.66
</p>
<p>2 5.17 .461 4.27 6.07
</p>
<p>3 5.54 .489 4.59 6.50
</p>
<p>4 4.77 .402 3.98 5.56
</p>
<p>5 4.54 .391 3.78 5.31
</p>
<p>6 4.99 .439 4.13 5.85
</p>
<p>7 5.12 .453 4.23 6.01
</p>
<p>The mean health scores of the different age classes were, indeed, hardly 
</p>
<p>different.
</p>
<p>Example</p>
<p/>
</div>
<div class="page"><p/>
<p>502
</p>
<p>Estimates
</p>
<p>Psychological score Mean Std. error
</p>
<p>95 % Wald confidence interval
</p>
<p>Lower Upper
</p>
<p>3 5.03 .997 3.08 6.99
</p>
<p>4 5.02 .404 4.23 5.81
</p>
<p>5 4.80 .541 3.74 5.86
</p>
<p>7 4.96 .695 3.60 6.32
</p>
<p>8 4.94 .359 4.23 5.64
</p>
<p>9 5.64 .809 4.05 7.22
</p>
<p>11 5.03 .752 3.56 6.51
</p>
<p>12 4.95 .435 4.10 5.81
</p>
<p>13 5.49 .586 4.34 6.64
</p>
<p>14 4.31 1.752 .88 7.74
</p>
<p>15 3.80 .898 2.04 5.56
</p>
<p>16 5.48 .493 4.51 6.44
</p>
<p>17 6.10 .681 4.76 7.43
</p>
<p>18 7.05 1.075 4.94 9.15
</p>
<p>However, increasing psychological scores seem to be associated with increasing 
</p>
<p>levels of health.
</p>
<p>Estimates
</p>
<p>Social score Mean Std. error
</p>
<p>95 % Wald confidence 
</p>
<p>interval
</p>
<p>Lower Upper
</p>
<p>4 8.07 .789 6.52 9.62
</p>
<p>6 4.63 1.345 1.99 7.26
</p>
<p>8 6.93 .606 5.74 8.11
</p>
<p>9 4.07 1.266 1.59 6.55
</p>
<p>10 8.29 2.838 2.73 13.86
</p>
<p>11 3.87 .634 2.62 5.11
</p>
<p>12 5.55 .529 4.51 6.59
</p>
<p>13 5.58 .558 4.49 6.68
</p>
<p>14 3.96 .711 2.57 5.36
</p>
<p>15 5.19 .707 3.81 6.58
</p>
<p>16 3.70 .371 2.98 4.43
</p>
<p>17 7.39 2.256 2.96 11.81
</p>
<p>18 5.23 1.616 2.06 8.40
</p>
<p>19 4.10 1.280 1.59 6.61
</p>
<p>In contrast, increasing social scores are, obviously, associated with deceasing 
</p>
<p>levels of health, with mean health scores close to 3 in the higher social score patients, 
</p>
<p>and over 8 in the lower social score patients.
</p>
<p>80 Gamma Distribution for Estimating the Predictors of Medical Outcome Scores&hellip;</p>
<p/>
</div>
<div class="page"><p/>
<p>503
</p>
<p> Conclusion
</p>
<p>Gamma regression is a worthwhile analysis model complementary to linear 
</p>
<p> regression, ands may elucidate effects unobserved in the linear models. Data from 
</p>
<p>sick people may not be normally distributed, but, rather, skewed towards low health 
</p>
<p>scores. Gamma distributions are skewed to the left, and may, therefore, better fit 
</p>
<p>such data than traditional linear regression.
</p>
<p> Note
</p>
<p>More background, theoretical and mathematical information of linear and nonlinear 
</p>
<p>regression models is given in many chapters of the current book, particularly the 
</p>
<p>chapters in the section entitled (log) linear models.
</p>
<p>Note</p>
<p/>
</div>
<div class="page"><p/>
<p>505
</p>
<p>  A 
</p>
<p>  Absolute risk , 251  
   Absorbing Markov chains , 351  
   Accuracy , 47, 390  
</p>
<p> statistics , 50, 443  
 true positives and true negatives , 390  
</p>
<p>   Activation function , 310  
   Actual % outside specifi cation limits , 106  
   Adjusted p-values , 462  
   Advanced analysis of variance, random effects 
</p>
<p>and mixed effects models , 206  
   Amino acid sequences , 460  
   Analyses of covariances , 306  
   Analysis node , 407, 415  
   Analysis of moment structures (Amos) , 
</p>
<p>295, 301  
   Analysis of safety data , 65, 70, 75, 85  
   Analysis of variance (ANOVA) , 94, 166  
   Analysis with two layers , 63  
   Analyzing predictor categories , 175  
   Anomaly detection , 33, 170  
   ANOVA.    See  Analysis of variance (ANOVA) 
   Apply Models modus , 215  
   Area under curve (AUC) , 251  
</p>
<p> of the Gaussian curve , 367  
 of the normal distribution , 279  
</p>
<p>   ARIMA.    See  Autoregressive integrated 
moving average (ARIMA) 
</p>
<p>   Artifi cial intelligence , 52  
 multilayer perceptron , 400  
 radial basis functions , 400  
</p>
<p>   Association rules , 475  
   Auto Classifi er node , 412  
   Autocorrelation analysis , 423  
   Autocorrelation coeffi cients , 425  
</p>
<p>   Automatic data mining , 383  
   Automatic linear regression , 186, 189  
   Automatic modeling , 445  
</p>
<p> of clinical events , 409  
 of drug effi cacies , 401  
</p>
<p>   Automatic Newton modeling , 417  
   Automatic non-parametric testing , 
</p>
<p>175, 179, 182  
   Automatic regression , 189  
   Auto Numeric node , 404  
   Autoregressive integrated moving average 
</p>
<p>(ARIMA) , 211  
   Autoregressive models for longitudinal 
</p>
<p>data , 211  
   Auxiliary view , 22, 181  
</p>
<p>    B 
</p>
<p>  Balanced iterative reducing and clustering 
using hierarchies (BIRCH) , 31  
</p>
<p>   Bar charts , 36, 53, 67  
   Basic Local Alignment Search Tool (BLAST) 
</p>
<p>database system , 459  
   Bayesian network , 295, 301, 414, 439  
   Bell shape (Gaussian shape) , 253  
   Best fi t category 
</p>
<p> for making predictions , 25  
 and probability of being in it , 171  
</p>
<p>   Best fi t separation line , 445  
   Best unbiased estimate of the variance 
</p>
<p>components , 221  
   Between-group linkage , 4  
   Between-subjects factor(s) , 272  
   Bias , 53  
</p>
<p> of multiple testing , 321  
</p>
<p>                     Index 
</p>
<p>&copy; Springer International Publishing Switzerland 2015 
T.J. Cleophas, A.H. Zwinderman, Machine Learning in Medicine - a Complete 
Overview, DOI 10.1007/978-3-319-15195-3</p>
<p/>
</div>
<div class="page"><p/>
<p>506
</p>
<p>   Binary logistic regression , 118, 248  
   Binary outcome variable , 179  
   Binary partitioning , 353  
   Binning process , 354  
   Binning Rules in a Syntax fi le , 26  
   Binomial SE of the sample proportion , 482  
   Bins , 26, 41  
</p>
<p> variables , 356  
   Biplot , 323  
   Bit score = the standardized score (score 
</p>
<p>independent of any unit) , 460  
   Blast hits , 461  
   Blinding , 241  
   Bluebit Online Matrix Calculator , 347  
   Bonferroni-adjusted z-tests , 81  
   Bonferroni adjustment , 477, 499  
   Bootstrap aggregating (&ldquo;bagging&rdquo;) , 416  
   Bootstrap resampling , 141  
   Box and whiskers graphs , 180  
   Box and whiskers plots , 39  
   Breslow and the Tarone&rsquo;s tests , 74  
   b-values , 146, 478  
</p>
<p>    C 
</p>
<p>  Canonical analysis , 165  
   Canonical b-values (regression 
</p>
<p>coeffi cients) , 167  
   Canonical form , 346  
   Canonical predictors , 168  
   Canonical regression , 165, 166  
   Canonical weights (the multiple b-values 
</p>
<p>of canonical regression) , 168  
   Canvas , 385, 388, 403, 441, 466  
   Capacity indices , 106  
   Cases , 7  
   Categorical analysis of covariates , 179  
   Categorical and continuous outcome , 47  
   Categorical data , 182  
   Categorical variable , 187  
   Category merging , 189  
   Causality , 295, 301  
   Cause effect diagrams , 396  
   Cause effect relationships , 301  
   C5.0 classifi er , 389  
   C5.0 decision tree (C5.0) , 389, 414  
   CHAID.    See  Chi square automatic interaction 
</p>
<p>detector (CHAID) 
   Change points , 211  
   Checklists , 396  
   Child Node , 48  
   Chi-square , 475  
   Chi square automatic interaction detector 
</p>
<p>(CHAID) , 48, 406, 408  
   Chi-squared automatic interaction model , 328  
</p>
<p>   Chi-square goodness of fi t , 253  
 for Fit Statistics , 367  
</p>
<p>   Chi-square test , 63, 81, 105, 286  
   Circadian rhythms , 491  
   Classifi cation and regression tree (C&amp;R Tree) , 
</p>
<p>406, 414  
   Classifi cation trees , 445  
   Classify , 4, 48, 150  
   Classifying new medicines , 17  
   Class of drugs , 17  
   Clinical data where variability is more 
</p>
<p>important than averages , 
105, 110, 200  
</p>
<p>   Clinical scores with inconsistent intervals , 223  
   Clustering Criterion , 32  
   Cluster membership , 14, 32  
   Clusters , 6  
   Cochran and Mantel Haenszel Statistics , 73  
   Cochran&rsquo;s and Mantel Haenszel tests , 75  
   Coeffi cient of dispersion (COD) , 488  
   Cohen&rsquo;s Kappa , 79  
   Column coordinates , 342  
   Column proportions , 81  
</p>
<p> comparisons of interaction matrices , 85  
   Comma Delimited (*.csv) , 37, 441, 467  
   Complementary log-log transformations , 54  
   Complex samples 
</p>
<p> methodologies , 313  
 statistics , 318  
</p>
<p>   Composite outcome variables , 151  
   Concentration Index , 488  
   Concordant cells , 69  
   Conditional dependencies of nodes , 295, 301  
   Conditional probabilities , 475  
   Confi dence intervals , 39, 481  
</p>
<p> of differences in proportions , 483  
 of proportions , 481  
 for proportions and differences in 
</p>
<p>proportions , 481  
   Confi dence rule , 476  
   Confi gurate and execute commands , 442, 448  
   Confusion matrix , 443  
   Conjoint analysis , 359  
   Conjoint program , 364  
   Constructing an analysis plan , 359  
   Contingency coeffi cient , 63  
   Contingency table , 61, 67  
   Continuity correction , 483  
   Continuous data , 53  
   Continuous variables , 14  
   Control charts , 105, 396  
   Conviction rule , 476  
   Correlation , 180  
</p>
<p> by Kendall&rsquo;s method , 363  
 by Pearson , 138  
</p>
<p>Index</p>
<p/>
</div>
<div class="page"><p/>
<p>507
</p>
<p>   Correlation coeffi cients , 138  
 of the three best models , 407  
</p>
<p>   Correlation level , 180  
   Correlation matrix , 139  
   Correlations , 94  
   Correspondence analysis , 321  
</p>
<p> with multidimensional analyses , 325  
   Counted rates of events , 261  
   Covariances , 302  
   Covariates , 166  
   Cox regression , 119  
</p>
<p> with a time-dependent predictor , 371  
   Crossover studies , 123, 272  
   Crossover trial , 189  
   Crosstab , 61, 65, 67, 68, 94, 267  
   Cross-tables , 321  
   Crossvalidation , 465  
   C-statistics , 245  
   csv fi le type , 37  
   csv type Excel fi le , 38, 49, 441, 466  
   Cubes , 95  
   Cubic (best fi t third order, hyperbolic) 
</p>
<p>relationship , 431  
   Cubic spline , 419  
   Cumulative histograms , 419  
   Cumulative probabilities ( = areas under curve 
</p>
<p>left from the x-value) , 258  
   Curvilinear regression , 430  
   Cut-off values , 25  
</p>
<p>    D 
</p>
<p>  DAG.    See  Directed acyclic graph (DAG) 
   Data audit node , 385  
   Data imputation , 17  
   Data mining , 35  
   Data view screen , 33  
   Davidwees.com/polygrapher , 493  
   DBSCAN method , 10  
   Decision analysis , 327  
   Decision list (Decision..) , 414  
   Decision list models , 414  
   Decision Tree Learner , 50, 467  
   Decision Tree Predictor , 50, 467  
   Decision trees , 28, 47, 327  
</p>
<p> with binary outcome , 331  
 with continuous outcome , 331  
 for decision analysis , 29  
 model , 469  
</p>
<p>   Decision Tree View , 50  
   Degenerate solution , 340  
   Degrees of freedom (df) , 166  
   Demographic data fi les , 17, 24  
   Dendrogram , 4  
</p>
<p>   Density-based cluster analysis , 10  
   Density-based clustering , 7, 9  
   DeSarbo&rsquo;s and Shepard criteria , 340  
   Descriptives , 94  
   Descriptive Statistics , 55, 63, 68, 78, 253, 268, 
</p>
<p>317, 452, 488  
   df.    See  Degrees of freedom (df) 
   Diagonal , 255  
   Diagonal line , 255  
   Differences in units , 142  
   Dimension reduction , 139, 323  
   Directed acyclic graph (DAG) , 295, 301, 441  
   Discordant cells , 69  
   Discrete data , 53, 67  
   Discretize , 145  
   Discretized variables , 143  
   Discriminant analysis , 149, 180, 383, 445  
</p>
<p> for supervised data , 153, 414  
   Dispersion measures (dispersion accounted 
</p>
<p>for) , 340  
   Dispersion values , 337  
   Dissociation constant , 419  
   Distance Measure , 32  
   Distance network , 181  
   Distances , 3  
   Distribution free methods of validation , 465  
   Distribution node , 385  
   Dociles , 40  
   Dose-effectiveness curves , 419  
   Dose-effectiveness studies , 422  
   Dotter graph , 7  
   Doubly multivariate analysis of variance , 271  
   Drop box , 92  
   Duplicate observations , 77, 79  
</p>
<p>    E 
</p>
<p>  EAP score table.    See  Expected ability a 
posteriori (EAP) score table 
</p>
<p>   Elastic net method , 147  
   Elastic net optimal scaling , 148  
   Elimination constant , 421  
   EMS.    See  Expected mean squares (EMS) 
   Ensembled accuracy , 416  
   Ensembled correlation coeffi cient , 408  
   Ensembled model(ing) , 408, 409  
</p>
<p> of continuous data 387, (DOUBT) 
   Ensembled outcome , 408, 416  
   Ensembled procedure , 408  
   Ensembled results 
</p>
<p> of best fi t models , 445  
 of a number of best fi t models , 401  
</p>
<p>   Entropy method , 353  
   Eps , 10  
</p>
<p>Index</p>
<p/>
</div>
<div class="page"><p/>
<p>508
</p>
<p>   Equal intervals , 53  
   Error rates , 448, 468  
   Euclidean , 32  
   E-value = expected number similarity 
</p>
<p>alignment scores , 460  
   E-value = p-value adjusted for multiple 
</p>
<p>testing , 460  
   Event-rates , 131&ndash;135  
   Evolutionary operations (evops) , 435&ndash;437  
</p>
<p> calculators , 435  
   Examine , 94  
   Excel fi les , 37  
   Excel graph , 419  
   Expected ability a posteriori (EAP) score 
</p>
<p>table , 368  
   Expected counts , 323  
   Expected mean squares (EMS) , 221  
</p>
<p> of error (the residual effect) , 222  
   Expected proportion , 476  
   Expert Node , 405&ndash;407  
   Expert tab , 413&ndash;414  
   Explanatory variable , 208  
   Explorative data mining , 3  
   Exploratory purposes , 390  
   Exported eXtended Markup Language 
</p>
<p>(XML) , 33  
   Export modeler , 213  
   Exposure (x-value) variables , 137  
   eXtended Markup Language (XML) , 14, 33, 
</p>
<p>124, 126, 132, 150, 172, 177, 178, 
187, 192, 196, 213, 310, 315, 328, 
333, 398  
</p>
<p> fi les , 14, 15, 32, 33, 114, 115, 125  
 model , 33  
</p>
<p>   Extras.springer.com , 4, 26, 36, 62, 68, 72, 78, 
81, 87, 95, 114, 124, 134, 138, 150, 
151, 156, 160, 172, 179, 184, 190, 
196, 204, 224, 234, 254, 257, 280, 
290, 302, 310, 314, 360, 373, 392, 
401, 408, 416, 446, 447, 452, 466, 
467, 477  
</p>
<p>    F 
</p>
<p>  Factor analysis , 137&ndash;142, 169, 295, 300, 301  
   False positives , 248  
   Fifth order polynomes , 491&ndash;496  
   Fifth order polynomial , 492, 493  
   Fifth order relationship , 491  
   File reader , 441, 442  
   File reader node , 38, 447, 466  
   Filtered periodogram , 427  
   First order (linear) relationship , 491  
   Fitting non-linear functions , 418  
</p>
<p>   Fixed and random effects , 203  
   Fixed intercept log-linear analysis , 184  
   Fixed intercept models , 187  
   Flow charts , 396  
   Forecast , 215, 425  
   Fourier analyses , 423  
   Fourth order (sinusoidal) relationship , 491  
   Frequencies , 94&ndash;99  
</p>
<p> procedures , 53  
 tables , 53, 67  
</p>
<p>   F-tests , 146  
   Fundamental matrix (F) , 347  
   Fuzzy logic , 377&ndash;381  
   Fuzzy memberships , 379  
   Fuzzy-model , 381  
   F-value , 166  
</p>
<p>    G 
</p>
<p>  Gamma , 68, 497&ndash;503  
   Gamma frequency distribution , 497&ndash;503  
   Gamma regression , 498  
   Gaussian activation function , 397  
   Gaussian distributions , 175, 179  
   Gaussian-like patterns , 11  
   Generalized estimating equations , 126  
   Generalized linear mixed models , 184, 185, 
</p>
<p>187, 203&ndash;206  
   Generalized linear models , 123&ndash;129, 131&ndash;135, 
</p>
<p>263, 289, 291, 406, 499  
   General linear models , 94, 166, 220, 272, 
</p>
<p>314, 492  
   General loglinear model , 230  
   GoF.    See  Goodness of fi t (GoF) 
   Gold nugget , 390  
   Goodman coeffi cient , 65  
   Goodness of fi t (GoF) , 141, 237  
   Goodness of fi t tests , 365  
   Graphical tools of data analysis , 396  
   GraphPad Software QuickCalcs t test 
</p>
<p>calculator , 472  
</p>
<p>    H 
</p>
<p>  Health risk cut-offs , 25  
   Heterogeneity correction factor , 280  
   Heterogeneity due to chance , 244  
   Heterogeneity in clinical research , 241&ndash;244  
   Heterogeneity tests , 74  
   Heterogeneous studies and meta-
</p>
<p>regression , 244  
   Heterogeneous target populations , 318  
   Heteroscedasticity , 158  
   Heuristic studies , 445  
</p>
<p>Index</p>
<p/>
</div>
<div class="page"><p/>
<p>509
</p>
<p>   Hidden layers , 310  
   Hierarchical and k-means clustering , 11  
   Hierarchical cluster analysis , 4&ndash;6  
</p>
<p> for unsupervised data , 46  
   Hierarchical clustering , 3&ndash;8  
   Hierarchical cluster modeling , 46  
   High-Risk-Bin Memberships , 25&ndash;29  
   High risk cut-offs , 27, 353&ndash;357  
   Histogram , 40&ndash;41, 197, 254, 258, 419  
   Holdouts , 360, 363  
   Homogeneous data , 9&ndash;11  
   Homogenous populations , 7  
   Homologous (philogenetically from the same 
</p>
<p>ancestors) , 461  
   Homoscedastic , 155  
   http://blast.ncbi.nlm.nih.gov/Blast.cgi , 460  
   http://vassarstats.net/prop2_ind.html , 483  
   Hyperbola , 419  
   Hyperbolic tangens , 310  
   Hypotheses, data, stratifi cation , 53  
</p>
<p>    I 
</p>
<p>  Ideal point , 342  
   Ideal point map , 339  
   Identity (I) matrix , 347  
   Improved precision of analysis , 189  
   Imput and output relationships , 379  
   Incident rates with varying incident risks , 
</p>
<p>229&ndash;232  
   Inconsistent spread , 155&ndash;158  
   Index1 , 103  
   Individual proximities , 340  
   Instrumental variable , 207  
   Instrument response function (IRF) shape , 367  
   Integer overfl ow , 140  
   Interactions , 37, 159&ndash;163  
</p>
<p> effects , 159&ndash;163  
 matrices , 71, 75, 77, 79, 85  
 matrix , 65  
</p>
<p> of nominal variables , 61  
 between the outcome variables , 150  
 and trends with multiple response , 457  
 variable , 160, 161  
</p>
<p>   Interactive histogram , 40  
   Interactively rotating , 20  
   Interactive output sheets , 180  
   Interactive pivot tables , 94  
   Interactive set of views , 180  
   Interval censored data analysis , 289&ndash;292  
   Interval censored link function , 289  
   Interval coeffi cient of concentration , 488  
   Interval data , 327  
   IO option (import/export option nodes) , 39  
</p>
<p>   IRF shape.    See  Instrument response function 
(IRF) shape 
</p>
<p>   Item response modeling , 365&ndash;369  
   Iterate , 6  
   Iteration methods for crossvalidations , 
</p>
<p>465&ndash;469  
   Iterations plot , 139  
   Iterative random samples , 468  
</p>
<p>    J 
</p>
<p>  JAVA Applet , 10  
</p>
<p>    K 
</p>
<p>  Kaplan-Meier curves , 291  
   Kappa , 78  
   kappa-value , 79  
   Kendall&rsquo;s tau-b , 68  
   Kendall&rsquo;s tau-c , 68  
   Kinime.    See  Konstanz information miner 
</p>
<p>(Knime) 
   K-means cluster analysis , 6&ndash;7  
   K-means clustering , 3&ndash;8  
   k-means cluster model , 6  
   K nearest neighbors (KNN) algorithm , 414  
</p>
<p> clustering , 406  
   Kolmogorov Smirnov tests , 253  
   Konstanz information miner (Knime) , 300, 
</p>
<p>441&ndash;443, 446&ndash;447, 466&ndash;467  
 data miner , 37&ndash;38, 49&ndash;50, 446&ndash;447, 
</p>
<p>466&ndash;467  
 software , 35, 45, 441&ndash;442  
 welcome screen , 37, 441, 446, 466  
 workbench , 37, 441, 446, 466  
 workfl ow , 38, 50&ndash;51, 442&ndash;443  
</p>
<p>   Kruskall-Wallis , 266, 271  
   Kruskal&rsquo;s stress-I , 340  
   Kurtosis , 57  
</p>
<p>    L 
</p>
<p>  Lambda , 63, 167  
   Lambda value , 65  
   Laplace transformations , 485  
   Lasso , 145  
</p>
<p> optimal scaling , 148  
 regularization model , 146  
</p>
<p>   Latent factors , 137  
   Layer variable , 73  
   Learning sample , 397  
   Legacy Dialogs , 6, 36, 242  
   Lift Chart , 39&ndash;40  
   Lift rule (lift-up rule) , 476  
</p>
<p>Index</p>
<p/>
</div>
<div class="page"><p/>
<p>510
</p>
<p>   Likelihoods for different powers , 157  
   Linear-by-linear association , 268, 457  
   Linear cause-effect relationships , 151  
   Linear data , 159  
   Linear equation , 183  
   Linear, logistic, and Cox regression , 113&ndash;121  
   Linear regression , 142, 144&ndash;145, 406  
</p>
<p> for assessing precision, confounding, 
interaction , 116, 194  
</p>
<p> basic approach , 116, 194  
   Line plot , 41&ndash;42  
   Linguistic membership names , 379  
   Linguistic rules for the input and output 
</p>
<p>data , 379  
   Link function: select Power type-1 , 499&ndash;500  
   Ljung-Box tests , 214  
   Logarithmic regression model , 378  
   Logarithmic transformation , 59  
   Logical expression , 373  
   Logistic and Cox regression, Markov models, 
</p>
<p>Laplace transformations , 118, 121  
   Logistic regression , 116&ndash;118, 123, 179  
   Logit loglinear modeling , 233  
   Log likelihoods , 353  
   Loglinear , 229&ndash;239  
</p>
<p> equations , 183  
 modeling , 229&ndash;239, 503  
</p>
<p>   Log odds 
 of having the disease , 248  
 otherwise called logit , 279  
</p>
<p>   Log prob (otherwise called probit) , 279  
   Logtime in Target Variable , 59  
   Log transformed dependent variable , 264  
   Loss function , 340  
   Lower confi dence limits (LCL) , 108, 215  
   Lower control limit (LCL) , 108  
   LTA-2 (Latent Trait Analysis-2) free software 
</p>
<p>program , 367  
</p>
<p>    M 
</p>
<p>  Machine learning in medicine-cookbook 1 , 3, 
9, 13, 29, 113, 123, 131, 137, 143, 
149, 155, 159, 165, 309, 313, 335, 
345, 353, 359  
</p>
<p>   Machine learning in medicine-cookbook 2 , 17, 
25, 31, 171, 175, 183, 189, 195, 
203, 207, 211, 365, 371, 377, 383, 
391, 401, 409, 485  
</p>
<p>   Machine learning in medicine-cookbook 3 , 35, 
47, 219, 223, 229, 233, 241, 245, 
423, 429, 435, 439, 445, 451, 459, 
465, 471, 475, 481  
</p>
<p>   Machine learning in medicine part one , 46, 47, 
52, 102, 104, 142, 148, 163, 169, 
180, 206, 295, 301, 312, 353, 369, 
375, 381, 383, 400, 407, 414, 428, 
445, 469  
</p>
<p>   Machine learning in medicine part three , 29, 
158, 206, 222, 276, 277, 279, 287, 
319, 334, 351, 357, 364, 406, 414, 
422, 428, 433, 445, 457  
</p>
<p>   Machine learning in medicine part two , 8, 11, 
15, 33, 34, 174, 210, 217, 245, 251, 
325, 344, 406, 414, 439, 444, 449, 
464, 479  
</p>
<p>   Magnitude of variance due to residual error 
(unexplained variance, otherwise 
called Error) , 220  
</p>
<p>   Magnitude of variance due to subgroup 
effects , 222  
</p>
<p>   MANOVA.    See  Multivariate analysis of 
variance (MANOVA) 
</p>
<p>   Mantel Haenszel (MH) odds ratio (OR) , 74, 75  
   Marginalization , 439  
   Marginal means analysis , 501  
   Markov chains , 351  
   Markov modeling , 345  
   Mathematical functions , 417  
   Matrix algebra , 346  
   Matrix mean scores , 336  
   Matrix of scatter plots , 42&ndash;43  
   Matrix software , 482  
   Max score = best bit score between query 
</p>
<p>and database sequence , 460  
   McCallum-Layton calculator for 
</p>
<p>proportions , 477  
   McNemar&rsquo;s test , 477  
   Mean predicted probabilities , 292  
   Means and standard deviations , 53  
   Measuring agreement , 77&ndash;79  
   Median , 57, 180, 488  
   Menu bar , 90  
   Meta-analysis, review and update 
</p>
<p>of methodologies , 244  
   Meta-regression , 244  
   Michaelis-Menten equation , 419  
   Microsoft , 187  
   Microsoft&rsquo;s drawing commands , 6, 338  
   Missing data , 24  
</p>
<p> imputation , 24  
   Mixed data , 203&ndash;206  
   Mixed linear analysis , 103  
   Mixed linear models , 102  
   Mixed models , 184  
   Modeled regression coeffi cients , 192  
</p>
<p>Index</p>
<p/>
</div>
<div class="page"><p/>
<p>511
</p>
<p>   Model entropy , 26  
   Model viewer , 19, 32, 180  
   Modifi ed hierarchical cluster analysis , 45  
   Monte Carlo simulation , 195  
   MOTIF data base system , 459  
   Multidimensional clustering , 15  
   Multidimensional data , 87&ndash;94  
   Multidimensional datasets , 95  
   Multidimensional scaling , 335&ndash;344  
   Multidimensional Unfolding (PREFSCAL) , 
</p>
<p>339, 342  
   Multilayer neural network , 397  
   Multilayer perceptron , 310  
</p>
<p> modeling , 47  
   Multinomial and polynomial not 
</p>
<p>synonymous , 491  
   Multinomial logistic regression , 37  
   Multinomial, otherwise called polytomous, 
</p>
<p>logistic regression , 174  
   Multinomial regression 
</p>
<p> for outcome categories , 171&ndash;174  
   Multinomial regression , 171&ndash;174, 183, 223  
   Multiple bins for a single case , 29  
   Multiple dimensions , 87  
</p>
<p> with multiple response , 457  
   Multiple endpoints , 276, 277  
   Multiple groups chi-square test , 322  
   Multiple linear regression , 144&ndash;145, 176  
   Multiple paired outcomes and multiple 
</p>
<p>measures of the outcomes , 273  
   Multiple probit regression , 282&ndash;286  
   Multiple response crosstabs , 456  
   Multiple response sets , 451&ndash;457  
   Multiple testing , 276, 408  
   Multiple treatments , 277  
   Multistage regression , 295, 300  
   Multivariate analysis of time series , 217  
   Multivariate analysis of variance (MANOVA) , 
</p>
<p>140, 162, 164, 180, 271  
</p>
<p>    N 
</p>
<p>  Nearest neighbors , 17&ndash;24, 414  
 methodology , 17, 24  
</p>
<p>   Nested term , 104  
   Neural network (neural net) , 47, 52, 309&ndash;312, 
</p>
<p>397&ndash;400, 445  
   Newton&rsquo;s method , 417  
   Node box plot , 39  
   Node dialog , 39  
   Node repository , 39, 49, 441, 467  
   Node repository box , 467  
   Nodes , 29, 38, 385, 442  
   Nodes x-partitioner, svm learner, svm 
</p>
<p>predictor, x-aggregator , 448  
</p>
<p>   Noise handling , 32  
   Nominal and ordinal clinical data , 77  
   Nominal clinical data , 61&ndash;65  
   Nominal data , 53, 59, 67  
   Nominal variable , 55&ndash;56  
   Nominal x nominal crosstabs , 69  
   Non-algorithmic methods , 327  
   Non-linear modeling , 46  
   Nonlinear Regression Calculator of Xuru&rsquo;s 
</p>
<p>website , 418  
   Non-metric method , 327, 353  
   Nonnegative data , 497  
   Non-normal data , 196&ndash;200  
   Nonparametric tests , 94, 432  
   Non-proportional hazards , 372  
   Normal curve on histogram , 57, 59  
   Normal distributions , 177  
   Normality test , 253  
   Normalized stress , 340  
   Novel variables , 192  
   Nucleic acids sequences , 460  
   Numeric expression , 59  
</p>
<p>    O 
</p>
<p>  Observed counts , 312  
   Observed proportion , 476  
   Odds of being unhealthy , 73  
   Odds of disease , 245  
   Odds of event , 261  
   Odds of having had a particular prior 
</p>
<p>diagnosis , 150  
   Odds ratio , 72, 249, 279  
</p>
<p> (Exp (B)) , 235  
 and multiple regression , 118  
</p>
<p>   OLAP.    See  Online analytical processing 
(OLAP) 
</p>
<p>   One by one distances , 336  
   One way analysis of variance (ANOVA) , 
</p>
<p>271, 276  
   Online analytical procedure cubes , 95&ndash;99  
   Online analytical processing (OLAP) , 95  
   Online matrix-calculators , 345  
   Optimal binning , 26&ndash;28, 353&ndash;357  
   Optimal bins , 26&ndash;29  
   Optimal scaling , 143&ndash;148  
</p>
<p> discretization , 148  
 with elastic net regression , 147&ndash;148  
 with lasso regression , 147  
 with or without regularization , 148  
 regularization including ridge, lasso, 
</p>
<p>and elastic net regression , 148  
 with ridge regression , 146  
 of SPSS , 145  
 without regularization , 145&ndash;146  
</p>
<p>Index</p>
<p/>
</div>
<div class="page"><p/>
<p>512
</p>
<p>   Optimize Bins , 26  
   Ordered bar chart , 56  
   Ordinal clinical data , 67&ndash;70  
   Ordinal data , 53  
   Ordinal regression 
</p>
<p> with a complimentary log-log function , 225  
 including specifi c link functions , 227  
</p>
<p>   Ordinal scaling , 223&ndash;227  
 for clinical scores with inconsistent 
</p>
<p>intervals , 54, 60  
   Ordinal variable , 56&ndash;57  
   Ordinal x ordinal crosstabs , 69  
   Ordinary least squares (OLS) linear regression 
</p>
<p>analysis , 155  
   Original matrix partitioned , 346  
   Orthogonal design , 360  
   Orthogonality of the two outcomes , 180  
   Orthogonal modeling of the outcome 
</p>
<p>variables , 151  
   Outcome and predictor categories , 183&ndash;187  
   Outcome categories , 171&ndash;174, 233&ndash;239  
   Outcome prediction 
</p>
<p> with paired data , 123&ndash;129  
 with unpaired data , 113&ndash;121  
</p>
<p>   Outcome (y-value) variables , 151  
   Outliers , 253  
</p>
<p> category , 33  
 data , 253  
 detection , 31  
 groups , 9&ndash;11  
 memberships , 31&ndash;34  
 trimming , 189  
</p>
<p>   Output node , 390  
   Overall accuracy , 415  
   Overdispersion , 146, 257, 488  
   Overfi tting , 416  
</p>
<p>    P 
</p>
<p>  Paired binary (McNemar test) , 129  
   Paired chi-square tests , 475  
   Paired data , 113&ndash;121  
   Paired observations , 271  
   Pairwise comparisons , 181  
   Parallel coordinates , 43&ndash;44  
   Parallel-groups , 179  
   Parallel group study , 81, 87, 102  
   Parent node , 48  
   Pareto charts , 391&ndash;396  
   Pareto principle , 391  
   Parsimonious , 142  
   Partial correlation analysis , 161  
   Partial least squares (PLS) , 137&ndash;142, 169  
   Partial regression coeffi cients , 493  
</p>
<p>   Partitioning , 47, 310  
   Partitioning node , 50  
   Partitioning of a training and a test sample , 52  
   Path analysis , 295, 299  
   Pearson chi-square , 457  
   Pearson chi-square value , 268  
   Pearson goodness of fi t test , 280  
   Penalty term , 340  
   Percentages of misclassifi cations , 65  
   Performance evaluation of novel diagnostic 
</p>
<p>tests , 245&ndash;251  
   Performance indices , 109  
   Periodicity , 213, 423&ndash;428  
   Periodogram , 425  
   Periodogram&rsquo;s variance , 428  
   Pharmacokinetic parameters , 421  
   Phi and Cramer&rsquo;s V , 63  
   Phi value , 64  
   Pie charts , 53  
   Pivot , 87&ndash;94  
</p>
<p> fi gures , 107  
 tables , 32  
</p>
<p>   Pivoting 
 the data , 89  
 tray , 87&ndash;94  
 trays and tables , 87&ndash;94  
</p>
<p>   Placebo-controls , 241  
   Plot node , 387  
   Plot of the actual distances , 337  
   PLS.    See  Partial least squares (PLS) 
   Pocket calculator method for computing the 
</p>
<p>chi-square value , 324  
   Poisson , 232  
   Poisson distributions , 230  
   Poisson regression , 133, 230  
   Polynomial grapher of David Wees , 493  
   Polynomial modeling , 419, 492  
   Polytomous regression , 174  
   Pooled t-test , 471  
   Post-hoc analyses in clinical trials , 118  
   Precision , 193  
   Predicted cluster membership , 33  
   Predicting factors , 27  
   Prediction accuracy , 147  
   Prediction table , 448&ndash;449  
   Predictive model markup language (PMML) 
</p>
<p>document , 196  
   Predictive performance , 40  
   Predictor categories , 175&ndash;182  
   Preference scaling , 338&ndash;343  
   Preference scores , 335, 361  
   Principal components analysis , 139  
   Probabilistic graphical models using nodes 
</p>
<p>and arrows , 439  
</p>
<p>Index</p>
<p/>
</div>
<div class="page"><p/>
<p>513
</p>
<p>   Probability-probability (P-P) plot method , 258  
   Probit models , 279&ndash;287  
   Process capability indices , 106, 109  
   Process improvement , 435&ndash;438  
   Process stability , 109  
   Proportional hazard model of Cox , 289  
   Proportion of false positive , 248  
   Proportion of variance in the data due to the 
</p>
<p>random cad effect , 220  
   Proportions, fractions, percentages, risks, 
</p>
<p>hazards are synonymous , 481  
   Protein and DNA sequence mining , 459&ndash;464  
   Proximity and preference scores , 335  
   Proximity scaling , 336&ndash;338  
   P value calculator-GraphPad , 471  
   P-values , 83, 145, 166, 172  
</p>
<p>    Q 
</p>
<p>  Q-Q plots.    See  Quantile-quantile plots 
(Q-Q plots) 
</p>
<p>   Quadratic modeling , 432  
   Quadratic (best fi t second order, parabolic) 
</p>
<p>relationship , 431  
   Qualitative data , 53  
   Qualitative diagnostic tests , 251  
   Quality control , 392  
</p>
<p> of medicines , 105&ndash;110  
   Quantal effect histograms , 41  
   Quantile-quantile plots (Q-Q plots) , 53, 237, 
</p>
<p>253&ndash;259  
   Quantitative data , 53  
   Quartiles , 57, 180  
   Query coverage = percentage of amino acids 
</p>
<p>used , 460  
   Quest decision tree (Quest Tr..) , 414  
   Quick Unbiased Effi cient Statistical Trees 
</p>
<p>(QUEST) , 414  
   Quinlan decision trees , 389  
</p>
<p>    R 
</p>
<p>  Races as a categorical variable , 182  
   Radial basis neural networks , 397&ndash;400  
   Random effects , 185, 203  
   Random interaction effect , 203  
   Random intercept analysis , 183&ndash;187  
   Random intercept model , 183, 185, 186, 192  
   Randomization , 241  
   Random Number Generators , 14, 32, 114, 124, 
</p>
<p>133, 150, 177, 398  
   Random sample , 314  
   Ranges , 180  
   Rate analysis , 261&ndash;264  
</p>
<p>   Ratios , 314, 485  
 of the computed Pearson chi-square value 
</p>
<p>and the number of observations , 64  
 statistics , 485&ndash;489  
 successful prediction with /without the 
</p>
<p>model , 40  
   Receiver operated characteristic (ROC) 
</p>
<p>curves , 245  
   Recoded linear model , 178  
   Recode variables into multiple binary 
</p>
<p>(dummy) variables , 175  
   Regression , 94  
   Regression coeffi cient , 248  
   Regression equation , 177, 248  
   Regression lines of the 4th to 7th order , 491  
   Regression modeling for improved 
</p>
<p>precision , 194  
   Regularization , 147  
</p>
<p> procedure , 146  
   Regularized optimal scaling , 148  
   Relative health risks , 71&ndash;75, 77  
   Relative risk , 72, 251  
</p>
<p> assessments , 75  
   Reliability , 77, 138, 139, 491  
   Reliability analysis , 138  
   Reliability assessment of qualitative diagnostic 
</p>
<p>tests , 77  
   Repeated measures ANOVA , 271  
   Reports , 96  
   Reproducibility , 77  
</p>
<p> of mean values , 491  
 measures , 365  
</p>
<p>   Rescaled distance , 5  
   Rescaled phi values , 64  
   Rescaling , 189  
   Residual effect , 219  
   Residual error of a study , 219  
   Residual methods of validation , 465  
   Residuals , 157  
</p>
<p> sum of squares , 419  
   Residues , 431  
   Response rates , 279  
   Restructure , 103, 104  
</p>
<p> data wizard , 101  
 selected variables into cases , 103  
</p>
<p>   Ridge regression , 146, 148  
   Risk analysis , 261&ndash;264  
   Risk and classifi cation tables , 329  
   Risk of overestimating the precision , 210  
   Risk probabilities , 47, 52  
   R matrix , 347, 348  
   Robust Tests , 262  
   R partial least squares , 140  
   R-square values , 429  
</p>
<p>Index</p>
<p/>
</div>
<div class="page"><p/>
<p>514
</p>
<p>   Runs test , 429&ndash;433  
   R value , 157, 158, 399  
   r-values (correlation coeffi cients) , 141  
</p>
<p>    S 
</p>
<p>  Sampling plan , 314  
   Saved XML fi le , 115, 117  
   Scale data , 53  
   Scale variable , 54  
   Scattergram , 396, 423  
   Scatter plots , 42, 241, 253  
   Schwarz&rsquo;s Bayesian Criterion , 32  
   Scorer , 50  
   Scorer node , 443  
   Scoring Wizard , 14, 33, 115, 117, 120, 125, 
</p>
<p>128, 135, 152, 173, 311, 399  
   Seasonality , 211  
   Second derivatives , 419  
   Second order (parabolic) relationship , 491  
   SEM.    See  Structural equation modeling (SEM) 
   Sensitivity , 50, 146, 465, 469  
</p>
<p> of MANOVA , 180  
 of testing , 261  
</p>
<p>   Sequence similarity searching , 459  
   Settings tab , 407, 414  
   Shapiro-Wikens , 253  
   Shareware , 186, 192  
   Shrinking factor , 146  
   Sigma , 106  
   Silicon Valley , 35  
   Simple probit regression , 279  
   Simple random sampling (srs) method , 317  
   Simulation , 195  
   Simulation models , 195&ndash;201  
   Simulation plan &ldquo;splan,&rdquo; 197 , 198  
   Skewed curves , 248  
   Skewness , 57, 58, 253, 254, 257  
   Skewness to the right , 58  
   Somer&rsquo;s d , 68, 69  
   Spearman , 339  
   Specifi cation limits , 106  
   Spectral analysis , 425  
   Spectral density analysis , 426  
   Spectral density curve , 427  
   Spectral plot methodology , 423  
   Spectral plots , 423  
   Splan , 198  
   Splines , 43, 46, 145  
   Splitting methodology , 47  
   Spreadsheets programs like Excel , 94  
   Springer Heidelberg Germany , 8, 11, 15, 24, 
</p>
<p>29, 33, 34, 46, 47, 52, 53, 65, 70, 
75, 77, 79, 85, 102, 104, 105, 110, 
</p>
<p>116, 118, 121, 129, 135, 142, 148, 
153, 158, 163, 169, 174, 180, 182, 
194, 200, 201, 206, 210, 217, 222, 
230, 232, 244, 245, 248, 251, 253, 
259, 261, 264&ndash;266, 269, 271, 276, 
277, 279, 287, 289, 293&ndash;296, 
300&ndash;302, 306, 312, 319, 325, 334, 
344, 351, 353, 357, 364, 369, 375, 
381, 400, 406, 407, 414, 422, 428, 
433, 435, 438, 439, 444, 445, 449, 
457, 464, 469, 473, 479, 482, 484, 
485, 489, 491, 496  
</p>
<p>   Sps fi le , 29  
   SPSS 19.0 , 4, 26, 114, 117, 119, 124, 126, 
</p>
<p>132, 138, 144, 150, 310  
   SPSS data fi les , 37, 152  
   SPSS for starters part one , 77, 129, 206, 265, 
</p>
<p>266, 406  
   SPSS for starters part two , 135, 182, 261, 264, 
</p>
<p>295, 296, 300&ndash;302  
   SPSS modeler , 37, 300, 383, 390, 441, 
</p>
<p>447, 466  
   SPSS Modeler Stream fi le , 408  
   SPSS module Correlations , 161  
   SPSS statistical software , 87, 262, 290, 295, 
</p>
<p>378, 390, 440, 451, 486  
   SPSS&rsquo; syntax program , 258  
   SPSS tutorial case studies , 271  
   SQL Express , 95  
   Square boolean matrix , 141  
   Squared correlation coeffi cient , 167, 429  
   Squared Euclidean Distance , 4  
   Square matrix Q , 347  
   SSCP matrices , 272  
   S-shape dose-cumulative response curves , 378  
   Standard deviation , 107, 108  
   Standard errors , 248  
</p>
<p> of proportions , 482  
   Standardized (z transformed) canonical 
</p>
<p>coeffi cients , 168  
   Standardized covariances , 305  
   Standardized mean preference values , 343  
   Standardized regression coeffi cients , 299  
   Standardized x-and y-axes , 338  
   Standard multiple linear regression , 190  
   Stationary Markov chains , 351  
   Stationary R square , 214  
   Statistical data analysis , 35  
   Statistical power , 158  
   Statistics applied to clinical studies 5th 
</p>
<p>edition , 53, 65, 79, 85, 110, 116, 
118, 121, 182, 194, 200, 201, 230, 
232, 244, 253, 259, 269, 289, 469, 
473, 482, 484, 485, 489, 491, 496  
</p>
<p>Index</p>
<p/>
</div>
<div class="page"><p/>
<p>515
</p>
<p>   Statistics Base add-on module SPSS , 189  
   Statistics fi le node , 385, 403, 411  
   Statistics on a Pocket Calculator part 2 , 248  
   Std.deviations , 57  
   Stepping functions , 175  
   Stepping pattern , 53, 67  
   Stochastic processes , 345&ndash;351  
   Stream of nodes , 403, 411, 447, 466  
</p>
<p> called workfl ow in knime , 441  
   Stress (standard error) , 340  
   String variable , 6  
   Structural equation modeling (SEM) , 
</p>
<p>295&ndash;296, 300  
   Subgroup memberships , 13&ndash;15  
   Subgroup property , 219  
   Submatrices , 346  
   Subsummaries , 95  
   Summary tables , 53  
   Supervised data , 383  
   Support rule , 476  
   Support vector machine (SVM) , 406, 408, 
</p>
<p>414, 445&ndash;449  
   SUR_1 , 119  
   Survey data , 13  
   Surveys , 3&ndash;8  
   Survival studies with varying risks , 371&ndash;375  
   SVM.    See  Support vector machine (SVM) 
   svm , 448  
   Symbol &cap;, 473 (COMP: Please insert correct 
</p>
<p>symbol) 
   Synaptic weights estimates , 398  
   Syntax , 167  
   Syntax Editor dialog box , 167  
   Syntax fi le , 28, 360  
   Syntax text , 425  
</p>
<p>    T 
</p>
<p>  Tablet desintegration times , 105&ndash;110  
   Terminal node , 328, 330, 332  
   Testing parallel-groups with different sample 
</p>
<p>sizes and variances , 471&ndash;473  
   Testing reproducibility , 79  
   Test-retest reliability , 138, 139  
   Test sample , 47  
   Third order (hyperbolic) relationship , 491  
   Three-dimensional scaling model , 343  
   Threshold for a positive test , 248  
   Ties , 69  
   Time-concentration studies , 418  
   Time-dependent covariate (called &ldquo; T_&rdquo; in 
</p>
<p>SPSS) , 373, 374  
   Time-dependent Cox regression , 371, 373  
   Time-dependent factor analysis , 121  
</p>
<p>   Time series , 211  
   Total score = best bit score if some amino acid 
</p>
<p>pairs , 460  
   Traditional multivariate analysis of variance 
</p>
<p>(MANOVA) , 165  
   Traditional multivariate methods , 150  
   Trained Decision Trees , 47&ndash;52  
   Training , 396, 398  
</p>
<p> and outcome prediction , 310  
 sample , 47, 310, 311, 327, 328, 334  
</p>
<p>   Transform , 32, 124, 126, 133, 150, 177  
   Transient state , 347&ndash;350  
   Transition matrix , 345, 349  
   Trends , 211  
</p>
<p> to signifi cance , 85  
   Trend test , 265&ndash;269  
</p>
<p> for binary data , 265  
 for continuous data , 265  
</p>
<p>   T-tests , 94, 146  
   Two by two interaction matrix , 77, 79  
   Two-dimensional clustering , 8, 11, 15  
   Two-stage least squares , 207&ndash;210  
   Two Step Cluster Analysis , 32  
   Two step clustering , 13&ndash;15  
   Type and c5.0 nodes , 388, 389  
   Type I errors , 276, 363  
   Type node , 403, 404, 411, 412  
   Typology of medical data , 53&ndash;61, 67  
</p>
<p>    U 
</p>
<p>  UCL.    See  Upper confi dence limits (UCL) 
   Uebersax J. Free Software LTA (latent trait 
</p>
<p>analysis)-2 , 367, 369  
   Unadjusted p-values , 472  
   Uncertainty coeffi cient , 63&ndash;65  
   Univariate , 492  
   Univariate analyses , 277  
   Univariate multinomial logistic regression , 36  
   Univariate multiple linear regression , 140  
   Universal space of the imput variable , 379  
   Unpaired data , 123  
   Unpaired observations , 271  
   Unpaired t-tests , 471, 472  
   Unregularized , 146  
   Unrotated factor solution , 139  
   Unstandardized covariances , 304, 305  
   Upper and lower specifi cation limits , 107  
   Upper confi dence limits (UCL) , 108, 215  
   US National Center of Biotechnology 
</p>
<p>Information (NCBI) , 459  
   Utilities , 14, 33, 115, 117, 120, 125, 128, 135, 
</p>
<p>152, 173, 178, 311, 316, 330, 399  
   Utility scores , 362  
</p>
<p>Index</p>
<p/>
</div>
<div class="page"><p/>
<p>516
</p>
<p>    V 
</p>
<p>  Variance components , 219&ndash;222  
   Variance estimate , 220&ndash;222  
   Variance stabilization with Fisher 
</p>
<p>transformation , 408  
   Varimax , 139  
   Varying incident risks , 229&ndash;232  
   Varying predictors , 195&ndash;201  
   Vassarstats calculator , 483  
   View space , 181  
   Violations of the set control rules , 108  
   Visualization of health processes , 35&ndash;46  
</p>
<p>    W 
</p>
<p>  Web node , 387, 388  
   Weighted least squares (WLS) , 155&ndash;158  
</p>
<p> modeling , 158  
 regression , 133  
</p>
<p>   Weighted likelihood methodology , 439  
   Weighted population estimates , 313  
   Weka , 300  
   Weka Predictor node , 442  
   Weka software 3.6 for windows , 441  
   Welch&rsquo;s test , 471, 473  
   winRAR ZIP fi les , 186, 192  
   Within-Subject Factor Name , 272  
   WLS.    See  Weighted least squares (WLS) 
</p>
<p>   Workfl ow , 441, 442, 466, 467  
   Workfl ow editor , 37, 441, 442, 
</p>
<p>466, 467  
   Workfl ow in knime , 441, 447, 466  
   WPP superfamily , 462, 463  
   www.john-uebersax.com/stat/Ital.htm 358 , 
</p>
<p>367, 369  
   www.mccallum-layton.co.uk/ , 477  
   www.wessa.net/rwasp , 140&ndash;141  
</p>
<p>    X 
</p>
<p>  X-aggregator , 467, 468  
   XML.    See  eXtended Markup 
</p>
<p>Language (XML) 
   X-partitioner , 467  
   $XR-outcome , 408, 416  
   Xuru, the world largest business network 
</p>
<p>based in Auckland CA, USA , 418  
</p>
<p>    Z 
</p>
<p>  Zero (0) matrix , 347  
   ZIP (compressed fi le that can be unzipped) 
</p>
<p>fi le , 204  
   z-test , 248, 482  
   z-values , 279  
</p>
<p> of a normal Gaussian curve , 367         
</p>
<p>Index</p>
<p/>
</div>
<ul>	<li>Preface</li>
	<li>Contents</li>
	<li>Part I: Cluster and Classification Models</li>
<ul>	<li>Chapter 1: Hierarchical Clustering and&nbsp;K-Means Clustering to&nbsp;Identify Subgroups in&nbsp;Surveys (50 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Hierarchical Cluster Analysis</li>
	<li> K-Means Cluster Analysis</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 2: Density-Based Clustering to&nbsp;Identify Outlier Groups in&nbsp;Otherwise Homogeneous Data (50 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Density-Based Cluster Analysis</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 3: Two Step Clustering to&nbsp;Identify Subgroups and&nbsp;Predict Subgroup Memberships in&nbsp;Individual Future Patients (120 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> The Computer Teaches Itself to&nbsp;Make Predictions</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 4: Nearest Neighbors for&nbsp;Classifying New Medicines (2 New and&nbsp;25 Old Opioids)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 5: Predicting High-Risk-Bin Memberships (1,445&nbsp;Families)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Example</li>
	<li> Optimal Binning</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 6: Predicting Outlier Memberships (2,000 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 7: Data Mining for&nbsp;Visualization of&nbsp;Health Processes (150 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Knime Data Miner</li>
	<li> Knime Workflow</li>
	<li> Box and&nbsp;Whiskers Plots</li>
	<li> Lift Chart</li>
	<li> Histogram</li>
	<li> Line Plot</li>
	<li> Matrix of&nbsp;Scatter Plots</li>
	<li> Parallel Coordinates</li>
	<li> Hierarchical Cluster Analysis with&nbsp;SOTA (Self Organizing Tree Algorithm)</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 8: Trained Decision Trees for&nbsp;a&nbsp;More Meaningful Accuracy (150 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Downloading the&nbsp;Knime Data Miner</li>
	<li> Knime Workflow</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 9: Typology of&nbsp;Medical Data (51 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
<ul>	<li>Nominal Variable</li>
	<li> Ordinal Variable</li>
	<li> Scale Variable</li>
</ul>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 10: Predictions from&nbsp;Nominal Clinical Data (450 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 11: Predictions from&nbsp;Ordinal Clinical Data (450 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 12: Assessing Relative Health Risks (3,000&nbsp;Subjects)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 13: Measuring Agreement (30 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 14: Column Proportions for&nbsp;Testing Differences Between Outcome Scores (450 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 15: Pivoting Trays and&nbsp;Tables for&nbsp;Improved Analysis of&nbsp;Multidimensional Data (450 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 16: Online Analytical Procedure Cubes, a&nbsp;More Rapid Approach to&nbsp;Analyzing Frequencies (450 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 17: Restructure Data Wizard for&nbsp;Data Classified the&nbsp;Wrong Way (20 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 18: Control Charts for&nbsp;Quality Control of&nbsp;Medicines (164 Tablet Desintegration Times)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
</ul>
	<li>Part II: (Log) Linear Models</li>
<ul>	<li>Chapter 19: Linear, Logistic, and&nbsp;Cox Regression for&nbsp;Outcome Prediction with&nbsp;Unpaired Data (20, 55, and&nbsp;60 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Linear Regression, the&nbsp;Computer Teaches Itself to&nbsp;Make Predictions</li>
	<li> Conclusion</li>
	<li> Note</li>
	<li> Logistic Regression, the&nbsp;Computer Teaches Itself to&nbsp;Make Predictions</li>
	<li> Conclusion</li>
	<li> Note</li>
	<li> Cox Regression, the&nbsp;Computer Teaches Itself to&nbsp;Make Predictions</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 20: Generalized Linear Models for&nbsp;Outcome Prediction with&nbsp;Paired Data (100 Patients and&nbsp;139 Physicians)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Generalized Linear Modeling, the&nbsp;Computer Teaches Itself to&nbsp;Make Predictions</li>
	<li> Conclusion</li>
	<li> Generalized Estimation Equations, the&nbsp;Computer Teaches Itself to&nbsp;Make Predictions</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 21: Generalized Linear Models Event-Rates (50 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Example</li>
	<li> The Computer Teaches Itself to&nbsp;Make Predictions</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 22: Factor Analysis and&nbsp;Partial Least Squares (PLS) for&nbsp;Complex-Data Reduction (250 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Factor Analysis</li>
	<li> Partial Least Squares Analysis (PLS)</li>
	<li> Traditional Linear Regression</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 23: Optimal Scaling of&nbsp;High-Sensitivity Analysis of&nbsp;Health Predictors (250 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Traditional Multiple Linear Regression</li>
	<li> Optimal Scaling Without Regularization</li>
	<li> Optimal Scaling With&nbsp;Ridge Regression</li>
	<li> Optimal Scaling With&nbsp;Lasso Regression</li>
	<li> Optimal Scaling With&nbsp;Elastic Net Regression</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 24: Discriminant Analysis for&nbsp;Making a&nbsp;Diagnosis from&nbsp;Multiple Outcomes (45 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> The Computer Teaches Itself to&nbsp;Make Predictions</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 25: Weighted Least Squares for&nbsp;Adjusting Efficacy Data with&nbsp;Inconsistent Spread (78 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Weighted Least Squares</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 26: Partial Correlations for&nbsp;Removing Interaction Effects from&nbsp;Efficacy Data (64 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Partial Correlations</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 27: Canonical Regression for&nbsp;Overall Statistics of&nbsp;Multivariate Data (250 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Canonical Regression</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 28: Multinomial Regression for&nbsp;Outcome Categories (55 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> The Computer Teaches Itself to&nbsp;Make Predictions</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 29: Various Methods for&nbsp;Analyzing Predictor Categories (60 and&nbsp;30 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Questions</li>
	<li> Example 1</li>
	<li> Example 2</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 30: Random Intercept Models for&nbsp;Both Outcome and&nbsp;Predictor Categories (55 patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 31: Automatic Regression for&nbsp;Maximizing Linear Relationships (55 patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Data Example</li>
	<li> The Computer Teaches Itself to&nbsp;Make Predictions</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 32: Simulation Models for&nbsp;Varying Predictors (9,000 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Instead of&nbsp;Traditional Means and&nbsp;Standard Deviations, Monte Carlo Simulations of&nbsp;the&nbsp;Input and&nbsp;Outcome Variables are Used to&nbsp;Model the&nbsp;Data. This Enhances Precision, Particularly, With&nbsp;non-Normal Data</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 33: Generalized Linear Mixed Models for&nbsp;Outcome Prediction from&nbsp;Mixed Data (20 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 34: Two-Stage Least Squares (35 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 35: Autoregressive Models for&nbsp;Longitudinal Data (120 Mean Monthly Population Records)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 36: Variance Components for&nbsp;Assessing the&nbsp;Magnitude of&nbsp;Random Effects (40 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 37: Ordinal Scaling for&nbsp;Clinical Scores with&nbsp;Inconsistent Intervals (900 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Questions</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 38: Loglinear Models for&nbsp;Assessing Incident Rates with&nbsp;Varying Incident Risks (12 Populations)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 39: Loglinear Modeling for&nbsp;Outcome Categories (445 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 40: Heterogeneity in&nbsp;Clinical Research: Mechanisms Responsible (20 Studies)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 41: Performance Evaluation of&nbsp;Novel Diagnostic Tests (650 and&nbsp;588 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
<ul>	<li>Binary Logistic Regression</li>
	<li> C-Statistics</li>
</ul>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 42: Quantile-Quantile Plots, a&nbsp;Good Start for&nbsp;Looking at Your Medical Data (50 Cholesterol Measurements and&nbsp;58 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Q-Q Plots for&nbsp;Assessing Departures from&nbsp;Normality</li>
	<li> Q-Q Plots as&nbsp;Diagnostics for&nbsp;Fitting Data to&nbsp;Normal (and Other Theoretical) Distributions</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 43: Rate Analysis of&nbsp;Medical Data Better than Risk Analysis (52 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 44: Trend Tests Will Be&nbsp;Statistically Significant if Traditional Tests Are Not (30 and&nbsp;106 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Questions</li>
	<li> Example 1</li>
	<li> Example 2</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 45: Doubly Multivariate Analysis of&nbsp;Variance for&nbsp;Multiple Observations from&nbsp;Multiple Outcome Variables (16 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 46: Probit Models for&nbsp;Estimating Effective Pharmacological Treatment Dosages (14 Tests)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
<ul>	<li>Simple Probit Regression</li>
	<li> Multiple Probit Regression</li>
</ul>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 47: Interval Censored Data Analysis for&nbsp;Assessing Mean Time to&nbsp;Cancer Relapse (51 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 48: Structural Equation Modeling (SEM) with&nbsp;SPSS Analysis of&nbsp;Moment Structures (Amos) for&nbsp;Cause Effect Relationships I&nbsp; (35 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 49: Structural Equation Modeling (SEM) with&nbsp;SPSS Analysis of&nbsp;Moment Structures (Amos) for&nbsp;Cause Effect Relationships in&nbsp;Pharmacodynamic Studies II (35 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
</ul>
	<li>Part III: Rules Models</li>
<ul>	<li>Chapter 50: Neural Networks for&nbsp;Assessing Relationships That Are Typically Nonlinear (90 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> The Computer Teaches Itself to&nbsp;Make Predictions</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 51: Complex Samples Methodologies for&nbsp;Unbiased Sampling (9,678 Persons)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> The Computer Teaches Itself to&nbsp;Predict Current Health Scores from&nbsp;Previous Health Scores</li>
	<li> The Computer Teaches Itself to&nbsp;Predict Individual Odds Ratios of&nbsp;Current Health Scores Versus Previous Health Scores</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 52: Correspondence Analysis for&nbsp;Identifying the&nbsp;Best of&nbsp;Multiple Treatments in&nbsp;Multiple Groups (217 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Correspondence Analysis</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 53: Decision Trees for&nbsp;Decision Analysis (1,004 and&nbsp;953 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Decision Trees with&nbsp;a&nbsp;Binary Outcome</li>
	<li> Decision Trees with&nbsp;a&nbsp;Continuous Outcome</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 54: Multidimensional Scaling for&nbsp;Visualizing Experienced Drug Efficacies (14 Pain-Killers and&nbsp;42 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Proximity Scaling</li>
	<li> Preference Scaling</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 55: Stochastic Processes for&nbsp;Long Term Predictions from&nbsp;Short Term Observations</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Questions</li>
	<li> Example 1</li>
	<li> Example 2</li>
	<li> Example 3</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 56: Optimal Binning for&nbsp;Finding High Risk Cut-&shy;offs (1,445 Families)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Optimal Binning</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 57: Conjoint Analysis for&nbsp;Determining the&nbsp;Most Appreciated Properties of&nbsp;Medicines to&nbsp;Be&nbsp;Developed (15 Physicians)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Constructing an&nbsp;Analysis Plan</li>
	<li> Performing the&nbsp;Final Analysis</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 58: Item Response Modeling for&nbsp;Analyzing Quality of&nbsp;Life with&nbsp;Better Precision (1,000&nbsp;Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 59: Survival Studies with&nbsp;Varying Risks of&nbsp;Dying (50 and&nbsp;60 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Examples</li>
<ul>	<li>Cox Regression with&nbsp;a&nbsp;Time-Dependent Predictor</li>
	<li> Cox Regression with&nbsp;a&nbsp;Segmented Time-Dependent Predictor</li>
</ul>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 60: Fuzzy Logic for&nbsp;Improved Precision of&nbsp;Dose-&shy;Response Data (8 Induction Dosages)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 61: Automatic Data Mining for&nbsp;the&nbsp;Best Treatment of&nbsp;a&nbsp;Disease (90 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Example</li>
	<li> Step 1 Open SPSS Modeler</li>
	<li> Step 2 The&nbsp;Distribution Node</li>
	<li> Step 3 The&nbsp;Data Audit Node</li>
	<li> Step 4 The&nbsp;Plot Node</li>
	<li> Step 5 The&nbsp;Web Node</li>
	<li> Step 6 The&nbsp;Type and&nbsp;c5.0 Nodes</li>
	<li> Step 7 The&nbsp;Output Node</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 62: Pareto Charts for&nbsp;Identifying the&nbsp;Main Factors of&nbsp;Multifactorial Outcomes (2,000 Admissions to&nbsp;Hospital)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 63: Radial Basis Neural Networks for&nbsp;Multidimensional Gaussian Data (90 Persons)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Example</li>
	<li> The Computer Teaches Itself to&nbsp;Make Predictions</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 64: Automatic Modeling of&nbsp;Drug Efficacy Prediction (250 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Example</li>
	<li> Step 1 Open SPSS Modeler (14.2)</li>
	<li> Step 2 The&nbsp;Statistics File Node</li>
	<li> Step 3 The&nbsp;Type Node</li>
	<li> Step 4 The&nbsp;Auto Numeric Node</li>
	<li> Step 5 The&nbsp;Expert Node</li>
	<li> Step 6 The&nbsp;Settings Tab</li>
	<li> Step 7 The&nbsp;Analysis Node</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 65: Automatic Modeling for&nbsp;Clinical Event Prediction (200 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Example</li>
	<li> Step 1 Open SPSS Modeler (14.2)</li>
	<li> Step 2 The&nbsp;Statistics File Node</li>
	<li> Step 3 The&nbsp;Type Node</li>
	<li> Step 4 The&nbsp;Auto Classifier Node</li>
	<li> Step 5 The&nbsp;Expert Tab</li>
	<li> Step 6 The&nbsp;Settings Tab</li>
	<li> Step 7 The&nbsp;Analysis Node</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 66: Automatic Newton Modeling in&nbsp;Clinical Pharmacology (15 Alfentanil Dosages, 15 Quinidine Time-Concentration Relationships)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Examples</li>
<ul>	<li>Dose-Effectiveness Study</li>
	<li> Time-Concentration Study</li>
</ul>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 67: Spectral Plots for&nbsp;High Sensitivity Assessment of&nbsp;Periodicity (6 Years&rsquo; Monthly C Reactive Protein Levels)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 68: Runs Test for&nbsp;Identifying Best Regression Models (21 Estimates of&nbsp;Quantity and&nbsp;Quality of&nbsp;Patient Care)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 69: Evolutionary Operations for&nbsp;Process Improvement (8 Operation Room Air Condition Settings)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 70: Bayesian Networks for&nbsp;Cause Effect Modeling (600 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Binary Logistic Regression in&nbsp;SPSS</li>
	<li> Konstanz Information Miner (Knime)</li>
	<li> Knime Workflow</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 71: Support Vector Machines for&nbsp;Imperfect Nonlinear Data (200 Patients with&nbsp;Sepsis)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Knime Data Miner</li>
	<li> Knime Workflow</li>
	<li> File Reader Node</li>
	<li> The Nodes X-Partitioner, svm Learner, svm&nbsp;Predictor,&nbsp;X-Aggregator</li>
	<li> Error Rates</li>
	<li> Prediction Table</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 72: Multiple Response Sets for&nbsp;Visualizing Clinical Data Trends (811 Patient Visits)</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 73: Protein and&nbsp;DNA Sequence Mining</li>
<ul>	<li>General Purpose</li>
	<li> Specific Scientific Question</li>
	<li> Data Base Systems on&nbsp;the&nbsp;Internet</li>
	<li> Example 1</li>
	<li> Example 2</li>
	<li> Example 3</li>
	<li> Example 4</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 74: Iteration Methods for&nbsp;Crossvalidations (150 Patients with&nbsp;Pneumonia)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Downloading the&nbsp;Knime Data Miner</li>
	<li> Knime Workflow</li>
	<li> Crossvalidation</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 75: Testing Parallel-Groups with&nbsp;Different Sample Sizes and&nbsp;Variances (5 Parallel-Group Studies)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Examples</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 76: Association Rules Between Exposure and&nbsp;Outcome (50 and&nbsp;60 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
<ul>	<li>Example One</li>
	<li> Example Two</li>
</ul>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 77: Confidence Intervals for&nbsp;Proportions and&nbsp;Differences in&nbsp;Proportions (100 and&nbsp;75 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
<ul>	<li>Confidence Intervals of&nbsp;Proportions</li>
	<li> Confidence Intervals of&nbsp;Differences in&nbsp;Proportions</li>
</ul>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 78: Ratio Statistics for&nbsp;Efficacy Analysis of&nbsp;New Drugs (50 Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 79: Fifth Order Polynomes of&nbsp;Circadian Rhythms (1 Patient with&nbsp;Hypertension)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
	<li>Chapter 80: Gamma Distribution for&nbsp;Estimating the&nbsp;Predictors of&nbsp;Medical Outcome Scores&nbsp;(110&nbsp;Patients)</li>
<ul>	<li>General Purpose</li>
	<li> Primary Scientific Question</li>
	<li> Example</li>
	<li> Conclusion</li>
	<li> Note</li>
</ul>
</ul>
	<li>Index</li>
</ul>
</body></html>