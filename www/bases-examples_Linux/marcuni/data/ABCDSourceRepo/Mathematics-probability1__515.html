<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Untitled</title>
</head>
<body><div class="page"><p/>
</div>
<div class="page"><p/>
<p>INTUITIVE PROBABILITY
</p>
<p>AND
</p>
<p>RANDOM PROCESSES
</p>
<p>USING MATLAB&reg;</p>
<p/>
</div>
<div class="page"><p/>
</div>
<div class="page"><p/>
<p>INTUITIVE PROBABILITY
</p>
<p>AND
</p>
<p>RANDOM PROCESSES
</p>
<p>USING MATLAB&reg;
</p>
<p>STEVEN M. KAY
University ofRhode Island
</p>
<p>~ Springer</p>
<p/>
</div>
<div class="page"><p/>
<p>Author: 
</p>
<p>Steven M. Kay 
University of Rhode Island 
Dept. of Electrical  &amp; Computer Engineering 
Kingston, RI 02881 
 
</p>
<p> 
</p>
<p> 
</p>
<p> 
</p>
<p> 
</p>
<p> 
</p>
<p> 
</p>
<p> 
</p>
<p> 
</p>
<p> 
</p>
<p> 
 2006 Steven M. Kay (4th corrected version of the 
</p>
<p>All rights reserved. This work may not be translated or copied in whole or in part without 
the written permission of the publisher (Springer Science+Business Media, LLC, 233 Spring 
Street, New York, NY 10013, USA), except for brief excerpts in connection with reviews or 
scholarly analysis. Use in connection with any form of information storage and retrieval, 
electronic adaptation, computer software, or by similar or dissimilar methodology now 
known or hereafter developed is forbidden. 
The use in this publication of trade names, trademarks, service marks and similar terms, 
even if they are not identified as such, is not to be taken as an expression of opinion as to 
whether or not they are subject to proprietary rights. 
 
 
 
</p>
<p> 
</p>
<p> 
 
Printed on acid free paper 
 
9  8  7  6  5                                    
 
springer.com 
</p>
<p>5th printing (2012)) 
</p>
<p>ISBN 978-0-387-24157-9 e-ISBN 978-0-387-24158-6
 
Library of Congress Control Number: 2005051721 </p>
<p/>
</div>
<div class="page"><p/>
<p>To my wife
</p>
<p>Cindy,
</p>
<p>whose love and support
</p>
<p>are without measure
</p>
<p>and to my daughters
</p>
<p>Lisa and Ashley,
</p>
<p>who are a source of joy</p>
<p/>
</div>
<div class="page"><p/>
<p>NOTE TO INSTRUCTORS
</p>
<p>As an aid to instructors interested in using this book for a course, the solutions to
</p>
<p>the exercises are available in electronic form. They may be obtained by contacting
</p>
<p>the author at kay@ele.urLedu.</p>
<p/>
</div>
<div class="page"><p/>
<p>Preface
</p>
<p>The subject of probability and random processes is an important one for a variety of
</p>
<p>disciplines. Yet, in the author's experience, a first exposure to this subject can cause
</p>
<p>difficulty in assimilating the material and even more so in applying it to practical
</p>
<p>problems of interest. The goal of this textbook is to lessen this difficulty. To do
</p>
<p>so we have chosen to present the material with an emphasis on conceptualization.
</p>
<p>As defined by Webster, a concept is "an abstract or generic idea generalized from
</p>
<p>particular instances." This embodies the notion that the "idea" is something we
</p>
<p>have formulated based on our past experience. This is in contrast to a theorem,
</p>
<p>which according to Webster is "an idea accepted or proposed as a demonstrable
</p>
<p>truth". A theorem then is the result of many oth er persons' past experiences, which
</p>
<p>mayor may not coincide with our own. In presenting the material we prefer to
</p>
<p>first present "part icular instances" or examples and then generalize using a defi-
</p>
<p>nition/theorem. Many textbooks use the opposite sequence, which undeniably is
</p>
<p>cleaner and more compact, but omits the motivating examples that initially led
</p>
<p>to the definition/theorem. Furthermore, in using the definition/theorem-first ap-
</p>
<p>proach, for the sake of mathematical correctness multiple concepts must be presented
</p>
<p>at once. This is in opposition to human learning for which "under most conditions,
</p>
<p>the greater the number of attributes to be bounded into a single concept , the more
</p>
<p>difficult the learning becomes" 1 . The philosophical approach of specific examples
</p>
<p>followed by generalizations is embodied in this textbook. It is hoped that it will
</p>
<p>provide an alternative to the more traditional approach for exploring the subject of
</p>
<p>probability and random processes.
</p>
<p>To provide motivating examples we have chosen to use MATLAB2 , which is a
</p>
<p>very versatile scientific programming language. Our own engineering students at the
</p>
<p>University of Rhode Island are exposed to MATLAB as freshmen and continue to use
</p>
<p>it throughout their curriculum. Graduate students who have not been previously
</p>
<p>introduced to MATLAB easily master its use. The pedagogical utility of using
</p>
<p>MATLAB is that:
</p>
<p>1. Specific computer generated examples can be constructed to provide motivation
</p>
<p>for the more general concepts to follow.
</p>
<p>lEli Sal t z, Th e Cogniti ve Basis of Human Learning, Dorsey Press, Homewood, IL, 1971.
</p>
<p>2Registered trademark of TheMathWorks, Inc.</p>
<p/>
</div>
<div class="page"><p/>
<p>Vlll
</p>
<p>2. Inclusion of computer code within the text allows the reader to interpret the
</p>
<p>mathematical equations more easily by seeing them in an alternative form.
</p>
<p>3. Homework problems based on computer simulations can be assigned to illustrate
</p>
<p>and reinforce important concepts.
</p>
<p>4. Computer experimentation by the reader is easily accomplished.
</p>
<p>5. Typical results of probabilistic-based algorithms can be illustrated.
</p>
<p>6. Real-world problems can be described and "solved" by implementing the solution
</p>
<p>in code.
</p>
<p>Many MATLAB programs and code segments have been included in the book. In
</p>
<p>fact, most of the figures were generated using MATLAB . The programs and code
</p>
<p>segments listed within the book are available in the file pr'obbook.matLab.code . tex,
</p>
<p>which can be found at http://www.ele.uri.edu/faculty/kay/New%20web/Books.htm.
</p>
<p>The use of MATLAB, along with a brief description of its syntax, is introduced early
</p>
<p>in the book in Chapter 2. It is then immediately applied to simulate outcomes of
</p>
<p>random variables and to estimate various quantities such as means, variances, prob-
</p>
<p>ability mass functions, etc. even though these concepts have not as yet been formally
</p>
<p>introduced. This chapter sequencing is purposeful and is meant to expose the reader
</p>
<p>to some of the main concepts that will follow in more detail later. In addition,
</p>
<p>the reader will then immediately be able to simulate random phenomena to learn
</p>
<p>through doing, in accordance with our philosophy. In summary, we believe that
</p>
<p>the incorporation of MATLAB into the study of probability and random processes
</p>
<p>provides a "hands-on" approach to the subject and promotes better understanding.
</p>
<p>Other pedagogical features of this textbook are the discussion of discrete random
</p>
<p>variables first to allow easier assimilation of the concepts followed by a parallel dis-
</p>
<p>cussion for continuous random variables. Although this entails some redundancy, we
</p>
<p>have found less confusion on the part of the student using this approach. In a similar
</p>
<p>vein, we first discuss scalar random variables, then bivariate (or two-dimensional)
</p>
<p>random variables, and finally N-dimensional random variables, reserving separate
</p>
<p>chapters for each. All chapters, except for the introductory chapter, begin with a
</p>
<p>summary of the important concepts and point to the main formulas of the chap-
</p>
<p>ter, and end with a real-world example. The latter illustrates the utility of the
</p>
<p>material just studied and provides a powerful motivation for further study. It also
</p>
<p>will, hopefully, answer the ubiquitous question "Why do we have to study this?" .
</p>
<p>We have tried to include real-world examples from many disciplines to indicate the
</p>
<p>wide applicability of the material studied. There are numerous problems in each
</p>
<p>chapter to enhance understanding with some answers listed in Appendix E. The
</p>
<p>problems consist of four types. There are "formula" problems, which are simple ap-
</p>
<p>plications of the important formulas of the chapter; "word" problems, which require
</p>
<p>a problem-solving capability; and "theoretical" problems, which are more abstract</p>
<p/>
</div>
<div class="page"><p/>
<p>IX
</p>
<p>and mathematically demanding; and finally, there are "computer" problems, which
</p>
<p>are either computer simulations or involve the application of computers to facilitate
</p>
<p>analytical solutions. A complete solutions manual for all the problems is available
</p>
<p>to instructors from the author upon request. Finally, we have provided warnings on
</p>
<p>how to avoid common errors as well as in-line explanations of equations within the
</p>
<p>derivations for clarification.
</p>
<p>The book was written mainly to be used as a first-year graduate level course
</p>
<p>in probability and random processes. As such, we assume that the student has
</p>
<p>had some exposure to basic probability and therefore Chapters 3-11 can serve as
</p>
<p>a review and a summary of the notation. We then will cover Chapters 12-15 on
</p>
<p>probability and selected chapters from Chapters 16-22 on random processes. This
</p>
<p>book can also be used as a self-contained introduction to probability at the senior
</p>
<p>undergraduate or graduate level. It is then suggested that Chapters 1-7, 10, 11 be
</p>
<p>covered. Finally, this book is suitable for self-study and so should be useful to the
</p>
<p>practitioner as well as the student. The necessary background that has been assumed
</p>
<p>is a knowledge of calculus (a review is included in Appendix B) ; some linear/matrix
</p>
<p>algebra (a review is provided in Appendix C); and linear systems, which is necessary
</p>
<p>only for Chapters 18-20 (although Appendix D has been provided to summarize and
</p>
<p>illustrate the important concepts).
</p>
<p>The author would like to acknowledge the contributions of the many people who
</p>
<p>over the years have provided stimulating discussions of teaching and research prob-
</p>
<p>lems and opportunities to apply the results of that research. Thanks are due to my
</p>
<p>colleagues L. Jackson, R. Kumaresan, L. Pakula, and P. Swaszek of the University
</p>
<p>of Rhode Island. A debt of gratitude is owed to all my current and former graduate
</p>
<p>students. They have contributed to the final manuscript through many hours of
</p>
<p>pedagogical and research discussions as well as by their specific comments and ques-
</p>
<p>tions. In particular, Lin Huang and Cuichun Xu proofread the entire manuscript and
</p>
<p>helped with the problem solutions, while Russ Costa provided feedback. Lin Huang
</p>
<p>also aided with the intricacies of LaTex while Lisa Kay and Jason Berry helped with
</p>
<p>the artwork and to demystify the workings of Adobe Illustrator 10.3 The author
</p>
<p>is indebted to the many agencies and program managers who have sponsored his
</p>
<p>research, including the Naval Undersea Warfare Center, the Naval Air Warfare Cen-
</p>
<p>ter, the Air Force Office of Scientific Research, and the Office of Naval Research.
</p>
<p>As always, the author welcomes comments and corrections, which can be sent to
</p>
<p>kay@ele.uri.edu.
</p>
<p>Steven M. Kay
</p>
<p>University of Rhode Island
</p>
<p>Kingston, RI 02881
</p>
<p>3Registered trademark of Adobe Systems Inc.</p>
<p/>
</div>
<div class="page"><p/>
</div>
<div class="page"><p/>
<p>Contents
</p>
<p>Preface vii
</p>
<p>1 Introduction 1
</p>
<p>1.1 What Is Probability? . . . . . . 1
</p>
<p>1.2 Types of Probability Problems 3
</p>
<p>1.3 Probabilistic Modeling . . . . . 4
</p>
<p>1.4 Analysis versus Computer Simulation 7
</p>
<p>1.5 Some Notes to the Reader 8
</p>
<p>References . 9
</p>
<p>Problems 10
</p>
<p>2 Computer Simulation 13
</p>
<p>2.1 Introduction . . .. .... . . .. 13
</p>
<p>2.2 Summary . . . . . . . . . . . . . 13
</p>
<p>2.3 Why Use Computer Simulation? 14
</p>
<p>2.4 Computer Simulation of Random Phenomena 17
</p>
<p>2.5 Determining Characteristics of Random Var iables . 18
</p>
<p>2.6 Real-World Example - Digit al Communications . 24
</p>
<p>References . . . . . . . . . . . . . 26
</p>
<p>Problems 26
</p>
<p>2A Brief Introducti on to MATLAB . 31
</p>
<p>3 Basic Probability 37
</p>
<p>3.1 Introduction. . 37
</p>
<p>3.2 Summary . . . 37
</p>
<p>3.3 Review of Set Theory 38
</p>
<p>3.4 Assigning and Determining Probabilities. 43
</p>
<p>3.5 Properties of the Probabili ty Function . . 48
</p>
<p>3.6 Probabilities for Continuous Sample Spaces 52
</p>
<p>3.7 Prob abiliti es for Finite Sample Spaces - Equally Likely Ou tcomes 54
</p>
<p>3.8 Combinatorics 55
</p>
<p>3.9 Binomial Probability Law . . . . . . . . . . . . . . . . . . . . . .. 62</p>
<p/>
</div>
<div class="page"><p/>
<p>Xll
</p>
<p>3.10 Real-World Example - Quality Control
</p>
<p>References .
</p>
<p>Problems .
</p>
<p>4 Conditional Probability
</p>
<p>4.1 Introduction. . . . . . . . . .. ... .. ...
</p>
<p>4.2 Summary . . . . . . . . . . . . . . . . . . . .
</p>
<p>4.3 Joint Events and the Conditional Prob ability
</p>
<p>4.4 St atistically Independent Event s
</p>
<p>4.5 Bayes' Theorem . . . . . . . . . . . . . . . .
</p>
<p>4.6 Multiple Exp eriment s .
</p>
<p>4.7 Real-World Example - Cluster Recognition
</p>
<p>References .
</p>
<p>Problems .
</p>
<p>5 Discrete Random Variables
</p>
<p>5.1 Introduction .
</p>
<p>5.2 Summary . . . . . . . . . . . . . . . . .
</p>
<p>5.3 Definition of Discrete Random Variable
</p>
<p>5.4 Probability of Discrete Random Variables
</p>
<p>5.5 Important Probability Mass Functions . .
</p>
<p>5.6 Approximation of Binomial PMF by Poisson P MF
</p>
<p>5.7 Transformation of Discrete Random Variables .
</p>
<p>5.8 Cumulati ve Distributi on Funct ion .
</p>
<p>5.9 Computer Simul ation .
</p>
<p>5.10 Real-World Example - Servicing Customers
</p>
<p>References .
</p>
<p>Problems .
</p>
<p>6 Expected Values for Discrete Random Variables
</p>
<p>6.1 Introduction .
</p>
<p>6.2 Summary . . . . . . . . . . . . . . . . . . . . . . .
</p>
<p>6.3 Determining Averages from the PMF .
</p>
<p>6.4 Expected Values of Some Important Random Vari ables
</p>
<p>6.5 Expected Value for a Function of a Random Vari able.
</p>
<p>6.6 Variance and Moments of a Random Variable
</p>
<p>6.7 Characteristic Functions .
</p>
<p>6.8 Estimating Means and Varian ces .
</p>
<p>6.9 Real- World Example - Dat a Compression
</p>
<p>References . . . . . . . . . . . .
</p>
<p>Problems .
</p>
<p>6A Derivation of E [g(X )] Formula .
</p>
<p>6B MAT LAB Code Used to Estimate Mean and Variance
</p>
<p>CONTENTS
</p>
<p>64
</p>
<p>66
</p>
<p>66
</p>
<p>73
</p>
<p>73
</p>
<p>73
</p>
<p>74
</p>
<p>83
</p>
<p>86
</p>
<p>89
</p>
<p>97
</p>
<p>100
</p>
<p>100
</p>
<p>105
</p>
<p>105
</p>
<p>105
</p>
<p>106
</p>
<p>108
</p>
<p>111
113
115
117
122
</p>
<p>124
</p>
<p>128
</p>
<p>128
</p>
<p>133
</p>
<p>133
</p>
<p>133
</p>
<p>134
</p>
<p>137
</p>
<p>140
</p>
<p>143
</p>
<p>147
</p>
<p>153
</p>
<p>155
</p>
<p>157
</p>
<p>158
</p>
<p>163
</p>
<p>165</p>
<p/>
</div>
<div class="page"><p/>
<p>CONTENTS
</p>
<p>7 Multiple Discrete Random Variables
</p>
<p>7.1 Introduction .
</p>
<p>7.2 Summary .
</p>
<p>7.3 Jointly Distributed Random Variables
</p>
<p>7.4 Marginal PMFs and CDFs .
</p>
<p>7.5 Independence of Multiple Random Variables.
</p>
<p>7.6 Transformations of Multiple Random Variables
</p>
<p>7.7 Expected Values .
</p>
<p>7.8 Joint Moments .
</p>
<p>7.9 Prediction of a Random Variable Outcome .
</p>
<p>7.10 Joint Characteristic Functions .
</p>
<p>7.11 Computer Simulation of Random Vectors .
</p>
<p>7.12 Real-World Example - Assessing Health Risks .
</p>
<p>References . . . . . . . . . . . . . . . . . . . .
</p>
<p>Problems .
</p>
<p>7A Derivation of the Cauchy-Schwarz Inequality
</p>
<p>8 Conditional Probabili t y Mass Functions
</p>
<p>8.1 Introduction .
</p>
<p>8.2 Summary . . . . . . . . . . . . . . . . .
</p>
<p>8.3 Conditional Probability Mass Function .
</p>
<p>8.4 Joint, Conditional, and Marginal P MFs
</p>
<p>8.5 Simplifying Probability Calculations using Conditioning
</p>
<p>8.6 Mean of the Conditional PMF . . . . . . . . . . . .
</p>
<p>8.7 Computer Simulation Based on Conditioning . . .
</p>
<p>8.8 Real-World Example - Mod eling Human Learning
</p>
<p>References .
</p>
<p>Problems .
</p>
<p>9 Discrete N -D im ension a l Random Variables
</p>
<p>9.1 Introduct ion .
</p>
<p>9.2 Summary . . . . . . . . . . . . . . . . . . .
</p>
<p>9.3 Random Vectors and Probability Mass Functions
</p>
<p>9.4 Transformations .
</p>
<p>9.5 Expected Values .
</p>
<p>9.6 Joint Moments and the Characteristic Function
</p>
<p>9.7 Conditional Probability Mass Functions .
</p>
<p>9.8 Computer Simulation of Random Vectors
</p>
<p>9.9 Real-World Example - Image Coding .
</p>
<p>References .
</p>
<p>Problems .
</p>
<p>xiii
</p>
<p>167
</p>
<p>167
</p>
<p>168
</p>
<p>169
</p>
<p>174
</p>
<p>178
</p>
<p>181
</p>
<p>186
</p>
<p>189
</p>
<p>192
</p>
<p>198
</p>
<p>200
</p>
<p>202
</p>
<p>204
</p>
<p>204
</p>
<p>213
</p>
<p>215
</p>
<p>215
</p>
<p>216
</p>
<p>217
</p>
<p>220
</p>
<p>225
</p>
<p>229
</p>
<p>235
</p>
<p>237
</p>
<p>240
</p>
<p>240
</p>
<p>247
</p>
<p>247
</p>
<p>247
</p>
<p>248
</p>
<p>251
</p>
<p>255
</p>
<p>265
</p>
<p>266
</p>
<p>269
</p>
<p>272
</p>
<p>277
</p>
<p>277</p>
<p/>
</div>
<div class="page"><p/>
<p>XIV CONTENTS
</p>
<p>10 Continuous Random Variables 285
</p>
<p>10.1 Introduction. . . . . . . . . . . . . . . . . . . 285
</p>
<p>10.2 Summary . . . . . . . . . . . . . . . . . . . . 286
</p>
<p>10.3 Definition of a Continuous Random Vari able 287
</p>
<p>10.4 The PDF and Its Properties . . . . 293
</p>
<p>10.5 Important PDFs . . . . . . . . . . 295
</p>
<p>10.6 Cumulative Distribution Functions 303
</p>
<p>10.7 Transformations . ... . 311
</p>
<p>10.8 Mixed Random Vari ables . . . . . 317
</p>
<p>10.9 Computer Simulation. . . . . . . . 324
</p>
<p>10.10Real-World Example - Setting Clipping Levels 328
</p>
<p>References. . . . . . . . . . . . . . . . . . . . . 331
</p>
<p>Problems ... . . . . . . . . . . . . . . . . . . 331
</p>
<p>lOA Derivation of PDF of a Transformed Continuous Random Variable 339
</p>
<p>lOB MATLAB Subprograms to Compute Q and Inverse Q Functions . 341
</p>
<p>11 Expected Values for Continuous Random Variables 343
</p>
<p>11.1 Introduction. . . . . . . . . . . . 343
</p>
<p>11.2 Summary . . . . . . . . . . . . . . . . 343
</p>
<p>11.3 Determining the Exp ected Value . . . 344
</p>
<p>11.4 Expected Values for Imp ortant PDFs . 349
</p>
<p>11.5 Expected Valu e for a Function of a Random Vari able. 351
</p>
<p>11.6 Variance and Moments . . . . . . . . . . . . . . . . . 355
</p>
<p>11.7 Characteristic Functions . . . . . . . . . . . . . . . . 359
</p>
<p>11.8 Probability, Moments, and the Chebyshev Inequali ty 361
</p>
<p>11.9 Estimating the Mean and Variance . . . . . . . . 363
</p>
<p>11.10Real-World Example - Critical Software Testing 364
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . 367
</p>
<p>Problems 367
</p>
<p>11A Partial Proof of Expected Value of Function of Continuous Random
</p>
<p>Variable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
</p>
<p>12 Multiple Continuous Random Variables 377
</p>
<p>12.1 Introduction . . . . . . . . . . . . . . . 377
</p>
<p>12.2 Summary . . . . . . . . . . . . . . . . 378
</p>
<p>12.3 Jointly Distributed Random Variables 379
</p>
<p>12.4 Marginal PDFs and the Joint CDF . . 387
</p>
<p>12.5 Independence of Multiple Random Vari ables. 392
</p>
<p>12.6 Transformations 394
</p>
<p>12.7 Expected Values . . . . . . . . . . . . . . 404
</p>
<p>12.8 Joint Moments . . . . . . . . . . . . . . . 412
</p>
<p>12.9 Prediction of Random Variable Outcome. 412
</p>
<p>12.lOJoint Characterist ic Functions . . . . . . . 414</p>
<p/>
</div>
<div class="page"><p/>
<p>CONTENTS xv
</p>
<p>12.11Computer Simulation. . . . . . . . . . . . . . . . . . . 415
</p>
<p>12.12Real-World Example - Optical Character Recognition 419
</p>
<p>References . 423
</p>
<p>Problems 423
</p>
<p>13 Conditional Probability Density Functions 433
</p>
<p>13.1 Int roduct ion . . . 433
</p>
<p>13.2 Summary . . . . . . . . . . . . . . . . . 433
</p>
<p>13.3 Conditional P DF . . . . . . . . . . . . . 434
</p>
<p>13.4 Joint , Conditional, and Marginal PDFs 440
</p>
<p>13.5 Simplifying Probability Calculations Using Conditioning . 444
</p>
<p>13.6 Mean of Conditional P DF . . . . . . . . . . . . . . . . . . 446
</p>
<p>13.7 Computer Simulation of Jointly Continuous Random Variables 447
</p>
<p>13.8 Real-World Example - Retirement Planning . 449
</p>
<p>References . 452
</p>
<p>Problems 452
</p>
<p>14 Continuous N-D im ensional Random Variables 457
</p>
<p>14.1 Introduction . . . . . . . . . 457
</p>
<p>14.2 Summary . . . . . . . . . . 457
</p>
<p>14.3 Random Vectors and P DFs 458
</p>
<p>14.4 Transformations 463
</p>
<p>14.5 Expected Values . . 465
</p>
<p>14.6 Joint Moments and the Characteristic Function 467
</p>
<p>14.7 Conditional P DFs 471
</p>
<p>14.8 Prediction of a Random Variable Outcome . . . 471
</p>
<p>14.9 Computer Simulation of Gaussian Random Vectors 475
</p>
<p>14.10Real-World Example - Signal Detection 476
</p>
<p>References . 479
</p>
<p>Problems 479
</p>
<p>15 Probability and Moment Approximations U sing Limit Theorems 485
</p>
<p>15.1 Introduction . . . . . . . . . . . . . . . . . . 485
</p>
<p>15.2 Summary . . . . . . . . . . . . . . . . . . . 486
</p>
<p>15.3 Convergence and Approximation of a Sum. 486
</p>
<p>15.4 Law of Large Numbers . . . . . . . . . . 487
</p>
<p>15.5 Central Limit Theorem 492
</p>
<p>15.6 Real-World Example - Opinion Polling . 503
</p>
<p>References . . . . . . . . . . . . . . . . . 506
</p>
<p>Problems 507
</p>
<p>15A MATLAB P rogram to Compute Rep eated Convolution of PDFs 511
</p>
<p>15B Proof of Central Limit Theorem . . . . . . . . . . . . . . . . . . 513</p>
<p/>
</div>
<div class="page"><p/>
<p>XVI
</p>
<p>16 Basic Random Processes
</p>
<p>16.1 Int roduct ion .
</p>
<p>16.2 Summary . . . . . . . .
</p>
<p>16.3 What Is a Random P rocess? .
</p>
<p>16.4 Types of Random Processes .
</p>
<p>16.5 The Important Property of Stationarity
</p>
<p>16.6 Some More Examples .
</p>
<p>16.7 Joint Moments .
</p>
<p>16.8 Real-World Example - Statistical Data Analysis
</p>
<p>References .
</p>
<p>Problems .
</p>
<p>17 Wide Sense Stationary Random Processes
</p>
<p>17.1 Introduct ion .
</p>
<p>17.2 Summary .
</p>
<p>17.3 Definit ion of WSS Random P rocess.
</p>
<p>17.4 Autocorrelation Sequence .. .. .
</p>
<p>17.5 Ergodicity and Temporal Averages
</p>
<p>17.6 The Power Spectral Density ... .
</p>
<p>17.7 Estimation of the ACS and P SD .
</p>
<p>17.8 Continuous-Time WSS Random Processes
</p>
<p>17.9 Real-World Example - Random Vibration Test ing
</p>
<p>References .
</p>
<p>Problems .
</p>
<p>CONTENTS
</p>
<p>515
</p>
<p>515
</p>
<p>516
</p>
<p>517
</p>
<p>520
</p>
<p>523
</p>
<p>528
</p>
<p>533
</p>
<p>538
</p>
<p>542
</p>
<p>542
</p>
<p>547
</p>
<p>547
</p>
<p>548
</p>
<p>549
</p>
<p>552
</p>
<p>562
</p>
<p>567
</p>
<p>576
</p>
<p>580
</p>
<p>586
</p>
<p>589
</p>
<p>590
</p>
<p>18 Linear Systems and W ide Sense Stationar y Random Processes 597
</p>
<p>18.1 Introduction . . . . . . . . . . . . . . . . . . . 597
</p>
<p>18.2 Summary . . . . . . . . . . . . . . . . . . . . 598
</p>
<p>18.3 Random Process at Output of Linear System 598
</p>
<p>18.4 Interpretation of the PS D . . . . . . . . . . 607
</p>
<p>18.5 Wiener Fil tering . . . . . . . . . . . . . . . 609
</p>
<p>18.6 Continuous-Ti me Definitions and Formulas 623
</p>
<p>18.7 Real-World Example - Speech Synthesis 626
</p>
<p>References . . . . . . . . . . . . . . . . 630
</p>
<p>Problems 631
</p>
<p>18A Solution for Infinite Length Predictor . 637
</p>
<p>19 M u lt ip le Wide Sense Stationary Random Processes 641
</p>
<p>19.1 Introduction . . . . . . . . . . . . . . . . . . 641
</p>
<p>19.2 Summary . . . . ' .' . . . . . . . . . . . . . 642
</p>
<p>19.3 Jointly Distributed WSS Random Processes 642
</p>
<p>19.4 The Cross-Power Spectral Density . . . . . 647
</p>
<p>19.5 Transformations of Multiple Random Processes 652</p>
<p/>
</div>
<div class="page"><p/>
<p>CONTENTS
</p>
<p>19.6 Continuous-Time Definitions and Formulas
</p>
<p>19.7 Cross-Correlation Sequence Estimation. . .
</p>
<p>19.8 Real-World Example - Br ain Physiology Research
</p>
<p>References .
</p>
<p>Problems .
</p>
<p>20 Gaussian Random Processes
</p>
<p>20.1 Introduction .
</p>
<p>20.2 Summary . . . . . . . . . . . . . . . . . . .
</p>
<p>20.3 Definition of the Gaussian Random Process
</p>
<p>20.4 Linear Transformations .
</p>
<p>20.5 Nonlinear Transformations .
</p>
<p>20.6 Continuous-Time Definitions and Formulas
</p>
<p>20.7 Special Continuous-Time Gaussian Random Processes
</p>
<p>20.8 Computer Simulation . . . . . . . . . . . . . . . . . .
</p>
<p>20.9 Real-World Example - Estimating Fish Populations
</p>
<p>References . . . . . . . . . . . . .
</p>
<p>Problems .
</p>
<p>20A MATLAB Listing for Figure 20.2
</p>
<p>21 Poisson Random Processes
</p>
<p>21.1 Introduction .
</p>
<p>21.2 Summary . . . . . . . . . . . . . . . . . . . . . .
</p>
<p>21.3 Derivation of Poisson Count ing Random Process
</p>
<p>21.4 Interar rival Times .
</p>
<p>21.5 Arr ival Times . . . . . . . . . . . . .
</p>
<p>21.6 Compound Poisson Random Process
</p>
<p>21.7 Computer Simulation. . . . . . . . .
</p>
<p>21.8 Real-World Example - Automobile Traffic Signa l Planning.
</p>
<p>References . . . . . . . . . . . . .
</p>
<p>Problems .
</p>
<p>21A Joint PDF for Interarrival Times
</p>
<p>22 Markov Chains
</p>
<p>22.1 Introduction .
</p>
<p>22.2 Summary . .
</p>
<p>22.3 Definitions . .
</p>
<p>22.4 Computation of St at e P robabilities
</p>
<p>22.5 Ergodic Markov Chains .
</p>
<p>22.6 Further Stead y-State Characteristics
</p>
<p>22.7 K-State Markov Chains .
</p>
<p>22.8 Computer Simulation . . . . . . . . .
</p>
<p>22.9 Real-World Example - St range Markov Chain Dynamics.
</p>
<p>xvii
</p>
<p>657
</p>
<p>661
</p>
<p>663
</p>
<p>667
</p>
<p>667
</p>
<p>673
</p>
<p>673
</p>
<p>675
</p>
<p>676
</p>
<p>681
</p>
<p>683
</p>
<p>686
</p>
<p>689
</p>
<p>696
</p>
<p>698
</p>
<p>701
</p>
<p>702
</p>
<p>709
</p>
<p>711
</p>
<p>711
</p>
<p>713
</p>
<p>714
</p>
<p>718
</p>
<p>721
</p>
<p>723
</p>
<p>727
</p>
<p>728
</p>
<p>732
</p>
<p>732
</p>
<p>737
</p>
<p>739
</p>
<p>739
</p>
<p>744
</p>
<p>744
</p>
<p>748
</p>
<p>756
</p>
<p>759
</p>
<p>762
</p>
<p>764
</p>
<p>765</p>
<p/>
</div>
<div class="page"><p/>
<p>XVlll
</p>
<p>References . . . . . . . . . . . .
</p>
<p>Problems .
</p>
<p>22A Solving for the Stationary PMF
</p>
<p>A Glossary of Symbols and Abbrevations
</p>
<p>B Assorted Math Facts and Formulas
</p>
<p>B.1 Proof by Induction
</p>
<p>B.2 Trigonometry
</p>
<p>B.3 Limits .
</p>
<p>Bo4 Sums ..
</p>
<p>B.5 Calculus
</p>
<p>C Linear and Matrix Algebra
</p>
<p>C.1 Definitions .
</p>
<p>C.2 Special Matrices .
</p>
<p>C.3 Matrix Manipulation and Formulas .
</p>
<p>Co4 Some Properties of PD (PSD) Matrices
</p>
<p>C.5 Eigendecomposition of Matrices . . . . .
</p>
<p>CONTENTS
</p>
<p>767
</p>
<p>767
</p>
<p>775
</p>
<p>777
</p>
<p>783
</p>
<p>783
</p>
<p>784
</p>
<p>784
</p>
<p>785
</p>
<p>786
</p>
<p>789
</p>
<p>789
</p>
<p>791
</p>
<p>792
</p>
<p>793
</p>
<p>793
</p>
<p>D Summary of Signals, Linear Transforms, and Linear Systems 795
</p>
<p>D.1 Discrete-Time Signals . . . . . 795
</p>
<p>D.2 Linear Transforms . . . . . . . 796
</p>
<p>D.3 Discrete-Time Linear Systems. 800
</p>
<p>DA Continuous-Time Signals. . . . 804
</p>
<p>D.5 Linear Transforms . . . . . . . 805
</p>
<p>D.6 Continuous-Time Linear Systems 807
</p>
<p>E Answers to Selected Problems 809
</p>
<p>Index 823</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 1
</p>
<p>Introduction
</p>
<p>1.1 What Is Probability?
</p>
<p>Probability as defined by Webster's dictionary is "the chance that a given event will
</p>
<p>occur" . Examples that we are familiar with are the probability that it will rain
</p>
<p>the next day or the probability that you will win the lottery. In the first example,
</p>
<p>there are many factors that affect the weather-so many, in fact, that we cannot be
</p>
<p>certain that it will or will not rain the following day. Hence , as a predictive tool we
</p>
<p>usually assign a number between 0 and 1 (or between 0% and 100%) indicating our
</p>
<p>degree of certainty that the event, rain, will occur. If we say that there is a 30%
</p>
<p>chance of rain, we believe that if identical conditions prevail, then 3 times out of 10,
</p>
<p>rain will occur the next day. Alternatively, we believe that the relative frequency of
</p>
<p>rain is 3/10. Note that if the science of meteorology had accurate enough models,
</p>
<p>then it is conceivable that we could determine exactly whether rain would or would
</p>
<p>not occur. Or we could say that the probability is either 0 or 1. Unfortunately, we
</p>
<p>have not progressed that far. In the second example, winning the lottery, our chance
</p>
<p>of success, assuming a fair drawing, is just one out of the number of possible lottery
</p>
<p>number sequences. In this case, we are uncertain of the outcome, not because of the
</p>
<p>inaccuracy of our model, but because the experiment has been designed to produce
</p>
<p>uncertain results.
</p>
<p>The common thread of these two examples is the presence of a random experi-
</p>
<p>m ent, a set of outcomes, and the probabilities assigned to these outcomes. We will
</p>
<p>see later that these attributes are common to all probabilistic descriptions. In the
</p>
<p>lottery example, the experiment is the drawing, the outcomes are the lottery num-
</p>
<p>ber sequences, and the probabilities assigned are liN, where N = total number of
</p>
<p>lottery number sequences. Another common thread, which justifies the use of prob-
</p>
<p>abilistic methods, is the concept of statistical regularity. Although we may never
</p>
<p>be able to predict with certainty the outcome of an experiment, we are, nonethe-
</p>
<p>less, able to predict "averages". For example, the average rainfall in the summer in
</p>
<p>Rhode Island is 9.76 inches, as shown in Figure 1.1, while in Arizona it is only 4.40</p>
<p/>
</div>
<div class="page"><p/>
<p>2 CHAPTER 1. INTRODUCTION
</p>
<p>Average = 9.76 inches :
.... . . . . . . . .. ............. . ; : :.
</p>
<p>&middot; . .&middot; . .&middot; . .
</p>
<p>20 ..--r- - - ,--- - -,----,,....-- - ,-- - -n
</p>
<p>18 .......
</p>
<p>16 . &middot; .. . . ., .
</p>
<p>14 .
</p>
<p>200019801940 1960
Year
</p>
<p>1920
</p>
<p>4
</p>
<p>2
1900
</p>
<p>6
</p>
<p>8
</p>
<p>'"lj 12
u
</p>
<p>.=i 10 HP.-'ftJ-'-i-++H+"'-H-'fH-
</p>
<p>Figure 1.1: Annual summer rainfall in Rhode Island from 1895 to 2002
</p>
<p>[NOAAjNCDC 2003].
</p>
<p>. . Average = 4.40 inches :
18 .. ; , , , ; -;.
</p>
<p>&middot; . . . . .&middot; . . . . .&middot; . . . . .
</p>
<p>20 ,....-,- - - -,-- - -,-- - --,,....-- --,--------.-,
</p>
<p>16 . . : : : : ~ ~
&middot; . . . . .&middot; . . . . .&middot; . . . , .14 . . : : : : : :
&middot; . . . . .
</p>
<p>Cf.l : : : : : :
~ 12 .. , , , : ; -;-
</p>
<p>(J : : : : : :
</p>
<p>..s 10 .. ~ -- ~ ---- .. -- ~ ; ~ :.
&middot; . . . . .&middot; . . . . .&middot; .. .
</p>
<p>200019801940 1960
Year
</p>
<p>1920
</p>
<p>4
</p>
<p>2
1900
</p>
<p>. . . . . . . . . .. . . . .. . . . . . . ~ , ..&middot; . . .&middot; . . .&middot; . . .&middot; . .
</p>
<p>Figure 1.2: Annual summer rainfall in Arizona from 1895 to 2002 [NOAAjNCDC
</p>
<p>2003].
</p>
<p>inches, as shown in Figure 1.2. It is clear that the decision to plant certain types
</p>
<p>of crops could be made based on these averages. This is not to say, however, that
</p>
<p>we can predict the rainfall amounts for any given summer. For instance, in 1999
</p>
<p>the summer rainfall in Rhode Island was only 4.5 inches while in 1984 the summer</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2. TYPES OF PROBABILITY PROBLEMS 3
</p>
<p>rainfall in Arizona was 7.3 inches. A somewhat more controlled experiment is the
</p>
<p>repeated tossing of a fair coin (one that is equally likely to come up heads or tails).
</p>
<p>We would expect about 50 heads out of 100 tosses, but of course, we could not
</p>
<p>predict the outcome of anyone particular toss. An illustration of this is shown in
</p>
<p>Figure 1.3. Note that 53 heads were obtained in this particular experiment. This
</p>
<p>heads
+fff++f. +.+ ++-t+4++fH-.. I I 1 IIi 11111 II II II 111111 ..... ++
</p>
<p>Q)
</p>
<p>&sect; tails
u
;; 0 . .... 1111111111111+++ oftf+++++ . ~+++ : . +*
o
</p>
<p>o 20 40
Toss
</p>
<p>60 80 100
</p>
<p>Figure 1.3: Outcomes for repeated fair coin tossings.
</p>
<p>example, which is of seemingly little relevance to physical reality, actually serves as
</p>
<p>a good model for a variety of random phenomena. We will explore one example in
</p>
<p>the next section.
</p>
<p>In summary, probability theory provides us with the ability to predict the be-
</p>
<p>havior of random phenomena in the "long run." To the extent that this information
</p>
<p>is useful, probability can serve as a valuable tool for assessment and decision mak-
</p>
<p>ing. Its application is widespread, encountering use in all fields of scientific endeavor
</p>
<p>such as engineering, medicine, economics, physics, and others (see references at end
</p>
<p>of chapter) .
</p>
<p>1.2 Types of Probability Problems
</p>
<p>Because of the mathematics required to determine probabilities, probabilistic meth-
</p>
<p>ods are divided into two distinct types, discrete and continuous. A discrete approach
</p>
<p>is used when the number of experimental outcomes is finite (or infinite but count-
</p>
<p>able as illustrated in Problem 1.7). For example, consider the number of persons
</p>
<p>at a business location that are talking on their respective phones anytime between
</p>
<p>9:00 AM and 9:10 AM. Clearly, the possible outcomes are 0,1, ... , N , where N is
</p>
<p>the number of persons in the office. On the other hand, if we are interested in the</p>
<p/>
</div>
<div class="page"><p/>
<p>4 CHAPTER 1. INTRODUCTION
</p>
<p>length of time a particular caller is on the phone during that time period, then the
</p>
<p>outcomes may be anywhere from &deg;to T minutes, where T = 10. Now the outcomes
are infinite in number since they lie within the interval [0,T]. In the first case, since
the outcomes are discrete (and finite), we can assign probabilities to the outcomes
</p>
<p>{O, 1, ... ,N}. An equiprobable assignment would be to assign each outcome a prob-
ability of l/(N +1). In the second case, the outcomes are continuous (and therefore
infinite) and so it is not possible to assign a nonzero probability to each outcome
</p>
<p>(see Problem 1.6).
</p>
<p>We will henceforth delineate between probabilities assigned to discrete outcomes
</p>
<p>and those assigned to continuous outcomes, with the discrete case always discussed
</p>
<p>first. The discrete case is easier to conceptualize and to describe mathematically. It
</p>
<p>will be important to keep in mind which case is under consideration since otherwise,
</p>
<p>certain paradoxes may result (as well as much confusion on the part of the student!).
</p>
<p>1.3 Probabilistic Modeling
</p>
<p>Probability models are simplified approximations to reality. They should be detailed
</p>
<p>enough to capture important characteristics of the random phenomenon so as to be
</p>
<p>useful as a prediction device, but not so detailed so as to produce an unwieldy
</p>
<p>model that is difficult to use in practice. The example of the number of telephone
</p>
<p>callers can be modeled by assigning a probability p to each person being on the
</p>
<p>phone anytime in the given lO-minute interval and assuming that whether one or
</p>
<p>more persons are on the phone does not affect the probability of others being on
</p>
<p>the phone. One can thus liken the event of being on the phone to a coin toss-
</p>
<p>if heads, a person is on the phone and if tails, a person is not on the phone. If
</p>
<p>there are N = 4 persons in the office, then the experimental outcome is likened to
</p>
<p>4 coin tosses (either in succession or simultaneously-it makes no difference in the
</p>
<p>modeling). We can then ask for the probability that 3 persons are on the phone
</p>
<p>by determining the probability of 3 heads out of 4 coin tosses. The solution to this
</p>
<p>problem will be discussed in Chapter 3, where it is shown that the probability of k
</p>
<p>heads out of N coin tosses is given by
</p>
<p>(1.1)
</p>
<p>where
</p>
<p>(N) N!k - (N - k)!k!
for k = 0,1, ... , N , and where M! = 1 &middot; 2&middot; 3 &middot; &middot;&middot; M for M a positive integer and by
</p>
<p>definition O! = 1. For our example, if p = 0.75 (we have a group of telemarketers)
</p>
<p>and N = 4 a compilation of the probabilities is shown in Figure 1.4. It is seen that
the probability that three persons are on the phone is 0.42. Generally, the coin toss</p>
<p/>
</div>
<div class="page"><p/>
<p>1.3. PROBABILISTIC MODELING 5
</p>
<p>0.5.-------r------.----.--------,.--------,
</p>
<p>0.45
</p>
<p>0.4
</p>
<p>0.35
</p>
<p>~ 0.3
~
</p>
<p>~0.25
</p>
<p>0.2
</p>
<p>0.15
</p>
<p>0.1
</p>
<p>5432
</p>
<p>0.05
</p>
<p>O*--__----I ---..l... ....l- .L-__----'
</p>
<p>o
k
</p>
<p>Figure 1.4: Probabilities for N = 4 coin tossings with p = 0.75.
</p>
<p>model is a reasonable one for this type of situation. It will be poor, however , if the
</p>
<p>assumptions are invalid. Some practical objections to the model might be:
</p>
<p>1. Different persons have different probabilities p (an eager telemarketer versus a
</p>
<p>not so eager one) .
</p>
<p>2. The probability of one person being on the phone is affected by whether his
</p>
<p>neighbor is on the phone (the two neighbors tend to talk about their planned
</p>
<p>weekends), i.e., the events are not "independent".
</p>
<p>3. The probability p changes over time (later in the day there is less phone activity
</p>
<p>due to fatigue).
</p>
<p>To accommodate these objections the model can be made more complex. In the
</p>
<p>end, however, the "more accurate" model may become a poorer predictor if the
</p>
<p>additional information used is not correct. It is generally accepted that a model
</p>
<p>should exhibit the property of "parsimony"-in other words, it should be as simple
</p>
<p>as possible.
</p>
<p>The previous example had discrete outcomes. For continuous outcomes a fre-
</p>
<p>quently used probabilistic model is the Gaussian or "bell"-shaped curve. For the
</p>
<p>modeling of the length of time T a caller is on the phone it is not appropriate to
</p>
<p>ask for the probability that T will be exactly, for example, 5 minutes. This is be-
</p>
<p>cause this probability will be zero (see Problem 1.6). Instead, we inquire as to the
</p>
<p>probability that T will be between 5 and 6 minutes. This question is answered by
</p>
<p>determining the area under the Gaussian curve shown in Figure 1.5. The form of</p>
<p/>
</div>
<div class="page"><p/>
<p>6 CHAPTER 1. INTRODUCTION
</p>
<p>the curve is given by
</p>
<p>1 [1 2]PT(t) = - exp --(t - 7)
V2ir 2
</p>
<p>-oo&lt;t&lt;oo (1.2)
</p>
<p>and although defined for all t , it is physically meaningful only for 0 ~ t ~ Tm ax ,
</p>
<p>0.5,----.-----.-----.-----.----.-------,
</p>
<p>0.45 .
</p>
<p>12106 8
t (min.)
</p>
<p>42
</p>
<p>S 0.3 '
h
~0 .25 .
</p>
<p>0.2
</p>
<p>0.15 .
</p>
<p>0.1
</p>
<p>0.05 .
</p>
<p>0'-------''----=-------''-------'----'''-----'
o
</p>
<p>0.35 .
</p>
<p>0.4 .
</p>
<p>Figure 1.5: Gaussian or "bell"-shaped curve.
</p>
<p>where Tm ax = 10 for the current example. Since the area under the curve for times
</p>
<p>less than zero or greater than Tmax = 10 is nearly zero , this model is a reasonable
</p>
<p>approximation to physical reality. The curve has been chosen to be centered about
</p>
<p>t = 7 to relect an "average" time on the phone of 7 minutes for a given caller. Also,
note that we let t denote the actual value of the random time T. Now, to determine
</p>
<p>the probability that the caller will be on the phone for between 5 and 6 minutes we
</p>
<p>integrate PT(t) over this interval to yield
</p>
<p>P[5 ~ T ~ 6] = 16 PT(t)dt = 0.1359. (1.3)
The value of the int egral must be numerically determined. Knowing the function
</p>
<p>PT(t) allows us to determine the probability for any interval. (It is called the proba-
bility density function (PDF) and is the probability per unit length. The PDF will
</p>
<p>be discussed in Chapter 10.) Also, it is apparent from Figure 1.5 that phone usage
</p>
<p>of duration less than 4 minutes or greater than 10 minutes is highly unlikely. Phone
</p>
<p>usage in the range of 7 minutes, on the other hand, is most probable. As before,
</p>
<p>some objections might be raised as to the accuracy of this model. A particularly
</p>
<p>lazy worker could be on the phone for only 3 minutes, as an example.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.4. ANALYSIS VERSUS COMPUTER SIMULATION 7
</p>
<p>(1.4)
</p>
<p>In this book we will henceforth assume that the models, which are mathematical
</p>
<p>in nature, are perfect and thus can be used to determine probabilities. In practice,
</p>
<p>the user must ultimately choose a model that is a reasonable one for the application
</p>
<p>of interest.
</p>
<p>1.4 Analysis versus Computer Simulation
</p>
<p>In the previous section we saw how to compute probabilities once we were given
</p>
<p>certain probability functions such as (1.1) for the discrete case and (1.2) for the
</p>
<p>continuous case. For many practical problems it is not possible to determine these
</p>
<p>functions . However, if we have a model for the random phenomenon, then we
</p>
<p>may carry out the experiment a large number of times to obtain an approximate
</p>
<p>probability. For example, to determine the probability of 3 heads in 4 tosses of a
</p>
<p>coin with probability of heads being p = 0.75, we toss the coin four times and count
</p>
<p>the number of heads, say Xl = 2. Then, we repeat the experiment by tossing the
</p>
<p>coin four more times, yielding X2 = 1 head. Continuing in this manner we execute
</p>
<p>a succession of 1000 experiments to produce the sequence of number of heads as
</p>
<p>{Xl, X2, &middot; &middot;&middot;, XlOOO}. Then, to determine the probability of 3 heads we use a relative
</p>
<p>frequency interpretation of probability to yield
</p>
<p>P[ h d J Number of times 3 heads observed
3 ea s = 1000 .
</p>
<p>Indeed, early on probabilists did exactly this, although it was extremely tedious. It
</p>
<p>is therefore of utmost importance to be able to simulate this procedure. With the
</p>
<p>advent of the modern digital computer this is now possible. A digital computer
</p>
<p>has no problem performirig a calculation once , 100 times, or 1,000,000 times. What
</p>
<p>is needed to implement this approach is a means to simulate the toss of a coin.
</p>
<p>Fortunately, this is quite easy as most scientific software packages have built-in
</p>
<p>random number generators. In MATLAB, for example, a number in the interval
</p>
<p>(0,1) can be produced with the simple statement x=rand(1, 1). The number is
</p>
<p>chosen "at random" so that it is equally likely to be anywhere in the (0,1) interval.
</p>
<p>As a result, a number in the interval (0,1/2J will be observed with probability 1/2
</p>
<p>and a number in the remaining part of the interval (1/2,1) also with probability
</p>
<p>1/2. Likewise, a number in the interval (0,0.75J will be observed with probability
</p>
<p>p = 0.75. A computer simulation of the number of persons in the office on the
</p>
<p>telephone can thus be implemented with the MATLAB code (see Appendix 2A for
</p>
<p>a brief introduction to MATLAB):
</p>
<p>nurnber=O;
</p>
<p>for i=1:4 %set up simulation for 4 coin tosses
if rand(1,1)&lt;O.75 %toss coin with p=O.75
</p>
<p>x(i,1)=1; % head
</p>
<p>else</p>
<p/>
</div>
<div class="page"><p/>
<p>8 CHAPTER 1. INTRODUCTION
</p>
<p>x(i,1)=O; %tail
end
</p>
<p>number=number+x(i,1); %count number of heads
end
</p>
<p>Repeating this code segment 1000 times will result in a simulation of the previous
</p>
<p>experiment.
</p>
<p>Similarly, for a continuous outcome experiment we require a means to generate
</p>
<p>a continuum of outcomes on a digital computer. Of course, strictly speaking this is
</p>
<p>not possible since digital computers can only provide a finite set of numbers, which
</p>
<p>is determined by the number of bits in each word. But if the number of bits is
</p>
<p>large enough, then the approximation is adequate. For example, with 64 bits we
</p>
<p>could represent 264 numbers between 0 and 1, so that neighboring numbers would
</p>
<p>be 2-64 = 5 X 10-20 apart. With this ability MATLAB can produce numbers that
follow a Gaussian curve by invoking the statement x=randn C1 , 1) .
</p>
<p>Throughout the text we will use MATLAB for examples and also exercises.
</p>
<p>However, any modern scientific software package can be used.
</p>
<p>1.5 Some Notes to the Reader
</p>
<p>The notation used in this text is summarized in Appendix A. Note that boldface
</p>
<p>type is reserved for vectors and matrices while regular face type will denote scalar
</p>
<p>quantities. All other symbolism is defined within the context of the discussion. Also,
</p>
<p>the reader will frequently be warned of potential "pitfalls". Common misconcep-
</p>
<p>tions leading to student errors will be described and noted. The pitfall or caution
</p>
<p>symbol shown below should be heeded.
</p>
<p>The problems are of four types: computational or formula applications, word
</p>
<p>problems, computer exercises, and theoretical exercises. Computational or formula
</p>
<p>(denoted by f) problems are straightforward applications of the various formulas of
</p>
<p>the chapter, while word problems (denoted by w) require a more complete assimi-
</p>
<p>lation of the material to solve the problem. Computer exercises (denoted by c) will
require the student to either use a computer to solve a problem or to simulate the
</p>
<p>analytical results. This will enhance understanding and can be based on MATLAB,
</p>
<p>although equivalent software may be used. Finally, theoretical exercises (denoted by
</p>
<p>t) will serve to test the student's analytical skills as well as to provide extensions to
the material of the chapter. They are more challenging. Answers to selected prob-
</p>
<p>lems are given in Appendix E. Those problems for which the answers are provided
</p>
<p>are noted in the problem section with the symbol ( ~ ) .
</p>
<p>The version of MATLAB used in this book is 5.2, although newer versions
</p>
<p>should provide identical results. Many MATLAB outputs that are used for the</p>
<p/>
</div>
<div class="page"><p/>
<p>REFERENCES 9
</p>
<p>text figures and for the problem solutions rely on random number generation. To
</p>
<p>match your resul ts against those shown in the figures and the problem solutions, the
</p>
<p>same set of random numb ers can be generated by using the MATLAB statement s
</p>
<p>rand ( , state' ,0) and randn ( 'state' ,0) at the beginning of each program. These
</p>
<p>statements will ini ti alize the random number generators to produce the same set of
</p>
<p>random numbers . Finally, the MAT LAB programs and code segment s given in the
</p>
<p>book are indicated by the "typewriter" font , for example, x=randnO, 1).
</p>
<p>There are a number of ot her textboo ks that the reader may wish to consult .
</p>
<p>They are listed in the following reference list , along with some comments on their
</p>
<p>contents .
</p>
<p>Davenport , W.B. , Probability and Random Processes, McGraw-Hill, New York ,
</p>
<p>1970. (Excellent introductory text.)
</p>
<p>Feller , W. , An Introdu ction to Probability Th eory and its Applications, Vols. 1,
</p>
<p>2, John Wiley, New York , 1950. (Definitive work on probability-requires
</p>
<p>mature mathematical knowledge.)
</p>
<p>Hoel, P.G. , S.C. Port , C.J. Stone, Introduction to Probability Th eory, Houghton
</p>
<p>Mifflin Co., Boston , 1971. (Excellent introductory text but limited to proba-
</p>
<p>bility.)
</p>
<p>Leon-G arcia , A. , Probability and Random Processes for Electrical Engineering,
</p>
<p>Addison-Wesley, Reading, MA, 1994. (Excellent introductory text .)
</p>
<p>Parzen, E. , Modern Probabilit y Theory and It s Applications, John Wiley, New York ,
</p>
<p>1960. (Classic text in pr obability-useful for all disciplines).
</p>
<p>Parzen , E. , Sto chastic Processes, Holden-D ay, San Francisco, 1962. (Most useful
</p>
<p>for Markov process descriptions. )
</p>
<p>Pap oulis, A. , Probability, Random Variables, and Stochast ic Processes, McGraw-
</p>
<p>Hill , New York , 1965. (Classic but somewhat difficult text. Best used as a
</p>
<p>reference.)
</p>
<p>Ross, S., A First Course in Probability, Prentice-Hall , Upper Saddle River , NJ ,
</p>
<p>2002. (Excellent introductory text covering only probability.)
</p>
<p>Stark, H. , J .W. Woods, Probability and Random Processes with Application s to
</p>
<p>Signal Processing , Third Ed ., Prentice Hall , Upp er Saddle River , NJ , 2002.
</p>
<p>(Excellent introductory text but at a somewhat more advanced level.)
</p>
<p>References
</p>
<p>Burdic, W .S., Underwater Acoustic Sy stems Analysis , Prentice-Hall , Englewood
</p>
<p>Cliffs, NJ, 1984.</p>
<p/>
</div>
<div class="page"><p/>
<p>10 CHAPTER 1. INTRODUCTION
</p>
<p>Ellenberg, S.S. , D.M. Finklestein, D.A. Schoenfeld, "Statistical Issues Arising in
</p>
<p>AIDS Clinical Trials," J. Am. Statist. Asssoc., Vol. 87, pp. 562-569, 1992.
</p>
<p>Gustafson, D.E., A.S. Willsky, J.Y. Wang, M.C. Lancaster, J.H. Triebwasser,
</p>
<p>"ECG/VCG Rhythm Diagnosis Using Statistical Signal Analysis, Part II:
</p>
<p>Identification of Transient Rhythms," IEEE Trans. Biomed. Eng., Vol. BME-
</p>
<p>25, pp. 353-361 , 1978.
</p>
<p>Ives, R.B., "T he Applications of Advanced Pattern Recognition Techniques for
</p>
<p>the Discrimination Between Earthquakes and Nuclear Detonations," Pattern
</p>
<p>Recognition, Vol. 14, pp. 155-161, 1981.
</p>
<p>Justice, J.H., "Array Processing in Exploration Seismology, " in Array Signal Pro-
</p>
<p>cessing, S. Haykin, Ed., Prentice-Hall, Englewood Cliffs, NJ, 1985.
</p>
<p>Knight, W.S., R.G. Pridham, S.M. Kay, "Digital Signal Processing for Sonar,"
</p>
<p>Proc. IEEE, Vol. 69, pp. 1451-1506, Nov. 1981.
</p>
<p>NOAA/NCDC, Lawrimore, J., "Climate at a Glance," National Oceanic and Atmo-
</p>
<p>spheric Administration, http://www.ncdc.noaa.gov/ oafclimate/resarch/cag3
</p>
<p>/NA.html, 2003.
</p>
<p>Proakis, J. , Digital Communications, Second Ed. , McGraw-Hill, New York, 1989.
</p>
<p>Skolnik, M.L , Introduction to Radar Systems, McGraw-Hill, New York, 1980.
</p>
<p>Taylor, S., Modelling Financial Time Series , John Wiley, New York , 1986.
</p>
<p>Problems
</p>
<p>1.1 c.:.:J (w) A fair coin is tossed. Identify the random experiment, the set of
outcomes, and the probabilities of each possible outcome.
</p>
<p>1.2 (w) A card is chosen at random from a deck of 52 cards. Identify the ran-
</p>
<p>dom experiment, the set of outcomes, and the probabilities of each possible
</p>
<p>outcome.
</p>
<p>1.3 (w) A fair die is tossed and the number of dots on the face noted. Identify the
random experiment , the set of outcomes, and the probabilities of each possible
</p>
<p>outcome.
</p>
<p>1.4 (w) It is desired to predict the annual summer rainfall in Rhode Island for 2010.
</p>
<p>If we use 9.76 inches as our prediction, how much in error might we be , based
</p>
<p>on the past data shown in Figure 1.1? Repeat the problem for Arizona by
</p>
<p>using 4.40 inches as the prediction.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 11
</p>
<p>1.5 L..:J (w) Determine whether the following experiments have discrete or contin-
uous outcomes:
</p>
<p>a. Throw a dart with a point tip at a dartboard.
</p>
<p>b. Toss a die.
</p>
<p>c. Choose a lottery number.
</p>
<p>d. Observe the outdoor temperature using an analog thermometer.
</p>
<p>e. Determine the current time in hours, minutes, seconds, and AM or PM.
</p>
<p>1.6 (w) An experiment has N = 10 outcomes that are equally probable. What is
</p>
<p>the probability of each outcome? Now let N = 1000 and also N = 1, 000, 000
and repeat. What happens as N ---r oo?
</p>
<p>1. 7 c.:~) (f) Consider an experiment with possible outcomes {I, 2, 3, ... }. If we
assign probabilities
</p>
<p>P[k] = 21k k = 1,2,3, ...
</p>
<p>to the outcomes, will these probabilties sum to one? Can you have an infinite
</p>
<p>number of outcomes but still assign nonzero probabilities to each outcome?
</p>
<p>Reconcile these results with that of Problem 1.6.
</p>
<p>1.8 (w) An experiment consists of tossing a fair coin four times in succession. What
are the possible outcomes? Now count up the number of outcomes with three
</p>
<p>heads. If the outcomes are equally probable, what is the probability of three
</p>
<p>heads? Compare your results to that obtained using (1.1).
</p>
<p>1. 9 (w) Perform the following experiment by actually tossing a coin of your choice.
</p>
<p>Flip the coin four times and observe the number of heads. Then, repeat this
</p>
<p>experiment 10 times. Using (1.1) determine the probability for k = 0,1,2,3,4
</p>
<p>heads. Next use (1.1) to determine the number of heads that is most proba-
</p>
<p>ble for a single experiment? In your 10 experiments which number of heads
</p>
<p>appeared most often?
</p>
<p>1.10 L...:... ) (w) A coin is tossed 12 times. The sequence observed is the 12-tuple
(H, H, T, H, H, T, H, H, H, H ,T, H). Is this a fair coin? Hint: Determine
</p>
<p>P[k = 9] using (1.1) assuming a probability of heads of p = 1/2.
</p>
<p>1.11 (t) Prove that 2:f=o P[k] = 1, where P[k] is given by (1.1) . Hint: First prove
the binomial theorem</p>
<p/>
</div>
<div class="page"><p/>
<p>12 CHAPTER 1. INTRODUCTION
</p>
<p>by induction (see Appendix B). Use Pascal's "triangle" rule
</p>
<p>where
</p>
<p>( ~ ) = o k &lt; a and k &gt; M.
</p>
<p>1.12 (t) If f: PT(t)dt is the probability of observing T in the interval [a, b], what is
</p>
<p>f~ooPT(t)dt?
</p>
<p>1.13 C.:,,) (f) Using (1.2) what is the probability of T &gt; 7? Hint: Observe that
PT(t) is symmetric about t = 7.
</p>
<p>1.14 L ~ ) (c) Evaluate the integral
</p>
<p>r3 _1_ exp [_~t2] dt
} -3 "ffff 2
</p>
<p>by using the approximation
</p>
<p>L 1 [1 ]L - exp --(n.6)2 .6
"ffff 2
</p>
<p>n=-L
</p>
<p>where L is the integer closest to 3/.6 (the rounded value), for .6 = 0.1, .6 =
</p>
<p>0.01, .6 = 0.001.
</p>
<p>1.15 (c) Simulate a fair coin tossing experiment by modifying the code given in
</p>
<p>Section 1.4. Using 1000 repetitions of the experiment, count the number of
</p>
<p>times three heads occur. What is the simulated probability of obtaining three
</p>
<p>heads in four coin tosses? Compare your result to that obtained using (1.1).
</p>
<p>1.16 (c) Repeat Problem 1.15 but instead consider a biased coin with P = 0.75.
</p>
<p>Compare your result to Figure 1.4.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 2
</p>
<p>Computer Simulation
</p>
<p>2 .1 Introduction
</p>
<p>Computer simulation of random phenomena has become an indispensable tool in
</p>
<p>modern scientific investigations. So-called Monte Carlo computer approaches are
</p>
<p>now commonly used to promote understanding of probabilistic problems. In this
</p>
<p>chapter we continue our discussion of computer simulation, first introduced in Chap-
</p>
<p>ter 1, and set the stage for its use in later chapters. Along the way we will examine
</p>
<p>some well known properties of random events in the process of simulating their
</p>
<p>behavior. A more formal mathematical description will be introduced later but
</p>
<p>careful attention to the details now, will lead to a better intuitive understanding of
</p>
<p>the mathematical definitions and theorems to follow.
</p>
<p>2.2 Summary
</p>
<p>This chapter is an introduction to computer simulation of random experiments. In
</p>
<p>Section 2.3 there are examples to show how we can use computer simulation to pro-
</p>
<p>vide counterexamples, build intuition, and lend evidence to a conjecture. However,
</p>
<p>it cannot be used to prove theorems. In Section 2.4 a simple MATLAB program is
</p>
<p>given to simulate the outcomes of a discrete random variable. Section 2.5 gives many
</p>
<p>examples of typical computer simulations used in probability, including probability
</p>
<p>density function estimation, probability of an interval, average value of a random
</p>
<p>variable, probability density function for a transformed random variable, and scat-
</p>
<p>ter diagrams for multiple random variables . Section 2.6 contains an application of
</p>
<p>probability to the "real-world" example of a digital communication system. A brief
</p>
<p>description of the MATLAB programming language is given in Appendix 2A.</p>
<p/>
</div>
<div class="page"><p/>
<p>14 CHAPTER 2. COMPUTER SIMULATION
</p>
<p>2.3 Why Use Computer Simulation?
</p>
<p>A computer simulation is valuable in many resp ects. It can be used
</p>
<p>a . to provide counterexamples to proposed theorems
</p>
<p>b. to build intuition by experimenting with random numbers
</p>
<p>c. to lend evidence to a conjecture.
</p>
<p>We now explore these uses by posing the following question: What is the effect
</p>
<p>of adding together the numerical outcomes of two or more experiments, i.e., what
</p>
<p>are the probabilities of the summed outcomes? Specifically, if U1 represents the
</p>
<p>outcome of an experiment in which a number from 0 to 1 is chosen at random
</p>
<p>and U2 is the outcome of an experiment in which another number is also chosen at
</p>
<p>random from a to 1, what are the probabilities of X = Ul + U2? The mathematical
answer to this question is given in Chapter 12 (see Example 12.8) , although at
</p>
<p>this point it is unknown to us. Let 's say that someone asserts that there is a
</p>
<p>theorem that X is equally likely to be anywhere in the interval [0,2]. To see if this is
</p>
<p>reasonable, we carry out a computer simulation by generating values of Ul and U2
</p>
<p>and adding them together. Then we repeat this procedure M times. Next we plot a
</p>
<p>histogram, which gives the number of outcomes that fall in each subinterval within
</p>
<p>[0,2] . As an example of a histogram consider the M = 8 possible outcomes for
X of {1.7 ,0.7, 1.2, 1.3, 1.8, 1.4, 0.6, 0.4} . Choosing the four subintervals (also called
</p>
<p>bins) [0, 0.5]' (0.5,1], (1,1.5], (1.5,2]' the histogram appears in Figure 2.1. In this
</p>
<p>3
</p>
<p>~ 2.5
</p>
<p>a
8
~ 2
o....
o
~ 1.5
</p>
<p>,D
</p>
<p>a
;::l 1
Z
</p>
<p>0.5
</p>
<p>o
</p>
<p>: :
</p>
<p>~ --:-.-
</p>
<p>:
</p>
<p>~
</p>
<p>0.25 0.5 0.75 1 1.25 1.5 1.75 2
Value of X
</p>
<p>Figure 2.1: Example of a histogram for a set of 8 numbers in [0,2] interval.
</p>
<p>example, 2 outcomes were between 0.5 and 1 and are therefore shown by the bar</p>
<p/>
</div>
<div class="page"><p/>
<p>2.3. WHY USE COMPUTER SIMULATION? 15
</p>
<p>centered at 0.75. T he other bars are similarly obtained. If we now increase the
</p>
<p>number of experiments to M = 1000, we obtain the histogram shown in Figure 2.2.
</p>
<p>Now it is clear that the values of X are not equally likely. Values near one appear
</p>
<p>450
</p>
<p>400
</p>
<p>~
S 350
8
-;3 300
o
'0250
....
</p>
<p>] 200
S
i 150
</p>
<p>100
</p>
<p>50
</p>
<p>o
</p>
<p>, ~ ,
</p>
<p>; ; ; ;..:..:.:...:.. ;
</p>
<p>,
</p>
<p>; ; ; ; ;
</p>
<p>----=----
</p>
<p>; ; ;
</p>
<p>0.25 0.5 0.75 1 1.25 1.5 1.75 2
Value of X
</p>
<p>Figure 2.2: Histogram for sum of two equally likely numbers, both chosen in interval
</p>
<p>[0,1].
</p>
<p>to be much more probable. Hence , we have generated a "counterexample" to the
</p>
<p>proposed theorem, or at least some evidence to the contrary.
</p>
<p>We can build up our intuition by continuing with our experimentation. Attempt-
</p>
<p>ing to justify the observed occurrences of X, we might suppose that the probabilities
</p>
<p>are higher near one because there are more ways to obtain these values . If we con-
</p>
<p>trast the values of X = 1 versus X = 2, we note that X = 2 can only be obtained
by choosing U1 = 1 and U2 = 1 but X = 1 can be obtained from Ul = U2 = 1/2
or U1 = 1/4, U2 = 3/4 or U1 = 3/4, U2 = 1/ 4, etc. We can lend credibility to this
line of reasoning by supposing that Ul and U2 can only take on values in the set
</p>
<p>{O, 0.25, 0.5, 0.75, I} and finding all values of U1 + U2 . In essence, we now look at a
simpler problem in order to build up our intuit ion . An enumeration of the poss ible
</p>
<p>values is shown in Table 2.1 along with a "histogram" in Figure 2.3. It is clear
</p>
<p>now that the probability is highest at X = 1 because the number of combinations
</p>
<p>of U1 and U2 that will yield X = 1 is highest . Hence, we have learned about what
</p>
<p>happens when outcomes of experiments are added together by employing computer
</p>
<p>simulation.
</p>
<p>We can now try to extend this result to the addition of three or more exper-
</p>
<p>imental outcomes via computer simulation. To do so define X 3 = U1 + U2 + U3
and X 4 = Ul + U2 + U3 + U4 and repeat the simulation. A computer simulation
with M = 1000 trials produces the histograms shown in Figure 2.4. It appears to</p>
<p/>
</div>
<div class="page"><p/>
<p>16 CHAPTER 2. COMPUTER SIMULATION
</p>
<p>U2
0.00 0.25 0.50 0.75 1.00
</p>
<p>0.00 0.00 0.25 0.50 0.75 1.00
</p>
<p>0.25 0.25 0.50 0.75 1.00 1.25
</p>
<p>UI 0.50 0.50 0.75 1.00 1.25 1.50
0.75 0.75 1.00 1.25 1.50 1.75
</p>
<p>1.00 1.00 1.25 1.50 1.75 2.00
</p>
<p>Table 2.1: Possible values for X = UI + U2 for intuition-building experiment.
</p>
<p>5
</p>
<p>4,5
</p>
<p>en
Q) 4
8
o
~ 3.5
;j
</p>
<p>o 3....
o
:u 2.5
</p>
<p>.D
8 2
;j
</p>
<p>Z 1.5
</p>
<p>0.5
</p>
<p>o
</p>
<p>... - ... - '.'
'.' ; '. r-'- r-'- ; '.
</p>
<p>: ,...:... ,...:... : ~
</p>
<p>...
</p>
<p>... ...:.. ...:.. ;
</p>
<p>' .' "
</p>
<p>c-'- ....:..
</p>
<p>o 0.25 0.5 0.75 1 1.25 1.5 1.75 2
Value of X
</p>
<p>Figure 2.3: Histogram for X for intuition-building experiment.
</p>
<p>bear out the conjecture that the most probable values are near the center of the
</p>
<p>[0,3J and [0,4J intervals, respectively. Additionally, the histograms appear more like
</p>
<p>a bell-shaped or Gaussian curve. Hence, we might now conjecture, based on these
</p>
<p>computer simulations, that as we add more and more experimental outcomes to-
</p>
<p>gether, we will obtain a Gaussian-shaped histogram. This is in fact true, as will be
</p>
<p>proven later (see central limit theorem in Chapter 15). Note that we cannot prove
</p>
<p>this result using a computer simulation but only lend evidence to our theory. How-
</p>
<p>ever , the use of computer simulations indicates what we need to prove , information
</p>
<p>that is invaluable in practice. In summary, computer simulation is a valuable tool
</p>
<p>for lending credibility to conjectures, building intuition, and uncovering new results.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4. COMPUTER SIMULATION OF RANDOM PHENOMENA 17
</p>
<p>: : : :
r-'-
</p>
<p>: &bull;...:.2..:. :
</p>
<p>; ;
</p>
<p>'n' &middot;n&middot;
</p>
<p>r-e-
</p>
<p>: : : : : :
</p>
<p>,...:...
: : : : :
</p>
<p>r-'-
</p>
<p>,
</p>
<p>; ; .:..:..: ;
</p>
<p>&middot;n&middot; r-,
</p>
<p>350
~
8 300
o
o
-;; 250
o
'0200
....
</p>
<p>] 150
</p>
<p>8
~ 100
</p>
<p>50
</p>
<p>o
0.5 1.5 2 2.5
</p>
<p>Value of X3
</p>
<p>(a) Sum of 3 U's
</p>
<p>350
</p>
<p>~
8300
</p>
<p>8
-;; 250
o
'0200
....
</p>
<p>] 150
</p>
<p>8
~ 100
</p>
<p>50
</p>
<p>o
0.5 1.5 2 2.5 3 3.5
</p>
<p>Value of X4
</p>
<p>(b) Sum of 4 U's
</p>
<p>Figure 2.4: Histograms for addition of outcomes.
</p>
<p>C omp uter simulatio ns cannot be u sed t o p r ove t h eorems.
</p>
<p>In Figure 2.2, which displayed the outcomes for 1000 trials, is it possible that the
</p>
<p>computer simulation could have produced 500 outcomes in [0,0.5], 500 outcomes in
</p>
<p>[1.5,2] and no outcomes in (0.5,1.5)? The answer is yes, although it is improbable.
</p>
<p>It can be shown that the probability of this occuring is
</p>
<p>(see Problem 12.27) .
</p>
<p>2.4 Computer Simulation of R andom Phenomena
</p>
<p>In the previous chapter we briefly explained how to use a digital computer to simu-
</p>
<p>late a random phenomenon. We now continue that discussion in more detail. Then,
</p>
<p>the following section applies the techniques to specific problems ecountered in prob-
</p>
<p>ability. As before, we will distinguish between experiments that produce discrete
</p>
<p>outcomes from those that produce continuous outcomes.
</p>
<p>We first define a random variable X as the numerical outcome of the random
</p>
<p>experiment . Typical examples are the number of dots on a die (discrete) or the
</p>
<p>distance of a dart from the center of a dartboard of radius one (continuous). The</p>
<p/>
</div>
<div class="page"><p/>
<p>18 CHAPTER 2. COMPUTER SIMULATION
</p>
<p>random variable X can take on the values in the set {I, 2, 3, 4, 5, 6} for the first
</p>
<p>example and in the set {r : 0 :s: r :s: I} for the second example. We denote
the random variable by a capital letter, say X, and its possible values by a small
</p>
<p>letter, say Xi for the discrete case and x for the continuous case. The distinction is
</p>
<p>analogous to that between a function defined as g(x) = x 2 and the values y = g(x)
</p>
<p>that g(x) can take on.
</p>
<p>Now it is of interest to determine various properties of X. To do so we use
</p>
<p>a computer simulation, performing many experiments and observing the outcome
</p>
<p>for each experiment. The number of experiments, which is sometimes referred to
</p>
<p>as the number of trials, will be denoted by M. To simulate a discrete random
</p>
<p>variable we use rand, which generates a number at random within the (0,1) interval
</p>
<p>(see Appendix 2A for some MATLAB basics). Assume that in general the possible
</p>
<p>values of X are {Xl,X2, .. . , X N } with probabilities {PI, P2 , ... ,PN }. As an example,
if N = 3 we can generate M values of X by using the following code segment (which
</p>
<p>assumes M, x1 , x2 , x3 ,P1 ,p2 ,p3 have been previously assigned):
</p>
<p>for i=1:M
</p>
<p>u=rand(1,1);
</p>
<p>if u&lt;=p1
</p>
<p>x Ci , 1)=x1;
</p>
<p>elseif u&gt;p1 &amp; u&lt;=p1+p2
x(i,1)=x2;
</p>
<p>elseif u&gt;p1+p2
</p>
<p>x(i,1)=x3;
</p>
<p>end
</p>
<p>end
</p>
<p>After this code is executed, we will have generated M values of the random variable
</p>
<p>X. Note that the values of X so obtained are termed the outcomes or realizations
</p>
<p>of X. The extension to any number N of possible values is immediate. For a
</p>
<p>continuous random variable X that is Gaussian we can use the code segment:
</p>
<p>for i=1:M
</p>
<p>x(i,1)=randn(1,1);
</p>
<p>end
</p>
<p>or equivalently x=randn(M, 1). Again at the conclusion of this code segment we will
</p>
<p>have generated M realizations of X. Later we will see how to generate realizations
</p>
<p>of random variables whose PDFs are not Gaussian (see Section 10.9).
</p>
<p>2.5 Determining Characteristics of Random Variables
</p>
<p>There are many ways to characterize a random variable. We have already alluded to
</p>
<p>the probability of the outcomes in the discrete case and the PDF in the continuous</p>
<p/>
</div>
<div class="page"><p/>
<p>2.5. DETERMINING CHARACTERISTICS OF RANDOM VARIABLES 19
</p>
<p>case. To be more precise consider a discrete random variable, such as that describing
</p>
<p>the outcome of a coin toss. If we toss a coin and let X be 1 if a head is observed
</p>
<p>and let X be 0 if a tail is observed, then the probabilities are defined to be p for
</p>
<p>X = Xl = 1 and 1 - p for X = X2 = O. The probability p of X = 1 can be thought
of as the relative frequency of the outcome of heads in a long succession of tosses.
</p>
<p>Hence, to determine the probability of heads we could toss a coin a large number
</p>
<p>of times and estimate p by the number of observed heads divided by the number
</p>
<p>of tosses. Using a computer to simulate this experiment, we might inquire as to
</p>
<p>the number of tosses that would be necessary to obtain an accurate estimate of the
</p>
<p>probability of heads. Unfortunately, this is not easily answered. A practical means,
</p>
<p>though, is to increase the number of tosses until the estimate so computed converges
</p>
<p>to a fixed number. A computer simulation is shown in Figure 2.5 where the estimate
</p>
<p>0.9 ,
</p>
<p>&gt;.0.8
o
&gt;::
~ 0.7
0'
Ji 0.6
</p>
<p>Q)
</p>
<p>.::: 0.5 b..
</p>
<p>~ 0 4 ~""'~ .' ~ .---""' .--:--~ .~ . _,...,._.,..,.,._o:--._......,.,...,.,.~...,.,._ ........--l
Q) &bull;
</p>
<p>0:;
0.3
</p>
<p>0.2
</p>
<p>0.1
</p>
<p>2000500 1000 1500
Number of trials
</p>
<p>OL.-------'-----'------'------'
o
</p>
<p>Figure 2.5: Estimate of probability of heads for various number of coin tosses.
</p>
<p>appears to converge to about 0.4. Indeed, the true value (that value used in the
</p>
<p>simulation) was p = 0.4. It is also seen that the estimate of p is slightly higher
than 0.4. This is due to the slight imperfections in the random number generator
</p>
<p>as well as computational errors. Increasing the number of trials will not improve
</p>
<p>the results. We next describe some typical simulations that will be useful to us.
</p>
<p>To illustrate the various simulations we will use a Gaussian random variable with
</p>
<p>realizations generated using randn(1, 1). Its PDF is shown in Figure 2.6.
</p>
<p>Example 2.1 - Probability density function
</p>
<p>A PDF may be estimated by first finding the histogram and then dividing the
</p>
<p>number of outcomes in each bin by M, the total number of realizations, to obtain
</p>
<p>the probability. Then to obtain the PDF px(x) recall that the probability of X</p>
<p/>
</div>
<div class="page"><p/>
<p>20 CHAPTER 2. COMPUTER SIMULATION
</p>
<p>0.35 .. .
</p>
<p>0.3
</p>
<p>0.25
</p>
<p>~
~ 0.2
</p>
<p>0.15 . .
</p>
<p>0.1
</p>
<p>0.05
</p>
<p>-3 -2 -1 o
X
</p>
<p>2 3 4
</p>
<p>Figure 2.6: Gaussian probability density function.
</p>
<p>taking on a value in an interval is found as the area under the PDF of that interval
</p>
<p>(see Section 1.3). Thus,
</p>
<p>P[a :S X :S b] = l bpx(x)dx (2.1)
and if a = XQ - b.x/2 and b = XQ + b.x/2, where b.x is small, then (2.1) becomes
</p>
<p>P[XQ - b.x/2 :S X :S XQ + b.x/2] ~ px(xQ)b.x
</p>
<p>and therefore the PDF at x = XQ is approximately
</p>
<p>( )
P[xQ - b.x/2 :S X :S XQ + b.x/2]
</p>
<p>PX XQ ~ b.x .
</p>
<p>Hence, we need only divide the estimated probability by the bin width b.x. Also,
</p>
<p>note that as claimed in Chapter 1, px(x) is seen to be the probability per unit length.
</p>
<p>In Figure 2.7 is shown the estimated PDF for a Gaussian random variable as well
</p>
<p>as the true PDF 'as given in Figure 2.6. The MATLAB code used to generate the
</p>
<p>figure is also shown.
</p>
<p>Example 2.2 - Probability of an interval
</p>
<p>To determine P[a :S X :S b] we need only generate M realizations of X, then count
the number of outcomes that fall into the [a, b] interval and divide by M. Of course</p>
<p/>
</div>
<div class="page"><p/>
<p>2.5. DETERMINING CHARACTERISTICS OF RANDOM VARIABLES 21
</p>
<p>2 3
</p>
<p>&bull; &bull; &bull; &bull; 1 &bull;&bull; &bull; &bull;&bull; 1 &bull; &bull; &bull;&bull;&bull; &bull;&bull;&bull;&bull; &bull; &bull;&middot; . .&middot; . .&middot; . .&middot; . .&middot; . .
y.... .~ .... .~ .. ...
</p>
<p>K&middot;.. ;&middot;&middot; &middot;&middot;&middot;;&middot; &middot;&middot; &middot;&middot;
</p>
<p>o
x
</p>
<p>randn('state',O)
</p>
<p>x=randn (1000 , 1) j
</p>
<p>bincenters=[-3 .5 :0.5:3 .5]'j
</p>
<p>bins=length(bincenters) j
</p>
<p>h=zeros(bins,l)j
</p>
<p>for i=1:1ength(x)
</p>
<p>for k=l:bins
</p>
<p>if x(i&raquo;bincenters(k)-0.5/2
</p>
<p>&amp; x(i)&lt;=bincenters(k)+0.5/2
h(k,l)=h(k,l)+lj
</p>
<p>end
</p>
<p>end
</p>
<p>end
</p>
<p>pxest=h/(1000*0.5)j
</p>
<p>xaxis=[-4:0.01:4]'j
</p>
<p>px=(1/sqrt(2*pi))*exp(-0.5*xaxis.-2)j
</p>
<p>Figure 2.7: Estimated and true probabili ty dens ity functions.
</p>
<p>- 3 - 2 - 1
</p>
<p>0.1 . . . .. : ..;K
o .
</p>
<p>] 0.2 . . . . . ~ . .. . . : . .. .
</p>
<p>~
a
</p>
<p>'';:;
</p>
<p>~
</p>
<p>""Cl 0.3 ... . . ; . .. . . ; .. . . . ;
</p>
<p>&sect;
</p>
<p>1:0..
Cl
Po. 0.4 ' . . . . ~ ..... : .. ... : .. 'i
</p>
<p>CI&gt; &bull; &bull; &bull;
::l . . ...... ., ....
</p>
<p>M should be lar ge. In particular, if we let a = 2 and b = 00 , then we shou ld obtain
the value (which must be evaluated using numerical integration)
</p>
<p>1
00 1
</p>
<p>P [X &gt; 2] = /iL exp (-{1/2)x2 ) dx = 0.0228
2 v 21r
</p>
<p>and therefore very few realizations can be expected to fall in this inte rval. T he resul ts
</p>
<p>for an increasing number of realizations are shown in Figure 2.8. This illustrates the
</p>
<p>problem wit h the simulation of small probabili ty events. It requires a large number
</p>
<p>of realizations to obtain accurate results. (See P roblem 11.47 on how to reduce the
</p>
<p>number of realizations required.)
</p>
<p>Example 2.3 - A verage value
</p>
<p>It is frequently impor tant to measure characteristics of X in addit ion to the PDF.
</p>
<p>For example, we might only be interested in the average or m ean or expected value
</p>
<p>of X. If the random var iab le is Gaussian, t hen from Figure 2.6 we would expect X
</p>
<p>to be zero on the average. This conjecture is easily "verified" by using the sample
</p>
<p>m ean estimate
1 M
</p>
<p>ML xi
i=l</p>
<p/>
</div>
<div class="page"><p/>
<p>22 CHAPTER 2. COMPUTER SIMULATION
</p>
<p>0.0228
</p>
<p>0.0228
</p>
<p>0.0288
</p>
<p>0.0288
</p>
<p>True P[X &gt; 2] randn('state' ,0)
M=100;count=0;
</p>
<p>x=randn (M ,1) ;
</p>
<p>for i=l:M
</p>
<p>if x(i&raquo;2
</p>
<p>count=count+l;
</p>
<p>end
</p>
<p>end
</p>
<p>probest=count/M
</p>
<p>Figure 2.8: Estimated and true probabilities.
</p>
<p>0.0100
</p>
<p>0.0150
</p>
<p>0.0244
</p>
<p>0.0231
</p>
<p>Estimated P[X &gt; 2]M
100
</p>
<p>1000
</p>
<p>10,000
</p>
<p>100,000
</p>
<p>of the mean. The results are shown in Figure 2.9.
</p>
<p>M Estimated mean True mean
</p>
<p>100 0.0479 0
</p>
<p>1000 -0.0431 0
</p>
<p>10,000 0.0011 0
</p>
<p>100,000 0.0032 0
</p>
<p>r andn I ' state' ,0)
</p>
<p>M=100;
</p>
<p>meanest=O;
</p>
<p>x=randn(M,l);
</p>
<p>for i=l:M
</p>
<p>meanest=meanest+(l/M)*x(i);
</p>
<p>end
</p>
<p>meanest
</p>
<p>Figure 2.9: Estimated and true mean.
</p>
<p>Example 2.4 - A transformed random variable
</p>
<p>One of the most important problems in probability is to determine the PDF for
</p>
<p>a transformed random variable, i.e., one that is a function of X, say X 2 as an
</p>
<p>example. This is easily accomplished by modifying the code in Figure 2.7 from
</p>
<p>x=randn(1000,1) to x=randn(1000, 1) ;x=x. -2;. The results are shown in Figure
</p>
<p>2.10. Note that the shape of the PDF is completely different than the original
</p>
<p>Gaussian shape (see Example 10.7 for the true PDF). Additionally, we can obtain
</p>
<p>the mean of X 2 by using
</p>
<p>1 M
</p>
<p>MLx~
i= l</p>
<p/>
</div>
<div class="page"><p/>
<p>M Estimated mean True mean
</p>
<p>100 0.7491 1
</p>
<p>1000 0.8911 1
</p>
<p>10,000 1.0022 1
</p>
<p>100,000 1.0073 1
</p>
<p>2.5. DETERMINING CHARACTERISTICS OF RANDOM VARIABLES 23
</p>
<p>~ .
&deg;06 ..... : : : . . .." . . . .
~ ...
"0 : : :
</p>
<p>s : : :ro .8 0 .4 ..... : : : ....
.-...,
en
</p>
<p>PLl .
0.2 .. .. . ;.. .. . : .... .; ....
</p>
<p>-3 - 2 -1
</p>
<p>Figure 2.10: Estimated PDF of X 2 for X Gaussian.
</p>
<p>as we did in Example 2.3. The results are shown in Figure 2.11.
</p>
<p>randn C'state' ,0)
</p>
<p>M=100j
</p>
<p>meanest=Oj
</p>
<p>x=randn(M,l)j
</p>
<p>for i=l:M
</p>
<p>m e a n e s t = m e a n e s t + ( 1 / M ) * x ( i ) ~ 2 ;
</p>
<p>end
</p>
<p>meanest
</p>
<p>Figure 2.11: Estimated and true mean.
</p>
<p>Example 2.5 - Multiple random variables
</p>
<p>Consider an experiment that yields two random variables or the vector random
</p>
<p>variable [Xl x 2]T , where T denotes the transpose. An example might be the choice
of a point in the square {(x, y) : 0 ::; x ::; 1, 0 ::; y ::; 1} according to some procedure.
</p>
<p>This procedure mayor may not cause the value of X2 to depend on the value of
</p>
<p>Xl . For example, if the result of many repetitions of this experiment produced an
</p>
<p>even distribution of points indicated by the shaded region in Figure 2.12a , then we
</p>
<p>would say that there is no dependency between Xl and X2. On the other hand, if
</p>
<p>the points were evenly distributed within the shaded region shown in Figure 2.12b,
</p>
<p>then there is a strong dependency. This is because if, for example, Xl = 0.5, then
X2 would have to lie in the interval [0.25,0.75]. Consider next the random vector</p>
<p/>
</div>
<div class="page"><p/>
<p>24 CHAPTER 2. COMPUTER SIMULATION
</p>
<p>1.5 ,.- ---~----~--- --, 1.5 ,.----~---- ~--- __,
</p>
<p>1----------, .
</p>
<p>0.5
</p>
<p>1.50.5
O &amp;.&lt;:::.. ---~---- '-- - -----l
</p>
<p>o
</p>
<p>0.5
</p>
<p>1 : .
</p>
<p>C'l
</p>
<p>H
</p>
<p>1.50.5
</p>
<p>oL...- L...- -'
o
</p>
<p>C'l
</p>
<p>H
</p>
<p>(a) No dependency (b) Dependency
</p>
<p>Figure 2.12: Relationships between random variables.
</p>
<p>where each U, is generated using rand. The result of M = 1000 realizations is shown
in Figure 2.13a. We say that the random variables Xl and X 2 are independent. Of
</p>
<p>course, this is what we expect from a good random number generator. If instead,
</p>
<p>we defined the new random variables,
</p>
<p>[~:] [
then from the plot shown in Figure 2.13b, we would say that the random variables
</p>
<p>are dependent. Note that this type of plot is called a scatter diagram.
</p>
<p>2.6 Real-World Example - Digital Communications
</p>
<p>In a phase-shift keyed (PSK) digital communication system a binary digit (also
</p>
<p>termed a bit) , which is either a "0" or a "1" , is communicated to a receiver by
</p>
<p>sending either so(t) = A cos(211"Fot + 11") to represent a "0" or Sl(t) = Acos(211"Fot)
to represent a "1" , where A &gt; 0 [Proakis 1989]. The receiver that is used to decode
the transmission is shown in Figure 2.14. The input to the receiver is the noise</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6. REAL-WORLD EXAMPLE - DIGITAL COMMUNICATIONS 25
</p>
<p>1.5.-----~----~---___, 1.5,.-----~----~----,
</p>
<p>1 .;&bull;&bull; &bull;. ,.:.t~.,!.:; &middot;.&middot;.. ~_:""" '.,: "
&bull;&bull; , T' C'f&bull;&bull; ~,., .. : -&bull; &bull; v ~' ..
</p>
<p>i.:' ,it-.., .. ' . ."."'..'.t:\.~ &bull;&bull;&bull;".'&bull; .. ~ .. ~.-: .... .&bull;="'r&middot;- ..
:\S. .\ &bull;&bull;&bull; ~ - ,...c.: .
...:'.. i:.:,;.&middot;-y&middot;~: &middot;l ::;.::~ .~(.:.I
: !~ .. ~ . : : : : -:i- ~i&middot;:=-:. ~.:-. ~.-:' ~,
</p>
<p>0.5 ".!1 ~.~ ..I"' ; &raquo;r ~. "'''.~'t.
.:. ':::.,; .:' ;.:...,;~?&pound; I .'.-: '
..... ~ , _ " .. ~ &bull; &bull;, &bull;&bull;, &bull; - &bull; .. . ;.1
</p>
<p>... .. -:&yen;&bull;.&bull; ! '; :-11'&middot;&middot;'..~&middot;~tJ-&middot;
. :..~ -:- -: ...-:-... ~ : - .".f......-c,&bull;&bull;
-.. - ".,...... ,..."' -.r .'''.1&bull;.
</p>
<p>_10 &bull; ", &bull; 0- _1. &bull;o ....:,....... . .r.. . ' . ~ .
o 0.5
</p>
<p>(a) No depe ndency
</p>
<p>1.5 o 0.5
</p>
<p>(b ) Dep endency
</p>
<p>1.5
</p>
<p>Figure 2.13: Relationships between random vari ables.
</p>
<p>&gt; 0
:s;0
</p>
<p>x(t)
Lowpass I---_~
filter
</p>
<p>'-----
Decision device
</p>
<p>: 1o
cos(211"Fot)
</p>
<p>Figure 2.14: Receiver for a PSK digit al communicat ion system.
</p>
<p>corrupted signal or x (t ) = Si(t ) + w(t ), where w(t ) represents the channel noise.
Ignoring the effect of noise for the moment, the out put of the multiplier will be
</p>
<p>so(t) cos(211"Fot ) A cos(21rFot + 11") cos(211"Fot) = -A ( ~ + ~ cos(411"Fot ))
</p>
<p>Sl(t ) cos (211"Fot ) A cos(21rFot) cos (211"Fot) = A (~+ ~ C OS(411"Fot))
</p>
<p>for a 0 and 1 sent, respecti vely. After the lowpass filter , which filters out the
</p>
<p>cos(411"Fot) part of the signal , and sampler, we have
</p>
<p>{
</p>
<p>_ &pound;1 for a 0
e= ~
</p>
<p>'2 for a 1.
</p>
<p>The receiver decides a 1 was transmi tted if e&gt; 0 and a 0 if e::; O. To model the
channel noise we assume that the actual value of eobserved is
</p>
<p>~={
- 4+ W for a 0
4+ W for a 1</p>
<p/>
</div>
<div class="page"><p/>
<p>26 CHAPTER 2. COMPUTER SIMULATION
</p>
<p>where W is a Gaussian random variable. It is now of interest to determine how
</p>
<p>the error depends on the signal amplitude A. Consider the case of a 1 having been
</p>
<p>transmitted. Intuitively, if A is a large positive amplitude, then the chance that the
</p>
<p>noise will cause an error or equivalently, ~ ~ 0, should be small. This probability,
</p>
<p>termed the probability of error and denoted by Pe , is given by P[A/2 + W ~ OJ.
Using a computer simulation we can plot P; versus A with the result shown in Figure
</p>
<p>2.15. Also, the true P; is shown. (In Example 10.3 we will see how to analytically
</p>
<p>determine this probability.) As expected, the probability of error decreases as the
</p>
<p>54
</p>
<p>Simulated Pe .
True r,
</p>
<p>32
</p>
<p>oL_-----.i__-.i.__ ~ __ - - = : : : : : ~ ~ d
o
</p>
<p>0.1
</p>
<p>0.05 .
</p>
<p>0.15 .
</p>
<p>0.2 .
</p>
<p>0.35 .
</p>
<p>0.3 .
</p>
<p>c..."0.25 .
</p>
<p>A
</p>
<p>Figure 2.15: Probability of error for a PSK communication system.
</p>
<p>signal amplitude increases. With this information we can design our system by
</p>
<p>choosing A to satisfy a given probability of error requirement. In actual systems
</p>
<p>this requirement is usually about P; = 10-7 . Simulating this small probability
</p>
<p>would be exceedingly difficult due to the large number of trials required (but see
</p>
<p>also Problem 11.47). The MATLAB code used for the simulation is given in Figure
</p>
<p>2.16.
</p>
<p>References
</p>
<p>Proakis, J., Digitial Communications, Second Ed., McGraw-Hill, New York, 1989.
</p>
<p>Problems
</p>
<p>Note: All the following problems require the use of a computer simulation. A
</p>
<p>realization of a uniform random variable is obtained by using rand Ct , 1) while a</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS
</p>
<p>A= [0. 1 :0.1: 5] , ;
</p>
<p>for k=1:1ength(A)
</p>
<p>error=O;
</p>
<p>for i=1 :1000
</p>
<p>w=randnC1,1) ;
</p>
<p>if A(k)/2+w&lt;=0
</p>
<p>error=error+l;
</p>
<p>end
</p>
<p>end
</p>
<p>Pe(k,1)=error/l000j
</p>
<p>end
</p>
<p>27
</p>
<p>Figure 2.16: MATLAB code used to est imate the probability of error P; in Figure
</p>
<p>2.15.
</p>
<p>realization of a Gaussian random variable is obtained by using randn (1 , 1) .
</p>
<p>2.1 C : . . ~ ) (c) An experiment consists of tossing a fair coin twice. If a head occurs
</p>
<p>on the first toss, we let X l = 1 and if a tail occurs we let Xl = O. The
same assignment is used for the outcome X 2 of the second toss. Defining the
</p>
<p>random variable as Y = X IX2 , estimate the probabilities for the different
</p>
<p>possible values of Y. Explain your results.
</p>
<p>2.2 (c) A pair of fair dice is tossed. Estimate the probability of "snake eyes" or a
</p>
<p>one for each die?
</p>
<p>2.3 C:..:....&gt; (c) Estimate P[-l ::; X ::; 1] if X is a Gaussian random variable. Verify
the results of your computer simulation by numerically evaluating the integral
</p>
<p>t' _1 exp (_~x 2) dx.
t, y'2;i 2
</p>
<p>Hint: See Problem 1.14.
</p>
<p>2.4 (c) Estimate the PDF of the random variable
</p>
<p>where U, is a uniform random variable. Then, compare this PDF to the
</p>
<p>Gaussian PDF or
</p>
<p>1 (1 2)px(x) = -- exp --X .
y'2;i 2</p>
<p/>
</div>
<div class="page"><p/>
<p>28 CHAPTER 2. COMPUTER SIMULATION
</p>
<p>2.5 (c) Estimate the PDF of X = Ul - U2, where U l and U2 are uniform random
</p>
<p>variables. What is the most probable range of values?
</p>
<p>2.6 C..:J (c) Estimate the PDF of X = U1U2, where U; and U2 are uniform random
variables. What is the most probable range of values?
</p>
<p>2.7 (c) Generate realizations of a discrete random variable X, which takes on values
</p>
<p>1, 2, and 3 with probabilities Pl = 0.1, P2 = 0.2 and P3 = 0.7, respectively.
</p>
<p>Next based on the generated realizations estimate the probabilities of obtaining
</p>
<p>the various values of X.
</p>
<p>2.8 L..:J (c) Estimate the mean of U , where U is a uniform random variable. What
is the true value?
</p>
<p>2.9 (c) Estimate the mean of X +1, where X is a Gaussian random variable. What
</p>
<p>is the true value?
</p>
<p>2.10 (c) Estimate the mean of X 2 , where X is a Gaussian random variable.
</p>
<p>2.11 t.:...:,J (c) Estimate the mean of 2U, where U is a uniform random variable.
What is the true value?
</p>
<p>2.12 (c) It is conjectured that if Xl and X 2 are Gaussian random variables, then
</p>
<p>by subtracting them (let Y = Xl - X 2 ) , the probable range of values should
be smaller. Is this true?
</p>
<p>2.13 C.:') (c) A large circular dartboard is set up with a "bullseye" at the center of
</p>
<p>the circle, which is at the coordinate (0,0). A dart is thrown at the center but
</p>
<p>lands at (X, Y), where X and Yare two different Gaussian random variables.
</p>
<p>What is the average distance of the dart from the bullseye?
</p>
<p>2.14 C..:J (c) It is conjectured that the mean of VTJ, where U is a uniform random
variable, is Jmean of U. Is this true?
</p>
<p>2.15 (c) The Gaussian random variables Xl and X 2 are linearly transformed to the
</p>
<p>new random variables
</p>
<p>Yl Xl + 0.IX2
Y2 Xl + 0.2X2 .
</p>
<p>Plot a scatter diagram for Yl and Y2 . Could you approximately determine the
</p>
<p>value of Y2 if you knew that Yl = I?
</p>
<p>2.16 (c,w) Generate a scatter diagram for the linearly transformed random vari-
ables</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 29
</p>
<p>where Ul and U2 are uniform random variables. Can you explain why the
</p>
<p>scatter diagram looks like a parallelogram? Hint: Define the vectors
</p>
<p>x [;:]
</p>
<p>., [:]
</p>
<p>., [ ~ ]
and express X as a linear combination of el and e2.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 2A
</p>
<p>Brief Introduction to MATLAB
</p>
<p>A brief introduction to the scient ific software package MATLAB is contained in this
</p>
<p>appendix . Further information is available at the Web sit e www.mathworks.com.
</p>
<p>MATLAB is a scientific computation and data presentation language.
</p>
<p>Overview of MATLAB
</p>
<p>The chief advantage of MATLAB is its use of high-level instructions for matrix alge-
</p>
<p>bra and built-in routines for data processing. In this appendix as well as throughout
</p>
<p>the text a MATLAB command is indicated with the typewriter font such as end.
</p>
<p>MATLAB treats matrices of any size (which includes vectors and scalars as special
</p>
<p>cases) as elements and hence matrix multiplication is as simple as C=A*B, where
</p>
<p>A and B are conformable matrices. In addition to the usual matrix operations of
</p>
<p>addition C=A+B, multiplication C=A*B, and scaling by a constant c as B=c*A, certain
</p>
<p>matrix operators are defined that allow convenient manipulation. For example, as-
</p>
<p>sume we first define the column vector x = [1 23 4jT, where T denotes transpose, by
using x= [1: 4] ' . The vector starts with the element 1 and ends with the element
</p>
<p>4 and the colon indicates that the intervening elements are found by incrementing
</p>
<p>the start value by one , which is the default. For other increments, say 0.5, we use
</p>
<p>x= [1 : 0 . 5 :4] ' . To define the vector y = [12 22 32 42jT, we can use the matrix ele-
ment by eleme nt exponentiation operator. - to form y=x. -2 if x= [1: 4] '. Similarly,
</p>
<p>the operators . * and . / perform element by element multiplication and division of
</p>
<p>the matrices, respectively. For example, if
</p>
<p>A [~~ ]
</p>
<p>B [~~ ]</p>
<p/>
</div>
<div class="page"><p/>
<p>32
</p>
<p>Character
</p>
<p>+
</p>
<p>*
/
</p>
<p>*
./
</p>
<p>%
</p>
<p>I
&amp;
</p>
<p>CHAP TER 2. COMPUTER SIMULATION
</p>
<p>Meaning
</p>
<p>addit ion (scalars , vectors, matrices)
</p>
<p>subtraction (scalars , vectors, matrices)
</p>
<p>mul tiplication (scalars, vectors, matrices)
</p>
<p>divi sion (scalars)
</p>
<p>exponent iation (scalars, square matrices)
</p>
<p>element by element multiplication
</p>
<p>element by element division
</p>
<p>element by element exponent iation
</p>
<p>suppress printed output of operation
</p>
<p>spec ify intervening values
</p>
<p>conjuga te transpose (transpose for real vectors, matrices)
</p>
<p>line cont inua tion (when command must be split)
</p>
<p>remainder of line int erpreted as comment
</p>
<p>logical equa ls
</p>
<p>logical or
</p>
<p>logical and
</p>
<p>logical not
</p>
<p>Table 2A.1: Definition of common MAT LAB characters .
</p>
<p>then the statements C=A. *B and D=A. /B produce the results
</p>
<p>c [~1~]
</p>
<p>D [~~ ]
</p>
<p>respectively. A listing of some common charac ters is given in Table 2A.1. MAT LAB
</p>
<p>has the usual built-in functions of cos , sin, etc. for the trigonometric functions,
</p>
<p>sqrt for a square root , exp for the exponent ial function, and abs for absolute value,
</p>
<p>as well as many ot hers . When a function is applied to a matrix, the function is
</p>
<p>applied to each element of the matrix. Other built-in symbo ls and fun ctions and
</p>
<p>their meanings are given in Table 2A.2.
</p>
<p>Matrices and vectors are easily specified. For example, to define the column
</p>
<p>vecto r Cl = [1 2V, just use el=[1 2].' or equivalently cl=[1;2] . To define the C
matrix given previously, the construction C= [1 4; 9 16] is used . Or we could first
</p>
<p>define C2 = [4 16V by e2= [4 16].' and then use C= [el e2]. It is also possible
to extract portions of matrices to yield smaller matrices or vectors. For example,
</p>
<p>to extrac t the first column from the matrix C use e l=C ( : ,1). The colon indicates
</p>
<p>that all elements in the first column should be extrac ted. Many other convenient
</p>
<p>manipulations of matrices and vectors are possible.</p>
<p/>
</div>
<div class="page"><p/>
<p>APPENDIX 2A. BRIEF INTRODUCTION TO MATLAB 33
</p>
<p>Function
</p>
<p>pi
</p>
<p>i
</p>
<p>j
</p>
<p>round (x)
</p>
<p>floor(x)
</p>
<p>inv(A)
</p>
<p>x=zeros(N,l)
</p>
<p>x=ones(N,l)
</p>
<p>x=rand(N,l)
</p>
<p>x=randn(N,l)
</p>
<p>rand( 'state' ,0)
</p>
<p>randn('state' ,0)
</p>
<p>M=length(x)
</p>
<p>sum(x)
</p>
<p>mean (x)
</p>
<p>flipud(x)
</p>
<p>abs
</p>
<p>fft(x,N)
</p>
<p>ifft (x ,N)
</p>
<p>fftshift (x)
</p>
<p>pause
</p>
<p>break
</p>
<p>whos
</p>
<p>help
</p>
<p>Meaning
</p>
<p>tt
</p>
<p>A
A
rounds every element in x to the nearest integer
</p>
<p>replaces every element in x by the nearest integer less t ha n
</p>
<p>or equa l to x
</p>
<p>takes the inverse of the square matrix A
</p>
<p>assigns an N x 1 vector of all zeros to x
</p>
<p>ass igns an N x 1 vector of all ones to x
</p>
<p>generates an N x 1 vector of all uniform random variables
</p>
<p>generates an N x 1 vector of all Gau ssian random variables
</p>
<p>ini ti alizes uniform random number generator
</p>
<p>ini ti alizes Gaussian random number generator
</p>
<p>sets M equal to N if x is N x 1
</p>
<p>sums all elements in vecto r x
</p>
<p>computes t he sample mean of the elements in x
</p>
<p>flips the vecto r x upside down
</p>
<p>takes the absolute value (or complex magnit ude) of every
</p>
<p>element of x
</p>
<p>computes t he FFT of length N of x (zero pads if
</p>
<p>N&gt;length(x) )
</p>
<p>computes the inverse FFT of length N of x
</p>
<p>interchanges the two halves of an FFT output
</p>
<p>pauses the execut ion of a program
</p>
<p>terminates a loop when encountered
</p>
<p>list s all variabl es and their attributes in current workspace
</p>
<p>provides help on commands , e.g., help sqrt
</p>
<p>Table 2A.2: Definit ion of useful MATLAB symbols and functions.</p>
<p/>
</div>
<div class="page"><p/>
<p>34 CHAPTER 2. COMPUTER SIMULATION
</p>
<p>Any vector that is generated whose dimensions are not explicit ly specified is
</p>
<p>assumed to be a row vector. For example, if we say x=ones (10) , then it will be
</p>
<p>designated as the 1 x 10 row vector consist ing of all ones. To yield a column vector
</p>
<p>use x=ones (10,1).
</p>
<p>Loops are implemented with the construction
</p>
<p>for k=1: 10
</p>
<p>x(k,1)=1j
</p>
<p>end
</p>
<p>which is equivalent to x=ones (10 , 1). Logical flow can be accomplished with the
</p>
<p>construct ion
</p>
<p>if x&gt;O
</p>
<p>y=sqrt(x)j
</p>
<p>else
</p>
<p>y=Oj
</p>
<p>end
</p>
<p>Finally, a good practice is to begin each program or script , which is called an "m"
</p>
<p>file (due to its syntax, for example, pdf .m), with a clear all command. This
</p>
<p>will clear all vari ables in the workspace, since ot herwise the cur rent program may
</p>
<p>inad vertently (on the par t of the programmer) use previously stored variab le data.
</p>
<p>Plotting in MATLAB
</p>
<p>Plot ting in MATLAB is illustrated in the next secti on by example. Some useful
</p>
<p>fun ctions are summarized in Table 2A.3.
</p>
<p>Function
</p>
<p>figure
</p>
<p>plot(x,y)
</p>
<p>plot(x1,y1,x2,y2)
</p>
<p>plot (x , y , , . ' )
</p>
<p>title ('my plot')
</p>
<p>x'label C' x ")
</p>
<p>ylabel( , y')
</p>
<p>grid
</p>
<p>axis ([0 1 2 4])
</p>
<p>text(1,1,'curve 1')
</p>
<p>hold on
</p>
<p>hold off
</p>
<p>Meaning
</p>
<p>opens up a new figur e window
</p>
<p>plots the elements of x versus the elements of y
</p>
<p>same as above except multiple plots are made
</p>
<p>same as plot except the points are not connected
</p>
<p>pu ts a title on the plot
</p>
<p>lab els the x axis
</p>
<p>lab els the y axis
</p>
<p>draws grid on the plot
</p>
<p>plots only the points in range 0 ::; x ::; 1 and 2 ::; Y ::; 4
</p>
<p>places the text "curve I" at t he point (1,1)
</p>
<p>holds cur rent plot
</p>
<p>releases current plot
</p>
<p>Table 2A.3: Definition of useful MAT LAB plotting fun ctions.</p>
<p/>
</div>
<div class="page"><p/>
<p>APPENDIX 2A. BRIEF INTRODUCTION TO MATLAB
</p>
<p>An Example Program
</p>
<p>35
</p>
<p>A comp lete MAT LAB program is given below to illustrate how one might compute
</p>
<p>the samples of several sinusoids of different amplitudes. It also allows the sinusoids
</p>
<p>to be clipped. The sinusoid is s(t) = A cos(271'Fot + 71'/3), with A = 1, A = 2, and
A = 4, Fo = 1, and t = 0,0.01,0.02, . . . , 10. The clipping level is set at &plusmn; 3, i.e., any
</p>
<p>sample above +3 is clipped to + 3 and any sample less than -3 is clipped to -3.
</p>
<p>% matlabexample.m
%
% This program computes and plots samples of a sinusoid
% with amplitudes 1, 2, and 4. If desired, the sinusoid can be
% clipped to simulate the effect of a limiting device.
% The frequency is 1 Hz and the time duration is 10 seconds.
% The sample interval is 0.1 seconds. The code is not efficient but
% is meant to illustrate MATLAB statements.
%
clear all %clear all variables from workspace
delt=O.Ol; %set sampling time interval
FO=l ; % set frequency
t=[0:delt:l0]'; %compute time samples 0,0.01,0.02, ... ,10
A=[l 2 4]'; %set amplitudes
clip='yes'; %set option to clip
for i=l:length(A) %begin computation of sinusoid samples
</p>
<p>s(:,i)=A(i)*cos(2*pi*FO*t+pi/3); %note that samples for sinusoid
%are computed all at once and
%stored as columns in a matrix
</p>
<p>if clip=='yes' %determine if clipping desired
for k=l:length(s( :, i)) %note that number of samples given as
</p>
<p>%dimension of column using length command
if s(k,i&raquo;3 %check to see if sinusoid sample exceeds 3
</p>
<p>s(k,i)=3j %if yes, then clip
elseif s(k,i)&lt;-3 %check to see if sinusoid sample is less
</p>
<p>s(k,i)=-3j %than -3 if yes, then clip
end
</p>
<p>end
</p>
<p>end
</p>
<p>end
</p>
<p>figure %open up a new figure window
plot(t,s(:,l),t,s(: ,2),t,s(:,3)) %plot sinusoid samples versus time
</p>
<p>% samples for all three sinusoids
grid %add grid to plot
xlabel('time, t') % label x-axis</p>
<p/>
</div>
<div class="page"><p/>
<p>36 CHAPTER 2. COMPUTER SIMULATION
</p>
<p>ylabel('s(t)') 'l. label y-axis
axis([O 10 -4 4]) 'l. set up axes using axis([xmin xmax ymin ymax])
</p>
<p>legend('A=1' ,'A=2 ','A=4 ') 'l. display a legend to distinguish
'l. different s inusoids
</p>
<p>The output of the program is shown in Figure 2A.1. Note that the different graphs
will appear as different colors.
</p>
<p>4,-- --,----,-- -.---- - .---- ----,.-- ----,-- - ---.-- - -,-- - ,--- ---,
</p>
<p>3
</p>
<p>2
</p>
<p>~ 0 . .
</p>
<p>-1
</p>
<p>-3 ..
</p>
<p>2 3 4 5 6 7 8 9 10
time,!
</p>
<p>Figure 2A.l : Output of MATLAB program matlabexample .m,</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 3
</p>
<p>Basic Probability
</p>
<p>3.1 Introduction
</p>
<p>We now begin the formal st udy of probability. We do so by utilizing the properties
</p>
<p>of sets in conjunction with the axiomatic approach to probability. In particular , we
</p>
<p>will see how to solve a class of probability problems via counting methods. These
</p>
<p>are problems such as determining the probability of obtaining a royal flush in poker
</p>
<p>or of obtaining a defective item from a batch of mostly good items , as examples.
</p>
<p>Furthermo re, the axiomatic approach will provide the basis for all our further studies
</p>
<p>of probability. Only the methods of determining the probabilities will have to be
</p>
<p>mod ified in accordance wit h the problem at hand.
</p>
<p>3.2 Summary
</p>
<p>Section 3.3 reviews set theory, with Figure 3.1 illustrating the standard definitions.
</p>
<p>Manipulation of sets can be facilitated using De Morgan 's laws of (3.6) and (3.7) .
</p>
<p>The applicat ion of set theory to probability is summarized in Table 3.1. Using the
</p>
<p>three axioms described in Section 3.4 a theory of probability can be formulated
</p>
<p>and a means for computing probabilities constructed. Properties of the probability
</p>
<p>functi on are given in Section 3.5. In addition, the probability for a union of three
</p>
<p>events is given by (3.20). An equally likely probability assignment for a cont inuous
</p>
<p>sample space is given by (3.22) and is shown to satisfy the basic axioms. Section 3.7
</p>
<p>introduces the determination of probabilities for discrete sample spaces with equally
</p>
<p>likely outcomes. The basic formula is given by (3.24). To implement this approach
</p>
<p>for more complicated problems in which brute-force count ing of outcomes is not
</p>
<p>possible, the subject of combina torics is describ ed in Section 3.8. Permutations and
</p>
<p>combinations are defined and applied to severa l examples for computing probabili-
</p>
<p>ties. Based on these counting methods the hypergeometric probability law of (3.27)
</p>
<p>and the binomial probability law of (3.28) are der ived in Section 3.9. Finally, an
</p>
<p>example of the applicat ion of the binomial law to a quality control problem is given
</p>
<p>in Section 3.10.</p>
<p/>
</div>
<div class="page"><p/>
<p>38 CHAPTER 3. BASIC PROBABILITY
</p>
<p>3.3 Review of Set Theory
</p>
<p>The reader has undoubtedly been introduced to set theory at some point in his/her
</p>
<p>education. We now summarize only the salient definitions and properties that are
</p>
<p>germane to probability. A set is defined as a collection of objects, for example,
</p>
<p>the set of students in a probability class. The set A can be defined either by the
</p>
<p>enumeration method, i.e., a listing of the students as
</p>
<p>A = {Jane, Bill, Jessica, Fred}
</p>
<p>or by the description method
</p>
<p>A = {students: each student is enrolled in the probability class}
</p>
<p>(3.1)
</p>
<p>where the '':" is read as "such that". Another example would be the set of natural
</p>
<p>numbers or
</p>
<p>B
</p>
<p>B
</p>
<p>{I, 2, 3, ...}
</p>
<p>{I: I is an integer and I ~ I}
</p>
<p>(enumeration)
</p>
<p>(description).
</p>
<p>(3.2)
</p>
<p>Each object in the set is called an element and each element is distinct. For example,
</p>
<p>the sets {I, 2, 3} and {I , 2, 1, 3} are equivalent. There is no reason to list an element
</p>
<p>in a set more than once. Likewise, the ordering of the elements within the set
</p>
<p>is not important. The sets {I, 2, 3} and {2, 1, 3} are equivalent. Sets are said to
</p>
<p>be equal if they contain the same elements. For example, if C1 = {Bill , Fred}
and C2 = {male members in the probability class}, then C1 = C2 &bull; Although the
description may change, it is ultimately the contents of the set that is of importance.
</p>
<p>An element x of a set A is denoted using the symbolism x E A, and is read as "x
</p>
<p>is contained in A" , as for example, 1 E B for the set B defined in (3.2) . Some sets
</p>
<p>have no elements. If the instructor in the probability class does not give out any
</p>
<p>grades of "A", then the set of students receiving an "A" is D = {}. This is called
the empty set or the null set. It is denoted by 0 so that D = 0. On the other hand,
the instructor may be an easy grader and give out all "A" s. Then, we say that
</p>
<p>D = S, where S is called the universal set or the set of all students enrolled in the
probability class. These concepts, in addition to some others, are further illustrated
</p>
<p>in the next example.
</p>
<p>Example 3.1 - Set concepts
</p>
<p>Consider the set of all outcomes of a tossed die. This is
</p>
<p>A = {1,2,3,4,5,6}. (3.3)
</p>
<p>The numbers 1,2,3,4,5,6 are its elements, which are distinct. The set of integer
</p>
<p>numbers from 1 to 6 or B = {I : 1 ~ I ~ 6} is equal to A. The set A is also
the universal set S since it contains all the outcomes. This is in contrast to the set</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3. REVIEW OF SET THEORY 39
</p>
<p>C = {2, 4, 6}, which contains only the even outcomes. The set C is called a subset
of A. A simple set is a set containing a single element, as for example, C = {I }.
</p>
<p>.ffi Element vs. simple set
In the example of the prob abili ty class consider the set of ins tructors. Usually,
</p>
<p>there is only one inst ructor and so the set of instructors can be defined as the
</p>
<p>simple set A = {Professor Laplace}. However , this is not the same as the "element"
given by Professor Laplace. A distinction is therefore made between the instructors
</p>
<p>teaching probability and an individual instructor. As another example, it is clear
</p>
<p>that somet imes elements in a set can be added, as, for example, 2 + 3 = 5, but it
makes no sense to add sets as in {2} + {3} = {5}.
</p>
<p>More form ally, a set B is defined as a subset of a set A if every element in B is also
</p>
<p>an element of A. We write this as B c A . This also includes the case of B = A . In
fact , we can say that A = B if A c Band Be A.
</p>
<p>Besides subsets, new sets may be derived from other set s in a number of ways. If
</p>
<p>S = {x : - 00 &lt; x &lt; oo} (called the set of real numbers) , then A = {x : 0 &lt; x ~ 2}
is clearl y a subset of S . The complement of A , denoted by AC, is the set of elements
</p>
<p>in S but not in A . This is AC= {x : x ~ 0 or x &gt; 2}. Two sets can be combined
together to form a new set. For example, if
</p>
<p>A
</p>
<p>B
</p>
<p>{x: 0 ~ x ~ 2}
</p>
<p>{x : 1 ~ x ~ 3} (3.4)
</p>
<p>then the union of A and B , denoted by A U B, is the set of elements that belong to
</p>
<p>A or B or both A and B (so-called inclusive or). Hence, AU B = {x : 0 ~ x ~ 3}.
</p>
<p>This definition may be extended to multiple sets AI , A2 , &bull;&bull;&bull; , AN so that the union
</p>
<p>is the set of elements for which each element belongs to at least one of these sets .
</p>
<p>It is denoted by
</p>
<p>N
</p>
<p>Al U A2 U A2 U . .. U AN = UAi.
i= l
</p>
<p>The intersection of sets A and B , denoted by An B , is defined as the set of elements
</p>
<p>that belong to both A and B. Hence, An B = {x : 1 ~ x ~ 2} for the sets of (3.4) .
</p>
<p>We will somet imes use the shortened symbolism AB to denote AnB. This definition
</p>
<p>may be extended to multiple sets AI , A2 , ... , AN so that the intersection is the set</p>
<p/>
</div>
<div class="page"><p/>
<p>40 CHAPTER 3. BASIC PROBABILITY
</p>
<p>of elements for which each element belongs to all of these sets. It is denoted by
</p>
<p>N
</p>
<p>Al n A2 n A2 n ... nAN = nAi&middot;
i=I
</p>
<p>The difference between sets, denoted by A - B , is the set of elements in A but not
</p>
<p>in B. Hence , for the sets of (3.4) A - B = {x : 0 ::; x &lt; I}. These concepts can
be illustrated pictorially using a Venn diagram as shown in Figure 3.1. The darkly
</p>
<p>(a) Universal set S
</p>
<p>(d) Set AU B
</p>
<p>(b) Set A
</p>
<p>(e) Set An B
</p>
<p>(c) Set AC
</p>
<p>(f) Set A - B
</p>
<p>Figure 3.1: Illustration of set definitions - darkly shaded region indicates the set.
</p>
<p>shaded regions are the sets described. The dashed portions are not included in the
</p>
<p>sets. A Venn diagram is useful for visualizing set operations. As an example, one
</p>
<p>might inquire whether the sets A - B and An B C are equivalent or if
</p>
<p>A-B = AnBc . (3.5)
</p>
<p>From Figures 3.2 and 3.1&pound; we see that they appear to be. However, to formally
</p>
<p>prove that this relationship is true requires one to let C = A - B, D = A n BC and
prove that (a) C C D and (b) Dee. To prove (a) assume that x E A-B. Then,
</p>
<p>by definition of the difference set (see Figure 3.1&pound;) x E A but x is not an element of
</p>
<p>B. Hence, x E A and x must also be an element of B C. Since D = An B C, x must
be an element of D. Hence , x E A n B C and since this is true for every x E A - B ,</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3. REVIEW OF SET THEORY 41
</p>
<p>n =
</p>
<p>Figure 3.2: Using Venn diagrams to "validate" set relationships.
</p>
<p>we have that A - B cAn B C &bull; The reader is asked to complete the proof of (b) in
Problem 3.6.
</p>
<p>With the foregoing set definitions a number of results follow. They will be useful
</p>
<p>in manipulating sets to allow easier calculation of probabilities. We now list these.
</p>
<p>3. A U 0 = A, A n 0 = 0
</p>
<p>4. AUS=S,AnS=A
</p>
<p>5. S C= 0, 0c = S.
</p>
<p>If two sets A and B have no elements in common, they are said to be disjoint.
</p>
<p>The condition for being disjoint is therefore An B = 0. If, furthermore, the sets
contain between them all the elements of S, then the sets are said to partition the
</p>
<p>universe. This latter additional condition is that AU B = S. An example of sets
</p>
<p>that partition the universe is given in Figure 3.3. Note also that the sets A and AC
</p>
<p>= U
</p>
<p>Figure 3.3: Sets that partition the universal set.
</p>
<p>are always a partitioning of S (why?). More generally, mutually disjoint sets or sets
</p>
<p>AI, A 2 , . . . ,AN for which Ai n Aj = 0 for all i i- j are said to partition the universe
if S = U~IAi (see also Problem 3.9 on how to construct these sets in general). For
</p>
<p>example, the set of students enrolled in the probability class, which is defined as the
</p>
<p>universe (although of course other universes may be defined such as the set of all</p>
<p/>
</div>
<div class="page"><p/>
<p>42 CHAPTER 3. BASIC PROBABILITY
</p>
<p>students attending the given university), is partitioned by
</p>
<p>Al = {males} = {Bill, Fred}
</p>
<p>A2 = {females} = {Jane, Jessica}.
</p>
<p>Algebraic rules for manipulating multiple sets, which will be useful, are
</p>
<p>1. AUB=BuA
</p>
<p>AnB=BnA
</p>
<p>2. A U (B U C) = (A U B) U C
An (B n C) = (A n B) n C
</p>
<p>3. An (B U C) = (A n B) U (A n C)
Au (B n C) = (A U B) n (A U C)
</p>
<p>commutative properties
</p>
<p>associative properties
</p>
<p>distributive properties.
</p>
<p>Another important relationship for manipulating sets is De Morgan's law. Referring
</p>
<p>(a) Set Au B
</p>
<p>Figure 3.4: Illustration of De Morgan's law.
</p>
<p>to Figure 3.4 it is obvious that
</p>
<p>(3.6)
</p>
<p>(3.7)
</p>
<p>which allows one to convert from unions to intersections. To convert from intersec-
</p>
<p>tions to unions we let A = CC and B = DC in (3.6) to obtain
</p>
<p>and therefore
</p>
<p>cnD = (CCUDC)c.
</p>
<p>In either case we can perform the conversion by the following set of rules:
</p>
<p>1. Change the unions to intersections and the intersections to unions (A U B =}
</p>
<p>AnB)
</p>
<p>2. Complement each set (A n B =} AC nBC)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4. ASSIGNING AND DETERMINING PROBABILITIES
</p>
<p>3. Complement the overall express ion (ACn B C ::::} (ACn BC)C).
</p>
<p>43
</p>
<p>Finally, we discuss the size of a set. This will be of ext reme importance in assign-
</p>
<p>ing probabili t ies. The set {2,4, 6} is a finite set , having a finit e number of elements .
The set {2,4, 6, ... } is an infinite set , having an infini te number of elements. In
</p>
<p>the lat ter case, although the set is infinite, it is said to be countably infinite . This
</p>
<p>means that "in theory" we can count the number of elements in the set . (We do so
</p>
<p>by pairing up each element in the set with an element in the set of natural numbers
</p>
<p>or {I , 2, 3, .. .}). In eit her case, the set is said to be discrete. The set may be pic-
</p>
<p>ture d as points on t he real line. In contras t to these sets the set {x : 0 ::; x ::; I} is
</p>
<p>infinite and cannot be counted. This set is termed continuous and is pictured as a
</p>
<p>line segment on the real line. Another example follows.
</p>
<p>Example 3.2 - Size of sets
</p>
<p>The sets
</p>
<p>A
</p>
<p>B
</p>
<p>G
</p>
<p>{
I I II} finit e set _ discrete
8 ' 4' 2 '
</p>
<p>{ 1, ~ , l,~ , ...} countably infinite set - discret e
{x : 0 ::; x ::; I} infinite set - cont inuous
</p>
<p>are pictured in Figure 3.5.
</p>
<p>3rd 2nd 1st element
</p>
<p>1-&middot; &bull;
o
</p>
<p>+
1
</p>
<p>\ t t
I.. &middot;&bull; &bull; +
o 1 o 1
</p>
<p>(a) Finite set , A (b) Countably infinit e
</p>
<p>set, B
(c) Infinite cont inuous
</p>
<p>set, C
</p>
<p>Figure 3.5: Examples of sets of different sizes.
</p>
<p>3.4 Assigning and Determining Probabilities
</p>
<p>In the previous section we reviewed various aspects of set theory. This is because the
</p>
<p>concept of set s and operations on sets provide an ideal description for a probabilistic</p>
<p/>
</div>
<div class="page"><p/>
<p>44 CHAPTER 3. BASIC PROBABILITY
</p>
<p>model and the means for determining the probabilites associated with the model.
</p>
<p>Consider the tossing of a fair die . The possible outcomes comprise the elements
</p>
<p>of the set S = {I , 2, 3, 4, 5, 6}. Note that this set is composed of all the possible
</p>
<p>outcomes, and as such is the universal set . In probability theory S is t ermed the
</p>
<p>sample space and its elements s are the out comes or sample points. At times we may
</p>
<p>be interest ed in a particular outcome of the die tossing exper iment . Other times we
</p>
<p>might not be interested in a particular outcome, but whether or not the outcome
</p>
<p>was an even number , as an example. Hence, we would inquire as to whether the
</p>
<p>outcome was included in the set Eeven = {2, 4, 6}. Clearly, E even is a subset of S
and is termed an event. The simplest type of events are the ones that contain only
</p>
<p>a single outcome such as E 1 = {I}, E2 = {2}, or E 6 = {6}, as examples. These are
called simple events. Other events are S , the sample space itself, and 0 = {}, the
set with no outcomes. These events are termed the certain event and the impossible
</p>
<p>event, respectively. This is because the outcome of the experiment must be an
</p>
<p>element of S so that S is certain to occur. Also, the event that does not contain any
</p>
<p>outcomes cannot occur so that this event is impossible. Note that we are saying that
</p>
<p>an event occurs if the outcome is an eleme nt of the defining set of that event. For
</p>
<p>example, the event that a tossed die produces an even number occurs if it comes up
</p>
<p>a 2 or a 4 or a 6. These numbers are just the elements of E even . Disjoint sets such
</p>
<p>as {I , 2} and {3,4} are said to be mutually exclusive , in that an outcome cannot
</p>
<p>be in both sets simultaneously and hence both events cannot occur. The events
</p>
<p>then are said to be mutually exclusive. It is seen that probabilistic questions can
</p>
<p>be formulated using set theory, albeit with its own terminology. A summary of the
</p>
<p>equivalent terms used is given in Table 3.1.
</p>
<p>Set theory Probability theory Probability symbol
</p>
<p>universe sample space (certain event ) S
</p>
<p>element outcome (sample point) s
</p>
<p>subset event E
</p>
<p>disjoint sets mutually exclusive events E 1 nE2 = 0
null set impossible event 0
simple set simple event E = {s}
</p>
<p>Table 3.1: Terminology for set and probability theory.
</p>
<p>In order to develop a theory of probability we must next assign probabilities to
</p>
<p>events. For example, what is the probability that the tossed die will produce an
</p>
<p>even outcome? Denoting this probability by P[Eeven}, we would intuitively say that
</p>
<p>it is 1/2 since there are 3 chances out of 6 to produce an even outcome. Note that P
is a probability function or a func tion that assigns a number between 0 and 1 to sets.
</p>
<p>It is sometimes called a set function. The reader is familiar with ordinary functions
</p>
<p>such as g(x ) = exp( x) , in which a number y , where y = g(x) , is assigned to each x</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4. ASSIGNING AND DETERMINING PROBABILITIES 45
</p>
<p>for - 00 &lt; x &lt; 00, and where each x is a distinct num ber. The probability function
must assign a number to every event , or to every set. For a coin toss whose outcome
</p>
<p>is eit her a head H or a tail T , all the events are E 1 = {H} , E2 = {T} , E3 = S ,
and E4 = 0. For a die toss all t he events are Eo = 0, E 1 = {I}, . . . , E6 = {6},
E 12 = {I , 2}, . . . , E56 = {5, 6}, . . ., E 12345 = {I , 2, 3, 4, 5}, .. . , E23456 = {2, 3, 4, 5, 6},
E123456 = {I , 2, 3, 4, 5, 6} = S . There are a total of 64 event s. In general , if the
</p>
<p>sample space has N simple events , t he total number of events is 2N (see Problem
</p>
<p>3.15). We must be able to assign probabilities to all of these. In accordance with
</p>
<p>our intuitive notion of probability we assign a number , eit her zero or positive, to
</p>
<p>each event . Hence, we require that
</p>
<p>Axiom 1 prE] ~ 0 for every event E.
</p>
<p>Also, since the die toss will always produce an outcome that is included in S =
</p>
<p>{I, 2, 3, 4, 5, 6} we should require that
</p>
<p>Axiom 2 P[S] = 1.
</p>
<p>Next we might inquire as to the assignment of a probability to the event that the
</p>
<p>die comes up either less than or equa l to 2 or equal to 3. Intuitively, we would say
</p>
<p>that it is 3/ 6 since
</p>
<p>P[{l ,2} U {3}] = P[{1,2} ] + P[{3}]
2 1 1
(; + (; = 2&middot;
</p>
<p>However , we would not assert that the probability of the die coming up either less
</p>
<p>than or equa l to 3 or equal to 3 is
</p>
<p>P[{l ,2,3} U{3}] P[{l ,2, 3}] + P[{3}]
31 4
- + - = - .
6 6 6
</p>
<p>This is because the event {I , 2, 3} U {3} is just {I , 2, 3} (we should not count the
</p>
<p>3 twice) and so the probability should be 1/2. In the first example, the events are
</p>
<p>mutually exclusive (the sets are disjoint) while in the second example they are not.
</p>
<p>Hence, the probability of an event that is the union of two mutually exclusive events
</p>
<p>should be the sum of the probabilities. Combining this axiom with the previous ones
</p>
<p>produces the full set of axioms, which we summarize next for convenience.
</p>
<p>Axiom 1 prE] ~ 0 for every event E
</p>
<p>Axiom 2 P[S] = 1
</p>
<p>Axiom 3 prE U F ]= prE ]+ P[F] for E and F mutually exclusive.
</p>
<p>Using induction (see Problem 3.17) the third axiom may be extended to</p>
<p/>
</div>
<div class="page"><p/>
<p>46 CHAPTER 3. BASIC PROBABILITY
</p>
<p>N
</p>
<p>Axiom 3' P [Ui:1 Ei] = LP[Ei] for all Ei 's mutually exclusive.
</p>
<p>i=l
</p>
<p>The acceptance of these axioms as the basis for probability is called the axiomatic
</p>
<p>approach to probabilit y. It is remarkable that these three axioms, along with a fourth
</p>
<p>axiom to be introduced lat er , are adequate to formulate the entire theory. We now
</p>
<p>illustrate the application of these axioms to probability calculat ions.
</p>
<p>Example 3.3 - Die toss
</p>
<p>Determine the probability that the outcome of a fair die toss is even. The event
</p>
<p>is Eeven = {2, 4, 6}. The assumption that the die is fair means that each outcome
must be equally likely. Defining E; as the simple event {i} we note that
</p>
<p>and from Axiom 2 we must have
</p>
<p>P [,Q, E,] = PIS] = 1. (3.8)
</p>
<p>But since each Ei is a simple event and by definition the simple events are mutually
</p>
<p>exclusive (only one outcome or simple event can occur ), we have from Axiom 3' that
</p>
<p>(3.9)
</p>
<p>Next we note that the outcomes are assumed to be equally likely which means that
</p>
<p>prEll = P[E2 ] = ... = P[E6 ] = p. Hence, we must have from (3.8) and (3.9) that
</p>
<p>6
</p>
<p>LP[Ei] = 6p = 1
i= l
</p>
<p>or P[Ei] = 1/6 for all i. We can now finally det ermine P[Eeven ] since Eeven
E2 U E4 U E6 &bull; By applying Axiom 3' once again we have
</p>
<p>&lt;:;
In general, the probabilities assigned to each simple event need not be the same,
</p>
<p>i.e., the outcomes of a die toss may not have equal probabilities. One might have
</p>
<p>weighted the die so tha t the number 6 comes up twice as ofte n as all the others. The
</p>
<p>numbers 1,2,3, 4,5 could still be equally likely. In such a case , since the probabilities
</p>
<p>of the all the simple events must sum to one, we would have the assignment P[{i }] =</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4. ASSIGNING AND DETERMINING PROBABILITIES 47
</p>
<p>1/7 for i = 1,2, 3, 4, 5 and P[{6}] = 2/7. In either case, to compute the probabili ty
of any event it is only necessary to sum the probabili ties of the simple events that
</p>
<p>make up that event . Let ti ng P[{sill be the probability of the it h simple event we
have that
</p>
<p>prE] = L P[{sd ]&middot; (3.10)
{i :SiE E }
</p>
<p>We now simplify the notation by omitting the { } when referring to events . Instead
</p>
<p>of P[{I }] we will use P[I ]. Anot her example follows.
</p>
<p>Example 3.4 - Defective die toss
</p>
<p>A defective die is to ssed whose sides have been mistakenly manufactured with the
</p>
<p>number of dots being 1,1 ,2,2,3,4. The simple events are S 1 = 1, S 2 = 1, S 3 = 2,
</p>
<p>S 4 = 2, S 5 = 3, S6 = 4. Even though some of the outcomes have the same number
</p>
<p>of dot s, they are actually different in that a different side is being observed . Each
</p>
<p>side is equa lly likely to appear. What is the probability that the outcome is less
</p>
<p>than 3? Noting that the event of interest is {S1 , S 2 ,S3 , S 4} , we use (3.10) to obtain
</p>
<p>4 4
prE] = P[outcome &lt; 3] = L P[Si] = -.
</p>
<p>i =::1 6
</p>
<p>o
The formula given by (3.10) also applies to probabil ity problems for which the sample
</p>
<p>space is count ably infini t e. T herefore, it applies to all discre te sample spaces (see
</p>
<p>also Example 3.2).
</p>
<p>Example 3.5 - Countably infinite sample space
</p>
<p>A habitually tardy person ar rives at the theater late by s, minutes, where
</p>
<p>Si = i i = 1,2,3 .. . .
</p>
<p>If P[Si] = (1/ 2)i, what is the probability that he will be more than 1 minute late?
The event is E = {2, 3, 4, ... }. Using (3.10) we have
</p>
<p>PIE] = t, G)'
Using the formula for the sum of a geometric progression (see Appendix B)
</p>
<p>00 k
</p>
<p>L
&middot; aa l - __
</p>
<p>I- a
i =::k
</p>
<p>for lal &lt; 1
</p>
<p>we have that
</p>
<p>p rE] = ( ~) : = ~.
1- 2" 2
</p>
<p>In the above example we have implicitly used the relationship</p>
<p/>
</div>
<div class="page"><p/>
<p>48 CHAPTER 3. BASIC PROBABILITY
</p>
<p>(3.11)
</p>
<p>where E; = {Si} and hence the Ei'S are mutually exclusive. This does not automat-
ically follow from Axiom 3' since N is now infinite. However, we will assume for our
</p>
<p>problems of interest that it does. Adding (3.11) to our list of axioms we have
</p>
<p>00
</p>
<p>Axiom 4 P[U~l Ei] = LP[Ei] for all Ei's mutually exclusive.
i=l
</p>
<p>See [Billingsley 1986] for further details.
</p>
<p>3.5 Properties of the Probability Function
</p>
<p>From the four axioms we may derive many useful properties for evaluating proba-
</p>
<p>bilities. We now summarize these properties.
</p>
<p>Property 3.1 - Probability of complement event
</p>
<p>P[E C ] = 1 - prE]. (3.12)
</p>
<p>Proof: By definition E U E C = S. Also, by definition E and E C are mutually
exclusive. Hence,
</p>
<p>1 P[S]
</p>
<p>P[EUEC ]
</p>
<p>prE] +P[EC]
</p>
<p>(Axiom 2)
</p>
<p>(definition of complement set)
</p>
<p>(Axiom 3)
</p>
<p>from which (3.12) follows.
</p>
<p>o
We could have determined the probability in Example 3.5 without the use of the
</p>
<p>geometric progression formula by using prE] = 1 - P[EC ] = 1 - P[l] = 1/2.
</p>
<p>Property 3.2 - Probability of impossible event
</p>
<p>P[0] = o.
Proof: Since 0 = SCwe have
</p>
<p>(3.13)
</p>
<p>P[0] P[SC]
</p>
<p>= 1-P[S]
</p>
<p>1- 1
</p>
<p>O.
</p>
<p>(from Property 3.1)
</p>
<p>(from Axiom 2)
</p>
<p>o</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5. PROPERTIES OF THE PROBABILITY FUNCTION 49
</p>
<p>We will see later that t here are other events for which the probability can be zero.
</p>
<p>Thus, t he converse is not tru e.
</p>
<p>Property 3.3 - All probabilities are between 0 and 1.
</p>
<p>S E U E C (definit ion of complement set)
</p>
<p>P[S] p rE ] + P[E C ] (Axiom 3)
1 prE] + P[EC ] (Axiom 2)
</p>
<p>But from Axiom 1 P[EC] 2: 0 and therefore
</p>
<p>(3.14)
</p>
<p>Combining this result with Axiom 1 proves Property 3.3.
</p>
<p>o
</p>
<p>Property 3.4 - Formula for prE U F ] where E and F are not mutually
</p>
<p>exclusive
</p>
<p>p rE U F] = p rE] + P[F] - P[EF]. (3.15)
</p>
<p>(We have shortened En F to E F .)
</p>
<p>P roof: By the definition of E - F we have that E U F = (E - F ) U F (see Figure
</p>
<p>3.1d,f) . Also, t he events E - F and F are by definition mutu ally exclusive. It follows
</p>
<p>that
</p>
<p>prE U F ] = prE - F] + P[F] (Axiom 3). (3.16)
</p>
<p>But by definition E = (E - F ) U EF (draw a Venn diagram) and E - F and EF
</p>
<p>are mutua lly exclusive. Thus,
</p>
<p>prE] = prE - F] + P[EF]
</p>
<p>Combining (3.16) and (3.17) produces Property 3.4.
</p>
<p>(Axiom 3). (3.17)
</p>
<p>o
The effect of this formula is to make sure that the intersection E F is not counted
</p>
<p>twice in the probability calculation. This would be the case if Axiom 3 were mis-
</p>
<p>takenly applied to sets that were not mutually exclusive. In the die example, if we
</p>
<p>wanted the probability of the die coming up eit her less than or equal to 3 or equal
</p>
<p>to 3, then we would first define
</p>
<p>E {1,2,3}
</p>
<p>F {3}</p>
<p/>
</div>
<div class="page"><p/>
<p>50 CHAPTER 3. BASIC PROBABILITY
</p>
<p>so that EF = {3}. Using Property 3.4, we have that
</p>
<p>311 3
P[E U F] = P[E] + P[F] - P[EF] = "6 + 6 - "6 = 6&middot;
</p>
<p>Of course, we could just as easily have noted that E U F = {I, 2, 3} = E and then
applied (3.10). Another example follows.
</p>
<p>Example 3.6 - Switches in parallel
</p>
<p>A switching circuit shown in Figure 3.6 consists of two potentially faulty switches in
</p>
<p>parallel. In order for the circuit to operate properly at least one of the switches must
</p>
<p>switch 1
</p>
<p>switch 2
</p>
<p>Figure 3.6: Parallel switching circuit.
</p>
<p>close to allow the overall circuit to be closed. Each switch has a probability of 1/2 of
</p>
<p>closing. The probability that both switches close simultaneously is 1/4. What is the
</p>
<p>probability that the switching circuit will operate correctly? To solve this problem
</p>
<p>we first define the events E 1 = {switch 1 closes} and E 2 = {switch 2 closes}. The
</p>
<p>event that at least one switch closes is E 1 U E2. This includes the possibility that
</p>
<p>both switches close. Then using Property 3.4 we have
</p>
<p>P[E1] + P[E2 ] - P[E1E2 ]
1 1 1 3
2: + 2: - 4 = 4&middot;
</p>
<p>Note that by using two switches in parallel as opposed to only one switch, the
</p>
<p>probability that the circuit will operate correctly has been increased. What do you
</p>
<p>think would happen if we had used three switches in parallel? Or if we had used N
</p>
<p>switches? Could you ever be assured that the circuit would operate flawlessly? (See
</p>
<p>Problem 3.26.)
</p>
<p>Property 3.5 - Monotonicity of probability function
</p>
<p>Monotonicity asserts that the larger the set , the larger the probability of that set.
</p>
<p>Mathematically, this translates into the statement that if E c F , then P[E] ~ P[F].</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5. PROPERTIES OF THE PROBABILITY FUNCTION 51
</p>
<p>Proof: If E c F , then by definition F = E U (F - E) , where E and F - E are
mutually exclusive by definition. Hence,
</p>
<p>P[F] prE] + P [F - E]
&gt; p rE]
</p>
<p>(Axiom 3)
</p>
<p>(Axiom 1).
</p>
<p>o
Note that since EF c F and EF c E , we have that P[EF] ::; prE] and also that
P [EF ] ::; P[F]. The probability of an intersection is always less than or equal to
t he probability of t he set with the smallest probability.
</p>
<p>Example 3.7 - Switches in series
</p>
<p>A switching circuit shown in Figure 3.7 consist s of two potentially faulty switches in
</p>
<p>series. In order for the circuit to operate properly both switches must close. For the
</p>
<p>switch 2
&bull; ~----~
</p>
<p>switch 1
</p>
<p>Figure 3.7: Series switching circuit .
</p>
<p>same switches as described in Example 3.6 what is the probability that t he circuit
</p>
<p>will operate properly? Now we need to find P[E1E2]. This was given as 1/4 so that
</p>
<p>1 1
"4 = P [E1E2] ::; p rEll = 2"
</p>
<p>Could t he series circuit ever outperform t he parallel circuit? (See Problem 3.27.)
</p>
<p>o
One last property that is often useful is the probability of a union of more than
</p>
<p>two events. This extends P roperty 3.4. Cons ider first three events so that we wish
</p>
<p>to derive a formula for P[El U E2 U E3], which is equivalent to P [(El U E2) U E3] or
</p>
<p>P[El U (E2 U E3)] by the associat ive property. Writing this as P[El U (E2 U E3)],
we have
</p>
<p>P[El U (E2 U E3)]
</p>
<p>prEll + P[E2 U E3] - P[EdE2 U E3)]
prEll + (P [E2]+ P[E3] - P[E2E3])
</p>
<p>- P [E l (E2 U E3)]
</p>
<p>(Property 3.4)
</p>
<p>(Proper ty 3.4)
</p>
<p>(3.18)
</p>
<p>But E l (E2UE3) = E1E2UE1E3 by t he distributive property (draw a Venn diagram )
</p>
<p>so that
</p>
<p>P [El (E2 U E3)] = P [E1E2 U E1E3]
</p>
<p>P [E1E2] + P[E1E3] - P [E1E2E3] (Property 3.4). (3.19)</p>
<p/>
</div>
<div class="page"><p/>
<p>52
</p>
<p>Substituting (3.19) into (3.18) produces
</p>
<p>CHAPTER 3. BASIC PROBABILITY
</p>
<p>P[E1UE2UE3] = P[El]+P[E2]+P[E3]-P[E2E3]-P[ElE2]-P[ElE3]+P[ElE2E3]
(3.20)
</p>
<p>which is the desired result. It can further be shown that (see Problem 3.29)
</p>
<p>so that
</p>
<p>(3.21)
</p>
<p>which is known as Bool e's inequality or the union bound. Clearly, equality holds if
</p>
<p>and only if the Ei's are mutually exclusive. Both (3.20) and (3.21) can be extended
</p>
<p>to any finite number of unions [Ross 2002].
</p>
<p>3.6 Probabilities for Continuous Sample Spaces
</p>
<p>We have introduced the axiomatic approach to probability and illustrated the ap-
</p>
<p>proach with examples from a discrete sample space. The axiomatic approach is
</p>
<p>completely general and applies to continuous sample spaces as well. However, (3.10)
</p>
<p>cannot be used to determine probabilities of events. This is because the simple events
</p>
<p>of the continuous sample space are not countable. For example, suppose one throws
</p>
<p>a dart at a "linear" dartboard as shown in Figure 3.8 and measures the horizontal
</p>
<p>distance from the "bullseye" or center at x = O. We will then have a sample space
</p>
<p>- 1/2 o 1/ 2
</p>
<p>x
</p>
<p>Figure 3.8: "Linear" dartboard.
</p>
<p>s = {x : -1/2 :s; x :s; 1/2}, which is not countable. A possible approach is to assign
probabilities to intervals as opposed to sample points. If the dart is equally likely
</p>
<p>to land anywhere, then we could assign the interval [a, b] a probability equal to the
length of the interval or
</p>
<p>P[a :s; x :s; b] = b - a - 1/2 :s; a :s; b :s; 1/2. (3.22)
</p>
<p>Also, we will assume that the probability of disjoint intervals is the sum of the
</p>
<p>probabilities for each interval. This assignment is entirely consistent with our axioms</p>
<p/>
</div>
<div class="page"><p/>
<p>3.6. PROBABILITIES FOR CONTINUOUS SAMPLE SPACES
</p>
<p>since
</p>
<p>53
</p>
<p>= P [a ~ x ~ b] = b - a ~ O.p rE]
</p>
<p>P [S] =
</p>
<p>P[E UF]
</p>
<p>P [-1/2 ~ x ~ 1/2] = 1/2 - (- 1/ 2) = 1.
</p>
<p>P [a ~ x ~ b U e ~ x ~ d]
</p>
<p>(b -a)+(d -e)
</p>
<p>P[a ~ x ~ b] + P[e ~ x ~ d]
P[E]+P[F]
</p>
<p>(Axiom 1)
</p>
<p>(Axiom 2)
</p>
<p>(assumption)
</p>
<p>(Axiom 3)
</p>
<p>for a ~ b &lt; e ~ d so that E and F are mutually exclusive. Hence, an equally
likely type probability assignment for a continuous sample space is a valid one and
</p>
<p>produces a probability equal to the length of the interval. If the sample space does
</p>
<p>not have unity length, as for example, a dartboard with a length L, then we should
</p>
<p>use
p rE] = Length of int erval = Length of interval.
</p>
<p>Length of dartboard L
</p>
<p>Probability of a bullseye
</p>
<p>(3.23)
</p>
<p>It is an inescapable fact that the prob ability of the dar t landing at say x = 0 is
</p>
<p>zero since the length of this interval is zero. For that matter the prob ability of
</p>
<p>the dart landing at anyone particular point XQ is zero as follows from (3.22) with
</p>
<p>a = b = XQ . The first- time reader of probability will find this particularly disturbing
</p>
<p>and argue that "How can the probability of landing at every point be zero if indeed
</p>
<p>the dart had to land at some point?" From a pragmatic viewpoint we will seldom be
</p>
<p>interested in probabilities of points in a cont inuous sample space but only in those of
</p>
<p>intervals. How many dar ts are there whose tips have width zero and so can be said
</p>
<p>to land at a point? It is more realistic in practice then to ask for the probability that
</p>
<p>the dart lands in the bullseye, which is a small int erval with some nonz ero length.
</p>
<p>That probability is found by using (3.22). From a mathematical viewpoint it is not
</p>
<p>possible to "sum" up an infinite number of positive numbers of equal value and not
</p>
<p>obtain infinity, as opposed to one, as assumed in Axiom 2. The latter is true for
</p>
<p>cont inuous sample spaces , in which we have an uncountably infinite set, and also
</p>
<p>for discrete sample spaces, which is composed of a infinite but countable set. (Note
</p>
<p>that in Example 3.5 we had a countably infini te sample space but the probabilities
</p>
<p>were not equal.)
</p>
<p>Lfl
Since the probability of a point event occur ring is zero, the probability of any interval</p>
<p/>
</div>
<div class="page"><p/>
<p>54 CHAPTER 3. BASIC PROBABILITY
</p>
<p>is the same whether or not the endpoints are included . Thus, for our example
</p>
<p>P[a :S x :S b] = P[a &lt; x :S b] = P [a :S x &lt; b] = P[a &lt; x &lt; b].
</p>
<p>3.7 Probabilities for Finite Sample Spaces - Equally
</p>
<p>Likely Outcomes
</p>
<p>We now consider in more detail a discrete sample space with a finit e number of
</p>
<p>outcomes. Some examples that we are already familiar with are a coin toss , a die
</p>
<p>toss, or the students in a class. Furthermore, we assume that the simple events
</p>
<p>or outcomes are equally likely. Many problems have this structure and can be
</p>
<p>approached using counting m ethods or combinatorics. For example, if two dice are
</p>
<p>tossed , then the sample space is
</p>
<p>s = {( i , j) : i = 1, .. . , 6; j = 1, . .. , 6}
</p>
<p>which consists of 36 outcomes with each outcome or simple event denoted by an
</p>
<p>ordered pair of numbers. If we wish to assign probabilities to events, then we need
</p>
<p>only assign probabilities to the simple events and then use (3.10). But if all the
</p>
<p>simple event s, denoted by Si j , are equa lly likely, t hen
</p>
<p>where Ns is the number of outcomes in S. Now using (3.10) we have for any event
</p>
<p>that
</p>
<p>prE] = LL P[Si j]
{ (i ,j ): Sij EE}
</p>
<p>1
</p>
<p>.L L Ns
{(t,J) : Si jEE}
</p>
<p>N E
</p>
<p>Ns
</p>
<p>Number of outcomes in E
</p>
<p>Number of outcomes in S&middot;
</p>
<p>We will use combina torics to det ermine N E and Ns and hence p rE].
</p>
<p>Example 3.8 - Probability of equal values for two-dice toss
</p>
<p>Each outcome with equa l values is of the form (i , i ) so that
</p>
<p>p rE] = Number of outcomes with (i, i) .
Total number of outcomes
</p>
<p>(3.24)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.8. COMBINATORICS
</p>
<p>There are 6 outcomes with equal values or (i , i ) for i = 1,2, . .. , 6. Thus,
</p>
<p>P[E] = ~ = ~ .
36 6
</p>
<p>55
</p>
<p>E xample 3.9 - A m ore challenging problem - urns
</p>
<p>An urn contains 3 red balls and 2 black balls. Two balls are chosen in succession.
</p>
<p>The first ball is returned to the urn before the second ball is chosen. Each ball is
</p>
<p>chosen at random, which means that we are equally likely to choose any ball. What
</p>
<p>is the probability of choosing first a red ba ll and then a black ball? To solve this
</p>
<p>problem we first need to define the sample space. To do so we ass ign numbers to the
</p>
<p>balls as follows. T he red balls are numbered 1, 2, 3 and the black balls are numbered
</p>
<p>4,5. The sample space is then S = {(i , j) : i = 1,2,3,4,5; j = 1, 2, 3, 4, 5}. The
</p>
<p>event of interest is E = {(i , j) : i = 1, 2, 3; j = 4, 5}. We assume that all the simple
events are equally likely. An enumeration of the outcomes is shown in Table 3.2.
</p>
<p>The outcomes with the aste risks comprise E. Hence, the probability is P[E] = 6/25.
This problem could also have been solved using combinatorics as follows. Since there
</p>
<p>j = 1 j=2 j=3 j =4 j=5
</p>
<p>i = 1 (1,1) (1,2) (1,3) (1, 4)* (1,5)*
</p>
<p>i = 2 (2,1) (2,2) (2,3) (2, 4)* (2,5)*
</p>
<p>i = 3 (3,1) (3,2) (3,3) (3,4)* (3,5)*
</p>
<p>i =4 (4,1 ) (4,2) (4,3) (4,4) (4,5)
</p>
<p>i = 5 (5,1) (5,2) (5,3) (5,4) (5,5)
</p>
<p>Table 3.2: Enumeration of outcomes for urn problem of Example 3.9.
</p>
<p>are 5 poss ible choices for each ball, there are a total of 52 = 25 outcomes in the
</p>
<p>sample space. T here are 3 possible ways to choose a red ball on the first draw and 2
</p>
<p>possible ways to choose a black ball on the second draw, yielding a total of 3 &middot;2 = 6
</p>
<p>possible ways of choos ing a red ba ll followed by a black ball. We thus arrive at the
</p>
<p>same probability.
</p>
<p>3.8 Combinatorics
</p>
<p>Combinatorics is the study of counting. As illustrated in Example 3.9, we often
</p>
<p>have an outcome that can be represented as a 2-tuple or ( Zl , Z2 ) , where Zl can take
</p>
<p>on one of N 1 values and Z2 can take on one of N 2 values. For that example, the total
</p>
<p>number of 2-tuples in S is N 1N2 = 5&middot;5 = 25, while that in E is N 1N2 = 3&middot;2 = 6, as
</p>
<p>can be verified by referring to Table 3.2. It is important to note that order matters</p>
<p/>
</div>
<div class="page"><p/>
<p>56 CHAPTER 3. BASIC PROBABILIT Y
</p>
<p>P[E] =
</p>
<p>in the description of a 2-tuple. For example, the 2-t up le (1,2) is not the same as the
</p>
<p>2-tuple (2, 1) since each one describes a different outcome of the experiment. We will
</p>
<p>frequ ently be using 2-tuples and more generally r -tuples denoted by ( Zl , Z2, ' " ,zr )
</p>
<p>to describe the outcomes of urn experiments.
</p>
<p>In drawing balls from an urn there are two possible strategies. One met hod is to
</p>
<p>draw a ball, note which one it is, return it to the urn, and then draw a second ball.
</p>
<p>This is called sampling with replacement and was used in Example 3.9. However , it
</p>
<p>is also possible that the first ball is not returned to the urn before the second one is
</p>
<p>chosen. This method is called sampling without replacement. The contrast between
</p>
<p>the two strategies is illustrated next .
</p>
<p>E xample 3.10 - Computing probabilities of d rawing balls from urns -
</p>
<p>with and without replacement
</p>
<p>An urn has k red balls and N - k black balls. If two balls are chosen in succession
</p>
<p>and at random with replacem ent, what is the probability of a red ball f ollowed by a
</p>
<p>black ball? We solve this problem by first labeling the k red balls with 1, 2, . . . ,k
</p>
<p>and the black balls with k + 1, k + 2, . . . , N . In doing so the poss ible outcomes of
the experiment can be represented by a 2-tuple ( Zl ' Z2) , where Zl E {1, 2, .. . ,N }
and Z2 E {1, 2, ... , N }. A successful outcome is a red ball followed by a black one
</p>
<p>so that the successful event is E = {(Zl ' Z2 ) : Zl = 1, .. . , k ;Z2 = k + 1, .. . ,N}. The
total number of 2-tuples in the sample space is N s = N 2 , while the total number of
</p>
<p>2-tuples in E is NE = k(N - k) so that
</p>
<p>NE
</p>
<p>N s
k(N - k)
</p>
<p>N2
</p>
<p>= ~ (1- ~ ) .
Not e that if we let p = kiN be the proport ion of red balls, then P[E] = p( 1 - p) .
Next consider the case of sampling without replacement. Now since the same ball
</p>
<p>cannot be chosen twice in succession, and therefore, Zl i= Z2, we have one fewer
choice for the second ball. Therefore, N s = N (N - 1). As before, t he number of
successful 2-tuples is NE = k(N - k) , resu lting in
</p>
<p>P[E] = k(N-k) ~ N - k ~
N(N - 1) N N N - 1
</p>
<p>N
= p(1 - p) N - l '
</p>
<p>T he probability is seen to be higher. Can you explain this? (It may be helpful to
</p>
<p>think about the effect of a successful first draw on the probability of a success on
</p>
<p>the second draw.) Of course, for large N the probabilities for sampling with and
</p>
<p>without replacement are seen to be approximately the same, as expected.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.8. COMBINATORICS 57
</p>
<p>&lt;&gt;
If we now choose r ball s without replacement from an urn containing N balls, then
</p>
<p>all t he possible ou t comes are of t he form (Zl, Z2, ... , zr ), where t he Zi ' S must be
</p>
<p>different . On t he first draw we have N possible balls, on the second draw we have
</p>
<p>N - 1 possible balls, etc. Hence, t he total number of possible outcomes or number
</p>
<p>of r -tuples is N (N - 1) .. . (N - r + 1). We denote this by (N) r. If all the balls are
selected , forming an N-tuple, then the number of out comes is
</p>
<p>(N )N = N(N - 1)&middot;&middot;&middot;1
</p>
<p>whi ch is defined as N! and is termed N fact orial. As an example, if there are 3
</p>
<p>balls labeled A,B,C , t hen the number of 3-tuples is 3! = 3 . 2 . 1 = 6. To verify this
we have by enumerat ion that the possible 3-tuples are (A,B,C) , (A,C,B) , (B,A,C) ,
</p>
<p>(B,C,A), (C ,A,B) , (C ,B,A). Note that 3! is the number of ways that 3 objects can
</p>
<p>be arranged. These arrangements are termed the permutations of the letters A, B,
</p>
<p>and C. Note that with the definition of a factorial we have that (N)r = N!/(N -r)!.
</p>
<p>Another example follows.
</p>
<p>Example 3.11 - More urns - using permutations
</p>
<p>Five balls numbered 1,2,3, 4, 5 are dr awn from an urn without replacem ent. What
</p>
<p>is the probability t hat t hey will be drawn in t he same order as their number ? Each
</p>
<p>outcome is represent ed by t he 5-tuple (Zl' Z2 , Z3 , Z4, zs ). The only ou t come in E
is (1, 2, 3,4, 5) so t hat N E = 1. To find Ns we require t he number of ways that
</p>
<p>t he nu mb ers 1, 2,3, 4,5 can be arranged or the number of permutations. This is
</p>
<p>5! = 120. Hence, the desir ed probability is prE] = 1/120.
</p>
<p>&lt;&gt;
Before cont inuing, we give one more example to explain our fixation with drawing
</p>
<p>balls out of urns.
</p>
<p>Example 3.12 - The birthday problem
</p>
<p>A probability class has N students enrolled . What is the probability that at leas t
</p>
<p>two of the students will have the same birthday? We first ass ume that each st udent
</p>
<p>in the class is equally likely to be born on any day of the year. To solve this
</p>
<p>problem consider a "birt hday urn" that contains 365 balls. Each ball is labeled with
</p>
<p>a different day of the year. Now allow each student to select a ball at random, note
</p>
<p>it s date, and return it to the urn. The day of the year on the ball becomes his/her
</p>
<p>birthday. The probability desired is of the event that two or more students choose
</p>
<p>the same ball. It is more convenient to determine the probability of the complement
</p>
<p>event or that no two st udents have the same birthday. Then , using Prop erty 3.1
</p>
<p>P lat leas t 2 students have same birthday] = I -P[no st udents have same birthday].
</p>
<p>The sample space is composed of Ns = 365N N-tuples (sampling with replacement) .
</p>
<p>T he number of N-tuples for whi ch all the outcomes are different is N E = (365)N.
</p>
<p>This is because t he event t hat no two students have the same birthday occurs if</p>
<p/>
</div>
<div class="page"><p/>
<p>58 CHAPTER 3. BASIC PROBABILITY
</p>
<p>the first student chooses any of the 365 balls , the second student chooses any of the
</p>
<p>remaining 364 balls, etc ., which is the same as if sampling without replacement were
</p>
<p>used. The probability is then
</p>
<p>(365)N
Plat least 2 students have same birthday] = 1 - 365N
</p>
<p>This probability is shown in Figure 3.9 as a function of the number of students. It is
</p>
<p>seen that if the class has 23 or more students, there is a probability of 0.5 or greater
</p>
<p>that two students will have the same birthday.
</p>
<p>5010 20 30 40
Number of students, N
</p>
<p>0'--"""------'-----'-----'------'------'
o
</p>
<p>0.3 .
</p>
<p>0.1 .
</p>
<p>&gt;,0.7 .
.~
] 0.6 .
</p>
<p>ro
..g 0.5 .
....
</p>
<p>p.... 0.4 . .,
</p>
<p>0.8 .
</p>
<p>0.9 .
</p>
<p>Figure 3.9: Probability of at least two students having the same birthday.
</p>
<p>Why this doesn't appear to make sense.
</p>
<p>This result may seem counterintuitive at first, but this is only because the reader
</p>
<p>is misinterpreting the question. Most persons would say that you need about 180
</p>
<p>people for a 50% chance of two identical birthdays. In contrast, if the question was
</p>
<p>posed as to the probability that at least two persons were born on January 1, then
</p>
<p>the event would be at least two persons choose the ball labeled "January I" from the
</p>
<p>birthday urn. For 23 people this probability is considerably smaller (see Problem
</p>
<p>3.38). It is the possibility that the two identical birthdays can occur on any day
</p>
<p>of the year (365 possibilities) that leads to the unexpected large probability. To
</p>
<p>verify this result the MATLAB program given below can be used. When run, the
</p>
<p>estimated probability for 10,000 repeated experiments was 0.5072. The reader may
</p>
<p>wish to reread Section 2.4 at this point.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.8. COMBINATORICS 59
</p>
<p>%birthday.m
%
clear all
</p>
<p>rand ( 'state' ,0)
</p>
<p>BD= [0 :365] , ;
</p>
<p>event=zeros(10000,1); %initialize to no successful events
for ntrial=1:10000
</p>
<p>for i=1:23
</p>
<p>x(i,1)=ceil(365*rand(1,1)); %chooses birthdays at random
% (ceil rounds up to nearest integer)
</p>
<p>end
</p>
<p>y=sort(x); %arranges birthdays in ascending order
z=y(2:23)-y(1:22); %compares successive birthdays to each other
w=find(z==O); %flags same birthdays
if Lengt.h Cc) &gt;0
</p>
<p>event(ntrial)=l; %event occurs if one or more birthdays the same
end
</p>
<p>end
</p>
<p>prob=sum(event)/10000
</p>
<p>~
We summarize our counting formulas so far . Each outcome of an experiment
</p>
<p>produces an r-tuple, which can be written as (Zl, Z2,'" , zr ). If we are choos-
ing balls in succession from an urn containing N balls , then with replacement
</p>
<p>each Zi can take on one of N possible values. The number of possible r-tuples
</p>
<p>is then N", If we sample without replacement, then the number of r-tuples is only
</p>
<p>(N)r = N(N - 1) ... (N - r + 1). If we sample without replacement and r = N
or all the balls are chosen, then the number of r-tuples is NL In arriving at these
</p>
<p>formulas we have used the r-tuple representation in which the ordering is used in
</p>
<p>the counting. For example, the 3-tuple (A,B,C) is different than (C,A,B), which is
</p>
<p>different than (C,B,A), etc. In fact, there are 3! possible orderings or permutations
</p>
<p>of the letters A, B, and C. We are frequently not interested in the ordering but only
</p>
<p>in the number of distinct elements. An example might be to determine the number
</p>
<p>of possible sum-values that can be made from one penny (p), one nickel (n), and
</p>
<p>one dime (d) if two coins are chosen . To determine this we use a tree diagram as
</p>
<p>shown in Figure 3.10. Note that since this is essentially sampling without replace-
</p>
<p>ment, we cannot have the outcomes pp, nn, or dd (shown in Figure 3.10 as dashed).
</p>
<p>The number of possible outcomes are 3 for the first coin and 2 for the second so
</p>
<p>that as usual there are (3h = 3 . 2 = 6 outcomes. However, only 3 of these are
</p>
<p>distinct or produce different sum-values for the two coins. The outcome (p,n) is
</p>
<p>counted the same as (n,p) for example. Hence, the ordering of the outcome does
</p>
<p>not matter. Both orderings are treated as the same outcome. To remind us that</p>
<p/>
</div>
<div class="page"><p/>
<p>60
</p>
<p>choose first
</p>
<p>CHAPTER 3. BASIC PROBABILITY
</p>
<p>6 cents
</p>
<p>11 cents
</p>
<p>6 cents
</p>
<p>15 cents
</p>
<p>11 cents
</p>
<p>15 cents
</p>
<p>r " ' ~ , &middot;
choose second
</p>
<p>Figure 3.10: Tree diagram enumerating possible outcomes.
</p>
<p>ordering is immaterial we will replace the 2-tuple description by the set description
</p>
<p>(recall that the elements of a set may be arranged in any order to yield the same
</p>
<p>set). The outcomes of this experiment are therefore {p,n}, {p,d}, {n,d} . In effect,
</p>
<p>all permutations are considered as a single combination. Thus, to find the number
</p>
<p>of combinations:
</p>
<p>Number of combinations x Number of permutations
</p>
<p>or for this example,
</p>
<p>Number of combinations x 2! = (3h
</p>
<p>which yields
</p>
<p>Total number of
</p>
<p>r-tuple outcomes
</p>
<p>b f
.. (3h 3!
</p>
<p>Num er 0 combinations = -,- = -'-I = 3.
2. 1.2.
</p>
<p>The number of combinations is given by the symbol ( ~ ) and is said to be "3 things
</p>
<p>taken 2 at a time" . Also, (~) is termed the binomial coefficient due to its appearance
</p>
<p>in the binomial expansion (see Problem 3.43) . In general the number of combinations
</p>
<p>of N things taken k at a time, i.e., order does not matter, is
</p>
<p>(
N ) (N)k N!
k = ~ = (N -k)!k!'
</p>
<p>Example 3.13 - Correct change
</p>
<p>If a person has a penny, nickel, and dime in his pocket and selects two coins at
</p>
<p>random, what is the probability that the sum-value will be 6 cents? The sample</p>
<p/>
</div>
<div class="page"><p/>
<p>3.8. COMBINATORICS
</p>
<p>space is now S = {{p, n}, [p,d} , {n , d}} and E = {{p, n}}. Thus,
</p>
<p>61
</p>
<p>P[6 cents] =
N E
</p>
<p>P [{p ,n}] = Ns
</p>
<p>1
</p>
<p>3&middot;
</p>
<p>Note that each simple event is of the form {.,.} . Also, Ns can be found from the
original problem statement as ( ~) = 3.
</p>
<p>Example 3.14 - How probable is a royal flush?
</p>
<p>A person draws 5 cards from a deck of 52 freshly shuffled cards. What is the
</p>
<p>probability that he obtains a royal flush? To obtain a royal flush he must draw an
</p>
<p>ace, king, queen, jack, and ten of the same suit in any order. There are 4 possible
</p>
<p>sui ts that will be produce the flush. The total number of combinations of cards
</p>
<p>or "hands" that can be dr awn is (5;) and a royal flush will result from 4 of these
combinations. Hence,
</p>
<p>4
P[royal flush] = en ~ 0.00000154.
</p>
<p>&amp; Ordered vs. unordered
It is somet imes confusing that (5n is used for Ns - It might be argued that the
</p>
<p>first card can be chosen in 52 ways, the second card in 51 ways, et c. for a total of
</p>
<p>(52)5 possible outcomes. Likewise, for a royal flush in hearts we can choose any of
5 cards, followed by any of 4 cards, et c. for a total of 5! possible outcomes. Hence,
</p>
<p>the probability of a royal flush in hearts should be
</p>
<p>P[royal flush in hearts] = _(5!) .
52 5
</p>
<p>But this is just the same as 1/ (5n which is the same as obtained by counting
combinat ions. In essence , we have redu ced the sample space by a factor of 5! but
</p>
<p>addit ionally each event is commensura tely reduced by 5!, yielding the same proba-
</p>
<p>bility. Equivalently, we have grouped toget her each set of 5! permutations to yield
</p>
<p>a single combination.</p>
<p/>
</div>
<div class="page"><p/>
<p>62 CHAPTER 3. BASIC PROBABILITY
</p>
<p>3.9 Binomial Probability Law
</p>
<p>In Chapter 1 we cited the binomial probability law for the number of heads obtained
</p>
<p>for N tosses of a coin. The same law also applies to the problem of drawing balls from
</p>
<p>an urn. First, however, we look at a related problem that is of considerable practical
</p>
<p>interest. Specifically, consider an urn consisting of a proportion p of red balls and the
</p>
<p>remaining proportion 1 - p of black balls. What is the probability of drawing k red
</p>
<p>balls in M drawings without replacement? Note that we can associate the drawing
</p>
<p>of a red ball as a "success" and the drawing of a black ball as a "failure" . Hence ,
</p>
<p>we are equivalently asking for the probability of k successes out of a maximum of
</p>
<p>M successes. To determine this probability we first assume that the urn contains
</p>
<p>N balls, of which N R are red and N B are black. We sample the urn by drawing M
</p>
<p>balls without replacement. To make the balls distinguishable we label the red balls
</p>
<p>as 1,2, . .. ,NR and the black ones as NR + 1,NR + 2, . . . ,N. The sample space is
</p>
<p>s = {(Zl , Z2, &bull; &bull; &bull; , ZM) : Zi = 1, ... ,N and no two Zi'S are the same}.
</p>
<p>We assume that the balls are selected at random so that the outcomes are equally
</p>
<p>likely. The total number of outcomes is Ns = (N) M. Hence, the probability of
obtaining k red balls is
</p>
<p>NE
P[k] = (N)M' (3.25)
</p>
<p>NE is the number of M -tuples that contain k distinct integers in the range from
</p>
<p>1 to N Rand M - k distinct integers in the range N R + 1 to N. For example, if
NR = 3, NB = 4 (and hence N = 7), M = 4, and k = 2, the red balls are contained
in {I, 2, 3}, the black balls are contained in {4, 5, 6, 7} and we choose 4 balls without
</p>
<p>replacement. A successful outcome has two red balls and two black balls. Some
</p>
<p>successful outcomes are (1,4,2,5) , (1,4,5,2) , (1,2,4,5), etc. or (2,3,4,6) , (2,4,3,6),
</p>
<p>(2,6,3,4) , etc. Hence, NE is the total number of outcomes for which two of the Zi 'S
</p>
<p>are elements of {I , 2, 3} and two of the Zi'S are elements of {4, 5, 6, 7}. To determine
</p>
<p>this number of successful M-tuples we
</p>
<p>1. Choose the k positions of the M-tuple to place the red balls. (The remaining
</p>
<p>positions will be occupied by the black balls.)
</p>
<p>2. Place the N R red balls in the k positions obtained from step 1.
</p>
<p>3. Place the N B black balls in the remaining M - k positions.
</p>
<p>Step 1 is accomplished in (!If) ways since any permutation of the chosen positions
produces the same set of positions. Step 2 is accomplished in (NR)k ways and step</p>
<p/>
</div>
<div class="page"><p/>
<p>3.9. BINOMIAL PROBABILITY LAW
</p>
<p>3 is accomplished in (NB)M-k ways. Thus, we have that
</p>
<p>NE = (~) (NR)k(NB)M-k
M'
</p>
<p>(M - ~)!k! (NRh(NB)M-k
</p>
<p>= M! (~R) (:~k)
so that finally we have from (3.25)
</p>
<p>63
</p>
<p>(3.26)
</p>
<p>P[k]
M! (~R) (:!!k)
</p>
<p>(N)M
</p>
<p>( ~ R ) (:!!k)
(t;)
</p>
<p>(3.27)
</p>
<p>This law is called the hypergeometric law and describes the probability of k successes
</p>
<p>when sampling without replacement is used. If sampling with replacement is used,
</p>
<p>then the binomial law results. However, instead of repeating the entire derivation
</p>
<p>for sampling with replacement, we need only assume that N is large. Then, whether
</p>
<p>the balls are replaced or not will not affect the probability. To show that this is
</p>
<p>indeed the case , we start with the expression given by (3.26) and note that for N
</p>
<p>large and M &laquo; N , then (N)M ;::j N M. Similarly, we assume that M &laquo; NR and
M &laquo; N B and make similar approximations. As a result we have from (3.25) and
(3.26)
</p>
<p>P[k] ;::j
</p>
<p>Letting NR/N = p and NB/N = (N - NR)/N = 1- p, we have at last the binomial
</p>
<p>law
</p>
<p>(3.28)
</p>
<p>To summarize, the binomial law not only applies to the drawing of balls from urns
</p>
<p>with replacement but also applies to the drawing of balls without replacement if the
</p>
<p>number of balls in the urn is large. We next use our results in a quality control
</p>
<p>application.</p>
<p/>
</div>
<div class="page"><p/>
<p>64 CHAPTER 3. BASIC PROBABILITY
</p>
<p>3.10 Real-World Example - Quality Control
</p>
<p>A manufacturer of electronic memory chips produces batches of 1000 chips for ship-
</p>
<p>ment to computer companies. To determine if the chips meet specifications the
</p>
<p>manufacturer initially tests all 1000 chips in each batch. As demand for the chips
</p>
<p>grows, however, he realizes that it is impossible to test all the chips and so proposes
</p>
<p>that only a subset or sample of the batch be tested. The criterion for acceptance
</p>
<p>of the batch is that at least 95% of the sample chips tested meet specifications. If
</p>
<p>the criterion is met, then the batch is accepted and shipped. This criterion is based
</p>
<p>on past experience of what the computer companies will find acceptable, i.e., if the
</p>
<p>batch "yield" is less than 95% the computer companies will not be happy. The
</p>
<p>production manager proposes that a sample of 100 chips from the batch be tested
</p>
<p>and if 95 or more are deemed to meet specifications, then the batch is judged to
</p>
<p>be acceptable. However, a quality control supervisor argues that even if only 5 of
</p>
<p>the sample chips are defective, then it is still quite probable that the batch will not
</p>
<p>have a 95% yield and thus be defective.
</p>
<p>The quality control supervisor wishes to convince the production manager that
</p>
<p>a defective batch can frequently produce 5 or fewer defective chips in a chip sample
</p>
<p>of size 100. He does so by determining the probability that a defective batch will
</p>
<p>have a chip sample with 5 or fewer defective chips as follows. He first needs to
</p>
<p>assume the proportion of chips in the defective batch that will be good. Since
</p>
<p>a good batch has a proportion of good chips of 95%, a defective batch will have
</p>
<p>a proportion of good chips of less than 95%. Since he is quite conservative, he
</p>
<p>chooses this proportion as exactly p = 0.94, although it may actually be less. Then,
according to the production manager a batch is judged to be acceptable if the sample
</p>
<p>produces 95,96,97,98,99, or 100 good chips. The quality control supervisor likens
</p>
<p>this problem to the drawing of 100 balls from an "chip urn" containing 1000 balls.
</p>
<p>In the urn there are 1000p good balls and 1000(1 - p) bad ones. The probability of
</p>
<p>drawing 95 or more good balls from the urn is given approximately by the binomial
</p>
<p>probability law. We have assumed that the true law, which is hypergeometric due
</p>
<p>to the use of sampling without replacement, can be approximated by the binomial
</p>
<p>law, which assumes sampling with replacement. See Problem 3.48 for the accuracy
</p>
<p>of this approximation.
</p>
<p>Now the defective batch will be judged as acceptable if there are 95 or more
</p>
<p>successes out of a possible 100 draws. The probability of this occurring is
</p>
<p>where p = 0.94. The probability P[k 2: 95] versus p is plotted in Figure 3.11.
For p = 0.94 we see that the defective batch will be accepted with a probability
</p>
<p>of about 0.45 or almost half of the defective batches will be shipped. The quality
</p>
<p>control supervisor is indeed correct. The production manager does not believe the</p>
<p/>
</div>
<div class="page"><p/>
<p>3.10. REAL-WORLD EXAMPLE - QUALITY CONTROL 65
</p>
<p>0.2 &middot; .
</p>
<p>0.1 ." :
</p>
<p>0.3 .
</p>
<p>0.9 .
</p>
<p>0.8 .
</p>
<p>'i:Qo7 . .
~.
</p>
<p>1\10.6 .
..\C
c... 0.5 . . . .
</p>
<p>0.4 .
</p>
<p>0.950.940.930.920.91
OL------'-----'-------'-----'----'
0.9
</p>
<p>p
</p>
<p>Figure 3.11: Probability of accepting a defective batch versus proportion of good
</p>
<p>chips in the defective batch - accept if 5 or fewer bad chips in a sample of 100.
</p>
<p>result since it appears to be too high. Using sampling with replacement, which
</p>
<p>will produce results in accordance with the binomial law, he performs a computer
</p>
<p>simulation (see Problem 3.49). Based on the simulated results he reluctantly accepts
</p>
<p>the supervisor's conclusions. In order to reduce this probability the quality control
</p>
<p>supervisor suggests changing the acceptance strategy to one in which the batch
</p>
<p>is accepted only if 98 or more of the samples meet the specifications. Now the
</p>
<p>probability that the defective batch will be judged as acceptable is
</p>
<p>where p = 0.94, the assumed proportion of good chips in the defective batch. This
</p>
<p>produces the results shown in Figure 3.12. The acceptance probability for a defective
</p>
<p>batch is now reduced to only about 0.05.
</p>
<p>There is a price to be paid, however, for only accepting a batch if 98 or more of
</p>
<p>the samples are good. Many more good batches will be rejected than if the previous
</p>
<p>strategy were used (see Problem 3.50). This is deemed to be a reasonable tradeoff.
</p>
<p>Note that the supervisor may well be advised to examine his initial assumption
</p>
<p>about p for the defective batch. If, for instance, he assumed that a defective batch
</p>
<p>could be characterized by p = 0.9, then according to Figure 3.11, the production
</p>
<p>manager's original strategy would produce a probability of less than 0.1 of accepting
</p>
<p>a defective batch.</p>
<p/>
</div>
<div class="page"><p/>
<p>~0.7
</p>
<p>1\10.6 .
....\Cc.. 0.5 .
</p>
<p>66 CHAPTER 3. BASIC PROBABILITY
</p>
<p>0.9 : : .
. .
</p>
<p>0.8 .
</p>
<p>-_ .
</p>
<p>0.4 .
</p>
<p>0.3 .
</p>
<p>0.2 .
</p>
<p>0.1 . .
.~
</p>
<p>0.91 0.92
</p>
<p>p
0.93 0.94 0.95
</p>
<p>Figure 3.12: Probability of accepting a defective batch versus proportion of good
</p>
<p>chips in the defective batch - accept if 2 or fewer bad chips in a sample of 100.
</p>
<p>References
</p>
<p>Billingsley, P. , Probability and Measure, John Wiley &amp; Sons, New York, 1986.
</p>
<p>Ross , S., A First Cours e in Probability, Prentice-Hall, Upper Saddle River, NJ ,
</p>
<p>2002.
</p>
<p>Problems
</p>
<p>3.1 C:.:... ) (w) The universal set is given by S = {x : -00 &lt; x &lt; oo} (the real line).
If A = {x : x &gt; I} and B = {x : x ~ 2}, find the following:
</p>
<p>b. AUB and AnB
</p>
<p>c. A - Band B - A
</p>
<p>3.2 (w) Repeat Problem 3.1 if S = {x : x 2: O}.
</p>
<p>3.3 (w) A group of voters go to the polling place. Their names and ages are Lisa ,
</p>
<p>21, John, 42, Ashley, 18, Susan , 64, Phillip, 58, Fred , 48, and Brad, 26. Find
</p>
<p>the following sets:</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS
</p>
<p>a. Voters older than 30
</p>
<p>b. Vot ers younger than 30
</p>
<p>c. Male vote rs older than 30
</p>
<p>d. Female voters younger than 30
</p>
<p>e. Vote rs that are male or younger than 30
</p>
<p>f. Voters that are female and older than 30
</p>
<p>67
</p>
<p>Next find any two sets that partition the uni verse.
</p>
<p>3.4 (w) Given the sets Ai = {x : 0 ~ x ~ i} for i = 1,2, ... , N , find U ~IA and
N A A h A ' di . . ?n i= l i . re t e i s isjoint:
</p>
<p>3.5 (w) P rove that the sets A = {x : x ;::: -I} and B = {x : 2x + 2;::: O} are equal.
</p>
<p>3.6 (t) Prove that if x E A n B e, then x E A-B.
</p>
<p>3.7 C:...:.J (w) If S = {I , 2, 3, 4, 5, 6}, find sets A and B that are disjoint. Next find
sets C and D that partition the universe.
</p>
<p>3.8 (w) If S = {(x ,y) : 0 ~ x ~ 1 and 0 ~ y ~ I} , find sets A and B that are
disjoint. Next find sets C and D that par ti tion the universe.
</p>
<p>3.9 (t) In this problem we see how to construct disjoint sets from ones that are not
</p>
<p>disjoint so that their unions will be the same. We consider only three sets and
</p>
<p>ask the reader to generalize the result. Calling the nondisjoint sets A , B , C
</p>
<p>and the union D = AU B U C , we wish to find three disjoint sets E I , E2 , and
E 3 so that D = E I U E 2 U E3 . To do so let
</p>
<p>E I A
</p>
<p>E2 B - E I
</p>
<p>E3 C - (E I U E2 ) .
</p>
<p>Using a Venn diagram explain this procedure. Ifwe now have set s A I , A2 , . &bull;. , AN,
</p>
<p>explain how to construct N disjoint sets with the same union.
</p>
<p>3.10 c.:..:.,) (f) Replace the set expression AUBU C with one using intersections and
complements . Replace the set expression A n B nC with one using uni ons and
complements .
</p>
<p>3.11 (w) The sets A , B , C are subsets of S = {(x ,y) : 0 ~ x ~ 1 and 0 ~ y ~ I}.
They are defined as
</p>
<p>A { ( x , y) : x~I/2 ,0~ y~l }
</p>
<p>B { ( x , y ): x;:::I/2 , 0~ y~l}
</p>
<p>C {(x ,y) : 0 ~ x ~ l ,y ~ 1/2}.</p>
<p/>
</div>
<div class="page"><p/>
<p>68 CHAPTER 3. BASIC PROBABILITY
</p>
<p>Explicitly determine the set AU (B nC)Cby drawing a picture of it as well as
pictures of all the individual sets. For simplicity you can ignore the edges of
</p>
<p>the sets in dr awing any diagrams. Can you represent the resultant set using
</p>
<p>only unions and complements?
</p>
<p>3.12 L..:J (w) Give the size of each set and also whether it is discrete or cont inuous.
If the set is infinite, determine if it is countably infinite or not.
</p>
<p>a. A = {seven-digit numbers}
</p>
<p>b. B = {x : 2x = I}
</p>
<p>c. C = {x : 0 ::; x ::; 1 and 1/2 ::; x ::; 2}
</p>
<p>d. D = {(x ,y): x2 +y2 = I}
</p>
<p>e. E = {x : x2 + 3x + 2 = O}
f. F = {positive even integers}
</p>
<p>3.13 (w) Two dice are tossed and the number of dots on each side that come up
</p>
<p>are ad ded together. Det ermine the sample space, outcomes, impossible event ,
</p>
<p>three different event s including a simple event, and two mutually exclusive
</p>
<p>events . Use appropriate set notation.
</p>
<p>3.14 t.:..:..-) (w) The temperature in Rhode Island on a given day in August is found
to always be in the range from 30&deg; F to 100&deg; F . Determine the sample space,
</p>
<p>outco mes, impossible event, three different events including a simple event,
</p>
<p>and two mutually exclusive events. Use appropriate set notation.
</p>
<p>3 .15 (t) Prove that if the sample space has size N , then the to tal number of events
(including the imp ossible event and the certain event) is 2N . Hint: There are
</p>
<p>( ~ ) ways to choose an event with k outcomes from a total of N outcomes.
Also, use the binomial formul a
</p>
<p>which was proven in Problem 1.11.
</p>
<p>3.16 (w) An urn contains 2 red balls and 3 black balls. The red balls are lab eled
</p>
<p>with the numbers 1 and 2 and the black balls are lab eled as 3, 4, and 5. Three
</p>
<p>balls are drawn without replacement. Consider the events that
</p>
<p>A {a majority of the balls drawn are black}
</p>
<p>B = {the sum of the numbers of the balls drawn ~ 1O}.
</p>
<p>Are these events mutually exclusive? Explain your answer.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 69
</p>
<p>3.17 (t) Prove Axiom 3' by using mathematical induction (see Appendix B) and
Axiom 3.
</p>
<p>3.18 C:..:...) (w) A roulette wheel has numbers 1 to 36 equally spaced around its
perimeter. The odd numbers are colored red while the even numbers are
</p>
<p>colored black. If a spun ball is equally likely to yield any of the 36 numbers,
</p>
<p>what is the probability of a black number, of a red number? What is the
</p>
<p>probability of a black number that is greater than 24? What is the probability
</p>
<p>of a black number or a number greater than 24?
</p>
<p>3.19 C:..:...) (c) Use a computer simulation to simulate the tossing of a fair die. Based
on the simulation what is the probability of obtaining an even number? Does
</p>
<p>it agree with the theoretical result? Hint: See Section 2.4.
</p>
<p>3.20 (w) A fair die is tossed. What is the probability of obtaining an even number,
an odd number, a number that is even or odd, a number that is even and odd?
</p>
<p>3.21 C.:....) (w) A die is tossed that yields an even number with twice the probability
of yielding an odd number. What is the probability of obtaining an even
</p>
<p>number, an odd number, a number that is even or odd, a number that is even
</p>
<p>and odd?
</p>
<p>3.22 (w) If a single letter is selected at random from {A , B ,e} , find the probability
of all events. Recall that the total number of events is 2N , where N is the
</p>
<p>number of simple events. Do these probabilities sum to one? If not, why not?
</p>
<p>Hint: See Problem 3.15.
</p>
<p>3.23 C ~ ) (w) A number is chosen from {I , 2, 3, ... } with probability
</p>
<p>Find P[i ~ 4J.
</p>
<p>P[iJ = { :
7 .
</p>
<p>(kr-2
</p>
<p>i = 1
</p>
<p>i=2
</p>
<p>i ~ 3
</p>
<p>3.24 (f) For a sample space S = {a, 1,2, ... } the probability assignment
</p>
<p>2i
P[iJ = exp( -2)1"
</p>
<p>2.
</p>
<p>is proposed. Is this a valid assignment?
</p>
<p>3.25 C:.:....) (w) Two fair dice are tossed. Find the probability that only one die
comes up a 6.</p>
<p/>
</div>
<div class="page"><p/>
<p>70 CHAPTER 3. BASIC PROBABILITY
</p>
<p>3.26 (w) A circuit consists of N switches in parallel (see Example 3.6 for N = 2).
</p>
<p>The sample space can be summarized as S = {(Zl, Z2, ... , ZN) : Zi = S or f},
where s indicates a success or the switch closes and f indicates a failure or
</p>
<p>the switch fails to close. Assuming that all the simple events are equally
</p>
<p>likely, what is the probability that a circuit is closed when all the switches are
</p>
<p>activated to close? Hint: Consider the complement event.
</p>
<p>3.27 c.:..:.-) (w) Can the series circuit of Figure 3.7 ever outperform the parallel cir-
cuit of Figure 3.6 in terms of having a higher probability of closing when both
</p>
<p>switches are activated to close? Assume that switch 1 closes with probability
</p>
<p>p, switch 2 closes with probability p , and both switches close with probability
p2.
</p>
<p>3.28 (w) Verify the formula (3.20) for P[El UE2 UE 3J if e; E2, E 3 are events that
are not necessarily mutually exclusive. To do so use a Venn diagram.
</p>
<p>3.29 (t) Prove that
</p>
<p>3.30 (w) A person always arrives at his job between 8:00 AM and 8:20 AM. He is
equally likely to arrive anytime within that period. What is the probability
</p>
<p>that he will arrive at 8:10 AM? What is the probability that he will arrive
</p>
<p>between 8:05 and 8:10 AM?
</p>
<p>3.31 (w) A random number generator produces a number that is equally likely to
</p>
<p>be anywhere in the interval (0,1). What are the simple events? Can you use
</p>
<p>(3.10) to find the probability that a generated number will be less than 1/2?
</p>
<p>Explain.
</p>
<p>3.32 (w) If two fair dice are tossed, find the probability that the same number will
</p>
<p>be observed on each one. Next , find the probability that different numbers
</p>
<p>will be observed.
</p>
<p>3.33 ( ~ ) (w) Three fair dice are tossed. Find the probability that 2 of the numbers
</p>
<p>will be the same and the third will be different.
</p>
<p>3.34 (w,c) An urn contains 4 red balls and 2 black balls. Two balls are chosen at
</p>
<p>random and without replacement. What is the probability of obtaining one
</p>
<p>red ball and one black ball in any order? Verify your results by enumerating
</p>
<p>all possibilities using a computer evaluation.
</p>
<p>3.35 C:...:.-) (f) Rhode Island license plate numbers are of the form GR315 (2 letters
followed by 3 digits). How many different license plates can be issued?</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 71
</p>
<p>3.36 (f) A baby is to be named using four letters of the alphabet. The letters can
</p>
<p>be used as often as desired. How many different names are there? (Of course,
</p>
<p>some of the names may not be pronounceable).
</p>
<p>3.37 (c) It is difficult to compute N! when N is large . As an approximation, we
</p>
<p>can use Stirling's formula, which says that for large N
</p>
<p>Compare Stirling's approximation to the true value of N! for N = 1,2, ... , 100
</p>
<p>using a digital computer. Next try calculating the exact value of N! for N =
</p>
<p>200 using a computer. Hint: Try printing out the logarithm of N! and compare
</p>
<p>it to the logarithm of its approximation.
</p>
<p>3.38 c.:..:...) (t) Determine the probability that in a class of 23 students two or more
students have birthdays on January 1.
</p>
<p>3.39 (c) Use a computer simulation to verify your result in Problem 3.38.
</p>
<p>3.40 c.:..:...) (w) A pizza can be ordered with up to four different toppings. Find the
total number of different pizzas (including no toppings) that can be ordered.
</p>
<p>Next, if a person wishes to pay for only two toppings, how many two-topping
</p>
<p>pizzas can he order?
</p>
<p>3.41 (f) How many subsets of size three can be made from {A , B , C, D , E}?
</p>
<p>3.42 (w) List all the combinations of two coins that can be chosen from the follow-
ing coins: one penny (p) , one nickel (n) , one dime (d), one quarter (q) . What
</p>
<p>are the possible sum-values?
</p>
<p>3.43 (f) The binomial theorem states that
</p>
<p>Expand (a + b)3 and (a + b)4 into powers of a and b and compare your results
to the formula.
</p>
<p>3.44 C:..:,,) (w) A deck of poker cards contains an ace, king, queen, jack, 10, 9, 8,
7, 6, 5, 4, 3, 2 in each of the four suits, hearts (h) , clubs (c) , diamonds (d),
</p>
<p>and spades (s), for a total of 52 cards. If 5 cards are chosen at random from
</p>
<p>a deck, find the probability of obtaining 4 of a kind, as for example, 8-h, 8-c,
</p>
<p>8-d , 8-s, 9-c. Next find the probability of a flush, which occurs when all five
</p>
<p>cards have the same suit , as for example, 8-s, queen-s, 2-s, ace-s , 5-s.</p>
<p/>
</div>
<div class="page"><p/>
<p>72 CHAPTER 3. BASIC PROBABILITY
</p>
<p>3.45 (w) A class consists of 30 students, of which 20 are freshmen and 10 are
</p>
<p>sophomores. If 5 students are selected at random, what is the probability that
</p>
<p>they will all be sophomores?
</p>
<p>3.46 (w) An urn containing an infinite number of balls has a proportion p of red
</p>
<p>balls, and the remaining portion 1 - p of black balls. Two balls are chosen at
</p>
<p>random. What value of p will yield the highest probability of obtaining one
</p>
<p>red ball and one black ball in any order?
</p>
<p>3.47 (w) An urn contains an infinite number of coins that are either two-headed or
two-tailed. The proportion of each kind is the same. If we choose M coins at
</p>
<p>random, explain why the probability of obtaining k heads is given by (3.28)
</p>
<p>with p = 1/2. Also, how does this experiment compare to the tossing of a fair
</p>
<p>coin M times?
</p>
<p>3.48 (c) Compare the hypergeometric law to the binomial law if N = 1000, M =
</p>
<p>100, p = 0.94 by calculating the probability P[k] for k = 95,96, ... ,100.
</p>
<p>Hint: To avoid computational difficulties of calculating N! for large N, use
</p>
<p>the following strategy to find x = 1000!/900! as an example.
</p>
<p>1000 900
</p>
<p>Y = In(x) = In(1000!) -In(900!) = L In(i) - L In(i)
i= l i = l
</p>
<p>and then x = exp(y). Alternatively, for this example you can cancel out the
common factors in the quotient of x and write it as x = (1000hoo , which is
easier to compute. But in general, this may be more difficult to set up and
</p>
<p>program.
</p>
<p>3.49 C:.:.,) (c) A defective batch of 1000 chips contains 940 good chips and 60 bad
chips. If we choose a sample of 100 chips , find the probability that there will
</p>
<p>be 95 or more good chips by using a computer simulation. To simpify the
</p>
<p>problem assume sampling with replacement for the computer simulation and
</p>
<p>the theoretical probability. Compare your result to the theoretical prediction
</p>
<p>in Section 3.10.
</p>
<p>3.50 (c) For the real-world problem discussed in Section 3.10 use a computer simu-
</p>
<p>lation to determine the probability of rejecting a good batch. To simpify your
</p>
<p>code assume sampling with replacement. A good batch is defined as one with
</p>
<p>a probability of obtaining a good chip of p = 0.95. The two strategies are to
</p>
<p>accept the batch if 95 or more of the 100 samples are good and if 98 or more
</p>
<p>of the 100 samples are good. Explain your results. Can you use Figures 3.11
</p>
<p>and 3.12 to determine the theoretical probabilities?</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 4
</p>
<p>Conditional Probability
</p>
<p>4.1 Introduction
</p>
<p>In the previous chapter we determined the probabilities for some simple experiments.
</p>
<p>An example was the die toss that produced a number from 1 to 6 "at random".
</p>
<p>Hence , a probability of 1/6 was assigned to each possible outcome. In many real-
</p>
<p>world "experiments", the outcomes are not completely random since we have some
</p>
<p>prior knowledge. For instance, knowing that it has rained the previous 2 days might
</p>
<p>influence our assignment of the probability of sunshine for the following day. Another
</p>
<p>example is to determine the probability that an individual chosen from some general
</p>
<p>population weighs more than 200 lbs ., knowing that his height exceeds 6 ft. This
</p>
<p>motivates our interest in how to determine the probability of an event, given that we
</p>
<p>have some prior knowledge. For the die tossing experiment we might inquire as to the
</p>
<p>probability of obtaining a 4, if it is known that the outcome is an even number. The
</p>
<p>additional knowledge should undoubtedly change our probability assignments. For
</p>
<p>example, if it is known that the outcome is an even number, then the probability
</p>
<p>of any odd-numbered outcome must be zero. It is this interaction between the
</p>
<p>original probabilities and the probabilities in light of prior knowledge that we wish
</p>
<p>to describe and quantify, leading to the concept of a conditional probability.
</p>
<p>4.2 Summary
</p>
<p>Section 4.3 motivates and then defines the conditional probability as (4.1). In do-
</p>
<p>ing so the concept of a joint event and its probability are introduced as well as
</p>
<p>the marginal probability of (4.3). Conditional probabilities can be greater than,
</p>
<p>less than, or equal to the ordinary probability as illustrated in Figure 4.2. Also,
</p>
<p>conditional probabilities are true probabilities in that they satisfy the basic axioms
</p>
<p>and so can be manipulated in the usual ways. Using the law of total probability
</p>
<p>(4.4) , the probabilities for compound experiments are easily determined. When the
</p>
<p>conditional probability is equal to the ordinary probability, the events are said to</p>
<p/>
</div>
<div class="page"><p/>
<p>74 CHAPTER 4. CONDITIONAL PROBABILITY
</p>
<p>be statist ically independent. Then, knowledge of the occurrence of one event does
</p>
<p>not change the probability of the other event. The condit ion for two events to
</p>
<p>be independent is given by (4.5). Three events are statistically independent if the
</p>
<p>condit ions (4.6)-(4.9) hold. Bayes' theorem is defined by either (4.13) or (4.14).
</p>
<p>Embodied in the theorem are the concepts of a prior probability (before the experi-
</p>
<p>ment is conducted) and a posterior probability (aft er the exp eriment is conducted).
</p>
<p>Conclusions may be dr awn based on the outcome of an exp eriment as to whether
</p>
<p>certain hypotheses are true. When an experiment is repeated multiple times and
</p>
<p>the experiments are independent , the probability of a joint event is easily found
</p>
<p>via (4.15). Some probability laws that result from the independent multiple experi-
</p>
<p>ment assumption are the binomial (4.16) , the geometric (4.17) , and the multinomial
</p>
<p>(4.19). For dependent multiple experiments (4.20) must be used to determine prob-
</p>
<p>abilities of joint events. If, however, the experimental outcomes probabilities only
</p>
<p>depend on the previous experimental outcome, then the Markov condition is satis-
</p>
<p>fied. This results in the simpler formula for determining joint probabilities given by
</p>
<p>(4.21). Also , this assumption leads to the concept of a Markov chain, an example of
</p>
<p>which is shown in Figure 4.8. Finally, in Section 4.7 an example of the use of Bayes '
</p>
<p>theorem to detect the presence of a cluster is investigated.
</p>
<p>4.3 Joint Events and the Conditional Probability
</p>
<p>In formulating a useful theory of conditional probability we are led to consider
</p>
<p>two events. Event A is our event of interest while event B represents the event
</p>
<p>that embodies our prior knowledge. For the fair die toss example described in the
</p>
<p>introduction, the event of interest is A = {4} and the event describing our prior
knowledge is an even outcome or B = {2, 4, 6}. Note that when we say that the
</p>
<p>outcome must be even, we do not elaborate on why this is the case. It may be
</p>
<p>because someone has observed the outcome of the experiment and conveyed this
</p>
<p>partial information to us. Alternatively, it may be that the experimenter loathes
</p>
<p>odd outcomes, and therefore keeps tossing the die until an even outcome is obtained.
</p>
<p>Conditional probability does not address the reasons for the prior information, only
</p>
<p>how to accommodate it into a probabilistic framework . Continuing with the fair
</p>
<p>die example, a typical sequence of outcomes for a repeated experiment is shown in
</p>
<p>Figure 4.1. The odd outcomes are shown as dashed lines and are to be ignored.
</p>
<p>From the figure we see that the probability of a 4 is about 9/25 = 0.36, or about
1/3, using a relative frequency interpretation of probability. This has been found
</p>
<p>by taking the total number of 4's and dividing by the total number of 2's , 4's , and
</p>
<p>6's. Specifically, we have that
N A 9
</p>
<p>N B 25
</p>
<p>Another problem might be to det ermine the probability of A = {I , 4}, knowing
that the outcome is even. In this case, we should use N AnB / N B to make sure we</p>
<p/>
</div>
<div class="page"><p/>
<p>4.3. JOINT EVENTS AND THE CONDITIONA L PROBABILITY 75
</p>
<p>. . . ... ...... ... . .. .... . - .. .. .. . . . .
</p>
<p>. . . ......
. . .. '.' ........ .. . .. . .. ... .... .II : I I I 1
</p>
<p>II . I 1 I 1
</p>
<p>, .. . .. 11.:.. .. 1. 1 .. .. I .. . j .. ,
II : 1 I I I
</p>
<p>II : I I I I, . " ' t l :' . , , " '1'1 " ... .. t &middot;. &middot; j ....
I III : I 1 I III I I I I
</p>
<p>I III : I I I III I I I I
</p>
<p>I In r rrrur T ' I ' 1 I" r
I I II I I I III I I I I I
</p>
<p>I III I I I III I I I I I , .&bull; .. , ... .. ....... . &bull; '. .'. ... jI I III I I I 11111 I I II I II II
I I III I I I 11111 I I II I II II I
</p>
<p>III I. 1,Ii
</p>
<p>6
</p>
<p>5
</p>
<p>2
</p>
<p>o
o 10 20 30
</p>
<p>Trial number
40 50
</p>
<p>F igure 4.1: Outcomes for repeated tossing of a fair die.
</p>
<p>only count the outcomes that can occur in light of our knowledge of B. For this
</p>
<p>example, only the 4 in {1,4} could have occurred. If an outcome is not in B, then
</p>
<p>that outcome will not be included in An B and will not be counted in NAnB. Now
</p>
<p>let t ing S = {I, 2, 3, 4,5, 6} be the sample space and Ns its size, the probability of
</p>
<p>A given B is
</p>
<p>NAnB _ Ntr2
B ~ P[A n B]
</p>
<p>NB - &amp; ~ P[B]
Ns
</p>
<p>This is termed the conditi onal probability and is denoted by P[AIB] so that we have
as our definition
</p>
<p>P[AIB] = P[A n B]
P[B] .
</p>
<p>(4.1)
</p>
<p>Note that to determine it , we require P[A n B] which is the probability of both A
and B occurring or the probability of the intersection. Intuit ively, the conditional
</p>
<p>probability is the proportion of time A and B occurs divided by the proportion of
</p>
<p>time that B occurs. The event B = {2, 4, 6} comprises a new sample space and is
</p>
<p>sometimes called the reduced sample space. The denominator term in (4.1) serves to
</p>
<p>normalize the conditional probabilities so that the probability of the reduced sample
</p>
<p>space is one (set A = B in (4.1)). Returning to the die toss , the probability of a 4,
given that the out come is even , is found as
</p>
<p>A n B
</p>
<p>B
</p>
<p>{4} n {2,4,6} = {4} = A
{2,4,6}</p>
<p/>
</div>
<div class="page"><p/>
<p>76 CHAPTER 4. CONDITIONAL PROBABILITY
</p>
<p>WI W2 W3 W4 W5 P[Hi]
</p>
<p>100-130 130- 160 160-190 190-220 220-250
</p>
<p>H I 5 - 5 4" 0.08 0.04 0.02 0 0 0.14
</p>
<p>H 2 5'4" - 5'8" 0.06 0.12 0.06 0.02 0 0.26
</p>
<p>H 3 5' 8"- 6' 0 0.06 0.14 0.06 0 0.26
</p>
<p>H 4 6'- 6' 4" 0 0.02 0.06 0.10 0.04 0.22
</p>
<p>H 5 6'4"- 6'8" 0 0 0 0.08 0.04 0.12
</p>
<p>Table 4.1: Joint probabilit ies for heights and weights of college students.
</p>
<p>and therefore
</p>
<p>P[AIB]
p[AnB]
</p>
<p>P[B]
</p>
<p>1/6 1
= - -
</p>
<p>3/6 3
</p>
<p>P[A]
</p>
<p>P[B]
</p>
<p>as expected. Note that P[A n B] and P[B] are computed based on the original
sample space, S. .
</p>
<p>The event A n B is usually called the joint event sin ce both events must occur
for a nonempty intersection. Likewise, P[A n B] is termed the j oint probability, but
of course, it is nothing more than the probability of an intersection. Also, P[A]
</p>
<p>is called the marg inal probabilit y to distinguish it from the joint and conditional
</p>
<p>probabilities. The reason for this terminology will be discussed shortly.
</p>
<p>In defining the condit ional pr obability of (4.1) it is assume d that P[B] f:. O. Oth-
erwise, t heoretically and pract ically, t he definition would not make sense. Another
</p>
<p>example follows.
</p>
<p>Example 4.1 - Heights and weights of college students
</p>
<p>A population of college student s have heights H and weights W which are grouped
</p>
<p>into ranges as shown in Table 4.1. The table gives the joint pr obability of a student
</p>
<p>having a given height and weight , which can be denoted as P[Hi nWj]. For example,
</p>
<p>if a st udent is selected, the probabili ty of his/her height being between 5'4" and 5' 8"
</p>
<p>and also his/her weight being between 130 lbs. and 160 lbs. is 0.12. Now consider the
</p>
<p>event that the student has a weight in the ra nge 130-160 lbs. Calling this event A
</p>
<p>we next determine its probability. Since A = {( H , W) : H = HI , ... , H 5 ; W = W2} ,
it is explicitly
</p>
<p>and since the simple events are by definition mutually exclusive, we have by Axiom</p>
<p/>
</div>
<div class="page"><p/>
<p>4.3. JOINT EVENTS AND THE CONDITIONAL PROBABILITY
</p>
<p>3' (see Section 3.4)
</p>
<p>5
</p>
<p>P[A] L P[(Hi ,W2)] = 0.04 + 0.12 + 0.06 + 0.02 + 0
i=1
</p>
<p>= 0.24.
</p>
<p>77
</p>
<p>Next we determine the probability that a student 's weight is in the range of 130-160
</p>
<p>lbs. , given that the student has height less than 6' . The event of interest A is the
</p>
<p>same as before. The conditioning event is B = {(H,W) : H = H I ,H2,H3;W =
</p>
<p>WI , ... , W5} so that An B = {(HI ,W2), (H2,W2), (H3' W2)} and
</p>
<p>P[AIB]
p[AnB]
</p>
<p>P[B]
</p>
<p>= 0.33.
</p>
<p>0.04 + 0.12 + 0.06
0.14 + 0.26 + 0.26
</p>
<p>We see that it is more probable that the student has weight between 130 and 160
</p>
<p>lbs. if it is known beforehand that his/her height is less than 6'. Not e that in finding
</p>
<p>P[B] we have used
3 5
</p>
<p>P[B] = L L P[(Hi , Wj)]
i=1 j=l
</p>
<p>(4.2)
</p>
<p>which is determined by first summing along each row to produce the entries shown
</p>
<p>in Table 4.1 as P[Hi] ' These are given by
</p>
<p>5
</p>
<p>P[Hi] = LP[(Hi ' Wj)]
j=1
</p>
<p>(4.3)
</p>
<p>and then summing the P [Hi] 's for i = 1,2,3. Hence , we could have written (4.2)
</p>
<p>equivalent ly as
3
</p>
<p>P[B] = LP[Hi].
</p>
<p>i=1
</p>
<p>The probabilities P[Hi] are called the marginal probabilities since they are written
</p>
<p>in the margin of the table. If we were to sum along the columns, then we would
</p>
<p>obtain the marginal probabilities for the weights or P[Wj]. These are given by
</p>
<p>5
</p>
<p>P[Wj] = LP[(Hi , Wj)].
</p>
<p>i= 1
</p>
<p>It is important to observe that by utilizing the information that the student 's
</p>
<p>height is less than 6', the probability of the event has changed; in this case, it
</p>
<p>has increased from 0.24 to 0.33. It is also possible that the opposite may occur.
</p>
<p>If we were to determine the probability that the student's weight is in the range</p>
<p/>
</div>
<div class="page"><p/>
<p>78 CHAPTER 4. CONDITIONAL PROBABILITY
</p>
<p>130-160 lbs., given that he/she has a height greater than 6', then defining the
</p>
<p>conditioning event as B = {(H, W) : H = H4, Hs; W = Wi, " " Ws} and noting
that An B = {(H4 , W2) , (Hs,W2} we have
</p>
<p>P[AIB]
0.02 + 0
</p>
<p>0.22 + 0.12
0.058.
</p>
<p>Hence, the conditional probability has now decreased with respect to the uncondi-
</p>
<p>tional probability or P[A].
</p>
<p>In general we may have
</p>
<p>P[AIB]
</p>
<p>P[AIB]
</p>
<p>P[AIB]
</p>
<p>&gt; P[A]
</p>
<p>&lt; P[A]
</p>
<p>P[A].
</p>
<p>See Figure 4.2 for another example. The last possibility is of particular interest since
</p>
<p>1
</p>
<p>y
</p>
<p>B
</p>
<p>A
</p>
<p>x
</p>
<p>(a)
</p>
<p>2/3 = P[AIB] &gt; P[A] = 1/2
</p>
<p>y
</p>
<p>' ~ '+-- B
</p>
<p>A
x
</p>
<p>(b)
</p>
<p>1/3 = P[AjB] &lt; P[A] = 1/2
</p>
<p>y
</p>
<p>A
</p>
<p>(c)
</p>
<p>1/2 = P[AIB] = P[A] = 1/2
</p>
<p>Figure 4.2: Illustration of possible relationships of conditional probability to ordi-
</p>
<p>nary probability.
</p>
<p>it states that the probability of an event A is the same whether or not we know that
</p>
<p>B has occurred. In this case, the event A is said to be statistically independent of
</p>
<p>the event B. In the next section, we will explore this further.
</p>
<p>Before proceeding, we wish to emphasize that a conditional probability is a true
</p>
<p>probability in that it satisfies the axioms described in Chapter 3. As a result, all the
</p>
<p>rules that allow one to manipulate probabilities also apply to conditional probabili-
</p>
<p>ties. For example, since Property 3.1 must hold, it follows that P[ACIB] = 1-P[AIB]
(see also Problem 4.10). To prove that the axioms are satisfied for conditional prob-
</p>
<p>abilities we first assume that the axioms hold for ordinary probabilities. Then,</p>
<p/>
</div>
<div class="page"><p/>
<p>Axiom 1
</p>
<p>4.3. JOINT EVENTS AND THE CONDITIONAL PROBABILITY
</p>
<p>P[AIB] = P [A n B] &gt; 0
P[B] -
</p>
<p>since P[A n B] ~ 0 and P[B] ~ O.
</p>
<p>79
</p>
<p>Axiom 2
P[SIB] = P[S n B] = P[B] = 1
</p>
<p>P[B] P[B] .
</p>
<p>Axiom 3 If A and C are mutually exclusive events, then
</p>
<p>=
</p>
<p>P[AUOIB]
P[(A U 0) nB]
</p>
<p>P[B]
</p>
<p>P[(A n B) U (C n B)]
P[B]
</p>
<p>P[A n B] + P[O n B]
P[B]
</p>
<p>= P[AIB] + P[OIB]
</p>
<p>(definition)
</p>
<p>(distributive property)
</p>
<p>(Axiom 3 for ordinary probability,
</p>
<p>An 0 = 0* (A n B) n (0 n B) = 0)
(definition of conditional probability).
</p>
<p>Conditional probabilities are useful in that they allow us to simplify probability
</p>
<p>calculat ions. One particularly important relationship based on conditional proba-
</p>
<p>bility is described next . Consider a partitioning of the sample space S. Recall that
</p>
<p>a partition is defined as a group of sets B I , B2 , . &bull;&bull; ,BN such that S = U~I B, and
e, n e, = 0 for i i= j. Then we can rewrite the probability P[A] as
</p>
<p>Bu t by a slight extension of the distributive property of sets, we can express this as
</p>
<p>P[A] = P[ (A n Bd U (A n B2 ) U ... U (A n BN )] .
</p>
<p>Since the Bi's are mutually exclusive, then so are the An Bi 'S , and therefore
</p>
<p>N
</p>
<p>P[A] = L P[A n Bi]
i=1
</p>
<p>or finall y
N
</p>
<p>P[A] = L P[A IBi]P[Bi]'
i = 1
</p>
<p>(4.4)
</p>
<p>This relationship is called the law of total probability. Its utility is illustrated next.
</p>
<p>Example 4.2 - A compound experiment
</p>
<p>Two urns contain different proportions of red and black balls. Urn 1 has a pro-
</p>
<p>portion PI of red balls and a proportion 1 - PI of black balls whereas urn 2 has</p>
<p/>
</div>
<div class="page"><p/>
<p>80 CHAPTER 4. CONDITIONAL PROBABILITY
</p>
<p>proportions of P2 and 1 - P2 of red balls and black balls, respectively. A compound
</p>
<p>experiment is performed in which an urn is chosen at random, followed by the se-
</p>
<p>lection of a ball. We would like to find the probability that a red ball is selected.
</p>
<p>To do so we use (4.4) with A = {red ball selected}, B I = {urn 1 chosen}, and
B 2 = {urn 2 chosen}. Then
</p>
<p>P[red ball selected] = P[red ball selectedjurn 1 chosen]P[urn 1 chosen]
</p>
<p>+P[red ball selectedIurn 2 chosen]P[urn 2 chosen]
</p>
<p>PI ~ + P2~ = ~(PI +P2).
</p>
<p>&amp;. Do B I and B2 really partition the sample space?
To verify that the application of the law of total probability is indeed valid for this
</p>
<p>problem, we need to show that B I U B 2 = Sand B I n B 2 = 0. In our description
of B I and B 2 we refer to the choice of an urn. In actuality, this is shorthand for all
</p>
<p>the balls in the urn. If urn 1 cont ains balls numbered 1 to N I , then by choosing urn
</p>
<p>1 we are really saying that the event is that one of the balls numbered 1 to N I is
</p>
<p>chosen and similarly for urn 2 being chosen. Hence, since the sample space consists
</p>
<p>of all the numbered balls in urns 1 and 2, it is observed that the union of B I and
</p>
<p>B 2 is the set of all possible outcomes or the sample space. Also , B I and B 2 are
</p>
<p>mutually exclusive since we choose urn 1 or urn 2 but not both.
</p>
<p>Some more examples follow.
</p>
<p>Example 4.3 - Probability of error in a digital communication system
</p>
<p>In a digital communication system a "0" or "I" is transmitted to a receiver. Typi-
</p>
<p>cally, either bit is equally likely to occur so that a prior probability of 1/2 is assumed.
</p>
<p>At the receiver a decoding error can be made due to channel noise, so that a 0 may
</p>
<p>be mistaken for a 1 and vice versa. Defining the probability of decoding a 1 when a
</p>
<p>o is transmitted as E and a 0 when a 1 is transmitted also as E, we are interested in
the overall probability of an error. A probabilistic model summarizing the relevant
</p>
<p>features is shown in Figure 4.3. Note that the problem at hand is essentially the
</p>
<p>same as the previous one . If urn 1 is chosen, then we transmit a 0 and if urn 2
</p>
<p>is chosen, we transmit a 1. The effect of the channel is to introduce an error so
</p>
<p>that even if we know which bit was transmitted, we do not know the received bit.
</p>
<p>This is analogous to not knowing which ball was chosen from the given urn. The</p>
<p/>
</div>
<div class="page"><p/>
<p>4.3. JOINT EVENTS AND THE CONDITIONAL PROBABILITY 81
</p>
<p>Choose
</p>
<p>oor 1
</p>
<p>P[O] = P[l] = 1/2
</p>
<p>t
transmit
</p>
<p>t
receive
</p>
<p>Figure 4.3: Probabilistic model of a digital communication system.
</p>
<p>probability of error is from (4.4)
</p>
<p>P[error] = P[errorlO transmitted]P[O transmitted]
</p>
<p>+P [error I1 transmitted]P[l transmitted]
1 1
</p>
<p>EZ + EZ = E.
</p>
<p>o
Conditional probabilities can be quite tricky, in that they sometimes produce coun-
</p>
<p>terintuitive results. A famous instance of this is the Monty Hall or Let 's Make a
</p>
<p>Deal problem.
</p>
<p>Example 4.4 - Monty Hall problem
</p>
<p>About 40 years ago there was a television game show called "Let's Make a Deal".
</p>
<p>The game show host , Monty Hall , would present the contestant with three closed
</p>
<p>doors. Behind one door was a new car, while the others concealed less desireable
</p>
<p>prizes, for instance, farm animals. The contestant would first have the opportunity
</p>
<p>to choose a door, but it would not be opened. Monty would then choose one of the
</p>
<p>remaining doors and open it. Since he would have knowledge of which door led to
</p>
<p>the car, he would always choose a door to reveal one of the farm animals. Hence,
</p>
<p>if the contestant had chosen one of the farm animals, Monty would then choose the
</p>
<p>door that concealed the other farm animal. If the contestant had chosen the door
</p>
<p>behind which was the car , then Monty would choose one of the other doors, both
</p>
<p>concealing farm animals, at random. At this point in the game, the contestant was
</p>
<p>faced with two closed doors, one of which led to the car and the other to a farm
</p>
<p>animal. The contestant was given the option of either opening the door she had
</p>
<p>originally chosen or deciding to open the other door. What should she do? The
</p>
<p>answer, surprisingly, is that by choosing to switch doors she has a probability of 2/3
</p>
<p>of winning the car! If she stays with her original choice, then the probability is only
</p>
<p>1/3. Most people would say that irregardless of which strategy she decided upon,
</p>
<p>her probability of winning the car is 1/2.</p>
<p/>
</div>
<div class="page"><p/>
<p>82 CHAPTER 4. CONDITIONAL PROBABILITY
</p>
<p>M &middot;J
1 2 3
</p>
<p>1 0 .!. .!.6 6
Ci 2 0 0 1*3'
</p>
<p>3 0 1* 0:l
</p>
<p>Table 4.2: Joint probabilities (P[Ci ,M j] = P[Mj!Ci]P[Ci]) for contestant's initial
and Monty's choice of doors. Winning door is 1.
</p>
<p>To see how these probabilities are determined first assume she stays with her
</p>
<p>original choice . Then, since the car is equally likely to be placed behind any
</p>
<p>of the three doors, the probability of the contestant's winning the car is 1/3.
</p>
<p>Monty's choice of a door is irrelevant since her final choice is always the same
</p>
<p>as her initial choice. However, if as a result of Monty's action a different door
</p>
<p>is selected by the contestant , then the probability of winning becomes a condi-
</p>
<p>tional probability. We now compute this by assuming that the car is behind door
</p>
<p>one. Define the events C; = {contestant initially chooses door i} for i = 1,2,3 and
</p>
<p>Mj = {Monty opens door j} for j = 1,2,3. Next we determine the joint probabili-
ties P[Ci , M j ] by using
</p>
<p>Since the winning door is never chosen by Monty, we have P[M1ICi ] = O. Also,
Monty never opens the door initially chosen by the contestant so that P[MiICi] = O.
</p>
<p>Then, it is easily verified that
</p>
<p>P[MzIC3]
</p>
<p>P[M3IC1]
</p>
<p>P[M3ICz]= 1
1
</p>
<p>= P[MzIC1] = '2
</p>
<p>(contestant chooses losing door)
</p>
<p>(contestant chooses winning door)
</p>
<p>and P[Ci ] = 1/3. The joint probabilities are summarized in Table 4.2. Since
</p>
<p>the contestant always switches doors, the winning events are (2,3) (the contestant
</p>
<p>initially chooses door 2 and Monty chooses door 3) and (3,2) (the contestant initially
</p>
<p>chooses door 3 and Monty chooses door 2). As shown in Table 4.2 (the entries with
</p>
<p>asterisks) , the total probability is 2/3. This may be verified directly using
</p>
<p>P[final choice is door 1] P[M3ICz]P[Cz]+ P[MzIC3]P[C3]
P[Cz,M 3] + P[C3 , M z].
</p>
<p>Alternatively, the only way she can lose is if she initially chooses door one since she
</p>
<p>always switches doors. This has a probability of 1/3 and hence her probability of
</p>
<p>winning is 2/3. In effect , Monty, by eliminating a door, has improved her odds!</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4. STATISTICALLY INDEPENDENT EVENTS
</p>
<p>4.4 Statistically Independent Events
</p>
<p>83
</p>
<p>T wo events A and B are said to be statistically independent (or sometimes just
</p>
<p>independent) if P[AIB] = P[A]. If this is t rue, t hen
</p>
<p>P[AIB] = P[A n B] = P[A]
P[B]
</p>
<p>which results in the condition for statistical independence of
</p>
<p>P[A n B] = P [A]P[B]. (4.5)
</p>
<p>P [AIB]
</p>
<p>An example is shown in Figure 4.2c. There, the probability of A is unchanged if we
</p>
<p>know that the outcome is contained in the event B. Note, however , that once we
</p>
<p>know that B has occur red, t he outcome could not have been in the un cross-hat ched
</p>
<p>region of A but must be in the cross-hatched region. Knowing that B has occur red
</p>
<p>does in fact affect the possible outcomes. However , it is the ratio of P[A n B] to
P[B] t hat remains the same.
</p>
<p>Example 4.5 - Statistical independence does not mean one event does
</p>
<p>not affect another event.
</p>
<p>If a fair die is tossed , the probability of a 2 or a 3 is P[A = {2,3}] = 1/3. Now
assume we know that t he outcome is an even number or B = {2, 4, 6}. Recomputing
the probability
</p>
<p>p [AnB] P[{2}]
=
</p>
<p>P[B ] P[{2,4,6}]
1
3 = P [A].
</p>
<p>Hence, A and B are independent . Yet , knowledge of B occurring has affected the
</p>
<p>possible outcomes. In particular, the event A n B = {2} has half as many elements
as A , but the reduced sample space S' = B also has half as many elements.
</p>
<p>c
The condit ion for the event A to be independent of the event B is P[A n B] =
P[A]P[B] . Hence, we need only know the marginal probabilit ies or P[A], P[B] to
det ermine the jo in t probabilit y P[A n B]. In practice, this property turns out to be
very useful. Finally, it is imp ortant to observe th at stat ist ical independence has a
</p>
<p>symmetry property, as we might expect . If A is independ ent of B , t hen B must be
</p>
<p>independent of A since
</p>
<p>P[B IA] =
p[BnA]
</p>
<p>P[A]
</p>
<p>p[A nB]
=
</p>
<p>P[A]
</p>
<p>P[A]P[B]
</p>
<p>P [A]
</p>
<p>= P[B ]
</p>
<p>(definition)
</p>
<p>(commutative property)
</p>
<p>(A is independent of B )</p>
<p/>
</div>
<div class="page"><p/>
<p>84 CHAPTER 4. CONDITIONAL PROBABILITY
</p>
<p>and therefore B is independent of A. Henceforth, we can say that the events A and
</p>
<p>B are statistically independent of each other, without further elaboration.
</p>
<p>~ Statistically independent events are different than mutually ex-
clusive events.
</p>
<p>If A and B are mutually exclusive and B occurs, then A cannot occur. Thus,
</p>
<p>P[AIB] = O. If A and B are statistically independent and B occurs, then P[AIB] =
P[A]. Clearly, the probabilities P[AIB] are only the same if P[A] = O. In general
then, the conditions of mutually exclusivity and independence must be different
</p>
<p>since they lead to different values of P[AIB]. A specific example of events that
</p>
<p>B A
</p>
<p>+&bull;
</p>
<p>Figure 4.4: Events that are mutually exclusive (since An B = 0) and independent
(since P[A n B] = P[0] = 0 and P[A]P[B] = o&middot; P[B] = 0).
</p>
<p>are both mutually exclusive and statistically independent is shown in Figure 4.4.
</p>
<p>Finally, the two conditions produce different relationships, namely
</p>
<p>P[A U B] = P[A] + P[B]
P[A n B] = P[A]P[B]
</p>
<p>mutually exclusive events
</p>
<p>statistically independent events.
</p>
<p>See also Figure 4.2c for statistically independent but not mutually exclusive events.
</p>
<p>Can you think of a case of mutually exclusive but not independent events?
</p>
<p>~
Consider now the extension of the idea of statistical independence to three events.
</p>
<p>Three events are defined to be independent if the knowledge that anyone or two
</p>
<p>of the events has occurred does not affect the probability of the third event. For
</p>
<p>example, one condition is that P[AIB n C] = P[A]. We will use the shorthand
notation P[AIB, C] to indicate that this is the probability of A given that Band
C has occurred. Note that if Band C has occurred, then by definition B n C has
occurred. The full set of conditions is
</p>
<p>P[AJB]
</p>
<p>P[BIA] =
</p>
<p>P[CIA] =
</p>
<p>P[AIC] = P[AIB, C] = P[A]
</p>
<p>P[BIG] = P[BIA, G] = P[B]
P[CIB] = P[GIA,B] = P[G].</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4. STATISTICALLY INDEPENDENT EVENTS
</p>
<p>These conditions are satisfied if and only if
</p>
<p>85
</p>
<p>P[AB]
</p>
<p>P[AC]
</p>
<p>P[BC]
</p>
<p>P[ABC]
</p>
<p>P[A]P[B]
</p>
<p>P[A]P[C]
</p>
<p>P[B]P[C]
</p>
<p>P[A]P[B]P[C].
</p>
<p>(4.6)
</p>
<p>(4.7)
</p>
<p>(4.8)
</p>
<p>(4.9)
</p>
<p>If the first three conditions (4.6)-(4.8) are satisfied, then the events are said to be
</p>
<p>pairwise independent. They are not enough, however, to ensure independence. The
</p>
<p>last condition (4.9) is also required since without it we could not assert that
</p>
<p>P[AIB, C] P[AIBC]
</p>
<p>P[ABC]
=
</p>
<p>P[BC]
</p>
<p>P[ABC]
</p>
<p>P[B]P[C]
</p>
<p>P[A]P[B]P[C]
</p>
<p>P[B]P[C]
</p>
<p>P[A]
</p>
<p>(definition of Band C occurring)
</p>
<p>(definition of conditional probability)
</p>
<p>(from (4.8))
</p>
<p>(from (4.9))
</p>
<p>and similarly for the other conditions (see also Problem 4.20 for an example). In
</p>
<p>general, events E 1 , E 2 , ... , EN are defined to be statistically independent if
</p>
<p>P[EiEj]
</p>
<p>P[EiEjEk]
</p>
<p>P[Ei]P[Ej]
</p>
<p>P[Ei]P[Ej ]P[Ek]
</p>
<p>i=l=j
</p>
<p>i=l=j=l=k
</p>
<p>Although statistically independent events allow us to compute joint probabilities
</p>
<p>based on only the marginal probabilities, we can still determine joint probabilities
</p>
<p>without this property. Of course, it becomes much more difficult. Consider three
</p>
<p>events as an example. Then, the joint probability is
</p>
<p>P[ABC] P[AIB, C]P[BC]
</p>
<p>= P[AIB, C]P[BIC]P[C]. (4.10)
</p>
<p>This relationship is called the probability chain rule. One is required to determine
</p>
<p>conditional probabilities, not always an easy matter. A simple example follows.</p>
<p/>
</div>
<div class="page"><p/>
<p>86 CHAPTER 4. CONDITIONAL PROBABILITY
</p>
<p>Example 4.6 - Tossing a fair die - once again
</p>
<p>If we toss a fair die, then it is clear that the probability of the outcome being 4 is
</p>
<p>1/6. We can, however, rederive this result by using (4.10). Letting
</p>
<p>A {even number} = {2,4,6}
</p>
<p>B = {numbers&gt;2}={3,4,5,6}
</p>
<p>C {numbers &lt; 5} = {1,2,3,4}
</p>
<p>we have that ABC = {4}. These events can be shown to be dependent (see Problem
4.21). Now making use of (4.10) and noting that BC = {3, 4} it follows that
</p>
<p>P[ABC]
</p>
<p>4.5 Bayes' Theorem
</p>
<p>P[AIB ,C]P[BjC]P[C]
</p>
<p>(
1/ 6 ) (2/6) (i) = ~.
2/6 4/6 6 6
</p>
<p>The definition of conditional probability leads to a famous and sometimes contro-
</p>
<p>versial formula for computing conditional probabilities. Recalling the definition, we
</p>
<p>have that
</p>
<p>P[AIB] = P[AB]
P[B]
</p>
<p>and
</p>
<p>P[BIA] = P[AB]
P[A] .
</p>
<p>Upon substitution of P[AB] from (4.11) into (4.12)
</p>
<p>P[BIA] = P[AIB]P[B]
P[A] .
</p>
<p>(4.11)
</p>
<p>(4.12)
</p>
<p>(4.13)
</p>
<p>This is called Bayes' theorem. By knowing the marginal probabilities P[A], P[B]
</p>
<p>and the conditional probability P[AIB], we can determine the other conditional
</p>
<p>probability P[BIA]. The theorem allows us to perform "inference" or to assess
(with some probability) the validity of an event when some other event has been
</p>
<p>observed. For example, if an urn containing an unknown composition of balls is
</p>
<p>sampled with replacement and produces an outcome of 10 red balls, what are we to
</p>
<p>make of this? One might conclude that the urn contains only red balls. Yet, another
</p>
<p>individual might claim that the urn is a "fair" one , containing half red balls and
</p>
<p>half black balls, and attribute the outcome to luck. To test the latter conjecture we
</p>
<p>now determine the probability of a fair urn given that 10 red balls have just been
</p>
<p>drawn. The reader should note that we are essentially going "backwards" - usually</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5. BAYES'THEOREM 87
</p>
<p>we compute the probability of choosing 10 red balls given a fair urn. Now we are
</p>
<p>given the outcomes and wish to determine the probability of a fair urn. In doing so
</p>
<p>we believe that the urn is fair with probability 0.9. This is due to our past experience
</p>
<p>with our purchases from urn.com. In effect, we assume that the prior probability of
</p>
<p>B = {fair urn} is P[B] = 0.9. If A = {10 red balls drawn}, we wish to determine
P[B IA], which is the probability of the urn being fair after the experiment has been
</p>
<p>performed or the posterior probability. This probability is our reassessment of the
</p>
<p>fair urn in light of the new evidence (10 red balls drawn). Let 's compute P[BJA]
</p>
<p>which according to (4.13) requires knowledge of the prior probability P[B] and the
</p>
<p>conditional probability P[AJB]. The former was assumed to be 0.9 and the latter is
</p>
<p>the probability of drawing 10 successive red balls from an urn with p = 1/2. From
our previous work this is given by the binomial law as
</p>
<p>P[AIB] = P[k = 10] = (~) pk(l _ p)M-k
</p>
<p>= (~~) (~) 10 (~) 0 = (~) 10
</p>
<p>We still need to find P[A]. But this is easily found using the law of total probability
</p>
<p>as
</p>
<p>P[A] = P[AIB]P[B] + P[AJBC]P[BC]
</p>
<p>= P[AIB]P[B] + P[AIBC](l - P[B])
</p>
<p>and thus only P[AIBC] needs to be determined (and which is not equal to 1- P[AIB]
</p>
<p>as is shown in Problem 4.9). This is the conditional probability of drawing 10 red
</p>
<p>balls from a unfair urn. For simplicity we will assume that an unfair urn has all red
</p>
<p>balls and thus P[AJB C] = 1. Now we have that
</p>
<p>P[A] = (~) 10 (0.9) + (1)(0.1)
</p>
<p>and using this in (4.13) yields
</p>
<p>(1) 10 (0 9)
P[BIA] = 2 . = 0.0087.
</p>
<p>(~)10 (0.9) + (1)(0.1)
</p>
<p>The posterior probability (after 10 red balls have been drawn) that the urn is fair
</p>
<p>is only 0.0087. Our conclusion would be to reject the assumption of a fair urn.
</p>
<p>Another way to quantify the result is to compare the posterior probability of the
</p>
<p>unfair urn to the probability of the fair urn by the ratio of the former to the latter.
</p>
<p>This is called the odds ratio and it is interpreted as the odds against the hypothesis
</p>
<p>of a fair urn. In this case it is
</p>
<p>odds = P[BCIA] = 1 - 0.0087 = 113.
P[BIA] 0.0087</p>
<p/>
</div>
<div class="page"><p/>
<p>88 CHAPTER 4. CONDITIONAL PROBABILITY
</p>
<p>It is seen from this example that based on observed "data", prior beliefs embodied
</p>
<p>in P[B] = 0.9 can be modified to yield posterior beliefs or P[B\AJ = 0.0087. This
is an important concept in statistical inference [Press 2003].
</p>
<p>In the previous example, we used the law of total probability to determine the
</p>
<p>posterior probability. More generally, if a set of Bi's partition the sample space,
</p>
<p>then Bayes' theorem can be expressed as
</p>
<p>k = 1,2, . .. , N . (4.14)
</p>
<p>The denominator in (4.14) serves to normalize the posterior probability so that the
</p>
<p>conditional probabilities sum to one or
</p>
<p>N
</p>
<p>L P[BkIA] = 1.
k=l
</p>
<p>In many problems one is interested in determining whether an observed event
</p>
<p>or effect is the result of some cause. Again the backwards or inferential reasoning
</p>
<p>is implicit. Bayes' theorem can be used to quantify this connection as illustrated
</p>
<p>next.
</p>
<p>Example 4.7 - Medical diagnosis
</p>
<p>Suppose it is known that 0.001% of the general population has a certain type of
</p>
<p>cancer. A patient visits a doctor complaining of symptoms that might indicate the
</p>
<p>presence of this cancer. The doctor performs a blood test that will confirm the
</p>
<p>cancer with a probability of 0.99 if the patient does indeed have cancer. However,
</p>
<p>the test also produces false positives or says a person has cancer when he does not.
</p>
<p>This occurs with a probability of 0.2. If the test comes back positive, what is the
</p>
<p>probability that the person has cancer?
</p>
<p>To solve this problem we let B = {person has cancer}, the causitive event, and
A = {test is positive}, the effect of that event. Then, the desired probability is
</p>
<p>P[BIA] =
P[AIB]P[B]
</p>
<p>P[AIB]P[B] + P[AIBc]P[Bc]
(0.99) (0.00001)
</p>
<p>(0.99)(0.00001) + (0.2)(0.99999)
</p>
<p>The prior probability of the person having cancer is P[B] = 10-5 while the posterior
probability of the person having cancer (after the test is performed and found to
</p>
<p>be positive) is P[BIA] = 4.95 x 10-5 . With these results the doctor might be hard
pressed to order additional tests. This is quite surprising, and is due to the prior
</p>
<p>probability assumed, which is quite small and therefore tends to nullify the test
</p>
<p>results. If we had assumed that P[B] = 0.5, for indeed the doctor is seeing a patient</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6. MULTIPLE EX PERIMENT S 89
</p>
<p>who is complaining of symptoms consistent with cancer and not some person chosen
</p>
<p>at random from the general population, then
</p>
<p>(0.99)(0 .5)
P [B IA] = (0.99)(0.5) + (0.2)(0.5) = 0.83
</p>
<p>which seems more reasonable (see also P roblem 4.23). The controversy surrounding
</p>
<p>the use of Bayes ' theorem in probability calculations can almost always be traced
</p>
<p>back to the prior probability assumption. Bayes' theorem is mathematically correct
</p>
<p>- only its application is sometimes in doubt!
</p>
<p>4 .6 Multiple Experiments
</p>
<p>4 .6 .1 Independent Subexperiments
</p>
<p>An experiment that was discussed in Chapter 1 was the repeated tossing of a coin.
</p>
<p>We can alternatively view this experiment as a succession of subexperiments, with
</p>
<p>each sub experiment being a single toss of the coin. It is of interest to invest igate the
</p>
<p>relationship between the probabilities defined on the experiment and those defined
</p>
<p>on the subexperiments. To be more concrete, assume a coin is tossed twice in
</p>
<p>succession and we wish to determine the probability of the event A = {(H,Tn.
Recall that t he notation (H,T) denotes an ordered 2-tuple and represents a head
</p>
<p>on toss 1 and a tail on toss 2. For a fair coin it was determined to be 1/4 since
</p>
<p>we assumed that all 4 possible outcomes were equally likely. This seemed like a
</p>
<p>reasonable assumption. However, if the coin had a probability of heads of 0.99, we
</p>
<p>might not have been so quick to agree with the equally likely assumption. How
</p>
<p>then are we to determine the probabilities? Let 's first consider the experiment to
</p>
<p>be composed of two separate sub experiments with each subexperiment having a
</p>
<p>sample space S1 = {H,T }. The sample space of the overall experiment is obtained
</p>
<p>by forming the cartesian product, which for this example is defined as
</p>
<p>S S1 X S1
</p>
<p>{(i ,j) : i E S\ j E S1}
</p>
<p>= {(H,H ), (H, T ), (T ,H) , (T, T)} .
</p>
<p>It is formed by taking an outcome from S1 for the first element of the 2-tuple and an
</p>
<p>outcome from S1 for the second element of the 2-tup le and doing this for all possib le
</p>
<p>outcomes. It would be exceedingly useful if we could determine probabilities for
</p>
<p>events defined on S from those probabilities for events defined on S1 . In this way
</p>
<p>the determination of probabilities of very complicated events could be simplified.
</p>
<p>Such is the case if we assume that the subexperiments are independent. Continuing
</p>
<p>on , we next calculate P[A] = P[(H,T) ] for a coin with an arbitrary probability of</p>
<p/>
</div>
<div class="page"><p/>
<p>90 CHAPTER 4. CONDITIONAL PROBABILITY
</p>
<p>heads p. This event is defined on the sample space of 2-tuples, which is S. We can,
</p>
<p>however , express it as an intersection
</p>
<p>{(H,T)} {(H,H) , (H,T)} n {(H,T) , (T ,T)}
{heads on toss 1} n {tails on toss 2}
</p>
<p>= HI nT2&middot;
</p>
<p>We would expect the events HI and T2 to be independent of each other. Whether a
</p>
<p>head or tail appears on the first toss should not affect the probability of the outcome
</p>
<p>of the second toss and vice versa. Hence, we will let P[(H,T)] = P[Hl]P[T2] in
</p>
<p>accordance with the definition of statistically independent events. We can determine
</p>
<p>P[Hl] either as P[(H, H), (H, T)], which is defined on S or equivalently due to the
</p>
<p>independence assumption as P[H], which is defined on SI. Note that P[H] is the
</p>
<p>marginal probability and is equal to P[(H, H)] + P[(H, T)]. But the latter was
specified to be p and therefore we have that
</p>
<p>P[Hl ] = p
</p>
<p>P[T2] = 1- p
</p>
<p>and finally ,
</p>
<p>P[(H,T)] = p(1 - p).
</p>
<p>For a fair coin we recover the previous value of 1/4, but not otherwise.
</p>
<p>Experiments that are composed of subexperiments whose probabilities of the
</p>
<p>outcomes do not depend on the outcomes of any of the other subexperiments are
</p>
<p>defined to be independent subexperiments. Their utility is to allow calculation of joint
</p>
<p>probabilities from marginal probabilities. More generally, if we have M independent
</p>
<p>subexperiments, with Ai an event described for experiment i , then the joint event
</p>
<p>A = Al n A 2 n .. . n AM has probability
</p>
<p>(4.15)
</p>
<p>Apart from the differences in sample spaces upon which the probabilities are defined,
</p>
<p>independence of subexperiments is equivalent to statistical independence of events
</p>
<p>defined on the same sample space.
</p>
<p>4.6.2 Bernoulli Sequence
</p>
<p>The single tossing of a coin with probability p of heads is an example of a Bernoulli
</p>
<p>trial. Consecutive independent Bernoulli trials comprise a Bernoulli sequence. More
</p>
<p>generally, any sequence of M independent subexperiments with each subexperiment
</p>
<p>producing two possible outcomes is called a Bernoulli sequence. Typically, the
</p>
<p>subexperiment outcomes are labeled as 0 and 1 with the probability of a 1 being p.
</p>
<p>Hence , for a Bernoulli trial prO] = 1-p and P[1] = p. Several important probability
laws are based on this model.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6. MULTIPLE EXPERIMENTS 91
</p>
<p>Binomial Probability Law
</p>
<p>Assume that M independent Bernoulli trials are carried out. We wish to determine
</p>
<p>the probability of k l 's (or successes) . Each outcome is an M -tuple and a successful
</p>
<p>outcome would consist of k 1's and M - k O's in any order. T hus, each successful
</p>
<p>outcome has a probability of pk( l - p)M-k due to independence. T he total number
</p>
<p>of successful outcomes is the number of ways k l 's may be placed in the M-tuple.
</p>
<p>This is known from combinatorics to be ( ~) (see Section 3.8). Hence, by summing
up the probabilities of the successful simple events, which are mutually exclusive ,
</p>
<p>we have
</p>
<p>k = O, l , . . . , M (4.16)
</p>
<p>which we immediately recognize as the binomia l probability law. We have previously
</p>
<p>encountered the same law when we chose M balls at random from an urn with
</p>
<p>replacement and desired the probability of obtaining k red ba lls. The proportion of
</p>
<p>red balls was p . In that case, each subexperiment was the choosing of a ball and all
</p>
<p>the subexperiments were independent of each other. The binomial probabilit ies are
</p>
<p>shown in Figure 4.5 for var ious values of p.
</p>
<p>10
</p>
<p>. t
2 4 6
</p>
<p>k
</p>
<p>0.1
</p>
<p>0.3
</p>
<p>0.05
</p>
<p>0 .35 ,-- - ~--~--~-~-----,
</p>
<p>0.25
</p>
<p>~
n... 0.2
</p>
<p>0.15
</p>
<p>1086
k
</p>
<p>42
</p>
<p>:
</p>
<p>-.
</p>
<p>&bull; r -r &bull;o
o
</p>
<p>0.1
</p>
<p>0.3
</p>
<p>0.05
</p>
<p>0.25
</p>
<p>0.35
</p>
<p>~
n... 0.2
</p>
<p>0.15
</p>
<p>(a) M = 10, p = 0.5 (b) M = 10, P = 0.7
</p>
<p>Figure 4.5: The binomial probability law for different values of p.
</p>
<p>Geometric Probability Law
</p>
<p>Another impor tant aspect of a Bernoulli sequence is the appearance of the first
</p>
<p>success. If we let k be the Bernoulli trial for which the first success is observed,
</p>
<p>then the event of interest is the simple event (f, f , ... , f , s) , where s, f denote success
</p>
<p>and failure, respectively. This is a k-tuple with the first k - 1 elements all f 's. The</p>
<p/>
</div>
<div class="page"><p/>
<p>92 CHAPTER 4. CONDITIONAL PROBABILITY
</p>
<p>probability of the first success at trial k is therefore
</p>
<p>P[k] = (1 _ p)k-lp k = 1,2, ... (4.17)
</p>
<p>where 0 &lt; p &lt; 1. This is called the geometric probability law. The geometric
probabilities are shown in Figure 4.6 for various values of p. It is interesting to note
</p>
<p>that the first success is always most likely to occur on the first trial or for k = 1.
</p>
<p>This is true even for small values of p, which is somewhat counterintuitive. However,
</p>
<p>upon further reflection, for the first success to occur on trial k = 1 we must have
</p>
<p>a success on trial 1 and the outcomes of the remaining trials are arbitrary. For a
</p>
<p>success on trial k = 2, for example, we must have a failure on trial 1 followed by a
</p>
<p>success on trial 2, with the remaining outcomes arbitrary. This additional constraint
</p>
<p>reduces the probability. It will be seen later, though, that the average number of
</p>
<p>trials required for a success is lip, which is more in line with our intuition. An
</p>
<p>108246
k
</p>
<p>l T , &bull;
</p>
<p>0.6
</p>
<p>0.5
</p>
<p>~0.4
</p>
<p>::s...
Q-. 0.3
</p>
<p>0.2 .
</p>
<p>0.1
</p>
<p>0
10 0
</p>
<p>k
</p>
<p>0.2 .
</p>
<p>0.6 .------,---~--~-~-----,
</p>
<p>~0.4 .
</p>
<p>::s...
c.... 0.3 . .
</p>
<p>0.5 .
</p>
<p>(a) p = 0.25 (b) p = 0.5
</p>
<p>Figure 4.6: The geometric probability law for different values of p.
</p>
<p>example of its use follows.
</p>
<p>Example 4.8 - Telephone calling
</p>
<p>A fax machine dials a phone number that is typically busy 80% of the time. The
</p>
<p>machine dials it every 5 minutes until the line is clear and the fax is able to be
</p>
<p>transmitted. What is the probability that the fax machine will have to dial the
</p>
<p>number 9 times? The number of times the line is busy can be considered the number
</p>
<p>of failures with each failure having a probability of 1 - p = 0.8. If the number is
</p>
<p>dialed 9 times, then the first success occurs for k = 9 and
</p>
<p>P[9] = (0.8)8(0.2) = 0.0336.
</p>
<p>c</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6. MULTIPLE EXPERIMENTS 93
</p>
<p>(4.18)
</p>
<p>A useful property of the geometric probability law is that it is memoryless. Assume
</p>
<p>it is known that no successes occurred in the first m trials. Then, the probability of
</p>
<p>the first success at trial m + l is the same as if we had started the Bernoulli sequence
experiment over again and determined the probability of the first success at trial l
</p>
<p>(see Problem 4.34) .
</p>
<p>4.6.3 Multinomial Probability Law
</p>
<p>Consider an extension to the Bernoulli sequence in which the trials are still inde-
</p>
<p>pendent but the outcomes for each trial may take on more than two values. For
</p>
<p>example, let Sl = {I , 2, 3} and denote the probabilities of the outcomes 1, 2, and
3 by P1, P2, and P3, respectively. As usual, the assignment of these probabilities
</p>
<p>must satisfy :z=f=l Pi = 1. Also, let the number of trials be M = 6 so that a pos-
sible outcome might be (2,1,3,1 ,2,2), whose probability is P2P1P3P1P2P2 = PIP~P!,
</p>
<p>The multinomial probability law specifies the probability of obtaining k1 1's , k2
</p>
<p>2's , and k3 3's , where k1 + k2 + k3 = M = 6. In the current example, k1 = 2,
k2 = 3, and k3 = 1. Some outcomes with the same number of 1's , 2's ' , and 3's
</p>
<p>are (2,1 ,3,1 ,2,2) , (1,2,3,1 ,2,2) , (1,2 ,1 ,2,2,3), etc ., with each outcome having a
</p>
<p>probability of PIP~P!. The total number of these outcomes will be the total number
</p>
<p>of distinct 6-tuples that can be made with the numbers 1,1,2,2,2,3. If the numbers
</p>
<p>to be used were all different, then the total number of 6-tuples would be 6! , or all
</p>
<p>permutations. However , since they are not , some of thes e permutations will be the
</p>
<p>same. For example, we can arrange the 2's 3! ways and still have the same 6-tuple.
</p>
<p>Likewise , the l 's can be arranged 2! ways without changing the 6-tuple. As a result ,
</p>
<p>the total number of distin ct 6-tuples is
</p>
<p>6!
</p>
<p>2!3!l!
</p>
<p>which is called the multinomial coefficient. (See also Problem 4.36 for another way
</p>
<p>to derive this.) It is sometimes denoted by
</p>
<p>Finally, for our example the probability of the sequence exhibiting two 1's, three
</p>
<p>2's , and one 3 is
6! 2 3 1
</p>
<p>2!3!l!P1P2P3'
</p>
<p>This can be generalized to the case of M trials with N possible outcomes for each
</p>
<p>trial. The probability of k1 l 's , k2 2's ,..., kN N 's is
</p>
<p>k1 + k2 + ... + kN = M
</p>
<p>(4.19)</p>
<p/>
</div>
<div class="page"><p/>
<p>94 CHAPTER 4. CONDITIONAL PROBABILITY
</p>
<p>and where L ~l Pi = 1. This is te rmed the multinomial probability law. Note that if
N = 2, t hen it reduces to the binomi al law (see Problem 4.37) . An example follows .
</p>
<p>Example 4.9 - A version of scrabble
</p>
<p>A person chooses 9 letters at random from the English alphabet with replacement.
</p>
<p>What is the probability that she will be able to make the word "commit tee" ? Here
</p>
<p>we have t hat the outcome on each trial is one of 26 letters. To be able to make the
</p>
<p>word she needs kc = 1, ke = 2, ki = 1, km = 2, ko = 1, kt = 2, and kother = O. We
have denoted the outcomes as c, e, i, m , 0 , t, and "ot her" . "Other" represents the
rem aining 20 let ters so t hat N = 7. Thus, the probability is from (4.19)
</p>
<p>P[kc = l , ke = 2,ki = l ,km = 2,ko = l ,kt = 2, kother = 0] =
</p>
<p>(1 ,2 ,1 ,~ ,1 ,2 ,0) (2
16)9
</p>
<p>( ~ ~ ) O
since Pc = Pe = Pi = Pm = Po = Pt = 1/26 and Pother = 20/26 due to the assumption
of "at rando m" sampling and with replacement . This becomes
</p>
<p>P [kc = l , ke = 2, ki = l ,km = 2, ko = l ,kt = 2,kother = 0] =
</p>
<p>9! ( 1 ) 9 -9
112'1'211'2'01 26 = 8.35 x 10 .
</p>
<p>4.6.4 Nonindependent Subexperiments
</p>
<p>When the subexpe riment s are indep endent, the calcula t ion of probabilities can be
</p>
<p>greatly simplified . An event that can be wri t ten as A = Al n A2 n ... n AM can be
found via
</p>
<p>P[A] = P[AI]P[A2] &middot; &middot; &middot; P[AM]
</p>
<p>where each P[Ai] can be found by considering only the individual subexperiment.
</p>
<p>However , the assumption of indep endence can somet imes be unreasonable. In the
</p>
<p>ab sence of independence, the probability would be found by using the chain rule
</p>
<p>(see (4.10) for M = 3)
</p>
<p>P [A] = P [AM IAM- I , .. . ,AI ]P [AM -IIAM-2 , . .. , A I ] .. . P [A 2IA dP[AI ]. (4.20)
</p>
<p>Such would be the case if a Bernoulli sequence were composed of nonindependent
</p>
<p>trials as illustrated next.
</p>
<p>Example 4.10 - Dependent Bernoulli trials
</p>
<p>Assume that we have two coins. One is fair and the ot her is weighted to have
</p>
<p>a probability of heads of P =1= 1/2. We begin the experime nt by first choosing at
</p>
<p>random one of the two coins and then tossing it. If it comes up heads, we choose</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6. MULTIPLE EXPERIMENTS 95
</p>
<p>the fair coin to use on the next t rial. If it comes up tails, we choose the weighted
</p>
<p>coin to use on the next trial. We repeat this procedure for all the succeeding trials.
</p>
<p>One possible sequence of outcomes is shown in Figure 4.7a for the weighted coin
</p>
<p>having p = 1/4. Also shown is the case when p = 1/2 or a fair coin is always used ,
</p>
<p>heads :
1 I'IIII!I - - ~ ... - - - ...... -* ~ - -
</p>
<p>Q)
</p>
<p>s
8 : tails : :
~ 0 1I11111111111Iillofllllllllo~ ...............
</p>
<p>o
</p>
<p>. heads : .
1 ft+~+fIIo III !II I I - ' + + + H + - ' ~ - H -
</p>
<p>Q)
</p>
<p>S -
8 . : tails :
~ oj+lt- --tt++iH+ 11111:1 I ..11111111 ..... ~ : t + I . . . - t ~
o .
</p>
<p>o 20 40 60
Trial
</p>
<p>80 100 o 20 40 60
Trial
</p>
<p>80 100
</p>
<p>(a) M = 100, P = 0.25 (b) M = 100, p = 0.5
</p>
<p>Figure 4.7: Dependent Bernoulli sequence for different values of p .
</p>
<p>so that we are equally likely to observe a head or a tail on each trial. Note that in
</p>
<p>the case of p = 1/4 (see Figure 4.7a), if t he outcome is a tail on any trial, then we
use the weighted coin for the next t rial. Since the weight ed coin is biased towards
</p>
<p>producing a tail, we would expect to again see a tail, and so on. This accounts for
</p>
<p>the long run of tails observed. Clearl y, t he trials are not independent.
</p>
<p>o
If we think some more about the previous experiment, we realize that the depen-
</p>
<p>dency between trials is due only to the outcome of the (i - 1)st trial affect ing the
</p>
<p>outcome of the ith trial. In fact , once the coin has been chosen, the probabilities
</p>
<p>for the next trial are eit her prO] = P[l] = 1/2 if a head occurred on the pre-
vious trial or prO] = 3/4, P [1] = 1/4 if the previous trial produced a tail. The
previous outcome is called the state of the sequence. This behavior may be sum-
</p>
<p>marized by the state probability diagram shown in Figure 4.8. The probabilities
</p>
<p>shown are actually condit ional probabilities. For example, 3/4 is the probability
</p>
<p>P[tail on ith tossltail on i - 1st toss] = P[OIO], and similarly for the others. This
type of Bernoulli sequence, in which the probabilities for t rial i depend only on the
</p>
<p>outcome of the previous t rial, is called a M arkov sequence . Mathematically, the
</p>
<p>probability of the event Ai on the ith trial given all t he previous outcomes can be
writ ten as</p>
<p/>
</div>
<div class="page"><p/>
<p>96 CHAPTER 4. CONDITIONAL PROBABILITY
</p>
<p>1
4
</p>
<p>3
4
</p>
<p>1
2
</p>
<p>1
2
</p>
<p>Figure 4.8: Markov state probability diagram.
</p>
<p>Using this in (4.20) produces
</p>
<p>P[A] = P[AMIAM-1]P[AM-1IAM-2]&middot;&middot;&middot; P[A2\A1]P[A1]. (4.21)
</p>
<p>The conditional probabilities P[AiIAi-1] are called the state transition probabilities,
</p>
<p>and along with the initial probability P[A1l, the probability of any joint event can
be determined. For example, we might wish to determine the probability of N = 10
tails in succession or of the event A = {(O, 0, 0, 0, 0, 0, 0, 0, 0, On. If the weighted
coin was actually fair , then P[A] = (1/2)10 = 0.000976, but if p = 1/4, we have by
letting Ai = {O} for i = 1,2, ... ,10 in (4.21)
</p>
<p>PIA] ~ (fiPIAM,-l]) PIA,].
But P[AiIAi-1] = P[OIO] = P[tailslweighted coin] = 3/4 for i = 2,3, . . . , 10. Since
we initially choose one of the coins at random, we have
</p>
<p>P[A1] = prO] = P[taillweighted coin]P[weighted coin]
</p>
<p>+P[taillfair coin]P[fair coin]
</p>
<p>(~) (~) + (~) (~) =~.
Thus, we have that
</p>
<p>PIA] ~ (fi ~) m~ 0.0469
or about 48 times more probable than if the weighted coin were actually fair. Note
</p>
<p>that we could also represent the process by using a trellis diagram as shown in Figure
</p>
<p>4.9. The probability of any sequence is found by tracing the sequence values through
</p>
<p>the trellis and multiplying the probabilities for each branch together, along with the
</p>
<p>initial probability. Referring to Figure 4.9 the sequence 1,0,&deg;has a probability of
(3/8)(1/2)(3/4). The foregoing example is a simple case of a Markov chain. We will
</p>
<p>study this modeling in much more detail in Chapter 22.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7. REAL-WORLD EXAMPLE - CLUSTER RECOGNITION
</p>
<p>outcome 0
</p>
<p>choose coin --+
</p>
<p>and toss
</p>
<p>97
</p>
<p>1 1/2 2 1/2 3 i (toss number)
</p>
<p>Figure 4.9: Trellis diagram.
</p>
<p>4.7 Real-World Example - Cluster Recognition
</p>
<p>In many areas an important problem is the detection of a "cluster." Epidemiology
</p>
<p>is concerned with the incidence of a greater than expected number of disease cases
</p>
<p>in a given geographic area. If such a situation is found to exist, then it may indicate
</p>
<p>a problem with the local water supply, as an example. Police departments may wish
</p>
<p>to focus their resources on areas of a city that exhibit an unusually high incidence
</p>
<p>of crime. Portions of a remotely sensed image may exhibit an increased number of
</p>
<p>noise bursts. This could be due to a group of sensors that are driven by a faulty
</p>
<p>power source. In all these examples, we wish to determine if a cluster of events
</p>
<p>has occurred. By cluster, we mean that more occurrences of an event are observed
</p>
<p>than would normally be expected. An example could be a geographic area which
</p>
<p>is divided into a grid of 50 x 50 cells as shown in Figure 4.10. It is seen that
</p>
<p>an event or "hit", which is denoted by a black square, occurs rather infrequently.
</p>
<p>In this example, it occurs 29/2500 = 1.16% of the time. Now consider Figure
</p>
<p>4.11. We see that the shaded area appears to exhibit more hits than the expected
</p>
<p>145 x 0.0116 = 1.68 number. One might be inclined to call this shaded area a cluster.
</p>
<p>But how probable is this cluster? And how can we make a decision to either accept
</p>
<p>the hypothesis that this area is a cluster or to reject it? To arrive at a decision we
</p>
<p>use a Bayesian approach. It computes the odds ratio against the occurrence of a
</p>
<p>cluster (or in favor of no cluster), which is defined as
</p>
<p>odds = P[no clusterlobserved data] .
</p>
<p>P[clusterlobserved data]
</p>
<p>If this number is large, typically much greater than one, we would be inclined to
</p>
<p>reject the hypothesis of a cluster, and otherwise, to accept it. We can use Bayes' the-
</p>
<p>orem to evaluate the odds ratio by letting B = {cluster} and A = {observed data}.
Then,
</p>
<p>P[BCIA]
odds = P[BIA]
</p>
<p>P[AIBC]P[BC]
</p>
<p>P[AIB]P[B] .</p>
<p/>
</div>
<div class="page"><p/>
<p>98
</p>
<p>45
</p>
<p>40
</p>
<p>35
</p>
<p>30
</p>
<p>25
</p>
<p>20
</p>
<p>15
</p>
<p>10
</p>
<p>5
</p>
<p>CHAPTER 4. CONDITIONAL PROBABILITY
</p>
<p>5 10 15 20 25 30 35 40 45 50
</p>
<p>Figure 4.10: Geographic area with incidents shown as black squares - no cluster
</p>
<p>present.
</p>
<p>Note that P[A] is not needed since it cancel outs in the ratio. To evaluate this we
</p>
<p>need to determine P[B], P[AIB C ] , P[AIB]. The first probability P[B] is the prior
probability of a cluster. Since we believe a cluster is quite unlikely, we assign a
</p>
<p>probability of 10-6 to this. Next we need P[AIB C] or the probability of the observed
data if there is no cluster. Since each cell can take on only one of two values,
</p>
<p>either a hit or no hit, and if we assume that the outcomes of the various cells are
</p>
<p>independent of each other, we can model the data as a Bernoulli sequence. For this
</p>
<p>problem, we might be tempted to call it a Bernoulli array but the determination
</p>
<p>of the probabilities will of course proceed as usual. If M cells are contained in the
</p>
<p>supposed cluster area (shown as shaded in Figure 4.11 with M = 145), then the
probability of k hits is given by the binomial law
</p>
<p>Next must assign values to p under the hypothesis of a cluster present and no
</p>
<p>cluster present. From Figure 4.10 in which we did not suspect a cluster, the relative</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7. REAL-WORLD EXAMPLE - CLUSTER RECOGNITION
</p>
<p>50
</p>
<p>45
</p>
<p>40
</p>
<p>35
</p>
<p>30
</p>
<p>25
</p>
<p>20
</p>
<p>15
</p>
<p>10
</p>
<p>5
</p>
<p>5 10 15 20 25 30 35 40 45 50
</p>
<p>99
</p>
<p>Figure 4.11: Geographic area with incidents shown as black squares - possible cluster
</p>
<p>pr esent.
</p>
<p>frequency of hits was about 0.0116 so that we assume Pnc = 0.01 when there is
</p>
<p>no cluster. When we believe a cluster is present, we assume that Pc = 0.1 in
accordance wit h the relative frequency of hits in the shaded area of Figure 4.11,
</p>
<p>which is 11/145=0.07. Thus,
</p>
<p>P [AIBC] = P [observed datalno cluster] = ( ~) P~c(1 - Pnc)M-k
</p>
<p>P [k = Ll jno cluster] = (1:1
5
</p>
<p>) (0.01)11 (0.99)134
</p>
<p>P[AIB] P[observed datalcluster] = (~ ) p~(l - Pc)M-k
</p>
<p>P[k = Lljcluster] = ( 11
4
1
5)
</p>
<p>(0.1)11 (0.9)134
</p>
<p>which results in an odds ratio of
</p>
<p>(0.01)11(0.99)134(1 - 10- 6 )
odds = (0.1)11(0.9)134(10-6 ) = 3.52 .</p>
<p/>
</div>
<div class="page"><p/>
<p>100 CHAPTER 4. CONDITIONAL PROBABILITY
</p>
<p>Since the posterior probability of no cluster is 3.52 times larger than the posterior
</p>
<p>probability of a cluster, we would reject the hypothesis of a cluster present. However,
</p>
<p>the odds against a cluster being present are not overwhelming. In fact, the computer
</p>
<p>simulation used to generate Figures 4.11 employed p = 0.01 for the unshaded region
and p = 0.1 for the shaded cluster region. The reader should be aware that it is
</p>
<p>mainly the influence of the small prior probability of a cluster, P[B] = 10-6 , that
has resulted in the greater than unity odds ratio and a decision to reject the cluster
</p>
<p>present hypothesis.
</p>
<p>References
</p>
<p>S. Press, Subjective and Objective Bayesian Statistics, John Wiley &amp; Sons, New
</p>
<p>York, 2003.
</p>
<p>D. Salsburg, The Lady Tasting Tea: How Statistics Revolutionized Science in the
</p>
<p>Twentieth Century, W.H. Freeman, New York, 2001.
</p>
<p>Problems
</p>
<p>4.1 (f) If Be A, what is P[AIB]? Explain your answer.
</p>
<p>4.2 (...:..:...) (f) A point x is chosen at random within the interval (0,1). If it is known
</p>
<p>that x ~ 1/2, what is the probability that x ~ 7/S?
</p>
<p>4.3 (w) A coin is tossed three times with each 3-tuple outcome being equally likely.
Find the probability of obtaining (H,T, H) if it is known that the outcome
</p>
<p>has 2 heads. Do this by 1) using the idea of a reduced sample space and 2)
</p>
<p>using the definition of conditional probability.
</p>
<p>4.4 (w) Two dice are tossed. Each 2-tuple outcome is equally likely. Find the
</p>
<p>probability that the number that comes up on die 1 is the same as the number
</p>
<p>that comes up on die 2 if it is known that the sum of these numbers is even.
</p>
<p>4.5 (..:..:...) (f) An urn contains 3 red balls and 2 black balls. If two balls are chosen
</p>
<p>without replacement, find the probability that the second ball is black if it is
</p>
<p>known that the first ball chosen is black .
</p>
<p>4.6 (f) A coin is tossed 11 times in succession. Each 11-tuple outcome is equally
</p>
<p>likely to occur. If the first 10 tosses produced all heads, what is the probability
</p>
<p>that the 11t h toss will also be a head?
</p>
<p>4.7 (...:..:...) (w) Using Table 4.1, determine the probability that a college student will
</p>
<p>have a weight greater than 190 lbs. if he/she has a height exceeding 5'S". Next,
</p>
<p>find the probability that a student 's weight will exceed 190 lbs.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 101
</p>
<p>4.8 (w) Using Table 4.1 , find the probability that a student has weight less than
160 lbs . if he/she has height greater than 5' 4". Also, find the probability that
</p>
<p>a student 's weight is less than 160 lbs . if he/she has height less than 5' 4". Are
</p>
<p>these two results related?
</p>
<p>4.9 (t) Show that the statement P[AIB] +P[AIBC ] = 1 is false. Use Figure 4.2a to
provide a counterexample.
</p>
<p>4.10 (t) Prove that for the events A , B, G, which are not necessarily mutually ex-
</p>
<p>clusive,
</p>
<p>P[A U BIG] = P[AIG] + P[BIG] - p[ABle].
</p>
<p>4.11 c . : ~ . - &gt; (w) A group of 20 patients afflicted with a disease agree to be part of a
clinical drug trial. The group is divided up into two groups of 10 subjects each,
</p>
<p>with one group given the drug and the other group given sugar water, i.e., this
</p>
<p>is the control group. The drug is 80% effective in curing the disease. If one
</p>
<p>is not given the drug, there is still a 20% chance of a cure due to remission.
</p>
<p>What is the probability that a randomly selected subject will be cured?
</p>
<p>4.12 (w) A new bus runs on Sunday, Tuesday, Thursday, and Saturday while an
older bus runs on the other days. The new bus has a probability of being on
</p>
<p>time of 2/3 while the older bus has a probability of only 1/3. If a passenger
</p>
<p>chooses an arbitrary day of the week to ride the bus, what is the probability
</p>
<p>that the bus will be on time?
</p>
<p>4.13 (w) A digital communication system transmits one of the three values -1, 0, 1.
A channel adds noise to cause the decoder to sometimes make an error. The
</p>
<p>error rates are 12.5% if a -1 is transmitted, 75% if a 0 is transmitted, and
</p>
<p>12.5% if a 1 is transmitted. If the probabilities for the various symbols being
</p>
<p>transmitted are P[-1] = P[1] = 1/4 and prO] = 1/2, find the probability of
erro r. Repeat the problem if P[-l] = p rO] = P[1] and explain your results.
</p>
<p>4.14 C:.:,) (w) A sample space is given by S = {(x,y) : 0 ~ x ~ 1,0 ~ y ~ 1}.
Determine P[AIB] for the events
</p>
<p>A {(x, y) : y ~ 2x ,0 ~ x ~ 1/2 and y ~ 2 - 2x , 1/2 ~ x ~ 1}
</p>
<p>B {( x ,y):1/2~x~1 ,0~y~1}.
</p>
<p>Are A and B independent?
</p>
<p>4.15 (w) A sample space is given by S = {(x ,y) : 0 ~ x ~ 1,0 ~ y ~ 1}. Are the
events
</p>
<p>A {(x,y):y~x}
</p>
<p>B {(x,y):y~1-x}
</p>
<p>independent? Repeat if B = {(x ,y) : x ~ 1/4}.</p>
<p/>
</div>
<div class="page"><p/>
<p>102 CHAPTER 4. CONDITIONAL PROBABILITY
</p>
<p>4.16 (t) Give an example of two events that are mutually exclusive but not inde-
</p>
<p>pendent. Hint: See Figure 4.4.
</p>
<p>4.17(t) Consider the sample space 5 = {(x ,y ,z) : 0:::; x:::; 1,0:::; y:::; 1,0:::; z:::;
I} , which is the unit cube. Can you find three events that are independent?
</p>
<p>Hint: See Figure 4.2c.
</p>
<p>4.18 (t) Show that if (4.9) is satisfied for all possible events, then pairwise inde-
pendence follows. In this case all events are independent.
</p>
<p>4.19 C:...:....) (f) It is known that if it rains, there is a 50% chance that a sewer will
overflow. Also, if the sewer overflows, then there is a 30% chance that the road
</p>
<p>will flood. If there is a 20% chance that it will rain, what is the probability
</p>
<p>that the road will flood?
</p>
<p>4.20 (w) Consider the sample space 5 = {I, 2, 3, 4}. Each simple event is equally
</p>
<p>likely. If A = {I , 2}, B = {I , 3}, C = {I, 4}, are these events pairwise indepen-
dent? Are they independent?
</p>
<p>4.21 C.:..:J (w) In Example 4.6 determine if the events are pairwise independent.
Are they independent?
</p>
<p>4.22 C:..:....) (w) An urn contains 4 red balls and 2 black balls. Two balls are chosen
in succession without replacement. If it is known that the first ball drawn is
</p>
<p>black, what are the odds in favor of a red ball being chosen on the second
</p>
<p>draw?
</p>
<p>4.23 (w) In Example 4.7 plot the probability that the person has cancer given that
</p>
<p>the test results are positive, i.e., the posterior probability, as a function of the
</p>
<p>prior probability prE]. How is the posterior probability that the person has
cancer related to the prior probability?
</p>
<p>4.24 (w) An experiment consists of two subexperiments. First a number is chosen
</p>
<p>at random from the interval (0, 1). Then, a second number is chosen at random
</p>
<p>from the same interval. Determine the sample space 52 for the overall exper-
</p>
<p>iment. Next consider the event A = {(x, y) : 1/4 :::; x :::; 1/2,1/2 :::; Y :::; 3/4}
and find P[A]. Relate P[A] to the probabilities defined on 8 1 = {u : 0 &lt; u &lt;
I} , where 51 is the sample space for each subexperiment.
</p>
<p>4.25 (w ,c) A fair coin is tossed 10 times. What is the probability of a run of exactly
</p>
<p>5 heads in a row? Do not count runs of 6 or more heads in a row. Now verify
</p>
<p>your solution using a computer simulation.
</p>
<p>4.26 C:...:....) (w) A lady claims that she can tell whether a cup of tea containing
milk had the tea poured first or the milk poured first . To test her claim an
</p>
<p>experiment is set up whereby at random the milk or tea is added first to an</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 103
</p>
<p>empty cup. This experiment is repeated 10 times. If she correctly identifies
</p>
<p>which liquid was poured first 8 times out of 10, how likely is it that she is
</p>
<p>guessing? See [Salsburg 2001] for a further discussion of this famous problem.
</p>
<p>4.27 (f) The probability P[k] is given by the binomial law. If M = 10, for what
</p>
<p>value of pis P[3] maximum? Explain your answer.
</p>
<p>4.28 c.:..:..-) (f) A sequence of independent subexperiments is conducted. Each subex-
periment has the outcomes "success", "failure", or "don' t know". IfP[success] =
</p>
<p>1/2 and P[failure] = 1/4, what is the probability of 3 successes in 5 trials?
</p>
<p>4.29 (c) Verify your results in Problem 4.28 by using a computer simulation.
</p>
<p>4.30 (w) A drunk person wanders aimlessly along a path by going forward one step
</p>
<p>with probability 1/2 and going backward one step with probability 1/2. After
</p>
<p>10 steps what is the probability that he has moved 2 steps forward?
</p>
<p>4.31 (f) Prove that the geometric probability law (4.17) is a valid probability as-
signment.
</p>
<p>4.32 (w) For a sequence of independent Bernoulli trials find the probability of the
</p>
<p>first failure at the kth trial for k = 1,2, . . ..
</p>
<p>4.33 c.:..:J (w) For a sequence of independent Bernoulli trials find the probability
of the second success occurring at the kth trial.
</p>
<p>4.34 (t) Consider a sequence of independent Bernoulli trials. If it is known that
</p>
<p>the first m trials resulted in failures , prove that the probability of the first
</p>
<p>success occurring at m + l is given by the geometric law with k replaced by
l. In other words, the probability is the same as if we had started the process
</p>
<p>over again after the mth failure. There is no memory of the first m failures.
</p>
<p>4.35 (f) An urn contains red, black, and white balls. The proportion of red is 0.4,
</p>
<p>the proportion of black is 0.4, and the proportion of white is 0.2. If 5 balls
</p>
<p>are drawn with replacement, what is the probability of 2 red, 2 black, and 1
</p>
<p>white in any order?
</p>
<p>4.36 (t) We derive the multinomial coefficient for N = 3. This will yield the number
of ways that an M-tuple can be formed using k1 l's, k2 2's and k3 3's. To do
</p>
<p>so choose k1 places in the M-tuple for the l 's. There will be M - k1 positions
</p>
<p>remaining. Of these positions choose k2 places for the 2's. Fill in the remaining
</p>
<p>k3 = M - k1 - k2 positions using the 3's. Using this result , determine the
</p>
<p>number of different M digit sequences with k1 l 's, k2 2's, and k3 3's.
</p>
<p>4 .37 (t) Show that the multinomial probability law reduces to the binomial law for
N=2.</p>
<p/>
</div>
<div class="page"><p/>
<p>104 CHAPTER 4. CONDITIONAL PROBABILITY
</p>
<p>4.38 C.:..) (w,c) An urn contains 3 red balls, 3 black balls, and 3 white balls. If
6 balls are chosen with replacement, how many of each color is most likely?
</p>
<p>Hint: You will need a computer to evaluate the probabilities.
</p>
<p>4.39 (w,c) For the problem discussed in Example 4.10 change the probability of
heads for the weighted coin from p = 0.25 to p = 0.1. Redraw the Markov state
</p>
<p>probability diagram. Next, using a computer simulation generate a sequence
</p>
<p>of length 100. Explain your results.
</p>
<p>4.40 C..:..) (f) For the Markov state diagram shown in Figure 4.8 with an initial
state probability of prO] = 3/4, find the probability of the sequence 0,1,1, O.
</p>
<p>4.41 (f) A two-state Markov chain (see Figure 4.8) has the state transition probabil-
</p>
<p>ities P[OIO] = 1/4,P[011] = 3/4 and the initial state probability of prO] = 1/2.
What is the probability of the sequence 0,1,0,1 , O?
</p>
<p>4.42 (w) A digital communication system model is shown in Figure 4.12. It consists
of two sections with each one modeling a different portion of the communi-
</p>
<p>cation channel. What is the probability of a bit error? Compare this to the
</p>
<p>probability of error for the single section model shown in Figure 4.3, assuming
</p>
<p>that E&lt; 1/2, which is true in practice. Note that Figure 4.12 is a trellis.
</p>
<p>1-E 1-E
0
</p>
<p>2&lt;J8:
0
</p>
<p>Choose
</p>
<p>oor 1
1 1
</p>
<p>prO] = P[l] = 1/2 1-E 1-E
</p>
<p>Figure 4.12: Probabilistic model of a digital communication system with two sec-
</p>
<p>tions.
</p>
<p>4.43 t:..:..) (f) For the trellis shown in Figure 4.9 find the probability of the event
A = {(O, 1,0,0), (0,0,0, On.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 5
</p>
<p>Discrete Random Variables
</p>
<p>5.1 Introduction
</p>
<p>Having been introduced to the basic probabilistic concepts in Chapters 3 and 4,
</p>
<p>we now begin their application to solving problems of interest. To do so we define
</p>
<p>the random variable. It will be seen to be a function , also called a mapping, of the
</p>
<p>outcomes of a random experiment to the set of real numbers. With this association
</p>
<p>we are able to use the real number description to quantify items of interest. In
</p>
<p>this chapter we describe the discrete random variable, which is one that takes on
</p>
<p>a finit e or countably infinite number of values. Later we will extend the definition
</p>
<p>to a random variable that takes on a continuum of values , the continuous random
</p>
<p>vari able. The mathematics associated with a discrete random variable are inherently
</p>
<p>simpler and so conceptualization is facilitated by first concentrating on the discrete
</p>
<p>problem. The reader has already been introduced to the concept of a random
</p>
<p>vari able in Chapter 2 in an informal way and hence may wish to review the computer
</p>
<p>simulation methodology described therein.
</p>
<p>5.2 Summary
</p>
<p>The random variable, which is a mapping from the sample space into the set of
</p>
<p>real numbers, is formally discussed and illustrated in Section 5.3. In Section 5.4
</p>
<p>the probability of a random variable taking on its possible values is given by (5.2).
</p>
<p>Next the probability mass function is defined by (5.3). Some important probability
</p>
<p>mass functions are summarized in Section 5.5. They include the Bernoulli (5.5), the
</p>
<p>binomial (5.6) , the geometric (5.7) , and the Poisson (5.8). The binomial probability
</p>
<p>mass function can be approximated by the Poisson as shown in Figure 5.8 if M --+ 00
and p --+ 0, with M p remaining constant. This motivates the use of the Poisson
probability mass function for traffic modeling. If a random variable is transformed
</p>
<p>to a new one via a mapping, then the new random variable has a probability mass
</p>
<p>fun ction given by (5.9). Next the cumulative distribution function is introduced and</p>
<p/>
</div>
<div class="page"><p/>
<p>106 CHAPTER 5. DISCRETE RANDOM VARIABLES
</p>
<p>is given by (5.10). It can be used as an equivalent description for the probability
</p>
<p>of a discrete random variable. Its properties are summarized in Section 5.8. The
</p>
<p>computer simulation of discrete random variables is revisited in Section 5.9 with the
</p>
<p>estimate of the probability mass function and the cumulative distribution function
</p>
<p>given by (5.14) and (5.15),(5.16), respectively. Finally, the application of the Poisson
</p>
<p>probability model to determining the resources required to service customers is
</p>
<p>described in Section 5.10.
</p>
<p>5.3 Definition of Discrete Random Variable
</p>
<p>We have previously used a coin toss and a die toss as examples of a random ex-
</p>
<p>periment. In the case of a die toss the outcomes comprised the sample space
</p>
<p>S = {I , 2, 3, 4, 5, 6}. This was because each face of a die has a dot pattern con-
sisting of 1, 2, 3, 4, 5, or 6 dots. A natural description of the outcome of a die toss
</p>
<p>is therefore the number of dots observed on the face that appears upward. In effect,
</p>
<p>we have mapped the dot pattern into the numb er of dots in describing the outcome.
</p>
<p>This type of experiment is called a numerically valued random phenomenon since the
</p>
<p>basic output is a real number. In the case of a coin toss the outcomes comprise the
</p>
<p>nonnumerical sample space S = {head, tail}. We have, however , at times replaced
</p>
<p>the sample space by one consisting only ofreal numbers such as Sx = {O, I}, where
</p>
<p>a head is mapped into a 1 and a tail is mapped into a O. This mapping is shown
</p>
<p>in Figure 5.1. For many applications this is a convenient mapping. For example, in
</p>
<p>x
X 2 = 1
</p>
<p>Sx = {O, I}
</p>
<p>X l = 0
</p>
<p>X(5d
</p>
<p>S
</p>
<p>Figure 5.1: Mapping of the outcome of a coin toss into the set of real numbers.
</p>
<p>a succession of M coin tosses, we might be interested in the total number of heads
</p>
<p>observed. With the defined mapping of
</p>
<p>{
0 51 = tail
</p>
<p>X(5i) = 1 52 = head</p>
<p/>
</div>
<div class="page"><p/>
<p>5.3. DEFINITION OF DISCRETE RANDOM VARIABLE 107
</p>
<p>we could represent the number of heads as L~1 X(5i), where s, is the outcome of
the ith toss. The function that maps S into Sx and which is denoted by X(&middot;) is
called a random variable. It is a function that takes each outcome of S (which may
</p>
<p>not necessarily be a set of numbers) and maps it into the subset of the set of real
</p>
<p>numbers. Note that as previously mentioned in Chapter 2, a capital letter X will
</p>
<p>denote the random variable and a lowercase letter x its value. This convention for
</p>
<p>the coin toss example produces the assignment
</p>
<p>i = 1,2
</p>
<p>where 51 = tail and thus Xl = 0, and 52 = head and thus X2 = 1. The name
random variable is a poor one in that the function X (.) is not random but a known
</p>
<p>one and usually one of our own choosing. What is random is the input argument s,
and hence the output of the function is random. However, due to its long-standing
</p>
<p>usage in probability we will retain this terminology.
</p>
<p>Sometimes it is more convenient to use a particular random variable for a given
</p>
<p>experiment. For example, in Chapter 2 we described a digital communication system
</p>
<p>called a PSK system. A bit is communicated using the transmitted signals
</p>
<p>s(t) = { -Acos27fFot
A cos 27fFot
</p>
<p>for a 0
</p>
<p>for a 1.
</p>
<p>Usually a 1 or a 0 occurs with equal probability so that the choice of a bit can be
</p>
<p>modeled as the outcome of a fair coin tossing experiment. If a head is observed, then
</p>
<p>a 1 is transmitted and a 0 otherwise. As a result, we could represent the transmitted
</p>
<p>signal with the model
</p>
<p>where 51 = tail and 52 = head and hence we have the defined random variable
</p>
<p>X(5i) = {-1 51 = tail
+1 52 = head.
</p>
<p>This random variable is a convenient one for this application.
</p>
<p>In general, a random variable is a function that maps the sample space S into a
</p>
<p>subset of the real line. The real line will be denoted by R (R = {x : -00 &lt; x &lt; oo}).
For a discrete random variable this subset is a finite or countably infinite set of
</p>
<p>points. The subset forms a new sample space which we will denote by Sx , and
which is illustrated in Figure 5.2. A discrete random variable may also map multiple
</p>
<p>elements of the sample space into the same number as illustrated in Figure 5.3. An
</p>
<p>example would be a die toss experiment in which we were only interested in whether
</p>
<p>the outcome is even or odd. To quantify this outcome we could define
</p>
<p>X() {
0 if s, = 1,3,5 dots
</p>
<p>s, = 1
if s, = 2,4, 6 dots.</p>
<p/>
</div>
<div class="page"><p/>
<p>108 CHAPTER 5. DISCRETE RANDOM VARIABLES
</p>
<p>&bull;
</p>
<p>S2----;r-- _
</p>
<p>Sl_---,,I-----
</p>
<p>x
</p>
<p>Figure 5.2: Discrete random variable as a one-to-one mapping of a countably infinite
</p>
<p>sample space into set of real numbers.
</p>
<p>Sl_-.f-----
</p>
<p>x
</p>
<p>Figure 5.3: Discrete random variable as a many-to-one mapping of a countably
</p>
<p>infinite sample space into set of real numbers.
</p>
<p>This type of mapping is usually called a many-to-one mapping while the previous
</p>
<p>one is called a one-to-one mapping. Note that for a many-to-one mapping we cannot
</p>
<p>recover the outcome of S if we know the value of X(s). But as already explained,
</p>
<p>this is of little concern since we initially defined the random variable to output the
</p>
<p>item of interest. Lastly, for numerically valued random experiments in which s is
contained in R, we can still use the random variable approach if we define X (s) = s
</p>
<p>for all s. This allows the concept of a random variable to be used for all random
experiments, with either numerical or nonnumerical outputs.
</p>
<p>5.4 Probability of Discrete Random Variables
</p>
<p>We would next like to determine the probabilities of the random variable taking on
</p>
<p>its possible values. In other words , what is the probability P[X(sd = Xi] for each</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4. PROBABILITY OF DISCRETE RANDOM VARIABLES 109
</p>
<p>Xi E S X? Since the sample space S is discrete, the random variable can take on at
</p>
<p>most a count ably infinite number of values or X(Si) = Xi for i = 1,2, .. .. It should
be clear that if X (&middot; ) maps each Si into a different Xi (or X( &middot;) is one-to-one) , then
</p>
<p>because s, and Xi are just two different names for the same event
</p>
<p>(5.1)
</p>
<p>or we assign a probability to the value of the random variable equal to that of the
</p>
<p>simple event in S that yields that value. If, however , there are multiple outcomes
</p>
<p>in S t ha t map into the same value Xi (or X(&middot;) is many-to-one) then
</p>
<p>P[X (S) = Xi ] P[{Sj : X(Sj) = xd]
</p>
<p>L P[{Sj}]
{j :X(Sj)=Xi}
</p>
<p>(5.2)
</p>
<p>since the s1's are simple events in S and are therefore mutually exclusive. It is said
that the events {X = xd, defined on Sx , and {s, : X(Sj) = xd, defined on S, are
equivalent events. As such they are assigned the same probability. Not e that the
</p>
<p>probability assignment (5.2) subsumes that of (5.1) and that in either case we can
</p>
<p>summarize the probabilities that the random variable values take on by defining the
</p>
<p>probabilit y mass function (PMF) as
</p>
<p>(5.3)
</p>
<p>and use (5.2) to evaluate it from a knowledge of the mapping. It is important to
</p>
<p>observe that in the notation pX [X i] the subscript X refers to the random variable and
</p>
<p>also the [.] not ation is meant to remind the reader that the argument is a discrete
</p>
<p>one. Later , we will use (-) for continuous arguments. In summary, the probability
</p>
<p>mass fun ction is the probability that the random variable X takes on the value Xi
</p>
<p>for each possible Xi . An example follows.
</p>
<p>Example 5.1 - Coin toss - one-to-one mapping
</p>
<p>The experiment consists of a single coin toss with a probability of heads equal to
</p>
<p>p. The sample space is S = {head, tail} and we define the random variable as
</p>
<p>X(sd = {O s, = tail
1 s, = head.
</p>
<p>The PMF is therefore from from (5.3) and (5.1)
</p>
<p>PX [O] P[X(s) = 0] = 1 - P
px [1 ] P[X(s) = 1] = p.</p>
<p/>
</div>
<div class="page"><p/>
<p>110 CHAPTER 5. DISCRETE RANDOM VARIABLES
</p>
<p>Example 5.2 - Die toss - many-to-one mapping
</p>
<p>The experiment consists of a single fair die toss. Wi th a sample space of S =
</p>
<p>{I , 2, 3, 4,5, 6} and an int erest only in whether the outcome is even or odd we define
</p>
<p>the random variable
</p>
<p>X(sd = {O ~f ~ = 1,3,5
1 Ifz = 2,4,6.
</p>
<p>Thus, using (5.3) and (5.2) we have the PMF
</p>
<p>PX[O]
</p>
<p>px[l]
</p>
<p>3
P[X(s) = 0] = '"' P[{Sj}] = -
</p>
<p>.~ 6
3=1 ,3,5
</p>
<p>3
P[X(s) = 1] = L P[{S j}] = -.
</p>
<p>j=2,4,6 6
</p>
<p>c
The use of (5.2) may seem familiar and indeed it should. We have summed the
</p>
<p>probabilities of simple events in S to obtain the probability of an event in S using
</p>
<p>(3.10). Here, the event is just the subset of S for which X(s) = Xi holds. The
int roduction of a random variable has quantified the events of interest!
</p>
<p>Finally, because PMFs PX[Xi] are just new names for the probabilities P[X(s) =
Xi ] they must satisfy the usual properties:
</p>
<p>Property 5.1 - Range of values
</p>
<p>o
</p>
<p>Property 5.2 - Sum of values
</p>
<p>M
</p>
<p>LPX[Xi] = 1 if Sx consists of M outcomes
i=l
</p>
<p>00
</p>
<p>LPX[Xi] = 1 if Sx is countably infinite.
i= l
</p>
<p>o
We will frequently omit the S argument of X to write PX[Xi] = P[X = Xi ].
</p>
<p>Once the PMF has been specified all subsequent probability calculations can be
</p>
<p>based on it , without referring back to the original sample space S. Specifically, for
</p>
<p>an event A defined on SX the probability is given by
</p>
<p>P[X E A] = L PX[Xi].
{i :Xi EA}
</p>
<p>(5.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5. IMPORTANT PROBABILITY MASS FUNCTIONS 111
</p>
<p>An example follows.
</p>
<p>Example 5.3 - Calculating probabilities based on the PMF
</p>
<p>Consider a die whose sides have been labeled with two sides having 1 dot, two
</p>
<p>sides having 2 dots , and two sides having 3 dots. Hence, S = {51 , 52 , ... , 56} =
</p>
<p>{side 1, side 2, side 3, side 4, side 5, side 6}. Then if we are interested in the prob-
</p>
<p>abilities of the outcomes displaying either 1, 2, or 3 dots, we would define a random
</p>
<p>variable as
</p>
<p>{
</p>
<p>I i = 1,2
</p>
<p>X(5i) = 2 ~ = 3,4
3 2 = 5,6.
</p>
<p>It easily follows then that the PMF is from (5.2)
</p>
<p>1
px[l] = px[2] = px[3] = 3'
</p>
<p>Now assume we are interested in the probability that a 2 or 3 occurs or A = {2,3}.
</p>
<p>Then from (5.4) we have
</p>
<p>2
P[X E {2,3}] = px[2] +px[3] = 3'
</p>
<p>There is no need to reconsider the original sample space S and all probability cal-
</p>
<p>culations of interest are obtainable from the PMF.
</p>
<p>5.5 Important Probability Mass Functions
</p>
<p>We have already encountered many of these in Chapter 4. We now summarize these
</p>
<p>in our new notation. Since the sample spaces SX consist of integer values we will
</p>
<p>replace the notation Xi by k, which indicates an integer.
</p>
<p>5.5.1 Bernoulli
</p>
<p>[k] = { 1 - P k = 0
PX P k = 1.
</p>
<p>(5.5)
</p>
<p>The PMF is shown in Figure 5.4 and is recognized as a sequence of numbers that is
</p>
<p>nonzero only for the indices k = 0, 1. It is convenient to represent the Bernoulli PMF
</p>
<p>using the shorthand notation Ber(p). With this notation we replace the description
</p>
<p>that "X is distributed according to a Bernoulli random variable with PMF Ber(p)"
</p>
<p>by the shorthand notation X '" Ber(p), where r- means "is distributed according
</p>
<p>to" .</p>
<p/>
</div>
<div class="page"><p/>
<p>112 CHAPTER 5. DISCRETE RANDOM VARIABLES
</p>
<p>0.9
</p>
<p>0.8
</p>
<p>0.7
</p>
<p>~ O . 6 .
</p>
<p>:&gt;&lt;
~ 0 . 5 . . .
</p>
<p>0.4 ._- _ -.
</p>
<p>0.3
. .. . . .. .. . . ' .- .
</p>
<p>02 .
</p>
<p>5432
</p>
<p>0.1
</p>
<p>oL-__--l.-__----&lt;l&gt;-- --_---....
o
</p>
<p>k
</p>
<p>Figure 5.4: Bernoulli probability mass function for p = 0.25.
</p>
<p>5.5.2 Binomial
</p>
<p>px[k] = ( ~) pk( l _ p)M -k k = 0,1, .. . , M . (5.6)
</p>
<p>The PMF is shown in Figure 5.5. The shorthand notat ion for the binomial PMF is
</p>
<p>-. ..- . ..-
</p>
<p>'. -. -. ' . '
</p>
<p>l
-' : '.
</p>
<p>;
</p>
<p>0.3
</p>
<p>0.35
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>o
o 2 3 4 5 6 7 8 9 10
</p>
<p>k
</p>
<p>0.25
</p>
<p>~&gt;&lt; 0.2
~
</p>
<p>0.15
</p>
<p>Figure 5.5: Binomial probabili ty mass function for M = 10,p = 0.25.
</p>
<p>bin(M,p) . The location of the maximum of the PMF can be shown to be given by
</p>
<p>[(M + l)p], where [x] denotes the largest int eger less than or equal to x (see Problem
5.7).</p>
<p/>
</div>
<div class="page"><p/>
<p>5.6. A PPR OXIMATION OF BINO MIAL PMF BY POISSON PMF 113
</p>
<p>5.5.3 G eometric
</p>
<p>k = 1, 2, .. . . (5.7)
</p>
<p>The PMF is shown in Figure 5.6. T he shorthand notation for the geometric P MF
</p>
<p>is geom(p) .
</p>
<p>., ., .. . ,
</p>
<p>., - ... ., .,
</p>
<p>.. .. '"
</p>
<p>.. ' "
</p>
<p>r
.' ' "
</p>
<p>f T T ;o
o 2 3 4 5 6 7 8 9 10
</p>
<p>k
</p>
<p>0.6
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>~
:&gt;&lt;
&amp;:l., 0.3
</p>
<p>0.5
</p>
<p>Figure 5.6: Geometric probability mass funct ion for M = 10, p = 0.25.
</p>
<p>5.5.4 Poisson
</p>
<p>k = 0,1 ,2, ... (5.8)
</p>
<p>where A &gt; O. The PMF is shown in Figure 5.7 for several values of A. Note that
the maximum occurs at [A] (see Problem 5.11). The shorthand notation is Pois(A).
</p>
<p>5.6 Approximation of Binomial PMF by Poisson PMF
</p>
<p>The binomial and Poisson P MFs are related to each other under certain cond i-
</p>
<p>tions. This relationship helps to explain why the Poisson PMF is used in var ious
</p>
<p>applications, primarily traffic modeling as described further in Section 5.10. The re-
</p>
<p>lationship is as follows. If in a binomial PMF, we let M -r 00 as p -r 0 such that the
product A = Mp remains constant , then bin(M,p) -r PoisfX) . Not e that A = Mp
represents the expected or average number of successes in M Bernoulli trials (see
</p>
<p>Chapter 6 for definition of exp ectation). Hence, by keeping the average number of
</p>
<p>successes fixed but assuming more and more trials with smaller and smaller prob-
</p>
<p>ab ilities of success on each trial, we are led to a Poisson PMF. As an example, a
</p>
<p>comparison is shown in Figure 5.8 for M = 1O,p = 0.5 and M = 100,p = 0.05. This</p>
<p/>
</div>
<div class="page"><p/>
<p>114 CHAPTER 5. DISCRETE RANDOM VARIABLES
</p>
<p>0.5 ,---~--~--~--~-___, 0 .5r--~--~--~--.,--___,
</p>
<p>0.4 0.4
</p>
<p>~
~0 .3 .
</p>
<p>I;:l..
</p>
<p>~
~0 .3 .
</p>
<p>I;:l..
</p>
<p>0.2 . . 0.2 .
. .. . . . . . . . . . . : .
</p>
<p>0.1 . .
</p>
<p>"1T &bull;
2 4 6
</p>
<p>k
8 10
</p>
<p>0.1
</p>
<p>2 4
</p>
<p>k
6 8
</p>
<p>(a) oX = 2 (b) x= 5
</p>
<p>Figure 5.7: The Poisson probability mass function for different values of A.
</p>
<p>03 , - - - - - ~ - - - - ~ - - - _ _ _ , 0.3 ,-----~----~---__,
</p>
<p>0.25 . 0.25
</p>
<p>1510
k
</p>
<p>"
</p>
<p>5
</p>
<p>0.1
</p>
<p>~ 0.2 '
</p>
<p>~
1;:l..0.15
</p>
<p>1510
k
</p>
<p>5
</p>
<p>:1
</p>
<p>. :1. . . . . .
: , 1_ 'hlllomlai
, I
</p>
<p>. . . . . . . . . . . . . .
'I Poisson
</p>
<p>I , , I I
.... , . ,. , . I '
</p>
<p>I I , I
</p>
<p>" , 1 , 1, ', ', .
</p>
<p>, I I , I
</p>
<p>0.1
</p>
<p>0.05 .
</p>
<p>~ 02
</p>
<p>~
1;:l..0.15
</p>
<p>(a) M = 1O,p = 0.5 (b) M = 100,p = 0.05
</p>
<p>Figure 5.8: The Poisson approximation to the binomial probability mass function.
</p>
<p>result is primarily useful since Poisson PMFs are easier to manipulate and also arise
</p>
<p>in the modeling of point processes as described in Chapter 21.
</p>
<p>To make this connection we have for the binomial PMF with p = &gt;.jM -+ 0 as</p>
<p/>
</div>
<div class="page"><p/>
<p>5.7. TRANSFORMATION OF DISCRETE RANDOM VARIABLES 115
</p>
<p>M -t 00 (and &gt;. fixed)
</p>
<p>px [k] ( ~ ) pk(l _ p)M-k
</p>
<p>( M ~ ~ ) ! k ! ( ~ ) k (1- ~) M - k
(Mh &gt;.k (1 - &gt;.jM) M
</p>
<p>~ Mk (1 - &gt;./M)k
</p>
<p>&gt;.k (M)k (1 - &gt;./M)M
</p>
<p>k! M k (1 - &gt;./M)k .
</p>
<p>But for a fixed k, as M -t 00, we have that (M)k/Mk -t 1. Also, for a fixed k,
</p>
<p>(1 - &gt;./M)k -t 1 so that we need only find the limit of g(M) = (1 - &gt;.jM)M as
M -t 00. This is shown in Problem 5.15 to be exp(-&gt;') and therefore
</p>
<p>&gt;.k
px[k] -t k! exp(-&gt;.).
</p>
<p>Also, since the binomial PMF is defined for k = 0,1 , ... , M, as M -t 00 the limiting
</p>
<p>P MF is defined for k = 0,1 , .. .. This result can also be found using charac terist ic
funct ions as shown in Chapter 6.
</p>
<p>5.7 Transformation of Discrete Random Variables
</p>
<p>It is frequently of interest to be able to det ermine the PMF of a transform ed random
</p>
<p>variable. Mathematically, we desire the P MF of the new random variable Y = g(X ),
</p>
<p>where X is a discrete random vari able. For example, consider a die whose faces are
</p>
<p>labeled with the numbers 0,0,1 ,1 ,2,2. We wish to find the PMF of the number
</p>
<p>observed when the die is tossed, assuming all sides are equa lly likely to occur. If
</p>
<p>t he original sample space is composed of the possible cube sides that can occur, so
</p>
<p>that Sx = {I , 2, 3, 4, 5, 6}, then the transformation appears as shown in Figure 5.9.
</p>
<p>Specifically, we have that
</p>
<p>{
</p>
<p>YI = &deg; if x = X l = 1 or x = X2 = 2
Y = Y2 = 1 ~f x = X3 = 3 or x = X4 = 4
</p>
<p>Y3 = 2 If x = Xs = 5 or x = X6 = 6.
</p>
<p>Note that the transformation is many-to- one. Since events such as {y : Y = YI = O]
and {x : x = Xl = 1, x = X2 = 2}, for example, are equivalent, they should be
assigned the same probability. Thus, using the property that the events {X = Xi}
</p>
<p>are simple events defined on Sx , we have that
</p>
<p>{
</p>
<p>px [l] + px [2] = ! i = 1
</p>
<p>PY[Yi] = px[3] + px [4]= ! i = 2
px[5]+ px[6] =! i = 3.</p>
<p/>
</div>
<div class="page"><p/>
<p>116 CHAPTER 5. DISCRETE RANDOM VARIABLES
</p>
<p>11 22 3 4 5 6
</p>
<p>SX = cube sides
</p>
<p>x y
</p>
<p>SY = numbers on sides
0
</p>
<p>Figure 5.9: Transformation of discrete random variable.
</p>
<p>In general, we have that
</p>
<p>pY [yi] =
&sum;
</p>
<p>{j:g(xj)=yi}
pX [xj]. (5.9)
</p>
<p>We just sum up the probabilities for all the values of X = xj that are mapped
into Y = yi. This is reminiscent of (5.2) in which the transformation was from
the objects Sj defined on S to the numbers xi defined on SX . In fact, it is nearly
identical except that we have replaced the objects that are to be transformed by
numbers, i.e., the xj&rsquo;s. Some examples of this procedure follow.
</p>
<p>Example 5.4 &ndash; One-to-one transformation of Bernoulli random variable
</p>
<p>If X &sim; Ber(p) and Y = 2X &minus; 1, determine the PMF of Y . The sample space for X
is SX = {0, 1} and consequently that for Y is SY = {&minus;1, 1}. It follows that x1 = 0
maps into y1 = &minus;1 and x2 = 1 maps into y2 = 1. As a result, we have from (5.9)
</p>
<p>pY [&minus;1] = pX [0] = 1&minus; p
pY [1] = pX [1] = p.
</p>
<p>Note that this mapping is particularly simple since it is one-to-one. A slightly more
complicated example is next.
</p>
<p>&diams;
</p>
<p>Example 5.5 &ndash; Many-to-one transformation
</p>
<p>Let the transformation be Y = g(X) = X2 which is defined on the sample space
SX = {&minus;1, 0, 1} so that SY = {0, 1}. Clearly, g(xj) = x2j = 0 only for xj = 0.
Hence,
</p>
<p>pY [0] = pX [0].
</p>
<p>However, g(xj) = x
2
j = 1 for xj = &minus;1 and xj = 1. Thus, using (5.9) we have
</p>
<p>pY [1] =
&sum;
</p>
<p>{xj :x2j=1}
pX [xj ]
</p>
<p>= pX [&minus;1] + pX [1].</p>
<p/>
</div>
<div class="page"><p/>
<p>5.8. CUMULATIVE DISTRIBUTION FUNCTION 117
</p>
<p>Note that we have determined PY[Yi] by summing the probabilities of all the xi's
that map into Yi via the transformation Y = g(x). This is in essence the meaning of
(5.9) .
</p>
<p>Example 5.6 - Many-to-one transformation of Poisson random variable
</p>
<p>Now consider X "" Pois(A) and define the transformation Y = g(X) as
</p>
<p>Y={
To find the PMF for Y we use
</p>
<p>1 if X = k is even
</p>
<p>-1 if X = k is odd.
</p>
<p>y[k] = pry = k] = { P[X ~ s even] k = 1
P P[X IS odd] k = -1.
</p>
<p>We need only determine py[l] since py[-I] = 1 - py[I] . Thus, from (5.9)
</p>
<p>py[l] =
00
</p>
<p>L px[j]
j=O and even
</p>
<p>00 Aj
L exp(-A)---:-;-.
</p>
<p>j=O and even J.
</p>
<p>To evaluate the infinite sum in closed form we use the following "trick"
</p>
<p>00
AjL .,
</p>
<p>j=O and even J .
</p>
<p>~~Aj ~~(-A)j
2 LJ ., + 2 LJ .,
</p>
<p>j=O J. j=O J.
</p>
<p>1 1
2exp(X] + 2exp( -A)
</p>
<p>since the Taylor expansion of exp(x) is known to be "f:.f=o x j fj! (see Problem 5.22).
Finally, we have that
</p>
<p>py[l]
</p>
<p>py[-I]
</p>
<p>[
1 1 ] 1exp(-A) 2 exp(A) + 2 exp( -A) = 2(1 + exp( -2A))
</p>
<p>1
1 - py[l] = 2(1- exp(-2A)).
</p>
<p>5.8 Cumulative Distribution Function
</p>
<p>An alternative means of summarizing the probabilities of a discrete random variable
</p>
<p>is the cumulative distribution function (CDF). It is sometimes referred to more</p>
<p/>
</div>
<div class="page"><p/>
<p>118 CHAPTER 5. DISCRETE RANDOM VARIABLES
</p>
<p>succinctly as the distribution function. The CDF for a random variable X and
</p>
<p>evaluated at x is given by P[{real numbers x' : x' ~ x}], which is the probability
</p>
<p>that X lies in the semi-infinite interval (-00, x]. It is therefore defined as
</p>
<p>Fx(x) = P[X ~ x] - 00 &lt; x &lt; 00. (5.10)
</p>
<p>It is important to observe that the value X = x is included in the interval. As an
</p>
<p>example, if X '" Ber(p) , then the PMF and the corresponding CDF are shown in
</p>
<p>Figure 5.10. Because the random variable takes on only the values 0 and 1, the CDF
</p>
<p>12 . - - - ~ ~ - ~ - ~ - - ~ - - - , 12 r - - ~ ~ - ~ - ~ - - ~ - - - - - '
</p>
<p>02 .
</p>
<p>0.4 - - .
</p>
<p>~OB
</p>
<p>~
~0.6
</p>
<p>.. .1.~.p.
</p>
<p>1 . . . . .
</p>
<p>0.4
</p>
<p>02
</p>
<p>1 : 1
i :p
</p>
<p>m&middot; I ..... ..&bull;&bull;&bull;&bull;&bull;&bull;&bull;&bull;&bull;&bull;&bull;&bull;&bull; &bull;&bull;&bull;&bull;
u ~ v .
&middot;1&middot;&middot; &middot;&middot;&middot; &middot;&middot; &middot; &middot; &middot;&middot; .
</p>
<p>I
.. j . .
</p>
<p>1
</p>
<p>o 2
k
</p>
<p>(a) PMF
</p>
<p>4 6 8
x
</p>
<p>(b) CDF
</p>
<p>Figure 5.10: The Bernoulli probability mass function and cumulative distribution
</p>
<p>function for p = 0.25.
</p>
<p>changes its value only at these points, where it jumps. The CDF can be thought of
</p>
<p>as a "running sum" which adds up the probabilities of the PMF starting at -00 and
</p>
<p>ending at +00. When the value x of Fx(x) encounters a nonzero value of the PMF,
</p>
<p>the additional mass causes the CDF to jump, with the size of the jump equal to the
</p>
<p>value of the PMF at that point. For example, referring to Figure 5.10b, at x = 0 we
</p>
<p>have Fx(O) = px[O] = I-p = 3/4 and at x = 1 we have Fx(l) = px[O] +px[l] = 1,
with the jump having size px[l] = p = 1/4. Another example follows.
</p>
<p>Example 5.7 - CDF for geometric random variable
</p>
<p>Since px[k] = (1- p)k-lp for k = 1,2, ... , we have the CDF
</p>
<p>{
</p>
<p>0
[z]
</p>
<p>Fx(x) = ~(1 _ p)i-lp
</p>
<p>x&lt;l
</p>
<p>x"21</p>
<p/>
</div>
<div class="page"><p/>
<p>5.8. CUMULATIVE DISTRIBUTION FUNCTION 119
</p>
<p>where [x] denotes the largest int eger less than or equal to x. This evaluates to
</p>
<p>{
</p>
<p>0
</p>
<p>F x - p
x() - p+ (l-p )p
</p>
<p>etc .
</p>
<p>x &lt; l
</p>
<p>1 :Sx &lt; 2
</p>
<p>2:S x&lt;3
</p>
<p>The PMF and CDF are plot ted in Figure 5.11 for p = 0.5. Since the CDF jumps at
</p>
<p>0.4 , .
</p>
<p>7 8602345
X
</p>
<p>0.2
</p>
<p>1.2 r-~~~-~----,-~-..,-----,---:--'
</p>
<p>1 . . .. . O.937~ &middot; &middot; &middot; &middot;
</p>
<p>: ~ ~
~0 .8 . . . f"75! .. . &bull;...... ..
</p>
<p>~ 0.6 &middot;&middot;&middot;:&middot;&middot;&middot;&middot;&middot;:&middot; &middot; &middot; &middot; 1
</p>
<p>f-5 I
0.4 . . .. &bull; ... . "&middot; 1 ..
</p>
<p>1
.:. . . . !..
</p>
<p>I
</p>
<p>0.25
</p>
<p>l O'y 2~ .~625. ~
2 3 4 5 6 7 8
</p>
<p>k
</p>
<p>... .. . ... &middot;&middot; 0.5
</p>
<p>0.2
</p>
<p>~O .8
</p>
<p>~
~0 .6
</p>
<p>(a) PM F (b) CDF
</p>
<p>Figure 5.11: The geometric probability mass fun ction and cumulative distribution
</p>
<p>funct ion for p = 0.5.
</p>
<p>each nonzero value of the PMF and the jump size is that value of the PMF, we can
</p>
<p>recover the PMF from the CDF. In particular, we have that
</p>
<p>where x + denotes a value just slight ly larger than x and x" denotes a value just
</p>
<p>slightly smaller than x . Thus, if Fx(x) does not have a discontinuity at x the
</p>
<p>value of the PMF is zero. At a discontinuity the value of the PMF is just the
</p>
<p>jump size as previously asserted. Also, becaus e of the definition of the CDF, i.e.,
</p>
<p>that Fx(x ) = P[X :s x] = P[X &lt; x or X = x ], the value of Fx(x) is the value
after the jump. The CDF is said to be right-continuous which is sometimes stated
</p>
<p>mathematically as limx-tx+ Fx(x) = Fx(xo) at the point x = xo.
o
</p>
<p>o
From the previous example we see that the PMF and CDF are equivalent descrip-
</p>
<p>t ions of the probability assignment for X. Either one can be used to find the
</p>
<p>pro bability of X being in an interval (even an interval of length zero). For example,</p>
<p/>
</div>
<div class="page"><p/>
<p>120 CHAPTER 5. DISCRETE RANDOM VARIABLES
</p>
<p>to determine P [3/2 &lt; X ::; 7/2] for the geometric random variable
</p>
<p>P [~ &lt; X ::;~] = px[2] +px[3]
</p>
<p>Fx ( ~ ) - Fx ( ~ )
</p>
<p>as is evident by referring to Figure 5.11b. We need to be careful, however, to
</p>
<p>note whether the endpoints of the interval are included or not. This is due to the
</p>
<p>discontinuities of the CDF. Because of the definition of the CDF as the probability
</p>
<p>of X being within the interval (-00, x], which includes the right-most point , we have
for the interval (a, b]
</p>
<p>(5.11)
</p>
<p>Also, the other intervals (a, b), [a, b) , and [a ,b] will in general have different prob-
abilities than that given by (5.11). From Figure 5.11b and (5.11) we have as an
</p>
<p>example that
</p>
<p>but
</p>
<p>From the definition of the CDF and as further illustrated in Figures 5.10 and
</p>
<p>5.11 the CDF has several important properties. They are now listed and proven.
</p>
<p>Property 5.3 - CDF is between 0 and 1.
</p>
<p>0::; Fx(x) ::; 1 -oo&lt; x&lt;oo
</p>
<p>Proof: Since by definition Fx(x) = P[X ::; x] is a probability for all x, it must lie
between 0 and 1.
</p>
<p>o
</p>
<p>Property 5.4 - Limits of CDF as x -7 -00 and as x -7 00
</p>
<p>lim Fx(x) 0
x-t-oo
</p>
<p>lim Fx(x) = l.
x-t+oo
</p>
<p>Proof:
</p>
<p>lim Fx(x) = P[{s: X(s) &lt; -oo}] = P[0] = 0
x-t -oo</p>
<p/>
</div>
<div class="page"><p/>
<p>5.8. CUMULATIVE DISTRIBUTI ON F UNCTI ON
</p>
<p>since the values that X(s) can take on do not include -00. Also,
</p>
<p>lim Fx(x) = P[{s : X(s) &lt; + oo}] = P[S ] = 1
x -++oo
</p>
<p>since the values that X(s) can take on are all included on the real line.
</p>
<p>121
</p>
<p>o
</p>
<p>Property 5.5 - CDF is m onot onically increasing.
</p>
<p>A monotonically increasing function g(.) is one in which for every X l and X2 with
</p>
<p>X l ~ X2, it follows that g(xd ~ g(X2) or the function increases or stays the same as
</p>
<p>the argument increases (see also Problem 5.29).
</p>
<p>Proof:
</p>
<p>P[ X ~ X2 ]
</p>
<p>P[(X ~ X l ) U (X l &lt; X ~ X2) ]
</p>
<p>P [X ~ Xl] + P[XI &lt; X ~ X2]
Fx(xd + P[ XI &lt; X ~ X2] ~ Fx(xd.
</p>
<p>(definition)
</p>
<p>(Axiom 3)
</p>
<p>(definition and Axiom 1)
</p>
<p>Alternatively, if A = {-oo &lt; X ~ X l } and B = {-oo &lt; X ~ X2} with X l ~ X2,
then A c B. From Property 3.5 (montonicity) FX( X2) = P[B] ~ P [A] = Fx(xd .
</p>
<p>o
</p>
<p>P rop er t y 5.6 - CDF is r ight -continuou s.
</p>
<p>By right-continuous it is meant that as we approach the point Xo from the right,
</p>
<p>the limiting value of the CDF should be the value of the CDF at that point. Math-
</p>
<p>ematically, it is expressed as
</p>
<p>lim Fx(x) = Fx(xo).
x -+xci
</p>
<p>Proof:
</p>
<p>The proof relies on the continuity property of the probability function . It can be
</p>
<p>found in [Ross 2002].
</p>
<p>o
</p>
<p>P roperty 5.7 - Probabilit y of interval found using t he CDF
</p>
<p>P[a &lt; X ~ b] = Fx(b) - Fx(a)
</p>
<p>or more explicitly to remind us of possible discontinuities
</p>
<p>(5.12)
</p>
<p>(5.13)</p>
<p/>
</div>
<div class="page"><p/>
<p>122 CHAPTER 5. DISCRETE RANDOM VARIABLES
</p>
<p>Proof:
</p>
<p>Since for a &lt; b
</p>
<p>{-(X) &lt; X ~ b} = {-(X) &lt; X ~ a} U {a &lt; X ~ b}
</p>
<p>and the intervals on the right-hand-side are disjoint (mutually exclusive events) , by
</p>
<p>Axiom 3
</p>
<p>P[-00 &lt; X ~ b] = P[-00 &lt; X ~ a] + P[a &lt; X ~ b]
</p>
<p>or rearranging terms we have that
</p>
<p>P[a &lt; X ~ b] = P[-oo &lt; X ~ b] - P[-oo &lt; X ~ a] = Fx{b) - Fx{a).
</p>
<p>o
</p>
<p>5.9 Computer Simulation
</p>
<p>In Chapter 2 we discussed how to simulate a discrete random variable on a digital
</p>
<p>computer. In particular, Section 2.4 presented some MATLAB code. We now
</p>
<p>continue that discussion to show how to simulate a discrete random variable and
</p>
<p>estimate its PMF and CDF. Assume that X can take on values in Sx = {I, 2,3}
with a PMF
</p>
<p>{
</p>
<p>P I = 0.2 if x = Xl = 1
px[x] = P2 = 0.6 if x = X2 = 2
</p>
<p>P3 = 0.2 if x = X3 = 3.
</p>
<p>The PMF and CDF are shown in Figure 5.12. The code from Section 2.4 for gener-
</p>
<p>ating M realizations of X is
</p>
<p>for i=1:M
</p>
<p>u=randO, 1) ;
</p>
<p>if u&lt;=O.2
</p>
<p>xCi,0=1;
</p>
<p>elseif u&gt;O.2 &amp;u&lt;=O.8
xCi,1)=2;
</p>
<p>elseif u&gt;O.8
</p>
<p>xCi ,0=3;
</p>
<p>end
</p>
<p>end
</p>
<p>(5.14)k = 1,2,3.
</p>
<p>Recall that U is a random variable whose values are equally likely to fall within the
</p>
<p>interval (0,1). It is called the uniform random variable and is described further in
</p>
<p>Chapter 10. Now to estimate the PMF px[k] = P[X = k] for k = 1,2,3 we use the
relative frequency interpretation of probability to yield
</p>
<p>px[k] = Number of outcomes equal to k
M</p>
<p/>
</div>
<div class="page"><p/>
<p>5.9. COMPUTER SIMULATION 123
</p>
<p>1.2 .-----~-~-~--.----~----, 1.2 .---~-~-~--.----~----,
</p>
<p>1 .. . 1 &middot; &middot; &middot; ~
</p>
<p>1
</p>
<p>~0 .8
</p>
<p>&gt;&lt;
&gt;:l..0.6
</p>
<p>432
</p>
<p>. . ~ .
</p>
<p>i
''' 1 .
</p>
<p>1
. . .. . j ...
</p>
<p>I
</p>
<p>. ... . . ~ ... ..
</p>
<p>I
</p>
<p>o
o'--_~_~_---L __ ' - _ ~ _ - '
- 2 -1
</p>
<p>0.2 . .
</p>
<p>0.4 ..
</p>
<p>H O.8 . . . .
"-'"
</p>
<p>&gt;&lt;
~0 .6 .
</p>
<p>3 4
</p>
<p>"' 1'"
1 2
k
</p>
<p>"1""
o-1
</p>
<p>0.2 . ..
</p>
<p>0.4
</p>
<p>(a) PMF (b) CDF
</p>
<p>Figure 5.12: The probability mass function and cumulative distribution fun ction for
</p>
<p>compute r simulation example.
</p>
<p>For M = 100 this is shown in Figure 5.13a. Also, the CDF is estimated for all x via
</p>
<p>Fx(x) = Number of outcomes ~ x
M
</p>
<p>(5.15)
</p>
<p>or equivalently by
</p>
<p>(5.16)Fx (x ) = L px[k]
{k :k:Sx}
</p>
<p>and is shown in Figure 5.13b. For finite sample spaces this approach to simulate
</p>
<p>a discret e random variable is adequate . But for infinite sample spaces such as for
</p>
<p>the geomet ric and Poisson random variables a different approach is needed. See
</p>
<p>Problem 5.30 for a fur ther discussion.
</p>
<p>Before concluding our discussion we wish to point out a useful property of CDFs
</p>
<p>that simplifies the computer generation of random variable outcomes. Not e from
</p>
<p>Figure 5.12b with u = Fx(x) that we can define an inverse CDF as x = FX1(u)
</p>
<p>where
</p>
<p>{
</p>
<p>1 if 0 &lt; u &lt; 0.2
</p>
<p>x = FX1(u) = 2 ifO.2&lt;u~0.8
</p>
<p>3 if 0.8 &lt; u &lt; 1
</p>
<p>or we choose the value of x as shown in Figure 5.14. But if u is the outcome
</p>
<p>of a uniform random variable U on (0, 1), then this procedure is identi cal to that
</p>
<p>implemented in the previous MAT LAB program used to generate realizations of
</p>
<p>X. A more general program is given in App endix 6B as PMFdata.m. This is not
</p>
<p>merely a coincidence but can be shown to follow from the definition of the CDF.</p>
<p/>
</div>
<div class="page"><p/>
<p>124 CHA PTER 5. DISCRETE RANDOM VARIABLES
</p>
<p>1 . 2 r--~-~- ~--~ -~--, 1.2 r- -~-~- ----, -- "- - -'- - --'
</p>
<p>432
</p>
<p>i...........r---
0.76 !
</p>
<p>.. . . .... ~ .
</p>
<p>o
</p>
<p>...... .. ......!. : .
I :
</p>
<p>I :
. , : .
0.17 I
</p>
<p>. ~ : .
</p>
<p>I . :
</p>
<p>-1
</p>
<p>(b) CDF
</p>
<p>o '---~-~_----I._-'---~-----'
-2
</p>
<p>1 . .
</p>
<p>0.4
</p>
<p>0.2
</p>
<p>~ O. 8 .
......... .
~ :
</p>
<p>&lt;J:;r, 0 .6 :
</p>
<p>. . . . . . . . . .. .. . - . .
</p>
<p>0.59
</p>
<p>0.17
0.24
</p>
<p>. . ., .&middot;1
0 1 2 3 4
</p>
<p>k
</p>
<p>(a) PMF
</p>
<p>-1
</p>
<p>1 . .
</p>
<p>0.2 . ..
</p>
<p>0.4 ...
</p>
<p>~ 0 . 8
</p>
<p>~
&lt; ~ 0 . 6
</p>
<p>Figure 5.13: The est imated pro bability mass function and corresponding estimated
</p>
<p>cumulat ive distribut ion function .
</p>
<p>12 r- -~ -~-~--~-~--,
</p>
<p>. . .. ~ .
</p>
<p>i
. . I &middot; .
</p>
<p>"::Q ...&bull;&bull;.............
</p>
<p>0.4 fS&middot;:!
02KJ&middot; &middot; &middot; &middot; &middot; : ~
</p>
<p>o 1
X
</p>
<p>2 3 4
</p>
<p>Figure 5.14: Relationship of inverse CDF to generation of discret e random variable.
</p>
<p>Value of u is mapped into value of x.
</p>
<p>Alt hough little more than a cur iousity now, it will become important when we
</p>
<p>simulate cont inuous random variables in Chap ter 10.
</p>
<p>5.10 Real-World Example - Servicing Customers
</p>
<p>A standard problem in many disciplines is the allocation of resources to service
</p>
<p>customers . It occurs in determining the number of cash iers needed at a store,
</p>
<p>the computer capacity needed to service download requests, and the amount of
</p>
<p>equipment necessary to service phone customers , as examples. In order to service</p>
<p/>
</div>
<div class="page"><p/>
<p>5.10. REAL-WORLD EXAMPLE - SERVICING CUSTOMERS 125
</p>
<p>these customers in a timely manner, it is necessary to know the distribution of
</p>
<p>arrival times of their requests. Since this will vary depending on many factors
</p>
<p>such as time of day, popularity of a file request, etc. , the best we can hope for is
</p>
<p>a determination of the probabilities of these arrivals. As we will see shortly, the
</p>
<p>Poisson probability PMF is particularly suitable as a model. We now focus on the
</p>
<p>problem of determining the number of cashiers needed in a supermarket.
</p>
<p>A supermarket has one express lane open from 5 to 6 PM on weekdays (Monday
</p>
<p>through Friday). This time of the day is usually the busiest since people tend to
</p>
<p>stop on their way home from work to buy groceries. The number of items allowed in
</p>
<p>the express lane is limited to 10 so that the average time to process an order is fairly
</p>
<p>constant at about 1 minute. The manager of the supermarket notices that there is
</p>
<p>frequently a long line of people waiting and hears customers grumbling about the
</p>
<p>wait. To improve the situation he decides to open additional express lanes during
</p>
<p>this time period. If he does, however, he will have to "pull" workers from other jobs
</p>
<p>around the store to serve as cashiers. Hence, he is reluctant to open more lanes than
</p>
<p>necessary. He hires Professor Poisson to study the problem and tell him how many
</p>
<p>lanes should be opened. The manager tells Professor Poisson that there should be
</p>
<p>no more than one person waiting in line 95% of the time. Since the processing time
</p>
<p>is 1 minute, there can be at most two arrivals in each time slot of 1 minute length.
</p>
<p>He reasons that one will be immediately serviced and the other will only have to
</p>
<p>wait a maximum of 1 minute. After a week of careful study, Professor Poisson tells
</p>
<p>the manager to open two lanes from 5 to 6 PM. Here is his reasoning.
</p>
<p>First Professor Poisson observes the arrivals of customers in the express lane
</p>
<p>on a Monday from 5 to 6 PM. The observed arrivals are shown in Figure 5.15,
</p>
<p>where the arrival times are measured in seconds. On Monday there are a total of
</p>
<p>+ + ++++++ : ++ +.f+ .. II( 111111111. +fifo ++tw-* :.,
</p>
<p>o 500 1000 1500 2000 2500 3000 3500
Time (sec)
</p>
<p>Figure 5.15: Arrival times at one express lane on Monday (a '+' indicates an arrival).</p>
<p/>
</div>
<div class="page"><p/>
<p>126 CHAPTER 5. DISCRETE RANDOM VARIABLES
</p>
<p>80 arrivals. He repeats his experiment on the following 4 days (Tuesday through
</p>
<p>Friday) and notes total arrivals of 68, 70, 59, and 66 customers, respectively. On
</p>
<p>the average there are 68.6 arrivals, which he rounds up to 70. Thus, the arrival rate
</p>
<p>is 1.167 customers per minute. He then likens the arrival process to one in which
</p>
<p>the 5 to 6 PM time interval is broken up into 3600 time slots of 1 second each . He
</p>
<p>reasons that there is at most 1 arrival in a given time slot and there may be no
</p>
<p>arrivals in that time slot. (This of course would not be valid if for instance, two
</p>
<p>friends did their shopping together and arrived at the same time.) Hence, Professor
</p>
<p>Poisson reasons that a good arrival model is a sequence of independent Bernoulli
</p>
<p>trials, where 0 indicates no arrival and 1 indicates an arrival in each l-second time
</p>
<p>slot. The probability P of a 1 is estimated from his observed data as the number
</p>
<p>of arrivals from 5 to 6 PM divided by the total number of time slots in seconds.
</p>
<p>This yields p = 70/3600 = 0.0194 for each I-second time slot. Instead of using the
binomial PMF to describe the number of arrivals in each l-minute time slot (for
</p>
<p>which P = 0.0194 and M = 60), he decides to approximate it using his favorite
</p>
<p>PMF, the Poisson model. Therefore, the probability of k arrivals (or successes) in
</p>
<p>a time interval of 60 seconds would be
</p>
<p>k = 0,1 , . . . (5.17)
</p>
<p>where the subscripts on X and A are meant to remind us that we will initially
</p>
<p>consider the arrivals at one express lane. The value of Al to be used is Al = Mp,
</p>
<p>which is estimated as '\1 = Mp = 60(70/3600) = 7/6. This represents the expected
number of customers arriving in the I-minute interval. According to the manager's
</p>
<p>requirements, within this time interval there should be at most 2 customers arriving
</p>
<p>95% of the time. Hence , we require that
</p>
<p>2
</p>
<p>P[X1 :S 2] = LPXl [k] 2: 0.95.
k=O
</p>
<p>But from (5.17) this becomes
</p>
<p>P[X1 :S 2] = exp(-Ad (1 + Al + ~Ai) = 0.88
</p>
<p>using Al = 7/6. Hence, the probability of 2 or fewer customers arriving at the
</p>
<p>express lane is not greater than 0.95. If a second express lane is opened, then the
</p>
<p>average number of arrivals at each lane during the I-minute time interval will be
</p>
<p>halved to 35. Therefore, the Poisson PMF for the number of arrivals at each lane
</p>
<p>will be characterized by A2 = 7/12. Now, however, there are two lanes and two sets
</p>
<p>of arrivals. Since the arrivals are modeled as independent Bernoulli trials, we can</p>
<p/>
</div>
<div class="page"><p/>
<p>5.10. REAL-WORLD EXAMPLE - SERVICING CUSTOMERS
</p>
<p>assert that
</p>
<p>127
</p>
<p>P[2 or fewer arrivals at both lanes]
</p>
<p>so that
</p>
<p>P[2 or fewer arrivals at both lanes]
</p>
<p>P[2 or fewer arrivals at lane 1]
</p>
<p>.P[2 or fewer arrivals at lane 2]
</p>
<p>P[2 or fewer arrivals at lane IF
P[X1 :::; 2F
</p>
<p>(t,px, [kl) 2
[exp( -&gt;'2) (1 + &gt;'2 + ~&gt;.~) r= 0.957
</p>
<p>which meets the requirement. An example is shown for one of the two express lanes
</p>
<p>with an average number of customer arrivals per minute of 7/12 in Figures 5.16 and
</p>
<p>5.17, with the latter an expanded version of the former. The dashed vertical lines
</p>
<p>+++ + +++ ++~ .+ + +
</p>
<p>o 500 1000 1500 2000 2500 3000 3500
Time (sec)
</p>
<p>Figure 5.16: Arrival times at one of the two express lanes (a '+' indicates an arrival).
</p>
<p>in Figure 5.17 indicate l-minute intervals. There are no l-minute intervals with
</p>
<p>more than 2 arrivals, as we expect.</p>
<p/>
</div>
<div class="page"><p/>
<p>128 CHAPTER 5. DISCRETE RANDOM VARIABLES
</p>
<p>I 1 I: 1 1 1 I: 1 1 :1 J I I: 1 1 1 :1
</p>
<p>.. I + : I+, 1 ~I 1 :1 J +I ~ I + J :1
</p>
<p>J 1 I: 1 1 1 I : 1 J :1 1 ILl I 1 :1
</p>
<p>J 1 I: 1 1 1 I : 1 1 :1 1 1 I ; 1 I 1 ;1
</p>
<p>o 200 400 600 800 1000 1200 1400 1600 1800
Time (sec)
</p>
<p>Figure 5.17: Expanded version of Figure 5.16 (a '+' indicates an arrival) . Time
slot s of 60 seconds are shown by dashed lines.
</p>
<p>References
</p>
<p>Ross, S., A First Course in Probability , Prentice-Hall , Upper Saddle River, NJ ,
</p>
<p>2002.
</p>
<p>Problems
</p>
<p>5.1 (w) Draw a picture depicting a mapping of the outcome of a die toss , i.e., the
pat tern of dots that appear, to the numbers 1,2,3, 4, 5,6.
</p>
<p>5.2 (w) Repeat Problem 5.1 for a mapping of the sides that display 1, 2, or 3 dots
</p>
<p>to the number &deg;and the remaining sides to the numb er 1.
5.3 (w) Consider a random experiment for which S = {s, : s, = i , i = 1,2, .. . , 10}
</p>
<p>and the outcomes are equally likely. If a random vari able is defined as X{Si) =
</p>
<p>sl, find SX and the PMF.
</p>
<p>5.4 C..:...) (w) Consider a random experimentfor whichS = {Si: s, = -3,-2,-1,0,
1,2,3} and the outcomes are equally likely. If a random variable is defined as
</p>
<p>X{ Si ) = sl, find Sx and the PMF.
</p>
<p>5.5 (w) A man is late for his job by s, = i minutes, where i = 1,2, .... If P [SiJ =
{1 /2) i and he is fined $0.50 per minute, find the P MF of his fine. Next find
</p>
<p>the probability that he will be fined more than $10.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 129
</p>
<p>5.6 (..:.:.-) (w) If px[k] = apk for k = 2,3, . .. is to be a valid PMF, what are the
possible values for a and p?
</p>
<p>5.7 (t) The maximum value of the binomial PMF occurs for the unique value k =
[(M + 1)p], where [x] denotes the largest integer less than or equal to x, if
(M + 1)p is not an integer. If, however, (M + 1)p is an integer, then the PMF
will have the same maximum value at k = (M + 1)p and k = (M + 1)p - l.
For the latter case when (M + 1)p is an integer you are asked to prove this
result. To do so first show that
</p>
<p>(M+1)p-k
px[k]/px[k - 1] = 1 + k(1 _ p) .
</p>
<p>5.8 C:..:.-) (w) At a party a large barrel is filled with 99 gag gifts and 1 diamond ring,
all enclosed in identical boxes. Each person at the party is given a chance to
</p>
<p>pick a box from the barrel, open the box to see if the diamond is inside, and if
</p>
<p>not, to close the box and return it to the barrel. What is the probability that
</p>
<p>at least 19 persons will choose gag gifts before the diamond ring is selected?
</p>
<p>5.9 (f,c) If X is a geometric random variable with p = 0.25, what is the probability
</p>
<p>that X 2: 4? Verify your result by performing a computer simulation.
</p>
<p>5.10 (c) Using a computer simulation to generate a geom(0.25) random variable,
</p>
<p>determine the average value for a large number of realizations. Relate this to
</p>
<p>the value of p and explain the results.
</p>
<p>5.11 (t) Prove that the maximum value of a Poisson PMF occurs at k = [&gt;.]. Hint:
See Problem 5.7 for the approach.
</p>
<p>5.12 (w,c) If X,...., PoisfX), plot P[X 2: 2] versus&gt;' and explain your results.
</p>
<p>5.13 (..:...:.-) (c) Use a computer simulation to generate realizations of a PoisfX) ran-
</p>
<p>dom variable with&gt;. = 5 by approximating it with a bin(100,0.05) random
</p>
<p>variable. What is the average value of X?
</p>
<p>5.14 (..:.:.-) (w) If X ,...., bin(100,0.01), determine px[5]. Next compare this to the
value obtained using a Poisson approximation.
</p>
<p>5.15 (t) Prove the following limit:
</p>
<p>lim g(M) = lim (1 + ~)M = exp(x).
M -too M -too M
</p>
<p>To do so note that the same limit is obtained if M is replaced by a continuous
</p>
<p>variable, say u, and that one can consider In g(u) since the logarithm is a
</p>
<p>continuous function. Hint: Use L'Hospital's rule.</p>
<p/>
</div>
<div class="page"><p/>
<p>130 CHAPTER 5. DISCRETE RANDOM VARIABLES
</p>
<p>5.16 (f,e) Compare the PMFs for Pois(1) and bin(100,0.01) random variables.
</p>
<p>5.17 (c) Generate realizations of a Pois(1) random variable by using a binomial
</p>
<p>approximation.
</p>
<p>5.18 C:...:...) (c) Compare the theoretical value of P[X = 3] for the Poisson random
variable to the estimated value obtained from the simulation of Problem 5.17.
</p>
<p>5.19 (f) If X rv Ber(p) , find the PMF for Y = -X.
</p>
<p>5.20 C:..:...) (f) If X rv Pois(&gt;.), find the PMF for Y = 2X.
</p>
<p>5.21 (f) A discrete random variable X has the PMF
</p>
<p>! Xl = -1
i X2 = -!
</p>
<p>PX[Xi] = k xa =&deg;
1 _ 1
</p>
<p>16 X4 - 2"
</p>
<p>l6 Xs = 1.
</p>
<p>If Y = sin 7fX, find the PMF for Y.
</p>
<p>5.22 (t) In this problem we derive the Taylor expansion for the function g(x) =
</p>
<p>exp(x). To do so note that the expansion about the point X = &deg;is given by
00 g(n)(0)
</p>
<p>g(x) = ""' z"
6 n!
n=O
</p>
<p>where g(O) (0) = g(O) and g(n)(o) is the nth derivative of g(x) evaluated at
</p>
<p>X = 0. Prove that it is given by
</p>
<p>00 n
</p>
<p>exp(x) = L;'
n.
</p>
<p>n=O
</p>
<p>5.23 (f) Plot the CDF for
</p>
<p>{
</p>
<p>i k = 1
px[k] = ! k = 2
</p>
<p>i k = 3.
</p>
<p>5.24 (w) A horizontal bar of negligible weight is loaded with three weights as shown
</p>
<p>in Figure 5.18. Assuming that the weights are concentrated at their center
</p>
<p>locations, plot the total mass of the bar starting at the left end (where X = &deg;
meters) to any point on the bar. How does this relate to a PMF and a CDF?</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS
</p>
<p>~ kg
</p>
<p>131
</p>
<p>o 1 2 3 4 5 6 meters
</p>
<p>Figure 5.18: Weightless bar supporting three weights.
</p>
<p>5.25 (f) Find and plot the CDF of Y = - X if X'" Ber(i).
</p>
<p>5. 26 k :') (w ) Fi nd the PMF if X is a discret e random variable with the CDF
</p>
<p>{
</p>
<p>O x &lt; 0
</p>
<p>Fx(x) = ~ 0 ~ x s 5
1 x&gt; 5.
</p>
<p>5.27 (w ) Is the following a valid CDF? If not, why not, and how could you modify
</p>
<p>it to become a valid one?
</p>
<p>0 x &lt; 2
1
</p>
<p>2~ x~3
Fx(x) = 2"
</p>
<p>3
3&lt; x~44:
</p>
<p>1 x ~4 .
</p>
<p>5 .28 C:.:J (f) If X has the CDF shown in Figure 5.11b, determine P[2 ~ X ~ 4]
from the CDF.
</p>
<p>5 .2 9 (t) Prove that the fun ction g(x) = exp( x) is a monotonically increasing func-
t ion by showing that g(X2) ~ g(Xl) if X2 ~ X l .
</p>
<p>5.30 (c) Estimate the PMF for a geom(0.25) random variable for k = 1,2, . .. ,20
</p>
<p>using a computer simulation and compare it to the true P MF. Also, est imate
</p>
<p>the CDF from your computer simulation.
</p>
<p>5.31 C:.:J (f,c) The arrival rate of calls at a mob ile switching station is 1 per second.
The probability of k calls in a T second interval is given by a Poisson PMF
</p>
<p>with A = arrival rate x T . What is the probability that there will be more
than 100 calls placed in a l-rninute interval?</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 6
</p>
<p>Expected Values for Discrete
</p>
<p>Random Variables
</p>
<p>6 .1 Introduction
</p>
<p>The probability mass function (PMF) discussed in Chapter 5 is a complete de-
</p>
<p>scription of a discrete random variable. As we have seen, it allows us to determine
</p>
<p>probabilities of any event. Once the probability of an event of interest is determined,
</p>
<p>however , the question of its interpretation arises. Consider, for example, whether
</p>
<p>there is adequate rainfall in Rhode Island to sustain a farming endeavor. The past
</p>
<p>history of yearly summer rainfall was shown in Figure 1.1 and is repeated in Figure
</p>
<p>6.1a for convenience. Along with it, the estimated PMF of this yearly data is shown
</p>
<p>in Figure 6.1b (see Section 5.9 for a discussion on how to estimate the PMF). For
</p>
<p>a particular crop we might need a rainfall of between 8 and 12 inches. This event
</p>
<p>has probability 0.5278, obtained by Lk:sPx[kj for the estimated PMF shown in
Figure 6.1b. Is this adequate or should the probability be higher? Answers to such
</p>
<p>questions are at best problematic. Rather we might be better served by ascertaining
</p>
<p>the average rainfall since this is closer to the requirement of an adequate amount
</p>
<p>of rainfall. In the case of Figure 6.1a the average is 9.76 inches, and is obtained by
</p>
<p>summing all the yearly rainfalls and dividing by the number of years. Based on the
</p>
<p>given data it is a simple matter to estimate the average value of a random variable
</p>
<p>(the rainfall in this case). Some computer simulation results pertaining to averages
</p>
<p>have already been presented in Example 2.3. In this chapter we address the topic of
</p>
<p>the average or expected value of a discrete random variable and study its properties.
</p>
<p>6.2 Summary
</p>
<p>The expected value of a random variable is the average value of the outcomes of
</p>
<p>a large number of experimental trials. It is formally defined by (6.1). For discrete
random variables with integer values it is given by (6.2) and some examples of its</p>
<p/>
</div>
<div class="page"><p/>
<p>134 CHAPTER 6. EXPECTED VALUES FOR DISCRETE RAND. VAR.
</p>
<p>16 : : : : ..
&middot; . . .&middot; .&middot; .
</p>
<p>252010 15
k (inches)
</p>
<p>5
</p>
<p>oL..+-.....L..L..L.L-'-'-'-'L....I-L..L...L..L...........__-'
o
</p>
<p>&middot; .
0.2 ; i ; ~ .
</p>
<p>0.05 .. .. .. .. .:. .
</p>
<p>0.25 .--------.---~ - -T-"-____r--...,
</p>
<p>~ .
~0 . 15 . .. .. .... ~ &middot; .. : r ~ .... ....&middot;
</p>
<p>&middot; . .
0.1 ; ; ; .
</p>
<p>&middot; . .&middot; . .&middot; ..&middot; ..
</p>
<p>1960 2000
</p>
<p>.. .. .&middot; .&middot; .&middot; .
</p>
<p>1920
2
</p>
<p>1900
</p>
<p>4
</p>
<p>6
</p>
<p>6
</p>
<p>: A.verage = 9.76 inche
18 : ; , ; ; " :
</p>
<p>&middot; . . . .&middot; . . . .
</p>
<p>~ 14 .... .... ..
</p>
<p>-&pound; 12
oS 10 ~...,..r+I:t-'-t-tr
</p>
<p>(a) Annual summer rainfall (b) Estimated PMF
</p>
<p>Figure 6.1: Annual summer rainfall in Rhode Island and its estimated probability
</p>
<p>mass function.
</p>
<p>determination given in Section 6.4. The expected value does not exist for all PMFs
</p>
<p>as illustrated in Section 6.4. For functions of a random variable the expected value
</p>
<p>is easily computed via (6.5). It is shown to be a linear operation in Section 6.5.
</p>
<p>Another interpretation of the expected value is as the best predictor of the outcome
</p>
<p>of an experiment as shown in Example 6.3. The variability of the values exhibited by
</p>
<p>a random variable is quantified by the variance. It is defined in (6.6) with examples
</p>
<p>given in Section 6.6. Some properties of the variance are summarized in Section
</p>
<p>6.6 as Properties 1 and 2. An alternative way to determine means and variances of
</p>
<p>a discrete random variable is by using the characteristic function. It is defined by
</p>
<p>(6.10) and for integer valued random variables it is evaluated using (6.12), which is
</p>
<p>a Fourier transform of the PMF. Having determined the characteristic function, one
</p>
<p>can easily determine the mean and variance by using (6.13). Some examples of this
</p>
<p>procedure are given in Section 6.7, as are some further important properties of the
</p>
<p>characteristic function. An important property is that the PMF may be obtained
</p>
<p>from the characteristic function as an inverse Fourier transform as expressed by
</p>
<p>(6.19). In Section 6.8 an example is given to illustrate how to estimate the mean
</p>
<p>and variance of a discrete random variable. Finally, Section 6.9 describes the use of
</p>
<p>the expected value to reduce the average code length needed to store symbols in a
</p>
<p>digital format. This is called data compression.
</p>
<p>6.3 Determining Averages from the PMF
</p>
<p>We now discuss how the average of a discrete random variable can be obtained from
</p>
<p>the PMF. To motivate the subsequent definition we consider the following game of</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3. DETERMINING AVERAGES FROM THE PMF 135
</p>
<p>chance. A barrel is filled with US dollar bills with denominations of $1, $5, $10,
</p>
<p>and $20. The proportion of each denomination bill is the same. A person playing
</p>
<p>the game gets to choose a bill from the barrel, but must do so while blindfolded. He
</p>
<p>pays $10 to play the game, which consists of a single draw from the barrel. After he
</p>
<p>observes the denomination of the bill , the bill is returned to the barrel and he wins
</p>
<p>that amount of money. Will he make a profit by playing the game many times?
</p>
<p>A typical sequence of outcomes for the game is shown in Figure 6.2. His average
</p>
<p>504020 30
Play number
</p>
<p>10
</p>
<p>.. . . . . . . . . ...
</p>
<p>. . . . ... .
</p>
<p>. . . .... "' . . . ... .
</p>
<p>..
</p>
<p>I' I'
,
</p>
<p>I' I"" I;, I'
, ITo
</p>
<p>o
</p>
<p>5
</p>
<p>25
</p>
<p>'"
~a 10
o
</p>
<p>20
</p>
<p>Figure 6.2: Dollar winnings for each play.
</p>
<p>winnings per play is found by adding up all his winnings and dividing by the number
</p>
<p>of plays N. This is computed by
</p>
<p>1 N
x = - ""' XiNL.J
</p>
<p>z=l
</p>
<p>where Xi is his winnings for play i. Alternatively, we can compute x using a slightly
different approach. From Figure 6.2 the number of times he wins k dollars (where
</p>
<p>k = 1,5,10,20) is given by N k , where
</p>
<p>N l 13
</p>
<p>Ns 13
</p>
<p>NlO 10
</p>
<p>N20 = 14.</p>
<p/>
</div>
<div class="page"><p/>
<p>x =
</p>
<p>136 CHAPTER 6. EXPECTED VALUES FOR DISCRETE RAND. VAR.
</p>
<p>As a result, we can determine the average winnings per play by
</p>
<p>1 . N 1 + 5 . Ns + 10 . NlO + 20 . N 20
N 1 + N s + N lO + N 20
</p>
<p>N 1 Ns N lO N20
1 &middot; N + 5&middot; N + 10&middot; N + 20&middot; N
</p>
<p>13 13 10 14
1 . - + 5 . - + 10 . - + 20 . -
</p>
<p>50 50 50 50
= 9.16
</p>
<p>since N = N 1 + Ns + N lO + N 20 = 50. If he were to play the game a large number
of times, then as N -+ 00 we would have Nk/N -+ px[k], where the latter is just
</p>
<p>the PMF for choosing a bill with denomination k, and results from the relative
</p>
<p>frequency interpretation of probability. Then, his average winnings per play would
</p>
<p>be found as
</p>
<p>x -+ 1&middot; px[l] + 5 . px[5] + 10 &middot; px[10] + 20&middot; px[20]
1 1 1 1
</p>
<p>1 . :4 + 5 . :4 + 10 . :4 + 20 . :4
9
</p>
<p>where px[k] = 1/4 for k = 1,5,10,20 since the proportion of bill denominations in
the barrel is the same for each denomination. It is now clear that "on the average"
</p>
<p>he will lose $1 per play. The value that the average converges to is called the expected
</p>
<p>value of X , where X is the random variable that describes his winnings for a single
</p>
<p>play and takes on the values 1,5,10,20. The expected value is denoted by E[X].
</p>
<p>For this example, the PMF as well as the expected value is shown in Figure 6.3.
</p>
<p>The exp ected value is also called the expectation of X, the average of X, and the
</p>
<p>mean of X. With this example as motivation we now define the expected value of a
</p>
<p>discrete random variable X as
</p>
<p>(6.1)
</p>
<p>where the sum is over all values of Xi for which PX[Xi] is nonzero. It is determined
</p>
<p>from the PMF and as we have seen coincides with our notion of the outcome of an
</p>
<p>experiment in the "long run" or "on the average." The expected value may also be
</p>
<p>intepreted as the best prediction of the outcome of a random experiment for a single
</p>
<p>trial (to be described in Example 6.3). Finally, the expected value is analogous to
</p>
<p>the center of mass of a system of linearly arranged masses as illustrated in Problem
</p>
<p>6.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4. EXPECTED VALUES OF SOME IMPORTANT RAND. VAR. 137
</p>
<p>5
</p>
<p>..
</p>
<p>..
</p>
<p>..
</p>
<p>..
</p>
<p>..
</p>
<p>1 5 10 15 20 2
k
</p>
<p>o
</p>
<p>0.05
</p>
<p>~
&gt;&lt;:
~0 .1 5
</p>
<p>0.3
</p>
<p>02
</p>
<p>0.25
</p>
<p>0.1
</p>
<p>Expected value, E[X] = 9
</p>
<p>Figure 6.3: PMF and expected value of dollar bill denomination chosen.
</p>
<p>6.4 Expected Values of Some Important Random
</p>
<p>Variables
</p>
<p>T he definition of the expected value was given by (6.1). When the random variable
</p>
<p>takes on only integer values, we can rewrite it as
</p>
<p>00
</p>
<p>E[X] = L kpx[k] .
k=-oo
</p>
<p>(6.2)
</p>
<p>We next determine the expected values for some important discrete random variables
</p>
<p>(see Chapter 5 for a definition of the PMFs).
</p>
<p>6 .4. 1 B ernoulli
</p>
<p>If X '"" Ber(p) , then the expected value is
</p>
<p>1
</p>
<p>E[X] L kpx[k]
k=O
</p>
<p>= O&middot; (1 - p) + 1 . p
p.
</p>
<p>Note that E[X] need not be a value that the random variab le takes on . In this case,
it is between X = 0 and X = 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>E[X] =
</p>
<p>138 CHAPTER 6. EXPECTED VALUES FOR DISCRETE RAND. VAR.
</p>
<p>6.4.2 Binomial
</p>
<p>If X '" bin(M,p) , then the expected value is
</p>
<p>M
</p>
<p>E[X] = Lkpx[k]
</p>
<p>k=O
M
</p>
<p>~ k (~) pk(l _ p)M-k.
</p>
<p>To evaluate this in closed form we will need to find an expression for the sum.
</p>
<p>Continuing, we have that
</p>
<p>M
</p>
<p>'"' M! k M-k
E[X] = L..Jk (M _ k)!k!P (1 - p)
</p>
<p>k=O
</p>
<p>M (M -1)'M '"' . k-l(l _ )M-l-(k-l)
p L..J (M - k)!(k - l)!P P
</p>
<p>k=l
</p>
<p>and letting M' = M - 1, k' = k - 1, this becomes
</p>
<p>M'
'"' M'! k' M'-k'
</p>
<p>Mp L..J (M' _ k'!)k'!P (1 - p)
k'=O
M'
</p>
<p>= Mp L (~') pk'(1- p)M'-k'
k'=O
</p>
<p>Mp
</p>
<p>since the summand is just the PMF of a bin(M',p) random variable. Therefore, we
</p>
<p>have that E[X] = Mp for a binomial random variable. This derivation is typical in
</p>
<p>that we attempt to manipulate the sum into one whose summands are the values of
</p>
<p>a PMF and so the sum must evaluate to one. Intuitively, we expect that if p is the
</p>
<p>probability of success for a Bernoulli trial, then the expected number of successes
</p>
<p>for M independent Bernoulli trials (which is binomially distributed) is Mp.
</p>
<p>6.4.3 Geometric
</p>
<p>If X '" geom(p), the the expected value is
</p>
<p>00
</p>
<p>E[X] = L k(l - p)k-lp.
</p>
<p>k=l
</p>
<p>To evaluate this in closed form , we need to modify the summand to be a PMF,
</p>
<p>which in this case will produce a geometric series. To do so we use differentiation</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4. EXPECTED VALUES OF SOME IMPORTANT RAND. VAR.
</p>
<p>by first letting q = 1 - P to produce
</p>
<p>139
</p>
<p>E[X]
</p>
<p>But since a &lt; q &lt; 1 we have upon using the formula for the sum of a geometric
series or l:~l qk = q/(l - q) that
</p>
<p>E[X] = p:q (1 ~ q)
(1- q) - q(-l)
</p>
<p>P (1 - q)2
</p>
<p>1
</p>
<p>=
</p>
<p>The expected number of Bernoulli trials until the first success (which is geometrically
</p>
<p>distributed) is E[X] = l/p. For example, if p = 1/10, then on the average it takes
</p>
<p>10 trials for a success, an intuitively pleasing result.
</p>
<p>6.4.4 Poisson
</p>
<p>If X r-.J PoisfX} , then it can be shown that E[X] = &gt;.. The reader is asked to
verify this in Problem 6.5. Note that this result is consistent with the Poisson
</p>
<p>approximation to the binomial PMF since the approximation constrains M p (the
</p>
<p>expected value of the binomial random variable) to be &gt;. (the expected value of the
Poisson random variable).
</p>
<p>N at all PMFs have expected values.
</p>
<p>Discrete random vari ables with a finite number of values always have expected
</p>
<p>values. In the case of a countably infinite number of values , a discrete random
</p>
<p>variable may not have an expected value. As an example of this , consider the PMF
</p>
<p>k = 1,2, .... (6.3)
</p>
<p>This is a valid PMF since it can be shown to sum to one. Attempting to find the</p>
<p/>
</div>
<div class="page"><p/>
<p>140 CHAPTER 6. EXPECTED VALUES FOR DISCRETE RAND. VAR.
</p>
<p>expected value produces
</p>
<p>E[X] =
</p>
<p>&infin;
&sum;
</p>
<p>k=1
</p>
<p>kpX [k]
</p>
<p>=
4
</p>
<p>2
</p>
<p>&infin;
&sum;
</p>
<p>k=1
</p>
<p>1
</p>
<p>k
&rarr; &infin;
</p>
<p>since 1/k is a harmonic series which is known not to be summable (meaning that
the partial sums do not converge). Hence, the random variable described by the
PMF of (6.3) does not have a finite expected value. It is even possible for a sum
&sum;&infin;
</p>
<p>k=&minus;&infin; kpX [k] that is composed of positive and negative terms to produce different
results depending upon the order in which the terms are added together. In this
case the value of the sum is said to be ambiguous. These difficulties can be avoided,
however, if we require the sum to be absolutely summable or if the sum of the
absolute values of the terms is finite [Gaughan 1975]. Hence we will say that the
expected value exists if
</p>
<p>E[|X|] =
&infin;
&sum;
</p>
<p>k=&infin;
|k|px[k] &lt; &infin;.
</p>
<p>In Problem 6.6 a further discussion of this point is given.
</p>
<p>Lastly, note the following properties of the expected value.
</p>
<p>1. It is located at the &ldquo;center&rdquo; of the PMF if the PMF is symmetric about some
point (see Problem 6.7).
</p>
<p>2. It does not generally indicate the most probable value of the random variable
(see Problem 6.8).
</p>
<p>3. More than one PMF may have the same expected value (see Problem 6.9).
</p>
<p>6.5 Expected Value for a Function of a Random
</p>
<p>Variable
</p>
<p>The expected value may easily be found for a function of a random variable X if the
PMF pX [xi] is known. If the function of interest is Y = g(X), then by the definition
of expected value
</p>
<p>E[Y ] =
&sum;
</p>
<p>i
</p>
<p>yipY [yi]. (6.4)
</p>
<p>But as shown in Appendix 6A we can avoid having to find the PMF for Y by using
the much more convenient form
</p>
<p>E[g(X)] =
&sum;
</p>
<p>i
</p>
<p>g(xi)pX [xi]. (6.5)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.5. EXPECTED VALUE FOR A FUNCTION OF A RAND. VAR. 141
</p>
<p>Otherwise, we would be forced to determine PY[Yi] from PX[Xi] and g(X) using (5.9) .
This result proves to be very useful , especially when the function is a complicat ed
</p>
<p>one such as g(x ) = sin[Crr/ 2)x] (see Problem 6.10). Some examples follow.
</p>
<p>Example 6.1 - A linear function
</p>
<p>If g(X) = aX + b, where a and b are constants, t hen
</p>
<p>E[g(X)] E [aX + b]
</p>
<p>:L)axi + b)PX[Xi] (from (6.5))
</p>
<p>= aE[X] +b (definition of E[X] and PMF values sum to one.)
</p>
<p>In particular , if we set a = 1, then E[X + b] = E[X] + b. This allows us to set the
expec ted value of a random variable to any desired value by adding the appropriate
</p>
<p>constant to X. Finally, a simple extension of this example produces
</p>
<p>for any two constants a1 and a2 and any two funct ions gl and g2 (see Problem 6.11).
</p>
<p>It is said tha t t he expecta tion operator E is lin ear.
</p>
<p>Example 6.2 - A nonlinear function
</p>
<p>Assume that X has a PMF given by
</p>
<p>1
px[k] =:5 k = 0,1 ,2 ,3 , 4
</p>
<p>and determine E [Y] for Y = g(X ) = X 2. Then , using (6.5) produces
</p>
<p>4
</p>
<p>E[X
2
</p>
<p>] = L k2px[k]
k=O
</p>
<p>4
</p>
<p>Lk2~
k=O
</p>
<p>6.
</p>
<p>&amp; It is not true that E [g (X)] = g(E[X]).
From the previous example with g(X) = X 2, we had tha t E [g (X)] = E [X 2] = 6 but
</p>
<p>g(E [X ]) = (E[X])2 = 22 = 4 =1= E[g(X)]. It is said that the expectation operator</p>
<p/>
</div>
<div class="page"><p/>
<p>142 CHAPTER 6. EXPECTED VALUES FOR DISCRETE RAND. VAR.
</p>
<p>does not commute (or we cannot just take E[g(X)] and interchange the E and g) for
</p>
<p>nonlinear functions. This manipulation is valid , however, for linear (actually affine)
</p>
<p>functions as Example 6.1 demonstrates. Henceforth, we will use the notation E2[X]
</p>
<p>to replace the more cumbersome (E[X])2.
</p>
<p>Example 6.3 - Predicting the outcome of an experiment
</p>
<p>It is always of great interest to be able to predict the outcome of an experiment
</p>
<p>before it has occurred. For example, if the experiment were the summer rainfall in
</p>
<p>Rhode Island in the coming year, then a farmer would like to have this information
</p>
<p>before he decides upon which crops to plant. One way to do this is to check the
</p>
<p>Farmer's almanac, but its accuracy may be in dispute! Another approach would be
</p>
<p>to guess this number based on the PMF (statisticians, however , use the more formal
</p>
<p>term "predict" or "est imate" which sounds better). Denoting the prediction by the
</p>
<p>number b, we would like to choose a number so that on the average it is close to the
</p>
<p>true outcome of the random variable X. To measure the error we could use x - b,
</p>
<p>where x is the outcome, and to account for positive and negative errors equally we
</p>
<p>could use (x - b)2 . This squared error may at times be small and at other times
</p>
<p>large, depending on the outcome of X. What we want is the average value of the
</p>
<p>squared error. This is measured by E[(X - bf] , and is termed the mean square
</p>
<p>error (MSE). We denote it by mse(b) since it will depend on our choice of b. A
</p>
<p>reasonable method for choosing b is to choose the value that minimizes the MSE.
</p>
<p>We now proceed to find that value of b.
</p>
<p>mse(b) E[(X - b)2]
</p>
<p>_ E[X2 - 2bX + b2]
</p>
<p>- E[X2] - 2bE[X] + E[b2] (linearity of E(&middot;))
= E[X2] - 2bE[X] + b2 (expected value of constant is the constant).
</p>
<p>To find the value of b that minimizes the MSE we need only differentiate the MSE,
</p>
<p>set the derivative equal to zero, and solve for b. This is because the MSE is a
</p>
<p>quadratic function of b whose minimum is located at the stationary point. Thus,
</p>
<p>we have
</p>
<p>dmse(b) = -2E[X] + 2b = 0
db
</p>
<p>which produces the minimizing or optimal value of b given by bopt = E[X]. Hence,
the best predictor of the outcome of an experiment is the expected value or mean
</p>
<p>of the random variable. For example, the best predictor of the outcome of a die
</p>
<p>toss would be 3.5. This result provides another interpretation of the expected value.
</p>
<p>The expected value of a random variable is the best predictor of the outcome of the
</p>
<p>experiment, where "best" is to be interpreted as the value that minimizes the MSE.
</p>
<p>o</p>
<p/>
</div>
<div class="page"><p/>
<p>6.6. VARIANCE AND MOMENTS OF A RANDOM VARIABLE
</p>
<p>6.6 Variance and Moments of a Random Variable
</p>
<p>143
</p>
<p>Another function of a random variable that yields important information about its
</p>
<p>behavior is that given by g(X) = (X - E[X]) 2 . Whereas E[X] measures the mean
</p>
<p>of a random variable, E[(X - E[X])2] measures the average squared deviation from
</p>
<p>the mean. For example, a uniform discrete random variable whose PMF is
</p>
<p>1
px[k] = 2M + 1 k = -M,-M + 1, ... ,M
</p>
<p>is easily shown to have a mean of zero for any M. However, as seen in Figure 6.4 the
</p>
<p>variability of the outcomes of the random variable becomes larger as M increases.
</p>
<p>This is because the PMF for M = 10 can have values exceeding those for M = 2.
The variability is measured by the variance which is defined as
</p>
<p>var(X) = E[(X - E[X])2]. (6.6)
</p>
<p>var(X)
</p>
<p>Note that the variance is always greater than or equal to zero. It is determined from
</p>
<p>the PMF using (6.5) with g(X) = (X - E[X])2 to yield
</p>
<p>var(X) = :~:)X i - E[X])2pX [Xi]. (6.7)
</p>
<p>For the current example, E[X] = 0 due to the symmetry of the PMF about k = 0
so that
</p>
<p>But it can be shown that
</p>
<p>tk2 = M(M + 1)(2M + 1)
k=l 6
</p>
<p>which yields
</p>
<p>2 M(M + 1)(2M + 1)
2M +1 6
</p>
<p>M(M + 1)
3
</p>
<p>Clearly, the variance increases with M, or equivalently with the width of the PMF, as
</p>
<p>is also evident from Figure 6.4. We next give another example of the determination
</p>
<p>of the variance and then summarize the results for several important PMFs.</p>
<p/>
</div>
<div class="page"><p/>
<p>144 CHAPTER 6. EXPECTED VALUES FOR DISCRETE RAND. VAR.
</p>
<p>:
</p>
<p>0.25 .-~-~-~---.---~---,
</p>
<p>~
~ O .1 5 . . '; ; , ; ; .
</p>
<p>I;:l., . .
</p>
<p>..... . . .. . . . . . . . . ......... . .
. .
</p>
<p>0.1 ; ; .
</p>
<p>0.2 .. , . . . .
</p>
<p>105o
k
</p>
<p>- 5-10
o
</p>
<p>0.2
</p>
<p>0.1
</p>
<p>0.05
</p>
<p>0.25
</p>
<p>~
~ 0 . 1 5
</p>
<p>I;:l.,
</p>
<p>(a) Uniform P MF, M = 2 (b) Uniform PMF, M = 10
</p>
<p>10 . . . . . . . . . &middot; ..
</p>
<p>5 . .
</p>
<p>en
Q)
</p>
<p>S
o
u...,
</p>
<p>- 10 "" " -'. - 10 . .. . . . . &bull;. .. . ..
</p>
<p>o 10 20 30 40
Trial number
</p>
<p>50 o 10 20 30 40
Tr ial number
</p>
<p>50
</p>
<p>(c) Typ ical outcomes, M = 2 (d) Typical outcomes, M = 10
</p>
<p>Figure 6.4: Illust ration of effect of width of P MF on vari ability of outcomes.
</p>
<p>Example 6.4 - Variance of Bernoulli random variable
</p>
<p>If X "" Ber(p), then since E [X] = p, we have
</p>
<p>1
</p>
<p>= 2)k - p)2px [k]
k=O
</p>
<p>(0 - p)2(1 - p) + (1- p)2p
p(1 - p) .</p>
<p/>
</div>
<div class="page"><p/>
<p>6.6. VARIANCE AND MOMENTS OF A RANDOM VARIABLE 145
</p>
<p>Values PMF E[X] var(X) X()
</p>
<p>Uniform k=&minus;M,...,M 12M+1 0
M(M+1)
</p>
<p>3
sin[(2M+1)/2]
(2M+1) sin[/2]
</p>
<p>Bernoulli k=0,1 pk(1&minus; p)1&minus;k p p(1&minus;p) p exp(j)+(1&minus;p)
</p>
<p>Binomial k=0,1,...,M
(
</p>
<p>M
k
</p>
<p>)
</p>
<p>pk(1&minus; p)M&minus;k Mp Mp(1&minus;p) [p exp(j)+(1&minus;p)]M
</p>
<p>Geometric k=1,2,... (1&minus; p)k&minus;1p 1p
1&minus;p
p2
</p>
<p>p
exp(&minus;j)&minus;(1&minus;p)
</p>
<p>Poisson k=0,1,... exp(&minus;)kk!   exp[(exp(j)&minus;1)]
</p>
<p>Table 6.1: Properties of discrete random variables.
</p>
<p>It is interesting to note that the variance is minimized and equals zero if p = 0 or
p = 1. Also, it is maximized for p = 1/2. Can you explain this? Important PMFs
with their means, variances, and characteristic functions (to be discussed in Section
6.7) are listed in Table 6.1. The reader is asked to derive some of these entries in
the Problems.
</p>
<p>An alternative useful expression for the variance can be developed based on the
properties of the expectation operator. We have that
</p>
<p>var(X) = E[(X &minus; E[X])2]
= E[X2 &minus; 2XE[X] + E2[X]]
= E[X2]&minus; 2E[X]E[X] + E2[X]
</p>
<p>where the last step is due to linearity of the expectation operator and the fact that
E[X] is a constant. Hence
</p>
<p>var(X) = E[X2]&minus;E2[X]
</p>
<p>and is seen to depend on E[X] and E[X2]. In the case where E[X] = 0, we have
the simple result that var(X) = E[X2]. This property of the variance along with
some others is now summarized.
</p>
<p>Property 6.1 &ndash; Alternative expression for variance
</p>
<p>var(X) = E[X2]&minus;E2[X] (6.8)
</p>
<p/>
</div>
<div class="page"><p/>
<p>146 CHAPTER 6. EXPECTED VALUES FOR DISCRETE RAND. VAR.
</p>
<p>Property 6.2 - Variance for random variable modified by a constant
</p>
<p>For c a constant
</p>
<p>var(c)
</p>
<p>var(X + c)
var(cX)
</p>
<p>o
var(X)
</p>
<p>c2var(X)
</p>
<p>o
The reader is asked to verify Property 6.2 in Problem 6.21.
</p>
<p>The expectations E[X] and E[X2] are called the first and second moments of
</p>
<p>X, respectively. The term moment has been borrowed from physics, where E[X]
</p>
<p>is called the center of mass or moment of mass (see also Problem 6.1). In general,
</p>
<p>the nth moment is defined as E[Xn ] and exists (meaning that the value can be
</p>
<p>determined unambiguously and is finite) if E[IXln] is finite. The latter is called the
</p>
<p>n absolute moment. It can be shown that if E[X S ] exists, then E[XT ] exists for
</p>
<p>r &lt; s (see Problem 6.23). As a result, if E[X2] is finite, then E[X] exists and by
(6.8) the variance will also exist. In summary, the mean and variance of a discrete
</p>
<p>random variable will exist if the second moment is finite.
</p>
<p>A variant of the notion of moments is that of the central moments. They are
</p>
<p>defined as E[(X - E[x])n], in which the mean is first subtracted from X before the
</p>
<p>n moment is computed. They are useful in assessing the average deviations from
</p>
<p>the mean. In particular, for n = 2 we have the usual definition of the variance. See
</p>
<p>also Problem 6.26 for the relationship between the moments and central moments.
</p>
<p>Variance is a nonlinear operator.
</p>
<p>The variance of a random variable does not have the linearity property of the
</p>
<p>expectation operator. Hence , in general
</p>
<p>var(gl (X) + g2(X)) = var(gl (X)) + var(g2(X)) is not true.
</p>
<p>Just consider var(X + X), where E[X] = 0 as a simple example.
</p>
<p>J1
As explained previously, an alternative interpretation of E[X] is as the best predictor
</p>
<p>of X. Recall that this predictor is the constant bopt = E[X] when the mean square
</p>
<p>error is used as a measure of error. We wish to point out that the minimum mse is
</p>
<p>then
</p>
<p>msemin = E[(X - bopt )2]
</p>
<p>E[(X - E[X])2]
</p>
<p>= var(X). (6.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.7. CHA RACTERISTIC FUNCTIONS 147
</p>
<p>Thus, how well we can predict the outcome of an experiment depends on the variance
</p>
<p>of the random variable. As an example, consider a coin toss with a probability of
</p>
<p>heads (X = 1) of p and of tails (X = 0) of 1 - p , i.e., a Bernoulli random variable.
</p>
<p>We would predict the outcome of X to be bopt = E[X] = p and the minimum mse is
the variance which from Example 6.4 is msemin = p(1 - p). This is plotted in Figure
</p>
<p>6.5 versus p. It is seen that the minimum mse is smallest when p = a or p = 1 and
largest when p = 1/2, or most predictable for p = 0 and p = 1 and least predictable
for p = 1/2. Can you explain this?
</p>
<p>0.3
</p>
<p>0.25 ..
</p>
<p>Q)
</p>
<p>'" 0.2S
S
;:::I
</p>
<p>0.15S
'2
</p>
<p>~ 0.1
</p>
<p>0.05 .... ~ . .
</p>
<p>0.2
</p>
<p>p
</p>
<p>Figure 6.5: Measure of predictability of the outcome of a coin toss .
</p>
<p>6.7 Characteristic Functions
</p>
<p>Determining the moments E [Xn ] of a random var iab le can be a difficult task for
some P MFs. An alternative method that can be considerably easier is based on
</p>
<p>the characteristic function . In addition, the characterist ic func tion can be used to
</p>
<p>examine convergence of PMFs, as, for example, in the convergence of the binomial
</p>
<p>PMF to the Poisson PMF, and to determine the PMF for a sum of independent
</p>
<p>random variables, which will be examined in Chapter 7. In this sect ion we discuss the
</p>
<p>use of the characteristic function for the calcu lation of moments and to investigate
</p>
<p>the convergence of a PMF.
</p>
<p>The characteristic function of a random variable X is defined as
</p>
<p>&lt;/&gt;x(w) = E[exp(jwX) ] (6.10)
</p>
<p>where j is the square root of -1 and where w takes on a suitable range of values .
</p>
<p>Not e that the function g(X) = exp(jwX) is complex but by defining E[g(X)] =</p>
<p/>
</div>
<div class="page"><p/>
<p>148 CHAPTER 6. EXPECTED VALUES FOR DISCRETE RAND. VAR.
</p>
<p>E[cos(wX) + j sin(wX)] = E[cos(wX)] + jE[sin(wX)], we can apply (6.5) to the real
and imaginary parts of c/Jx (w) to yield
</p>
<p>c/Jx(w) E[exp(jwX)]
</p>
<p>= E[cos(wX) + j sin(wX)]
E[cos(wX)] + jE[sin(wX)]
</p>
<p>L COS(WXi )pX[Xi] + j L sin(wxi)px[xi]
</p>
<p>(6.11)
</p>
<p>To simplify the discussion, yet still be able to apply our results to the important
</p>
<p>PMFs, we assume that the sample space Sx is a subset of the integers. Then (6.11)
becomes
</p>
<p>00
</p>
<p>c/Jx(w) = L exp(jwk)px[k]
k=-oo
</p>
<p>or rearranging
00
</p>
<p>c/Jx(w) = L px[k] exp(jwk)
k=-oo
</p>
<p>(6.12)
</p>
<p>where px[k] = 0 for those integers not included in Sx- For example, in the Poisson
</p>
<p>PMF the range of summation in (6.12) would be k 2': O. In this form, the char-
acteristic function is immediately recognized as being the Fourier transform of the
</p>
<p>sequence px[k] for -00 &lt; k &lt; 00. Its definition is slightly different than the usual
Fourier transform, called the discrete-time Fourier transform, which uses the func-
</p>
<p>tion exp(-jwk) in its definition [Jackson 1991]. As a Fourier transform, it exhibits
</p>
<p>all the usual properties. In particular, the Fourier transform of a sequence is pe-
</p>
<p>riodic with period of 21r (see Property 6.4 for a proof). As a result , we need only
</p>
<p>examine the characteristic function over the interval -1r ~ W ~ 1r, which is defined
</p>
<p>to be the fundamental period. For our purposes the most useful property is that we
</p>
<p>can differentiate the sum in (6.12) "term by term" or
</p>
<p>dc/Jx(w)
</p>
<p>dw
</p>
<p>d 00
dw L px[k] exp(jwk)
</p>
<p>k=-oo
</p>
<p>00 d
L px[k]dw exp(jwk).
</p>
<p>k=-oo
</p>
<p>The utility in doing so is to produce a formula for E[X]. Carrying out the differen-
</p>
<p>tiation
</p>
<p>dc/Jx(w) ~ . .
dw = LJ px[kJJkexp(Jwk)
</p>
<p>k=-oo</p>
<p/>
</div>
<div class="page"><p/>
<p>6.7. CHARACTERISTIC FUNCTIONS
</p>
<p>so that
</p>
<p>149
</p>
<p>! dt/Jx(w) I
j dw w= O
</p>
<p>00
</p>
<p>L kpx[k]
k=-oo
</p>
<p>= E[X].
</p>
<p>In fact , repeated differentiation produces the formula for the nth moment as
</p>
<p>(6.13)
</p>
<p>All the moments that exist may be found by repeated differentiation of the charac-
</p>
<p>teristic function. An example follows.
</p>
<p>Example 6.5 - First two moments of geometric random variable
</p>
<p>Since the PMF for a geometric random variable is given by px[k] = (1 - p)k-lp for
k = 1,2, . . ., we have that
</p>
<p>00
</p>
<p>t/Jx(w) Lpx[k] exp(jwk)
</p>
<p>k=l
00
</p>
<p>= L(1- p)k-lpexp(jwk)
</p>
<p>k=l
00
</p>
<p>= pexp(jw) L [(1 - p) exp(jw)]k- l .
k=l
</p>
<p>But since 1(1 - p) exp(jw)1 &lt; 1, we can use the result
</p>
<p>00 00 1
'"'" zk-l = '"'" zk = __
LJ LJ 1- z
k=l k=O
</p>
<p>for z a complex number with Izi &lt; 1 to yield the characterist ic function
</p>
<p>t/Jx(w) =
pexp(jw)
</p>
<p>1 - [(1 - p) exp(jw)]
p
</p>
<p>exp( -jw) - (1 - p)'
(6.14)
</p>
<p>(6.15)
</p>
<p>(6.16)
</p>
<p>Note that as claimed the characteristic function is periodic with period 211". To find
</p>
<p>the mean we use (6.13) with n = 1 to produce
</p>
<p>E[X] = ! d&cent;&gt;x(w) I
j dw w= O
</p>
<p>!p( -1) -j exp( -jw) I
j [exp(-jw) - (1 - p)J2 w= O
</p>
<p>1 j 1
= -p- =-
</p>
<p>j p2 P</p>
<p/>
</div>
<div class="page"><p/>
<p>150 CHAPTER 6. EXPECTED VALUES FOR DISCRETE RAND. VAR.
</p>
<p>which agrees with our earlier results based on using the definition of expected value.
</p>
<p>To find the second moment and hence the variance using (6.8)
</p>
<p>~ d2cPx(w) I
P dw2 w=o
p d exp( -jw) I (f (6 15))rom .
j dw [exp(-jw) - (1 - p)J2 w=o
</p>
<p>'&pound;. D2(-j) exp( -jw) - exp( -jw)2D(-j) exp(-jw) I
. D4
</p>
<p>J w=o
</p>
<p>where D = exp(-jw) - (1 - p). Since Dlw=o = p, we have that
</p>
<p>E[X
2
J = (5) (-jp:~ 2jP)
</p>
<p>2p _p2
</p>
<p>p3
</p>
<p>2 1
p2 - P
</p>
<p>so that finally we have
</p>
<p>var(X) E[X2J - E2[XJ
</p>
<p>2 1 1
= p2- p- p2
</p>
<p>I-p
----p;:-.
</p>
<p>As a second example, we consider the binomial PMF.
</p>
<p>Example 6.6 - Expected value of binomial PMF
</p>
<p>We first determine the characteristic function as
</p>
<p>00
</p>
<p>cPx(w) = L px[kJ exp(jwk)
k=-oo
</p>
<p>to (~) pk(l_ p)M-k exp(jwk)
</p>
<p>to (~) [rex~(jW),nyr-k (617)
(a+ b)M (binomial theorem)
[pexp(jw) + (1 - p)JM. (6.18)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.7. CHARACTERISTIC FUNCTIONS
</p>
<p>The expected value then follows as
</p>
<p>151
</p>
<p>E[X] = ! d&lt;/&gt;x(w) I
j dw w=O
</p>
<p>yM [pexp(jw) + (1 - p)]M-lpj exp(jw) Iw=o
Mp
</p>
<p>which is in agreement with our earlier results. The variance can be found by using
</p>
<p>(6.8) and (6.13) for n = 2. It is left as an exercise to the reader to show that (see
</p>
<p>Problem 6.29)
</p>
<p>var(X) = Mp(l - p).
</p>
<p>o
The characteristic function for the other important PMFs are given in Table 6.1.
</p>
<p>Some important properties of the characteristic function are listed next.
</p>
<p>Property 6.3 - Characteristic function always exists since l&lt;/&gt;x(w)1 &lt; 00
Proof:
</p>
<p>l&lt;/&gt;x(w)1
</p>
<p>00
</p>
<p>L px[k] exp(jwk)
k= - oo
</p>
<p>00
</p>
<p>&lt; L Ipx[k] exp (j wk)I
k= - oo
</p>
<p>00
</p>
<p>L Ipx [k]1
k=-oo
</p>
<p>00
</p>
<p>L px[k]
k=-oo
</p>
<p>1.
</p>
<p>(magnitude of sum of complex numbers
</p>
<p>cannot exceed sum of magnitudes)
</p>
<p>(I exp(jwk)I = 1)
</p>
<p>(Px[k] ~ 0)
</p>
<p>o
</p>
<p>Property 6.4 - Characteristic function is periodic with period 27f.
</p>
<p>Proof: For m an integer
</p>
<p>00
</p>
<p>&lt;/&gt;x(w + 27fm) = L px [k] exp[j(w + 27fm)k]
k=-oo
</p>
<p>00
</p>
<p>L px[k] exp[jwk] exp[j27fmk]
k= -oo
</p>
<p>00
</p>
<p>L px [k]exp[jwk]
k= - oo
</p>
<p>= &lt;/&gt;x(w).
</p>
<p>(since exp(j27fmk) = 1
</p>
<p>for mk an integer)
</p>
<p>o</p>
<p/>
</div>
<div class="page"><p/>
<p>152 CHAPTER 6. EXPECTED VALUES FOR DISCRETE RAND. VAR.
</p>
<p>Property 6.5 &ndash; The PMF may be recovered from the characteristic
function.
</p>
<p>Given the characteristic function, we may determine the PMF using
</p>
<p>pX [k] =
</p>
<p>&int; 
</p>
<p>&minus;
X() exp(&minus;jk)
</p>
<p>d
</p>
<p>2
&minus;&infin; &lt; k &lt; &infin;. (6.19)
</p>
<p>Proof: Since the characteristic function is the Fourier transform of a sequence (al-
though its definition uses a +j instead of the usual &minus;j), it has an inverse Fourier
transform. Although any interval of length 2 may be used to perform the integra-
tion in the inverse Fourier transform, it is customary to use [&minus;, ] which results in
(6.19).
</p>
<p>
</p>
<p>Property 6.6 &ndash; Convergence of characteristic functions guarantees
convergence of PMFs.
</p>
<p>This property says that if we have a sequence of characteristic functions, say 
(n)
X (),
</p>
<p>which converges to a given characteristic function, say X(), then the correspond-
</p>
<p>ing sequence of PMFs, say p
(n)
X [k], must converge to a given PMF say pX [k], where
</p>
<p>pX [k] is given by (6.19). The importance of this theorem is that it allows us to
approximate PMFs by simpler ones if we can show that the characteristic functions
are approximately equal. An illustration is given next. This theorem is known as
the continuity theorem of probability. Its proof is beyond the scope of this text but
can be found in [Pollard 2002].
</p>
<p>
</p>
<p>We recall the approximation of the binomial PMF by the Poisson PMF under the
conditions that p &rarr; 0 and M &rarr; &infin; with Mp =  fixed (see Section 5.6). To show
this using the characteristic function approach (based on Property 6.6) we let Xb
denote a binomial random variable. Its characteristic function is from (6.18)
</p>
<p>Xb() = [p exp(j) + (1&minus; p)]M
</p>
<p>and replacing p by /M we have
</p>
<p>Xb() =
</p>
<p>[
</p>
<p>
</p>
<p>M
exp(j) +
</p>
<p>(
</p>
<p>1&minus; 
M
</p>
<p>)]M
</p>
<p>=
</p>
<p>[
</p>
<p>1 +
(exp(j)&minus; 1)
</p>
<p>M
</p>
<p>]M
</p>
<p>&rarr; exp[(exp(j) &minus; 1)] (see Problem 5.15, results are also
valid for a complex variable)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.8. ESTIMATING MEANS AND VARIANCES
</p>
<p>as M -+ 00. For a Poisson random variable X&raquo; we have that
</p>
<p>153
</p>
<p>00 ,\ k
</p>
<p>L exp (-,\) k! exp(jwk)
k=O
</p>
<p>(
_ ' ) ~ ['\ exp(jw)]k
</p>
<p>exp /\ LJ k!
k=O
</p>
<p>exp (- ,\) exp[,\ exp(jw)]
</p>
<p>exp['\ (exp (jw) - 1)].
</p>
<p>(using results from Problem
</p>
<p>5.22 which also hold for a
</p>
<p>complex variable)
</p>
<p>Since cPXb(W) -+ cPxp(w) as M -+ 00, by Property 6.6, we must have that pXb[k]-+
pXp[k] for all k. Hence, under the stated conditions the binomial PMF becomes the
</p>
<p>Poisson PMF as M -+ 00 . This was previously proven by other means in Section
</p>
<p>5.6. Our derivation here though is considerably simpler.
</p>
<p>6.8 Estimating Means and Variances
</p>
<p>As alluded to earlier , an imp ortant aspect of the mean and variance of a PMF is
</p>
<p>that they are easily est imated in practice. We have alrea dy briefly discussed this in
</p>
<p>Chapter 2 where it was demonstrated how to do this with computer simulated data
</p>
<p>(see Example 2.3). We now cont inue that discussion in more detail. To illustrate
</p>
<p>the approach we will consider the PMF shown in Figure 6.6a. Since the theoretical
</p>
<p>0 .5r---~-~-~-~-~------, 6r---~ --~-~--~-____r,
</p>
<p>0.4
5
</p>
<p>. .
.. . .... .. . . .
</p>
<p>~
~0 .3
</p>
<p>&gt;:l..
</p>
<p>0.2 .
</p>
<p>0 .1 . l &middot;&middot;&middot;&middot;&middot;&middot;...
2 3
</p>
<p>k
</p>
<p>(a) PMF
</p>
<p>4
</p>
<p>'r
5 6
</p>
<p>ff:J
Q) 4
S
o
~ 3
;:j
</p>
<p>o
2
</p>
<p>o
o 10 20 30
</p>
<p>Tri al number
</p>
<p>(b) Simulated dat a
</p>
<p>40 50
</p>
<p>Figure 6.6: PMF and computer generated data used to illustrate estimation of mean
</p>
<p>and variance.</p>
<p/>
</div>
<div class="page"><p/>
<p>154 CHAPTER 6. EXPECTED VALUES FOR DISCRETE RAND. VAR.
</p>
<p>expected value or mean is given by
</p>
<p>5
</p>
<p>E[X] = L kpx[k]
k=1
</p>
<p>then by the relative frequency interpretation of probability we can use the approxi-
</p>
<p>mation
Nk
</p>
<p>px[k];:::: ]Ii
</p>
<p>where Nk is the number of trials in which a k was the outcome and N is the total
</p>
<p>number of trials. As a result , we can estimate the mean by
</p>
<p>5
</p>
<p>EfXj = Lk;k.
k=1
</p>
<p>The "hat" will always denote an estimated quantity. But kNk is just the sum of all
</p>
<p>the k outcomes that appear in the N trials and therefore 2:%=1 kNk is the sum of
all the outcomes in the N trials. Denoting the latter by 2:~1 Xi, we have as our
estimate of the mean
</p>
<p>___ 1 N
</p>
<p>E[X] = - '" Xi
N ~ t=1
</p>
<p>(6.20)
</p>
<p>where Xi is the outcome of the ith trial. Note that we have just reversed our line of
</p>
<p>reasoning used in the introduction to motivate the use of E[X] as the definition of
</p>
<p>the expected value of a random variable. Also, we have previously seen this type of
</p>
<p>estimate in Example 2.3 where it was referred to as the sample mean. It is usually
</p>
<p>denoted by ii , For the data shown in Figure 6.6b we plot the sample mean in Figure---6.7a versus N. Note that as N becomes larger, we have that E[X] -+ 3 = E[X].
The true variance of the PMF shown in Figure 6.6a is computed as
</p>
<p>var(X) E[X2 ] - E2[X]
</p>
<p>5
</p>
<p>Lk2px[k] - E 2[X]
</p>
<p>k=1
</p>
<p>which is easily shown to be var(X) = 1.2. It is estimated as
</p>
<p>and by the same rationale as before we use</p>
<p/>
</div>
<div class="page"><p/>
<p>6.9. REAL-WORLD EXAMPLE - DATA COMPRESSION
</p>
<p>so that our estimate of the variance becomes
</p>
<p>155
</p>
<p>(6.21)
</p>
<p>This estimate is shown in Figure 6.7b as a function of N . Note that as the number of
</p>
<p>6 2
</p>
<p>5 . ........... . . . . . . ....
</p>
<p>1.5 . .. . ....,. . . .
</p>
<p>4
</p>
<p>( ~ 1( ~ 3
:&gt;
</p>
<p>2
0.5 .
</p>
<p>0 0
0 10 20 30 40 50 0 10 20 30 40 50
</p>
<p>N, Number of trials N, Number of trials
</p>
<p>(a) Estimated mean (b) Estimated variance
</p>
<p>Figure 6.7: Estimated mean and variance for computer data shown in Figure 6.6.
</p>
<p>trials increases the estimate of variance converges to the true value of var(X) = 1.2.
</p>
<p>The MATLAB code used to generate the data and estimate the mean and variance
</p>
<p>is given in Appendix 6B. Also, in that appendix is listed the MATLAB subprogram
</p>
<p>PMFdata. m which allows easier generation of the outcomes of a discrete random
</p>
<p>variable. In practice, it is customary to use (6.20) and (6.21) to analyze real-world
</p>
<p>data as a first step in assessing the characteristics of an unknown P MF .
</p>
<p>6.9 Real-World Example - Data Compression
</p>
<p>The digital revolution of the past 20 years has made it commonplace to record and
</p>
<p>store information in a digital format . Such information consists of speech data in
</p>
<p>te lephone transmission, music data stored on compact discs, video data stored on
</p>
<p>digital video discs, and facsimile data, to name but a few. The amount of data
</p>
<p>can become quite large so that it is important to be able to reduce the amount of
</p>
<p>storage required. The process of storage reduction is called data compression. We
</p>
<p>now illustrate how this is done. To do so we simplify the discussion by assuming
</p>
<p>that the data consists of a sequence of the letters A, B, C, D. One could envision
</p>
<p>these letters as representing the chords of a rudimentary musical instrument, for</p>
<p/>
</div>
<div class="page"><p/>
<p>156 CHAPTER 6. EXPECTED VALUES FOR DISCRETE RAND. VAR.
</p>
<p>example. The extension to the entire English alphabet consisting of 26 letters will
</p>
<p>be apparent. Consider a typical sequence of 50 letters
</p>
<p>AAAAAAAAAAABAAAAAAAAAAAAA
</p>
<p>AAAAAACABADAABAAABAAAAAAD.
</p>
<p>To encode these letters for storage we could use the two-bit code
</p>
<p>A -t 00
</p>
<p>B -t 01
</p>
<p>C -t 10
</p>
<p>D -t 11
</p>
<p>(6.22)
</p>
<p>(6.23)
</p>
<p>which would then require a storage of 2 bits per letter for a total storage of 100
</p>
<p>bits. However, as seen above the typical sequence is characterized by a much larger
</p>
<p>probability of observing an "A" as opposed to the other letters. In fact, there are
</p>
<p>43 A's, 4 B's, 1 C, and 2 D's. It makes sense then to attempt a reduction in storage
</p>
<p>by assigning shorter code words to the letters that occur more often, in this case, to
</p>
<p>the "A" . As a possible strategy, consider the code assignment
</p>
<p>A -t 0
</p>
<p>B -t 10
</p>
<p>C -t 110
</p>
<p>D -t 111. (6.24)
</p>
<p>Using this code assignment for our typical sequence would require only 1 &middot;43 + 2 .
4 + 3 . 1 + 3 . 2 = 60 bits or 1.2 bits per letter. The code given by (6.24) is called
a Huffman code. It can be shown to produce less bits per letter "on the average"
</p>
<p>[Cover, Thomas 1991].
</p>
<p>To determine actual storage savings we need to determine the average length of
</p>
<p>the code word per letter. First we define a discrete random variable that measures
</p>
<p>the length of the code word . For the sample space S = {A, B, C, D} we define the
random variable
</p>
<p>{
</p>
<p>I 81 = A
</p>
<p>X(8i) = 2 82 = B
3 83 = C
</p>
<p>3 84 = D
</p>
<p>which yields the code length for each letter. The probabilities used to generate the
</p>
<p>sequence of letters shown in (6.22) are P[A] = 7/8, P[B] = 1/16, P[e] = 1/32,
P[D] = 1/32. As a result the PMF for X is
</p>
<p>px[k] = { ::
</p>
<p>16
</p>
<p>k=l
</p>
<p>k=2
</p>
<p>k = 3.</p>
<p/>
</div>
<div class="page"><p/>
<p>REFERENCES 157
</p>
<p>The average code length is given by
</p>
<p>3
</p>
<p>E[X] = 'Lkpx[k]
k=l
</p>
<p>7 1 1
1 . S+ 2 . 16 + 3 . 16
1.1875 bits per letter.
</p>
<p>(6.25)bits per letter.
</p>
<p>This results in a compression ratio of 2: 1.1875 = 1.68 or we require about 40% less
storage.
</p>
<p>It is also of interest to note that the average code word length per letter can be
</p>
<p>reduced even further. However, it requires more complexity in coding (and of course
</p>
<p>in decoding). A fundamental theorem due to Shannon, who in many ways laid the
</p>
<p>groundwork for the digital revolution, says that the average code word length per
</p>
<p>letter can be no less than [Shannon 1948]
</p>
<p>4 1
H = 'LP[Si] log2 -[-.]
</p>
<p>i=l P S1
</p>
<p>This quantity is termed the entropy of the source. In addition, he showed that a
</p>
<p>code exists that can attain, to within any small deviation, this minimum average
</p>
<p>code length. For our example, the entropy is
</p>
<p>H
7 11 11 11 1
Slog2 7/8 + 16 log2 1/16 + 32 log2 1/32 + 32 log2 1/32
</p>
<p>= 0.7311 bits per letter.
</p>
<p>bits per letter.
</p>
<p>Hence , the potential compression ratio is 2 : 0.7311 = 2.73 for about a 63% reduc-
tion.
</p>
<p>Clearly, it is seen from this example that the amount of reduction will depend
</p>
<p>critically upon the probabilities of the letters occuring. If they are all equally likely
</p>
<p>to occur, then the minimum average code length is from (6.25) with P[Si] = 1/4
</p>
<p>H = 4 (~log2 1~ 4) = 2
In this case no compression is possible and the original code given by (6.23) will be
</p>
<p>optimal. The interested reader should consult [Cover and Thomas 1991] for further
</p>
<p>details.
</p>
<p>References
</p>
<p>Cover , T.M. , J.A. Thomas, Elements of Information Theory, John Wiley &amp; Sons,
</p>
<p>New York, 1991.</p>
<p/>
</div>
<div class="page"><p/>
<p>158 CHAPTER 6. EXPECTED VALUES FOR DISCRETE RAND. VAR.
</p>
<p>Gaughan, E.D. , Introduction to Analysis, Brooks/Cole, Monterey, CA, 1975.
</p>
<p>Jackson, L.B. , Signals , Systems , and Transforms, Addison-Wesley, Reading, MA,
</p>
<p>1991.
</p>
<p>Pollard., D. A User 's Guide to Measure Theoretic Probability, Cambridge Univer-
</p>
<p>sity Press, New York, 2002.
</p>
<p>Shannon, C.E. , "A Mathematical Theory of Communication," Bell System Tech.
</p>
<p>Journal, Vol. 27, pp. 379-423, 623-656, 1948.
</p>
<p>Problems
</p>
<p>6.1 (w) The center of mass of a system of masses situated on a line is the point at
</p>
<p>which the system is balanced. That is to say that at this point the sum of
</p>
<p>the moments, where the moment is the distance from center of mass times the
</p>
<p>mass , is zero. If the center of mass is denoted by CM, then
</p>
<p>M
</p>
<p>2:)Xi - CM)mi = 0
i= l
</p>
<p>where Xi is the position of the ith mass along the x direction and m i is its
</p>
<p>corresponding mass. First solve for CM. Then, for the system of weights
</p>
<p>shown in Figure 6.8 determine the center of mass. How is this analogous to
</p>
<p>the expected value of a discrete random variable?
</p>
<p>10 kg 10 kg 10 kg 10 kg
</p>
<p>center of mass
</p>
<p>20 x (meters)
</p>
<p>k = 0,1 , . . . ,9
</p>
<p>Figure 6.8: Weightless bar supporting four weights.
</p>
<p>6.2 C:.:,) (f) For the discrete random variable with PMF
</p>
<p>1
px[k] = 10
</p>
<p>find the expected value of X.
</p>
<p>6.3 (w) A die is tossed. The probability of obtaining a I , 2, or 3 is the same. Also,
</p>
<p>the probability of obtaining a 4, 5, or 6 is the same. However, a 5 is twice as
</p>
<p>likely to be observed as a 1. For a large number of tosses what is the average
</p>
<p>value observed?</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 159
</p>
<p>6.4 L_:J (f) A coin is tossed with the probability of heads being 2/3. A head is
mapped into X = 1 and a tail into X = 0. What is the expected outcome of
this experiment?
</p>
<p>6.5 (f) Determine the expected value of a Poisson random variable. Hint: Differ-
</p>
<p>entiate 'Lr'=o &gt;..k /k! with respect to x.
</p>
<p>6.6 (t) Consider the PMF px[k] = (2/7r)/k 2 for k = ... , -1 ,0,1 , .... The expected
value is defined as
</p>
<p>00
</p>
<p>E[X] = L kpx[k]
k=-oo
</p>
<p>which is actually shorthand for
</p>
<p>Nu
</p>
<p>E[X] = N l ~ ~oo I: kpx[k]
NU-&gt;oo k=NL
</p>
<p>where the Land U represent "lower" and "upper" , respectively. This may be
</p>
<p>written as
-1 Nu
</p>
<p>E[X] = lim I: kpx[k] + lim I: kpx[k]
NL -t -OO N u-too
</p>
<p>k=NL k=l
</p>
<p>where the limits are taken independently of each other. For E[X] to be un-
</p>
<p>ambiguous and finite both limits must be finite. As a result , show that the
</p>
<p>expected value for the given PMF does not exist. If, however , we were to con-
</p>
<p>strain NL = Ni] , show that the expected value is zero. Note that if NL = Nij ,
</p>
<p>we are reordering the terms before performing the sum since the partial sums
</p>
<p>become 'Lk=-l kpx[k], ' L ~ = - 2 kpx[k], etc. But for the expected value to be
unambiguous, the value should not depend on the ordering. If a sum is abso-
</p>
<p>lutely summable, any ordering will produce the same result [Gaughan 1975],
</p>
<p>hence our requirement for the existence of the expected value.
</p>
<p>6.7 (t) Assume that a discrete random variable takes on the values k = ... , -1, 0, 1, ...
and that its PMF satisfies px[m + i] = px[m - i], where m is a fixed integer
and i = 1,2, .... This says that the PMF is symmetric about the point x = m.
Prove that the expected value of the random variable is E[X] = m.
</p>
<p>6.8 L...:,) (t) Give an example where the expected value of a random variable is not
its most probable value.
</p>
<p>6.9 (t) Give an example of two PMFs that have the same expected value.
</p>
<p>6.10 (f) A discrete random variable X has the PMF px[k] = 1/5 for k = 0, 1,2,3,4.
If Y = sin[(7r/2)X], find E[Y] using (6.4) and (6.5). Which way is easier?</p>
<p/>
</div>
<div class="page"><p/>
<p>160 CHAPTER 6. EXPECTED VALUES FOR DISCRETE RAND. VAR.
</p>
<p>6.11 (t) Prove the linearity property of the expectation operator
</p>
<p>where al and a2 are constants.
</p>
<p>6.12 L..:...) (f) Determine E[X2 ] for a geom(p) random variable using (6.5). Hint:
You will need to differentiate twice.
</p>
<p>px[k] =
</p>
<p>6.13 (..:...:...) (t) Can E[X2 ] ever be equal to E2[X]? If so, when?
</p>
<p>6.14 (..:...:...) (w) A discrete random variable X has the PMF
</p>
<p>k k = 1
~ k = 2
</p>
<p>t k = 3
k k = 4.
</p>
<p>If the experiment that produces a value of X is conducted, find the minimum
</p>
<p>mean square error predictor of the outcome. What is the minimum mean
</p>
<p>square error of the predictor?
</p>
<p>6.15 (..:...:...) (c) For Problem 6.14 use a computer to simulate the experiment for
</p>
<p>many trials. Compare the estimate to the actual outcomes of the computer
</p>
<p>experiment. Also, compute the minimum mean square error and compare it
</p>
<p>to the theoretical value obtained in Problem 6.14.
</p>
<p>6.16 (w) Of the three PMFs shown in Figure 6.9, which one has the smallest vari-
ance? Hint: You do not need to actually calculate the variances.
</p>
<p>2 k 4-2
rr l
</p>
<p>0.6
</p>
<p>~0 .5
</p>
<p>~04
0.3
</p>
<p>0.2
</p>
<p>0.1
</p>
<p>0.7
0.6f &middot; &middot; &middot; &middot; &middot; &middot; &middot; : . &bull;
</p>
<p>0 .7h=~== ===""l
</p>
<p>~0 .5
</p>
<p>~04f &middot; &middot; . &middot; &middot; &middot; &middot; &middot; &middot; ; &middot; &middot; &middot; &middot; &middot; &middot; &middot; ; &middot; &middot;, &middot; &middot; . &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; ; &middot; &middot;
</p>
<p>0.3
</p>
<p>2 k 4-2
</p>
<p>7
</p>
<p>6
</p>
<p>4
</p>
<p>2
</p>
<p>1
</p>
<p>I
...
</p>
<p>I
0
</p>
<p>o.
</p>
<p>~05
</p>
<p>~o
0.3
</p>
<p>o.
</p>
<p>o.
</p>
<p>o.
</p>
<p>(a) (b) (c)
</p>
<p>Figure 6.9: PMFs for Problem 6.16.
</p>
<p>6.17 (w) If Y = aX + b, what is the variance of Y in terms of the variance of X?</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 161
</p>
<p>6 .18 (f) Find the variance of a Poisson random variable. See the hint for Problem
</p>
<p>6.12.
</p>
<p>6 .19 (f) For the PMF given in Problem 6.2 find the variance.
</p>
<p>6.20 C : ~ J (f) Find the second moment for a Poisson random variable by using the
</p>
<p>characterist ic fun ction, which is given in Table 6.1.
</p>
<p>6.21 (t) If X is a discrete random variable and c is a constant, prove the following
</p>
<p>properties of the variance:
</p>
<p>var(c)
</p>
<p>var(X + c) =
var(cX)
</p>
<p>&deg;var(X)
c2var(X).
</p>
<p>6. 22 (t) If a discret e random variable X has var(X) = 0, prove that X must be
</p>
<p>a constant c. This provides a converse to the property that if X = c, then
</p>
<p>var(X) = 0.
</p>
<p>6 .23 (t) In this problem we prove that if E[XS] exists, meaning that E [!X IS] &lt; 00,
then E[XT ] also exists for 0&lt; r &lt; s. Provide the explanations for the following
steps:
</p>
<p>a. For Ixl ~ 1, Ixl T ~ 1
</p>
<p>b . For Ixl &gt; 1, [z]" ~ Ixls
</p>
<p>c. For all [z] , Ixl T ~ Ixl s + 1
d. E[ IXn = L:i IXiITpx[xi] ~ L:i( lxi IS+ 1)pX[XiJ= E[lX IS] + 1 &lt; 00.
</p>
<p>6.24 (f) If a discrete random variable has the PMF px[k] = 1/4 for k = -1 and
</p>
<p>p X [k] = 3/4 for k = 1, find the mean and variance.
</p>
<p>6 .25 (t) A symmetric PMF satisfies the relationship px[-k] = px[k] for k =
... , -1 ,0, 1,.. .. P rove that all the odd order moments, E[Xn ] for n odd,
</p>
<p>are zero .
</p>
<p>6 .26 C : ~ J (t ) A central moment of a discrete random variable is defined as
E[(X - E[x])n], for n a posi tive int eger. Derive a formu la that relates the
</p>
<p>central moment to the usual moments. Hint: You will need the binomial
</p>
<p>formula.
</p>
<p>6.2 7 C.':"' ) (t) If Y = aX + b, find the characteristic function of Y in terms of that
for X. Next use your result to prove that E[Y] = aE[X] + b.
</p>
<p>6.28 C..:... ) (f) Find the characteristic funct ion for the P MF px [k] = 1/5 for k =
-2, -1, 0,1 ,2.</p>
<p/>
</div>
<div class="page"><p/>
<p>162 CHAPTER 6. EXPECTED VALUES FOR DISCRETE RAND. VAR.
</p>
<p>6.29 (f) Determine the variance of a binomial random variable by using the prop-
</p>
<p>erties of the characteristic function. You can assume knowledge of the char-
</p>
<p>acteristic function for a binomial random variable.
</p>
<p>6.30 (f) Determine the mean and variance of a Poisson random variable by using
</p>
<p>the properties of the characteristic function. You can assume knowledge of
</p>
<p>the characteristic function for a Poisson random variable.
</p>
<p>6.31 (f) Which PMF px[k] for k = ... ,-1,0,1, ... has the characteristic function
</p>
<p>&lt;Px(w) = cosw?
</p>
<p>6.32 c.:..:..-) (c) For the random variable described in Problem 6.24 perform a com-
puter simulation to estimate its mean and variance. How does it compare to
</p>
<p>the true mean and variance?</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 6A
</p>
<p>Derivation of E[g(X)] Formula
</p>
<p>Assume that X is a discrete random variable taking on values in Sx = {Xl,X2, &bull;&bull; .}
with PMF PX[Xi]. Then, if Y = g(X) we have from the definit ion of expected value
</p>
<p>(6A. l)
</p>
<p>(6A.2)
</p>
<p>where the sum is over all Yi E Sy. Note that it is assumed that the Yi are distinct
</p>
<p>(all different). But from (5.9)
</p>
<p>PY[Yi] = L pX [Xj].
{Xj :g(Xj )=Yi}
</p>
<p>To simplify the notation we will define the indicator junction , which indicates
</p>
<p>whether a number X is within a given set A , as
</p>
<p>{
I x E A
</p>
<p>I A (x) = 0 otherwise.
</p>
<p>T hen (6A.2) can be rewritten as
</p>
<p>00
</p>
<p>PY[Yi] = L px[xj]I{o}(Yi - g(Xj))
j=l
</p>
<p>since the sum will include the term pX [Xj ] only if Yi - g(Xj) = O. Using this, we
have from (6A.l)
</p>
<p>00
</p>
<p>E [Y] L Yi L px [xj]I{o}(Yi - g(Xj))
j=l
</p>
<p>~ ~ [ ~ Y i I { O } ( Y i - g(Xj))] pX[XjJ.</p>
<p/>
</div>
<div class="page"><p/>
<p>164 CHAPTER 6. EXPECTED VALUES FOR DISCRETE RAND. VAR.
</p>
<p>Now for a given i , g(Xj) is a fixed number and since the Yi'S are distinct, there is
only one Yi for which Yi = g(Xj). Thus, we have that
</p>
<p>LYiI{O}(Yi - g(Xj)) = g(Xj)
i
</p>
<p>and finally
00
</p>
<p>E[Y] = E[g(X)] = Lg(Xj)px[Xj].
j=l</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 6B
</p>
<p>MATLAB Code Used to
</p>
<p>Estimate Mean and Variance
</p>
<p>Figures 6.6 and 6.7 are based on the following MATLAB code.
</p>
<p>This program generates the outcomes for N trials
</p>
<p>of an experiment for a discrete random variable.
</p>
<p>Uses the method of Section 5.9.
</p>
<p>It is a function subprogram.
</p>
<p>- number of trials desired
</p>
<p>values of x_i's of discrete random variable (M x 1 vector)
</p>
<p>- PMF of discrete random variable (M x 1 vector)
</p>
<p>- outcomes of N trials (N x 1 vector)x
</p>
<p>pX
</p>
<p>xi
</p>
<p>N
</p>
<p>'I. PMFdata.m
</p>
<p>'I.
'I.
'I.
'I.
'I.
'I.
'I. Input parameters:
</p>
<p>'I.
'I.
'I.
'I.
'I.
'I. Output parameters:
</p>
<p>'I.
'I.
'I.
function x=PMFdata(N,xi,pX)
</p>
<p>M=length(xi);M2=length(pX) ;
</p>
<p>if W=M2
</p>
<p>message='xi and pX must have the same dimension'
</p>
<p>end
</p>
<p>for k=1:M ; 'I. see Section 5.9 and Figure 5.14 for approach used here
if k==1</p>
<p/>
</div>
<div class="page"><p/>
<p>166 CHAPTER 6. EXPECTED VALUES FOR DISCRETE RAND. VAR.
</p>
<p>bin(k,l)=pX(k); %set up first interval of COF as [O,pX(l)]
</p>
<p>else
</p>
<p>bin(k,l)=bin(k-l,l)+pX(k); %set up succeeding intervals
%of COF
</p>
<p>end
</p>
<p>end
</p>
<p>u=rand(N,l); %generate N outcomes of uniform random variable
for i=l :N %determine which interval of COF the outcome lies in
</p>
<p>%and map into value of xi
if u(i&raquo;O&amp;u(i)&lt;=bin(l)
</p>
<p>x Ci, l)=xi(U ;
end
</p>
<p>for k=2:M
</p>
<p>if u(i&raquo;bin(k-l)&amp;u(i)&lt;=bin(k)
</p>
<p>x(i, l)=xiCk) ;
</p>
<p>end
</p>
<p>end
</p>
<p>end</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 7
</p>
<p>Multiple Discrete Random
</p>
<p>Variables
</p>
<p>7 .1 Introduction
</p>
<p>In Chapter 5 we introduced the concept of a discrete random variable as a mapping
</p>
<p>from the sample space S = {sil to a countable set of real numbers (either finite
or countably infinite) via a mapping X(Si)' In effect, the mapping yields useful
</p>
<p>numerical information about the outcome of the random phenomenon. In some
</p>
<p>instances, however, we would like to measure more than just one attribute of the
</p>
<p>outcome. For example, consider the choice of a student at random from a population
</p>
<p>of college students. Then, for the purpose of assessing the student's health we might
</p>
<p>wish to know his/her height, weight, blood pressure, pulse rate, etc. All these
</p>
<p>measurements and others are used by a physician for a disease risk assessment.
</p>
<p>Hence, the mapping from the sample space of college students to the important
</p>
<p>measurements of height and weight , for example, would be H(Si) = hi and W(Si) =
Wi , where Hand W represent the height and weight of the student selected. In Table
</p>
<p>4.1 we summarized a hypothetical set of probabilities for heights and weights. The
</p>
<p>table is a two-dimensional array that lists the probabilities P[H = hi and W = Wj].
This information can also be displayed in a three-dimensional format as shown in
</p>
<p>Figure 7.1, where we have associated the center point of each interval of height and
</p>
<p>weight given in Table 4.1 with the probability displayed. These probabilities were
</p>
<p>termed joint probabilities. In this chapter we discuss the case of multiple random
</p>
<p>variables. For example, the height and weight could be represented as a 2 x 1 random
</p>
<p>vector
</p>
<p>and as such, its value is located in the plane (also called R 2 ) . We will initially
</p>
<p>describe the simplest case of two random variables but all concepts are easily ex-</p>
<p/>
</div>
<div class="page"><p/>
<p>168 CHAPTER 7. MULTIPLE DISCRETE RANDOM VARIABLES
</p>
<p>80
</p>
<p>.. . . .. . . ..
</p>
<p>70 75
</p>
<p>Height (inches)
6560
</p>
<p>~ 0.15 .
</p>
<p>~
.s
~ 0.1
o....
</p>
<p>0..
</p>
<p>00: . ...1.
1
.&bull;.1.
</p>
<p>1
</p>
<p>II &middot;&middot; ...&bull; ..
Weights ; : ~ ~ ' l r l &middot; &middot; &middot; r &middot;1.&middot; .&middot;.1&middot;&middot;.&middot;&middot;....t&middot;......
</p>
<p>150 . &bull; &bull; . &bull;
</p>
<p>Figure 7.1: Joint probabilities for heights and weights of college students.
</p>
<p>tended to any finite number of random variables (see Chapter 9 for this extension).
</p>
<p>As we will see throughout our discussions, the new and very important concept
</p>
<p>will be the dependencies between the multiple random variables. Questions such
</p>
<p>as "Can we predict a person 's height from his weight?" naturally arise and can be
</p>
<p>addresse d once we extend our description of a single random variable to multiple
</p>
<p>random variables.
</p>
<p>7.2 Summary
</p>
<p>The concept of jointly distributed discrete random vari ables is illustrated in Figure
</p>
<p>7.2. Two random variables can be thought of as a random vector and assigned a joint
</p>
<p>PMF PX,y[Xi ,Yj] as described in Section 7.3, and which has Properties 7.1 and 7.2.
</p>
<p>The joint PMF may be obtained if the probabilities on the original experimental
</p>
<p>sample space is known by using (7.2), and is illustrated in Example 7.1. Once
</p>
<p>the joint PMF is specified, the probability of any event concerning the random
</p>
<p>variables is determined via (7.3). The marginal PMFs of the two random variables,
</p>
<p>which are the probabilities of each random variable taking on its possible values , is
</p>
<p>obtained from the joint PMF using (7.5) and (7.6) . However , the joint PMF is not
</p>
<p>uniquely determined from the marginal PMFs. The joint CnF is defined by (7.7)
</p>
<p>and evaluated using (7.8). It has the usual properties as summarized via Properties
</p>
<p>7.3-7.6. Random variables are defined to be independent if the probabilities of
</p>
<p>all t he joint events can be found as the product of the probabilities of the single
</p>
<p>events. If the random variables are independent , then the joint PMF factors as in
</p>
<p>(7.11). Given a joint PMF, independence can be established by determining if the
</p>
<p>PMF factors. Conversely, if we know the random variables are independent , and</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3. JOINTLY DISTRIBUTED RANDOM VARIABLES 169
</p>
<p>we are given the marginal PMFs, then the joint PMF is found as the product of
</p>
<p>the marginals. The joint PMF of a transformed vector random variable is given by
</p>
<p>(7.12) and illustrated in Example 7.6. The PMF for the sum of two independent
</p>
<p>discrete random variables can be found using (7.22) or via characteristic functions
</p>
<p>using (7.24). The expected value of a function of two random variables is found
</p>
<p>from (7.28). Also, the variance of the sum of two random variables is given by
</p>
<p>(7.33) and involves the covariance, which is defined by (7.34). The interpretation of
</p>
<p>the covariance is given in Section 7.8 and is seen to provide a quantification of the
</p>
<p>knowledge of the outcome of one random variable on the probability of the other.
</p>
<p>Independent random variables have a covariance of zero, but the converse is not
</p>
<p>true. In Section 7.9 linear prediction of one random variable based on observation
</p>
<p>of another random variable is explored. The optimal linear predictor is given by
</p>
<p>(7.41). A variation of this prediction equation results in the important parameter
</p>
<p>called the correlation coefficient (7.43). It quantifies the relationship of one random
</p>
<p>variable with another. However, a nonzero correlation does not indicate a causal
</p>
<p>relationship. The joint characteristic function is introduced in Section 7.10 and
</p>
<p>is defined by (7.45) and evaluated by (7.46). It is shown to provide a convenient
</p>
<p>means of determining the PMF for a sum of independent random variables. In
</p>
<p>Section 7.11 a method to simulate a random vector is described. Also, methods to
</p>
<p>estimate joint PMFs, marginal PMFs, and other quantities of interest are given.
</p>
<p>Finally, in Section 7.12 an application of the methods of the chapter to disease risk
</p>
<p>assessment is described.
</p>
<p>7.3 Jointly Distributed Random Variables
</p>
<p>We consider two discrete random variables that will be denoted by X and Y. As
</p>
<p>alluded to in the introduction, they represent the functions that map an outcome
</p>
<p>of an experiment s, to a value in the plane. Hence, we have the mapping
</p>
<p>for all s, E S. An example is shown in Figure 7.2 in which the experiment consists
of the simultaneous tossing of a penny and a nickel. The outcome in the sample
</p>
<p>space S is represented by a TH, for example, if the penny comes up tails and the
</p>
<p>nickel comes up heads. Explicitly, the mapping is</p>
<p/>
</div>
<div class="page"><p/>
<p>170 CHAPTER 7. MULTIPLE DISCRETE RANDOM VARIABLES
</p>
<p>- - - - - " ' ~ - - _ - L Y
</p>
<p>r---:::::::I.....---It---.. x
</p>
<p>SX,y C R2
</p>
<p>Figure 7.2: Example of mapping for jointly distributed discrete random variables.
</p>
<p>[:] if s, = TT
[ Xls,) ] [ ~ ] if s, = TH
</p>
<p>Y(sd [~ ] if s, = HT
[:] if s, = HH.
</p>
<p>Two random variables that are defined on the same sample space S are said to be
</p>
<p>jointly distributed. In this example, the random variables are also discrete random
</p>
<p>variables in that the possible values (which are actually 2 x 1 vectors) are countable.
</p>
<p>In this case there are just four vector values. These values comprise the sample
</p>
<p>space which is the subset of the plane given by
</p>
<p>SX,Y = { [ : ] , [ : ] , [ ~ ] , [ : ]}
</p>
<p>We can also refer to the two random variables as the single random vector [X YjT,
where T denotes the vector transpose. Hence, we will use the terms multiple random
</p>
<p>variables and random vector interchangeably. The values of the random vector will
</p>
<p>be denoted either by (x, V), which is an ordered pair or a point in the plane, or by
[xyjT, which denotes a two-dimensional vector. These notations will be synonomous.
</p>
<p>The size of the sample space for discrete random variables can be finite or count-
</p>
<p>ably infinite. In the example of Figure 7.2, since X can take on 2 values, denoted</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3. JOINTLY DISTRIB UTED RANDOM VARIABLES 171
</p>
<p>by Nx = 2, and Y can take on 2 values , denoted by Ny = 2, the total number
</p>
<p>of elements in SX ,y is NxNy = 4. More generally, if X can take on values in
Sx = { Xl , X2, &middot; &middot; &middot;, XNx } and Y can take on values in Sy = {YI , Y2 ,. &middot; &middot; ,YNy} , then
the random vector can take on values in
</p>
<p>SX,y = Sx x S y = {(Xi ,Yj) : i = 1,2, .. . , Nx; j = 1,2, . . . , N y }
</p>
<p>for a total of NX,y = Nx Nv values. This is shown in Figure 7.3 for the case of
N x = 4 and Ny = 3. The notation A x B , where A and B are sets, denotes a
cartesian product set. It consists of all ordered pairs (ai , bj) , where ai E A and
</p>
<p>bj E B. If either Sx or Sy is countably infinite, then the random vector will also
</p>
<p>have a countably infinite set of values .
</p>
<p>Y Y
</p>
<p>Y3 &bull; &bull; &bull; &bull;
Y2 &bull; &bull; &bull; &bull; SX ,y = Sx x Sy
</p>
<p>S y
</p>
<p>YI &bull; &bull; &bull; &bull;
X
</p>
<p>Xl X2 X3 X4
</p>
<p>I &bull; &bull; &bull; &bull; &bull; X
Sx
</p>
<p>Figure 7.3: Example of sample space for jointly distributed discrete random vari-
</p>
<p>ables.
</p>
<p>Just as we defined the PMF for a single discrete random variable in Chapter 5
</p>
<p>as PX[Xi] = P [X (s ) = Xi], we can define the joint PMF (or sometimes called the
bivariate PMF) as
</p>
<p>i = 1,2, . . . ,Nx;j = 1, 2, . . . ,Ny.
</p>
<p>Note that the set of all outcomes s for which X(s) = Xi,Y(s) = Yj is t he same as
the set of outcomes for which
</p>
<p>[
X(s) ] = [ Xi ]
Y(s) Yj
</p>
<p>so that for the random vector to have the value [XiYjjT , both X(s) = Xi and Y(s) =
Yj must be satisfied. Thus, the comma used in the statement X(s) = Xi, Y( s) = Yj is
</p>
<p>to be read as "and" . An example of the joint PMF for students ' heights and weights
</p>
<p>is given in Figure 7.1 in which we set X = height and Y = weight and the vertical</p>
<p/>
</div>
<div class="page"><p/>
<p>172 CHAPTER 7. MULTIPLE DISCRETE RANDOM VARIABLES
</p>
<p>axis represents PX,y[Xi,Yj]. To verify that a set of probabilities as in Figure 7.1 can
</p>
<p>be viewed as a joint PMF we need only verify the usual properties of probability.
</p>
<p>Assuming NX and Nv are finite, these are:
</p>
<p>Property 7.1 - Range of values of joint PMF
</p>
<p>i = 1,2, ... , NXij = 1,2, .. . , Nv .
</p>
<p>o
</p>
<p>Property 7.2 - Sum of values of joint PMF
</p>
<p>Nx Ny
</p>
<p>L LPX,y[Xi' Yj] = 1
i=l j=l
</p>
<p>o
and similarly for a countably infinite sample space. For the coin toss example of
</p>
<p>Figure 7.2 we require that
</p>
<p>&deg;:S PX,y[O, 0] :S 1
&deg;:S PX,y[O, 1] :S 1&deg;:S px,y[l, 0] :S 1
&deg;:S px,y[l, 1] :S 1
</p>
<p>and
1 1
</p>
<p>LLPx,Y[i,j] = 1.
i = O j=O
</p>
<p>Many possibilities exist. For two fair coins that do not interact as they are tossed
</p>
<p>(i.e., they are independent) we might assign px,y[i,j] = 1/4 for all i and j. For two
</p>
<p>coins that are weighted but again do not interact with each other as they are tossed,
</p>
<p>we might assign
</p>
<p>{
</p>
<p>(I - p)2 i = O,j = &deg;
. . (1 - p)p i = O,j = 1
</p>
<p>PX,y[z,J] = p(1 - p) i = l,j = &deg;
p2 i = l,j = 1
</p>
<p>if each coin has a probability of heads of p. It is easily shown that the joint PMF
</p>
<p>satisfies Properties 7.1 and 7.2 for any &deg;:s P :s 1. In obtaining these values for
the joint PMF we have used the concept of equivalent events, which allows us to
</p>
<p>determine probabilities for events defined on SX,y from those defined on the original
</p>
<p>sample space S. For example, since the events TH and (0,1) are equivalent as seen</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3. JOINTLY DISTRIBUTED RANDOM VARIABLES
</p>
<p>in Figure 7.2, we have that
</p>
<p>173
</p>
<p>px,y[O,I] = P[X(s) = 0, Y(s) = 1]
</p>
<p>= P[{Si: X(Si) = 0,Y(sd = I}]
</p>
<p>= P[Si = TH]
</p>
<p>(1 - p)p
</p>
<p>(equivalent event in S)
</p>
<p>(mapping is one-to-one)
</p>
<p>(independence)
</p>
<p>where we have assumed independence of the penny and nickel toss subexperiments
</p>
<p>as described in Section 4.6.l.
</p>
<p>In general, the procedure to determine the joint PMF from the probabilities
</p>
<p>defined on S depends on whether the random variable mapping is one-to-one or
</p>
<p>many-to-one. For a one-to-one mapping from S to SX,y we have
</p>
<p>PX,y[Xi, Yj] P[X(s) = Xi, Y(s) = Yj]
= P[{s: X(s) = Xi,Y(s) = Yj}]
</p>
<p>P[{sd] (7.1)
</p>
<p>where it is assumed that Sk is the only solution to X(s) = Xi and X(s) = Yj. For a
many-to-one transformation the joint PMF is found as
</p>
<p>PX,y[Xi,Yj] = L P[{sd]&middot;
{k:X(Sk)=Xi,Y(Sk)=Yj}
</p>
<p>(7.2)
</p>
<p>This is the extension of (5.1) and (5.2) to a two-dimensional random vector. An
</p>
<p>example follows.
</p>
<p>Example 7.1 - Two dice toss with different colored dice
</p>
<p>A red die and a blue die are tossed. The die that yields the larger number of dots
</p>
<p>is chosen. If they both display the same number of dots, the red die is chosen. The
</p>
<p>numerical outcome of the experiment is defined to be 0 if the blue die is chosen and
</p>
<p>1 if the red die is chosen, along with its corresponding number of dots. The random
</p>
<p>vector is therefore defined as
</p>
<p>x {O blue die chosen
1 red die chosen
</p>
<p>Y = number of dots on chosen die.
</p>
<p>The outcomes of the experiment can be represented by (i,j) where i = 0 for blue,
</p>
<p>i = 1 for red, and j is the number of dots observed. What then is px,y[l , 3], for
example? To determine this we first list all outcomes in Table 7.1 for each number of
</p>
<p>dots observed on the red and blue dice. It is seen that the mapping is many-to-one.
</p>
<p>For example, if the red die displays 6 dots, then the outcome is the same, which is
</p>
<p>(1,6), for all possible blue outcomes. To determine the desired value of the PMF,</p>
<p/>
</div>
<div class="page"><p/>
<p>174 CHAPTER 7. MULTIPLE DISCRETE RANDOM VARIABLES
</p>
<p>blue=1 blue=2 blue=3 blue=4 blue=5 blue=6
</p>
<p>red=1 (1,1) (0,2) (0,3) (0,4) (0,5) (0,6)
</p>
<p>red=2 (1,2) (1,2) (0,3) (0,4) (0,5) (0,6)
</p>
<p>red=3 (1,3) (1,3) (1,3) (0,4) (0,5) (0,6)
</p>
<p>red=4 (1,4) (1,4) (1,4) (1,4) (0,5) (0,6)
</p>
<p>red=5 (1,5) (1,5) (1,5) (1,5) (1,5) (0,6)
</p>
<p>red=6 (1,6) (1,6) (1,6) (1,6) (1,6) (1,6)
</p>
<p>Table 7.1: Mapping of outcomes in S to outcomes in SX,Y. The outcomes of (X, Y)
</p>
<p>are (i,j), where i indicates the color of the die with more dots (red=1, blue=O), j
</p>
<p>indicates the number of dots on that die.
</p>
<p>we assume that each outcome in S is equally likely and therefore is equal to 1/36.
</p>
<p>Then, from (7.2)
</p>
<p>=
</p>
<p>pX'y[1 , 3] L P[{Sk}]
{k:X(Sk)=1 ,Y(Sk)=3}
</p>
<p>1
</p>
<p>L 36
{k:X(Sk)=1 ,Y(Sk)=3}
</p>
<p>3 1
= - -
</p>
<p>36 12
</p>
<p>(7.3)
</p>
<p>since there are three outcomes of the experiment in S that map into (1,3). They
</p>
<p>are (red=3,blue=1), (red=3,blue=2), and (red=3,blue=3).
</p>
<p>&lt;:;
In general, as in the case of a single random variable we can use the joint PMF
</p>
<p>to compute probabilities of all events defined on SX,Y = Sx x Sv . For the event
A c SX,y, the probability is
</p>
<p>P[(X, Y) E A] = L PX,y[Xi, Yj].
{(i,j) :(Xi,Yj )EA}
</p>
<p>Once we have knowledge of the joint PMF, we no longer need to retain the underlying
</p>
<p>sample space S of the experiment. All our probability calculations can be made
</p>
<p>concerning values of (X, Y) by using (7.3).
</p>
<p>7.4 Marginal PMFs and CDFs
</p>
<p>If the joint PMF is known, then the PMF for X, i.e., PX[Xi], and the PMF for Y,
</p>
<p>i.e., PY[Yj], can be determined. These are termed the marginal PMFs. Consider
</p>
<p>first the determination of PX[Xi]' Since {X = Xi} does not specify any particular
</p>
<p>value for Y, the event {X = Xi} is equivalent to the joint event {X = Xi, Y E Sy}.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4. MARGINAL PMFS AND CDFS 175
</p>
<p>To determine the probability of the latter event we assume the general case of a
</p>
<p>countably infinite sample space. Then, (7.3) becomes
</p>
<p>P[(X,Y) E A] =
00 00
</p>
<p>L L PX,y[Xi , Yj]&middot; (7.4)
</p>
<p>i = l j=l
</p>
<p>{(i, j ):(X i, Yj )EA}
</p>
<p>Next let A = {xd x Sy , which is illustrated in Figure 7.4 for k = 3. Then, we have
</p>
<p>Y
</p>
<p>Y3 &bull; &bull;
</p>
<p>Y2 &bull; &bull;
</p>
<p>Yl &bull; &bull;
</p>
<p>&bull;
&bull;
</p>
<p>&bull;
</p>
<p>&bull; &bull;&bull;&bull;
&bull; &bull;&bull;&bull;
</p>
<p>&bull; &bull;&bull;&bull;
_ - - + - + - - - + _ ~ I - - _ I - - - _ ~ x
</p>
<p>Figure 7.4: Determination of marginal PMF value PX[X3] from joint PMF
</p>
<p>PX,Y[Xi , Yj] by summing along Y direction.
</p>
<p>P[(X,Y) E {xd x Sy] P[X = Xk, Y E Sy]
</p>
<p>= P[X = Xk]
</p>
<p>PX[Xk]
</p>
<p>so that from (7.4) with i = k only
</p>
<p>00
</p>
<p>PX[Xk] = LPX,Y[Xk, Yj]
j=l
</p>
<p>(7.5)
</p>
<p>and is obtained for k = 3 by summing the probabilities along the column shown
in Figure 7.4. The terminology "marginal" PMF originates from the process of
</p>
<p>summing the probabilities along each column and writing the results in the margin
</p>
<p>(below the x axis) , much the same as the process for computing the marginal prob-
</p>
<p>ability discussed in Section 4.3. Likewise, by summing along each row or in the x
</p>
<p>direction we obtain the marginal PMF for Y as
</p>
<p>00
</p>
<p>PY[Yk] = LPX,y[Xi ,Yk].
i= l
</p>
<p>(7.6)
</p>
<p>In summary, we see that from the joint PMF we can obtain the marginal PMFs.
</p>
<p>Another example follows.</p>
<p/>
</div>
<div class="page"><p/>
<p>176 CHAPTER 7. MULTIPLE DISCRET E RANDOM VARIABLES
</p>
<p>pX,y[i,j] =
</p>
<p>Example 7.2 - Two coin t oss
</p>
<p>As before we toss a penny and a nickel and map the outcomes into a 1 for a head
</p>
<p>and a 0 for a tail. The random vector is (X,Y) , where X is the random variable
</p>
<p>representing the penny outcome and Y is the random variable representing the nickel
</p>
<p>outcome. The mapping is shown in Figure 7.2. Consider the joint PMF
</p>
<p>k i = O,j = &deg;
k i = O,j = 1
i i=l ,j=O
~ i = 1,j = 1.
</p>
<p>Then, the marginal PMFs are given as
</p>
<p>px[i] ~ t,Px,Y[i,j] ~ {
</p>
<p>py[j] ~ t,PX,Y[i,j] ~ {
</p>
<p>1 1 1 . 0
8+8=4 2=
</p>
<p>i + ~ = i i=l
</p>
<p>k+i=i j=O
</p>
<p>k + ~ = i j=1.
</p>
<p>As expected, L : ~ = o p x [ i ] = 1 and L:}=opy[j] = 1. We could also have arranged the
joint PMF and marginal PMF values in a table as shown in Table 7.2. Note that
</p>
<p>j=O j=l px[i]
</p>
<p>i=O 1 1 18 8 4
</p>
<p>i = 1 1 1 34 2 4
</p>
<p>py[j] 3 58 8
</p>
<p>Table 7.2: Joint P MF and marginal PMF values for Examples 7.2 and 7.4.
</p>
<p>the marginal P MFs are found by summing across a row (for px) or a column (for
py) and are written in the "margins".
</p>
<p>Joint P M F cannot be d etermined fr om m arginal P MFs.
</p>
<p>Having obtained the marginal PMFs from the joint PMF, we might suppose we
</p>
<p>could reverse the process to find the joint P MF from the marginal PMFs. However,
</p>
<p>this is not possible in general. To see why, consider the joint PMF summarized in
</p>
<p>Table 7.3. The marginal PMFs are the same as the ones shown in Table 7.2. In</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4. MARGINAL PMFS AND CDFS
</p>
<p>j =O j = l px [i]
</p>
<p>i =O 1 3 116 16 4"
</p>
<p>i = 1 5 7 316 16 4"
</p>
<p>py[j] 3 58" 8"
</p>
<p>Table 7.3: Joint P MF values for "caut ion" example.
</p>
<p>177
</p>
<p>fact, there are an infinite number of joint PMFs that have the same ma rginal PMFs.
</p>
<p>Hence,
</p>
<p>joint PMF =} marginal PMFs
</p>
<p>but
</p>
<p>marginal PMFs =/? joint PMF.
</p>
<p>L1h
A joint cumulative distribution function (CDF) can also be defined for a random
</p>
<p>vecto r. It is given by
</p>
<p>Fx ,Y( x , y) = P [X :s x ,Y :s y]
and can be found exp licitly by summing the joint PMF as
</p>
<p>Fx ,Y( x , y) = L L PX,y[Xi, Yj].
{(i,j) :Xi :-S: x,Yj:-S:Y}
</p>
<p>(7.7)
</p>
<p>(7.8)
</p>
<p>An example is shown in Figure 7.5, along with the joint P MF. The marginal CDFs
</p>
<p>can be easily found from the joint CDF as
</p>
<p>Fx (x )
</p>
<p>Fy (y)
</p>
<p>P[X :s x] = P[X :s x ,Y &lt; 00] = Fx,Y(x, 00)
p ry :s y] = P[X &lt; 00,Y :s y] = Fx ,y (oo, y).
</p>
<p>T he joint CDF has the usual propert ies which are :
</p>
<p>Property 7.3 - Range of values
</p>
<p>o:s Fx ,Y (x ,y) :s 1
o
</p>
<p>P roperty 7.4 - Values at "endpoints"
</p>
<p>Fx,y(-oo, - 00) 0
</p>
<p>Fx,y(oo, 00) 1
</p>
<p>o</p>
<p/>
</div>
<div class="page"><p/>
<p>178 CHAPTER 7. MULTIPLE DISCRETE RANDOM VARIABLES
</p>
<p>Property 7.5 - Monotonically increasing
</p>
<p>Fx,y(x, y) monotonically increases as x and/or y increases.
o
</p>
<p>Property 7.6 - "Right" continuous
</p>
<p>As expected, the joint CDF takes the value after the jump. However, in this case
</p>
<p>the jump is a line discontinuity as seen, for example, in Figure 7.5b. After the jump
</p>
<p>means as we move in the northeast direction in the x-y plane.
</p>
<p>.1
1
</p>
<p>:--1
</p>
<p>1
:.:.:.::
</p>
<p>" ' , " , ".
</p>
<p>. .
</p>
<p>~1. ~ . ..
</p>
<p>;:...
</p>
<p>~O .5 . . . . .
</p>
<p>o
4
</p>
<p>J 2
</p>
<p>o 0
</p>
<p>. .. .. . . . . ...... . . . . .
</p>
<p>. : .
</p>
<p>2
</p>
<p>(a) Joint PMF
</p>
<p>3 4
</p>
<p>'""&pound;1
~
</p>
<p>;:...
</p>
<p>~O .5~ . . .
</p>
<p>y
</p>
<p>x
</p>
<p>(b) Joint CDF
</p>
<p>4
</p>
<p>Figure 7.5: Joint PMF and corresponding joint CDF.
</p>
<p>o
The reader is asked to verify some of these properties in Problem 7.17. Finally, to
</p>
<p>recover the PMF we can use
</p>
<p>The reader should verify this formula for the joint CDF shown in Figure 7.5b. In
</p>
<p>particular, consider the joint PMF at the point (Xi,Yj) = (2,2) to see why we need
</p>
<p>four terms.
</p>
<p>7.5 Independence of Multiple Random Variables
</p>
<p>Consider the experiment of tossing a coin and then a die. The outcome of the coin
</p>
<p>toss is denoted by X and equals 0 for a tail and 1 for a head. The outcome for the
</p>
<p>die is denoted by Y, which takes on the usual values 1,2,3,4,5,6. In determining</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5. INDEPENDENCE OF MULTIPLE RANDOM VARIABLES 179
</p>
<p>the probability of the random vector (X,Y) taking on a value , there is no reason
</p>
<p>to believe that the probability of Y = Yj should depend on the outcome of the coin
toss. Likewise, the probability of X = Xi should not depend on the outcome of the
</p>
<p>die toss (especially since the die toss occurs at a later time). We expect that these
</p>
<p>two events are independent. The formal definition of independent random variables
</p>
<p>X and Y is that they are independent if all the joint events on SX,Y are independent.
</p>
<p>Mathematically X and Yare independent random variables if for all events A C Sx
and B C Sy
</p>
<p>P[X E A,Y E B] = P[X E A]P[Y E B]. (7.10)
</p>
<p>The probabilities on the right-hand-side of (7.10) are defined on Sx and Sy, respec-
</p>
<p>tively (see Figure 7.3 for an example of the relationship of Sx ,Sy to SX,y). The
utility of the independence property is that the probabilities of joint events may
</p>
<p>be reduced to probabilities of "marginal events" (defined on Sx and Sy), which
are always easier to determine. Specifically, if X and Yare independent random
</p>
<p>variables, then it follows from (7.10) that
</p>
<p>(7.11)
</p>
<p>as we now show. If A
</p>
<p>becomes
</p>
<p>{Xi} and B = {Yj} , then the left-hand-side of (7.10)
</p>
<p>P[X E A ,Y E B] P[X = Xi, Y = Yj]
</p>
<p>= PX,y[Xi ,Yj]
</p>
<p>and the right-hand-side of (7.10) becomes
</p>
<p>P[X E A]P[Y E B] = PX[Xi]Py[Yj].
</p>
<p>Hence , if X and Yare independent random variables, the joint PMF factors into
</p>
<p>the product of the marginal PMFs. Furthermore, the converse is true-if the joint
</p>
<p>PMF factors, then X and Yare independent. To prove the converse assume that
</p>
<p>the joint PMF factors according to (7.11). Then for all A and B we have
</p>
<p>P[X E A, Y E B] = ~ ~ PX,y[Xi, Yj]
{i :XiEA} {j:YjEB}
</p>
<p>= ~ ~ PX[Xi]PY[Yj]
{i:XiEA} {j:YjEB}
</p>
<p>~ PX[Xi] ~ PY[Yj]
{i:XiEA} {j :YjEB}
</p>
<p>P[X E A]P[Y E B].
</p>
<p>(from (7.3))
</p>
<p>(assumption)
</p>
<p>We now illustrate the concept of independent random variables with some examples.</p>
<p/>
</div>
<div class="page"><p/>
<p>180 CHAPTER 7. MULTIPLE DISCRETE RANDOM VARIABLES
</p>
<p>Example 7.3 - Two coin toss - independence
</p>
<p>Assume that we toss a penny and a nickel and that as usual a tail is mapped into
</p>
<p>a 0 and a head into a 1. If all outcomes are equally likely or equivalently the joint
</p>
<p>PMF is given in Table 7.4, then the random variables must be independent. This is
</p>
<p>j=O j = 1 px[i]
</p>
<p>i=O 1 1 14" 4" 2"
</p>
<p>i = 1 1 1 14" 4" 2"
</p>
<p>py[j] 1 12" 2"
</p>
<p>Table 7.4: Joint PMF and marginal PMF values for Example 7.3.
</p>
<p>because we can factor the joint PMF as
</p>
<p>px,Y[i,j] = (~) (~) = px[i]py[j]
</p>
<p>for all i and j for which px,y[i,j] is nonzero. Furthermore, the marginal PMFs
</p>
<p>indicate that each coin is fair since px[O] = px[l] = 1/2 and py(O] = py(l] = 1/2.
</p>
<p>o
</p>
<p>Example 7.4 - Two coin toss - dependence
</p>
<p>Now consider the same experiment but with a joint PMF given in Table 7.2. We
</p>
<p>see that PX,y[O,O] = 1/8 =I- (1/4)(3/8) = px[O]py[O] and hence X and Y cannot
be independent. If two random variables are not independent, they are said to be
dependent.
</p>
<p>Example 7.5 - Two coin toss - dependent but fair coins
</p>
<p>Consider the same experiment again but with the joint PMF given in Table 7.5.
</p>
<p>Since PX,y[O, 0] = 3/8 =I- (1/2)(1/2) = px[O]py[O] , X and Yare dependent. However,
</p>
<p>j=O j=1 px[i]
</p>
<p>i=O 3 1 18" 8" 2"
</p>
<p>i = 1 1 3 18" 8" 2"
</p>
<p>py[j] 1 12" 2"
</p>
<p>Table 7.5: Joint PMF and marginal PMF values for Example 7.5.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.6. TRANSFORMATIONS OF MULTIPLE RANDOM VARIABLES 181
</p>
<p>by examining the marginal PMFs we see that the coins are in some sense fair since
</p>
<p>P[heads] = 1/2, and therefore we might conclude that the random variables were
</p>
<p>independent. This is incorrect and underscores the fact that the marginal PMFs
</p>
<p>do not tell us much about the joint PMF. The joint PMF of Table 7.4 also has the
</p>
<p>same marginal PMFs but there X and Y were independent.
</p>
<p>&lt;)
</p>
<p>Finally, note that if the random variables are independent, the joint CDF factors
</p>
<p>as well. This is left as an exercise for the student (see Problem 7.20). Intuitively, if
</p>
<p>X and Yare independent random variables, then knowledge of the outcome of X
</p>
<p>does not change the probabilities of the outcomes of Y. This means that we cannot
</p>
<p>predict Y based on knowing that X = X i. Our best predictor of Y is just E[Y], as
</p>
<p>described in Example 6.3. When X and Yare dependent , however, we can improve
</p>
<p>upon the predictor E[Y] by using the knowledge that X = Xi. How we actually do
</p>
<p>this is described in Section 7.9.
</p>
<p>7.6 Transformations of Multiple Random Variables
</p>
<p>In Section 5.7 we have seen how to find the PMF of Y = g(X) if the PMF of X is
given. It is determined using
</p>
<p>PY[Yi] = L pX[Xj].
{j :g(Xj )=Yi}
</p>
<p>We need only sum the probabilities of the xi's that map into Vi. In the case of two
</p>
<p>discrete random variables X and Y that are transformed into W = g(X, Y) and
</p>
<p>Z = h(X, Y) , we have the similar result
</p>
<p>i = 1,2, ... .Nw; j = 1,2, ... ,Nz
</p>
<p>(7.12)
</p>
<p>where Nw and/or Nz may be infinite. An example follows.
</p>
<p>Example 7.6 - Independent Poisson random variables
</p>
<p>Assume that the joint PMF is given as the product of the marginal PMFs, where
</p>
<p>each marginal PMF is a Poisson PMF. Then,
</p>
<p>&gt;..k &gt;..1
px,y[k,l] = exp[-(&gt;..x + &gt;..y)] ~zr k = 0,1 , ... j l = 0,1, ... (7.13)
</p>
<p>Note that X rv Pois(&gt;..x) , Y rv Pois(&gt;..y) , and X and Yare independent random
</p>
<p>variables. Consider the transformation
</p>
<p>W
</p>
<p>Z
</p>
<p>g(X,Y) = X
</p>
<p>h(X, Y) = X + Y. (7.14)</p>
<p/>
</div>
<div class="page"><p/>
<p>182 CHAPTER 7. MULTIPLE DISCRETE RANDOM VARIABLES
</p>
<p>The possible values of Ware those of X, which are a, 1, ... , and the possible values
of Z are also a, 1, .... According to (7.12), we need to determine all (k, l) so that
</p>
<p>g(Xk' Yl) ui;
</p>
<p>h(Xk, Yl) = Zj. (7.15)
</p>
<p>But Xk and Yl can be replaced by k and l, respectively, for k = a, 1, ... and l =
a, 1, .... Also, Wi and Zj can be replaced by i and j, respectively, for i = a, 1, ... and
j = a,1, .... The transformation equations become
</p>
<p>g(k, l) i
</p>
<p>h(k, l) j
</p>
<p>which from (7.14) become
</p>
<p>Z k
</p>
<p>J k + l.
</p>
<p>Solving for (k,l) for the given (i,j) desired, we have that k = i and l = j - i ~ a,
which is the only solution. Note that from (7.13) the joint PMF for X and Y is
</p>
<p>nonzero only if l = a, 1, .... Therefore, we must have l ~ a so that l = j - i ~ a.
From (7.12) we now have
</p>
<p>pw,z[i,j]
</p>
<p>00 00
</p>
<p>L L px,y[k, l]
k=OI=O
</p>
<p>{(k,l) :k=i,l=j - i ~ O }
</p>
<p>px,y[i,j - i]u[i]u[j - i] (7.16)
</p>
<p>where urn] is the discrete unit step sequence defined as
</p>
<p>urn] = { ~
</p>
<p>Finally, we have upon using (7.13)
</p>
<p>n= ... ,-2,-1
</p>
<p>n= a,l, ....
</p>
<p>pW,z[i,j] =
Ai Aj - i
</p>
<p>exp[-(AX + Ay)] 'It ! .),u[i]U[j - i]
z. J Z.
'i ,j-i . a 1
</p>
<p>[( ' )]AXAy Z= " ...exp - AX + Ay " ( ' _ ')'
z: J z. j = i, i + 1, ...
</p>
<p>(7.17)
</p>
<p>(7.18)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.6. TRANSFORMATIONS OF MULTIPLE RANDOM VARIABLES 183
</p>
<p>&amp;. Use the discrete unit step sequence to avoid mistakes.
As we have seen in the preceding example, the discrete unit step sequence was
</p>
<p>introduced to designate the region of the w-z plane over which pw,z[i , j) is nonzero.
</p>
<p>A common mistake in problems of this type is to disregard this region and assert
</p>
<p>that the joint PMF given by (7.18) is nonzero over i = 0,1, ... ; j = 0,1 , . ... Note,
however , that the transformation will generally change the region over which the
</p>
<p>new joint PMF is nonzero. It is as important to determine this region as it is to
</p>
<p>find the analytical form of Pw,z. To avoid possible errors it is advisable to replace
</p>
<p>(7.13) at the outset by
</p>
<p>&gt;..k &gt;..1
px,y[k, l) = exp[-(&gt;..x + &gt;..y&raquo;) :!lr u[k)u[l).
</p>
<p>Then, the use of the unit step functions will serve to keep track of the nonzero PMF
</p>
<p>regions before and after the transformation. See also Problem 7.25 for another
</p>
<p>example.
</p>
<p>&amp;.
We sometimes wish to determine the PMF of Z = heX, Y) only, which is a trans-
formation from (X, Y) to Z. In this case, we can use an auxiliary random variable.
That is to say, we add another random variable W so that the transformation be-
</p>
<p>comes a transformation from (X,Y) to (W, Z) as before. We can then determine
</p>
<p>PW,Z[Wi ,Zj) by once again using (7.12), and then tiz , which is the marginal PMF,
can be found as
</p>
<p>pZ[Zj] = L PW,Z[Wi, Zj).
{i :WiESw}
</p>
<p>(7.19)
</p>
<p>As we have seen in the previous example, we will first need to solve (7.15) for Xk and
</p>
<p>YI. To facilitate the solution we usually define a simple auxiliary random variable
</p>
<p>such as W = X.
</p>
<p>Example 7.7 - PMF for sum of independent Poisson random variables
</p>
<p>(continuation of previous example)
</p>
<p>To find the PMF of Z = X + Y from the joint PMF given by (7.13), we use (7.19)
with W = X. We then have Sw = Sx = {O, I, ... } and
</p>
<p>00
</p>
<p>pz[j) = LPw,z[i ,j) (from (7.19&raquo; (7.20)
i=O
</p>
<p>00 &gt;..i &gt;..j-it;exp[-(&gt;..x + &gt;..y&raquo;) i!(j !i)! u[i)u[j - i) (from (7.17&raquo;</p>
<p/>
</div>
<div class="page"><p/>
<p>184 CHAPTER 7. MULTIPLE DISCRETE RANDOM VARIABLES
</p>
<p>j = 0,1, . ...
</p>
<p>pz[j]
</p>
<p>and since uri] = 1 for i = 0,1, ... and u[j - i] = 1 for i = 0,1 , ... .i and u[j - i] = &deg;
for i &gt; i , this reduces to
</p>
<p>j &gt;..i &gt;..j-i
</p>
<p>pz[j] = ~exp[-(&gt;..x + &gt;"Y)]i!(~ !i)!
</p>
<p>Note that Z can take on values j = 0,1 , ... since Z = X + Y and both X and Y
take on values in {O, 1, ... }. To evaluate this sum we can use the binomial theorem
</p>
<p>as follows:
</p>
<p>[ ( ' ')] 1 ~ j! d d-iexp - /IX + /lY 1 LJ ( . _ ')1" /lX/ly
J. i=O J 2 .2.
</p>
<p>1 j (j) . . .
exp[-(&gt;..x + &gt;..y)] j! ~ i &gt;"x&gt;..Fz
</p>
<p>1 .
exp[-(&gt;..x + &gt;..y)] 1 (&gt;..x + &gt;..y)J (use binomial theorem)
</p>
<p>J.
&gt;..j
</p>
<p>exp(-&gt;..)~ (let X = &gt;"x + &gt;..y)
J.
</p>
<p>for j = 0,1, .... This is recognized as a Poisson PMF with X = &gt;"x + &gt;"y. By this
example then, we have shown that if X rv Pois(&gt;..x), Y rv Pois(&gt;..y), and X and
</p>
<p>Yare independent, then X + Y rv Pois(&gt;..x + &gt;..y). This is called the reproducing
PMF property. It is also extendible to any number of independent Poisson random
</p>
<p>variables that are added together.
</p>
<p>&lt;)
</p>
<p>The formula given by (7.20) when we let pW,z[i,j] = px,y[i,j - i] from (7.16) is
valid for the PMF of the sum of any two discrete random variables, whether they
</p>
<p>are independent or not. Summarizing, if X and Yare random variables that take
</p>
<p>on integer values from -00 to +00, then Z = X + Y has the PMF
</p>
<p>00
</p>
<p>pz[j] = L px,y[i,j - i].
i= - oo
</p>
<p>(7.21)
</p>
<p>This result says that we should sum all the values of the joint PMF such that the
</p>
<p>x value, which is i, and the y value, which is j - i, sums to the z value of j. In
</p>
<p>particular, if the random variables are independent, then since the joint PMF must
</p>
<p>factor, we have the result
</p>
<p>00
</p>
<p>pz[j] = L px[i]py[j - i].
i= - oo
</p>
<p>(7.22)
</p>
<p>But this summation operation is a discrete convolution [Jackson 1991]. It is usually
</p>
<p>written succinctly as pz = PX *py, where * denotes the convolution operator. This</p>
<p/>
</div>
<div class="page"><p/>
<p>7.6. TRANSFORMATIONS OF MULTIPLE RANDOM VARIABLES 185
</p>
<p>result suggests that the use of Fourier transforms would be a useful tool since a
</p>
<p>convolution can be converted into a simple multiplication in the Fourier domain.
</p>
<p>We have already seen in Chapter 6 that the Fourier transform (defined with a +j)
</p>
<p>of a PMF px[k] is the characteristic function &cent;x(w) = E[exp(jwX)] . Therefore,
taking the Fourier transform of both sides of (7.22) produces
</p>
<p>&cent;z(w) = &cent;x(w)&cent;y(w)
</p>
<p>and by converting back to the original sequence domain, the PMF becomes
</p>
<p>pz[j] = F- 1 {&cent;x(w)&cent;y(w)}
</p>
<p>(7.23)
</p>
<p>(7.24)
</p>
<p>where F- 1 denotes the inverse Fourier transform. An example follows.
</p>
<p>Example 7.8 - PMF for sum of independent Poisson random variables
</p>
<p>using characteristic function approach
</p>
<p>From Section 6.7 we showed that if X,...., Pois(A), then
</p>
<p>&cent;x(w) = exp [A(exp(jw) - 1)]
</p>
<p>and thus using (7.23) and (7.24)
</p>
<p>pz[j] F- 1 {exp [AX (exp(jw) - 1)] exp [Ay(exp(jw) - I)]}
</p>
<p>F-1 {exp [(AX + Ay)(exp(jw) - I)]} .
</p>
<p>But the characteristic function in the braces is that of a Poisson random variable.
</p>
<p>Using Property 6.5 we see that z,...., Pois(Ax + AY). The use of characteristic func-
tions for the determination of the PMF for a sum of independent random variables
</p>
<p>has considerably simplified the derivation.
</p>
<p>\!
In summary, if X and Yare independent random variables with integer values, then
</p>
<p>the PMF of Z = X + Y is given by
</p>
<p>pz[k] F- 1 {&cent;x(w)&cent;y(w)}
</p>
<p>t' dwJ-1r &cent;x(w)&cent;y(w) exp( -jwk) 21r ' (7.25)
</p>
<p>When the sample space SX,y is finite, it is sometimes possible to obtain the
</p>
<p>PMF of Z = g(X, Y) by a direct calculation, thus avoiding the need to use (7.19).
</p>
<p>The latter requires one to first find the transformed joint PMF Pw,z. To do so we
</p>
<p>1. Determine the finite sample space Sz.
</p>
<p>2. Determine which sample points (Xi,Yj) in SX,y map into each Zk E Sz-
</p>
<p>3. Sum the probabilities of those (Xi,Yj) sample points to yield PZ[Zk] '</p>
<p/>
</div>
<div class="page"><p/>
<p>186 CHAPTER 7. MULTIPLE DISCRETE RANDOM VARIABLES
</p>
<p>Mathematically, this is equivalent to
</p>
<p>PZ[Zk] = L L PX,y[Xi, Yj]&middot;
{( i ,j) :Zk=9(Xi ,Yj)}
</p>
<p>(7.26)
</p>
<p>Px,y[i,j] =
</p>
<p>An example follows.
</p>
<p>Example 7.9 - Direct computation of PMF for transformed random
</p>
<p>variable, Z = g(X, Y)
Consider the transformation of the random vector (X,Y) into the scalar random
</p>
<p>variable Z = X 2 + y 2 . The joint PMF is given by
</p>
<p>i i = O,j = 0
k i=l,j=O
k i = O,j = 1
i i = 1,j = 1.
</p>
<p>To find the PMF for Z first note that (X, Y) takes on the values (i, j) = (0,0), (1,0) ,
(0,1) , (1, 1). Therefore, Z must take on the values Zk = i 2 + j2 = 0,1,2. Then from
(7.26)
</p>
<p>PZ[O] L L pX,y(i,j]
{(i,j) :O=i2 + j2 }
</p>
<p>o 0
</p>
<p>L LPx,y[i,j]
i=O j=O
</p>
<p>3
PX,y[O, 0] = 8
</p>
<p>and similarly
</p>
<p>pz[l]
</p>
<p>pz[2]
</p>
<p>7.7 Expected Values
</p>
<p>2
PX,y[O, 1] +px,y[l , 0] = 8
</p>
<p>3
px,y[l,l] = 8'
</p>
<p>In addition to determining the PMF of a function of two random variables, we
</p>
<p>are frequently interested in the average value of that function. Specifically, if Z =
</p>
<p>g(X, Y), then by definition its expected value is
</p>
<p>E[Z] = L ZiPZ[ziJ. (7.27)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.7. EXPECTED VALUES 187
</p>
<p>To determine E[Z] according to (7.27) we need to first find the PMF of Z and
</p>
<p>then perform the summation. Alternatively, by a similar derivation to that given in
</p>
<p>Appendix 6A, we can show that a more direct approach is
</p>
<p>E[Z] = I:I: g(Xi, Yj)PX,Y[Xi, Yj]&middot;
j
</p>
<p>(7.28)
</p>
<p>To remind us that we are using PX,Y as the averaging PMF, we will modify our
</p>
<p>previous notation from E[Z] to Ex,y[Z], where of course, Z depends on X and Y .
</p>
<p>We therefore have the useful result that the expected value of a function of two
</p>
<p>random variables is
</p>
<p>Ex,y[g(X,Y)] = I:I: g(Xi ,Yj)pX,Y[Xi ,Yj]&middot;
j
</p>
<p>Some examples follow.
</p>
<p>Example 7.10 - Expected value of a sum of random variables
</p>
<p>If Z = g(X,Y) = X + Y, then
</p>
<p>(7.29)
</p>
<p>Ex,y[X + Y] = I:I:(Xi + Yj)pX,Y[Xi, Yj]
j
</p>
<p>I:I:XiPX,y [Xi, Yj] +I:I:YjPX,y[Xi, Yj]
j j
</p>
<p>I:Xi I:PX,y[Xi ,Yj] +I:Yj I:PX,Y[Xi' Yj] (from (7.6))
j j,
</p>
<p>v
</p>
<p>PX[Xi]
</p>
<p>Ex[X] + Ey[Y]
</p>
<p>v
</p>
<p>PY[Yj]
</p>
<p>(definition of expected value).
</p>
<p>Hence , the expected value of a sum of random variables is the sum of the expected
</p>
<p>values. Note that we now use the more descriptive notation Ex [X] to replace E[X]
</p>
<p>used previously.
</p>
<p>Similarly
</p>
<p>Ex,y[aX + bY] = aEx[X] + bEy[Y]
</p>
<p>and thus as we have seen previously for a single random variable, the expectation
</p>
<p>Ex,y is a linear operation.
</p>
<p>Example 7.11 - Expected value of a product of random variables
</p>
<p>If g(X, Y) = XY, then
</p>
<p>Ex,y[XY] = I:I:XiYjPX,y[Xi ,Yj].
i j</p>
<p/>
</div>
<div class="page"><p/>
<p>188 CHAPTER 7. MULTIPLE DISCRETE RANDOM VARIABLES
</p>
<p>We cannot evaluate this further without specifying PX,y. If, however, X and Yare
</p>
<p>independent, then since the joint PMF factors, we have
</p>
<p>Ex,y[XY) 2: 2: XiYjPX [Xi)PY [Yj)
j
</p>
<p>2: XiPX[Xi) 2: YjPY[Yj)
j
</p>
<p>= Ex [X)Ey(Y). (7.30)
</p>
<p>More generally, we can show by using (7.29) that if X and Yare independent, then
</p>
<p>(see Problem 7.30)
</p>
<p>Ex,y(g(X)h(Y)) = Ex [g(X))Ey(h(Y)). (7.31)
</p>
<p>&lt;&gt;
</p>
<p>(definition of variance)
</p>
<p>(from (7.28))
</p>
<p>Example 7.12 - Variance of a sum of random variables
</p>
<p>Consider the calculation of var(X + Y). Then, letting Z = g(X,Y) = (X + Y -
Ex,y[X + y))2 , we have
</p>
<p>var(X + Y)
Ez[Z)
</p>
<p>Ex,y[g(X,Y))
</p>
<p>Ex,y[(X + Y - Ex,y[X + y])2)
= Ex,y[[(X - Ex [X)) + (Y - Ey[Y))f)
</p>
<p>Ex,y[(X - Ex [X]) 2 + 2(X - Ex[X))(Y - Ey[Y))
+ (Y _ E y [y ])2)
</p>
<p>Ex[(X - Ex[X)f] + 2Ex,y((X - Ex[X])(Y - Ey[YJ)]
+ Ey[(Y - Ey[y])2] (linearity of expectation)
</p>
<p>var(X) + 2Ex,y[(X - Ex[X])(Y - Ey[YJ)] + var(Y) (definition of variance)
</p>
<p>where we have also used Ex,y[g(X)] = Ex[g(X)] and Ex,y(h(Y)] = Ey[h(Y)] (see
</p>
<p>Problem 7.28). The cross-product term is called the covariance and is denoted by
</p>
<p>cov(X,Y) so that
</p>
<p>cov(X,Y) = Ex,y[(X - Ex[X])(Y - Ey[YJ)] . (7.32)
</p>
<p>Its interpretation is discussed in the next section. Hence, we finally have that the
</p>
<p>variance of a sum of random variables is
</p>
<p>var(X + Y) = var(X) + var(Y) + 2cov(X, Y). (7.33)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.8. JOINT MOMENTS 189
</p>
<p>Unlike the expected value or mean, the variance of a sum is not in general the sum
</p>
<p>of the variances. It will only be so when cov(X, Y) = O. An alternative expression
</p>
<p>for the covariance is (see Problem 7.34)
</p>
<p>cov(X, Y) = Ex,y[XY] - Ex [X]Ey[Y] (7.34)
</p>
<p>which is analogous to Property 6.1 for the variance.
</p>
<p>7.8 Joint Moments
</p>
<p>Joint PMFs describe the probabilistic behavior of two random variables completely.
</p>
<p>At times it is important to answer questions such as "If the outcome of one random
</p>
<p>variable is a given value, what can we say about the outcome of the other random
</p>
<p>variable? Will it be about the same or have the same magnitude or have no relation-
</p>
<p>ship to the other random variable?" For example, in Table 4.1, which lists the joint
</p>
<p>probabilities of college students having various heights and weights, there is clearly
</p>
<p>some type of relationship between height and weight. It is our intention to quantify
</p>
<p>this type of relationship in a succinct and meaningful way as opposed to a listing
</p>
<p>of probabilities of the various height-weight pairs. The concept of the covariance
</p>
<p>allows us to accomplish this goal. Note from (7.32) that the covariance is a joint
</p>
<p>central moment. To appreciate the information that it can provide we refer to the
</p>
<p>three possible joint PMFs depicted in Figure 7.6. The possible values of each joint
</p>
<p>PMF are shown as solid circles and each possible outcome has a probability of 1/2.
</p>
<p>In Figure 7.6a if X = 1, then Y = 1, and if X = -1, then Y = -1. The relationship
</p>
<p>---+--,f--+--__ X
</p>
<p>y y
</p>
<p>,
-1,1'
</p>
<p>",' -1
</p>
<p>, ,, ,
''''', 1 ,,_'
</p>
<p>, ,, ,, ,
</p>
<p>y
</p>
<p>-1
</p>
<p>-1
</p>
<p>,
</p>
<p>"." 1
,,,
</p>
<p>---+--+.--+--__ X
1
</p>
<p>,,
,.'
</p>
<p>,,,,
,
</p>
<p>-1 ",
,,'- -1
</p>
<p>(a) pX,Y[ -1, -1] =
px,Y[l, 1] = 1/2
</p>
<p>(b) px,y[l, -1]
</p>
<p>PX,Y[ - 1,1] = 1/2
(c) px,y[l,l] =
</p>
<p>px,y[l, -1] = 1/2
</p>
<p>Figure 7.6: Joint PMFs depicting different relationships between the random vari-
</p>
<p>ables X and Y.
</p>
<p>is Y = X. Note, however, that we cannot determine the value of Y until after the
</p>
<p>experiment is performed and we are told the value of X. If X = Xl , then we know
</p>
<p>that Y = X = Xl. Likewise, in Figure 7.6b we have that Y = -X and so if X = Xl,</p>
<p/>
</div>
<div class="page"><p/>
<p>190 CHAPTER 7. MULTIPLE DISCRETE RANDOM VARIABLES
</p>
<p>then Y = -Xl. However, in Figure 7.6c if X = 1, then Y can equal either +1 or
-1. On the average if X = 1, we will have that Y = 0 since Y = &plusmn; 1 with equal
probability. To quantify these relationships we form the product XY, which can
</p>
<p>take on the values +1 , -1, and &plusmn;1 for the joint PMFs of Figures 7.6a, 7.6b, and
</p>
<p>7.6c, respectively. To determine the value of XY on the average we define the joint
</p>
<p>moment as Ex,y[XY]. From (7.29) this is evaluated as
</p>
<p>Ex,y[XY] = LLxiYjPX,Y[Xi ,Yj] .
</p>
<p>j
</p>
<p>(7.35)
</p>
<p>The reader should compare the joint moment with the usual moment for a single
</p>
<p>random variable Ex[X] = Li XiPX[Xi]. For the joint PMFs of Figure 7.6 the joint
moment is
</p>
<p>Ex,y[XY]
</p>
<p>2 2
</p>
<p>L L XiYjPX,Y[Xi , Yj]
</p>
<p>i = l j = l
</p>
<p>1 1
(1)(1)2" + (-1)( -1)2" = 1
</p>
<p>(1)( -1)~ + (-1)(1)~ = -1
2 2
1 1
</p>
<p>(1)(-1)2" + (1)(1)2" = 0
</p>
<p>(for PMF of Figure 7.6a)
</p>
<p>(for PMF of Figure 7.6b)
</p>
<p>(for PMF of Figure 7.6c)
</p>
<p>as we might have expected.
</p>
<p>In Figure 7.6a note that Ex[X] = Ey[Y] = O. If they are not zero, as for the
joint PMF shown in Figure 7.7 in which Ex,y[XY] = 2, then the joint moment will
</p>
<p>Y
</p>
<p>2
</p>
<p>1
</p>
<p>.',,,,,,,,,,,
---Jf--+---f--+X,,,, 1 2
</p>
<p>Figure 7.7: Joint PMF for nonzero means with equally probable outcomes.
</p>
<p>depend on the values of the means. It is seen that even though the relationship
</p>
<p>Y = X is preserved, the joint moment has changed. To nullify this effect of having
nonzero means influence the joint moment it is more convenient to use the joint
</p>
<p>central moment
</p>
<p>Ex,y[(X - Ex [X]) (Y - Ey[Y])] (7.36)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.8. JOINT MOMENTS 191
</p>
<p>which will produce the desired +1 for the joint PMF of Figure 7.7. This quantity
</p>
<p>is recognized as the covariance of X and Y so that we denote it by cov(X, Y). As
</p>
<p>we have just seen, the covariance may be positive, negative, or zero. Note that the
</p>
<p>covariance is a measure of how the random variables covary with respect to each
</p>
<p>other. If they vary in the same direction, i.e., both positive or negative at the same
</p>
<p>time, then the covariance will be positive. If they vary in opposite directions, the
</p>
<p>covariance will be negative. This explains why var(X + Y) may be greater than
</p>
<p>var(X) + var(Y), for the case of a positive covariance. Similarly, the variance of
</p>
<p>the sum of the random variables will be less than the sum of the variances if the
</p>
<p>covariance is negative.
</p>
<p>If X and Yare independent random variables, then from (7.31) we have
</p>
<p>cov(X, Y) Ex,y[(X - Ex [X])(Y - Ey[Y])]
</p>
<p>Ex[X - Ex[X]]Ey[Y - Ey[Y]] = 0. (7.37)
</p>
<p>Hence, independent random variables have a covariance of zero. This also says that
</p>
<p>for independent random variables the variance of the sum of random variables is the
</p>
<p>sum of the variances, i.e., var(X + Y) = var(X) +var(Y) (see (7.33)). However, the
</p>
<p>covariance may still be zero even if the random variables are not independent - the
</p>
<p>converse is not true. Some other properties of the covariance are given in Problem
</p>
<p>7.34.
</p>
<p>~ Independence implies zero covariance but zero covariance does
not imply independence.
</p>
<p>Consider the joint PMF which assigns equal probability of 1/4 to each of the four
</p>
<p>points shown in Figure 7.8. The joint and marginal PMFs are listed in Table 7.6.
</p>
<p>y
</p>
<p>(0,1)
</p>
<p>------t--.....--..... x
(-1,0) (1,0)
</p>
<p>(0, -1)
</p>
<p>Figure 7.8: Joint PMF of random variables having zero covariance but that are
</p>
<p>dependent.
</p>
<p>For this joint PMF the covariance is zero since
</p>
<p>Ex [X] = -1 ( ~ ) +&deg;(~) + 1 (~) = &deg;</p>
<p/>
</div>
<div class="page"><p/>
<p>192 CHAPTER 7. MULTIPLE DISCRETE RANDOM VARIABLES
</p>
<p>j =-1 j=O j = 1 px[i]
</p>
<p>i =-1 0 1 0 14 4
</p>
<p>i=O 1 0 1 14 4 2
</p>
<p>i = 1 0 1 0 14 4
</p>
<p>py[j] 1 1 14 2 4
</p>
<p>Table 7.6: Joint PMF values.
</p>
<p>(7.38)
</p>
<p>and thus from (7.34)
</p>
<p>cov(X, Y) = Ex,y[XY]
</p>
<p>1 1
</p>
<p>= L L ijpx,y[i,j] = 0
i=-lj=-l
</p>
<p>since either x or Y is always zero. However, X are Yare dependent because
</p>
<p>px,y[l,O] = 1/4 but px[l]py[O] = (1/4)(1/2) = 1/8. Alternatively, we may ar-
</p>
<p>gue that the random variables must be dependent since Y can be predicted from X.
</p>
<p>For example, if X = 1, then surely we must have Y = O.
</p>
<p>More generally the joint k-lth moment is defined as
</p>
<p>EX,y[Xky
l]
</p>
<p>= L L xfY;PX,Y[Xi, Yj]
</p>
<p>j
</p>
<p>for k = 1,2, ... ;1 = 1,2, ... , when it exists. The joint k-lth central moment is
</p>
<p>defined as
</p>
<p>Ex,y[(X - Ex[X])k(y - Ey[y])l] = L L(Xi - Ex [X])k(Yj - Ey[y])lpX,Y[Xi, Yj]
</p>
<p>i j
</p>
<p>(7.39)
</p>
<p>for k = 1,2, .. . ;1= 1,2, ... , when it exists.
</p>
<p>7.9 Prediction of a Random Variable Outcome
</p>
<p>The covariance between two random variables has an important bearing on the
</p>
<p>predictability of Y based on knowledge of the outcome of X. We have already seen
</p>
<p>in Figures 7.6a,b that y can be perfectly predicted from X as Y = X (see Figure
</p>
<p>7.6a) or as Y = -X (see Figure 7.6b). These are extreme cases. More generally, we
</p>
<p>seek a predictor of Y that is linear (actually affine) in X or
</p>
<p>Y = aX +b</p>
<p/>
</div>
<div class="page"><p/>
<p>7.9. PREDICTION OF A RANDOM VARIABLE OUTCOME 193
</p>
<p>where the "hat" indicates an estimator. The constants a and b are to be chosen so
</p>
<p>that "on the average" the observed value of Y, which is ax + b if the experimental
outcome is (x , y) , is close to the observed value of Y, which is y. To determine these
</p>
<p>constants we shall adopt as our measure of closeness the mean square error (MSE)
</p>
<p>criterion described previously in Example 6.3. It is given by
</p>
<p>mse(a, b) = Ex,Y[(Y - Y)2]. (7.40)
</p>
<p>Note that since the predictor Y depends on X, we need to average with respect
to X and Y. Previously, we let Y = b, not having the additional information of
the outcome of another random variable. It was found in Example 6.3 that the
</p>
<p>optimal value of b, i.e., the value that minimized the MSE, was bopt = Ey[Y] and
</p>
<p>therefore Y = Ey[Y]. Now, however, we presume to know the outcome of X. With
the additional knowledge of the outcome of X we should be able to find a better
</p>
<p>predictor. To find the optimal values of a and b we minimize (7.40) over a and b.
</p>
<p>Before doing so we simplify the expression for the MSE. Starting with (7.40)
</p>
<p>mse(a,b) Ex,y[(Y - aX - b)2]
</p>
<p>Ex,y[(Y - aX)2 - 2b(Y - aX) + b2]
</p>
<p>Ex,y[y2 - 2aXY + a2X 2 - 2bY + 2abX + b2]
</p>
<p>Ey[y2] - 2aEx,y[XY] + a2Ex[X2]- 2bEy[Y] + 2abEx[X] + b2.
</p>
<p>To find the values of a and b that minimize the function mse(a, b), we determine a
</p>
<p>stationary point by partial differentiation. Since the function is quadratic in a and
</p>
<p>b, this will yield the minimizing values of a and b. Using partial differentiation and
</p>
<p>setting each partial derivative equal to zero produces
</p>
<p>8mse(a , b)
</p>
<p>8a
8mse(a, b)
</p>
<p>8b
</p>
<p>-2Ex,y[XY] + 2aEx[X2] + 2bEx[X] = 0
</p>
<p>= -2Ey[Y] + 2aEx[X] + 2b = 0
</p>
<p>and rearranging yields the two simultaneous linear equations
</p>
<p>Ex [X
2]a + Ex [X]b
</p>
<p>Ex[X]a+b =
</p>
<p>The solution is easily shown to be
</p>
<p>Ex,y[XY]
</p>
<p>Ey[Y].
</p>
<p>aopt
</p>
<p>bopt
</p>
<p>Ex,y[XY] - Ex [X]Ey[Y] cov(X, Y)
</p>
<p>EX[X2]- El[X] var(X)
</p>
<p>cov(X,Y)
Ey [Y]- aoptEx[X] = Ey[Y] - var(X) Ex [X]</p>
<p/>
</div>
<div class="page"><p/>
<p>194 CHAPTER 7. MULTIPLE DISCRETE RANDOM VARIABLES
</p>
<p>so that the optimal linear prediction of Y given the outcome X = x is
</p>
<p>Y aoptx + bopt
cov(X, Y) x + Ey[Y) _ cov(X, Y) Ex [X)
</p>
<p>var(X) var(X)
</p>
<p>or finally
</p>
<p>Y = E [Y) cov(X, Y) ( - E [X))
y + var(X) x X . (7.41)
</p>
<p>Note that we refer to Y= aX + b as a predictor but Y= ax + b as the prediction,
which is the value of the predictor. As expected, the prediction of Y based on X = x
</p>
<p>depends on the covariance. In fact , if the covariance is zero , then Y = Ey[Y], which
is the best linear predictor of Y without knowledge of the outcome of X. In this
</p>
<p>case, X provides no information about Y. An example follows.
</p>
<p>Example 7.13 - Predicting one random variable outcome from knowledge
</p>
<p>of second random variable outcome
</p>
<p>Consider the joint PMF shown in Figure 7.9a as solid circles where all the outcomes
</p>
<p>are equally probable. Then, SX,y = {(O, 0), (1, 1), (2,2), (2, 3)} and the marginals
</p>
<p>43012
</p>
<p>(x-Ex [XD/v!var(X)
</p>
<p>I ~ 3
2
</p>
<p>: . ' : : : : .. . . . . &bull; .&gt; ..
~ &middot;&middot; .:&middot;.. ~&middot;,~&middot;&lt;t &middot; :&middot;
::::::: 1 ;... : 1.:..
~ .. . Y:
</p>
<p>s
:.....
</p>
<p>rS 0 / .
I .. ~ :
</p>
<p>~-1/ 'r :
.... :
</p>
<p>-2'----'---~-~-~-~----'
</p>
<p>-2 -14321
X
</p>
<p>o-1
</p>
<p>2
</p>
<p>3
</p>
<p>o
</p>
<p>,
</p>
<p>" ' ~ ' / / l :. / .~ .
/ ' .
</p>
<p>. ."., :
~/ y
</p>
<p>. . . . /..
". /..
</p>
<p>/ .
/
</p>
<p>-1 : ., .,
-2 L.-_-'--_~_---'-_--'-_~_----l
</p>
<p>-2
</p>
<p>(a) Nonstandardized X and Y (b) Standardized X and Y
</p>
<p>Figure 7.9: Joint PMF (shown as solid circles having equal probabilities) and best
</p>
<p>linear prediction of Y when X = x is observed (shown as dashed line).</p>
<p/>
</div>
<div class="page"><p/>
<p>7.9. PREDICTION OF A RANDOM VARIABLE OUTCOME
</p>
<p>are found by summing along each direction to yield
</p>
<p>195
</p>
<p>py[j] =
</p>
<p>px[i] ~ U
1
4
1
4
1
4
1
4
</p>
<p>i=O
</p>
<p>i = 1
</p>
<p>i=2
</p>
<p>j=O
</p>
<p>j=1
</p>
<p>j=2
</p>
<p>j = 3.
</p>
<p>Y =
</p>
<p>As a result , we have from the marginals that Ex[X] = 5/4, Ey[Y] = 3/2, Ex[X2] =
9/4, and var(X) = Ex[X2 ] - El[X] = 9/4 - (5/4)2 = 11/16. From the joint PMF
</p>
<p>we find that Ex,y[XY] = (0)(0)1/4 + (1)(1)1/4 + (2)(2)1/4 + (2)(3)1/4 = 11/4,
which results in cov(X, Y) = Ex,y[XY] - Ex [X] Ey[Y] = 11/4 - (5/4)(3/2) = 7/8.
</p>
<p>Substituting these values into (7.41) yields the best linear prediction of Y as
</p>
<p>3 7/8 ( 5)
2" + 11/16 x -:4
14 1
- x--
11 11
</p>
<p>which is shown in Figure 7.9a as the dashed line . The line shown in Figure 7.9a is
</p>
<p>referred to as a regression line in statistics. What do you think would happen if the
</p>
<p>probability of (2,3) were zero , and the remaining three points had probabilities of
</p>
<p>1/3?
</p>
<p>c
The reader should be aware that we could also have predicted X from Y = Y
</p>
<p>by interchanging X and Y in (7.41). Also, we note that if cov(X,Y) = 0, then
Y = Ev [Y] or X = x provides no information to help us predict Y. Clearly, this
will be the case if X and Yare independent (see (7.37)) since independence of two
</p>
<p>random variables implies a covariance of zero. However, even if the covariance is
</p>
<p>zero , the random variables can still be dependent (see Figure 7.8) and so prediction
</p>
<p>should be possible. This apparent paradox is explained by the fact that in this
</p>
<p>case we must use a nonlinear predictor, not the simple linear fun ction aX + b (see
Problem 8.27).
</p>
<p>The optimal linear prediction of (7.41) can also be expressed in standardized
</p>
<p>form. A standardized random variable is defined to be one for which the mean is
</p>
<p>zero and the variance is one. An example would be a random variable that takes
</p>
<p>on the values &plusmn;1 with equal probability. Any random variable can be standardized
</p>
<p>by subtracting the mean and dividing the result by the square root of the variance
</p>
<p>to form
x, = X -Ex[X]
</p>
<p>vvar(X)</p>
<p/>
</div>
<div class="page"><p/>
<p>196 CHAPTER 7. MULTIPLE DISCRETE RANDOM VARIABLES
</p>
<p>(see Problem 7.42). For example, if X f'J Pois(A), then X; = (X - A)/.JX, which is
easily shown to have a mean of zero and a variance of one. We next seek the best
</p>
<p>linear prediction of the standardized Y based on a standardized X = x. To do so we
define the standardized predictor based on a standardized X , = X s as
</p>
<p>~ Y - Ey[Y]
Ys = .
</p>
<p>Jvar(Y)
</p>
<p>Then from (7.41), we have
</p>
<p>Y - Ey[Y] cov(X, Y) x - Ex [X]
=
</p>
<p>Jvar(Y) Jvar(Y)var(X) Jvar(X)
</p>
<p>and therefore
Y
</p>
<p>s
= cov(X, Y) X
</p>
<p>S
</p>
<p>'
</p>
<p>Jvar(X)var(Y)
</p>
<p>Example 7.14 - Previous example continued
</p>
<p>For the previous example we have that
</p>
<p>x - 5/4
</p>
<p>Jll/16
</p>
<p>Y -3/2
</p>
<p>074
and
</p>
<p>cov(X, Y) 7/8
---;====;:::::::::::::;=====;=:=.::: = ~ 0.94
Jvar(X)var(Y) J(1l/16)(5/4)
</p>
<p>so that
</p>
<p>and is displayed in Figure 7.9b.
</p>
<p>The factor that scales X s to produce Ys is denoted by
</p>
<p>cov(X, Y)
px y = ----r=~=='=7.==:;:;:
</p>
<p>, Jvar(X)var(Y)
</p>
<p>(7.42)
</p>
<p>(7.43)
</p>
<p>and is called the correlation coeffi cient. When X and Y have PX,y i- 0, then X and
Yare said to be correlated. If, however, the covariance is zero and hence PX ,y = 0,
</p>
<p>then the random variables are said to be uncorrelated. Clearly, independent ran-
</p>
<p>dom variables are always uncorrelated, but not the other way around. Using the
</p>
<p>correlation coefficient allows us to express the best linear prediction in its standard-
</p>
<p>ized form as Ys = PX,yxs ' The correlation coefficient has an important property</p>
<p/>
</div>
<div class="page"><p/>
<p>7.9. PREDICTION OF A RANDOM VARIABLE OUTCOME 197
</p>
<p>in that it is always less t han one in magnitude. In t he pr evious example, we had
</p>
<p>PX,y ~ 0.94.
</p>
<p>Property 7.7- Correlation coefficient is always less than or equal to one
</p>
<p>in magnitude or Ipx ,y I ::S 1.
Proof: The proof relies on the Cauchy-Schwar z inequ ality for random vari ables.
</p>
<p>This inequ ality is analogous to t he usual one for the dot product of Euclidean vectors
</p>
<p>v and w , wh ich is
</p>
<p>Iv.w] ::S llvllllwll
</p>
<p>where llvll denotes t he length of the vector. Equality holds if and only if the vectors
are collinear . Collinearity means that w = cv for c a constant or t he vect ors point in
the same direction. For random vari ables V an d W the Cau chy-Schwarz inequ ality
</p>
<p>says that
</p>
<p>(7.44)
</p>
<p>with equality if and only if W = cV for c a constant . See Appendix 7A for a
derivation . Thus letting V = X - Ex [X] and W = Y - Ey[Y], we have
</p>
<p>Ipx,yl =
Icov(X, Y)I
</p>
<p>J var (X )var (Y )
</p>
<p>IEv,w[VW]1 &lt; 1
J Ev [V2] J Ew [W2] -
</p>
<p>using (7.44). Equality will hold if and only if W = cV or equivalent ly if Y -Ey [Y] =
c(X - Ex [Xj) , which is eas ily shown to imply t hat (see Problem 7.45)
</p>
<p>{
1 if Y = aX + b with a &gt; 0
</p>
<p>PX ,y = -1 if Y = aX + b with a &lt; 0
</p>
<p>for a and b constants.
</p>
<p>o
Note that when PX ,y = &plusmn;1, Y can be perfectly predicted from X by using Y =
aX + b. See also Figures 7.6a and 7.6b for examples of when PX,y = +1 and
</p>
<p>PX ,Y = -1 , resp ectively.
</p>
<p>.&amp; Correlation between random variables does not imply a causal
relationship between the random variables.
</p>
<p>A frequ ent misappl ication of probability is to assert t hat two qu anti ties t hat are
</p>
<p>correlated (pX ,y =I 0) are such because one causes the other. To dis pe l this myth
consider a survey in which all individuals older than 55 years of age in the U.S. are
</p>
<p>asked whet her t hey have ever had prostate cancer and also t heir height in inches.
</p>
<p>T hen , for each height in inches we compute t he average number of indi viduals per</p>
<p/>
</div>
<div class="page"><p/>
<p>198 CHAPTER 7. MULTIPLE DISCRETE RANDOM VARIABLES
</p>
<p>.... . : : .:.. :...... .. . .
. :. ; ; : .130 .. .. ... : .. ..
</p>
<p>140
</p>
<p>160.---,..----.---.-------,r---,..----.---.-------,
</p>
<p>150 .
</p>
<p>. . . . .
</p>
<p>Q) 120 ; ; ~ , ; : : ..
~ . : ...
~ 110 : : : ~ . &bull; . .: ..
</p>
<p>'g 100 &middot; &middot; : : : ..
......
</p>
<p>908580
</p>
<p>. ; ... .&bull; .. ; ; ; .
: .. : . .
</p>
<p>60 65 70 75
Height (inches)
</p>
<p>...... ~ .
</p>
<p>55
</p>
<p>90 .
</p>
<p>80
</p>
<p>70
</p>
<p>60 L-_-'--_--'-_----'-__'--_-'--_--'-_----'-_---'
50
</p>
<p>Figure 7.10: Incidence of prostate cancer per 1000 individuals older than age 55
</p>
<p>versus height.
</p>
<p>1000 who have had cancer. If we plot the average number , also called the incidence
</p>
<p>of cancer, versus height, a typical result would be as shown in Figure 7.10. This
</p>
<p>indicates a strong positive correlation of cancer with height. One might be tempted
</p>
<p>to conclude that growing taller causes prostate cancer. This is of course nonsense.
</p>
<p>What is actually shown is that segments of the population who are tall are associated
</p>
<p>with a higher incidence of cancer. This is because the portion of the population of
</p>
<p>individuals who are taller than the rest are predominately male. Females are not
</p>
<p>subject to prostate cancer , as they have no prostates! In summary, correlation
</p>
<p>between two variables only indicates an association, i.e., if one increases, then so
</p>
<p>does the other (if positively correlated). No physical or causal relationship need
</p>
<p>exist .
</p>
<p>7.10 Joint Characteristic Functions
</p>
<p>The characteristic function of a discrete random variable was introduced in Section
</p>
<p>6.7. For two random variables we can define a joint characteristic function. For the
</p>
<p>random variables X and Y it is defined as
</p>
<p>&cent;X,y(WX,wy) = Ex,y[exp[j(wx X +WyY)]] . (7.45)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.10. JOINT CHARACTERISTIC FUNCTIONS 199
</p>
<p>Assuming both random variables take on integer values , it is evaluated using (7.29)
</p>
<p>as
00 00
</p>
<p>&cent;X,y(wx,wy) = L L px,y[k,l]exp[j(wxk+wyl)] .
k=-ool=-oo
</p>
<p>(7.46)
</p>
<p>It is seen to be the two-dimensional Fourier transform of the two-dimensional se-
</p>
<p>quence px,y[k, l] (note the use of +j as opposed to the more common -j in the
</p>
<p>exponent ial). As in the case of a single random variable, the characteristic function
</p>
<p>can be used to find moments. In this case, the joint moments are given by the
</p>
<p>formula
</p>
<p>E [Xmyn] = _1_ 8
m+n&cent;x
</p>
<p>,y(wx,wy) I .
X,Y 'm+n 8wm8wn
</p>
<p>J X Y wx=wy=O
</p>
<p>In particular, the first joint moment is found as
</p>
<p>Ex,y[XY] = _ 8
2&cent;x
</p>
<p>,y(wx ,wy) I .
8wx8wy wx =wy = O
</p>
<p>(7.47)
</p>
<p>(joint PMF factors)
</p>
<p>Another important application is to finding the PMF for the sum of two independent
</p>
<p>random variables. This application is based on the result that if X and Yare
</p>
<p>independent random variables, the joint characteristic function factors due to the
</p>
<p>property Ex,y[g(X)h(Y)] = Ex[g(X)]Ey[h(Y)] (see (7.31)). Before deriving the
</p>
<p>PMF for the sum of two independent random variables, we prove the factorization
</p>
<p>result, and then give a theoretical application. The factorization of the characteristic
</p>
<p>function follows as
</p>
<p>00 00
</p>
<p>&cent;X,y(wx,wy) = L L px,y[k,l]exp[j(wxk+wyl)]
k=-ool=-oo
</p>
<p>00 00
</p>
<p>L L PX [k]py [l] exp[jwx k] exp[jwyl]
k=-ool=-oo
</p>
<p>00 00
</p>
<p>L px[k] exp[jwxk] L py[l] exp[jwyl]
k=-oo 1=-00
</p>
<p>&cent;x(wx)&cent;y(wy). (definition of characteristic function (7.48)
</p>
<p>for single random variable).
</p>
<p>The converse is also true-if the joint characteristic function factors, then X and
</p>
<p>Yare independent random variables. This can easily be shown to follow from the
</p>
<p>inverse Fourier transform relationship. As an application of the converse result,
</p>
<p>consider the tranformed random variables W = g(X) and Z = h(Y) , where X and
Yare independent. We prove that Wand Z are independent as well, which is to
</p>
<p>say junctions o] independent random variables are independent. To do so we show</p>
<p/>
</div>
<div class="page"><p/>
<p>200 CHAPTER 7. MULTIPLE DISCRETE RANDOM VARIABLES
</p>
<p>that the joint characteristic function factors. The joint characteristic function of the
</p>
<p>transformed random variables is
</p>
<p>4&gt;w,z(ww,wz) = Ew,z[exp[j(wwW + wzZ)]].
</p>
<p>But we have that
</p>
<p>4&gt;w,z(ww, wz) Ex,y[exp[j(wwg(X) + wzh(Y)]]
Ex [exp(jww g(X)) ]Ey [exp(jwz h(Y))]
</p>
<p>Ew[exp(jwwW)]Ez[exp(jwz Z)]
</p>
<p>4&gt;w(ww)4&gt;z(wz)
</p>
<p>(slight extension of (7.28))
</p>
<p>(same argument as used to
</p>
<p>yield (7.31))
</p>
<p>(from (6.5))
</p>
<p>(definition)
</p>
<p>and hence Wand Z are independent random variables. As a general result, we can
</p>
<p>now assert that if X and Yare independent random variables, then so are g(X)
</p>
<p>and h(Y) for any functions 9 and h.
</p>
<p>Finally, consider the problem of determining the PMF for Z = X +Y, where X
and Yare independent random variables. We have already solved this problem using
</p>
<p>the joint PMF approach with the final result given by (7.22). By using characteristic
</p>
<p>functions, we can simplify the derivation. The derivation proceeds as follows.
</p>
<p>Ez[exp(jwzZ)]
</p>
<p>Ex,y[exp(jwz(X + Y)]
Ex,y[exp(jwzX) exp(jwzY)]
</p>
<p>Ex [exp(jwzX)]Ey[exp(jwzY)]
</p>
<p>4&gt;x (wz )4&gt;y(wz).
</p>
<p>(definition)
</p>
<p>(from (7.28) and (7.29))
</p>
<p>(from (7.31))
</p>
<p>To find the PMF we take the inverse Fourier transform of 4&gt;z(wz) , replacing wz by
</p>
<p>the more usual notation w, to yield
</p>
<p>pz[k] = i: 4&gt;x (w)4&gt;y (w) exp( -jwk) ~ ~
00
</p>
<p>L px[i]py[k - i]
i=-oo
</p>
<p>which agrees with (7.22). The last result follows from the property that the Fourier
</p>
<p>transform of a convolution sum is the product of the Fourier transforms of the
</p>
<p>individual sequences.
</p>
<p>7.11 Computer Simulation of Random Vectors
</p>
<p>The method of generating realizations of a two-dimensional discrete random vector
</p>
<p>is nearly identical to the one-dimensional case. In fact, if X and Yare independent,</p>
<p/>
</div>
<div class="page"><p/>
<p>7.11 . COMPUTER SIMULATION OF RANDOM VECTORS 201
</p>
<p>then we generate a realization of X, say Xi , according to PX[Xi] and a realization ofY,
</p>
<p>say Yj, according to py[Yj] using the method of Chapter 5. Then we concatenate the
</p>
<p>realizations together to form the realization of the vector random variable as (Xi,Yj).
Furthermore, independence reduces the problems of estimating a joint PMF, a joint
</p>
<p>CDF, etc. to that of the one-dimensional case. The joint PMF, for example, can be
</p>
<p>estimated by first estimating PX[Xi] as PX[Xi], then estimating PY[Yj] as PY[Yj], and
</p>
<p>finally forming the estimate of the joint PMF as PX,y[Xi,Yj] = PX[Xi]PY[Yj] .
When the random variables are not independent, we need to generate a realiza-
</p>
<p>tion of (X, Y) simultaneously since the value obtained for X is dependent on the
</p>
<p>value obtained for Y and vice versa. If both Sx and Sy are finite , then a simple
procedure is to consider each possible realization (Xi, Yj) as a single outcome with
</p>
<p>probability PX,y[Xi,Yj]&middot; Then, we can apply the techniques of Section 5.9 directly.
An example is given next.
</p>
<p>Example 7.15 - Generating realizations of jointly distributed random
</p>
<p>variables
</p>
<p>Assume a joint PMF as given in Table 7.7. A simple MATLAB program to generate
</p>
<p>j=O j=1
</p>
<p>i=O 1 18" 8"
</p>
<p>i = 1 1 14 2
</p>
<p>Table 7.7: Joint PMF values for Example 7.15.
</p>
<p>a set of M realizations of (X,Y) is given below.
</p>
<p>for m=l:M
</p>
<p>u=randC1, 1) j
</p>
<p>if u&lt;=1/8
</p>
<p>x(m,l)=Ojy(m,l)=O;
</p>
<p>elseif u&gt;1/8&amp;u&lt;=1/4
</p>
<p>x(m,l)=O;y(m,l)=l;
</p>
<p>elseif u&gt;1/4&amp;u&lt;=1/2
</p>
<p>x(m,l)=l;y(m,l)=O;
</p>
<p>else
</p>
<p>x(m,l)=l;y(m,l)=l;
</p>
<p>end
</p>
<p>end
</p>
<p>Once the realizations are available we can estimate the joint PMF and marginal</p>
<p/>
</div>
<div class="page"><p/>
<p>202
</p>
<p>PMFs as
</p>
<p>CHAPTER 7. MULTIPLE DISCRETE RANDOM VARIABLES
</p>
<p>i = 0, 1
</p>
<p>j = 0,1
</p>
<p>px,y[i ,j]
</p>
<p>px[i]
</p>
<p>py[j]
</p>
<p>Number of outcomes equal to (i,j)
</p>
<p>M
</p>
<p>= px,y[i ,O] +px,y[i , 1]
= PX,y[O,j] +px,y[l,j]
</p>
<p>i = 0, 1; j = 0, 1
</p>
<p>and the joint moments are estimated as
</p>
<p>where (xrn , Yrn) is the mth realization. Other quantities of interest are discussed in
</p>
<p>Problems 7.49 and 7.51.
</p>
<p>7.12 Real-World Example - Assessing Health Risks
</p>
<p>An increasingly common health problem in the United States is obesity. It has been
</p>
<p>found to be associated with many life-threatening illnesses, especially diabetes. One
</p>
<p>way to define what constitutes an obese person is via the body mass index (BMI)
</p>
<p>[CDC 2003]. It is computed as
</p>
<p>BMI = 703W
H2
</p>
<p>(7.49)
</p>
<p>where W is the weight of the person in pounds and H is the person's height in inches.
</p>
<p>BMls greater than 25 and less than 30 are considered to indicate an overweight
</p>
<p>person, and 30 and above an obese person [CDC 2003]. It is of great importance to
</p>
<p>be able to estimate the PMF of the BMI for a population of people. For example,
</p>
<p>in Chapter 4 we displayed a table of the joint probabilities of heights and weights
</p>
<p>for a hypothetical population of college students. For this population we would
</p>
<p>like to know the probability or percentage of obese persons. This percentage of the
</p>
<p>population would then be at risk for developing diabetes. To do so we could first
</p>
<p>determine the PMF of the BMI and then determine the probability of a BMI of 30
</p>
<p>and above. From Table 4.1 or Figure 7.1 we have the joint PMF for the random
</p>
<p>vector (H, W). To find the PMF for the BMI we note that it is a function of Hand
</p>
<p>W or in our previous notation, we wish to determine the PMF of Z = g(X, Y), where
</p>
<p>Z denotes the BMI, X denotes the height, and Y denotes the weight. The solution
</p>
<p>follows immediately from (7.26). One slight modification that we must make in
</p>
<p>order to fit the data of Table 4.1 into our theoretical framework is to replace the
</p>
<p>height and weight intervals by their midpoint values. For example, in Table 4.1 the
</p>
<p>probability of observing a person with a height between 5' 8" and 6' and a weight of
</p>
<p>between 130 and 160 lbs. is 0.06. We convert these intervals so that we can say that</p>
<p/>
</div>
<div class="page"><p/>
<p>7.12. REAL-WORLD EXAMPLE - ASSESSING HEALTH RISKS 203
</p>
<p>the probability of a person having a height of 5'10/1 and a weight of 145 lbs, is 0.06.
</p>
<p>Next to determine the PMF we first find the BMI for each height and weight using
</p>
<p>(7.49), rounding the result to the nearest integer. This is displayed in Table 7.8.
</p>
<p>WI W 2 W 3 W4 Ws
</p>
<p>115 145 175 205 235
</p>
<p>HI 52 21 27 32 37 43
H
</p>
<p>25'6/1 19 23 28 33 38
H
</p>
<p>3
5'10/1 16 21 25 29 34
</p>
<p>H46'2/1 15 19 22 26 30
Hs 6'6/1 13 17 20 24 27
</p>
<p>Table 7.8: Body mass indexes for heights and weights of hypothetical college stu-
</p>
<p>dents.
</p>
<p>353025
BMI
</p>
<p>20
</p>
<p>lI"o
15
</p>
<p>0.02
</p>
<p>0.04
</p>
<p>0.16
</p>
<p>0.06
</p>
<p>~ 0.1
</p>
<p>:=:E
0.. 0.08
</p>
<p>0.12
</p>
<p>0.14
</p>
<p>Figure 7.11: Probability mass function for body mass index of hypothetical college
</p>
<p>population.
</p>
<p>Then, we determine the PMF by using (7.26). For example, for a BMI = 21, we
require from Table 7.8 the entries (H, W) = (5'2" ,115) and (H, W) = (5'10/1 ,145).
</p>
<p>But from Table 4.1 we see that
</p>
<p>P[H = 5'2", W = 115] 0.08
P[H = 5'10/1 , W = 145] 0.06
</p>
<p>and therefore P[BMI = 21] = 0.14. The other values of the PMF of the BMI
are found similarly. This produces the PMF shown in Figure 7.12. It is seen that</p>
<p/>
</div>
<div class="page"><p/>
<p>204 CHAPTER 7. MULTIPLE DISCRETE RANDOM VARIABLES
</p>
<p>the probability of being obese as defined by the BMI (BMI ;::: 30) is 0.08. Stated
</p>
<p>another way 8% of the population of college students are obese and so are at risk
</p>
<p>for diabetes.
</p>
<p>References
</p>
<p>CDC, "Nutrit ion and Physical Activity," Center for Disease Control, http://www.
</p>
<p>cdc.gov /nccdphp/ dnpa/bmi/bmi-adult-formula.htm, 2003.
</p>
<p>Jackson, L.B., Signals, Systems, and Transforms, Addison-Wesley, Reading, MA,
</p>
<p>1991.
</p>
<p>Problems
</p>
<p>7.1 (w) A chess piece is placed on a chessboard, which consists of an 8 x 8 array
</p>
<p>of 64 squares. Specify a numerical sample space SX,Y for the location of the
</p>
<p>chess piece.
</p>
<p>7.2 (w) Two coins are tossed in succession with a head being mapped into a +1 and
</p>
<p>a tail being mapped into a -1. If a random vector is defined as (X, Y) with
</p>
<p>X representing the mapping of the first toss and Y representing the mapping
</p>
<p>of the second toss , draw the mapping. Use Figure 7.2 as a guide. Also, what
</p>
<p>is SX,Y?
</p>
<p>7.3 C.:....) (w) A woman has a penny, a nickel, and a dime in her pocket. If she
chooses two coins from her pocket in succession, what is the sample space S
</p>
<p>of possible outcomes? If these outcomes are next mapped into the values of
</p>
<p>the coins , what is the numerical sample space SX,Y?
</p>
<p>7.4 (w) If Sx = {1,2} and Sy = {3,4}, plot the points in the plane comprising
Sx y = Sx x Sy. What is the size of Sx y?, ,
</p>
<p>7.5 (w) Two dice are tossed. The number of dots observed on the dice are added
</p>
<p>together to form the random variable X and also differenced to form Y. De-
</p>
<p>termine the possible outcomes of the random vector (X, Y) and plot them in
</p>
<p>the plane. How many possible outcomes are there?
</p>
<p>7.6 (f) A two-dimensional sequence is given by
</p>
<p>i = 1,2, . .. ; j = 1,2, ...
</p>
<p>where 0 &lt; PI &lt; 1, 0 &lt; P2 &lt; 1, and c is a constant. Find c to make PX ,y a
valid joint PMF.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.7 (f) Is
</p>
<p>PROBLEMS
</p>
<p>(
1 )i+j
</p>
<p>px,y[i,j] = "2
</p>
<p>a valid joint PMF?
</p>
<p>i = O,l, ... ;j = 0,1, .. .
</p>
<p>205
</p>
<p>7.8 C...:...) (w) A single coin is tossed twice. A head outcome is mapped into a 1 and
a tail outcome into a 0 to yield a numerical outcome. Next, a random vector
</p>
<p>(X, Y) is defined as
</p>
<p>X outcome of first toss + outcome of second toss
Y outcome of first toss - outcome of second toss.
</p>
<p>Find the joint PMF for (X, Y), assuming the outcomes (Xi,Yj) are equally
</p>
<p>likely.
</p>
<p>7.9 (f) Find the joint PMF for the experiment described in Example 7.1. Assume
each outcome in S is equally likely. How can you check your answer?
</p>
<p>7.10 (...:..:...) (f) The sample space for a random vector is SX,y = {(i, j) : i = 1,2,3,4, 5j
</p>
<p>j = 1,2,3, 4}. If the outcomes are equally likely, find P[(X, Y) E A], where
</p>
<p>A = {(i,j) : 1 5: i 5: 2; 35: j 5: 4}.
</p>
<p>7.11 (f) A joint PMF is given as px,y[i,j] = (1/2)i+j for i = 1,2, ... .i = 1,2, ....
If A = {(i,j): 15: i 5: 3jj ~ 2}, find P[A].
</p>
<p>7.12 (f) The values of a joint PMF are given in Table 7.9. Determine the marginal
</p>
<p>PMFs.
</p>
<p>j=O j=l j=2
</p>
<p>i=O 1 0 18" 4"
</p>
<p>i = 1 0 1 18" 4"
</p>
<p>i=2 1 0 18" 8"
</p>
<p>Table 7.9: Joint PMF values for Problem 7.12.
</p>
<p>7.13 C..:...) (f) If a joint PMF is given by
</p>
<p>px,y[i,j] = p2(1 _ p)i+j - 2
</p>
<p>find the marginal PMFs.
</p>
<p>7.14 (f) If a joint PMF is given by px,y[i,j]
1,2,3,4,5,6, find the marginal PMFs.
</p>
<p>i = 1,2, ... ; j = 1,2, ...
</p>
<p>1/36 for i = 1,2,3,4,5,6jj</p>
<p/>
</div>
<div class="page"><p/>
<p>206 CHAPTER 7. MULTIPLE DISCRETE RANDOM VARIABLES
</p>
<p>7.15 (w) A joint PMF is given by
</p>
<p>(
10) (1) 10pX,y(i,j] = e j "2 i = O,l;j = 0,1 , ... , 10
</p>
<p>where e is some unknown constant. Find c so that the joint PMF is valid and
</p>
<p>then determine the marginal PMFs. Hint: Recall the binomial PMF.
</p>
<p>7.16 L..:.,) (w) Find another set of values for the joint PMF that will yield the same
marginal PMFs as given in Table 7.2.
</p>
<p>7.17 (t) Prove Properties 7.3 and 7.4 for the joint CDF by relying on the standard
properties of probabilities of events.
</p>
<p>7.18 (w) Sketch the joint CDF for the joint PMF given in Table 7.2. Do this by
shading each region in the x-y plane that has the same value.
</p>
<p>7.19 (..:.:..-) (w) A joint PMF is given by
</p>
<p>Px,y[i,j] =
</p>
<p>Are X and Y independent?
</p>
<p>~ (i,j) = (0,0)
</p>
<p>~ (i,j) = (1,1)
</p>
<p>~ (i,j) =(1,0)
</p>
<p>~ (i,j) = (1, -1)
</p>
<p>7.20 (t) Prove that if the random variables X and Yare independent, then the
</p>
<p>joint CDF factors as Fx,Y(x, y) = Fx(x)Fy(y).
</p>
<p>7.21 (t) If a joint PMF is given by
</p>
<p>{
</p>
<p>a (i,j) =(0,0)
b (i ,j) = (0,1)
</p>
<p>px,y[i,j] = e (i,j) = (1,0)
</p>
<p>d (i,j) = (1,1)
</p>
<p>where of course we must have a+b+c+d = 1, show that a necessary condition
</p>
<p>for the random variables to be independent is ad = be. This can be used to
</p>
<p>quickly assert that the random variables are not independent as for the case
</p>
<p>shown in Table 7.5.
</p>
<p>7.22 (f) If X '" Ber(px) and Y '" Ber(py), and X and Yare independent, what is
the joint PMF?</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS
</p>
<p>7.23 C.:,.,) (w) If the joint PMF is given as
</p>
<p>(
10 ) (11) (1)21px,y[i,j] = i j "2 i = 0,1, ... , 10;j = 0,1, ... , 11
</p>
<p>207
</p>
<p>are X and Y independent? What are the marginal PMFs?
</p>
<p>7.24 (t) Assume that X and Yare discrete random variables that take on all integer
values and are independent. Prove that the PMF of Z = X - Y is given by
</p>
<p>00
</p>
<p>pz[l] = L px[k]py[k -l]
k= -oo
</p>
<p>l= ... ,-l,O,l, ...
</p>
<p>by following the same procedure as was used to derive (7.22). Note that the
</p>
<p>transformation from (X, Y) to (W, Z) is one-to-one. Next show that if X and
</p>
<p>Y take on nonnegative integer values only, then
</p>
<p>00
</p>
<p>pz[l] = L px[k]py[k -l]
k=max(O,I)
</p>
<p>l = ... ,-1,0,1, ...
</p>
<p>7.25 (f) Using the result of Problem 7.24 find the PMF for Z = X - Y if X '"
Pois(.\x) , Y '" Pois(.\y) , and X and Yare independent. Hint: The result will
</p>
<p>be in the form of infinite sums.
</p>
<p>7.26 (w) Find the PMF for Z = max(X, Y) if the joint PMF is given in Table 7.5.
</p>
<p>7.27 C..:',) (f) If X '" Ber(1/2)' Y '" Ber(1/2) , and X and Yare independent , find
the PMF for Z = X + Y. Why does the width of the PMF increase? Does
the variance increase?
</p>
<p>7.28 (t) Prove that Ex,y[g(X)] = Ex[g(X)]. Do X and Y have to be independent?
</p>
<p>7.29 (t) Prove that
</p>
<p>Ex,y[ag(X) + bh(Y)] = aEx[g(X)] + bEy[h(Y)].
</p>
<p>7.30 (t) Prove (7.31).
</p>
<p>7.31 (t) Find a formula for var(X - Y) similar to (7.33). What can you say about
</p>
<p>the relationship between var(X + Y) and var(X - Y) if X and Yare uncor-
related?
</p>
<p>7.32 (f) Find the covariance for the joint PMF given in Table 7.4. How do you
know the value that you obtained is correct?
</p>
<p>7.33 C:..:,.,) (f) Find the covariance for the joint PMF given in Table 7.5.</p>
<p/>
</div>
<div class="page"><p/>
<p>208 CHAPTER 7. MULTIPLE DISCRETE RANDOM VARIABLES
</p>
<p>7.34 (t) Prove the following properties of the covariance:
</p>
<p>cov(X, Y)
</p>
<p>cov(X,X)
</p>
<p>cov(Y, X)
</p>
<p>cov(cX, Y)
</p>
<p>cov(X,cY)
</p>
<p>cov(X,X + Y)
cov(X + Y,X)
</p>
<p>for c a constant.
</p>
<p>= Ex,Y[XY] - Ex [X]Ey[Y]
</p>
<p>var(X)
</p>
<p>cov(X,Y)
</p>
<p>c [cov(X, Y)]
</p>
<p>c [cov(X ,Y)]
</p>
<p>= cov(X, X) + cov(X, Y)
cov(X, X) + cov(Y, X)
</p>
<p>7.35 (t) If X and Y have a covariance of cov(X, Y) , we can transform them to a
new pair of random variables whose covariance is zero. To do so we let
</p>
<p>w X
Z aX+Y
</p>
<p>where a = -cov(X, Y)/var(X). Show that cov(W, Z) = O. This process is
</p>
<p>called decorrelatinq the random variables. See also Example 9.4 for another
</p>
<p>method.
</p>
<p>7.36 (f) Apply the results of Problem 7.35 to the joint PMF given in Table 7.5.
</p>
<p>Verify by direct calculation that cov(W, Z) = O.
</p>
<p>7.37 C.:...:.J (f) If the joint PMF is given as
</p>
<p>(
</p>
<p>1 )i+j
px,y[i,j] = 2
</p>
<p>compute the covariance.
</p>
<p>i = 1,2, . . . ; j = 1,2, ...
</p>
<p>7.38 C:.:J (f) Determine the minimum mean square error for the joint PMF shown
in Figure 7.9a. You will need to evaluate Ex,y[(Y - ((14/11)X - 1/11))2] .
</p>
<p>7.39 (t,f) Prove that the minimum mean square error of the optimal linear predic-
tor is given by
</p>
<p>msemin = Ex,y[(Y - (aoptX + bopt ))2] = var(Y) (1 - p~ ,y) .
</p>
<p>Use this formula to check your result for Problem 7.38.
</p>
<p>7.40 C.:....:... ) (w) In this problem we compare the prediction of a random variable with
and without the knowledge of a second random variable outcome. Consider
</p>
<p>the joint PMF shown below. First determine the optimal linear prediction of Y</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS
</p>
<p>j=O j=l
</p>
<p>i=O 1 1"8 4
</p>
<p>i = 1 1 34 "8
</p>
<p>Table 7.10: Joint PMF values for Problem 7.40.
</p>
<p>209
</p>
<p>without any knowledge of the outcome of X (see Section 6.6). Also, compute
</p>
<p>the minimum mean square error. Next determine the optimal linear prediction
</p>
<p>of Y based on the knowledge that X = x and compute the minimum mean
</p>
<p>square error. Plot the predictions versus x in the plane. How do the minimum
</p>
<p>mean square errors compare?
</p>
<p>7.41 C:..:...) (w,c) For the joint PMF of height and weight shown in Figure 7.1 deter-
mine the best linear prediction of weight based on a knowledge of height. You
</p>
<p>will need to use Table 4.1 as well as a computer to carry out this problem.
</p>
<p>Does your answer seem reasonable? Is your prediction of a person's weight if
</p>
<p>the height is 70 inches reasonable? How about if the height is 78 inches? Can
</p>
<p>you explain the difference?
</p>
<p>7.42 (f) Prove that the transformed random variable
</p>
<p>X -Ex[X]
</p>
<p>Jvar(X)
</p>
<p>has an expected value of 0 and a variance of 1.
</p>
<p>7.43 L..:.J (w) The linear prediction of one random variable based on the outcome
of another becomes more difficult if noise is present. We model noise as the
</p>
<p>addition of an un correlated random variable. Specifically, assume that we wish
</p>
<p>to predict X based on observing X + N, where N represents the noise. If X
and N are both zero mean random variables that are uncorrelated with each
</p>
<p>other, determine the correlation coefficient between W = X and Z = X + N.
How does it depend on the power in X, which is defined as Ex[X2], and the
</p>
<p>power in N, also defined as EN[N
2 ]?
</p>
<p>7.44 (w) Consider var(X + Y), where X and Yare correlated random variables.
How is the variance of a sum of random variables affected by the correlation
</p>
<p>between the random variables? Hint: Express the variance of the sum of the
</p>
<p>random variables using the correlation coefficient.
</p>
<p>7.45 (f) Prove that if Y = aX + b, where a and b are constants, then PX,Y = 1 if
a&gt; 0 and PX,Y = -1 if a &lt; O.</p>
<p/>
</div>
<div class="page"><p/>
<p>210 CHAPTER 7. MULTIPLE DISCRETE RANDOM VARIABLES
</p>
<p>7.46 C..:..) (w) If X rv Ber(1/2) , Y rv Ber(1/2), and X and Yare independent, find
the PMF for Z = X + Y. Use the characteristic function approach to do so.
Compare your results to that of Problem 7.27.
</p>
<p>7.47 (w) Using characteristic functions prove that the binomial PMF has the re-
producing property. That is to say, if X rv bin(Mx,p), Y rv bin(My,p), and
</p>
<p>X and Yare independent, then Z = X + Y rv bin(Mx + My,p) . Why does
this make sense in light of the fact that a sequence of independent Bernoulli
</p>
<p>trials can be used to derive the binomial PMF?
</p>
<p>7.48 C:.:..) (c) Using the joint PMF shown in Table 7.7 generate realizations of the
random vector (X,Y) and estimate its joint and marginal PMFs. Compare
</p>
<p>your estimated results to the true values.
</p>
<p>7.49 (-.:...:...) (c) For the joint PMF shown in Table 7.7 determine the correlation coef-
</p>
<p>ficient. Next use a computer simulation to generate realizations of the random
</p>
<p>vector (X,Y) and estimate the correlation coefficient as
</p>
<p>1 "",M --
M wm=l XmYm - xy
</p>
<p>V( 1"",M 2 -2) (1"",M 2 -2)
M wm=l X m - X M wm=l Ym - Y
</p>
<p>PX,y = -,===:::::::::=================
</p>
<p>where
</p>
<p>1 M
</p>
<p>X = M L Xm
m=l
</p>
<p>1 M
</p>
<p>fj M LYm
m=l
</p>
<p>and (xm, Ym) is the mth realization.
</p>
<p>7.50 (w,c) If X rv geom(p), Y rv geom(p), and X and Yare independent, show
</p>
<p>that the PMF of Z = X + Y is given by
</p>
<p>k = 2,3, . ...
</p>
<p>To avoid errors use the discrete unit step sequence. Next, for p = 1/2 gen-
erate realizations of Z by first generating realizations of X, then generating
</p>
<p>realizations of Y and adding each pair of realizations together. Estimate the
</p>
<p>PMF of Z and compare it to the true PMF.
</p>
<p>7.51 (w,c) Using the joint PMF given in Table 7.5 determine the covariance to
</p>
<p>show that it is nonzero and hence X and Yare correlated . Next use the
</p>
<p>procedure of Problem 7.35 to determine transformed random variables Wand</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 211
</p>
<p>Z that are uncorrelated. Verify that Wand Z are uncorrelated by estimating
</p>
<p>the covariance as
__ 1 M
</p>
<p>cov(W, Z) = M 2: WmZm - wE
m=l
</p>
<p>where
</p>
<p>1 M
</p>
<p>M 2: W m
m=l
</p>
<p>1 M
</p>
<p>M 2: Zm
m=l
</p>
<p>and (wm, zm) is the mth realization. Be sure to generate the realizations of W
</p>
<p>and Z as W m = Xm and Zm = aXm +Ym, where (xm , Ym) is the mth realization
of (X, Y) .</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 7A
</p>
<p>Derivation of the
</p>
<p>Cauchy-Schwarz Inequality
</p>
<p>The Cauchy-Schwarz inequality was given by
</p>
<p>IEv,w[VW] I :::; J EV[V2]JEw[W2] (7A.1)
</p>
<p>with equality holding if and only if W = cV, for c a constant. To prove this , we
</p>
<p>first note that for all a =1= 0 and f3 =1= 0
</p>
<p>If we let
</p>
<p>a = JEw [W2]
</p>
<p>f3 = J EV[V2]
</p>
<p>then we have that
</p>
<p>(7A.2)
</p>
<p>Ev,w[( JEW[W2]V - JEV[V2]W)2] &gt; 0
</p>
<p>Ev,w[Ew[W2]V2 - 2JEw[W2]JEV[V2]VW + Ev[V2]W2] &gt; 0
</p>
<p>Ew[W2]Ev[V2] - 2JEw[W2]JEv[V2]Ev,w[VW] + EV[V2]Ew[W2] &gt; 0
</p>
<p>since Ev,w[g(W)] = Ew[g(W)], etc. , which results in
</p>
<p>Ew[W2]Ev[V2] - JEw [W2] JEv [V2]Ev,w[VW] 2: O.
</p>
<p>Dividing by Ew[W2]Ev[V2] produces
</p>
<p>1- Ev,w[VW] &gt; 0
J EW[W2] J EV[V2] -</p>
<p/>
</div>
<div class="page"><p/>
<p>214 CHAPTER 7. MULTIPLE DISCRETE RANDOM VARIABLES
</p>
<p>or finally, upon rearranging terms we have that
</p>
<p>Ev,w[VW] &lt; 1
JEV[V2] JEW[W2] -
</p>
<p>or
</p>
<p>By replacing the negative sign in (7A.2) by a positive sign and proceeding in an
</p>
<p>identical manner, we will obtain
</p>
<p>and hence combining the two results yields the desired inequality. To determine
</p>
<p>when the equal sign will hold, we note that
</p>
<p>Vi Wj
</p>
<p>which can only equal zero when (avi-{3Wj)2 = 0 for all i and j since PV,W[Vi, Wj] &gt; O.
Thus, for equality to hold we must have
</p>
<p>all i and j
</p>
<p>which is equivalent to requiring
</p>
<p>aV = {3W
</p>
<p>or finally dividing by {3 (asssumed not equal to zero), we obtain the condition for
</p>
<p>equality as
a
</p>
<p>W= -V=cV
{3
</p>
<p>for c a constant.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 8
</p>
<p>Conditional Probability Mass
</p>
<p>Functions
</p>
<p>8.1 Introduction
</p>
<p>In Chapter 4 we discussed the concept of conditional probability. We recall that a
</p>
<p>conditional probability P[AIB] is the probability of an event A, given that we know
that some other event B has occurred . Except for the case when the two events are
</p>
<p>independent of each other , the knowledge that B has occurred will change the prob-
</p>
<p>ability P[A]. In other words, P[AIB] is our new probability in light of the addit ional
knowledge. In many practical situations, two random mechanisms are at work and
</p>
<p>are described by events A and B . An example of such a compound experiment was
</p>
<p>given in Example 4.2. To compute probabilities for a compound experiment it is
</p>
<p>usually convenient to use a condit ioning argument to simplify the reasoning. For
</p>
<p>example, say we choose one of two coins and toss it 4 times. We might inquire
</p>
<p>as to the probability of observing 2 or more heads. However , this probability will
</p>
<p>depend upon which coin was chosen, as for example in the situation where one coin
</p>
<p>is fair and the other coin is weighted. It is therefore convenient to define conditional
</p>
<p>probability mass fun ctions, px[klcoin 1 chosen] and px[klcoin 2 chosen] , since once
</p>
<p>we know which coin is chosen, we can easily specify the PMF. In particular, for
</p>
<p>this example the conditional PMF is a binomial one whose value of p depends upon
</p>
<p>which coin is chosen and with k denoting the number of heads (see (5.6)). Once
</p>
<p>the condit ional PMFs are known, we have by the law of total probability (see (4.4))
</p>
<p>that the probability of observing k heads for this experiment is given by the PMF
</p>
<p>px [k] px [klcoin 1 chosen]P[coin 1 chosen]
</p>
<p>+px[k lcoin 2 chosen]P[coin 2 chosen].</p>
<p/>
</div>
<div class="page"><p/>
<p>216 CHAPTER 8. CONDITIONAL PROBABILITY MASS FUNCTIONS
</p>
<p>Therefore, the desired probability of observing 2.or more heads is
</p>
<p>4
</p>
<p>P[X ~ 2] = Lpx[k]
k=2
</p>
<p>4
</p>
<p>= L(Px[klcoin 1 chosen]P[coin 1 chosen]
</p>
<p>k = 2
</p>
<p>+ px[klcoin 2 chosen]P[coin 2 chosen]) .
</p>
<p>The PMF that is required depends directly on the conditional PMFs (of which there
</p>
<p>are two). The use of conditional PMFs greatly simplifies our task in that given
</p>
<p>the event, i.e., the coin chosen, the PMF of the number of heads observed readily
</p>
<p>follows. Also, in many problems, including this one, it is actually the conditional
</p>
<p>PMFs that are specified in the description of the experimental procedure. It makes
</p>
<p>sense, therefore, to define a conditional PMF and study its properties. For the most
</p>
<p>part , the definitions and properties will mirror those of the conditional probabillity
</p>
<p>P[AIB]' where A and B are events defined on SX ,Y.
</p>
<p>8.2 Summary
</p>
<p>The utility of defining a conditional PMF is illustrated in Section 8.3. It is especially
</p>
<p>appropriate when the exp eriment is a compound one, in which the second part of
</p>
<p>the experiment depends upon the outcome of the first part. The definition of the
</p>
<p>conditional PMF is given in (8.7) . It has the usual properties of a PMF, that of
</p>
<p>being between 0 and 1 and also summing to one. Its properties and relationships
</p>
<p>are summarized by Properties 8.1-8.5. The conditional PMF is related to the joint
</p>
<p>PMF and the marginal PMFs by these properties. They are also depicted in Figure
</p>
<p>8.4 for easy reference. If the random variables are independent , then the conditional
</p>
<p>PMF reduces to the usual marginal PMF as shown in (8.22). For general probability
</p>
<p>calculat ions based on the conditional PMF one can use (8.23). In Section 8.5 it is
</p>
<p>shown how to use conditioning arguments to simplify the derivation of the PMF for
</p>
<p>Z = g(X, Y). The PMF can be found using (8.24), which makes use of the condi-
tional PMF. In particular, if X and Yare independent, the procedure is especially
</p>
<p>simplified with examples given in Section 8.5. The mean of the conditional PMF is
</p>
<p>defined by (8.30). It is computed by the usual procedures but uses the conditional
</p>
<p>PMF as the "averaging" PMF. It is next shown that the mean of the unconditional
</p>
<p>PMF can be found by averaging over the means of the conditional PMFs as given
</p>
<p>by (8.35). This simplifies the computation. Generation of realizations of random
</p>
<p>vectors (X,Y) can be simplified using conditioning arguments. An illustration and
</p>
<p>MATLAB code segment is given in Section 8.7. Finally, an application of condi-
</p>
<p>tioning to the modeling of human learning is described in Section 8.8. Utilizing the
</p>
<p>posterior PMF, which is a condit ional PMF, one can demonstrate that "learn ing"</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3. CONDITIONAL PROBABILITY MASS FUNCTION 217
</p>
<p>takes place as the result of observing the outcomes of repeated experiments. The
</p>
<p>degree of learning is embodied in the posterior PMF.
</p>
<p>8.3 Conditional Probability Mass Function
</p>
<p>We continue with the introductory example to illustrate the utility of the conditional
</p>
<p>probability mass function. Summarizing the introductory problem, we have an
</p>
<p>experimental procedure in which we first choose a coin, either coin 1 or coin 2. Coin
</p>
<p>1 has a probability of heads of PI, while coin 2 has a probability of heads of P2. Let
</p>
<p>X be the discrete random variable describing the outcome of the coin choice so that
</p>
<p>X = {1 if coin 1 is chosen
2 if coin 2 is chosen.
</p>
<p>Since Sx = {1, 2}, we assign a PMF to X of
</p>
<p>{
a i = 1
</p>
<p>px[i] = 1 - a i = 2 (8.1)
</p>
<p>where 0 &lt; a &lt; 1. The second part of the experiment consists of tossing the chosen
coin 4 times in succession. Call the outcome of the number of heads observed
</p>
<p>as Y and note that Sy = {O, 1,2,3, 4}. Hence, the overall set of outcomes of the
compound experiment is SX,y = Sx x Sy, which is shown in Figure 8.1. The overall
</p>
<p>y
</p>
<p>4
.-----,
,. &bull; I
</p>
<p>I :~A
3
</p>
<p>I,. &bull;&bull;
I
</p>
<p>,
2
</p>
<p>I
,I&middot; &bull;&bull;------
</p>
<p>SX,y
1 &bull; &bull;
</p>
<p>x
1 2
</p>
<p>Figure 8.1: Mapping for coin toss example, x denotes the coin chosen while y denotes
</p>
<p>the number of heads observed.
</p>
<p>outcome is described by the random vector (X, Y), where X is the coin chosen and
Y is the number of heads observed for the 4 coin tosses. If we wish to determine
</p>
<p>the probability of 2 or more heads, then this is the probability of the set A shown</p>
<p/>
</div>
<div class="page"><p/>
<p>218 CHAPTER 8. CONDITIONAL PROBABILITY MASS FUNCTIONS
</p>
<p>in Figure 8.1. It is given mathematically as
</p>
<p>P[A] = 2: px,y[i,j]
{(i,j) :(i ,j)EA}
</p>
<p>2 4
</p>
<p>= 2:2:PX,y[i,j].
i = 1 j=2
</p>
<p>(8.2)
</p>
<p>Hence , we need only specify the joint PMF to determine the desired probability. To
</p>
<p>do so we make use of our definition of the joint PMF as well as our earlier concepts
</p>
<p>from conditional probability (see Chapter 4). Recall from Chapter 7 the definitions
</p>
<p>of the joint PMF and marginal PMF as
</p>
<p>pX,y[i,j]
</p>
<p>px[i]
</p>
<p>P[X = i , Y =j]
</p>
<p>P[X = i].
</p>
<p>By using the definition of conditional probability for events we have
</p>
<p>Px,y[i,j] P[X = i,Y =j]
</p>
<p>pry = jlX = i]P[X = i]
</p>
<p>pry = jlX = i]px[i]
</p>
<p>(definition of joint PMF)
</p>
<p>(definition of conditional prob.)
</p>
<p>(definition of marginal PMF). (8.3)
</p>
<p>From (8.1) we have px[i] and from the experimental description we can determine
</p>
<p>pry = jlX = i]. When X = 1, we toss a coin with a probability of heads PI, and
when X = 2, we toss a coin with a probability of heads P2. Also, we have previously
shown that for a coin with a probability of heads Pi that is tossed 4 times, the
</p>
<p>number of heads observed has a binomial PMF. Thus, for i = 1,2
</p>
<p>pry = j!X = i] = (;) Pi (1 - Pi)4- j j = 0,1 ,2,3,4. (8.4)
</p>
<p>Note that the probability depends on the outcome X = i via Pi. Also, for a given
</p>
<p>value of X = i, the probability has all the usual properties of a PMF. These prop-
ert ies are
</p>
<p>&deg;:S pry = jlX = i] :S 1
4
</p>
<p>2:P[Y = jlX = i] = 1.
j=O
</p>
<p>It is therefore appropriate to define pry = jlX = i] as a conditional PMF. We will
denote it by
</p>
<p>pYlx[jli] = pry = j!X = i] j = 0,1,2,3,4.
</p>
<p>Examples are plotted in Figure 8.2 for PI = 1/4 and P2 = 1/2. Returning to our</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3. CONDITIONAL PROBABILITY MASS FUNCTION 219
</p>
<p>5
</p>
<p>1
3 42
</p>
<p>J
o
1
</p>
<p>0.5
</p>
<p>~0.4
</p>
<p>~
</p>
<p>:-::
~ .3
</p>
<p>~
</p>
<p>0.2
</p>
<p>0.1
</p>
<p>0
5 -14
</p>
<p>t
2 3
j
</p>
<p>o
</p>
<p>0.1 .
</p>
<p>0 .5.--~-~-~-~-~----,
</p>
<p>';::;"'0.4
</p>
<p>~
</p>
<p>:-::
~.3
</p>
<p>~
</p>
<p>0.2
</p>
<p>(a) i = 1, PI = 1/4 (b)i=2 ,P2=1/2
</p>
<p>Figure 8.2: Conditional PMFs given by (8.4).
</p>
<p>problem we can now determine the joint PMF. Using (8.3) we have
</p>
<p>px,y[i,j] = PYlx[j!i]px[i] (8.5)
</p>
<p>and using (8.4) and (8.1) the joint PMF is
</p>
<p>px,y[i,j] (;) Pi (1 - PI)4- ja
</p>
<p>(;) ~ ( 1 - P2)4-j (1 - a)
</p>
<p>i = l;j = 0,1,2,3,4
</p>
<p>i = 2;j = 0,1 ,2,3,4.
</p>
<p>Finally the desired probability is from (8.2)
</p>
<p>4 4
</p>
<p>P[A] = LPx,y[l ,j] + LPX,y[2,j]
j=2 j=2
</p>
<p>4 4
</p>
<p>L (~) Pi (1- pd4- ja +L (~) ~(1 - P2)4-j (1 - a).
j=2 J j=2 J
</p>
<p>As an example, if PI = 1/4 and P2 = 3/4, we have for a = 1/2, that P[A] = 0.6055,
but if a = 1/8, then P[A] = 0.8633. Can you explain this?
</p>
<p>Note from (8.5) that the conditional PMF is also expressed as
</p>
<p>[ '1'] - px,Y[i,jJPYIX J 't - ["J
Px't
</p>
<p>(8.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>220 CHAPTER 8. CONDITIONAL PROBABILITY MASS FUNCTIONS
</p>
<p>and is only a renaming for the conditional probability of the event that Aj = {s :
Y(s) = j} given that B, = {s : X(s) = i}. To make this connection we have
</p>
<p>PYlx[jli] = pry = jlX = i] = P[Xp[;-i':i] j]
</p>
<p>P[Aj n B i ]
=
</p>
<p>P[Bi ]
</p>
<p>P[AjIB i ]
</p>
<p>and hence PYIX[jli] is a conditional probability for the events A j and Bi.
</p>
<p>8.4 Joint, Conditional, and Marginal PMFs
</p>
<p>As evidenced by (8.6), there are relationships between the joint, conditional, and
</p>
<p>marginal PMFs. In this section we describe these relationships. To do so we rewrite
</p>
<p>the definition of the conditional PMF in slightly more generality as
</p>
<p>[ I ]
PX,y[Xi,Yj]
</p>
<p>PYIX Yj Xi = []
PX Xi
</p>
<p>(8.7)
</p>
<p>for a sample space SX ,Y which may not consist solely of integer two-tuples. It is
</p>
<p>always assumed that PX[Xi] i= O. Otherwise, the definition does not make any sense .
The conditional PMF, although appearing to be a function of two variables, Xi and
</p>
<p>Yj , should be viewed as a family or set of PMFs. Each PMF in the family is a valid
</p>
<p>PMF when Xi is considered to be a constant. In the example of the previous section,
</p>
<p>we had PYlxb l1] and PYlxbI2]. The family is therefore {PYlxbI1],PYlx[jI2]} and
each member is a valid PMF, whose values depend on j . Hence, we would expect
</p>
<p>that (see Problem 8.4)
</p>
<p>00
</p>
<p>L PYlx[jll] 1
j=-oo
</p>
<p>00
</p>
<p>L PYlx[jI2] 1
j=-oo
</p>
<p>but not I:~_ooPYlx[jli] = 1 (see also Problem 4.9). Before proceeding to list the
</p>
<p>relationships between the various PMFs, we give an example of the calculation of
</p>
<p>the conditional PMF based on (8.7).
</p>
<p>Example 8.1 - Two dice toss
</p>
<p>Two dice are tossed with all outcomes assumed to be equally likely. The number of
</p>
<p>dots observed on each die are added together. What is the conditional PMF of the
</p>
<p>sum ifit is known that the sum is even? We begin by letting Y be the sum and define
</p>
<p>X = 1 if the sum is even and X = 0 if the sum is odd. Thus, we wish to determine</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4. JOINT, CONDITIONAL, AND MARGINAL PMFS 221
</p>
<p>PYlx[jI1] and PYlx[jIO] for all j. The sample space for Y is Sy = {2, 3, . . . ,12}
</p>
<p>as can be seen from Table 8.1, which lists the sum of the two dice outcomes as a
</p>
<p>function of the outcomes for each die. The boldfaced entries are the ones for which
</p>
<p>j=l j=2 j=3 j=4 j=5 j=6
</p>
<p>i = 1 2 3 4 5 6 7
</p>
<p>i=2 3 4 5 6 7 8
</p>
<p>i=3 4 5 6 7 8 9
</p>
<p>i=4 5 6 7 8 9 10
</p>
<p>i=5 6 7 8 9 10 11
</p>
<p>i=6 7 8 9 10 11 12
</p>
<p>Table 8.1: The sum of the number of dots observed for two dice - boldface indicates
</p>
<p>an even sum.
</p>
<p>the sum is even and therefore comprise the sample space for PYlx[jI1]. Note that
</p>
<p>each outcome (i,j) has an assumed probability of occurring of 1/36. Now, using
</p>
<p>(8.7)
</p>
<p>[ '11] - px,y[l,j]PYIX J - px[l] j = 2,4,6,8,10,12 (8.8)
</p>
<p>where px,y[l , j] is the probability of the sum being even and also equaling j. Since we
</p>
<p>assume in (8.8) that j is even (otherwise PYlx[jI1] = 0), we have that pX,y[l,j] =
py[j] for j = 2,4,6,8,10,12. Also, there are 18 even outcomes, which results in
</p>
<p>px[l] = 1/2. Thus, (8.8) becomes
</p>
<p>py[j]
</p>
<p>1/2
</p>
<p>N j(1/36)
=
</p>
<p>1/2
</p>
<p>1
</p>
<p>18
Nj
</p>
<p>where N j is the number of outcomes in SX,y for which the sum is j. From Table
</p>
<p>8.1 we can easily find Nj so that
</p>
<p>1 j=218
3
</p>
<p>j=418
5
</p>
<p>j=6
pYlxUl1] =
</p>
<p>18 (8.9)
5 j=818
3
</p>
<p>j = 1018
1 j = 12.18</p>
<p/>
</div>
<div class="page"><p/>
<p>222 CHAPTER 8. CONDITIONAL PROBABILITY MASS FUNCTIONS
</p>
<p>Note that as expected L: j PYlx [j ll ] = 1. The reader is asked to verify by a similar
</p>
<p>calculation that (see Problem 8.7)
</p>
<p>2 j = 318
4 j = 518
</p>
<p>PYlx [j!O] =
6 j = 7 (8.10)18
4 j = 918
2 j = 11.18
</p>
<p>These condit ional PMFs are shown in Figure 8.3. Also, note that PYlx[j IO] =J.
</p>
<p>'00.4
</p>
<p>7 8 9 10 11 12
</p>
<p>t j
</p>
<p>123456
</p>
<p>&middot; .
</p>
<p>0.1 "':"':"l
o . .
</p>
<p>~
</p>
<p>~3
::...
I;:l., . .
</p>
<p>02 . . .; ;. .. ; . .; " ' ; .
&middot; .&middot; .
</p>
<p>1 2 3 4 5 6 7 8 9 10 11 12
</p>
<p>tJ
</p>
<p>'. .'
</p>
<p>.' '. '. "
</p>
<p>t to
0.1
</p>
<p>05
</p>
<p>Mean of condit ional PMF Mean of condit ional PMF
</p>
<p>(a) (b)
</p>
<p>Figure 8.3: Conditional PMFs for Example 8.1.
</p>
<p>1 - PYlx[jll]. Each condit ional PMF is generally different.
</p>
<p>o
There are several relationships between the joint, marginal, and conditional PMFs.
</p>
<p>We now summariz e these as properties.
</p>
<p>Property 8.1 - Joint PMF yields conditional PMFs.
</p>
<p>If the joint PMF PX,y[Xi ,Yj] is known , then the condit ional PMFs are found as
</p>
<p>PX'y[Xi ,Yj]
</p>
<p>L:j PX,y[Xi ,Yj]
PX,y[ Xi ,Yj]
</p>
<p>L: i PX,y[Xi ,Yj]"
</p>
<p>(8.11)
</p>
<p>(8.12)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4. JOINT, CONDITIONAL, AND MARGINAL PMFS 223
</p>
<p>Proof: Since the marginal PMF PX[Xi] is found as L jPX,y[Xi ,Yj], the denominator
</p>
<p>of (8.7) can be replaced by this to yield (8.11) . The equation (8.12) is similarly
</p>
<p>proven .
</p>
<p>o
Hence, we see that the conditional PMF is just the joint PMF with Xi fixed and then
</p>
<p>normalized by LjPX,y[Xi ,Yj] so that it sums to one. In Figure 8.3a, the conditional
</p>
<p>PMF PYlxbl1] evaluated at j = 8 is just px ,y [l , 8] = 5/36 divided by the sum of
the probabilities px,y[l ,'] = 18/36, where "." indicates all possible values of j. This
yields PYlx[811] = 5/18.
</p>
<p>Property 8.2 - Conditional PMFs are related.
</p>
<p>[ I]
PYlx[Yjlxi]PX[Xi]
</p>
<p>PXIY Xi Yj = [ ]
PY Yj
</p>
<p>Proof: By interchanging X and Y in (8.7) we have
</p>
<p>[ I ]
PY,X[Yj , Xi]
</p>
<p>PXIY Xi Yj = []
PY Yj
</p>
<p>but
</p>
<p>(8.13)
</p>
<p>PY,X[Yj , Xi] = pry = Yj ,X = Xi ]
P[X = Xi ,Y = Yj]
</p>
<p>PX,y[Xi ,Yj]
</p>
<p>and therefore
</p>
<p>(since A n B = B n A)
</p>
<p>(8.14)[ I ]
PX,y(Xi ,Yj]
</p>
<p>PX IY Xi Yj = [].
PY Yj
</p>
<p>Using PX,y(Xi ,Yj] = PYIX [YjI Xi]pX [Xi] from (8.7) in (8.14) yields the desired result
(8.13).
</p>
<p>o
</p>
<p>Property 8.3 - Conditional PMF is expressible using Bayes' rule.
</p>
<p>[ I]
PXIY [Xi IYj]pY[Yj]
</p>
<p>PYIX Yj Xi =
L j PX IY[x i IYj]pY[Yj]
</p>
<p>Proof: From (8.11) we have that
</p>
<p>and using (8.14) we have
</p>
<p>(8.15)
</p>
<p>(8.16)
</p>
<p>(8.17)</p>
<p/>
</div>
<div class="page"><p/>
<p>224 CHAPTER 8. CONDITIONAL PROBABILITY MASS FUNCTIONS
</p>
<p>which when substituted into (8.16) yields the desired result.
o
</p>
<p>Property 8.4 - Conditional PMF and its corresponding marginal PMF
</p>
<p>yields the joint PMF.
</p>
<p>PX,y[Xi, Yj] = PYIX[Yj!Xi]PX[Xi]
</p>
<p>PX,Y[Xi ,Yj] = PXIY [XiIYj]py [Yj]
</p>
<p>(8.18)
</p>
<p>(8.19)
</p>
<p>Proof: (8.18) follows from definition of conditional PMF (8.7) and (8.19) is just
</p>
<p>(8.17).
</p>
<p>o
</p>
<p>Property 8.5 - Conditional PMF and its corresponding marginal PMF
</p>
<p>yields the other marginal PMF.
</p>
<p>(8.20)
</p>
<p>Proof: This is just the law of total probability in disguise or equivalently just
</p>
<p>Py[Yj] = :EiPX,y[Xi ,Yj] (marginal PMF from joint PMF) .
o
</p>
<p>These relationships are summarized in Figure 8.4. Notice that the joint PMF can
</p>
<p>be used to find all the marginals and conditional PMFs (see Figure 8.4a). The
</p>
<p>conditional PMF and its corresponding marginal PMF can be used to find the
</p>
<p>joint PMF (see Figure 8.4b). Finally, the conditional PMF and its corresponding
</p>
<p>marginal PMF can be used to find the other conditional PMF (see Figure 8.4c). As
</p>
<p>emphasized earlier, we cannot determine the joint PMF from the marginals. This
</p>
<p>is only possible if X and Yare independent random variables since in this case
</p>
<p>(8.21)
</p>
<p>(8.22)
</p>
<p>In addition, for independent random variables, the use of (8.21) in (8.7) yields
</p>
<p>[ ]
PX[Xi]PY[Yj]
</p>
<p>PY\X YjlX i = [] = Py[Yj]
PX Xi
</p>
<p>or the conditional PMF is the same as the unconditional PMF. There is no change
</p>
<p>in the probabilities of Y whether or not X is observed. This is of course consistent
</p>
<p>with our previous definition of statistical independence.
</p>
<p>Finally, for more general conditional probability calculations we sum the appro-
</p>
<p>priate values of the condit ional PMF to yield (see Problem 8.14)
</p>
<p>pry E AIX = Xi] = L PYlx[YjlxiJ.
{j :YjEA}
</p>
<p>(8.23)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.5. SIMPLIFYING PROBABILITY CALCULATIONS 225
</p>
<p>PX = :LyPX,Y PY = :LxPX,Y PX PY
</p>
<p>_ PX.Y
</p>
<p>PYIX - L y PX ,Y
</p>
<p>(a)
</p>
<p>_ PX,Y
</p>
<p>PXIY - L xPX,Y
</p>
<p>PX
</p>
<p>PYIX
</p>
<p>(b)
</p>
<p>PY
</p>
<p>PXIY
</p>
<p>PYjX
</p>
<p>(c) (Can also interchange X
</p>
<p>and Y for similar results)
</p>
<p>_ PY IXPX
PXIY - PY
</p>
<p>Figure 8.4: Conditional PMF relationships.
</p>
<p>8.5 Simplifying Probability Calculations using
</p>
<p>Conditioning
</p>
<p>As alluded to in the introduction, conditional PMFs can be used to simplify prob-
</p>
<p>ability calculations. To illustrate the use of this approach we once again consider
</p>
<p>the determination of the PMF for Z = X + Y, where X and Yare independent
discrete random variables that take on integer values. We have already seen that the
</p>
<p>solution is pz = Px*py , where * denotes discrete convolution {see (7.22)). To solve
this problem using conditional PMFs, we ask ourselves the question: Could I find
</p>
<p>the PMF of Z if X were known? If so, then we should be able to use conditioning
</p>
<p>arguments to first find the conditional PMF of Z given X , and then uncondition</p>
<p/>
</div>
<div class="page"><p/>
<p>226 CHAPTER 8. CONDITIONAL PROBABILITY MASS FUNCTIONS
</p>
<p>the result to yield the PMF of Z. Let us say that X is known and that X = i. As
a result , we have that conditionally Z = i + Y, where i is just a constant. This is
sometimes denoted by ZI(X = i). But this is a transformation from one discrete
</p>
<p>random variable Y to another discrete random variable Z . We therefore wish to
</p>
<p>determine the PMF of a random variable that has been summed with a constant.
</p>
<p>It is not difficult to show that if a discrete random variable U has a PMF pu[j],
</p>
<p>then U + i has the PMF pu[j - i] or the PMF is just shifted to the right by i units.
Thus, the conditional PMFof Z evaluated at Z = j iSPzlx[jli] = PYlx[j -iii]. Now
</p>
<p>to find the unconditional PMF of Z we use (8.20) with an appropriate change of
</p>
<p>variables to yield
00
</p>
<p>pz[j] = L PZlx [jli]px [i]
i=-oo
</p>
<p>and since PZlx[jli] = PYlx[j - iii], we have
</p>
<p>00
</p>
<p>pz[j] = L PYlx[j - ili]px[i].
i=-oo
</p>
<p>But X and Yare independent so that PYIX = PY and therefore we have the final
</p>
<p>result
00
</p>
<p>PZ[j] = L py[j - i]px[i]
t=-oo
</p>
<p>which agrees with our earlier one. Another example follows.
</p>
<p>Example 8.2 - PMF for Z = max(X, Y)
Let X and Y be discrete random variables that take on integer values. Also, assume
</p>
<p>independence of the random variables X and Y and that the marginal PMFs of X
</p>
<p>and Yare known. To find the PMF of Z we use (8.20) or the law of total probability
</p>
<p>to yield
00
</p>
<p>pz[k] = L PZlx[kli]px[i].
i=-oo
</p>
<p>(8.24)
</p>
<p>Now PX is known so that we only need to determine PZlx for X = i. But given that
X = i, we have that Z = max(i, Y) for which the PMF is easily found. We have
thus reduced the original problem, which is to determine the PMF for the random
</p>
<p>variable obtained by transforming from (X, Y) to Z, to determining the PMF for
</p>
<p>a function of only one random variable. Letting g(Y) = max(i, Y) we see that the
</p>
<p>function appears as shown in Figure 8.5. Hence, using (5.9) for the PMF of a single
</p>
<p>transformed discrete random variable we have
</p>
<p>PZlx[kli] = L PYlx[jli].
{j :g(j)=k}</p>
<p/>
</div>
<div class="page"><p/>
<p>8.5. SIMPLIFYING PROBABILITY CALCULATIONS
</p>
<p>g(y) = max(i, y)
</p>
<p>227
</p>
<p>i
</p>
<p>y
</p>
<p>Figure 8.5: Plot of the function g(y) = max(i, y).
</p>
<p>Solving for j in g(j) = k (refer to Figure 8.5) yields no solution for k &lt; i, the
multiple solutions j = ... ,i-I, i for k = i, and the single solution j = k for
k = i + 1, i + 2, .... This produces
</p>
<p>PZlx[kli] = { b~=-ooPY'X[jli]
PYlx[kli]
</p>
<p>Using this in (8.24) produces
</p>
<p>k = . .. , i - 2,i-I
</p>
<p>k = i
</p>
<p>k = i + 1, i + 2, ....
(8.25)
</p>
<p>(use (8.25))
</p>
<p>k-l 00
</p>
<p>pz[k] = L PZlx[kli]px[i] +PZlx[klk]px[k] + L PZlx[kli]px[i] (break up sum)
i=-oo i=k+l
k-l k
</p>
<p>L PYlx[kli]px[i] + L PYlx[jlk]px[k] + 0
i=-oo j=-oo
</p>
<p>k-l k
</p>
<p>L py[k]px[i] + L PY [j]px [k] (since X and Yare independent)
i=- oo j=-oo
</p>
<p>k-l k
</p>
<p>py[k] L px[i] +px[k] L py[j].
i=-oo j=-oo
</p>
<p>Note that due to the independence assumption this final result can also be written
</p>
<p>as
k-l k
</p>
<p>pz[k] = L px,y[i, k] + L px,y[k,j]
i=- oo j=-oo
</p>
<p>so that the PMF of Z is obtained by summing all the points of the joint PMF
</p>
<p>shown in Figure 8.6 for k = 2, as an example. These point comprise the set {(x, y) :
max(x, y) = 2 and x = i, y = j}. It is now clear that we could have solved this
problem in a more direct fashion by making this observation. As in most problems,
</p>
<p>however , the solution is usually trivial once it is known!</p>
<p/>
</div>
<div class="page"><p/>
<p>228 CHAPTER 8. CONDITIONAL PROBABILITY MASS FUNCTIONS
</p>
<p>Y
</p>
<p>&bull; &bull; &bull; &bull; 2 &bull; &bull; (2,2)
1 &bull;
</p>
<p>x
</p>
<p>1 2&bull;
&bull;
</p>
<p>Figure 8.6: Points of joint PMF to be summed to find PMF of Z = max(X, Y) for
</p>
<p>k = 2.
</p>
<p>o
As we have seen, a general procedure for determining the PMF for Z = g(X, Y)
</p>
<p>when X and Yare independent is as follows:
</p>
<p>1. Fix X = Xi and let ZI(X = Xi ) = g(Xi,Y)
</p>
<p>2. Find the PMF for ZIX by using the techniques for a transformation of a single
random variable Y into another random variable Z. The formula is from (5.9),
</p>
<p>where the PMFs are first converted to conditional PMFs
</p>
<p>PZIX[Zklxi] = L PY!X[YjIXi]
{j :9(X i ,Yj )=zd
</p>
<p>L PY[Yj]
{j :9(X i ,Yj )=zd
</p>
<p>for each Xi
</p>
<p>for each Xi (due to independence) .
</p>
<p>3. Uncondition the conditional PMF to yield the desired PMF
</p>
<p>PZ[Zk] = LPzlx[Zklxi]PX[Xi].
i
</p>
<p>In general, to compute probabilities of events it is advantageous to use a condi-
</p>
<p>tioning argument, whether or not X and Yare independent. Where previously we
</p>
<p>have used the formula
</p>
<p>pry E A] = L PY[Yj]
{j :YjEA}
</p>
<p>to compute the probability, a conditioning approach would replace Py[Yj] by
</p>
<p>L iPYlx[Yjl xi]PX[Xi] to yield
</p>
<p>pry E A] = L LPYlx[Yjlxi]PX[Xi]
{j :YjEA} i
</p>
<p>(8.26)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.6. MEAN OF THE CONDITIONAL PMF
</p>
<p>to determine the probability. Equivalently, we have that
</p>
<p>pry E A] = ~ [ r,L PYIX[YjIXi]] ~
z , {J &middot;Y3EA} , unconditioning
</p>
<p>v
</p>
<p>condit ioning
</p>
<p>In this form we recognize the conditional probability of (8.23), which is
</p>
<p>pry E AIX = Xi] = L PYlx[Yjlxi]
{j:YjEA}
</p>
<p>and the unconditional probability
</p>
<p>with the latter being just a restatement of the law of total probability.
</p>
<p>8.6 Mean of the Conditional PMF
</p>
<p>229
</p>
<p>(8.27)
</p>
<p>(8.28)
</p>
<p>Since the conditional PMF is a PMF, it exhibits all the usual properties. In particu-
</p>
<p>lar, we can determine attributes such as the expected value of a random variable Y,
</p>
<p>when it is known that X = Xi. This expected value is the mean of the conditional
</p>
<p>PMF PYlx, Its definition is the usual one
</p>
<p>L YjPYIX [Yj IXi]
j
</p>
<p>(8.29)
</p>
<p>where we have replaced PY by PYlx , It should be emphasized that since the con-
</p>
<p>ditional PMF depends on Xi, so will its mean. Hence, the mean of the conditional
</p>
<p>PMF is a constant when we set Xi equal to a fixed value . We adopt the notation for
</p>
<p>the mean of the conditional PMF as EYlx[Ylxi] . This notation includes the sub-
</p>
<p>script "Y jX " to remind us that the averaging PMF is the conditional PMF PYlx ,
Also, the use of "Y lxi" as the argument will remind us that the averaging PMF is
the conditional PMF that is specified by X = Xi in the family of conditional PMFs.
The mean is therefore defined as
</p>
<p>EYIX[Ylxi] = LYjPYlx[Yjlxi].
j
</p>
<p>(8.30)
</p>
<p>Although we have previously asserted that the mean is a constant, here it is to be
</p>
<p>regarded as a function of Xi. An example of its calculation follows.</p>
<p/>
</div>
<div class="page"><p/>
<p>230 CHAPTER 8. CONDITIONAL PROBABILITY MASS FUNCTIONS
</p>
<p>Example 8.3 - Mean of conditional PMF - continuation of Example 8.1
</p>
<p>We now compute all the possible values of EYlx [Ylxi] for the problem described
</p>
<p>in Example 8.1. There Xi = 1 or Xi = &deg;and the corresponding conditional PMFs
are given by (8.9) and (8.10) , resp ectively. The means of the conditional PMFs are
</p>
<p>therefore
</p>
<p>EYlx [Y 11] 2 (118) + 4 (1
38)
</p>
<p>+ 6 (1
58)
</p>
<p>+ 8 (1
58)
</p>
<p>+ 10 (1
38)
</p>
<p>+ 12 (118) = 7
</p>
<p>Ey \x[YIO] = 3 (1
28)
</p>
<p>+ 5 (1~) + 7 (1
68)
</p>
<p>+ 9 (1~) + 11 (1
28)
</p>
<p>= 7
</p>
<p>and are shown in Figure 8.3. In this example the means of the conditional PMFs
are the same, but will not be in general. We can expect that g(xd = Ey1x[Ylxi]
</p>
<p>will vary with Xi.
</p>
<p>We could also compute the variance of the conditional PMFs. This would be
</p>
<p>var(Ylxi) = :L (Yj - Ey1x[Ylxi])2 PYlx[Yjl xiJ.
j
</p>
<p>(8.31)
</p>
<p>The reader is asked to do this in Problem 8.22. (See also Problem 8.23 for an
alternate expression for var(Ylxi)') Note from Figure 8.3 that we do not expect
these to be the same.
</p>
<p>&amp;. What is the "conditional expectation"?
The fun ction g(Xi) = EYlx[Ylxi] is the mean of the conditional PMF PYlx[YjlxiJ.
</p>
<p>Alternatively, it is known as the conditional mean. This terminology is widespread
</p>
<p>and so we will adhere to it , although we should keep in mind that it is meant to
</p>
<p>denote the usual mean of the conditional PMF. It is also of interest to determine
</p>
<p>the expectation of other quantities besides Y with respect to the conditional PMF.
</p>
<p>This is called the conditional expectation and is symbolized by EYlx[g(Y)lxi]. The
</p>
<p>latter is called the conditional expect at ion of g(Y). For example, if g(Y) = y 2,
</p>
<p>then it becomes the conditional expectation of y 2 or equivalently the conditional
</p>
<p>second moment. Lastly, the reader should be aware that the conditional mean is the
</p>
<p>optimal predictor of a random variable based on observation of a second random
</p>
<p>variable (see Problem 8.27) .
</p>
<p>We now give another example of t he computation of the conditional mean.
</p>
<p>Example 8.4 - Toss one of two dice.
</p>
<p>There are two dice having different numbers of dots on their faces. Die 1 is the
</p>
<p>usual type of die with faces having 1,2,3,4,5, or 6 dots. Die 2 has been mislabled</p>
<p/>
</div>
<div class="page"><p/>
<p>8.6. MEAN OF THE CONDITIONAL PMF 231
</p>
<p>with its faces having 2,3,2,3,2, or 3 dots. A die is selected at random and tossed.
</p>
<p>Each face of the die is equally likely to occur. What is the expected number of dots
</p>
<p>observed for the tossed die? To solve this problem first observe that the outcomes
</p>
<p>will depend upon which die has been tossed. As a result, the conditional expectation
</p>
<p>of the number of dots will depend upon which die is initially chosen. We can view
</p>
<p>this problem as a conditional one by letting
</p>
<p>X = {I if die 1 is chosen
2 if die 2 is chosen
</p>
<p>and Y is the number of dots observed. Thus, we wish to determine EYlx[Yll] and
</p>
<p>EY1x[Y12]. But if die 1 is chosen, the conditional PMF is
</p>
<p>and if die 2 is chosen
</p>
<p>j = 1,2,3,4,5,6
</p>
<p>j = 2,3.
</p>
<p>(8.32)
</p>
<p>(8.33)
</p>
<p>The latter conditional PMF is due to the fact that for die 2 half the sides show 2
</p>
<p>dots and the other half of the sides show 3 dots. Using (8.30) with (8.32) and (8.33),
</p>
<p>we have that
</p>
<p>6
</p>
<p>LjpYlx[jI1] = ~
j=l
</p>
<p>3
</p>
<p>EYlx[Y12] = LjpYlx[jI2] = ~.
j=2
</p>
<p>(8.34)
</p>
<p>An example of typical outcomes for this experiment is shown in Figure 8.7. For
</p>
<p>50 trials of the experiment Figure 8.7a displays the outcomes for which die 1 was
</p>
<p>chosen and Figure 8.7b displays the outcomes for which die 2 was chosen. It is
</p>
<p>interesting to note that the estimated mean for Figure 8.7a is 3.88 and for Figure
</p>
<p>8.7b it is 2.58. Note that from (8.34) the theoretical conditional means are 3.5 and
</p>
<p>2.5, respectively.
</p>
<p>(;
</p>
<p>In the previous example, we have determined the conditional means, which are the
</p>
<p>means of the conditional PMFs. We also might wish to determine the unconditional
</p>
<p>mean, which is the mean of Y. This is the number of dots observed as a result
</p>
<p>of the overall experiment, without first conditioning on which die was chosen. In
</p>
<p>essence , we wish to determine Ey [Y]. Intuitively, this is the average number of
dots observed if we combined Figures 8.7a and 8.7b together (just overlay Figure
</p>
<p>8.7b onto Figure 8.7a) and continued the experiment indefinitely. Hence , we wish
</p>
<p>to determine Ey [Y] for the following experiment:</p>
<p/>
</div>
<div class="page"><p/>
<p>232 CHAPTER 8. CONDITIONAL PROBABILITY MASS FUNCTIONS
</p>
<p>5010 20 30 40
Trial number
</p>
<p>'. ., "
</p>
<p>". .,
</p>
<p>"
</p>
<p>o
o5010 20 30 40
</p>
<p>Trial number
</p>
<p>. '
</p>
<p>r - 1o
o
</p>
<p>2 2
</p>
<p>8 8
</p>
<p>6 6
~ ~
</p>
<p>S S
o 0
Col ~ 4
-;; 4 ::l
</p>
<p>o 0
</p>
<p>(a) Outcomes when die 1 chosen (b) Outcomes when die 2 chosen
</p>
<p>Figure 8.7: Computer simulated outcomes ofrandomly selected die toss experiment.
</p>
<p>1. Choose die 1 or die 2 with probability of 1/2.
</p>
<p>2. Toss the chosen die.
</p>
<p>3. Count the number of dots on the face of tossed die and call this the outcome of
</p>
<p>the random variable Y .
</p>
<p>A simple MATLAB program to simulate this exp eriment is given as
</p>
<p>for m=1:M
</p>
<p>if rand(1,1)&lt;O.5
</p>
<p>y(m,1)=PMFdata(1,[12 3 4 5 6]',[1/6 1/6 1/6 1/6 1/6 1/6]');
</p>
<p>else
</p>
<p>y(m,1)=PMFdata(1,[2 3]',[1/2 1/2]');
</p>
<p>end
</p>
<p>end
</p>
<p>where the subprogram PMFdata.m is listed in Appendix 6B. After the code is ex-
</p>
<p>ecuted there is an array y, which is M x 1, containing M realizations of Y. By
</p>
<p>taking the sample mean of the elements in the array y, we will have estimated
</p>
<p>Ey[Y). But we expect about half of the realizations to have used the fair die and
the other half to use the mislabled die. As a result , we might suppose that the
</p>
<p>unconditional mean is just the average of the two conditional means. This would be
</p>
<p>(1/2)(7/2) + (1/2)(5/2) = 3, which turns out to be the true result. This conjecture
is also strengthened by the results of Figure 8.7. By overlaying the plots we have
</p>
<p>50 outcomes of the experiment for which the sample mean is 3.25. Let 's see how to
</p>
<p>verify this.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.6. MEAN OF THE CONDITIONAL PMF 233
</p>
<p>To determine the theoretical mean of Y, i.e., the unconditional mean, we will
</p>
<p>need py[j]. But given the conditional PMF and the marginal PMF we know from
</p>
<p>Figure 8.4c that the joint PMF can be found. Hence, from (8.32) and (8.33) and
</p>
<p>px[i] = 1/2 for i = 1,2, we have
</p>
<p>Px,y[i ,j]
</p>
<p>To find py [j] we use
</p>
<p>pYlx[jli]px[i]
</p>
<p>{
4
</p>
<p>{2 i = 1; j = 1,2,3,4,5,6
i = 2;j = 2,3.
</p>
<p>2
</p>
<p>py[j] = LPx,Y[i,j]
</p>
<p>i=l
</p>
<p>{
px,y[1 ,j] = 112 j = 1,4,5,6
px,y[1 ,j] +px,y[2,j] = 112 + ~ =! j =2,3.
</p>
<p>Thus, the unconditional mean becomes
</p>
<p>6
</p>
<p>Ey[Y] = Ljpy[j]
</p>
<p>j=l
</p>
<p>1 (112) + 2 (t) + 3 (t) + 4 (112) + 5 (112) + 6 (112)
3.
</p>
<p>This value is sometimes called the unconditional expectation. Note that for this
</p>
<p>example, we have upon using (8.34)
</p>
<p>or the unconditional mean is the average of the conditional means. This is true in
</p>
<p>general and is summarized by the relationship
</p>
<p>To prove this relationship is straightforward. Starting with (8.35) we have
</p>
<p>(8.35)</p>
<p/>
</div>
<div class="page"><p/>
<p>234 CHAPTER 8. CONDITIONAL PROBABILITY MASS FUNCTIONS
</p>
<p>L EYIX[YI Xi]PX [Xi]
i
</p>
<p>~ ~ ( ~ YiPYIX[YiIX;I) px[x;] (definition of condit ional mean)
</p>
<p>L LYjPX'Y[fi,tj] PX[Xi] (definition of conditional PMF)
. , PX Xi
Z J
</p>
<p>= LYj LPX,Y[Xi' Yj]
j
</p>
<p>LYjpy[Yj] (marginal PMF from joint PMF)
j
</p>
<p>= Ey[Y] .
</p>
<p>In (8.35) we can consider g(xd = EYjX[y!Xi] as the transformed outcome of the
</p>
<p>coin choice part of the experiment, where X = Xi is the outcome of the coin choice.
</p>
<p>Since before we choose the coin to toss , we do not know which one it will be , we
</p>
<p>can consider g(X) as a transformed random variable whose values are g(Xi)' By this
</p>
<p>way of viewing things, we can define a random variable as g(X) = EYlx[YIX] and
therefore rewrite (8.35) as
</p>
<p>Ey [Y] = Ex[g(X)]
</p>
<p>or explicit ly we have that
</p>
<p>(8.36)
</p>
<p>In effect , we have computed the expectation of a random variable in two steps.
</p>
<p>Step 1 is to compute a conditional expectation EYlx while step 2 is to undo the
</p>
<p>conditioning by averaging the result with respect to the PMF of X. An example is
</p>
<p>the previous coin tossing experiment. The utility in doing so is that the conditional
</p>
<p>PMFs were easily found and hence also the means of the conditional PMFs, and
</p>
<p>finally the averaging with respect to Px is easily carried out to yield the desired
</p>
<p>result. We illustrate the use of (8.36) with another example.
</p>
<p>Example 8.5 - Random number of coin tosses
</p>
<p>An experiment is conducted in which a coin with a probability of heads P is tossed
</p>
<p>M times. However, M is a random variable with M '" PoisfX]. For example, if a
</p>
<p>realization of M is generated, say M = 5, then the coin is tossed 5 times in succes-
</p>
<p>sion. We wish to determine the average number of heads observed. Conditionally
</p>
<p>on knowing the value of M , we have a binomial PMF for the number of heads Y.
</p>
<p>Hence, for M = i we have upon using the binomial PMF (see (5.6))
</p>
<p>j = 0,1, ... , i; i = 0,1, . . , .</p>
<p/>
</div>
<div class="page"><p/>
<p>8.7. COMPUTER SIMULATION BASED ON CONDITIONING
</p>
<p>Now using (8.36) and replacing X with M we have
</p>
<p>and for a binomial PMF we know that EYIM[Yli] = ip so that
</p>
<p>But for a Poisson random variable EM[M] = .x, which yields the final result
</p>
<p>Ey[Y] = .xp.
</p>
<p>235
</p>
<p>It can be shown more generally that Y '" Pois(.xp) (see Problem 8.26) so that our
</p>
<p>result for the mean of Y follows directly from knowledge of the mean of a Poisson
</p>
<p>random variable.
</p>
<p>8.7 Computer Sim ulation B ased on Con d ition ing
</p>
<p>In Section 7.11 we discussed a simple method for generating realizations of jointly
</p>
<p>distributed discrete random variables (X,Y) using MATLAB. To do so we required
</p>
<p>the joint PMF. Using conditioning arguments, however, we can frequently simplify
</p>
<p>the procedure. Since PX,y[Xi ,Yj] = PYlx[Yjl x i ]PX[Xi ], a realization of (X,Y) can
be obtained by first generating a realization of X according to its marginal PMF
</p>
<p>PX[Xi]. Then, assuming that X = Xi is obtained, we next generate a realization of
Y according to the condit ional PMF PYIX[Yj!Xi]. (Of course, if X and Yare inde-
</p>
<p>pendent , we replace the second step by the generation of Y according to PY[Yj] since
</p>
<p>in this case PYlx[Yj lx i] = PY[Yj] .) This is also advantageous when the problem de-
</p>
<p>scription is formulated in terms of conditional PMFs, as in a compound experiment .
</p>
<p>To illustrate this approach with the one described previously we repeat Example
</p>
<p>7.15.
</p>
<p>Example 8.6 - Generat ing rea lizat ions of joint ly d istributed ra ndom
</p>
<p>variables - E x ample 7.15 (continued)
</p>
<p>The joint PMF of Example 7.15 is shown in Figure 8.8, where the solid circles
</p>
<p>represent the sample points and the values of the joint PMF are shown to the right
</p>
<p>of the sample points. To use a conditioning approach we need to find PX and PYlx ,
</p>
<p>But from Figure 8.8, if we sum along the columns we obtain
</p>
<p>{
</p>
<p>I i = 0
px [i] = i
</p>
<p>4
- i = 1</p>
<p/>
</div>
<div class="page"><p/>
<p>236 CHAPTER 8. CONDITIONAL PROBABILITY MASS FUNCTIONS
</p>
<p>y
</p>
<p>1
1 &bull; .!.
8 2
</p>
<p>.!. .!.
X
</p>
<p>1
</p>
<p>Figure 8.8: Joint PMF for Example 8.6.
</p>
<p>and using the definition of the condit ional PMF, we have
</p>
<p>PYlxUIO] =
pX,y[O,j]
</p>
<p>px[O]
</p>
<p>{~ -l j = O1/4 - 2
1 8 1
</p>
<p>* =2 j=l
</p>
<p>and .
</p>
<p>PYlx [jl l ]
pX,y[ l , j ]
</p>
<p>px [l ]
</p>
<p>{.'L' _l j =O3/4 - 3
~ _2 j = 1.3/4 - 3
</p>
<p>The MATLAB segment of code shown below generates M realizat ions of (X ,Y )
</p>
<p>using this condit ioning approach.
</p>
<p>for m=1:M
</p>
<p>ux=rand(1, 1) ;
</p>
<p>uy=rand (1 ,1) ;
</p>
<p>if ux&lt;=1/4; I. Refer to px[i]
x(m,1)=O;
</p>
<p>i f uy&lt;=1/2 I. Refer to pylx[j IO]
y(m,1)=O;
</p>
<p>else
</p>
<p>y(m,1)=1;
</p>
<p>end
</p>
<p>else
</p>
<p>x(m,1)=1; I. Refer to px[i]</p>
<p/>
</div>
<div class="page"><p/>
<p>8.8. REAL-WORLD EXAMPLE - MODELING HUMAN LEARNING 237
</p>
<p>i f uy&lt;=1/3 %Refer t o pyl x [ jI 1]
y(m, 1)=0;
</p>
<p>else
</p>
<p>y(m, 1)=1;
</p>
<p>end
</p>
<p>end
</p>
<p>end
</p>
<p>The reader is asked to test this program in Problem 8.29.
</p>
<p>8.8 Real-World Example - Modeling Human Learning
</p>
<p>A 2 year-old child who has learned to walk can perform tasks that not even the
</p>
<p>most sophisticated robots can match. For example, a 2 year-old child can easily
</p>
<p>maneuver her way to a favorite toy, pick it up, and start to play with it. Robots,
</p>
<p>powered by machine vision and mechanical grippers, have a hard time performing
</p>
<p>this supposedly simple task. It is not surprisingly, therefore, that one of the holy
</p>
<p>grails in cognitive science and also machine learning is to figure out how a child
</p>
<p>does this. If we were able to understand the thought processes that were used
</p>
<p>to successfully complete this task, then it is conceivable that a machine might be
</p>
<p>built to do the same thing. Many models of human learning employ a Bayesian
</p>
<p>framework [Tenenbaum 1999]. This approach appears to be fruitful in that using
</p>
<p>Bayesian modeling we are ab le to discriminate with more and more accuracy as
</p>
<p>we repeatedly perform an experiment and observe the outcome. This is analogous
</p>
<p>to a child attempting to pick up the toy, dropping it , picking it up again after
</p>
<p>having learned something about how to pick it up , dropping it , etc., until finally
</p>
<p>she is successful. Each time the experiment, attempting to pick up the toy, is
</p>
<p>repeated the child learns something or equivalently narrows down the number of
</p>
<p>possible strategies. In Bayesian analysis, as we will show next, the width of the
</p>
<p>PMF decreases as we observe more outcomes. This is in some sense saying that
</p>
<p>our uncertainty about the outcome of the experiment decreases as it is performed
</p>
<p>more times. Although not a perfect analogy, it does seem to possess some critical
</p>
<p>elements of the human learning process. Therefore, we illustrate this modeling with
</p>
<p>the simple example of coin tossing.
</p>
<p>Suppose we wish to "learn" whether a coin is fair (p = 1/2) or is weighted
</p>
<p>(p 1'= 1/2). One way to do this is to repeatedly toss the coin and count the number
of heads observed. We would expect that our certainty about the conclusion, that
</p>
<p>the coin is fair or not, would increase as the number of trials increases. In the
</p>
<p>Bayesian model we quantify our knowledge about the value of p by assuming that
</p>
<p>p is a random variable. Our particular coin, however, has a fixed probability of
</p>
<p>heads. It is just that we do not know what it is and hence our belief about the value</p>
<p/>
</div>
<div class="page"><p/>
<p>238 CHAPTER 8. CONDITIONAL PROBABILITY MASS FUNCTIONS
</p>
<p>of p is embodied in an assumed PMF. This is a slightly different interpretation of
</p>
<p>probability than our previous relative frequency interpretation. To conform to our
</p>
<p>previous notation we let the probability of heads be denoted by the random variable
</p>
<p>Y and its values by Yj' Then, we determine its PMF. Our state of knowledge will be
</p>
<p>high if the PMF is highly concentrated about a particular value, as for example in
</p>
<p>Figure 8.9a. If, however, the PMF is spread out or "diffuse", our state of knowledge
</p>
<p>will be low, as for example in Figure 8.9b. Now let's say that we wish to learn the
</p>
<p>0.8 0.8
</p>
<p>.'"
.i:?o 6 .&gt;-.
~
</p>
<p>0.4 . .
</p>
<p>~
~0.6
</p>
<p>~
</p>
<p>0.4
</p>
<p>0.2 0.2 .
</p>
<p>1.50.5
</p>
<p>Yj
o
</p>
<p>o'----__--'----L...J..--'----.l..-J.--'----.l..-J.--L.-.L.-__-J
-0.51.5
</p>
<p>&bull; t ,
0.5
</p>
<p>Yj
o
</p>
<p>(a) Y = probability of heads - state of
knowledge is high.
</p>
<p>(b) Y = probability of heads - state of
knowledge is low.
</p>
<p>Figure 8.9: PMFs reflecting state of knowledge about coin 's probability of heads.
</p>
<p>value of the probability of heads. Before we toss the coin we have no idea what it
</p>
<p>is, and therefore it is reasonable to assume a PMF that is uniform, as , for example,
</p>
<p>the one shown in Figure 8.9b. Such a PMF is given by
</p>
<p>&pound; - 0 1 2 M -l 1or Yj - 'M' M " " ' ~ ' (8.37)
</p>
<p>for some large M (in Figure 8.9b M = 11). This is also called the prior PMF since
</p>
<p>it summarizes our state of knowledge before the experiment is performed. Now
</p>
<p>we begin to toss the coin and examine our state of knowledge as the number of
</p>
<p>tosses increases. Let N be the number of coin tosses and X denote the number of
</p>
<p>heads observed in the N tosses. We know that the PMF of the number of heads
</p>
<p>is binomially distributed. However, to specify the PMF completely, we require
</p>
<p>knowledge of the probability of heads. Since this is unknown, we can only specify
</p>
<p>the PMF of X conditionally or if Y = Yj is the probability of heads, then the
conditional PMF of the number of heads for X = i is
</p>
<p>i = 0,1, ... , N . (8.38)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.8. REAL-WORLD EXAMPLE - M ODELING HUMAN LEARNING 239
</p>
<p>Since we are actually interested in the probability of heads or the P MF of Y after
</p>
<p>observing the outcomes of N coin tosses, we need to determine the conditional PMF
</p>
<p>PYlx[Yjli]. T he latter is also called the posterior PMF, since it is to be determined
</p>
<p>after the experiment is peformed. The reader may wish to compare this terminology
</p>
<p>with that used in Chapter 4. The posterior P MF contains all the informat ion about
</p>
<p>the probability of heads that results from our prior knowledge, summarized by PY,
</p>
<p>and our "data" knowledge, summarized by PXIY ' T he posterior P MF is given by
</p>
<p>Bayes' rule (8.15) with X i = i as
</p>
<p>Using (8.37) and (8.38) we have
</p>
<p>Yj = O, l /M, ... , 1;i = 0,1 , .. . , N
</p>
<p>or finally,
</p>
<p>Yj = 0, 11M, . .. , 1; i = 0, 1, ... , N . (8.39)
. YJ(l - Yj)N-i
</p>
<p>PYlx[Yjlz] = " M i(l _ .)N- i
6j=O Yj Y]
</p>
<p>Note that the posterior PMF depends on the number of heads observed, which is
</p>
<p>i. To understand what this PMF is saying about our state of knowledge, assume
</p>
<p>that we toss the coin N = 10 times and observe i = 4 heads. The posterior PMF
is shown in Figure 8.10a . For N = 20, i = 11 and N = 40, i = 19, the posterior
</p>
<p>PMFs are shown in Figures 8.10b and 8.lOc, respectively. Note that as the number
</p>
<p>0.4 , ,
</p>
<p>1.50.5
</p>
<p>Yj
</p>
<p>1
</p>
<p>a
</p>
<p>6
</p>
<p>, ; ..
</p>
<p>0
. l
</p>
<p>0.4
</p>
<p>0.2
</p>
<p>.1 l
0.2
</p>
<p>0.5
</p>
<p>Yj
</p>
<p>0,2 .
</p>
<p>0.4 .
</p>
<p>..,..
'&pound;'!,a
</p>
<p>&gt;&lt;:;:u.6 .
."
</p>
<p>(a) N = 10, i = 4 (b) N = 20, i = 11 (c) N = 40, i = 19
</p>
<p>Figure 8.10: Posterior PMFs for coin toss ing analogy to human learning - coin
</p>
<p>appears to be fair . The yj's are possible probability values for a head.</p>
<p/>
</div>
<div class="page"><p/>
<p>240 CHAPTER 8. CONDITIONAL PROBABILITY MASS FUNCTIONS
</p>
<p>. ~
</p>
<p>;;,'J8 .
</p>
<p>&gt;&lt;:
:;;0.6 .
."
</p>
<p>0.4 ,. 0.4f ; " .&bull;. . . . ; ; " j 0.4 .. . .. . ;.. . . . .. . ; . ..
</p>
<p>1.5
</p>
<p>0.2 .
</p>
<p>0'---...........-'-'-....-.........-.+---'
1.5 -0.5
</p>
<p>0.2 . .
</p>
<p>0.5
</p>
<p>Yj
</p>
<p>(a) N = 10, i = 2 (b) N = 20, i = 5 (c) N = 40, i = 7
</p>
<p>Figure 8.11: Posterior PMFs for coin tossing analogy to human learning - coin
</p>
<p>appears to be weighted. The Yj'S are possible probability values for a head.
</p>
<p>of tosses increases the posterior PMF becomes narrower and centered about the
</p>
<p>value of 0.5. The Bayesian model has "learned" the value of p, with our confidence
</p>
<p>increasing as the number of trials increases. Note that for no trials (just set N = 0
and hence i = 0 in (8.39)) we have just the uniform prior PMF of Figure 8.9b.
From our experiments we could now conclude with some certainty that the coin
</p>
<p>is fair. However , if the outcomes were N = 10, i = 2, and N = 20, i = 5, and
N = 40, i = 7, then the posterior PMFs would appear as in Figure 8.11. We would
</p>
<p>then conclude that the coin is weighted and is biased against yielding a head, since
</p>
<p>the posterior PMF is concentrated about 0.2. See [Kay 1993] for futher descriptions
</p>
<p>of Bayesian approaches to estimation.
</p>
<p>References
</p>
<p>Kay, S., Fundamentals of Statistical Signal Processing; Estimation Theory, Vol. I,
</p>
<p>Prentice-Hall, Englewood Cliffs, NJ , 1993.
</p>
<p>Tennebaum, J.B. "Bayesian modeling of human learning", in Advances in Neural
</p>
<p>Information Processing Systems 11, MIT Press, Cambridge, MA, 1999.
</p>
<p>Problems
</p>
<p>8.1 (w) A fair coin is tossed. If it comes up heads, then X = 1 and if it comes
up tails, then X = O. Next, a point is selected at random from the area A
if X = 1 and from the area B if X = 0 as shown in Figure 8.12. Note that
</p>
<p>the area of the square is 4 and A and B both have areas of 3/2. If the point
</p>
<p>selected is in an upper quadrant, we set Y = 1 and if it is in a lower quadrant,</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 241
</p>
<p>we set Y = O. Find the conditional PMF PYlx[jli] for all values of i and j .
</p>
<p>Next, compute pry = 0].
</p>
<p>y
</p>
<p>1
</p>
<p>A
</p>
<p>1
r---+--+ X
</p>
<p>Figure 8.12: Areas for Problem 8.1.
</p>
<p>8 .2 C..:..) (w) A fair coin is tossed with the outcome mapped into X = 1 for a head
and X = 0 for a tail. If it comes up heads, then a fair die is tossed. The
</p>
<p>outcome of the die is denoted by Y and is set equal to the number of dots
</p>
<p>observed. If the coin comes up tails, then we set Y = O. F ind the conditional
PMF PYlx[jl i] for all values of i and j . Next, compute p ry = 1].
</p>
<p>8.3 (w) A fair coin is tossed 3 times in succession. All the outcomes (i.e., the
</p>
<p>3-tuples) are equally likely. The random vari ab les X and Yare defined as
</p>
<p>X = {O if outcome of first toss is a tail
1 if outcome of first toss is a head
</p>
<p>Y = number of heads observed for the three tosses
</p>
<p>Determine the conditional PMF PYlx[j li] for all i and j.
</p>
<p>8.4 (t ) Prove that L : ~ _ o o P Y l x [ Y j l x i ] = 1 for all Xi .
</p>
<p>8.5 (-.:..:.-) (w) Are the following functions valid conditional PMFs
</p>
<p>a . PYlx [jlxi]= (1- Xi)jxi j = 1,2 , . . . ;Xi = 1/4,1/2,3/4
</p>
<p>b . PYlx[j lxi] = (~) xl (1 - Xi)N- j j = 0,1 , .. . , N; Xi = -1/2,1/2
</p>
<p>c. PYIX[j!Xi] = ex{ j = 2,3, . . . ; Xi = 2 for e some constant?
</p>
<p>8.6 (-.:..:.- ) (f) If
</p>
<p>px ,y[i ,j] =
</p>
<p>i i = O,j = 0
l i = O,j = 1
l i = 1,j = 0
i i = 1,j = 1</p>
<p/>
</div>
<div class="page"><p/>
<p>242 CHAPTER 8. CONDITIONAL PROBABILITY MASS FUNCTIONS
</p>
<p>find PYIX and PXIY'
</p>
<p>8.7 (f) Verify the conditional PMF given in (8.10).
</p>
<p>8.8 C..:..) (f) For the sample space shown in Figure 8.1 determine PYIX and PXIY if
all the outcomes are equally likely. Explain your results.
</p>
<p>8.9 (w) Explain the need for the denominator term in (8.11) and (8.12).
</p>
<p>8.10 (w) IfPYIX and PY are known, can you find PX,y?
</p>
<p>8.11 c.:..:..) (w) A box contains three types of replacement light bulbs. There is an
equal proportion of each type. The types vary in their quality so that the
</p>
<p>probability that the light bulb fails at the jth use is given by
</p>
<p>PYlx[jll]
</p>
<p>PYlx[jI2]
</p>
<p>PYlx[jI3]
</p>
<p>(0.99)j-10.01
</p>
<p>(0.9)j- 10.1
</p>
<p>(0.8)j- 10.2
</p>
<p>for j = 1,2, .... Note that PYlx[jli] is the PMF of the bulb failing at the jth
</p>
<p>use if it is of type i. If a bulb is selected at random from the box, what is the
</p>
<p>probability that it will operate satisfactorily for at least 10 uses?
</p>
<p>8.12 (f) A joint PMF px,y[i,j] has the values shown in Table 8.2. Determine the
conditional PMF PYlx, Are the random variables independent?
</p>
<p>j=1 j=2 j=3
</p>
<p>i = 1 ...!. ...i ~10 10 10
</p>
<p>i=2 1 1 120 20 10
</p>
<p>i=3 3 1 110 20 20
</p>
<p>Table 8.2: Joint PMF for Problem 8.12.
</p>
<p>8.13 c.:..:..) (w) A random vector (X, Y) has a sample space shown in Figure 8.13
with the sample points depicted as solid circles . The four points are equally
</p>
<p>probable. Note that the points in Figure 8.13b are the corners of the square
</p>
<p>shown in Figure 8.13a after rotation by +450 &bull; For both cases compute PYIX
</p>
<p>and PY to determine if the random variables are independent.
</p>
<p>8.14 (t) Use the properties of conditional probability and the definition of the con-
ditional PMF to prove (8.23). Hint: Let A = Uj{s : yes) = Yj} and note that
the events {s : yes) = Yj} are mutually exclusive.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS
</p>
<p>y
</p>
<p>1/,,;2
</p>
<p>-~t----*-~x
</p>
<p>(a)
</p>
<p>y
</p>
<p>1"---
</p>
<p>- ~ . . . . . . - - .......--.. x
1
</p>
<p>(b)
</p>
<p>243
</p>
<p>Figure 8.13: Joint PMFs - each point is equally probable.
</p>
<p>8.15 (w) If X and Yare independent random variables, find the PMF of Z =
</p>
<p>IX - YI&middot; Assume that Sx = {a, I , ... } and Sy = {a, I , ... }. Hint: The answer
is
</p>
<p>[k] = { l:~opx[i]py[i]
pz l:~o (py[i]px[i + k] +px[i]py(i + k))
</p>
<p>As an intermediate step show that
</p>
<p>k=O
</p>
<p>k = 1,2, . . .
</p>
<p>. {PY[i] k = &deg;
PZlx[klz] = py[i + k] +py(i - k] k i= 0.
</p>
<p>8.16 (w) Two people agree to meet at a specified time. Person A will be late by
</p>
<p>i minutes with a probability px[i] = (1/2)i+l for i = 0,1, ... , while person B
</p>
<p>will be late by j minutes with a probability of py(j] = (1/2)i+ 1 for j = 0,1, ....
The persons arrive independently of each other. The first person to arrive will
</p>
<p>wait a maximum of 2 minutes for the second person to arrive. If the second
</p>
<p>person is more than 2 minutes late, the first person will leave. What is the
</p>
<p>probability that the two people will meet? Hint: Use the results of Problem
</p>
<p>8.15.
</p>
<p>8.17 C:..:...) (w) If X and Yare independent random variables, both of whose PMFs
take on values {a, 1, ... }, find the PMF of Z = min(X, Y).
</p>
<p>8.18 (w) If X and Y have the joint PMF
</p>
<p>i = 0,1 , ... .i = 0,1 , ...
</p>
<p>where &deg;&lt; PI &lt; 1, &deg;&lt; P2 &lt; 1, find pry &gt; X] using a conditioning argument.
In particular, make use of (8.23) and pry &gt; XIX = i] = pry &gt; ilX = i].</p>
<p/>
</div>
<div class="page"><p/>
<p>i = 0,1, ... jj = 0,1, ...
</p>
<p>244 CHAPTER 8. CONDITIONAL PROBABILITY MASS FUNCTIONS
</p>
<p>8.19 (f) If X and Y have the joint PMF given in Problem 8.6 , find Ey1x[Ylxi].
</p>
<p>8.20 (f) If X and Y have the joint PMF
</p>
<p>(1)i+1 ),Jpx,y[i,j] = 2 exp(-.-\) j!
find EYlx[Yli] for all i.
</p>
<p>8.21 C:..:..) (f) Find the conditional mean of Y given X if the joint PMF is uniformly
distributed over the points SX,Y = {(O,0), (1,0), (1, 1), (2,0), (2, 1), (2, 2)}.
</p>
<p>8.22 C:..:..) (f) For the joint PMF given in Problem 8.21 determine var(Ylxi) for all
Xi. Explain why your results appear to be reasonable.
</p>
<p>8.23 (t) Prove that var(Ylxi) = EYlx[y2Ixi] - E?lx[Ylxi] by using (8.31).
</p>
<p>8.24 (f) Find Ey[Y] for the joint PMF given in Problem 8.21. Do this by using
</p>
<p>the definition of the expected value and also by using (8.36).
</p>
<p>8.25 (t) Prove the extension of (8.36) which is
</p>
<p>Ey[g(Y)] = Ex [Ey1x[g(Y)IX]]
</p>
<p>where heX) = EYlx[g(Y)IX] is a function of the random variable X which
takes on values
</p>
<p>h(x i) = EYlx[g(Y)lxi] = Lg(Yj)PYlx[YjlxiJ,
j
</p>
<p>This says that Ey[g(Y)] can be computed using the formula
</p>
<p>8.26 (t) In this problem we prove that if M f'J Poisf X) and Y conditioned on M
is a binomial PMF with parameter P, then the unconditional PMF of Y is
</p>
<p>Pois(.-\p). This means that if
</p>
<p>and
</p>
<p>then
</p>
<p>.-\m
PM[m] = exp(-'-\)-,
</p>
<p>m .
</p>
<p>PYIM[jlm] = (7) pi(l - p)m-j
</p>
<p>. (.-\p)j
py[j] = exp(-.-\p)-.,-
</p>
<p>J.
</p>
<p>m = 0,1, ...
</p>
<p>j = O,l, .. . , m
</p>
<p>j = 0, 1, ....</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 245
</p>
<p>To prove this you will need to derive the characteristic function of Y and show
</p>
<p>that it corresponds to a Pois('xp) random variable. Proceed as follows, making
</p>
<p>use of the results of Problem 8.25
</p>
<p>&cent;y(w)
</p>
<p>and complete the derivation.
</p>
<p>Ey[exp(jwY)]
</p>
<p>EM [EyIM[exp(jwY)IM]
</p>
<p>EM [[pexp(jw) + (1 - p)]M]
</p>
<p>px,y[i,j] =
</p>
<p>8.27 (t) In Chapter 7 the optimal linear predictor of Y based on X = Xi was found.
The criterion of optimality was the minimum mean square error, where the
</p>
<p>mean square error was defined as Ex,y[(Y - (aX + b))2]. In this problem we
prove that the best predictor, now allowing for nonlinear predictors as well, is
</p>
<p>given by the conditional mean Ey1x[Ylxi]. To prove this we let the predictor
</p>
<p>be Y = g(X) and minimize
</p>
<p>Ex,y[(Y - g(X))2] = L L(Yj - g(xd)2pX,Y[Xi,Yj]
j
</p>
<p>~ ~ [~(Yi - 9(Xi))2PYIX[YiIXiJ] PX[XiJ .
But since PX[Xi] is nonnegative and we can choose a different value of g(xd
</p>
<p>for each Xi , we can equivalently minimize
</p>
<p>where we consider g(Xi) = C as a constant. Prove that this is minimized for
</p>
<p>g(xd = Ey1x[Ylxi]. Hint: You may wish to review Section 6.6.
</p>
<p>8.28 (..:..:,..) (f) For random variables X and Y with the joint PMF
</p>
<p>i (i,j) =(-1,0)
i (i,j) = (0, -1)
i (i ,j) = (0,1)
i (i,j) = (1,0)
</p>
<p>we wish to predict Y based on our knowledge of the outcome of X. Find the
</p>
<p>optimal predictor using the results of Problem 8.27. Also, find the optimal
</p>
<p>linear predictor for this problem (see Section 7.9) and compare your results.
</p>
<p>Draw a picture of the sample space using solid circles to indicate the sample
</p>
<p>points in a plane and then plot the prediction for each outcome of X = i for
</p>
<p>i = -1,0, 1. Explain your results.</p>
<p/>
</div>
<div class="page"><p/>
<p>246 CHAPTER 8. CONDITIONAL PROBABILITY MASS FUNCTIONS
</p>
<p>8.29 (c) Test out the MATLAB program given in Section 8.7 to generate realiza-
</p>
<p>tions of the vector random variable (X, Y) whose joint PMF is given in Figure
</p>
<p>8.8. Do so by estimating the joint PMF or px,y[i,j]. You may wish to review
</p>
<p>Section 7.11.
</p>
<p>8.30 t.:..:..) (w,c) For the joint PMF given in Figure 8.8 determine the conditional
mean EYIXUli] and then verify your results using a computer simulation. Note
</p>
<p>that you will have to separate the realizations (xm , Ym) into two sets, one in
</p>
<p>which X m = &deg;and one in which X m = 1, and then use the sample average of
each set as your estimator.
</p>
<p>8.31 (w,c) For the joint PMF given in Figure 8.8 determine Ey[Y] . Then, verify
(8.36) by using your results from Problem 8.30, and computing
</p>
<p>- - -Ey[Y] = EYlx[YIO]px[O] + Ey1x[Y11]px[1]
- -where EYlx[YIO] and Ey1x[Yll] are the values obtained in Problem 8.30. Also,
</p>
<p>the PMF of X ,which needs to be estimated, can be done so as described in
</p>
<p>Section 5.9.
</p>
<p>8.32 (w,c) For the posterior PMF given by (8.39) plot the PMF for i = N/2 ,
M = 11 and increasing N , say N = 10,30,50,70. What happens as N becomes
large? Explain your results. Hint: You will need a computer to evaluate and
</p>
<p>plot the posterior PMF.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 9
</p>
<p>Discrete N-Dimensional
</p>
<p>Random Variables
</p>
<p>9.1 Introduction
</p>
<p>In this chapter we extend the results of Chapters 5-8 to N-dimensional random vari-
</p>
<p>ables, which are represented as an N x 1 random vector. Hence, our discussions will
</p>
<p>apply to the 2 x 1 random vector previously studied. In fact , most of the concepts
</p>
<p>introduced earlier are trivially extended so that we do not dwell on the conceptu-
</p>
<p>alization. The only exception is the introduction of the covariance matrix, which
</p>
<p>we have not seen before. We will introduce more general notation in combination
</p>
<p>with vector/matrix representations to allow the convenient manipulation of N x 1
</p>
<p>random vectors. This representation allows many results to be easily derived and is
</p>
<p>useful for the more advanced theory of probability that the reader may encounter
</p>
<p>later. Also, it lends itself to straightforward computer implementations, particularly
</p>
<p>if one uses MATLAB, which is a vector-based programming language. Since many
</p>
<p>of the methods and subsequent properties rely on linear and matrix algebra, a brief
</p>
<p>summary of relevant concepts is given in Appendix C.
</p>
<p>9.2 Summary
</p>
<p>The N-dimensional joint PMF is given by (9.1) and satisfies the usual properties of
</p>
<p>(9.3) and (9.4). The joint PMF of any subset of the N random variables is obtained
</p>
<p>by summing the joint PMF over the undesired ones. If the joint PMF factors as
</p>
<p>in (9.7) , the random variables are independent and vice versa. The joint PMF
</p>
<p>of a transformed random vector is given by (9.9). In particular, if the transformed
</p>
<p>random variable is the sum of N independent random variables with the same PMF,
</p>
<p>then the PMF is most easily found from (9.14). The expected value of a random
</p>
<p>vector is defined by (9.15) and the expected value of a scalar function of a random</p>
<p/>
</div>
<div class="page"><p/>
<p>248 CHAPTER 9. DISCRETE N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>vector is found via (9.16). As usual, the expectation operator is linear with a special
</p>
<p>case given by (9.17). The variance of a sum of N random variables is given by (9.20)
</p>
<p>or (9.21). If the random variables are uncorrelated, then this variance is the sum of
</p>
<p>the variances as per (9.22). The covariance matrix of a random vector is defined by
</p>
<p>(9.25) . It has many important properties that are summarized in Properties 9.1-
</p>
<p>5. Particularly useful results are the covariance matrix of a linearly transformed
</p>
<p>random vector given by (9.27) and the ability to decorrelate the elements of a
</p>
<p>random vector using a linear transformation as explained in the proof of Property
</p>
<p>9.5. An example of this procedure is given in Example 9.4. The joint moments and
</p>
<p>characteristic function of an N-dimensional PMF are defined by (9.32) and (9.34) ,
</p>
<p>respectively. The joint moments are obtainable from the characteristic function by
</p>
<p>using (9.36). An important relationship is the factorization of the joint PMF into
</p>
<p>a product of conditional PMFs as given by (9.39). When the random variables
</p>
<p>exhibit the Markov property, then this factorization simplifies even further into the
</p>
<p>product of first-order conditional PMFs as given by (9.41). The estimates of the
</p>
<p>mean vector and the covariance matrix of a random vector are given by (9.44) and
</p>
<p>(9.46), respectively. Some MATLAB code for implementing these estimates is listed
</p>
<p>in Section 9.8. Finally, a real-world example of the use of transform coding to
</p>
<p>store/transmit image data is described in Section 9.9. It is based on decorrelation
</p>
<p>of random vectors and so makes direct use of the properties of the covariance matrix.
</p>
<p>9.3 Random Vectors and Probability Mass Functions
</p>
<p>Previously, we denoted a two-dimensional random vector by either of the equivalent
</p>
<p>notations (X, Y) or [X YV. Since we now wish to extend our results to an N x 1
random vector, we shall use (X1,X2 , ... ,XN) or X = [X1X2 ... XNV. Note that
a boldface character will always denote a vector or a matrix, in contrast to a scalar
</p>
<p>variable. Also, all vectors are assumed to be column vectors. A random vector
</p>
<p>is defined as a mapping from the original sample space S of the experiment to a
</p>
<p>numerical sample space, which we term SXl,X2 ,oo .,XN' The latter is normally referred
</p>
<p>to as R N , which is the N-dimensional Euclidean space. Hence, X takes on values
</p>
<p>in RN so that
</p>
<p>will have values
</p>
<p>X(s) =
[
</p>
<p>Xl(S) ]
X2 (s)
</p>
<p>XN(S)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3. RANDOM VECTORS AND PROBABILITY MASS FUNCTIONS 249
</p>
<p>where x is a point in the N-dimensional Euclidean space R N . A simple example is
</p>
<p>S = {all lottery tickets} with X(s) representing the number printed on the ticket.
Then , X 1(s) is the first digit of the number, X 2 (s) is the second digit of the number,
</p>
<p>... , and X N(S) is the Nth digit of the number.
</p>
<p>We are, as usual, interested in the probability that X takes on its possible values.
</p>
<p>This probability is P[X1 = Xl, X 2 = X2, . .. , X N = XN] and it is defined as the joint
</p>
<p>PMF. The joint PMF is therefore defined as
</p>
<p>PX1 ,X2,...,XN [Xl , X2, &middot; &middot; &middot;, XN] = P[X1 = Xl, X 2 = X2, &middot; &middot;&middot;, X N = XN] (9.1)
</p>
<p>or more succinctly using vector notation as
</p>
<p>px[x] = P[X = x]. (9.2)
</p>
<p>When x consists of integer values only, we will replace Xi by ki. Then, the joint
</p>
<p>PMF will be PX1,X2,...,XN[k1, k2, ... , kN] or more succintly as Px[k], where k =
[k1k2 ... kN]T. An example of an N-dimensional joint PMF, which is of consid-
</p>
<p>erable importance, is the multinomial PMF (see (4.19)). In our new notation the
</p>
<p>joint PMF is
</p>
<p>[k k k] - ( M ) kl k2 kNPX1 ,X2,...,XN 1, 2, &middot; &middot; &middot;, N - k k k P1 P2 .. ,PN
1, 2, .. &middot; , N
</p>
<p>where ki ~ 0 with 2 : ~ 1 ki = M , and 0 ::; Pi ::; 1 for all i with 2 : ~ 1 Pi = 1. That
this is a valid joint PMF follows from its adherence to the usual properties
</p>
<p>O::;P X1,X2,,,,,XN[k1,k2, , kN ] &lt; 1
</p>
<p>LL ... LPX1 ,X2"" ,XN[k1,k2, , kN] = 1.
k l k2 kN
</p>
<p>(9.3)
</p>
<p>(9.4)
</p>
<p>To prove (9.4) we need only use the multinomial expansion, which is (see Problem
</p>
<p>9.3)
</p>
<p>where 2 : ~ 1 ki = M.
The marginal PMFs are obtained from the joint PMF by summing over the other
</p>
<p>variables. For example, if PX
1
[Xl] is desired, then
</p>
<p>PX1 [X1] = L L
{X2:X2ESX 2} {X3:X3ESX3}
</p>
<p>L PX1 ,X2,,,,,XN[X1,X2, .. . , XN] (9.6)
{XN: XNESx N}
</p>
<p>and similarly for the other N - 1 marginals. This is because the right-hand side of
</p>
<p>(9.6) is</p>
<p/>
</div>
<div class="page"><p/>
<p>250 CHAPTER 9. DISCRETE N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>When the random vector is composed of more than two random variables, we can
</p>
<p>also obtain the joint PMF of any subset of the random variables. We do this by
</p>
<p>summing over the variables that we wish to eliminate. If, say, we wish to determine
</p>
<p>the joint PMF of Xl and X N , we have
</p>
<p>PXl ,XN[XI, XN] = 2:2: ... 2: PXl ,X2"",XN[XI , X2, ... , XN].
X2 X3 XN - l
</p>
<p>As in the case of N = 2 the marginal PMFs do not determine the joint PMF,
unless of course the random variables are independent. In the N-dimensional case
</p>
<p>the random variables are defined to be independent if the joint PMF factors or if
</p>
<p>Hence, if (9.7) holds, the random variables are independent, and if the random
</p>
<p>variables are independent (9.7) holds. Unlike the case of N = 2, it is possible that
the joint PMF may factor into two or more joint PMFs. Then, the subsets of random
</p>
<p>variables are said to be independent of each other. For example, if N = 4 and the
joint PMF factors as PXl ,X2,X3,X4[XI , X2 , X3 ,X4] = PXl ,X2[XI ,X2]PX3,X4[X3, X4], then
the random variables (Xl , X 2 ) ar e independent of the random variables (X3 , X 4 ) .
An example of the determination of a joint PMF follows.
</p>
<p>Example 9.1 - Joint PMF for independent Bernoulli trials
</p>
<p>Consider an experiment in which we toss a coin with a probability of heads P,
</p>
<p>N times in succession. We let X i = 1 if the ith outcome is a head and X i = 0
if it is a tail. Furthermore, assume that the trials are independent. As defined
</p>
<p>in Chapter 4, this means that the probability of the outcome on any trial is not
</p>
<p>affected by the outcomes of any of the other trials. Thus, the experiment is a
</p>
<p>sequence of independent Bernoulli trials. The sample space is N -dimensional and
</p>
<p>is given by SXl ,X2,...,XN = {(k l , k2, ... , kN) : ki = 0,1 ; i = 1,2, ... , N} , and since
pXi[ki] = pki(1 - p)l-ki , we have the joint PMF from (9.7)
</p>
<p>N
</p>
<p>PXl ,X2,...,XN[kl , k2, ... , kN] = IIpx;[ki]
i=l
</p>
<p>N
</p>
<p>IIpki(1 _ p)l-ki
</p>
<p>i = l
</p>
<p>p2: f:l ki(l _ p)N-2: f:l ki. (9.8)
</p>
<p>o
A joint cumulative distribution function (CDF) can be defined in the N-dimensional
</p>
<p>case as</p>
<p/>
</div>
<div class="page"><p/>
<p>9.4. TRANSFORMATIONS 251
</p>
<p>It has the usual properties of being between 0 and 1, being monotonically increasing
</p>
<p>as any of the variables increases, and being "right continuous" . Also ,
</p>
<p>FX1,X2,...,XN ( -00, -00, , -00) 0
</p>
<p>FX1,X2" " ,X N (+00, +00, , +00) 1.
</p>
<p>The marginal CDFs are easily found by letting the undesired variables be evaluated
</p>
<p>at +00. For example, to determine the marginal CDF for Xl , we have
</p>
<p>9.4 Transformations
</p>
<p>Since X is an N x 1 random vector, a transformation or mapping to a random vector
</p>
<p>Y can yield another N x 1 random vector or an M x 1 random vector with M &lt; N.
In the former case the formula for the joint PMF of Y is an extension of the usual
</p>
<p>one (see (7.12)). If the transformation is given as y = g(x), where g represents an
</p>
<p>N-dimensional function or more explicitly
</p>
<p>YI gl(XI , X2 , ,XN )
</p>
<p>Y2 g2(XI , X2 , , XN)
</p>
<p>then
</p>
<p>PY1 ,Y2,...,YN [YI, Y2, ... , YN ] = LL'" L PX1 ,X2,,,,,XN[XI ,X2 , ... , X N ] . (9.9)
{(Xl " " ,XN ):
</p>
<p>91 (Xl , ,XN )=Yl ,.. .,
</p>
<p>9N(Xl, ,XN )= YN }
</p>
<p>In the case where the transformation is one-to-one, there is only one solution for
</p>
<p>x in the equation y = g(x), which we denote symbolically by x = g -l(y). The
transformed joint PMF becomes from (9.9) py[y] = PX[g -l(y)], using vector no-
tation. A simple example of this is when the transformation is linear and so can
</p>
<p>be represented by y = Ax, where A is an N x N nonsingular matrix. Then, the
solution is x = A -ly and the transformed joint PMF becomes
</p>
<p>(9.10)
</p>
<p>The other case, in which Y has dimension less than N, can be solved using the
</p>
<p>technique of auxiliary random variables. We add enough random variables to make
</p>
<p>the dimension of the transformed random vector equal to N , find the joint PMF via</p>
<p/>
</div>
<div class="page"><p/>
<p>252 CHAPTER 9. DISCRETE N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>(9.9) , and finally sum the N-dimensional PMF over the auxiliary random variables.
</p>
<p>More specifically, ifY is M x 1 with M &lt; N , we define a new N x 1 random vector
</p>
<p>Z = [YI Y2 &middot; &middot; &middot; YM Z M+l = X M+l Z M+2 = X M+2'" Z N = x Nf
</p>
<p>so that the transformation becomes one-to-one, if possible. Once the joint PMF of
</p>
<p>Z is found, we can determine the joint PMF of Y as
</p>
<p>PYl,Y2"" ,YM[YI ,Y2,&middot;&middot; &middot; , YM] = L L . . . LPZl ,Z2,...,ZN[ZI, Z2, &middot; &middot; &middot; , ZN].
ZM+ l ZM+2 ZN
</p>
<p>The determination of the PMF of a transformed random vector is in general not an
</p>
<p>easy task. Even to determine the possible values of Y can be quite difficult. An
</p>
<p>example follows that illustrates the work involved.
</p>
<p>Example 9.2 - PMF for one-to-one transformation of N-dimensional
</p>
<p>random vector
</p>
<p>In Example 9.1 X has the joint PMF given by (9.8). We define a transformed
</p>
<p>random vector as
</p>
<p>Xl
</p>
<p>Xl +X2
</p>
<p>Xl +X2 +X3 &bull;
</p>
<p>This is a linear transformation that maps a 3 x 1 random vector X into another
</p>
<p>3 x 1 random vector Y . It can be represented by the 3 x 3 matrix
</p>
<p>Note that the transformed random variables are the sums of the outcomes of the first
</p>
<p>Bernoulli trial, the first and second Bernoulli trials, and finally the sum of the first
</p>
<p>three Bernoulli trials. As such the values of the transformed random variables must
</p>
<p>take on certain values. In particular, YI :::; Y2 :::; Y3 or the outcomes must increase
</p>
<p>as the index i increases. This is sometimes called a counting process and will be
</p>
<p>studied in more detail when we discuss random processes. Some typical realizations
</p>
<p>of the random vector Yare shown in Figure 9.1. To determine the sample space
</p>
<p>for Y we enumerate the possible values , making sure that the values in the vector
</p>
<p>increase or stay the same and that the increase is at most one unit from Yi to Yi+l.
</p>
<p>The sample space is composed of integer 3-tuples (LI, la, l3), which is given by
</p>
<p>SYl ,Y2,Y3 = {(O, 0, 0), (0,0,1 ), (0, 1, 1), (1, 1, 1), (0, 1,2)' (1, 1,2) , (1,2 ,2) , (1,2, 3)}.
(9.11)
</p>
<p>These are the values of y for which PYl,Y2,Y3 is nonzero and are seen to be integer-
</p>
<p>valued. Next, we need to solve for x according to (9.10). It is easily shown that the</p>
<p/>
</div>
<div class="page"><p/>
<p>9.4. TRANSFORMATIONS 253
</p>
<p>3 .. 3 .. 3 ....
</p>
<p>;:;.:'2 . . ;:;.:'2 . . ;:;.:'2 .. .. .. .... .......
</p>
<p>1 1 1
</p>
<p>0 0 0
0 2
</p>
<p>i
0 2
</p>
<p>i
0
</p>
<p>(a) (b) (c)
</p>
<p>Figure 9.1: Typical realizations for sum of outcomes of independent Bernoulli trials.
</p>
<p>linear transformation is one-to-one since A has an inverse (note that the determinant
</p>
<p>of A is nonzero since det(A) = 1, and so A has an inverse), which is
</p>
<p>This says that x = A -ly or Xl = YI, X 2 = Y2 - YI, X3 = Y3 - Y2. Thus, we can use
(9.10) and then (9.8) to find the joint PMF of Y , which becomes from (9.10)
</p>
<p>and since from (9.8)
</p>
<p>PX l ,X 2,X 3 [k l , ka, k 3] = pk1+k2+k3(1 _ p)3-(k
1+k2+k3)
</p>
<p>we have that
</p>
<p>(9.12)
</p>
<p>Note that the joint PMF is nonzero only over the sample space SYl ,Y2,Y3 given in
</p>
<p>(9.11).
</p>
<p>Always make sure PMF values sum to one.
</p>
<p>The result of the previous example looks strange in that the joint PMF of Y does
</p>
<p>not depend on hand [2. A simple check that should always be made when working
these types of problems is to verify that the PMF values sum to one. If not, then</p>
<p/>
</div>
<div class="page"><p/>
<p>254 CHAPTER 9. DISCRETE N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>there is an error in the calculation. If they do sum to one , then there could still
</p>
<p>be an error but it is not likely. For the previous example, we have from (9.11) 1
</p>
<p>outcome for which 13 = 0, 3 outcomes for which 13 = 1, 3 outcomes for which 13 = 2,
</p>
<p>and 1 outcome for which 13 = 3. If we sum the probabilities of these outcomes we
</p>
<p>have from (9.12)
</p>
<p>and hence we can assert with some confidence that the result is correct.
</p>
<p>A transformation that is not one-to-one but that frequently is of interest is the
</p>
<p>sum of N independent discrete random variables. It is given by
</p>
<p>(9.13)
</p>
<p>where the Xi 'S are independent random variables with integer values. For the case
</p>
<p>of N = 2 and integer-valued discrete random variables we saw in Section 7.6 that
py = PXl *PX2' where * denotes discrete convolution. This is most easily evaluated
using the characteristic functions and the inverse Fourier transform to yield
</p>
<p>1
~ ~
</p>
<p>py[k] = &lt;/JXl (W)&lt;/JX2(w) exp( -jwk)-.
_ ~ 2w
</p>
<p>For a sum of N independent random variables we have the similar result
</p>
<p>r N d
py[k] = J-~D &lt;/JXi(w)exp(-jwk)2~
</p>
<p>and if all the Xi'S have the same PMF and hence the same characteristic function,
</p>
<p>this becomes
</p>
<p>r dw
py[kJ = J-~ &lt;/J~(w)exp(-jwk)2w (9.14)
</p>
<p>where &lt;/Jx(w) is the common characteristic function. An example follows (see also
Problem 9.9).
</p>
<p>Example 9.3 - Binomial PMF derived as PMF of sum of independent
</p>
<p>Bernoulli random variables
</p>
<p>We had previously derived the binomial PMF by examining the number of successes
</p>
<p>in N independent Bernoulli trials (see Section 4.6.2). We can rederive this result by
</p>
<p>using (9.14) with Xi = 1 for a success and Xi = 0 for a failure and determining the</p>
<p/>
</div>
<div class="page"><p/>
<p>9.5. EXPECTED VALUES 255
</p>
<p>k = 0,1 , . . . , N .
</p>
<p>P MF of Y = 2:~1 Xi. The random variable Y will be the number of successes in
</p>
<p>N trials. The characteristic function of X is for a single Bernoulli trial
</p>
<p>&lt;px(w) = Ex [exp(jwX)]
</p>
<p>= exp(jw(l))p + exp(jw(O))(l - p)
= pexp(jw) + (1 - p).
</p>
<p>Now using (9.14) we have
</p>
<p>py[k] = i: [pexp(jw) + (1 - p)]N exp(-jwk) ~
j 1r N (N) . . dw-1r ~ i [pexp(jw)]t(1 - &raquo;":' exp( -jwk) 21f
</p>
<p>(use binomial theorem)
</p>
<p>;-., (N ) . N 'j1r dw~ ~ pt(l - p) - t exp[jw(i - k)]- .
i=O &bull; -1r 21f
</p>
<p>But the integral can be shown to be 0 if i i- k and 1 if i = k (see Problem 9.8) .
Using this result we have as the only term in the sum being nonzero the one for
</p>
<p>which i = k , and therefore
</p>
<p>py[k] = (~ ) pk(l _ &raquo;":'
</p>
<p>The sum of N independent Bernoulli random variables has the PMF bin(N,p) in
</p>
<p>accordance with our earlier results.
</p>
<p>9 .5 Expected Values
</p>
<p>The expected value of a random vector is defined as the vector of the expected values
</p>
<p>of the elements of the random vector. This is to say that we define
</p>
<p>[ [
</p>
<p>~ :.~ ]] = [ ~~: :. f~~l ].Ex[X] = EX1,X2,...,XN
X N EXN[XN]
</p>
<p>(9.15)
</p>
<p>We can view this definition as "passing" the expectation "through" the left bracket
</p>
<p>of the vector since E X1,X2"" ,XN[Xi] = Ex; [Xi ]'
</p>
<p>A particular exp ectation of interest is that of a scalar function of Xl , X 2, . . . , X N ,
</p>
<p>say g(X1 , X 2 , &bull; . . ,X N). Similar to previous results (see Section 7.7) this is deter-
</p>
<p>mined by using</p>
<p/>
</div>
<div class="page"><p/>
<p>256 CHAPTER 9. DISCRETE N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>E XI ,X 2,...,X N [g (X I , X 2 , " " X N )]
</p>
<p>= 2:2: ... 2:g(XI , X2, &middot;&middot; &middot;, XN )PXI ,X2, ,,, ,XN [Xl,X2, " " X N ]. (9.16)
Xl X2 XN
</p>
<p>As an example, if g(XI , X 2 , ... , XN) = 2: ~1 X i , then
</p>
<p>= 2: 2: ... 2:(XI+ X2 + ...+ XN )PXI,X2 ,,, ,,XN [XI,X2, ... ,XN]
Xl X2 XN
</p>
<p>2: 2: ...2: XIP X I,X2, ...,XN [Xl,X2, &middot; &middot; &middot;, X N ]
Xl X2 XN
</p>
<p>+2: 2: ... 2: X2P XI,X2 ,...,X N [Xl,X2, "" XN]
X l X2 XN
</p>
<p>+ ... +2: 2: ...2: XNPX I ,X 2,...,X N [X I , X2 , . " ,XN]
X l X2 XN
</p>
<p>By a slight modification we can also show that
</p>
<p>(9.17)
</p>
<p>which says that t he expectat ion is a linear operator . It is also possible to write
</p>
<p>(9.17) more succinctly by defining t he N x 1 vector a = [a I a2 ... aN jT to yield
</p>
<p>(9.18)
</p>
<p>We next determine the variance of a sum of random variables. Previously it was
</p>
<p>shown that
</p>
<p>var(XI + X 2 ) = var(Xd + var(X2 ) + 2cov(X I , X 2 ) . (9.19)
</p>
<p>Our goal is to extend this to v ar (2: ~1 Xd for any N. To do so we proceed as
follows.
</p>
<p>var (t,x,) = Ex [(t,x,-Ex [t,x,]) ']
</p>
<p>Ex [(t,(X'-EX.lX,])) '] (since Ex[X,1 ~ Ex,[X,])</p>
<p/>
</div>
<div class="page"><p/>
<p>9.5. EXPECTED VALUES 257
</p>
<p>Ex [(tu;)'J
Ex [ t ~ u ; U j ]
N N
</p>
<p>LLEx[UiUj].
i = l j=l
</p>
<p>But
</p>
<p>EX[(Xi - EX;[Xi])(Xj - EXj [Xj])]
</p>
<p>EXiXj[(Xi - EXi [Xi]) (Xj - EXj[Xj])]
</p>
<p>COV(Xi, Xj)
</p>
<p>so that we have as our final result
</p>
<p>(9.20)
</p>
<p>(9.21)
</p>
<p>Noting that since COV(Xi, Xd = var(Xd and cov(Xj, Xd = COV(Xi ,Xj) , we have
</p>
<p>for N = 2 our previous result (9.19). Also, we can write (9.20) in the alternative
</p>
<p>form
</p>
<p>var (t,x;) ~ t,var(X;) + t,~ cov(X;,Xj ) .
{(i ,j ):i#j}
</p>
<p>As an immediate and important consequence, we see that if all the random variables
</p>
<p>are uncorrelated so that COV(Xi, Xj) = 0 for i f:. i , then
</p>
<p>(9.22)
</p>
<p>which says that the variance of a sum of un correlated random variables is the sum
</p>
<p>of the variances.
</p>
<p>We wish to explore (9.20) further since it embodies some important concepts
</p>
<p>that we have not yet touched upon. For clarity let N = 2. Then (9.20) becomes
</p>
<p>2 2
</p>
<p>var(X1 + X 2 ) = L L COV(Xi,Xj) .
i = l j =l
</p>
<p>(9.23)</p>
<p/>
</div>
<div class="page"><p/>
<p>258 CHAPTER 9. DISCRETE N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>If we define a 2 x 2 matrix C X as
</p>
<p>Cx = [ var(Xt}
COV(X2, X t}
</p>
<p>then we can rewri te (9.23) as
</p>
<p>cov(XI, X 2) ]
var(X2)
</p>
<p>var(X l + X,) ~ [1 1] ex [ : ] (9.24)
</p>
<p>as is easily verified . The matrix Cx is called the covariance matrix. It is a matrix
</p>
<p>with the variances along the main diagonal and the covariances off the main diagonal.
</p>
<p>For N = 3 it is given by
</p>
<p>and in general it becomes
</p>
<p>cov(XI, X 2)
</p>
<p>var(X2)
cov(X3 , X 2 )
</p>
<p>[
</p>
<p>var (XI)
</p>
<p>cov(X:.2' X l)
Cx=
</p>
<p>COV(XN,Xt}
</p>
<p>cov(X I, X 2 )
</p>
<p>var (X2 )
cov (XI, XN ) ]
coV(X 2,X N)
</p>
<p>. .
</p>
<p>COV(XN ,XN)
</p>
<p>(9.25)
</p>
<p>The covariance matrix has many important properties, which are discussed next.
</p>
<p>Property 9.1 - Covariance matrix is symmetric, i.e., Ck = Cx-
Proof:
</p>
<p>(Why?)
</p>
<p>o
</p>
<p>Property 9.2 - Covariance matrix is positive semidefinite.
</p>
<p>Being positive semidefinite means that if a is the N x 1 column vector a =
</p>
<p>[al a2&middot;&middot;&middot; aN]T , then aTCxa ~ 0 for all a . Note that aTCxa is a scalar and is
</p>
<p>referred to as a quadratic fo rm (see Appendix C).
</p>
<p>Proof: Consider the case of N = 2 since the extension is immediate. Let U, =
</p>
<p>Xi - Ex ; [Xi ], which is zero mean , and therefore we have</p>
<p/>
</div>
<div class="page"><p/>
<p>9.5. EXPECTED VALUES 259
</p>
<p>var(alUl + a2U2) (since alXl + a2X2 = alUl + a2U2 + c for c a constant)
Ex[(alUl + a2U2)2] (EX[Ul] = EX[U2] = 0)
a~Ex[Ul] + a~Ex[Ui] + ala2Ex[UlU2] + a2alEx[U2Ul] (linearity of Ex)
a ~ v a r ( X t } + a~var(X2) + ala2cov(Xl, X 2) + a2alcov(X2, Xt}
</p>
<p>[al a2] [ var(Xl) coV(Xl,X2) ] [ al ]
cov(X2, Xt} var(X2) a2
</p>
<p>= aTCxa.
</p>
<p>Since var(alXl +a2X2) :2: 0 for all al and a2, it follows that Cx is positive semidef-
inite.
</p>
<p>o
Also, note that the covariance matrix of random variables that are not perfectly
</p>
<p>predictable by a linear predictor is positive definite. A positive definite covariance
</p>
<p>matrix is one for which aTCxa &gt; 0 for all a i- O. If, however, perfect prediction
is possible, as would be the case if for N = 2 we had alXl + a2X2 + c = 0, for c
a constant and for some al and a2, or equivalently if X2 = -(aI/a2)Xl - (c/a2),
then the covariance matrix is only positive semidefinite. This is because var(alXl +
a2X2) = aTCxa = 0 in this case.
</p>
<p>Finally, with the general result that (see Problem 9.14)
</p>
<p>(9.26)
</p>
<p>we have upon letting a = 1 = [11 ... IV be an N x 1 vector of ones that
</p>
<p>which is another way of writing (9.20) (the effect of premultiplying a matrix by IT
</p>
<p>and postmultiplying by 1 is to sum all the elements in the matrix).
</p>
<p>The fact that the covariance matrix is a symmetric positive semidefinite matrix
</p>
<p>is important in that it must exhibit all the properties of that type of matrix. For
</p>
<p>example, if a matrix is symmetric positive semidefinite, then it can be shown that
</p>
<p>its determinant is nonnegative. As a result, it follows that the correlation coefficient
</p>
<p>must have a magnitude less than or equal to one (see Problem 9.18). Some other
</p>
<p>properties of a covariance matrix follow.
</p>
<p>Property 9.3 - Covariance matrix for uncorrelated random variables is
</p>
<p>a diagonal matrix.
</p>
<p>Note that a diagonal matrix is one for which all the off-diagonal elements are zero.
</p>
<p>Proof: Let COV(Xi ,Xj) = 0 for i i- j in (9.25).
o</p>
<p/>
</div>
<div class="page"><p/>
<p>260 CHAPTER 9. DISCRETE N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>Before listing the next property a new definition is needed. Similar to the definition
</p>
<p>that the expected value of a random vector is the vector of expected values of the
</p>
<p>elements, we define the expectation of a random matrix as the matrix of expected
</p>
<p>values of its elements. As an example, if N = 2 the definition is
</p>
<p>E [gn(X)' g12(X)] _ [Ex[gn(X)] EX[g12(X)]]
X g21(X) g22(X) - Ex [g21 (X)] Ex [g22 (X)] .
</p>
<p>Property 9.4 - Covariance matrix of Y = AX, where A is an M x N
</p>
<p>matrix (with M :::; N), is easily determined.
</p>
<p>The covariance matrix of Y is
</p>
<p>(9.27)
</p>
<p>Proof:
</p>
<p>To prove this result without having to explicitly write out each element of the various
</p>
<p>matrices requires the use of matrix algebra. We therefore only sketch the proof and
</p>
<p>leave some details to the problems. The covariance matrix of Y can alternatively
</p>
<p>be defined by (see Problem 9.21)
</p>
<p>Cv = Ey [(Y - Ey[Y])(Y - Ey[Y]f] .
</p>
<p>Therefore,
</p>
<p>Cy Ex [(AX - Ex [AX]) (AX - Ex [AX])T]
</p>
<p>Ex [A(X - Ex [X])(A(X - EX[X]))T]
</p>
<p>= AEx [(X - Ex [X])(X - Ex[X]f] AT
</p>
<p>= ACXA T.
</p>
<p>(see Problem 9.22)
</p>
<p>(see Problem 9.23)
</p>
<p>o
This result subsumes many of our previous ones (try A = IT = [11 ... 1] and note
that Cy = var(Y) if M = 1, for example!).
</p>
<p>Property 9.5 - Covariance matrix can always be diagonalized.
</p>
<p>The importance of this property is that a diagonalized covariance matrix implies
</p>
<p>that the random variables are uncorrelated. Hence , by transforming a random
</p>
<p>vector of correlated random variable elements to one whose covariance matrix is
</p>
<p>diagonal, we can decorrelate the random variables. It is exceedingly fortunate that
</p>
<p>this transformation is a linear one and is easily found . In summary, if X has a
</p>
<p>covariance matrix C x , then we can find an N x N matrix A so that Y = AX has
the covariance matrix
</p>
<p>o</p>
<p/>
</div>
<div class="page"><p/>
<p>9.5. EXPECTED VALUES 261
</p>
<p>The matrix A is not unique (see Problem 7.35 for a particular method). One possible
</p>
<p>determination of A is contained within the proof given next.
</p>
<p>Proof:
</p>
<p>We only sketch the proof of this result since it relies heavily on linear and matrix
</p>
<p>algebra (see also Appendix C). More details are available in [Noble and Daniel
</p>
<p>1977]. Since Cx is a symmetric matrix, it has a set of N orthonormal eigenvectors
</p>
<p>with corresponding real eigenvalues. Since C X is also positive semidefinite, the
</p>
<p>eigenvalues are nonnegative. Hence, we can find N x 1 eigenvectors {VI, V2, ... , V N }
</p>
<p>so that
</p>
<p>i = 1,2, ... , N
</p>
<p>where v[Vj = 0 for i i- j (orthogonality) , v[Vi = 1 (normalized to unit length),
and Ai ~ O. We can arrange the N x 1 column vectors CXVi and also AiVi into
</p>
<p>N x N matrices so that
</p>
<p>(9.28)
</p>
<p>But it may be shown that for an N x N matrix A and N x 1 vectors bj , b 2, dj , d2,
</p>
<p>using N = 2 for simplicity (see Problem 9.24),
</p>
<p>[Ab i Ab2
</p>
<p>[cid l C2d2
</p>
<p>A [b i b 2 ]
</p>
<p>[d i d2 ] [~ ~].
</p>
<p>(9.29)
</p>
<p>(9.30)
</p>
<p>Using these relationships (9.28) becomes
</p>
<p>or
</p>
<p>C x [ VI
,
</p>
<p>V2 ...
</p>
<p>v
</p>
<p>V
</p>
<p>'V
</p>
<p>A
</p>
<p>~ ]
,
</p>
<p>CxY=YA.
</p>
<p>(The matrix Y is known as the modal matrix and is invertible.) Premultiplying
</p>
<p>both sides by y-I produces
</p>
<p>Next we use the property that the eigenvectors are orthonormal to assert that y-I =
</p>
<p>vr (a property of orthogonal matrices), and therefore
</p>
<p>yTCxY=A (9.31)</p>
<p/>
</div>
<div class="page"><p/>
<p>262 CHAPTER 9. DISCRETE N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>Now recall from Property 9.4 that if Y = AX, then C y = ACxAT. Thus, if we
let Y = AX = VTX , we will have
</p>
<p>(from Property 9.4)
</p>
<p>(from (9.31))
</p>
<p>and the covariance matrix of Y will be diagonal with it h diagonal element var (Yi ) =
Ai ~ O.
</p>
<p>o
This important result is used ext ensively in many disciplines. Later we will see that
</p>
<p>for some typ es of continuous random vectors, the use of this linear transformation
</p>
<p>will make the random variables not only un correlated but independent as well (see
</p>
<p>Example 12.14). An example follows.
</p>
<p>Example 9.4 - Decorrelation of random variables
</p>
<p>We consider a two-dimensional example whose joint PMF is given in Table 9.1. We
</p>
<p>X2 =-8 X2 = 0 X2 = 2 X2 = 6 p Xl[xd
</p>
<p>X l =-8 0 I 0 0 I4 4
</p>
<p>x l = 0 I 0 0 0 I4 4
</p>
<p>X l = 2 0 0 0 I I4 4
</p>
<p>X l = 6 0 0 I 0 I4 4
</p>
<p>PX2 [X 2 ]
I I I I
4 4 4 4
</p>
<p>Table 9.1: Joint P MF values.
</p>
<p>first det ermine the covariance matrix C X and then A so that Y = AX consists of
</p>
<p>uncorrelated random variables. From Table 9.1 we have that
</p>
<p>E Xl[Xd
</p>
<p>E Xl[Xf] =
</p>
<p>E XIX2[XIX2]
</p>
<p>and therefore we have tha t
</p>
<p>var(X I )
</p>
<p>cov(XI , X 2 ) =
</p>
<p>yielding a covariance matrix
</p>
<p>EX 2 [X2] = 0
</p>
<p>EX 2[Xi] = 26
</p>
<p>6
</p>
<p>[
26 6]
</p>
<p>Cx = 6 26 .</p>
<p/>
</div>
<div class="page"><p/>
<p>9.5. EXPECTED VALUES 263
</p>
<p>To find the eigenvectors we need to first find the eigenvalues and then solve (Cx -
</p>
<p>AI)v = 0 for each eigenvector v. To determine the eigenvalues we need to solve for
A in the equation det(Cx - AI) = O. This is
</p>
<p>( [
26 - A 6 ])
</p>
<p>det 6 26 _ A = 0
</p>
<p>or
</p>
<p>(26 - A)(26 - A) - 36 = 0
</p>
<p>and has solutions Al
eigenvectors yields
</p>
<p>20 and A2 = 32. Then, solving for the corresponding
</p>
<p>which yields after normalizing the eigenvector to have unit length
</p>
<p>Similarly,
</p>
<p>(ex - A2I)v2 ~ [~6 ~6] [: ] ~ [ ~ ]
which yields after normalizing the eigenvector to have unit length
</p>
<p>The modal matrix becomes
</p>
<p>and therefore
</p>
<p>A = V T = [~ - ~ ] .
V2 V2
</p>
<p>Hence, the transformed random vector Y = AX is explicitly</p>
<p/>
</div>
<div class="page"><p/>
<p>264 CHAPTER 9. DISCRETE N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>and Y1 and Y2 are uncorrelated random variables with
</p>
<p>Ey[Y] = Ey [AX] = AEx [X] = 0
</p>
<p>c - = ACxAT = VTCxV = A = [2
00
</p>
<p>3
02].
</p>
<p>It is interesting to note in this example, and in general, that A is a rotation matrix
</p>
<p>or
</p>
<p>A = [ C?s0 - sin 0 ]
sin e' cos 0
</p>
<p>where 0 = 1r/4. The effect of multiplying a 2 x 1 vector by this matrix is to rotate
the vector 45&deg; in the counterclockwise direction (see Problem 9.27). As seen in
</p>
<p>Figure 9.2 the values of X , indicated by the small circles and also given in Table
</p>
<p>9.1, become the values of Y , indicated by the large circles. One can easily verify
</p>
<p>the rotation.
</p>
<p>10 .---,---r-----r--,--,---,---r-----r----.-------,
. : I . .
</p>
<p>8 . . .. :. . . . .:... .. :. . . . 0..:: ......--....... -'-". :.... . / . . . . : . . . . ~ . . . .
</p>
<p>6
</p>
<p>4
</p>
<p>2
</p>
<p>~ 0
</p>
<p>-2
</p>
<p>-4
</p>
<p>-6
</p>
<p>-8 . . . .:.....:. . . . .:. . . .",.- A-- . "". :.... .:. ... .: .. .. , . . . .
</p>
<p>- 10 L..---'-_--'-----'-_--'-_"----'-_--'-----'-_--'------'
-1 0 -8 -6 -4 -2 0 2 4 6 8 10
</p>
<p>x
</p>
<p>Figure 9.2: Sample points for X (small circles) and Y (large circles). The dashed
</p>
<p>lines indicate a 45&deg; rotation.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.6. JOINT MOMENTS AND THE CHARACTERISTIC FUNCTION 265
</p>
<p>9.6 Joint Moments and the Characteristic Function
</p>
<p>The joint moments corresponding to an N-dimensional PMF are defined as
</p>
<p>E [Xh X I2 XI N ] - ~~ ~ 11 /2 IN [ ]X I,X2, . ..,XN I 2 ' " N - L..J L..J'" L..J X l X 2 .. . X N P XI ,X 2,.. .,X N X l , X2, &middot;&middot;&middot;, XN .
Xl X2 XN
</p>
<p>(9.32)
</p>
<p>As usual if the random vari ables are independent, the joint PMF factors and there-
</p>
<p>fore
</p>
<p>(9.33)
</p>
<p>The joint characteristic function is defined as
</p>
<p>&lt;PXI,X2, .. .,X N (W I , W2 , ... , W N ) = EXI,X2", .,XN [exp[j(wIXI + W2 X2 + ... + WNXN)]]
(9.34)
</p>
<p>and is evaluated as
</p>
<p>&lt;PXI ,X2" " ,XN (WI , W2 ,&middot;&middot;&middot; , W N)
</p>
<p>= L L '" Lexp[j(WIXI +W2X 2 + ... +WN XN)]PXI ,X 2,... ,X N [X I , X 2 , ... ,XN].
Xl X2 XN
</p>
<p>In particular, for independent random variables, we have (see Problem 9.28)
</p>
<p>Also, if X takes on integer values , the joint PMF can be found from the joint
</p>
<p>characterist ic function using the inverse Fourier transform or
</p>
<p>(9.35)
</p>
<p>All the properties of the 2-dimensional characteristic function extend to the general
</p>
<p>case. Not e that once &lt;PXI ,X2 ,.. .,XN(WI ,W2 , ... , WN ) is known, the characteristic func-
</p>
<p>tion for any subset of the X i 'S is found by setting Wi equal to zero for the ones not
</p>
<p>in the subset. For example, to find PXI ,X2[XI , X2 ], we let W3 = W4 = .. . = W N = 0 in
the joint characterist ic function to yield
</p>
<p>j
7r j 7r . dwl dw2
</p>
<p>P XI ,X 2[kl ,k2] = -7r _7r 1X I , X2, ,, ,,XN ( W ~ W 2' O, O, .. . , O), exp[- J (wl kl + W2k2)] 21f 21f'
t/lX I ,X 2 (WI ,W2)</p>
<p/>
</div>
<div class="page"><p/>
<p>266 CHAPTER 9. DISCRETE N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>As seen previously, the joint moments can be obtained from the characteristic func-
</p>
<p>tion. The general formula is
</p>
<p>EX 1,X 2,o,o,X N [X i
1 X~ 2 ... x ~ n
</p>
<p>9.7 Conditional Probability Mass Functions
</p>
<p>When we have an N-dimensional random vector, many different conditional PMFs
</p>
<p>can be defined. A straightforward extension of the conditional PMF PYIX encoun-
</p>
<p>tered in Chapter 8 is the conditional PMF of a single random variable conditioned
</p>
<p>on knowledge of the outcomes of all the other random variables. For example, it is
</p>
<p>of interest to study PXNlxl ,X2, ... ,XN - l ' whose definition is
</p>
<p>Then by rearranging (9.37) we have upon omitting the arguments
</p>
<p>(9.38)
</p>
<p>If we replace N by N - 1 in (9.37) , we have
</p>
<p>or
</p>
<p>Inserting this into (9.38) yields
</p>
<p>Continuing this process results in the general chain rule for joint PMFs (see also
</p>
<p>(4.10))
</p>
<p>(9.39)
</p>
<p>A particularly useful special case of this relationship occurs when the conditional
</p>
<p>PMFs satisfies
</p>
<p>for n = 3,4, ... ,N (9.40)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.7. CONDITIONAL PROBABILITY MASS FUNCTIONS 267
</p>
<p>or X n is independent of Xl .. " X n - 2 if Xn - l is known for all n ~ 3. If we view n
</p>
<p>as a time index, then this says that the probability of the current random variable
</p>
<p>X n is independent of the past outcomes once the most recent past outcome X n - l
is known. This is called the Markov property, which was described in Section 4.6.4.
</p>
<p>When the Markov property holds , we can rewrite (9.39) in the particularly simple
</p>
<p>form
</p>
<p>(9.41)
</p>
<p>which is a factorization of the N-dimensional joint PMF into a product of first-order
</p>
<p>conditional PMFs. It can be considered as the logical extension of the factorization
</p>
<p>of the N-dimensional joint PMF of independent random variables into the product
</p>
<p>of its marginals. As such it enjoys many useful properties, which are discussed
</p>
<p>in Chapter 22. A simple example of when (9.40) holds is for a "running" sum of
</p>
<p>independent random variables or X n = :L~=l Ui, where the Ui'S are independent.
</p>
<p>Then, we have
</p>
<p>= UI
</p>
<p>UI + U2 = X I + U2
UI + U2 + U3 = X 2 + U3
</p>
<p>For example, X 2 is known, the PMF of X3 = X2 +U3 depends only on U3 and not on
Xl. Also, it is seen from the definition of the random variables that U3 and U I = Xl
</p>
<p>are independent. Thus, once X2 is known, X3 (a function of U3 ) is independent of
</p>
<p>Xl (a function of UI ). As a result , PX3Ix2 ,Xl = PX31x2 and in general
</p>
<p>for n = 3,4, ... , N
</p>
<p>or (9.40) is satisfied. It is said that "the PMF of X n given the past samples depends
</p>
<p>only on the most recent past sample" . To illustrate this we consider a particular
</p>
<p>running sum of independent random variables known as a random walk.
</p>
<p>Example 9.5 - Random walk
</p>
<p>Let U, for i = 1,2, ... ,N be independent random variables with the same PMF
</p>
<p>[k] = { 1- p k = -1
PU P k = 1
</p>
<p>and define
n</p>
<p/>
</div>
<div class="page"><p/>
<p>268 CHAPTER 9. DISCRETE N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>(9.42)
</p>
<p>At each "t ime" n t he new ra ndom variable X n changes from the old random variable
</p>
<p>X n- l by &plusmn;1 since X n = X n- l + Un. The joint P MF is from (9.41)
N
</p>
<p>PX1,X2,...,XN = II PXnlXn-l
n=l
</p>
<p>where P Xl lx o is defined as Px1 &bull; But P XnlXn- l can be found by noting that X n =
X n- l + Un and therefore if X n- l = Xn- l we have that
</p>
<p>PUnlXn- l [xn - xn-I lxn- d
</p>
<p>PUn[xn - Xn- l ]
</p>
<p>pu[x n - Xn- l ]
</p>
<p>(step 1 - transform PMF)
</p>
<p>(step 2 - independence)
</p>
<p>(Un's have same PMF) .
</p>
<p>(9.43)
</p>
<p>Step 1 results from the transformed random vari able Y = X + c, where c is a con-
stant , having a PMF PY[Yi] = PX[Yi - c]. Step 2 results from Un being independent
of X n- l = E?:l Ui since all t he Ui'S are independent. Finally, we have from (9.42)
</p>
<p>N
</p>
<p>PX1,X2,...,XN[Xl, X2, &middot; &middot; &middot;, XN] = II PU[Xn - x n- d&middot;
n=l
</p>
<p>A realizati on of the random variables for P = 1/2 is shown in Figure 9.3. As justified
by the character of the outcomes in Figure 9.3b , t his random process is termed a
</p>
<p>random walk. We will say more about this later in Chap ter 16. Note that the
</p>
<p>. .
........&bull; .. . .. . ... . . . . .. ...
</p>
<p>5.---~-~-~-~- ~--r,
</p>
<p>4
</p>
<p>3
</p>
<p>2
</p>
<p>5 ..
</p>
<p>302520105
</p>
<p>-1 .
</p>
<p>-2
</p>
<p>-3 :
</p>
<p>-4
</p>
<p>- 5 ' - - - ~ - ~ - ~ - ~ - ~ - - - - ' - '
</p>
<p>o
</p>
<p>-2 : ' ; : .
</p>
<p>-3 .. . .
</p>
<p>-4 """ " ' ""
</p>
<p>-5'---~-~-~-~ -~----'-'
</p>
<p>o 5 10 15 20 25 30
n
</p>
<p>(a) Realization of Un's (b) Realizat ion of X n's
</p>
<p>Figure 9.3: Typical realization of a random walk .
</p>
<p>probability of the realization in Figure 9.3b is from (9.43)
</p>
<p>30 30 1 (1) 30
PX1 ,X2,...,x3o[l ,0, ... , - 2] = II PU[Xn - xn -d = II 2" = 2"
</p>
<p>n=l n=l</p>
<p/>
</div>
<div class="page"><p/>
<p>9.8. COMPUTER SIMULATION OF RANDOM VECTORS
</p>
<p>since pu[-l] = pu[l] = 1/2.
</p>
<p>9.8 Com puter Simulat ion of Random Vectors
</p>
<p>269
</p>
<p>To generate a realization of a random vector we can use the direct method described
</p>
<p>in Section 7.11 or the conditional approach of Section 8.7. The latter uses the general
</p>
<p>chain rule (see (9.39)) . We will not pursue this further as the extension to an N xl
</p>
<p>random vector is obvious. Instead we concentrate on two important descriptors of
</p>
<p>a random vector, those being the mean vector given by (9.15) and the covariance
</p>
<p>matrix given by (9.25) . We wish to see how to estimate these quantities. In practice,
</p>
<p>the N -dimensional PMF is usually quite difficult to estimate and so we settle for
</p>
<p>the estimation of the means and covariances. The mean vector is easily estimated
</p>
<p>by est imat ing each element by its sample mean as we have done in Section 6.8. Here
</p>
<p>we assume to have M realizations of the N x 1 random vector X , which we denote
</p>
<p>as {xj , X2 , . . . , X M }. The mean vector estimate becomes
</p>
<p>__ 1 M
</p>
<p>Ex[X] = M LXm
m=l
</p>
<p>(9.44)
</p>
<p>which is the same as estimating the ith component of Ex [X] by (11M) 2::;;=1 [Xm]i,
where [eli denotes the ith component of the vector e. To estimate the N x N
covariance matrix we first recall that the vectorImatrix definition is
</p>
<p>ex = Ex [(X - Ex[X]) (X - Ex[X])T] .
</p>
<p>This can also be shown to be equivalent to (see Problem 9.31)
</p>
<p>ex = Ex [XX T] - (Ex [X])(Ex [X]f . (9.45)
</p>
<p>We can now replace Ex [X] by the est imate of (9.44) . To est imate the N x N matrix
</p>
<p>we replace it by (11M) 2::;;=1 X mX ~ since it is easily shown that the (i ,j) element
of Ex [X X T ] is
</p>
<p>and</p>
<p/>
</div>
<div class="page"><p/>
<p>270 CHAPTER 9. DISCRETE N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>Thus we have that
</p>
<p>__ 1 M T (1 M ) (1 M )T
Cx= Ml;XmXm- Ml;Xm Ml;Xm
</p>
<p>which can also be written as
</p>
<p>M
</p>
<p>-- 1 ( --) ( __ )T
CX = M L X m -Ex[X] X m - Ex[X]
</p>
<p>m=l
</p>
<p>(9.46)
</p>
<p>--where Ex[X] is given by (9.44). The latter form of the covariance matrix estimate
is also more easily implemented. An example follows.
</p>
<p>Example 9.6 - Decorrelation of random variables - continued
</p>
<p>In Example 9.4 we showed that we could decorrelate the random variable compo-
</p>
<p>nents of a random vector by applying the appropriate linear transformation to the
</p>
<p>random vector. In particular, if the 2 x 1 random vector X whose joint PMF is
</p>
<p>given in Table 9.1 is transformed to a random vector Y, where
</p>
<p>Y = [ ~ - ~ ] X
V2 V2
</p>
<p>then the covariance matrix for X
</p>
<p>becomes the diagonal covariance matrix for Y
</p>
<p>[
20 0]
</p>
<p>Cy = 0 32 .
</p>
<p>To check this we generate realizations of X, as explained in Section 7.11 and then use
</p>
<p>the estimate of the covariance matrix given by (9.46). The results are for M = 1000
</p>
<p>realizations
</p>
<p>--Cy =
[
</p>
<p>25.9080
</p>
<p>6.1077
</p>
<p>[
19.7742
</p>
<p>0.0261
</p>
<p>6.1077 ]
25.8558
</p>
<p>0.0261 ]
31.9896
</p>
<p>and are near to the true covariance matrices. The entire MATLAB program is given
</p>
<p>next.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.8. COMPUTER SIMULATION OF RANDOM VECTORS
</p>
<p>% covexample.m
clear all % clears out all previous variables from workspace
</p>
<p>rand('state',O); % sets random number generator to initial value
</p>
<p>M=1000;
</p>
<p>for m=1:M % generate realizations of X (see Section 7.11)
</p>
<p>u=randC1, 1) ;
if u&lt;=O. 25
</p>
<p>x(1,m)=-8;x(2,m)=0;
</p>
<p>elseif u&gt;0.25&amp;u&lt;=0.5
x(1,m)=0;x(2,m)=-8;
</p>
<p>elseif u&gt;0.5&amp;u&lt;=0.75
</p>
<p>x(1,m)=2;x(2,m)=6;
</p>
<p>else
x(1,m)=6;x(2,m)=2;
</p>
<p>end
</p>
<p>end
</p>
<p>meanx=[O 0]'; % estimate mean vector of X
for m=l:M
</p>
<p>meanx=meanx+x( :,m)/M;
</p>
<p>end
</p>
<p>meanx
</p>
<p>CX=zeros(2,2);
</p>
<p>for m=1 :M % estimate covariance matrix of X
xbar C: ,m)=x(: ,m)-meanx;
CX=CX+xbar(:,m)*xbar( :,m)'/M;
</p>
<p>end
CX
</p>
<p>A=[1/sqrt(2) -1/sqrt(2);1/sqrt(2) 1/sqrt(2)];
</p>
<p>for m=1:M % transform random vector X
</p>
<p>y(: ,m)=A*x(: .m) ;
</p>
<p>end
</p>
<p>meany=[O 0]'; %estimate mean vector or Y
</p>
<p>for m=1:M
</p>
<p>meany=meany+y(:,m)/M;
</p>
<p>end
</p>
<p>meany
</p>
<p>CY=zeros(2,2);
</p>
<p>for m=1:M % estimate covariance matrix of Y
</p>
<p>ybar(: ,m)=y(: ,m)-meany;
</p>
<p>CY=CY+ybar(:,m)*ybar(:,m)'/M;
</p>
<p>end
</p>
<p>CY
</p>
<p>271</p>
<p/>
</div>
<div class="page"><p/>
<p>272 CHAPTER 9. DISCRETE N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>9.9 Real-World Example - Image Coding
</p>
<p>The methods for digital storage and transmission of images is an important consid-
</p>
<p>eration in the modern digital age. One of the st andard procedures used to convert
</p>
<p>an image to its digital representation is the JPEG encoding format [Sayood 1996].
</p>
<p>It makes the observation that many images contain portions that do not change
</p>
<p>significantly in content. Such would be the case for the image of a house in which
</p>
<p>the color and texture of the siding, whether it be aluminum siding or clapboards,
</p>
<p>is relatively constant as the image is scanned in the horizontal direction. To store
</p>
<p>and transmit all this redundant information is costly and time consuming. Hence,
</p>
<p>it is desirable to reduce the image to its basic set of information. Consider a gray
</p>
<p>scale image for simplicity. Each pixel, which is a dot of a given intensity level, is
</p>
<p>modeled as a random variable. For the house image example, note that for the
</p>
<p>siding pixels, the random variables are heavily correlated. For example, if X I and
</p>
<p>X2 denote neighboring pixels in the horizontal direction, then we would expect the
</p>
<p>correlation coefficient Px 1 ,x 2 = 1. If this is the case, then we know from Section
</p>
<p>7.9 that Xl = X 2, assuming zero mean random variables in our model. There is no
</p>
<p>economy in storing/transmitting the values Xl = Xl and X2 = X2 = Xl. We should
just store/transmit Xl = Xl and when it is necessary to reconstruct the image let
X2 = Xl = Xl. In this case , there is no image degradation in doing so. If, however,
!PX 1,X 21 &lt; 1, then there will be an error in the reconstructed X 2 . If the correlation
coefficient is close to &plusmn;1, this error will be small. Even if it is not, for many images
the errors introduced are perceptually unimportant. Human visual perception can
</p>
<p>tolerate gross errors before the image becomes unsatisfactory.
</p>
<p>To apply this idea to image coding we will consider a simple yet illustrative
</p>
<p>example. The amount of correlation between random variables is quantified by
</p>
<p>the covariances. In particular, for multiple random variables this information is
</p>
<p>embodied in the covariance matrix. For example, if N = 3 a covariance matrix of
</p>
<p>indicates that
</p>
<p>but
</p>
<p>[
4 0 0]
</p>
<p>ex = 0 4 3.8
o 3.8 4
</p>
<p>(9.47)
</p>
<p>3.8
PX 2,X 3 = J4. 4 = 0.95.
</p>
<p>Clearly, then (XI, X 2) or (XI, X 3) contain most of the information. For more com-
plicated covariance matrices these relationships are not so obvious. For example,
</p>
<p>if
</p>
<p>[
41 5]
</p>
<p>ex = 1 4 5
5 5 10
</p>
<p>(9.48)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.9. REAL-WORLD EXAMPLE - IMAGE CODING 273
</p>
<p>it is not obvious that X 3 = Xl + X 2 (assuming zero mean random variables). (This
is verified by showing that E[(X3 - (Xl + X 2))2] = 0 (see Problem 9.33)) .
</p>
<p>The technique of transform coding [Sayood 1996] used in the JPEG encoding
</p>
<p>scheme takes advantage of the correlation between random variables. The particular
</p>
<p>version we describe here can be shown to be an optimal approach [Kramer and
</p>
<p>Mathews 1956]. It is termed the Karhunen-Loeve transform and an approximate
</p>
<p>version is used in the JPEG encoding. Transform coding operates on a random
</p>
<p>vector X and proceeds as follows:
</p>
<p>1. Transform the random variables into uncorrelated ones via a linear transforma-
</p>
<p>tion Y = AX, where A is an invertible N x N matrix.
</p>
<p>2. Discard the random variables whose variance is small relative to the others by
</p>
<p>setting the corresponding elements of Y equal to zero. This yields a new N x 1
</p>
<p>random vector Y. T his vector would be stored or transmitted. (Of course,
the zero vecto r elements would not require encoding, thereby effecting data
</p>
<p>compression. Their locations, though, would need to be specified.)
</p>
<p>3. Transform back to X = A - IY to recover an approximation to the original ran-
dom variables (if the values Y were stored then this would occur upon retrieval
or if they were transmitted, this would occur at the receiver) .
</p>
<p>By decorrelating the random variables first it becomes obvious which components
</p>
<p>can be discarded without significantly affecting the reconstructed vector. To accom-
</p>
<p>plish the first step we have already determined that a suitable decorrelation matrix
</p>
<p>is v", where Y is the matrix of eigenvectors of Cx - T hus, we have that
</p>
<p>C y AC XAT
</p>
<p>y TCx Y
</p>
<p>[ var(YIl 0 0 ]A= 0 var(Y2) 0
0 0 var (Y3 )
</p>
<p>We now carry out the transform coding procedure for the covariance matrix of
</p>
<p>(9.48). This is done numerically using MAT LAB. The statement [V Lambda] =eig(CX)
</p>
<p>will produce the matrices Y and A, as
</p>
<p>[ 0.4082 -0.7071 0.5774 ]
v = 0.4082 -0.7071 0.5774
</p>
<p>0.8165 0 - 0.5774
</p>
<p>[15 0
</p>
<p>&bull;
A o 3
</p>
<p>o 0</p>
<p/>
</div>
<div class="page"><p/>
<p>274 CHAPTER 9. DISCRETE N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>Hence , var(Ya) = .Aa = 0 so that we discard it by setting -Va = 0 and therefore
</p>
<p>Y ~ [ ~ ] ~ l i i l l [ ~ ] &middot;
B '---v----'"
</p>
<p>y
</p>
<p>The reconstructed random vector becomes with A = yT
</p>
<p>X=A-Iy vv
</p>
<p>YBY
</p>
<p>= YByTX
</p>
<p>and since
</p>
<p>we have that
</p>
<p>[
</p>
<p>jXI - ~X2 + ~Xa ]
</p>
<p>X -~XI + jX2 + ~Xa
</p>
<p>~XI + ~X 2 + jXa
</p>
<p>= [ ~: ] (using Xa = Xl + X 2 , see Problem 9.33)
Xl +X2
</p>
<p>[;: ].
Here we see that the reconstructed vector X is identical to the original one. Gen-
erally, however, there will be an error. For the covariance matrix of (9.47) there
</p>
<p>will be an error since X2 and Xa are not perfectly correlated. For that covariance
matrix the eigenvector and eigenvalue matrices are
</p>
<p>y =
</p>
<p>A
</p>
<p>[
0.707~
0.7071
</p>
<p>[
</p>
<p>7.8 0
</p>
<p>o 4
o 0
</p>
<p>~ 0.707~ ]
o -0.7071
</p>
<p>O ~ 2 ]</p>
<p/>
</div>
<div class="page"><p/>
<p>9.9. REAL-WORLD EXAMPLE - IMAGE CODING 275
</p>
<p>and it is seen that the decorrelated random variables all have a nonzero variance
</p>
<p>(recall that var(Yi) = Ad . This indicates that no component of Y can be discarded
without causing an error upon reconstruction. By discarding Y3, which has the
</p>
<p>smallest variance, we will incur the least amount of error. Doing so produces the
</p>
<p>reconstructed random vector
</p>
<p>X VBVTX
</p>
<p>[ ~ ~ ~ ]xo 1 1
2 2
</p>
<p>which becomes
Xl
</p>
<p>X2 + X 3
X= 2
</p>
<p>X 2+X3
</p>
<p>2
</p>
<p>It is seen that the components X2 and X 3 are replaced by their averages. This is due
</p>
<p>to the nearly unity correlation coefficient coefficient (PX2,X3 = 0.95) between these
</p>
<p>components. As an example, we generate 20 realizations of X as shown in Figure
</p>
<p>9.4a , where the first realization is displayed in samples 1,2,3; the second realization
</p>
<p>in samples 4,5,6, etc. The reconstructed realizations are shown in Figure 9.4b.
</p>
<p>4 r--~ --r--~----. --~ -...,.,
</p>
<p>2 2 .
</p>
<p>&lt;&gt;&lt; 0
</p>
<p>- 2
</p>
<p>605020 30 40
Sample
</p>
<p>10
-4 ' - - - ~ - - ' - - - - - ' - - - - ' - - " ' - - - - ' - '
</p>
<p>o605020 30 40
Sample
</p>
<p>10
-4 ' - - - ~ - - ' - - - - - ' - - - - ' - - " ' - - - - ' - '
</p>
<p>o
</p>
<p>(a) Original (b) Reconstruction
</p>
<p>Figure 9.4: Realizations of original random vector { Xl , x2 , . .. , X20} and recon-
</p>
<p>structed random vectors {Xl, X2 , ... , X20}. The displayed samples shown are com-
</p>
<p>ponents of xj , followed by components of X2, etc.
</p>
<p>Finally, the error between the two is shown in Figure 9.5. Note that the total average</p>
<p/>
</div>
<div class="page"><p/>
<p>276 CHAPTER 9. DISCRETE N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>4.-------.-----.----r------.---.,---"
</p>
<p>3 . _..
</p>
<p>2
</p>
<p>&lt;&gt;&lt; 1 . '" .
</p>
<p>~ 0 .,~.,~.rl:'~.l.Tl.lT.lT.lr.lT-~'.lT.Tl-lT-,,-
. .
</p>
<p>-1 : : : :
. . . .. .
</p>
<p>-2 .
</p>
<p>-3 . .
</p>
<p>605020 30 40
Sample
</p>
<p>10
</p>
<p>-4 L--_----'-__---'-__...1....-_---''--_----'-__---'-'
</p>
<p>o
</p>
<p>Figure 9.5: Error between original random vector realizations and reconstructed
</p>
<p>ones shown in Figure 9.4.
</p>
<p>squared error or the total mean square error (MSE) is given by 2:~=1 Ex [(Xi - Xi )2]
which is
</p>
<p>Total mse
A 2 A 2 A 2
</p>
<p>E[(XI - Xl) + (X2 - X 2) + (X3 - X 3) ]
= E[(X2 - (X2+ X 3)/2)2] + E[(X3 - (X2+ X 3)/2)2]
= E[((X2 - X 3)/2)2] + E[((X3 - X 2)/2) 2]
</p>
<p>1 2
= 2E [(X 2 - X3) ]
</p>
<p>1
= 2[var(X2 ) + var(X3) - 2cov(X2 , X 3 ) ]
</p>
<p>1
2[4 + 4 - 2(3.8)] = 0.2.
</p>
<p>This total MSE is estimated by taking the sum of the squares of the values in Figure
</p>
<p>9.5 and dividing by 20, the number of vector realizations. Also, note what the total
</p>
<p>MSE would have been if PX 2,X a = 1.
</p>
<p>Finally, to appreciate the error in terms of human vision perception, we can
</p>
<p>convert the realizations of X and X into an image. This is shown in Figure 9.6.
The grayscale bar shown at the right can be used to convert the various shades of
</p>
<p>gray into numerical values. Also, note that as expected (see ex in (9.47)) Xl is
uncorrelated with X 2 and X 3, while X 2 and X 3 are heavily correlated in the upper
</p>
<p>image. In the lower image X 2 and X3 have been replaced by their average.</p>
<p/>
</div>
<div class="page"><p/>
<p>REFERENCES
</p>
<p>L original reconstruction t
</p>
<p>3
</p>
<p>2
</p>
<p>&middot;2
</p>
<p>-3
</p>
<p>277
</p>
<p>2 4 6 8 10 12 14 16 18 20
</p>
<p>Figure 9.6: Realizations of original random vector and reconstructed random vectors
</p>
<p>displayed as gray-scale images. The upper image is the original and the lower image
</p>
<p>is the reconstructed image.
</p>
<p>References
</p>
<p>Kramer , H.P. , Mathews, M.V. , "A Linear Coding for Transmitting a Set of Cor-
</p>
<p>related Signals," IRE Trans. on Information Theory, Vol. IT-2, pp. 41-46 ,
</p>
<p>1956.
</p>
<p>Noble , B., Daniel, J.W. , Applied Linear Algebra, Prentice-Hall, Englewood Cliffs,
</p>
<p>NJ , 1977.
</p>
<p>Sayood, K. Introduction to Data Compression, Morgan Kaufman, San Francisco,
</p>
<p>1996.
</p>
<p>Problems
</p>
<p>9.1 C:.:J (w) A retired person gets up in the morning and decides what to do that
day. He will go fishing with probability 0.3, or he will visit his daughter with
</p>
<p>probability 0.2, or else he will stay home and tend to his garden. If the decision</p>
<p/>
</div>
<div class="page"><p/>
<p>278 CHAPTER 9. DISCRETE N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>that he makes each day is independent of the decisions made on the other days,
</p>
<p>what is the probability that he will go fishing for 3 days, visit his daughter for
</p>
<p>2 days, and garden for 2 days of the week?
</p>
<p>9.2 (f,c) Compute the values of a multinomial PMF if N = 3, M = 4, PI = 0.2,
and P2 = 0.4 for all possible kl , k2, k3. Do the sum of the values equal one?
</p>
<p>Hint: You will need a computer to do this.
</p>
<p>9.3 (t) Prove the multinomial formula given by (9.5) for N = 3 by the following
method, Use the binomial formula to yield
</p>
<p>Then let b = a2 + a3 so that upon using the binomial formula again we have
</p>
<p>Finally, rearrange the sums and note that k3 = M - kl - k2 so that there is
</p>
<p>actually only a double sum in (9.5) for N = 3 due to this constraint.
</p>
<p>9.4 C:..:..J (f) Is the following function a valid PMF?
</p>
<p>kl = 0,1 " "
k2 = 0,1" ..
k3 = -1,0,1.
</p>
<p>9.5 (w) For the joint PMF
</p>
<p>kl = 0,1""
</p>
<p>k2 = 0,1" "
k3 = 0, 1,."
</p>
<p>where &deg;&lt; a &lt; 1, &deg;&lt; b &lt; 1, and &deg;&lt; c &lt; 1, find the marginal PMFs P XllPX 2
and PXs'
</p>
<p>9.6 C.:..:.-) (w) For the joint PMF given below are there any subsets of the random
variables that are independent of each other?
</p>
<p>kl = O,I, .. " M
k2 = M - kl
k3 = 0,1 , .. ,
</p>
<p>where &deg;&lt; PI &lt; 1, P2 = 1 - PI, and &deg;&lt; P3 &lt; 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS
</p>
<p>9.7 (f) A random vector X with the joint PMF
</p>
<p>is transformed according to Y = AX where
</p>
<p>Find the joint PMF of Y.
</p>
<p>9.8 (t) Prove that
</p>
<p>kl = 0,1 , .
k2 = 0,1 , .
k3 = 0,1 , .
</p>
<p>279
</p>
<p>I' . dw {O k:f: 0
J-1r exp(Jwk) 271" = 1 k = O.
</p>
<p>Hint: Expand exp(jwk) into its real and imaginary parts and note that J(g(w)+
</p>
<p>jh(w))dw = Jg(w)dw + j Jh(w)dw.
</p>
<p>9.9 (t) Prove that the sum of N independent Poisson random variables with X i '"
POiS(Ai) for i = 1,2, ... , N is again Poisson distributed but with parameter
A= L~l x, Hint: See Section 9.4.
</p>
<p>9.10 t.:.,:.,) (w) The components of a random vector X = [Xl X 2 &bull;&bull;&bull; X NV all have
the same mean Ex[X] and the same variance var(X). The "sample mean"
random variable
</p>
<p>1 N
X=- LXi
</p>
<p>N .
z=l
</p>
<p>is formed. If the Xi'S are independent, find the mean and variance of X. What
happens to the variance as N -+ oo? Does this tell you anything about the
</p>
<p>PMF of X as N -+ oo?
</p>
<p>9.11 (w) Repeat Problem 9.10 if we know that each X i '" Ber(p). How can this
</p>
<p>result be used to motivate the relative frequency interpretation of probability?
</p>
<p>9.12 (f) If the covariance matrix of a 3 x 1 random vector X is</p>
<p/>
</div>
<div class="page"><p/>
<p>280 CHAPTER 9. DISCRETE N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>9.13 C..:..) (w) A 2 x 1 random vector is given by
</p>
<p>where var(U) = 1. Find the covariance matrix for X. Next find the correlation
</p>
<p>coefficient PX 1,X 2' Finally, compute the determinant of the covariance matrix.
</p>
<p>Is the covariance matrix positive definite? Hint: A positive definite matrix
</p>
<p>must have a positive determinant.
</p>
<p>9.14 (t) Prove (9.26) by noting that
</p>
<p>N N
</p>
<p>aTCxa = L L aiaj cov(Xi , Xj).
i=l j=l
</p>
<p>9.15 (f) For the covariance matrix given in Problem 9.12, find var(XI + X 2 + X 3 ) .
</p>
<p>9.16 (t) Is it ever possible that var(XI + X 2 ) = var(XI ) without X 2 being a con-
stant?
</p>
<p>9.17 (..:..:..) (w) Which of the following matrices are not valid covariance matrices
and why?
</p>
<p>a. [~ ~] [-1 0]b. 0 -1 c. [~ ~] d. [~ ~]
9.18 (f) A positive semidefinite matrix A must have det(A) ~ O. Since a covari-
</p>
<p>ance matrix must be positive semidefinite, use this property to prove that the
</p>
<p>correlation coefficient satisfies Ipx 1 ,x 21 ::; 1. Hint: Consider a 2 x 2 covariance
matrix.
</p>
<p>9.19 (f) If a random vector X is transformed according to
</p>
<p>YI Xl
</p>
<p>Y2 = XI+X2
</p>
<p>and the mean of X is
</p>
<p>[ 4
</p>
<p>3 ]Ex [X] =
</p>
<p>find the mean of Y = [YI Y2V.
</p>
<p>9.20 (..:..:..) (f) If the random vector X given in Problem 9.19 has a covariance matrix
</p>
<p>ex = [~ ~]
find the covariance matrix for Y = [YI Y2V.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS
</p>
<p>9.21 (t) For N = 2 show that the covar iance matrix may be defined as
</p>
<p>Cx = Ex [(X - Ex [X])(X - Ex[Xlf ] .
</p>
<p>281
</p>
<p>Hint: Recall that the expected value of a matrix is the matrix of the expected
</p>
<p>values of its elements.
</p>
<p>9.22 (t) In this problem you are asked to prove that ifY = AX, where both X and
Y are N x 1 random vecto rs and A is an N x N matrix, then Ey [Y] = AEx [X].
</p>
<p>If we let [ALj be the (i ,j) element of A , then you will need to prove that
</p>
<p>N
</p>
<p>[EY[Y]]i = L[A]ij[Ex[X]Ji.
j=l
</p>
<p>This is because if b = Ax, then bi = L:f=l aijXj, for i = 1,2, ... , N where bi
is the ith element of band aij is the (i ,j) element of A.
</p>
<p>9.23 (t) In this problem we prove that
</p>
<p>Ex[AG(X)AT ] = AEx[G(X)]AT
</p>
<p>where A is an N x N matrix and G(X) is an N x N matrix whose elements
</p>
<p>are all fun ctions of X . To do so we note tha t if A , B , C , D ar e all N x N
</p>
<p>matrices then D = ABC is an N x N matrix with (i , l ) element
</p>
<p>N
</p>
<p>[Dl il = L [ABlik[C] kl
k=l
</p>
<p>f;; (t, [A]ij [B]i ' ) [Cl.,
N N
</p>
<p>L L[Alij[Bljk[C]kl.
k=l j=l
</p>
<p>Using this result and replacing A by itself, B by G(X) , and C by AT will
</p>
<p>allow the desired result to be proven.
</p>
<p>9.24 (f) Prove (9.29) and (9.30) for the case of N = 2 by letting
</p>
<p>A =</p>
<p/>
</div>
<div class="page"><p/>
<p>282 CHAPTER 9. DISCRETE N -DIMENSIONAL RANDOM VARIABLES
</p>
<p>and multiplying out all the matrices and vectors. Then, verify that the re-
</p>
<p>lationships are true by showing that the elements of the resultant N x N
</p>
<p>matrices are identical.
</p>
<p>9. 25 (c) Using MATLAB, find the eigenvectors and corresponding eigenvalues for
the covariance matrix .
</p>
<p>C X = [266 2
66]
</p>
<p>To do so use the statement [V Lambda] =eig( CX) .
</p>
<p>9 .26 C:..:..-) (f,c) Find a linear transformation to decorrelate the random vector X =
[X l x 2 ]T that has the covariance matrix
</p>
<p>[10 6]C x = 6 20 .
What are the variances of the decorrelated random variables?
</p>
<p>9. 27 (t) Prove that an orthogonal matrix, i.e., one that has the property U T =
U - I , rotat es a vector x to a new vector y . Do this by letting y = Ux and
</p>
<p>showing that the length of y is the same as the length of x. The length of a
</p>
<p>vector is defined to be Ilxll = v'xTx = v x ~ + x ~ + ... + x ~ .
</p>
<p>9.28 (t) Prove that if the random variables Xl , X 2 , . .&bull; , X N are independent, then
the joint characteristic function factors as
</p>
<p>Alt ernatively, if the joint characteristic func tion factors, what does this say
</p>
<p>about the random variables and why?
</p>
<p>9.29 (f) For the random walk described in Example 9.5 find the mean and the
</p>
<p>var iance of X n as a function of n if p = 3/4. What do they indicate about the
probable outcomes of X I,X2 , . . . ,XN?
</p>
<p>9.30 (c) For the random walk of Problem 9.29 simulate several realizations of the
</p>
<p>random vector X = [X l X 2 &bull;&bull;&bull; XN]T and plot these as X n versus n for n =
</p>
<p>1,2, ... ,N = 50. Does the appearance of the outcomes corroborate your
</p>
<p>results in Problem 9.29? Also, compare your results to those shown in Figure
</p>
<p>9.3b .
</p>
<p>9 .31 (t) Prove the relationship given by (9.45) as follows. Consider the (i, j) ele-
</p>
<p>ment of C x , which is cov(Xi, Xj ) = E X;,x j[XiXj] - Ex; [Xi]Ex j [Xj]. Then,
</p>
<p>show that the lat ter is just the (i ,j) element of the right-hand side of (9.45).
</p>
<p>Recall the definition of the expected value of a matrix/vector as the ma-
</p>
<p>trix/vector of expected values.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 283
</p>
<p>9.32 (c) A random vector is defined as X = [Xl X 2. . . XNV, where each compo-
</p>
<p>nent is X i ......, Ber(1/2) and all the random variables are independent. Since
</p>
<p>the random variables are independent, the covariance matrix should be di-
</p>
<p>agonal. Using MATLAB, generate realizations of X for N = 10 by using
</p>
<p>x=floor(rand(10,1)+O.5) to generate a single vector realization. Next gen-
</p>
<p>erate multiple random vector realizations and use them to estimate the covari-
</p>
<p>ance matrix. Presumably the random numbers that MATLAB produces are
</p>
<p>"pseudo-independent" and hence "pseudo-uncorrelated". Does this appear to
</p>
<p>be the case? Hint: Use the MATLAB command mesh(CXest) to plot the
</p>
<p>estimated covariance matrix CXest.
</p>
<p>9.33 (w) Prove that if Xl, X 2 , X 3 are zero mean random variables, then E[(X3 -
(Xl + X2))2] = 0 for the covariance matrix given by (9.48).
</p>
<p>9.34 (t) In this problem we explain how to generate a computer realization of a
random vector with a given covariance matrix. This procedure was used to
</p>
<p>produce the realizations shown in Figure 9.4a. For simplicity the desired N xl
</p>
<p>random vector X is assumed to have a zero mean vector. The procedure is
</p>
<p>to first generate an N x 1 random vector U whose elements are zero mean,
</p>
<p>uncorrelated random variables with unit variances so that its covariance matrix
</p>
<p>is I. Then transform U according to X = BU, where B is an appropriate
</p>
<p>N x N matrix. The matrix B is obtained from the N x N matrix .JA whose
elements are obtained from the eigenvalue matrix A of C x by taking the
</p>
<p>square root of the elements of A, and V , where V is the eigenvector matrix of
</p>
<p>ex , to form B = v.JA. Prove that the covariance matrix of BU will be C x .
</p>
<p>9.35 t:..:..-) (f) Using the results of Problem 9.34 find a matrix transformation B of
U = [UI U2V, where Cu = I , so that X = BU has the covariance matrix
</p>
<p>9.36 (..:...:,,) (c) Generate 30 realizations of a 2 x 1 random vector X that has a zero
</p>
<p>mean vector and the covariance matrix given in Problem 9.35. To do so use
</p>
<p>the results from Problem 9.35. For the random vector U assume that UI and
</p>
<p>U2 are uncorrelated and have the same PMF
</p>
<p>{
</p>
<p>~ k = -1
pu[k] = ~ k = 1.
</p>
<p>Note that the mean of U is zero and the covariance matrix of U is I. Next
</p>
<p>estimate the covariance matrix ex using your realizations and compare it to
the true covariance matrix.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 10
</p>
<p>Continuous Random Variables
</p>
<p>10.1 Introduction
</p>
<p>In Chapters 5-9 we discussed discrete random variables and the methods employed
</p>
<p>to describe them probabilistically. The principal assumption necessary in order to
</p>
<p>do so is that the sample space, which is the set of all possible outcomes, is finite or
</p>
<p>at most countably infinite. It followed then that a probability mass function (PMF)
</p>
<p>could be defined as the probability of each sample point and used to calculate the
</p>
<p>probability of all possible events (which are subsets of the sample space). Most
</p>
<p>physical measurements, however, do not produce a discrete set of values but rather
</p>
<p>a continuum of values such as the rainfall measurement data previously shown in
</p>
<p>Figures 1.1 and 1.2. Another example is the maximum temperature measured during
</p>
<p>the day, which might be anywhere between 20&deg;F and 60&deg;F. The number of possible
</p>
<p>temperatures in the interval [20,60] is infinite and uncountable. Therefore, we cannot
</p>
<p>assign a valid PMF to the temperature random variable. Of course, we could always
</p>
<p>choose to "round off" the measurement to the nearest degree so that the possible
</p>
<p>outcomes would then become {20, 21, ... , 60}. Then, many valid PMFs could be
</p>
<p>assigned. But this approach compromises the measurement precision and so is to
</p>
<p>be avoided if possible. What we are ultimately interested in is the probability of
</p>
<p>any interval, such as the probability of the temperature being in the interval [20,25]
</p>
<p>or [55,60] or the union of intervals [20,25] U [55,60]. To do so we must extend our
</p>
<p>previous approaches to be able to handle this new case. And if we later decide that
</p>
<p>less precision is warranted, such that the rounding of 20.6&deg; to 21&deg; is acceptable, we
</p>
<p>will still be able to determine the probability of observing 21&deg;. To do so we can
</p>
<p>regard the rounded temperature of 21&deg; as having arisen from all temperatures in
</p>
<p>the interval A = [20.5,21.5). Then, P[rounded temperature = 21] = P[A], so that
we have lost nothing by considering a continuum of outcomes (see Problem 10.2).
</p>
<p>Chapters 10-14 discuss continuous random variables in a manner similar to
</p>
<p>Chapters 5-9 for discrete random variables. Since many of the concepts are the
</p>
<p>same, we will not belabor the discussion but will concentrate our efforts on the al-</p>
<p/>
</div>
<div class="page"><p/>
<p>286 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>gebraic manipulations required to analyze continuous random variables. It may be
</p>
<p>of interest to note that discrete and continuous random variables can be subsumed
</p>
<p>under the topic of a general random variable. There exists the mathematical ma-
</p>
<p>chinery to analyze both types of random variables simultaneously. This theory is
</p>
<p>called measure theory [Capinski, Kopp 2004]. It requires an advanced mathematical
</p>
<p>background and does not easily lend itself to intuitive interpretations. An alterna-
</p>
<p>tive means of describing the general random variable that appeals more to engineers
</p>
<p>and scientists makes use of the Dirac delta function. This approach is discussed
</p>
<p>later in this chapter under the topic of mixed random variables .
</p>
<p>In the course of our discussions we will revisit some of the concepts alluded to in
</p>
<p>Chapters 1 and 2. With the appropriate mathematical tools we will now be able to
</p>
<p>define these concepts. Hence , the reader may wish to review the relevant sections
</p>
<p>in those chapters.
</p>
<p>10.2 Summary
</p>
<p>The definition of a continuous random variable is given in Section 10.3 and illus-
</p>
<p>trated in Figure 10.1. The probabilistic description of a continuous random variable
</p>
<p>is the probability density function (PDF) px(x) with its interpretation as the prob-
</p>
<p>ability per unit length. As such the probability of an interval is given by the area
</p>
<p>under the PDF (10.4). The properties of a PDF are that it is nonnegative and
</p>
<p>integrates to one, as summarized by Properties 10.1 and 10.2 in Section 10.4. Some
</p>
<p>important PDFs are given in Section 10.5, such as the uniform (10.6), the exponen-
</p>
<p>tial (10.5), the Gaussian or normal (10.7), the Laplacian (10.8) , the Cauchy (10.9),
</p>
<p>the Gamma (10.10), and the Rayleigh (10.14). Special cases of the Gamma PDF
</p>
<p>are the exponential, the chi-squared (10.12), and the Erlang (10.13). The cumu-
</p>
<p>lative distribution function (CDF) for a continuous random variable is defined the
</p>
<p>same as for the discrete random variable and is given by (10.16). The corresponding
</p>
<p>CDFs for the PDFs of Section 10.5 are given in Section 10.6. In particular, the
</p>
<p>CDF for the standard normal is denoted by &lt;p(x) and is related to the Q function
</p>
<p>by (10.17). The latter function cannot be evaluated in closed form but may be
</p>
<p>found numerically using the MATLAB subprogram Q.m listed in Appendix lOB. An
</p>
<p>approximation to the Q function is given by (10.23). The CDF is useful in that
</p>
<p>probabilities of intervals are easily found via (10.25) once the CDF is known. The
</p>
<p>transformation of a continuous random variable by a one-to-one function produces
</p>
<p>the PDF of (10.30). If the transformation is many-to-one, then (10.33) can be used
</p>
<p>to determine the PDF of the transformed random variable. Mixed random variables,
</p>
<p>ones that exhibit nonzero probabilities for some points but are continuous otherwise,
</p>
<p>are described in Section 10.8. They can be described by a PDF if we allow the use
</p>
<p>of the Dirac delta function or impulse. For a general mixed random variable the
</p>
<p>PDF is given by (10.36). To generate realizations of a continuous random variable
</p>
<p>on a digital computer one can use a transformation of a uniform random variable</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3. DEFINITION OF A CONTINUOUS RANDOM VARIABLE 287
</p>
<p>as summarized in Theorem 10.9.1. Examples are given in Section 10.9. Estimation
</p>
<p>of the PDF and CDF can be accomplished by using (10.38) and (10.39). Finally, an
</p>
<p>example of the application of the theory to the problem of speech clipping is given
</p>
<p>in Section 10.10.
</p>
<p>10.3 Definition of a Continuous Random Variable
</p>
<p>A continuous random variable X is defined as a mapping from the experimental
</p>
<p>sample space S to a numerical (or measurement) sample space Sx , which is a subset
of the real line R l . In contrast to the sample space of a discrete random variable,
</p>
<p>Sx consists of an infinite and uncountable number of outcomes. As an example,
consider an experiment in which a dart is thrown at the circular dartboard shown in
</p>
<p>Figure 10.1. The outcome of the dart-throwing experiment is a point Sl in the circle
</p>
<p>S
</p>
<p>X(sd
</p>
<p>x
</p>
<p>1
</p>
<p>Sx = [0,1]
</p>
<p>Figure 10.1: Mapping of the outcome of a thrown dart to the real line (example of
</p>
<p>continuous random variable).
</p>
<p>of radius one. The distance from the bullseye (center of the dartboard) is measured
</p>
<p>and that value is assigned to the random variable as X(sd = Xl. Clearly then,
</p>
<p>the possible outcomes of the random variable are in the interval [0, 1], which is an
</p>
<p>uncountably infinite set. We cannot assign a nonzero probability to each value of
</p>
<p>X and expect the sum of the probabilities to be one. One way out of this dilemma
</p>
<p>is to assign probabilities to intervals, as was done in Section 3.6. There we had a
</p>
<p>one-dimensional dartboard and we assigned a probability of the dart landing in an
</p>
<p>interval to be the length of the interval. Similarly, for our problem if each value of
</p>
<p>X is equally likely so that intervals of the same length are equally likely, we could
</p>
<p>assign
</p>
<p>P[a ~ X ~ b] = b - a (10.1)
</p>
<p>for the probability of the dart landing in the interval [a, b]. This probability assign-
ment satisfies the probability axioms given in Section 3.6 and so would suffice to
</p>
<p>calculate the probability of any interval or union of disjoint intervals (use Axiom 3
</p>
<p>for disjoint intervals). But what would we do if the probability of all equal length
</p>
<p>intervals were not the same? For example, a champion dart thrower would be more</p>
<p/>
</div>
<div class="page"><p/>
<p>288 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>likely to obtain a value near x = 0 than near x = 1. We therefore need a more
general approach. For discrete random variables it was just as easy to assign PMFs
</p>
<p>that were not uniform as ones that were uniform. Our goal then is to extend this
</p>
<p>approach to encompass continuous random variables. We will do so by examining
</p>
<p>the approximation afforded by using the P MF to calculate interval probabilities for
</p>
<p>continuous random variables.
</p>
<p>Consider first a possible approximation of (10.1) by a uniform PMF as
</p>
<p>Xi = ib..x for i = 1,2, ... ,M
</p>
<p>where b..x = 11M , so that Mb..x = 1 as shown in Figure 10.2. Then to approximate
</p>
<p>i
</p>
<p>I
</p>
<p>I
</p>
<p>i
I
</p>
<p>I
</p>
<p>i
r
r i
f i
1 I
</p>
<p>I 1
</p>
<p>f i
I I
</p>
<p>I
</p>
<p>0.12
</p>
<p>0.1
</p>
<p>~ 0 . 0 8
</p>
<p>~
~ 0 . 0 6
</p>
<p>0.04
</p>
<p>0.02
</p>
<p>o
o 0.380.52
</p>
<p>X
</p>
<p>0.12
</p>
<p>0.1
</p>
<p>~ 0. 0 8
</p>
<p>~
~ 0 . 06
</p>
<p>0.04
</p>
<p>0.02
</p>
<p>o
o 0.38 0.52
</p>
<p>X
</p>
<p>(a) M = 10, ~x = 0.1 (b) M = 20, ~x = 0.05
</p>
<p>Figure 10.2: Approximating the probability of an interval for a cont inuous random
</p>
<p>variable by using a PMF.
</p>
<p>the probability of the outcome of X in the interval [a, b] we can use
</p>
<p>P[a ::; X ::; b] = (10.2)
</p>
<p>For example, referring to Figure 1O.2a, if a = 0.38 and b = 0.52, then there are two
</p>
<p>values of X i that lie in that interval and therefore P[0.38 ::; X ::; 0.52] = 21M = 0.2,
</p>
<p>even though we know that the true value from (10.1) is 0.14 . To improve the
</p>
<p>quality of our approximation we increase M to M = 20 as shown in Figure 1O.2b.
</p>
<p>Then, we have three values of X i that lie in the interval and therefore P [0.38 ::; X ::;
</p>
<p>0.52] = 31M = 0.15, which is closer to the true value. Clearly, if we let M -+ 00 or
</p>
<p>equivalently let b..x -+ 0, our approximation will become exact. Considering again</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3. DEFINITION OF A CONTINUOUS RANDOM VARIABLE
</p>
<p>(10.2) with .6..x = l/M, we have
</p>
<p>P [a ::; X ::; b] = L 1 . .6..x
{ i : a ~ x i 9}
</p>
<p>and defining px (x) = 1 for 0 &lt; x &lt; 1 and zero otherwise, we can write this as
</p>
<p>289
</p>
<p>P[a ::; X ::; b] = L PX(Xi).6..X.
{ i :a ~ x i ~b}
</p>
<p>(10.3)
</p>
<p>Finally, letting .6..x -t 0 to yield no error in the approximation, the sum in (10.3)
</p>
<p>b ecomes an integral and PX(Xi) -t px(x) so that
</p>
<p>P[a ::; X ::; b] = l b px(x)dx (10.4)
which gives the same result for the probability of an interval as (10.1). Note that
</p>
<p>Px (x) is defined to be 1 for all 0 &lt; x &lt; 1. To interpret this new function Px (x) we
have from (10.3) with Xo = k.6..x for k an integer
</p>
<p>P[xo - .6..x/2 ::; X ::; Xo + .6..x/2]
</p>
<p>L px(xd.6..x
{ i : x o-b. x/ 2 ~ X i ~ x o +b. x / 2}
</p>
<p>which yields
</p>
<p>L PX( Xi).6..x
{i :Xi= XO}
</p>
<p>px(xo).6..x
</p>
<p>(only one value of Xi within interval)
</p>
<p>( )
_ P[xo - .6..x/2 ::; X ::; Xo+ .6..x/2]
</p>
<p>PX Xo - .6..x .
</p>
<p>This is the probability of X being in the interval [xo - .6..x/2,Xo + .6..x/2] divided
by the interval length .6..x. Hence, px(xo) is the probability per unit length and is
</p>
<p>termed the probability density function (PDF). It can be used to find the probability
</p>
<p>of any interval by using (10.4). Equivalently, since the value of an integral may be
</p>
<p>interpreted as the area under a curve, the probability is found by determining the
</p>
<p>area under the PDF curve. This is shown in Figure 10.3. The PDF is denoted
</p>
<p>by px(x) , where we now use parentheses since the argument is no longer discrete
</p>
<p>but continuous. Also , for the same reason we omit the subscript i , which was used
</p>
<p>for the PMF argument. Hence, the PDF for a cont inuous random variable is the
</p>
<p>extension of the PMF that we sought. Before continuing we examine this example
</p>
<p>further.</p>
<p/>
</div>
<div class="page"><p/>
<p>290 CHA PTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>1.2 r--~'-------'--"-- -- ---'---'
</p>
<p>oL...-----'L...-_ _ ~_ '-- --'-_...J
o 0.380.52
</p>
<p>X
</p>
<p>1 &middot;&middot; .. &middot;&middot;f t .. &bull; &middot; &bull; &middot;
! .. !
</p>
<p>H08 &middot; &middot; &middot; &middot; &middot; &middot;I : : I .
'-" . i : : i
</p>
<p>&gt;&lt; : :
\:l.,0.6 ...... .. . . . . ..... &bull; . .. ... . . . .. . . . ... . .. .
</p>
<p>0.380.52
X
</p>
<p>o
</p>
<p>I I
</p>
<p>.1 I.
</p>
<p>i i
I I
</p>
<p>I -.: :P[O:3S&middot;&lt;X"&lt;b.52ji - -,
' 1 I '
</p>
<p>I I
</p>
<p>. j j .
</p>
<p>I I
i io
</p>
<p>02
</p>
<p>12
</p>
<p>OA. . . . . . . . . . . ....... .. . . . . . . .. . . .....
</p>
<p>... .. . .. . . . .. .. ..... . .. ... ...... ..
</p>
<p>0.4 . .. ..
</p>
<p>0.2 . . . ..
</p>
<p>(a) Probability density function (b) Probability shown as shaded area
</p>
<p>Figure 10.3: Example of probability density function and how probability is found
</p>
<p>as the area under it .
</p>
<p>Exam p le 10 .1 - PDF for a uniform random varia b le and t h e MAT L A B
</p>
<p>co m mand rand
</p>
<p>The PDF given by
</p>
<p>{
I O&lt;x&lt; l
</p>
<p>px(x) = 0 otherwise
</p>
<p>is known as a uniform P DF. Equivalently, X is said to be a uniform random vari-
</p>
<p>able or we say that X is uniformly distributed on (0,1) . The shorthand notation is
</p>
<p>X f'J U(O , 1). Observe that this is the continuous random variable for which MAT-
LAB uses rand to produce a realization. Hence , in simulating a coin toss with a
</p>
<p>probability of heads of p = 0.75, we use (lOA) to obtain
</p>
<p>P [a ::; X ::; b] = l b px(x)dx
l b 1dx
</p>
<p>= b - a = 0.75
</p>
<p>and choose a = 0 and b = 0.75. The probability of obtaining an outcome in the
interval (0,0.75] for a random variable X f'J U(O, 1) is now seen to be 0.75. Hence,
</p>
<p>the code below can be used to generate the outcomes of a repeated coin tossing
</p>
<p>experiment with p = 0.75.
</p>
<p>f or i=1 :M
</p>
<p>u=randO , 1) ;</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3. DEFINITION OF A CONTINUOUS RANDOM VARIABLE
</p>
<p>if u&lt;=O.75
</p>
<p>x(i,1)=1; Yo head mapped into 1
</p>
<p>else
</p>
<p>x(i,1)=O; Yo tail mapped into 0
</p>
<p>end
</p>
<p>end
</p>
<p>Could we have used any other values for a and b?
</p>
<p>291
</p>
<p>(;
</p>
<p>Now returning to our dart thrower, we can acknowledge her superior dart-throwing
</p>
<p>ability by assigning a nonuniform PDF as shown in Figure 10.4. The probability of
</p>
<p>px(x) = 2(1 - x )
. . . .. . . .
</p>
<p>2 .; : -: ..
&middot; . .&middot; . .&middot; . .
</p>
<p>1.5 .
.........
~.......,
~
~
</p>
<p>1 .
</p>
<p>0.5 .
</p>
<p>Ol--_-L...---I... - - l . . . ~ _ - - - - l
</p>
<p>o 0.1
x
</p>
<p>0.9 1
</p>
<p>Figure 10.4: Nonuniform PDF.
</p>
<p>throwing a dart within a circle of radius 0.1 or X E [0,0.1] will be larger than for
</p>
<p>the region between the circles with radii 0.9 and 1 or X E [0.9,1]. Specifically, using
</p>
<p>(10.4)
</p>
<p>r: &deg;1prO ~ X ~ 0.1] Jo 2(1 - x )dx = 2(x - x2 / 2) lo' = 0.19
</p>
<p>P[0.9 ~ X ~ 1] = 1.: 2(1 - x )dx = 2(x - x2 /2)1~ .9 = 0.01.
Note that in this example px(x) ~ 0 for all x and also J ~ oopx(x)d x = 1. These
are properties that must be satisfied for a valid PDF. We will say more about these
</p>
<p>properties in the next section.
</p>
<p>It may be helpful to consider a mass an alogy to the PDF. An example is shown
</p>
<p>in Figure 10.5. It can be thought of as a slice of Jarlsberg cheese with length 2</p>
<p/>
</div>
<div class="page"><p/>
<p>292 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>xQ
</p>
<p>x
</p>
<p>&bull; 2
</p>
<p>Figure 10.5: Jarlsberg cheese slice used for mass analogy to PDF.
</p>
<p>meters, height of 1 meter, and depth of 1 meter, which might be purchased for a
</p>
<p>New Year 's Eve party (with a lot of guests!) . If its mass is 1 kilogram (it is a new
</p>
<p>"lite" cheese) , then its overall density D is
</p>
<p>D = mass = M = 1 kg = 1 kg/rn".
volume V 1 m3
</p>
<p>However, its linear density or mass per meter which is defined as D.M/ D.x will change
</p>
<p>with x. If each guest is allowed to cut a wedge of cheese of length D.x as shown in
</p>
<p>Figure 10.5, then clearly the hungriest guests should choose a wedge near x = 2 for
</p>
<p>the greatest amount of cheese . To determine the linear density we compute D.M/ D.x
</p>
<p>versus x. To do so first note that D.M = DD.V = D.V and D.V = 1 . (area of face),
</p>
<p>where the face is seen to be trapezoidal. Thus,
</p>
<p>D.V = ~D.x (xQ - D.x/2 + XQ + D.X/2) = ~XQD.x.
2 2 2 2
</p>
<p>Hence, D.M/ D.x = D.V / D.x = XQ /2 and this is the same even as D.x -+ O. Thus,
</p>
<p>dM 1
-=-x
dx 2
</p>
<p>and to obtain the mass for any wedge from x = a to x = b we need only integrate
dM/ dx to obtain the mass as a function of x. This yields
</p>
<p>l
b
</p>
<p>1 l bM([a, b)) = a "2 xdx = a m(x)dx
where m(x) = x/2 is the linear mass density or the mass per unit length. It is
</p>
<p>perfectly analogous to the PDF which is the probability per unit length. Can you
</p>
<p>find the total mass of cheese from M([a , b))? See also Problem 10.3.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4. THE PDF AND ITS PROPERTIES
</p>
<p>10.4 The PDF and Its Properties
</p>
<p>293
</p>
<p>The PDF must have certain properties so that the probabilities obtained using (lOA)
</p>
<p>satisfy the axioms given in Section 304. Since the probability of an interval is given
</p>
<p>by
</p>
<p>P[a ~ X ~ b] = l b px(x)dx
the PDF must have the following properties.
</p>
<p>Property 10.1 - PDF must be nonnegative.
</p>
<p>px(x) 2: 0 - 00 &lt; x &lt; 00.
</p>
<p>Proof: If px(x) &lt; 0 on some small interval [xo - !:1x/2, Xo + !:1x/2]' then
</p>
<p>l
XO+ b. X/ 2
</p>
<p>P[xo - !:1x/2 ~ X ~ Xo + !:1x/2] = px(x)dx &lt; 0
xo-b.x/2
</p>
<p>which violates Axiom 1 that prE] 2: 0 for all events E.
o
</p>
<p>Property 10.2 - PDF must integrate to one.
</p>
<p>i: px(x)dx = 1
1 = P[X E Sx] = P[-oo &lt; X &lt; 00] =i: px(x)dx
</p>
<p>o
Hence, any nonnegative function that integrates to one can be considered as a PDF.
</p>
<p>An example follows.
</p>
<p>Example 10.2 - Exponential PDF
</p>
<p>Consider the function
</p>
<p>( ) _ {..\exp(-..\x) x 2: 0
PX x - 0 x &lt; 0 (10.5)
</p>
<p>for ..\ &gt; O. This is called the exponential PDF and is shown in Figure 10.6. Note
that it is discontinuous at x = O. Hence, a PDF need not be continuous (see also
</p>
<p>Figure 10.3a for the uniform PDF which also has points of discontinuity) . Also , for
</p>
<p>..\ &gt; 1, we have px(O) = ..\ &gt; 1. In contrast to a PMF, the PDF can exceed one in</p>
<p/>
</div>
<div class="page"><p/>
<p>294 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>1.2.------,-----,---,------.------,--.-------,
</p>
<p>5432
x
</p>
<p>o-1
</p>
<p>&middot;&middot;&middot;&middot; &middot; &gt;i = 1&middot;&middot; .
</p>
<p>0.4 : J .
i
I
</p>
<p>0.2 : J
</p>
<p>!
I
</p>
<p>.--..,0.8 .
</p>
<p>&lt;-l
'--"
</p>
<p>:-::
&lt;:l..0.6 .
</p>
<p>Figure 10.6: Exponential PDF.
</p>
<p>value. It is the area under the PDF that cannot exceed one . As expected px(x) ~ 0
</p>
<p>for -00 &lt; x &lt; 00 and
</p>
<p>i:PX(X)dX 100 Aexp(-Ax)dx
- exp( -Ax)lgo = 1
</p>
<p>for A &gt; O. This PDF is often used as a model for the lifetime of a product. For
example, if X is the failure time in days of a lightbulb, then P[X &gt; 100] is the
probability that the lightbulb will fail after 100 days or it will last for at least 100
</p>
<p>days. This is found to be
</p>
<p>P[X&gt; 100] = roo Aexp(-Ax)dx
1100
- exp (- AX)I~o
</p>
<p>= exp(-lOOA)
</p>
<p>{
0.367 A = 0.01
</p>
<p>- 0.904 A = 0.001.
</p>
<p>The probability of a sample point is zero.
</p>
<p>If X is a continuous random variable, then it was argued in Section 3.6 that the
</p>
<p>probability of a point is zero. This is consistent with our definition of a PDF. If the</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5. IMPORTANT PDFS 295
</p>
<p>width of the interval shrinks to zero, then the area under the PDF also goes to zero.
</p>
<p>Hence , P[X = x] = O. This is true whether or not px(x) is continuous at the point
of interest (as long as the discontinuity is a finite jump). In the previous example
</p>
<p>of an exponential PDF P[X = 0] = 0 even though px(O) is discontinuous at x = O.
This means that we could, if desired, have defined the exponential PDF as
</p>
<p>( )
_ {&gt;.exp(-&gt;.x) x&gt; 0
</p>
<p>PX x - 0 x:::; 0
</p>
<p>for which px(O) is now defined to be O. It makes no difference in our probability
</p>
<p>calculations whether we include x = 0 in the interval or not. Hence, we see that
</p>
<p>rbpx(x)dx = r px(x)dx = r px(x)dx
Jo- Jo+ Jo
</p>
<p>and in a similar manner if X is a continuous random variable, then
</p>
<p>P[a :::; X :::; b] = P[a &lt; X :::; b] = P[a :::; X &lt; b] = P[a &lt; X &lt; b].
</p>
<p>In summary, the value assigned to the PDF at a discontinuity is arbitrary since
</p>
<p>it does not affect any subsequent probability calculation involving a continuous
</p>
<p>random variable. However, for discontinuities other than step discontinuities (which
</p>
<p>are jumps of finite magnitude) we will see in Section 10.8 that we must be more
</p>
<p>careful.
</p>
<p>10.5 Important PDFs
</p>
<p>There are a multitude of PDFs in use in various scientific disciplines. The books
</p>
<p>by [Johnson, Kotz, and Balakrishnan 1994] contain a summary of many of these
</p>
<p>and should be consulted for further information. We now describe some of the more
</p>
<p>important PDFs.
</p>
<p>10.5.1 Uniform
</p>
<p>We have already encountered a special case of the uniform PDF in Figure 10.3.
</p>
<p>More generally it is defined as
</p>
<p>{
</p>
<p>I a&lt; x&lt;b
px(x) = Ob-a
</p>
<p>otherwise
(10.6)
</p>
<p>and examples are shown in Figure 10.7. It is given the shorthand notation X '"
</p>
<p>U(a,b). If a = 0 and b = 1, then an outcome of a U(O , 1) random variable can be
</p>
<p>generated in MATLAB using rand.</p>
<p/>
</div>
<div class="page"><p/>
<p>296 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>0 . 6 r - ~ - ~ - ~ - ~ - ~ - ~ - - - , 0.6 .-~r-~-~-~-~-~---,
</p>
<p>0.5 . &bull; j.... ----, 0.5 . . .. . ' . . . - '" ~ .. .
I I
</p>
<p>~0.4 .. 11. . .1..
- !
~ i I&gt;:l.0.3 j j... .. . ... .... &bull;. . ... . .. .. ...
</p>
<p>j . i .
0.2 "1" : :. .. . . &bull;.. . .. .. . . . . .
</p>
<p>. . . .
I . I :
</p>
<p>0.1 &middot; &middot; &middot; &middot; &middot;&middot; 1 j .
</p>
<p>I I
</p>
<p>~0.4 .. . .
'-"''"" .
~ .
&gt;:l.0.3 : : :..
</p>
<p>0.2 r-i---:----:---:---:----,1 &middot; &middot; &middot; &middot; &middot; &middot;
</p>
<p>I I
0.1 .j. . .. . j .
</p>
<p>I I
</p>
<p>o '-----J'--~_----'_~_~_~_....J
2 3
</p>
<p>x
4 5 6 7 2 3
</p>
<p>x
4 5 6 7
</p>
<p>(a) a = 1, b = 3 (b) a = 1, b = 6
</p>
<p>Figure 10.7: Examples of uniform PDF.
</p>
<p>10.5.2 Exponential
</p>
<p>This was previously defined in Example 10.2. The shorthand notation is X rv
</p>
<p>exp(X).
</p>
<p>10.5.3 Gaussian or Normal
</p>
<p>This is the famous "bell-shaped" curve first introduced in Section 1.3. It is given by
</p>
<p>-oo&lt; x&lt;oo (10.7)
</p>
<p>where u2 &gt; 0 and -00 &lt; J..L &lt; 00. Its application in practical problems is ubiquitous.
It is shown to integrate to one in Problem 10.9. Some examples of this PDF as well
</p>
<p>as some outcomes for various values of the parameters (J..L, u2 ) are shown in Figures
</p>
<p>10.8 and 10.9. It is characterized by the two parameters J..L and u 2 . The parameter
</p>
<p>J..L indicates the center of the PDF which is seen in Figures 1O.8a and 1O.8c. It depicts
</p>
<p>the "average value" of the random variable as can be observed by examining Figures
</p>
<p>10.8b and 10.8d. In Chapter 11 we will show that J..L is actually the mean of X. The
</p>
<p>parameter u2 indicates the width of the PDF as is seen in Figures 1O.9a and 1O.9c.
</p>
<p>It is related to the variability of the outcomes as seen in Figures 1O.9b and 10.9d. In
</p>
<p>Chapter 11 we will show that u2 is actually the variance of X. The PDF is called the
</p>
<p>Gaussian PDF after the famous German mathematician K.F. Gauss and also the
</p>
<p>normal PDF, since "normal" populations tend to exhibit this type of distribution.
</p>
<p>A standard normal PDF is one for which J..L = 0 and u2 = 1. The shorthand notation
is X rv N(J..L , u2 ) . MATLAB generates a realization of a standard normal random
</p>
<p>variable using randn. This was used extensively in Chapter 2.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5. IMPORTANT PDFS 297
</p>
<p>0 .5.-------~-- - ---___,
</p>
<p>302510 15 20
Trial number
</p>
<p>5
</p>
<p>5r--~- ~-~-~ -""'--'~-:l
</p>
<p>4 ..
</p>
<p>3 . . .
</p>
<p>I:1 ~,II., &bull;&bull; tjl.i~tl Fl f
0 - 1 .-!- ..! : : I&middot;J&middot;&middot;&middot;;&middot;&middot;&middot; &middot;lL
</p>
<p>-2 . : '.
</p>
<p>-3 . .
</p>
<p>- 4 . .
</p>
<p>_5L.--~-~-~-~--~-~
</p>
<p>o5o
X
</p>
<p>oL.-__ ~ ______i ~ ____'
</p>
<p>-5
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.4 .
</p>
<p>~
,&gt;&lt; 0.3 . . .
</p>
<p>~
</p>
<p>(a) p, = 0, (1 2 = 1 (b) p, = 0, (1 2 = 1
</p>
<p>0.5 r---- --~--~---___,
</p>
<p>302510 15 20
Trial num ber
</p>
<p>5
</p>
<p>5r---~-~-~-~- ~ --:l
</p>
<p>jIIIIIII11r.IIIrl!r.lrIIII11tnQ)Soo...,
;::l
</p>
<p>0 - 1 .
</p>
<p>- 2 : .
</p>
<p>- 3 ..
</p>
<p>- 4 .
</p>
<p>_ 5 '-- -~-~-~-~-~ ~-..L.J
</p>
<p>o52o
X
</p>
<p>o '------"'"""""'---'--~----"""
- 5
</p>
<p>0.4
</p>
<p>0.2 .
</p>
<p>0.1 .
</p>
<p>(c) p,= 2,(12= 1 (d) p, = 2, (1 2 = 1
</p>
<p>Figure 10.8: Examples of Gaussian PDF with different j.t's.
</p>
<p>To find the probability of the outcome of a Gaussian random variable lying within
</p>
<p>an interval requires numerical integrat ion (see Problem 1.14) since the integral
</p>
<p>I
b 1
</p>
<p>fie exp( - (1/2 )x 2 )dx
a y27f
</p>
<p>cannot be evaluated analytically. A MAT LAB subprogram will be provided and
</p>
<p>described short ly to do this. The Gaussian PDF is commonly used to model noise in
</p>
<p>a communication system (see Section 2.6), as well as for numerous ot her applications.
</p>
<p>We will see in Chapter 15 that the PDF arises qui te naturally as the PDF of a large
</p>
<p>number of independent random variables that have been added together.</p>
<p/>
</div>
<div class="page"><p/>
<p>298 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>0.5 r-------~-----___,
</p>
<p>0.4 .. .
</p>
<p>~
,&gt;&lt;0.3 .
</p>
<p>~
</p>
<p>0.2
</p>
<p>0.1
</p>
<p>o
X
</p>
<p>(a) J.L = 0, a 2 = 1 (b) J.L = 0, a 2 = 1
</p>
<p>3025
</p>
<p>IIlownl.!Ojr[
</p>
<p>. . .. .. .... . .... ...... ', '
</p>
<p>10 15 20
Trial number
</p>
<p>5
</p>
<p>5r---r---r---.---~-~-----.-,
</p>
<p>4
</p>
<p>3
</p>
<p>_5L.--~-~-~-~-~----....J
</p>
<p>o
</p>
<p>0.5 r-------~-----___,
</p>
<p>OA .
</p>
<p>(c) J.L = 0, a 2 = 2 (d) J.L = 0, a 2 = 2
</p>
<p>Figure 10.9: Examples of Gaussian PDF with different (j2 ,s.
</p>
<p>10.5.4 Laplacian
</p>
<p>This PDF is named after Laplace, the famous French mathematician. It is similar
</p>
<p>to the Gaussian except that it does not decrease as rapidly from its maximum value .
</p>
<p>Its PDF is
</p>
<p>-oo&lt; x&lt;oo (10.8)
</p>
<p>where (j2 &gt; O. Again the parameter (j2 specifies the width of the PDF, and will be
shown in Chapter 11 to be the variance of X. It is seen to be symmetric about x = O.
Some examples of the PDF and outcomes are shown in Figure 10.10. Not e that for</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5. IMPORTANT PDFS 299
</p>
<p>302510 15 20
Trial number
</p>
<p>5
</p>
<p>5r---~-~-~-:---,-------n
</p>
<p>4
</p>
<p>3
</p>
<p>-5~-~-~-~-~-~-----'-'
</p>
<p>&deg;
</p>
<p>Q)
</p>
<p>S
o
u...,
</p>
<p>5&deg;X
oL----=----~ __-="-"_--J
-5
</p>
<p>0.8 .
</p>
<p>0.2 .
</p>
<p>~
,&gt;&lt;0 .6 .
</p>
<p>~
</p>
<p>0.4 .
</p>
<p>(a) (]" 2 = 1 (b) (]" 2 = 1
</p>
<p>0.8 .
</p>
<p>~
,&gt;&lt;0 .6 .
</p>
<p>~
</p>
<p>0.4
</p>
<p>0.2
</p>
<p>&deg;X
</p>
<p>5,.--~-~-~-~-----.---,....,
</p>
<p>-3 .
</p>
<p>-4 .
</p>
<p>-5 ' - - ' - _ ~ _ ~ . L - ~ _ ~ _ ~ _ ~
</p>
<p>&deg; 5 10 15 20 25 30
Trial number
</p>
<p>(c) (]"2 = 4
</p>
<p>Figure 10.10: Examples of Laplacian PDF with different O'2 ,s.
</p>
<p>the same 0'2 as the Gaussian PDF, the outcomes are larger as seen by comparing
</p>
<p>Figure 10.10b to Figure 10.9b. This is due to the larger probability in the "tails" of
</p>
<p>the PDF. The "tail" region of the PDF is that for which Ixl is large. The Laplacian
PDF is easily integrated to find the probability of an interval. This PDF is used as
</p>
<p>a model for speech amplitudes [Rabiner and Schafer 1978].
</p>
<p>10.5.5 Cauchy
</p>
<p>The Cauchy PDF is named after another famous French mathematician and is
</p>
<p>defined as
1
</p>
<p>px(x) = 71"(1 + x2) - 00 &lt; X &lt; 00. (10.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>300 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>It is shown in Figure 10.11 and is seen to be symmetric about x = O. The Cauchy
</p>
<p>PDF can easily be integrated to find the probability of any interval. It arises as the
</p>
<p>PDF of the ratio of two independent N(o, 1) random variables (see Chapter 12).
</p>
<p>0.5 r------~-----_____,
</p>
<p>0.4 .
</p>
<p>5o
x
</p>
<p>o"-------~--------=
-5
</p>
<p>0.1
</p>
<p>-----~
~0 .3
</p>
<p>~
</p>
<p>0.2
</p>
<p>Figure 10.11: Cauchy PDF.
</p>
<p>10.5.6 Gamma
</p>
<p>The Gamma PDF is a very general PDF that is used for nonnegative random vari-
</p>
<p>ables. It is given by
</p>
<p>{
</p>
<p>),'" a-I (')
Px(X) = r(a)X exp -AX X ~ 0
</p>
<p>o x&lt;O
(10.10)
</p>
<p>where .\ &gt; 0, a&gt; 0, and I'(z] is the Gamma function which is defined as
</p>
<p>r(z) = 100 tz - 1 exp( -t)dt. (10.11)
Clearly, the I'(o] factor in (10.10) is the normalizing constant needed to ensure that
the PDF integrates to one. Some examples of this PDF are shown in Figure 10.12.
</p>
<p>The shorthand notation is X ,.... I'(o, .\). Some useful properties of the Gamma
function are as follows.
</p>
<p>Property 10.3 - I'( z + 1) = zr(z )
Proof: See Problem 10.16.
</p>
<p>o</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5. IMPORTA N T PDFS 301
</p>
<p>1 .5.--~-~-~~-~-~---,
</p>
<p>x
2 3 4 5
</p>
<p>&middot; &gt;' = 4
</p>
<p>o
x
</p>
<p>O'--~---&lt;--~--"""-~-~~
</p>
<p>- 2 - 1
</p>
<p>0 .5
</p>
<p>2 3 4 5o
</p>
<p>&middot;&middot;&middot; &middot;&middot; a = l&middot;
</p>
<p>o L-~_...L-_-'-- -----"~"":::::::::~"';"';;;=
-2 - 1
</p>
<p>0.5
</p>
<p>(a).\= l (b) Q = 2
</p>
<p>Figure 10.12: Examples of Gamma PDF.
</p>
<p>Property 10.4 - f(N) = (N - 1)!
</p>
<p>P roof: Follows from Property 10.3 with z = N - 1 since
</p>
<p>f(N) = (N - 1)f(N - 1)
</p>
<p>(N - 1)(N - 2)f( N - 3) (let z = N - 2 now)
</p>
<p>(N - 1)(N - 2) . .. 1 = (N - 1)!
</p>
<p>o
</p>
<p>Property 10.5 - f(1/2) = .j7i
</p>
<p>P roof:
</p>
<p>f(1/2) = 100 r 1/ 2 exp(-t)dt
(Note t hat near t = 0 the integrand becomes infinite bu t C 1/ 2 exp( - t ) ~ C 1/ 2
</p>
<p>which is integrab le.) Now let t = u2/2 and thus dt = udu which yields
</p>
<p>f( 1/2) = 1
00 1
</p>
<p>JU272 exp( _ u2 /2 )udu
o u 2/2
</p>
<p>100 J2exp(_ u2 /2 )du
V2joo2 -00 exp( _ u2 /2 )du (integrand is symmetric about u = 0)
</p>
<p>v
</p>
<p>=J27r why?
</p>
<p>y'-i.
</p>
<p>o</p>
<p/>
</div>
<div class="page"><p/>
<p>302 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>The Gamma PDF reduces to many well known PDFs for appropriate choices of
</p>
<p>the parameters a and A. Some of these are :
</p>
<p>1. Exponential for a = 1
</p>
<p>From (10.10) we have
</p>
<p>px(x) = { ort1)exp( -AX) X 2: 0
x &lt; O.
</p>
<p>But I'{l ) = O! = 1, which results from Property 10.4 so that we have the
</p>
<p>exponential PDF.
</p>
<p>2. Chi-squared PDF with N degrees of freedom for a = N /2 and A= 1/2
From (10.10) we have
</p>
<p>{
</p>
<p>1 N /2-1 ( /2) &gt; 0
px(x) = 02N / 2r(N/2)x exp -x x -
</p>
<p>x &lt; O.
(10.12)
</p>
<p>This is called the chi-squared PDF with N degrees of freedom and is important
</p>
<p>in statistics. It can be shown to be the PDF for the sum of the squares of N
</p>
<p>independent random variables all with the same PDF N(O, 1) (see Problem
</p>
<p>12.44). The shorthand notation is X '" X ~ .
</p>
<p>3. Erlang for a = N
From (10.10) we have
</p>
<p>{
</p>
<p>&gt;.N N 1 (A)
px(x) = f(N)x - exp - x x 2: 0
</p>
<p>o x &lt; O
</p>
<p>and since r(N) = (N - I)! from Property 10.4, this becomes
</p>
<p>{
</p>
<p>&gt;.N N-l ( )
px(x) = (N_l)!x exp -AX x 2: 0
</p>
<p>o x &lt; o.
(10.13)
</p>
<p>(10.14)
</p>
<p>This PDF arises as the PDF of a sum of N independent exponential random
</p>
<p>variables all with the same A (see also Problem 10.17).
</p>
<p>10.5.7 Rayleigh
</p>
<p>The Rayleigh PDF is named after the famous British physicist Lord Rayleigh and
</p>
<p>is defined as
</p>
<p>{
</p>
<p>X (1 X 2 )
px(x) = ~ exp -2~ x 2: 0
</p>
<p>o x &lt; O.
</p>
<p>It is shown in Figure 10.13. The Rayleigh PDF is easily integrated to yield the</p>
<p/>
</div>
<div class="page"><p/>
<p>10.6. CUMULATIVE DISTRIBUTION FUNCTIONS
</p>
<p>0.8 . . . .
</p>
<p>303
</p>
<p>. : : .
---H
~0.6 .
</p>
<p>~
</p>
<p>2
x
</p>
<p>3 4 5
</p>
<p>Figure 10.13: Rayleigh PDF with 0-2 = 1.
</p>
<p>probability of any interval. It can be shown to arise as the PDF of the square root
</p>
<p>of the sum of the squares of two independent N(O, 1) random variables (see Example
</p>
<p>12.12).
</p>
<p>Finally, note that many of these PDFs arise as the PDFs of transformed Gaussian
</p>
<p>random variables. Therefore, realizations of the random variable may be obtained
</p>
<p>by first generating multiple realizations of independent standard normal or N(O,1)
</p>
<p>random variables, and then performing the appropriate transformation. An alterna-
</p>
<p>tive and more general approach to generating realizations of a random variable, once
</p>
<p>the PDF is known, is via the probability integral transformation to be discussed in
</p>
<p>Section 10.9.
</p>
<p>10.6 Cumulative Distribution Functions
</p>
<p>The cumulative distribution function (CD F) for a continuous random variable is
</p>
<p>defined exactly the same as for a discrete random variable. It is
</p>
<p>Fx(x) = P[X ~ x]
</p>
<p>and is evaluated using the PDF as
</p>
<p>Fx(x) = i: px(t)dt
-oo&lt;x&lt;oo
</p>
<p>- 00 &lt; x &lt; 00.
</p>
<p>(10.15)
</p>
<p>(10.16)
</p>
<p>Avoiding confusion in evaluating CDFs
</p>
<p>It is important to note that in evaluating a definite integral such as in (10.16) it
</p>
<p>is best to replace the variable of integration with another symbol. This is because</p>
<p/>
</div>
<div class="page"><p/>
<p>304 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>the upper limit depends on x which would conflict with the dummy variable of
</p>
<p>integration. We have chosen to use t but of course any other symbol that does not
conflict with x can be used.
</p>
<p>Some examples of the evaluation of the CDF are given next.
</p>
<p>10.6.1 Uniform
</p>
<p>Using (10.6) we have
</p>
<p>Fx(x) ~ { [: b ~ a d t
x ~ a
</p>
<p>a&lt;x&lt;b
</p>
<p>x?b
</p>
<p>which is
</p>
<p>{
</p>
<p>O x ~ a
</p>
<p>Fx(x) = 1b~a (x - a) a &lt; x &lt; b
x? b.
</p>
<p>An example is shown in Figure 10.14 for a = 1 and b = 2.
</p>
<p>1.2.--~-~-~-~-~---,
</p>
<p>Eo.s
~
</p>
<p>~0 .6
</p>
<p>0.4
</p>
<p>0.2 .
</p>
<p>4321
X
</p>
<p>o
o '---_~_~_..L..-_~ _ __'_____ ___l
-2 -1
</p>
<p>Figure 10.14: CDF for uniform random variable over interval (1,2).
</p>
<p>10.6.2 Exponential
</p>
<p>Using (10.5) we have
</p>
<p>F {O x&lt;O
x(x) = fox Aexp(-At)dt x? o.
</p>
<p>But
</p>
<p>l X Aexp(-At)dt = -exp(-At)l~ = 1- exp(-Ax)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.6. CUMULATIVE DISTRIBUTION FUNCTIONS 305
</p>
<p>so that
</p>
<p>{
O x &lt; 0
</p>
<p>Fx(x)= 1-exp(-Ax) x ~ O.
</p>
<p>An example is shown in F igure 10.15 for A = 1.
</p>
<p>1 . 2 r-- --~--~--- ~--___,
</p>
<p>.--..
H 0.8
</p>
<p>'-"
</p>
<p>&gt;&lt;
~ 0 .6
</p>
<p>0.4
</p>
<p>0.2
</p>
<p>64o
O'-----'------ ~---~- --'
</p>
<p>-2
</p>
<p>Figure 10.15: CDF for exponent ial random variable with A = 1.
</p>
<p>Note that for the uniform and exponent ial random variables the CDFs are con-
</p>
<p>tinuous even though the PDFs are discontinuous. This property motivates an al-
</p>
<p>ternative definition of a continuous random variable as one for which the CnF is
</p>
<p>continuous. Recall that the CDF of a discrete random variable is always discontin-
</p>
<p>uous, displaying multiple jumps.
</p>
<p>10. 6. 3 Gaussian
</p>
<p>Consider a st andard normal PDF, which is a Gaussian PDF with J-L = 0 and (J2 = 1.
(If J-L ::J 0 and / or (J2 ::J 1 the CDF is a simple modification as shown in Problem
10.22.) Then from (10.7) we have
</p>
<p>Fx(x) = i: vk exp ( _~t 2 ) dt - 00 &lt; x &lt; 00 .
This cannot be evaluated further but can be found numerically and is shown in
</p>
<p>Figure 10.16. The CDF for a standard normal is usually given the special symbol
</p>
<p>~ ( x ) so that
</p>
<p>Hence, ~(x) represents the area under the P DF to the left of the point x as seen
</p>
<p>in Figure 1O.17a. It is sometimes more convenient, however, to have knowledge of
</p>
<p>the area to the right instead. This is called the right- tail probability of a standard</p>
<p/>
</div>
<div class="page"><p/>
<p>306 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>1.2r----...,...-----.-------r-----,
</p>
<p>1 : : ~ . : _- ---i
&middot; . .&middot; . .&middot; . ............. . . .
</p>
<p>~0 .8 : : ; .
</p>
<p>:&gt;&lt; &bull; . .
~ 0.6 ~ ~ ~ .
</p>
<p>&middot; .&middot; .&middot; .
0.4 : : : .
</p>
<p>&middot; . .&middot; . .&middot; . .
0.2 : : : .
</p>
<p>&middot; . .&middot; . .&middot; . .. .
42o
</p>
<p>X
-2
</p>
<p>OL...---=---..........---""-----'
-4
</p>
<p>Figure 10.16: CDF for st andard normal or Gaussian random variable.
</p>
<p>0.5 ,..-........--.--..--..--..-----r--.---.
</p>
<p>. . .
0.4 .... . ; .. . .. : .. . .. : . . . .
</p>
<p>H .
~0 .3 .. . .. ; ; :
" ..... . .
</p>
<p>0.2 . . .. . ~ .. ... ~ .. ..
</p>
<p>oL...-_!Dllll
- 4 -3 - 2 - 1 0
</p>
<p>X
</p>
<p>234
</p>
<p>0.4 ; ; : : : ; .
</p>
<p>.........
H .
~0 .3 ; ; : ; ; ; ..
</p>
<p>~ :: . : . :. . . ... . .
0.2 . . . . . ~ . . . .. ~ . . . . ~ . . ... ~ . . . . . . .. . r &bull; &bull; &bull; &bull; &bull; &bull; &bull;&bull;&bull;&bull;
</p>
<p>&middot; . . .&middot; . . .&middot; . . .&middot; . . .
0.1 ..... : .... . : . .. . : .. ... : .. ...
</p>
<p>&middot; . . .&middot; . .&middot; . .&middot; . .&middot; ..
234
</p>
<p>(a) Sh aded area = &lt;1&gt;(1) (b) Sh aded area = Q(l )
</p>
<p>Figure 10.17: Definitions of &lt;I&gt;(x ) and Q(x) fun ctions.
</p>
<p>normal and is given the symbol Q(x) . It is termed the "Q" function and is defined
</p>
<p>as the area to the right of x , an example of which is shown in Figure 1O.17b. By its
</p>
<p>definition we have
</p>
<p>Q(x) = 1 - &lt;I&gt; (x)
</p>
<p>= 100 _1_ exp (_~ t 2) dt - 00 &lt; x &lt; 00
x V2ir 2
</p>
<p>(10.17)
</p>
<p>(10.18)
</p>
<p>and is shown in Figure 10.18, plot ted on a linear as well as a logarithmic vert ical
</p>
<p>scale. Some of the properties of the Q function that are easily verified are (see</p>
<p/>
</div>
<div class="page"><p/>
<p>10.6. CUMULATIVE DISTRiBUTION FUNCTIONS 307
</p>
<p>5&deg; 2 3 4
X
</p>
<p>&middot; . .. . . . , .
&middot; ..
&middot; .&middot; . ..... ..... ~ , .
&middot; . .
&middot; ..
</p>
<p>~ 10-3
</p>
<p>0
10
</p>
<p>-4
</p>
<p>10- 5 . . . . .". . .
</p>
<p>10- 6
</p>
<p>10-7 l.---,--~---,_~~~_~-,---,---'
</p>
<p>-5 -4 -3 -2 -12 3 4 5
ol.----'-----'-----'--'-------'----'-~=--~----'---'
</p>
<p>-5 -4 -3 -2 -1 &deg;
X
</p>
<p>0.2 . ..
</p>
<p>0.8
</p>
<p>8'0.6
..........
o
</p>
<p>0.4
</p>
<p>(a) Linear vertical scale (b) Logarithmic vertical scale - for display of
</p>
<p>small values of Q(x)
</p>
<p>Figure 10.18: Q(x) function.
</p>
<p>Problem 10.25)
</p>
<p>Q(-(0)
</p>
<p>Q(oo) =
</p>
<p>Q(O)
</p>
<p>Q(-x)
</p>
<p>1
</p>
<p>o
1
</p>
<p>2
</p>
<p>1 - Q(x) .
</p>
<p>(10.19)
</p>
<p>(10.20)
</p>
<p>(10.21)
</p>
<p>(10.22)
</p>
<p>Although the Q function cannot be evaluated analytically, it is related to the well
</p>
<p>known "error function". Thus, making use of the latter function a MATLAB sub-
</p>
<p>program Q.m, which is listed in Appendix lOB, can be used to evaluate it. An
example follows.
</p>
<p>Example 10.3 - Probability of error in a communication system
</p>
<p>In Section 2.6 we analyzed the probability of error for a PSK digital communication
</p>
<p>system. The probability of error P; was given by
</p>
<p>r; = P[A/2 + W ~ 0]
</p>
<p>where W "J N(O , 1). (In the MATLAB code we used w=randn(1, 1) and hence the
</p>
<p>random variable representing the noise was a standard normal random variable.)</p>
<p/>
</div>
<div class="page"><p/>
<p>308 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>To explicitly evaluate Pe we have that
</p>
<p>Pe P[A/2 + W ::; 0]
1 - P[A/2 + W &gt; 0]
</p>
<p>= 1 - P[W &gt; - A/2]
</p>
<p>1 - Q(- A/2) (definition)
</p>
<p>= Q(A/2) (use (10.22)).
</p>
<p>Hence , the true P; shown in Figure 2.15 as the dashed line can be found by using
</p>
<p>the MATLAB subprogram Q.m, which is listed in Appendix lOB, for the argument
</p>
<p>A/2 (see Problem 10.26). It is also sometimes important to determine A to yield
</p>
<p>a given Pe. This is found as A = 2Q-1(Pe), where Q-1 is the inverse of the Q
</p>
<p>function. It is defined as the value of x necessary to yield a given value of Q(x).
</p>
<p>It too cannot be expressed analytically but may be evaluated using the MATLAB
</p>
<p>subprogram Qinv .m, also listed in Appendix lOB.
o
</p>
<p>The Q function can also be approximated for large values of x using [Abramowitz
</p>
<p>and Stegun 1965]
</p>
<p>Q(x) ~ _1_ exp (_~x2) x&gt; 3. (10.23)
.;'Fix 2
</p>
<p>A comparison of the approximation to the true value is shown in Figure 10.19. If
</p>
<p>10'6 ..
</p>
<p>10,7
o 0.5
</p>
<p>. . .. . . . . . . . .. , ~ .
</p>
<p>1.5 2 2.5 3 3.5 4 4.5 5
X
</p>
<p>Figure 10.19: Approximation of Q function - true value is shown dashed.
</p>
<p>x "'" N(Il , 0-2 ) , then the right-tail probability becomes
</p>
<p>P[X &gt; x] = Q(x;;) (10.24)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.6. CUMULATIVE DISTRIBUTION FUNCTIONS 309
</p>
<p>(see Problem 10.24). Finally, note that the area under the standard normal Gaussian
</p>
<p>PDF is mostly contained in the interval [-3,3] . As seen in Figure 10.19 Q(3) ~
0.001, which means that the area to the right of x = 3 is only 0.001. Since the PDF
is symmetric, the total area to the right of x = 3 and to the left of x = -3 is 0.002 or
</p>
<p>the area in the [-3,3] interval is 0.998. Hence, 99.8% of the probability lies within
</p>
<p>this interval. We would not expect to see a value greater than 3 in magnitude very
</p>
<p>often. This is borne out by an examination of Figure 10.8b. How many realizations
</p>
<p>would you expect to see in the interval (1, oo)? Is this consistent with Figure 1O.8b ?
</p>
<p>As we have seen, the CDF for a continuous random variable has certain prop-
</p>
<p>erties. For the most part they are the same as for a discrete random variable: the
</p>
<p>CDF is 0 at x = -00, 1 at x = 00, and is monotonically increasing (or stays the
</p>
<p>same) between these limits. However, now it is continuous, having no jumps. The
</p>
<p>most important property for practical purposes is that which allows us to compute
</p>
<p>probabilities of intervals. This follows from the property
</p>
<p>P[a s X ::; b] = P[a &lt; X ::; b] = Fx(b) - Fx(a) (10.25)
</p>
<p>which is easily proven (see Problem 10.35). It can be seen to be valid by referring to
</p>
<p>Figure 10.20. Using the CDF we no longer have to integrate the PDF to determine
</p>
<p>px(x)
</p>
<p>area = Fx (b) area = Fx (a)
</p>
<p>x
</p>
<p>=
</p>
<p>a b
</p>
<p>P [a s X s b]
</p>
<p>x
</p>
<p>Figure 10.20: Illustration of use of CDF to find probability of interval.
</p>
<p>probabilities of intervals. In effect, all the integration has been done for us in finding
</p>
<p>the CDF. Some examples follow.
</p>
<p>Example 10.4 - Probability of interval for exponential PDF
</p>
<p>Since Fx(x) = 1 - exp( -&gt;.x) for x 2: 0, we have for a &gt; 0 and b &gt; 0
</p>
<p>P[a ::; X::; b] = Fx(b) - Fx(a)
</p>
<p>(l-exp(-&gt;.b)) - (l-exp(-&gt;.a))
</p>
<p>= exp( -&gt;.a) - exp(-&gt;.b)
</p>
<p>which should be compared to
</p>
<p>lb xexp( -&gt;.x)dx.</p>
<p/>
</div>
<div class="page"><p/>
<p>310 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>Since we obtained the CDF from the PDF, we might suppose that the PDF
</p>
<p>could be recovered from the CDF. For a discrete random variable this was the case
</p>
<p>since PX[Xi] = Fx(xi) - Fx(x:;). For a continuous random variable we consider a
small interval [xo - !:i.x/2, Xo + !:i.x/2] and evaluate its probability using (10.25) with
</p>
<p>Fx(x) = i: px(t)dt.
Then, we have
</p>
<p>Fx(xo + !:i.x/2) - Fx(xo - !:i.x/2)
</p>
<p>j
</p>
<p>XO+t:&gt;.X/ 2 jXO-t:&gt;.X/2
</p>
<p>-00 px(t)dt - -00 px(t)dt
</p>
<p>c: px(t)dtxo-t:&gt;.x/2
c:~ px(xo) 1 dt (Px(t) ~ constant as !:i.x --+ 0)xo-t:&gt;.x/2
</p>
<p>px(xo)!:i.x
</p>
<p>so that
</p>
<p>Fx(xo + !:i.x/2) - Fx(xo - !:i.x/2)
!:i.x
</p>
<p>as !:i.x --+ O.--+ dF;x(X)l.,
Hence, we can obtain the PDF from the CDF by differentiation or
</p>
<p>( )
_ dFx(x)
</p>
<p>PX x - dx . (10.26)
</p>
<p>This relationship is really just the fundamental theorem of calculus [Widder 1989].
</p>
<p>Note the similarity to the discrete case in which PX[Xi] = Fx(xi) - Fx(x:;). As an
</p>
<p>example, if X '" expfX), then
</p>
<p>Fx(x) = { 1 - exp( -AX) x ~ 0
o x &lt; O.
</p>
<p>For all x except x = 0 (at which the CDF does not have a derivative due to the
</p>
<p>change in slope as seen in Figure 10.15) we have
</p>
<p>px(x) = dFx(x) = 0
dx
</p>
<p>Aexp( - AX)
</p>
<p>and as remarked earlier, px(O) can be assigned any value.
</p>
<p>x&lt;O
</p>
<p>x&gt;O</p>
<p/>
</div>
<div class="page"><p/>
<p>10.7. TRANSFORMATIONS
</p>
<p>10 .7 Transform at ions
</p>
<p>311
</p>
<p>(10.27)
</p>
<p>In discussing transformations for discrete random variables we noted that a trans-
</p>
<p>formation can be either one-to-one or many-to-one. For example, the function
</p>
<p>g(x) = 2x is one-to-one while g(x) = x2 is many-to-one (in this case two-to-one
since - x and + x both map into x2 ) . The determination of the PDF of Y = g(X)
will depend upon which type of transformation we have. Initially, we will consider
</p>
<p>the one-to-one case , which is simpler. For the transformation of a discrete random
</p>
<p>vari able we saw from (5.9) that the PMF of Y = g(X) for any 9 could be found
</p>
<p>from the PMF of X using
</p>
<p>pY[Yi] = L pX[Xj].
{j :g(Xj)=Yi}
</p>
<p>But if 9 is one-to-one we have only a single solution for g( Xj) = Yi, so that Xj =
g-l (Yi) and therefore
</p>
<p>and we are done. For example, assume X takes on values {1,2} with a PMF px[l]
</p>
<p>and px[2] and we wish to determine the PMF of Y = g(X) = 2X, which is shown
in Figure 10.21. Then from (10.27)
</p>
<p>Y = g(x) = 2x
</p>
<p>x
</p>
<p>Figure 10.21: Transformation of a discrete random variable.
</p>
<p>py[2 ] px [g-1(2)] = px [l ]
</p>
<p>py [4] px[g-1(4)J = px [2J.
</p>
<p>Because we are now dealing with a PDF, which is a dens ity function, and not a
</p>
<p>PMF, which is a probability function, the simple relationship of (10.27) is no longer
</p>
<p>valid . To see what happens instead, consider the problem of determining the PDF
</p>
<p>of Y = 2X, where X "-J U(1,2) . Clearly, Sx = { x : 1 &lt; x &lt; 2} and therefore
Sy = {y : 2 &lt; Y &lt; 4} so that py(y) must be zero outside the interval (2,4) . The
results of a MATLAB computer simulation are shown in Figure 10.22. A total of
</p>
<p>50 realizations were obtained for X and Y. The generated X outcomes are shown
</p>
<p>on the x-axis and the resultant Y outcomes obtained from Y = 2x are shown on the</p>
<p/>
</div>
<div class="page"><p/>
<p>312 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>..... : : : .3 .
</p>
<p>4,....----.,.-----r-----r-- - - --,
</p>
<p>432
</p>
<p>OL.----....... -'-- -L- ----'
</p>
<p>o
</p>
<p>: I
~2 .
</p>
<p>I
</p>
<p>I
</p>
<p>I
</p>
<p>I
1 : &middot;1&middot;&middot; &middot; &middot; &middot;&middot;&middot; &middot;&middot; &middot; &middot;&middot; &middot;&middot;&middot; . l &bull; .&bull;&bull;&bull; &bull;&bull;:..-._--
</p>
<p>I
</p>
<p>I
</p>
<p>I
</p>
<p>I
</p>
<p>1
</p>
<p>x
</p>
<p>Figure 10.22: Computer generated realizations of X and Y = 2X for X ~ U(l , 2).
A 50% expanded version of the realizations is shown to the right.
</p>
<p>y-axis. Also , a 50% expanded version of the points is shown to the right. It is seen
</p>
<p>that the density of points on the y-axis is less than that on the x-axis. After some
</p>
<p>t hought the reader will realize that this is the result of the scaling by a factor of 2
</p>
<p>due to the transformation. Since the P DF is probability per unit length, we should
</p>
<p>expect py = trx /2 for 2 &lt; y &lt; 4. To prove that this is so, we note that a small
interval on the z-axis, say [xo - b.x/2, Xo+b.x/2], will map into [2xo - b.x, 2xo+b.x]
on the y-axis. However , the intervals are equivalent events and so their probabilities
</p>
<p>must be equal. It follows then that
</p>
<p>l
XO+ A X/ 2 12xo+AX
</p>
<p>px(x)dx = py(y)dy
xo- Ax/ 2 2xo-Ax
</p>
<p>and as b.x -+ 0, we have that px(x) -+ px(xo) and py(y) -+ py(2xo) in the small
intervals so that
</p>
<p>px(xo)b.x = py(2xo)2b.x
</p>
<p>or</p>
<p/>
</div>
<div class="page"><p/>
<p>10.7. TRANSFORMATIONS 313
</p>
<p>As expected, the PDF of Y is scaled by 1/2. If we now let Yo = 2xo, then this
becomes
</p>
<p>or for any arbitrary value of y
</p>
<p>1
py(y) = px(y/2) "2 2 &lt; y &lt; 4. (10.28)
</p>
<p>This results in the final PDF using px(x) = 1 for 1 &lt; x &lt; 2 as
</p>
<p>y ={ ~ 2&lt;y&lt;4
p (y) 0 otherwise (10.29)
</p>
<p>and thus if X f'V U(l , 2), then Y = 2X f'V U(2, 4). The general result for the PDF of
</p>
<p>Y = g(X) is given by
</p>
<p>(10.30)
</p>
<p>For our example, the use of (10.30) with g(x) = 2x and therefore g-1(y) = y/2
results in (10.29). The absolute value is needed to allow for the case when 9 is
</p>
<p>decreasing and hence g-1 is decreasing since otherwise the scaling term would be
</p>
<p>negative (see Problem 10.57). A formal derivation is given in Appendix lOA. Note
</p>
<p>the similarity of (10.30) to (10.27). The principal difference is the presence of the
</p>
<p>derivative or Jacobian factor dg- 1(y)/dy. It is needed to account for the change in
</p>
<p>scaling due to the mapping of a given length interval into an interval of a different
</p>
<p>length as illustrated in Figure 10.22. Some examples of the use of (10.30) follow.
</p>
<p>Example 10.5 - PDF for linear (actually an affine) transformation
</p>
<p>To determine the PDF of Y = aX + b, for a and b constants first assume that
Sx = {x : -00 &lt; x &lt; oo} and hence Sy = {y : -00 &lt; y &lt; oo}. Here we have
g(x) = ax + b so that the inverse function g-1 is found by solving y = ax + b for x.
This yields x = (y - b) / a so that
</p>
<p>and from (10.30) the general result is
</p>
<p>py(y) = pX (y : b) I~ I&middot; (10.31)
</p>
<p>As a further example, consider X f'V N(O, 1) and the transformation Y = ~X +J.1..</p>
<p/>
</div>
<div class="page"><p/>
<p>314 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>Then, letting a = .jdi &gt; 0 we have
</p>
<p>and therefore Y '" N(J.L, a 2 ) . A linear transformation of a Gaussian random vari -
</p>
<p>able results in another Gaussian random variable whose Gaussian PDF has dif-
</p>
<p>ferent values of the parameters. Because of this property we can easily gener-
</p>
<p>ate a realization of a N(J.L, a 2 ) random variable using the MATLAB construction
</p>
<p>y=sqrt Csigma2) *randn(1, l)+mu, since randn(1, 1) produces a realization of a
</p>
<p>standard normal random variable (see Problem 10.60).
</p>
<p>Example 10.6 - PDF of Y = exp(X) for X '" N(O, 1)
</p>
<p>Here we have that Sy = {y : y &gt; O}. To find g-l(y) we let y = exp(x) and solve
for x, which is x = In(y). Thus, g-l(y) = In(y) . From (10.30) it follows that
</p>
<p>( ) = (1 ( )) Idln(y) I= {px(ln(y))i y &gt; 0py y pX n Y d 0 &lt; 0
Y y-
</p>
<p>or
</p>
<p>()
{
</p>
<p>'F
l exp [-~(ln(y))2] y &gt; 0
</p>
<p>py y = V~7[Y
o y::;O.
</p>
<p>This PDF is called the log-normal PDF. It is frequently used as a model for a
</p>
<p>quantity that is measured in decibels (dB) and which has a normal PDF in dB
</p>
<p>quantities [Members of Technical Staff 1970].
</p>
<p>&amp;. Always determine the possible values for Y before using (10.30).
A common error in determining the PDF of a transformed random variable is
</p>
<p>to forget that py (y) may be zero over some regions. In the previous example of
</p>
<p>y = exp(x), the mapping of -00 &lt; x &lt; 00 is into y &gt; O. Hence, the PDF ofY must
be zero for y ::; 0 since there are no values of X that produce a zero or negative</p>
<p/>
</div>
<div class="page"><p/>
<p>10.7. TRANSFORMATIONS 315
</p>
<p>value of Y. Nonsensical results occur if we attempt to insert values in py(y) for
</p>
<p>y ~ O. To avoid this potential problem, we should first determine Sy and then use
</p>
<p>(10.30) to find the PDF over the sample space.
</p>
<p>When the transformation is not one-to-one, we will have multiple solutions for x in
</p>
<p>y = g(x). An example is for y = x2 for which the solutions are
</p>
<p>Xl -.,fY = gl l (y)
</p>
<p>x2 +.,fY = g2"l(y).
</p>
<p>This is shown in Figure 10.23. In this case we use (10.30) but must add the PDFs
</p>
<p>g(x) = x2
</p>
<p>y
</p>
<p>Xl =--IV
</p>
<p>Figure 10.23: Solutions for X in y = g(x) = x 2.
</p>
<p>(since both the x-intervals map into the same y-interval and the x-intervals are
</p>
<p>disjoint) to yield
</p>
<p>(10.32)
</p>
<p>Example 10.7- PDF of Y = X 2 for X"" N(O, 1)
</p>
<p>Since -00 &lt; X &lt; 00, we must have Y ~ O. Next because gl l (y) = --IV and
g2"l(y) = -IV we have from (10.32)</p>
<p/>
</div>
<div class="page"><p/>
<p>316 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>which reduces to
</p>
<p>py(y) = {~},;; exp( -y/2)] 2F. + [},;; exp(-y/2)] 2F.
</p>
<p>= { k exp( -y/2) u e: 0
o y &lt; O.
</p>
<p>y:2:0
</p>
<p>y&lt;O
</p>
<p>This is shown in Figure 10.24 and should be compared to Figure 2.10 in which this
</p>
<p>PDF was estimated (see also Problem 10.59). Note that the PDF is undefined at
</p>
<p>0.8,---,---.-----,----,.,---,.-----.-----,-----,
</p>
<p>0.7 '" .
</p>
<p>0.6 . . .
</p>
<p>432
oL.-_-'--_...L...-_....L.-_--'--_-'-_----'-_~_---'
-4 -3 -2 -1 o
</p>
<p>y
</p>
<p>Figure 10.24: PDF for Y = X 2 for X rv N(o,1).
</p>
<p>0.2 . .. . .
</p>
<p>0.1 .
</p>
<p>:&sect;O.s ..
</p>
<p>. .
0.3 : : : . .
</p>
<p>. .
</p>
<p>&gt;. .
R.OA : .
</p>
<p>y = 0 since py(O) -7 00, although the total area under the PDF is finite and of
course is equal to 1. Also, Y rv X ~ as can be seen by referring to (10.12) with
</p>
<p>N=1.
</p>
<p>In general, ify = g(x) has solutions Xi = g;l(y) for i = 1,2, ... ,M, then
</p>
<p>(10.33)
</p>
<p>An alternative means of finding the PDF of a transformed random variable is to first
</p>
<p>find the CDF and then differentiate it (see (10.26)) . We illustrate this approach by
</p>
<p>redoing the previous example.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.8. MIXED RANDOM VARIABLES
</p>
<p>Example 10.8 - CDF approach to determine PDF of Y = X 2 for
X rv N(O , 1)
</p>
<p>First we determine the CDF of Y in terms of the CDF for X as
</p>
<p>317
</p>
<p>Fy(y) pry :::; y]
</p>
<p>P[X2 :::;y]
</p>
<p>P[-v'Y :::; X :::; JY]
</p>
<p>= Fx(v'Y) - Fx(-v'Y).
</p>
<p>Then , differentiating we have
</p>
<p>(from (10.25))
</p>
<p>py(y) =
dFy(y)
</p>
<p>dy
</p>
<p>d
dy [Fx(v'Y) - Fx(-v'Y)]
</p>
<p>dyIY d(-ylY)
= px(v'Y) dy - px(-v'Y) dy (from (10.25) and chain rule of calculus)
</p>
<p>1 1
px(v'Y) 2yIY + px(-v'Y) 2yIY
</p>
<p>{
px(yIY) Jy y ~ 0 (since px(- x) = px (x) for X rv N(O , 1))
o y&lt;O
</p>
<p>{
</p>
<p>k exp(-y/2) y ~ 0
</p>
<p>o y &lt; O.
</p>
<p>10. 8 Mixed R an dom Variables
</p>
<p>We have so far described two types of random variables, the discrete random vari-
</p>
<p>able and the continuous random variable. The sample space for a discrete random
</p>
<p>variable consists of a countable (either finite or infinite) set of points while that for a
</p>
<p>cont inuous random variable has an infinite and uncountable set of points. The points
</p>
<p>in Sx for a discrete random variable have a nonzero probability while those for a
</p>
<p>cont inuous random variable have a zero probability. In some physical situations,
</p>
<p>however , we wish to assign a nonzero probability to some points but not others. As
</p>
<p>an example, consider an experiment in which a fair coin is tossed. If it comes up
</p>
<p>heads, we generate the outcome of a continuous random variable X rv N(O, 1) and
</p>
<p>if it comes up tails we set X = O. Then, the possible outcomes are - 00 &lt; x &lt; 00
and the probability of any point except x = 0 has a zero probability of occurring.
</p>
<p>However , the point x = 0 occurs with a probability of 1/2 since the probability of
</p>
<p>a tail is 1/2. A typical sequence of outcomes is shown in Figure 10.25. One could</p>
<p/>
</div>
<div class="page"><p/>
<p>318 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>3 .-------.-----,---,.---...,---------n
</p>
<p>2 . .
</p>
<p>s
s
~ 0
o
</p>
<p>-1
</p>
<p>-2 .
</p>
<p>504020 30
Trial number
</p>
<p>10
</p>
<p>-3 '---__--'-__---1. -'--__----'--__-----'-'
</p>
<p>o
</p>
<p>Figure 10.25: Sequence of outcomes for mixed random variable - X = 0 with nonzero
</p>
<p>probability.
</p>
<p>define a random variable as
</p>
<p>X N(O, 1)
</p>
<p>X = 0
</p>
<p>if heads
</p>
<p>if tails
</p>
<p>which is neither a discrete nor a continuous random variable. To find its CDF we
</p>
<p>use the law of total probability to yield
</p>
<p>Fx(x) = P[X ~ x]
</p>
<p>= P[X ~ x lheads]P[heads] + P[X ~ x ltails]P [tails]
</p>
<p>{
</p>
<p>&lt;I&gt;(x)~ + O(~) x &lt; 0
</p>
<p>&lt;I&gt;(x)~ + 1(~) x;::: 0
</p>
<p>which can be written more succinctly using the unit step function. The unit step
</p>
<p>function is defined as u(x) = 1 for x;::: 0 and u(x) = 0 for x &lt; O. With this definition
the CDF becomes
</p>
<p>1 1
Fx(x) = 2&lt;I&gt;(x) + 2u(x) - 00 &lt; x &lt; 00.
</p>
<p>The CDF is shown in Figure 10.26. Note the jump at x = 0, indicative of the
</p>
<p>contribution of the discrete part of the random variable. The CDF is continuous for
</p>
<p>all x i= 0 but has ajump at x = 0 of 1/2. It corresponds to neither a discrete random
variable, whose CDF consists only of jumps, nor a continuous random variable,
</p>
<p>whose CDF is continuous everywhere. Hence , it is called a mixed random variable.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.8. MIXED RANDOM VARIABLES 319
</p>
<p>0.4 . . ..
</p>
<p>1 .
</p>
<p>0.8 . . ..
</p>
<p>~ I
'-' I :
~0 .6 &middot; &middot;&middot;&middot;&middot;&middot;&middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; _ &middot; &middot; &middot; &middot; &middot;&middot;&middot; &middot;i &middot; &middot; &middot;&middot; &middot; &middot; &middot; &middot; &middot; &middot; : &middot; &middot; &middot;&middot; &middot; &middot; &middot; &middot; &middot; &middot; .. .
</p>
<p>I
</p>
<p>I
............ ..i ..
</p>
<p>I
</p>
<p>32
</p>
<p>0.25
</p>
<p>o
X
</p>
<p>-1-2
</p>
<p>oL...-_...-i==----i__----'-__---'-__.....L_-l
-3
</p>
<p>0.2 . .
</p>
<p>Figure 10.26: CDF for mixed random variable.
</p>
<p>Its CDF is in general continuous except for a countable number of jumps (either
</p>
<p>finite or infinite). As usual it is right-continuous at the jump.
</p>
<p>Strictly speaking, a mixed random variable does not have a PMF or a PDF.
</p>
<p>However , by the use of the Dirac delta function (also called an impulse), we can
</p>
<p>define a PDF which may then be used to find the probability of an interval via
</p>
<p>integration by using (lOA). To first find the PDF we attempt to differentiate the
</p>
<p>CDF
</p>
<p>d [1 1]px(x) = dx "2&lt;P(x) + "2u(x) .
The difficulty encountered is that u(x) is discontinuous at x = 0 and thus formally its
derivative does not exist there. We can, however, define a derivative for the purposes
</p>
<p>of probability calculations as well as for conceptualization. To do so requires the
</p>
<p>introduction of the Dirac delta function 8(x) which is defined as (see also Appendix
</p>
<p>D)
</p>
<p>8(x) = d~~X).
</p>
<p>The function 8(x) is usually thought of as a very narrow pulse with a very large
</p>
<p>amplitude which is centered at x = O. It has the property that 8(t) = 0 for all t =1= 0
but
</p>
<p>iff 8(t)dt = 1
for &euro; a small positive number. Hence , the area under the narrow pulse is one. Using
</p>
<p>this definition we can now differentiate the CDF to find that
</p>
<p>1 1 (1 2) 1px(x) = -- exp --x + -8(x)
2V27f" 2 2
</p>
<p>(10.34)</p>
<p/>
</div>
<div class="page"><p/>
<p>320 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>which is shown in Figure 10.27. This may be thought of as a generalized PDF. Note
</p>
<p>03 ,------,-----,------,------,-----,------,
</p>
<p>0.25 ~ ..
</p>
<p>32o
x
</p>
<p>-1-2
</p>
<p>o==_----'__----'-__----L.__-'-__-'-_--=
-3
</p>
<p>0.05
</p>
<p>02 . ..
</p>
<p>~
~
~0.15
</p>
<p>0.1 ..
</p>
<p>Figure 10.27: PDF for mixed random variable.
</p>
<p>that it is the strength, which is defined as the area under the approximating narrow
</p>
<p>pulse, that is equal to 1/2. The amplitude is theoretically infinite. The CDF can
</p>
<p>be recovered using (10.16) and the result that
</p>
<p>j
X+
</p>
<p>u(x) = - 00 8(t)dt
</p>
<p>where x+ means that the integration interval is (-00, x + &euro;] for &euro; a small positive
number. Thus, the impulse should be included in the integration interval if x = 0
</p>
<p>so that u(O) = 1 according to the definition of the unit step function.
</p>
<p>&amp; When do we include the impulse in the integration interval?
For a mixed random variable the presence of impulses in the PDF requires a mod-
</p>
<p>ification to (10.4). This is because an endpoint of the interval can have a nonzero
</p>
<p>probability. As a result , the probabilities prO &lt; X &lt; 1] and pro ~ X &lt; 1] will be
different if there is an impulse at x = O. Specifically, consider the computation of
</p>
<p>prO ~ X &lt; 1] and note that the probability of X = 0 should be included. Therefore,
if there is an impulse at x = 0, the area under the PDF should include the contri-
</p>
<p>bution of the impulse. Thus, the integration interval should be chosen as [0-, 1] so
that
</p>
<p>prO ~ X &lt; 1] = 1 ~ px(x)dx.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.8. MIXED RANDOM VARIABLES
</p>
<p>The more general modifications to (lOA) are
</p>
<p>l
b+
</p>
<p>P[a ::; X ::; b] a- px(x)dx
</p>
<p>l
b+
</p>
<p>P[a &lt; X ::; b] px(x)dx
a+
</p>
<p>b-
</p>
<p>P[a ::; X &lt; b] 1- px(x)dx
b-
</p>
<p>P[a &lt; X &lt; b] = 1 px(x)dx
a+
</p>
<p>321
</p>
<p>where x" is a number slightly less than x and x+ is a number slightly greater than
</p>
<p>x. Of course, if the PDF does not have any impulses at x = a or x = b, then all the
integrals above will be the same and, therefore there is no need to choose between
</p>
<p>them. See also Problem 10.51.
</p>
<p>L1h
Continuing with our example, let's say we wish to determine P[- 2 ::; X ::; 2]. Then,
</p>
<p>using (lOA) since the impulse does not occur at one of the interval endpoints, and
</p>
<p>our generalized PDF of (10.34) yields
</p>
<p>P[-2 ::; X ::; 2] = i: px(x)dx
= 12 [~_1 exp (_~x2) + ~8(X)] dx
</p>
<p>-2 2 ~ 2 2
</p>
<p>~ 12 _1_ exp (_~x2) dx + ~ 12 8(x)dx
2 -2 ~ 2 2 -2
1 1
</p>
<p>= 2 [Q(-2) - Q(2)] + 2
1 1
2 [1 - 2Q(2)] + 2 = 1 - Q(2).
</p>
<p>Alternatively, we could have obtained this result using P[-2 ::; X ::; 2] = Fx(2) -
</p>
<p>Fx( -2) with Fx(x) = (1/2)(1 - Q(x)) + (1/2)u(x).
Mixed random variables often arise as a result of a transformation of a continuous
</p>
<p>random variable. A final example follows.
</p>
<p>Example 10.9 - PDF for amplitude-limited Rayleigh random variable
</p>
<p>Consider a Rayleigh random variable whose PDF is given by (10.14) that is input
</p>
<p>to a device that limits its output. One might envision a physical quantity such as
</p>
<p>temperature and the device being a thermometer which can only read temperatures
</p>
<p>up to a maximum value. All temperatures above this maximum value are read as the</p>
<p/>
</div>
<div class="page"><p/>
<p>322 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>X max
</p>
<p>maximum. Then the effect of the device can be represented by the transformation
</p>
<p>()
{
</p>
<p>X 0 ~ x &lt; X max
y =g X =
</p>
<p>X max X ~ X max
</p>
<p>which is shown in Figure 10.28. The PDF of Y is zero for y &lt; 0 since X can only
</p>
<p>y =g(x)
</p>
<p>region 3 t..
region 2
</p>
<p>--'---f'-----+-----..X
</p>
<p>region 1 l
Figure 10.28: Amplitude limiter.
</p>
<p>take on nonnegative values. For 0 ~ y &lt; X max it is seen from Figure 10.28 that
g-l(y) = y. Finally, for y ~ X max we have from Figure 10.28 the infinite number of
</p>
<p>solutions x E [Xmax , 00). Thus, we have for region 1 or for y &lt; 0 that py(y) = O.
For region 2 or for 0 ~ y &lt; X max where g-l(y) = y, we have from (10.30)
</p>
<p>py(y) = PX(g-l(y)) Id9~~(Y) I
</p>
<p>px(y).
</p>
<p>For region 3 which is y ~ Xmax , we note that Y cannot exceed Xmax and so y = Xmax
is the only possible value for y in region 3. The probability of Y = X max is equal to
the probability that X ~ xmax . In particular, it is
</p>
<p>pry = X max ] = r
XJ
</p>
<p>px(x)dx
i.:
</p>
<p>(10.35)
</p>
<p>since from Figure 10.28 the x-interval [xmax , 00) is mapped into the y-point given by
y = X max . Since the probability of Y at the point y = X max is nonzero, we represent
its contribution to the PDF by using an impulse as
</p>
<p>py(y) = [l:xpx(X)dX] 8(y - xmax ) y = X max .</p>
<p/>
</div>
<div class="page"><p/>
<p>10.8. MIXED RANDOM VARIABLES 323
</p>
<p>In summary, the PDF of the transformed random variable is
</p>
<p>pY (y) =
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>0 y &lt; 0
pX(y) 0 &le; y &lt; xmax
[
</p>
<p>&int;&infin;
xmax
</p>
<p>pX(x)dx
]
</p>
<p>(y &minus; xmax) y = xmax
0 y &gt; xmax .
</p>
<p>It is seen to be the PDF of a mixed random variable in that it contains an impulse.
Finally, for x &ge; 0 the Rayleigh PDF is for 2 = 1
</p>
<p>pX(x) = x exp
</p>
<p>(
</p>
<p>&minus;1
2
x2
</p>
<p>)
</p>
<p>so that the PDF of Y becomes
</p>
<p>pY (y) =
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>0 y &lt; 0
y exp
</p>
<p>(
</p>
<p>&minus;12y2
)
</p>
<p>0 &le; y &lt; xmax
[
</p>
<p>&int;&infin;
xmax
</p>
<p>x exp
(
</p>
<p>&minus;12x2
)
</p>
<p>dx
]
</p>
<p>(y &minus; xmax) y = xmax
0 y &gt; xmax .
</p>
<p>=
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>0 y &lt; 0
y exp
</p>
<p>(
</p>
<p>&minus;12y2
)
</p>
<p>0 &le; y &lt; xmax
exp
</p>
<p>(
</p>
<p>&minus;12x2max
)
</p>
<p>(y &minus; xmax) y = xmax
0 y &gt; xmax.
</p>
<p>This is plotted in Figure 10.29b.
</p>
<p>0 1 2 3 4
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>x
</p>
<p>p
X
(x
)
</p>
<p>xmax
</p>
<p>area =exp(&minus;(1/2)x2
max
</p>
<p>)
</p>
<p>(a) PDF of X &ndash; continuous random variable
</p>
<p>0 1 2 3 4
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>y
</p>
<p>p
Y
(y
)
</p>
<p>xmax
</p>
<p>strength = exp(&minus;(1/2)x2
max
</p>
<p>)
</p>
<p>(b) PDF of Y = g(X) &ndash; mixed random variable
</p>
<p>Figure 10.29: PDFs before and after transformation of Figure 10.28.
</p>
<p>&diams;</p>
<p/>
</div>
<div class="page"><p/>
<p>324 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>In general , if a random variable X can take on a continuum of values as well
</p>
<p>as discrete values {Xl ,X2" '} with corresponding nonzero probabilities {Pl,p2," .},
then the PDF of the mixed random variable X can be wri t ten in the succinct form
</p>
<p>00
</p>
<p>px(x ) = Pc(x) + LPi8(X - xd
i=l
</p>
<p>(10.36)
</p>
<p>where Pc(x) rep resents the cont ribution to the PDF of the cont inuous par t (its
</p>
<p>int egral must be &lt; 1) and must sat isfy Pc(x) ~ O. To be a valid PDF we require
that
</p>
<p>1
00 00
</p>
<p>- 00 Pc(x)dx + ~P i = 1.
</p>
<p>For solely discrete random variables we can use the generalized PDF
</p>
<p>00
</p>
<p>px(x) = LPi8(X - Xi )
i= l
</p>
<p>or equivalently the PMF
</p>
<p>i = 1,2, . . .
</p>
<p>to perform probability calculations .
</p>
<p>10.9 Computer Simulation
</p>
<p>In simulating the outcome of a discrete random variable X we saw in Figure 5.14 that
</p>
<p>first an outcome of a U rv U(O, 1) random variable is generated and then mapped
</p>
<p>into a value of X . The mapping needed was the inverse of the CDF. This result
</p>
<p>is also valid for a continuous random variable so that X = Fi l (U) is a random
</p>
<p>variable with CDF Fx(x) . Stat ed another way, we have that U = Fx(X) or if
</p>
<p>a random variable is transformed according to its CDF, the transformed random
</p>
<p>variable U rv U(O, 1). This latter transformation is termed the probability integral
</p>
<p>transformation. The transformation X = Fi l (U) is called the inverse probability
</p>
<p>in tegral transformation. Before proving these results we give an example.
</p>
<p>Example 10 .10 - Probability integral transformation of exponential ran-
</p>
<p>dom variable
</p>
<p>Since the exponent ial PDF is given for A = 1 by
</p>
<p>( )
_ { exp( - x) x ~ 0
</p>
<p>ti x x - 0 x &lt; 0
</p>
<p>the CDF is from (10.16)
</p>
<p>{
0
</p>
<p>Fx x -
( ) - 1 - exp(- x)
</p>
<p>x ~ O
</p>
<p>x &gt; O.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.9. COMPUTER SIMULATION 325
</p>
<p>The probability integral transformation asserts that Y = g(X) = Fx(X) has a
U(O, 1) PDF. Considering the transformation g(x) = 1- exp(-x) for x &gt; 0 and zero
otherwise, we have that y = 1 - exp(-x) and, therefore the unique solution for x is
</p>
<p>x = -In(l - y) for 0 &lt; Y &lt; 1 and zero otherwise. Hence,
</p>
<p>g-l(y) = { o-ln(l - y) 0 &lt; Y &lt; 1
otherwise
</p>
<p>and using (10.30), we have for 0 &lt; Y &lt; 1
</p>
<p>py(y) = PX(g-l(y&raquo;) I d 9 ~ ~ ( Y ) I
</p>
<p>exp [- (-In(l _ y)] 1_1_1
1-y
</p>
<p>1.
</p>
<p>Finally, then
</p>
<p>{
1 O&lt;y&lt;l
</p>
<p>py (y) = 0 otherwise
</p>
<p>which is the PDF of a U(O, 1) random variable.
</p>
<p>To summarize our results we have the following theorem.
</p>
<p>Theorem 10.9.1 (Inverse Probability Integral Transformation) If a contin-
</p>
<p>uous random variable X is given as X = FX
1(U), where U rv U(O, 1), then X has
</p>
<p>the PDF px(x) = dFx(x)/dx.
</p>
<p>Proof:
</p>
<p>Let V = FX
1(U) and consider the CDF of V.
</p>
<p>Fv(v) P[V ::; v] = P[FX
1(U) ::; v]
</p>
<p>P[U::; Fx(v)] (Fx is monotonically increasing - see Problem 10.58)
</p>
<p>rFx(v)
</p>
<p>io pu(u)du
</p>
<p>l
FX (V)
</p>
<p>1du
o
</p>
<p>Fx(v).
</p>
<p>Hence, the CDFs of V and X are equal and therefore the PDF of V = FX
1(U) is
</p>
<p>px(x).
</p>
<p>Another example follows.</p>
<p/>
</div>
<div class="page"><p/>
<p>326 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>Example 10.11 - Computer generation of outcome of Laplacian random
</p>
<p>variable
</p>
<p>The Laplacian random variable has a PDF
</p>
<p>-oo&lt;x&lt;oo
</p>
<p>and therefore its CDF is found as
</p>
<p>Fx(x) = (X ~ exp [- n:1tl] dt.J-00 v 20-2 V-;;2
For x &lt; 0 we have
</p>
<p>Fx(x)
</p>
<p>and for x ~ 0 we have
</p>
<p>Fx(x) = {a ~ exp [ n:t ]&middot;dt + {X ~ exp [- n:t ] dt
J-00 v 20-2 V-;;2 Ja v 20-2 V-;;2
</p>
<p>~ ~ - ~ exp [-[!;t]: (first integral is 1/2 since px(-x) ~ px(x))
</p>
<p>1- ~ e x p [-[!;x]
</p>
<p>By letting y = Fx(x), we have
</p>
<p>x&lt;O
</p>
<p>x ~ O.
</p>
<p>We note that if x &lt; 0, then 0 &lt; y &lt; 1/2, and if x ~ 0, then 1/2 ::; y &lt; 1. Thus,
solving for x to produce Fi 1(y) yields
</p>
<p>{
J0-2 /2In(2y) 0 &lt; y &lt; 1/2
</p>
<p>x = J0-2/21n ( 2 ( 1 ~ Y ) ) 1/2::; Y &lt; 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.9. COMPUTER SIMULATION 327
</p>
<p>Finally to generate the outcome of a Laplacian random variable we can use
</p>
<p>{
</p>
<p>J(J2/2In(2u) 0 &lt; u &lt; 1/2
</p>
<p>x = J(J2/2In ( 2 ( 1 ~ U ) ) 1/2 ~ u &lt; 1
(10.37)
</p>
<p>where u is a realization of a U(O, 1) random variable. An example of the outcomes
</p>
<p>of a Laplacian random variable with (J2 = 1 is shown in Figure 1O.30a. In Figure
</p>
<p>10.30b the true PDF (the solid curve) along with the estimated PDF (the bar plot) is
</p>
<p>shown based on M = 1000 outcomes. The estimate of the PDF was accomplished by
</p>
<p>2 3 4
</p>
<p>. . . ~ ' " - .&middot; . . .&middot; . . .&middot; . . .&middot; . . .&middot; . . .&middot; . . .&middot; . . .&middot; . . .
</p>
<p>-4 -3 -2 -1 0
X
</p>
<p>~ 0.8 .-- -.-- --,----.--.----.-~-,.- -, ~--,
</p>
<p>o
p...
</p>
<p>E! 0.6 .. .. ~ ....:.. .. ~ .... ~ .. . .. . .. ~ .. .. :.. .. ~ .... ~ . .. .
~
</p>
<p>"0 .
~ .
</p>
<p>"0 0.4 . .. .: . ...; : : ..
</p>
<p>Q}
~
</p>
<p>.~ 0.2 . . ) .. .. :... . ~ .... :
~
</p>
<p>&middot; . . . .&middot; . . . .
4 : ~ ~ : :
</p>
<p>3 : ; ~ : :
</p>
<p>5.------.---.,....- - -..---~ -_..,
</p>
<p>&middot; . . . .
Q} 2 ~ ; : : :
</p>
<p>J=~ II~tl&middot;lel.n}tl1u&middot;~f.1~T.!'Y.t., .&bull;1
- 3 : : : : :
</p>
<p>&middot; . . . .
- 4 ;. .; ( .; :
</p>
<p>- 5 L-_--i....__ - ' - - _ ~ __"""--_-----..J
</p>
<p>o 10 20 30 40 50
Trial number
</p>
<p>(a) First 50 outcomes (b) True PDF and estimated PDF based
</p>
<p>on 1000 outcomes
</p>
<p>Figure 10.30: Computer generation of Laplacian random variable outcomes using
</p>
<p>inverse probability integral transformation.
</p>
<p>the procedure described in Example 2.1 (see Figure 2.7 for the code for a Gaussian
</p>
<p>PDF). We can now justify that procedure. Since from Section 10.3 we have
</p>
<p>( )
P[XQ - !::i.x/2 ~ X ~ XQ + !::i.x/2]
</p>
<p>PX XQ ~ !::i.x
</p>
<p>and
</p>
<p>P[xQ _ !::i.x/2 ~ X ~ XQ + !::i.x/2] ~ Number of outcomes i~XO - tlx/2, Xo + tlx/2)
</p>
<p>we use as our PDF estimator
</p>
<p>~ ( ) _ Number of outcomes in [xo - tlx/2 ,xo + tlx/2)
PX XQ - M!::i.x . (10.38)
</p>
<p>In Figure 10.30b we have chosen the bins or intervals to be [-4.25, -3.75]' [-3.75, -3.25],
</p>
<p>... , [3.75,4.25] so that Az = 0.5. We have therefore estimated pv]-4) ,px(-3.5) , ... ,</p>
<p/>
</div>
<div class="page"><p/>
<p>328 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>px (4). To est imate the PDF at more points we would have to decrease the binwidth
</p>
<p>or b.x. However , in doing so we cannot make it too small. This is because as the
</p>
<p>binwidth decreases, the probability of an outcome falling within the bin also de-
</p>
<p>creases. As a result , fewer of the outcomes will occur within each bin, resulting in
</p>
<p>a poor est imate. The only way to remedy this situation is to increase the number
</p>
<p>of trials M. What do you suppose would happen if we wanted to est imate px(5)?
</p>
<p>The MATLAB code for producing the PDF est imate is given below.
</p>
<p>Yo Assume outcomes are in x, which is M x 1 vector
</p>
<p>M=1000j
</p>
<p>bincenters=[-4:0.5:4]'j Yo set binwidth = 0 .5
</p>
<p>bins=length(bincenters)j
</p>
<p>h=zeros(bins,l)j
</p>
<p>for i=1:1ength(x) Yo count outcomes in each bin
</p>
<p>for k=l:bins
</p>
<p>if x(i&raquo;bincenters(k)-0 .5/2.
</p>
<p>&amp; x(i)&lt;bincenters(k)+0.5/2
h(k,l)=h(k,l)+lj
</p>
<p>end
</p>
<p>end
</p>
<p>end
</p>
<p>pxest=h/(M*0 .5)j i. see (10.38)
</p>
<p>The CDF can be estimated by using
</p>
<p>Fx(x ) = Number of outco mes ~ x
</p>
<p>M
(10.39)
</p>
<p>and is the same for either a discrete or a continuous random var iable. See also
</p>
<p>P roblems 10.60-62.
</p>
<p>10.10 Real-World Example - Setting Clipping Levels for
</p>
<p>Speech Signals
</p>
<p>In order to communicate speech over a transmission channel it is important to make
</p>
<p>sure that the equipment does not "clip" the speech signal. Commercial broadcast
</p>
<p>stations commonly use VU meters to monitor the power of the speech. If the power
</p>
<p>becomes too large, then the amplifier gains are manually decreased. Clipped speech
</p>
<p>sounds distorted and is objectionable. In other sit uat ions, the amplifier gains must
</p>
<p>be set automatically, as for example, in telephone speech transmission. This is
</p>
<p>necessary so that the speech, if transmitted in an analog form, is not distorted at
</p>
<p>the receiver, and if transmitted in a digital form is not clipped by an analog-to-
</p>
<p>digital convertor. To determine the highest amplit ude of the speech signal that can</p>
<p/>
</div>
<div class="page"><p/>
<p>10.10. REAL-WORLD EXAMPLE - SETTING CLIPPING LEVELS 329
</p>
<p>be expected to occur a common model is to use a Laplacian PDF for the amplitudes
</p>
<p>[Rabiner and Schafer 1978]. Hence, most of the amplitudes are near zero but larger
</p>
<p>level ones are possible according to
</p>
<p>- 00 &lt; x &lt; 00.
</p>
<p>(10040)
</p>
<p>As seen in Figure 10.10, the width of the PDF increases as (J2 increases. In effect,
</p>
<p>(J2 measures the width of the PDF and is actually its variance (to be shown in
</p>
<p>Problem 11.34). The parameter (J2 is also a measure of the speech power. In order
</p>
<p>to avoid excessive clipping we must be sure that an amplifier can accommodate a
</p>
<p>high level, even if it occurs rather infrequently. A design requirement might then be
</p>
<p>to transmit a speech signal without clipping 99% of the time. A model for a clipper
</p>
<p>is shown in Figure 10.31. As long as the input signal, i.e., x, remains in the interval
</p>
<p>g(x)
</p>
<p>--+---f------1t----1~ X
</p>
<p>Figure 10.31: Clipper input-output characteristics.
</p>
<p>- A ::; x ::; A, the output will be the same as the input and no clipping takes place.
</p>
<p>However, if x&gt; A, the output will be limited to A and similarly if x &lt; -A. Clipping
will then occur whenever [z] &gt; A. To satisfy the design requirement that clipping
should not occur for 99% of the time, we should choose A (which is a characteristic
</p>
<p>of the amplifier or analog-to-digital convertor) so that Pclip ::; 0.01. But
</p>
<p>Pclip = P[X &gt; A or X &lt; -A]
</p>
<p>and since the Laplacian PDF is symmetric about x = 0 this is just
</p>
<p>Pclip ~ 2P[X &gt; A] ~ 2 /.00 "':&lt;7
2
</p>
<p>exp [-{!;x] dx
</p>
<p>2 Hexp [-{!;x] ]
= exp [-{!;A] .</p>
<p/>
</div>
<div class="page"><p/>
<p>330 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>Hence, if this probability is to be no more than 0.01, we must have
</p>
<p>or solving for A produces the requirement that
</p>
<p>A &gt; [;;2 In (_1).- V2 0.01 (10.41)
</p>
<p>It is seen that as the speech power (T2 increases, so must the clipping level A. If the
</p>
<p>clipping level is fixed, then speech with higher powers will be clipped more often. As
</p>
<p>an example, consider a speech signal with (T2 = 1. The Laplacian model outcomes
are shown in Figure 10.32 along with a clipping level of A = 1. According to (10.40)
</p>
<p>5.----.,..------,---,..----,..--------"
</p>
<p>4 .
</p>
<p>3 .
</p>
<p>2 '.. .. . .. .. . . . ' , '.
</p>
<p>J~~ -.~ r ~ &middot; ! F l j l ! l ~ l i j ~ ~ ~ ' ! I ~ { . ~ . ~ ~
-3 :. .. . . &bull; : , :.
</p>
<p>-4 .. .. .. .
</p>
<p>504010
-5 '--__--'-__--.L -'---__----'--__--J...J
</p>
<p>o
</p>
<p>Figure 10.32: Outcomes of Laplacian random variable with (T2 = 1 - model for
</p>
<p>speech amplitudes.
</p>
<p>the probability of clipping is exp( -J2) = 0.2431. Since there are 50 outcomes in
Figure 10.32 we would expect about 50&middot; 0.2431 ~ 12 instances of clipping. From the
</p>
<p>figure we see that there are exactly 12. To meet the specification we should have
</p>
<p>that
</p>
<p>A ~ Jlj21n ( 0.~1) = 3.25.
</p>
<p>As seen from Figure 10.32 there are no instances of clipping for A = 3.25. In order
to set the appropriate clipping level A, we need to know (T2. In practice, this too
</p>
<p>must be estimated since different speakers have different volumes and even the same
</p>
<p>speaker will exhibit a different volume over time!</p>
<p/>
</div>
<div class="page"><p/>
<p>REFERENCES 331
</p>
<p>References
</p>
<p>Abramowitz, M., I.A. Stegun, Handbook of Mathematical Functions, Dover, New
York, 1965.
</p>
<p>Capinski, M., P.E. Kopp, Measure, Integral, and Probability, Springer-Verlag, New
York, 2004.
</p>
<p>Johnson, N.L., S. Kotz, N. Balakrishnan, Continuous Univariate Distributions,
Vols. 1,2, John Wiley &amp; Sons, New York, 1994.
</p>
<p>Members of Technical Staff, Transmission Systems for Communications, Western
Electric Co., Inc., Winston-Salem, NC, 1970.
</p>
<p>Rabiner, L.R., R.W. Schafer, Digital Processing of Speech Signals, Prentice-Hall,
Englewood Cliffs, NJ, 1978.
</p>
<p>Widder, D.A., Advanced Calculus, Dover, New York, 1989.
</p>
<p>Problems
</p>
<p>10.1 (w) Are the following random variables continuous or discrete?
</p>
<p>a. Temperature in degrees Fahrenheit
</p>
<p>b. Temperature rounded off to nearest 1
</p>
<p>c. Temperature rounded off to nearest 1/2
</p>
<p>d. Temperature rounded off to nearest 1/4
</p>
<p>10.2 (

</p>
<p>. . ) (w) The temperature in degrees Fahrenheit is modeled as a uniform ran-
dom variable with T &sim; U(20, 60). If T is rounded off to the nearest 1/2 to
form T , what is P [T = 30]? What can you say about the use of a PDF versus
a PMF to describe the probabilistic outcome of a physical experiment?
</p>
<p>10.3 (w) A wedge of cheese as shown in Figure 10.5 is sliced from x = a to x = b .
If a = 0 and b = 0.2, what is the mass of cheese in the wedge? How about if
a = 1.8 and b = 2?
</p>
<p>10.4 (

</p>
<p>. . ) (w) Which of the functions shown in Figure 10.33 are valid PDFs? If a
function is not a PDF, why not?
</p>
<p>10.5 (f) Determine the value of c to make the following function a valid PDF
</p>
<p>g(x) =
</p>
<p>{
</p>
<p>c(1 &minus; |x/5|) |x| &lt; 5
0 otherwise.</p>
<p/>
</div>
<div class="page"><p/>
<p>332 CHAPTER 10. CONTIN UOUS RANDOM VARIABLES
</p>
<p>&deg;o'---L----L----'
2
X
</p>
<p>-:
/
</p>
<p>17
</p>
<p>0 .4
</p>
<p>o
o
</p>
<p>0.2
</p>
<p>O.B
,..-.....,
---....0.s
0)
</p>
<p>(a) (b) (c)
</p>
<p>Figure 10.33: Possible PDFs for Problem lOA.
</p>
<p>10.6 L...:J (w) A Gaussian mixture PDF is defined as
</p>
<p>px (x ) = a l ~ exp (-2\ x2) + a2 ~ exp (-2\ x2)
27T(ll (11 27T(l2 (12
</p>
<p>for (If f:. (I ~ . What are the possible values for al and a2 so that this is a valid
PDF?
</p>
<p>10.7 (w) Find the area under the cur ves given by the following functions:
</p>
<p>92(X)
</p>
<p>{
x O:S x&lt;l
</p>
<p>l+ x 1:S x :S2
</p>
<p>0 otherwise
</p>
<p>{
x 0 :S x:S1
</p>
<p>l+ x 1&lt; x:S2
</p>
<p>0 otherwise
</p>
<p>and explain your results.
</p>
<p>10.8 (w) A memory chip has a pr ojected lifetime X in days that is modeled as
X rv exp(O.OOl) . What is the probability that it will fail within one year ?
</p>
<p>10.9 (t) In this problem we prove tha t the Gaussian PDF integrates to one. First
we let
</p>
<p>] = {'Xl _1_ exp (_ ~ x 2) dx
J-00 .../'irr 2
</p>
<p>and write ] 2 as the iterated integral
</p>
<p>] 2 = roo roo _1_ exp (_~ x 2) _1_ exp (_~ y2) dydx .
J-00 J- 00 .../'irr 2.../'irr 2
</p>
<p>Next, convert (x, y) into polar coordinates and evaluate the expression to prove
that ] 2 = 1. Finally, you can conclude tha t ] = 1 (why? ).</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 333
</p>
<p>10.10 (f,c) If X &sim; N (, 2), find P [X &gt; + a] for a = 1, 2, 3, where  =
&radic;
2.
</p>
<p>10.11 (t) The median of a PDF is defined as the point x = med for which P [X &le;
med] = 1/2. Prove that if X &sim; N (, 2), then med = .
</p>
<p>10.12 (

</p>
<p>. . ) (w) A constant or DC current source that outputs 1 amp is connected
to a resistor of nominal resistance of 1 ohm. If the resistance value can vary
according to R &sim; N (1, 0.1), what is the probability that the voltage across
the resistor will be between 0.99 and 1.01 volts?
</p>
<p>10.13 (w) An analog-to-digital convertor can convert voltages in the range [&minus;3, 3]
volts to a digital number. Outside this range, it will &ldquo;clip&rdquo; a positive voltage
at the highest positive level, i.e., +3, or a negative voltage at the most negative
level, i.e., &minus;3. If the input to the convertor is modeled as X &sim; N (, 1), how
should  be chosen to minimize the probability of clipping?
</p>
<p>10.14 (

</p>
<p>. . ) (f) Find P [X &gt; 3] for the two PDFs given by the Gaussian PDF with
 = 0, 2 = 1 and the Laplacian PDF with 2 = 1. Which probability is larger
and why? Plot both PDFs.
</p>
<p>10.15 (f) Verify that the Cauchy PDF given in (10.9) integrates to one.
</p>
<p>10.16 (t) Prove that (z+1) = z(z) by using integration by parts (see Appendix
B and Problem 11.7).
</p>
<p>10.17 (

</p>
<p>. . ) (f) The arrival time in minutes of the Nth person at a ticket counter
has a PDF that is Erlang with  = 0.1. What is the probability that the
first person will arrive within the first 5 minutes of the opening of the ticket
counter? What is the probability that the first two persons will arrive within
the first 5 minutes of opening?
</p>
<p>10.18 (f) A person cuts off a wedge of cheese as shown in Figure 10.5 starting at
x = 0 and ending at some value x = x0. Determine the mass of the wedge as
a function of the value x0. Can you relate this to the CDF?
</p>
<p>10.19 (

</p>
<p>. . ) (f) Determine the CDF for the Cauchy PDF.
</p>
<p>10.20 (f) If X &sim; N (0, 1) find the probability that |X| &le; a, where a = 1, 2, 3. Also,
plot the PDF and shade in the corresponding areas under the PDF.
</p>
<p>10.21 (f,c) If X &sim; N (0, 1), determine the number of outcomes out of 1000 that
you would expect to occur within the interval [1, 2]. Next conduct a computer
simulation to carry out this experiment. How many outcomes actually occur
within this interval?
</p>
<p>10.22 (

</p>
<p>. . ) (w) If X &sim; N (, 2), find the CDF of X in terms of (x).</p>
<p/>
</div>
<div class="page"><p/>
<p>334 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>10.23 (t) If a PDF is symmetric about x = 0 (also called an even function) , prove
that Fx (-x) = 1 - Fx (x). Does this property hold for a Gaussian PDF with
</p>
<p>J.L = O? Hint: See Figure 10.16.
</p>
<p>10.24 (t) Prove that if X rv N(J.L ,u2 ) , then
</p>
<p>P[X &gt; a] = Q ( a : J.L )
</p>
<p>where a = V7li.
</p>
<p>10.25 (t) Prove the properties of the Q function given by (10.19)-(10.22).
</p>
<p>10.26 (f) Plot the function Q(A/2) versus A for 0 :::; A :::; 5 to verify the true
probability of error as shown in Figure 2.15.
</p>
<p>10.27 (c) If X rv N(O,l), evaluate P[X &gt; 4] and then verify your results using
a computer simulation. How easy do you think it would be to determine
</p>
<p>P[X &gt; 7] using a computer simulation? (See Section 11.10 for an alternative
approach.)
</p>
<p>10.28 C...:...) (w) A survey is taken of the incomes of a large number of people in
a city. It is determined that the income in dollars is distributed as X rv
</p>
<p>N(50000,108 ) . What percentage of the people have incomes above $70,000?
</p>
<p>10.29 (w) In Chapter 1 an example was given of the length of time in minutes
an office worker spends on the telephone in a given 10-minute period. The
</p>
<p>length of time T was given as N(7, 1) as shown in Figure 1.5. Determine the
</p>
<p>probability that a caller is on the telephone more than 8 minutes by finding
</p>
<p>P[T &gt; 8].
</p>
<p>10.30 C..:...) (w) A population of high school students in the eastern United States
score X points on their SATs, where X rv N(500, 4900). A similar population
</p>
<p>in the western United States score X points, where X rv N(525, 3600). Which
</p>
<p>group is more likely to have scores above 700?
</p>
<p>10.31 (f) Verify the numerical results given in (1.3).
</p>
<p>10.32 (f) In Example 2.2 we asserted that P[X &gt; 2] for a standard normal random
variable is 0.0228. Verify this result.
</p>
<p>10.33 C..:...) (w) Is the following function a valid CDF?
</p>
<p>1
Fx(x) = --.....,.----,-
</p>
<p>1 + exp( -x)
- 00 &lt; x &lt; 00.
</p>
<p>10.34 (f) If Fx(x) = (2/,rr) arctan(x) for 0 :::; x &lt; 00, determine prO :::; X :::; 1].</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS
</p>
<p>10.35 (t) Prove that (10.25) is true.
</p>
<p>335
</p>
<p>10.36 C:.:,) (w) Professor Staff always scales his test scores. He adds a number of
points c to each score so that 50% of the class get a grade of C. A C is given if
</p>
<p>the score is between 70 and 80. If the scores have the distribution N(65 , 38),
</p>
<p>what should c be? Hint: There are two possible solutions to this problem but
</p>
<p>the students will prefer only one of them.
</p>
<p>10.37 (w) A Rhode Island weatherman says that he can accurately predict the
</p>
<p>temperature for the following day 95% of the time. He makes his prediction
</p>
<p>by saying that the temperature will be between T1Fahrenheit and T2Fahren-
heit. If he knows that the actual temperature is a random variable with PDF
</p>
<p>N(50, 10), what should his prediction be for the next day?
</p>
<p>10.38 (f) For the CDF given in Figure 10.14 find the PDF by differentiating. What
</p>
<p>happens at x = 1 and x = 2?
</p>
<p>10.39 (f,c) If Y = exp(X) , where X '" U(O, 1), find the PDF of Y. Next generate
realizations of X on a computer and transform them according to exp(X) to
</p>
<p>yield the realizations of Y . Plot the x's and y's in a similar manner to that
</p>
<p>shown in Figure 10.22 and discuss your results.
</p>
<p>10.40 c.:....:...) (f) Find the PDF of Y = X 4 + 1 if X '" exp (A).
</p>
<p>10.41 (w) Find the constants a and b so that Y = aX + b, where X '" U(O, 1),
yields Y '" U(2 , 6).
</p>
<p>10.42 (f) If Y = aX, find the PDF of Y if the PDF of X is px(x). Next, assume
that X '" exp(l) and find the PDFs of Y for a &gt; 1 and 0 &lt; a &lt; 1. Plot these
PDFs and explain your results .
</p>
<p>10.43 L...:...) (f) Find a general formula for the PDF ofY = IXI. Next, evaluate your
formula if X is a standard normal random variable.
</p>
<p>10.44 (f) If X '" N(o, 1) is transformed according to Y = exp(X), determine py(y)
by using the CDF approach. Compare your results to those given in Example
</p>
<p>10.6. Hint: You will need Leibnitz 's rule
</p>
<p>d 19(y) dg(y)
~ p(x)dx =p(g(y&raquo;--d---'
YaY
</p>
<p>10.45 (w) A random voltage X is input to a full wave rectifier that produces at its
</p>
<p>output the absolute value of the voltage. If X is a standard normal random
</p>
<p>variable, what is the probability that the output of the rectifier will exceed 2?</p>
<p/>
</div>
<div class="page"><p/>
<p>336 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>10.46 L..:.-) (f,c) If Y = X 2 , where X ""' U(O, 1), determine the PDF of Y . Next
perform a computer simulation using the realizations of Y (obtained as Ym =
</p>
<p>x;', where X m is the mth realization of X) to estimate the PDF py(y). Do
your theoretical results match the simulated results?
</p>
<p>10.47 (w) If a discrete random variable X has a Ber(p) PMF, find the PDF of X
using impulses. Next find the CDF of X by integrating the PDF.
</p>
<p>10.48 (t) In this problem we point out that the use of impulses or Dirac delta
</p>
<p>functions serves mainly as a tool to allow sums to be written as integrals. For
</p>
<p>example, the sum
</p>
<p>can be written as the integral
</p>
<p>s = i: g(x)dx
if we define g(x) as
</p>
<p>N
</p>
<p>g(x) = L aio(x - i) .
i= l
</p>
<p>Verify that this is true and show how it applies to computing probabilities of
</p>
<p>events of discrete random variables by using integration.
</p>
<p>10.49 (f) Evaluate the expression
</p>
<p>r (1 3 1 )il 2o(x - 2) + So(x - 4) + So(x - 3/2) dx.
Could the integrand represent a PDF? If it does, what does this integral rep-
</p>
<p>resent?
</p>
<p>10.50 (w) Plot the PDF and CDF if
</p>
<p>1 1 1
px(x) = 2exp( - x)u( x) + 4o(x + 1) + 4o(x - 1).
</p>
<p>10.51 C ~ . . : . . . ) (w) For the PDF given in Problem 10.50 determine the following:
</p>
<p>P[-2 ::; X ::; 2J, P[-l ::; X ::; 1]' P[-l &lt; X ::; 1]' P[-l &lt; X &lt; 1]'
P[-l ::; X &lt; 1J.
</p>
<p>10.52 (f) Find and plot the PDF of the transformed random variable
</p>
<p>Y = {22X 0::; X &lt; 1
X :2: 1
</p>
<p>where X ""' exp(L).</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 337
</p>
<p>10.53 (f) Find the PDF representation of the PMF of a bin(3 , 1/2) random variable.
</p>
<p>Plot the PMF and the PDF.
</p>
<p>10.54 C:.:.-) (f) Determine the function 9 so that X = g(U), where U ,....., U(O, 1), has
a Rayleigh PDF with (j2 = 1.
</p>
<p>10.55 (f) Find a transformation so that X = g(U) , where U ,....., U(O , 1), has the
PDF shown in Figure 10.34.
</p>
<p>1 . . ...
</p>
<p>0.5 ... ..
</p>
<p>21.50.5
OL--~_......Jo,------J
</p>
<p>a
</p>
<p>Figure 10.34: PDF for Problem 10.55
</p>
<p>10.56 (c) Verify your results in Problem 10.55 by generating realizations of the
</p>
<p>random variable whose PDF is shown in Figure 10.34. Next estimate the
</p>
<p>PDF and compare it to the true PDF.
</p>
<p>10.57 (t) A monotonically increasing function g(x) is defined as one for which if
</p>
<p>X2 2: Xl , then g(X2) 2: g(xt}. A monotonically decreasing function is one
for which if X2 2: Xl, then g(X2) ::; g(xt}. It can be shown that if g(x)
is differentiable, then a function is monotonically increasing (decreasing) if
</p>
<p>dg(x)/dx 2: 0 (dg(x)/dx ::; 0) for all x. Which of the following functions are
monotonically increasing or decreasing: exp(x), In(x), and l/x?
</p>
<p>10.58 (t) Explain why the values of X for which the inequality X 2: Xo is true do not
change if we take the logarithm of both sides to yield In(x) 2: In(xo). Would
the inequality still hold if we inverted both sides or equivalently applied the
</p>
<p>function g(x) = l/x to both sides? Hint: See Problem 10.57.
</p>
<p>10.59 (w) Compare the true PDF given in Figure 10.24 with the estimated PDF
shown in Figure 2.10. Are they the same and if not, why not?
</p>
<p>10.60 (c) Generate on the computer realizations of the random variable X ,.....,
</p>
<p>N(1 ,4). Estimate the PDF and compare it to the true one.</p>
<p/>
</div>
<div class="page"><p/>
<p>338 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>10.61 (c) Determine the PDF ofY = X 3 if X rv U(O, 1). Next generate realizations
</p>
<p>of X on the computer, apply the transformation g(x) = x3 to each realiza-
tion to yield realizations of Y , and finally estimate the PDF of Y from these
</p>
<p>realizations. Does it agree with the true PDF?
</p>
<p>10.62 (c) For the random variable Y described in Problem 10.61 determine the
</p>
<p>CDF. Then, generate realizations of Y , estimate the CDF, and compare it to
</p>
<p>the true one.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix IDA
</p>
<p>Derivation of PDF of a
</p>
<p>Transformed Continuous
</p>
<p>Random Variable
</p>
<p>The proof uses the CDF approach as described in Section 10.7. It assumes that 9
</p>
<p>is a one-to-one funct ion. If Y = g(X) , where 9 is a one-to-one and monotonically
</p>
<p>increasing function, then there is a single solution for x in y = g(x) . Thus,
</p>
<p>Fy(y) = P [g(X ) ::; y]
</p>
<p>= P [X::; g-l(y)]
</p>
<p>FX(g -l(y)].
</p>
<p>But py(y) = dFy(y)jdy so that
</p>
<p>py(y)
d 1
</p>
<p>= dy Fx(g- (y))
</p>
<p>= dFx(x) I dg-1(y)
dx x=g- l (y ) dy
</p>
<p>( - 1( ))d
g-
</p>
<p>1(y)
</p>
<p>= Px 9 Y dy '
</p>
<p>(chain rule of calculus )
</p>
<p>If g(x) is one-to-one and monotonically decreasing, then
</p>
<p>Fy(y) P [g(X ) ::; y]
</p>
<p>= P[X ~ g-l(y)]
</p>
<p>1 - P[X ::; g-l(y)] (since P [X = g-l(y)] = 0)
</p>
<p>= 1 - FX(g -l(y))</p>
<p/>
</div>
<div class="page"><p/>
<p>340
</p>
<p>and
</p>
<p>CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>py(y)
</p>
<p>Note that if 9 is montonically decreasing, then g-l is also montonically decreasing.
</p>
<p>Hence , dg-1(y)/dy will be negative. Thus, both cases can be subsumed by the
</p>
<p>formula
</p>
<p>-1 Id9-
1
(y) Ipy(y) = px(g (y)) dy .</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix lOB
</p>
<p>MATLAB Subprograms to
</p>
<p>Compute Q and Inverse Q
Functions
</p>
<p>Input Parameters :
</p>
<p>Output Parameters:
</p>
<p>Verification Test Case:
</p>
<p>x - Real column vector of x values
</p>
<p>y - Real column vector of right-tail probabilities
</p>
<p>The input x=[O 1 2]'; should produce y=[0.5 0.1587 0 .0228]'.
</p>
<p>This program computes the right-tail probability
</p>
<p>(complementary cumulative distribution function) for
</p>
<p>a N(0,1) random variable.
</p>
<p>'I. Q.m
</p>
<p>'I.
'I.
'I.
'I.
'I.
'I.
</p>
<p>'I.
'I.
</p>
<p>'I.
'I.
</p>
<p>'I.
'I.
'I.
'I.
'I.
</p>
<p>'I.
</p>
<p>'I.
function y=Q(x)
</p>
<p>y=0 .5*erfc(x/sqrt(2)); 'I. complementary error function
</p>
<p>'I. Qinv.m</p>
<p/>
</div>
<div class="page"><p/>
<p>342 CHAPTER 10. CONTINUOUS RANDOM VARIABLES
</p>
<p>Input Parameters:
</p>
<p>Output Parameters:
</p>
<p>Verification Test Case:
</p>
<p>y - Real column vector of values of random variable
</p>
<p>x - Real column vector of right-tail probabilities
(in interval [0,1])
</p>
<p>The input x=[0.5 0.1587 0.0228]'; should produce
y=[O 0 .9998 1.9991]'.
</p>
<p>y.
Y. This program computes the inverse Q function or the value
</p>
<p>Y. which is exceeded by a N(O,l) random variable with a
</p>
<p>Y. probability of x.
</p>
<p>Y.
</p>
<p>Y.
</p>
<p>Y.
</p>
<p>Y.
</p>
<p>Y.
</p>
<p>Y.
%
</p>
<p>Y.
</p>
<p>Y.
</p>
<p>%
</p>
<p>%
</p>
<p>Y.
</p>
<p>%
%
%
</p>
<p>function y=Qinv(x)
</p>
<p>y=sqrt(2)*erfinv(1-2*x); Y. inverse error function</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 11
</p>
<p>Expected Values for Continuous
</p>
<p>Random Variables
</p>
<p>11.1 Introduction
</p>
<p>We now define the expectation of a continuous random variable. In doing so we
</p>
<p>parallel the discussion of expected values for discrete random variables given in
</p>
<p>Chapter 6. Based on the probability density function (PDF) description of a con-
</p>
<p>tinuous random variable, the expected value is defined and its properties explored.
</p>
<p>The discussion is conceptually much the same as before , only the particular method
</p>
<p>of evaluating the expected value is different. Hence, we will concentrate on the
</p>
<p>manipulations required to obtain the expected value.
</p>
<p>11.2 Summary
</p>
<p>The expected value E[X] for a continuous random variable is motivated from the
analogous definition for a discrete random variable in Section 11.3. Its definition is
</p>
<p>given by (11.3). An analogy with the center of mass of a wedge is also described.
</p>
<p>For the expected value to exist we must have E[IXI] &lt; 00 or the expected value of
the absolute value of the random variable must be finite. The expected values for
</p>
<p>the common continuous random variables are given in Section 11.4 with a summary
</p>
<p>given in Table 11.1. The expected value of a function of a continuous random
</p>
<p>variable can be easily found using (11.10) , eliminating the need to find the PDF of
</p>
<p>the transformed random variable. The expectation is shown to be linear in Example
</p>
<p>11.2. For a mixed random variable the expectation is computed using (11.11). The
</p>
<p>variance is defined by (11.12) with some examples given in Section 11.6. It has
</p>
<p>the same properties as for a discrete random variable, some of which are given in
</p>
<p>(11.13) , and is a nonlinear operation. The moments of a continuous random variable
</p>
<p>are defined as E[Xn ] and can be found either by using a direct integral evaluation as</p>
<p/>
</div>
<div class="page"><p/>
<p>344 CHAPTER 11. EXPECTED VALUES
</p>
<p>in Example 11.6 or by using the characteristic function (11.18). The characteristic
</p>
<p>fun ction is the Fourier transform of the PDF as given by (11.17) . Central moments,
</p>
<p>which are the moments about the mean, are related to the moments by (11.15).
</p>
<p>The second central moment is just the variance. Although the probability of an
</p>
<p>event cannot in general be determined from the mean and variance, the Chebyshev
</p>
<p>inequality of (11.21) provides a formula for bounding the probability. The mean and
</p>
<p>variance can be estimated using (11.22) and (11.23). Finally, an application of mean
</p>
<p>estimation to test highly reliable software is described in Section 11.10. It is based
</p>
<p>on importance sampling, which provides a means of estimating small probabilities
</p>
<p>with a reasonable number of Monte Carlo trials.
</p>
<p>11.3 Determining the Expected Value
</p>
<p>The expected value for a discrete random variable X was defined in Chapter 6 to
</p>
<p>be
</p>
<p>(11.1)
</p>
<p>where PX[Xi] is the probability mass function (PMF) of X and the sum is over all i
</p>
<p>for which the PMF PX[Xi] is nonzero. In the case of a continuous random variable,
</p>
<p>the sample space Sx is not countable and hence (11.1) can no longer be used . For
example, if X /'oJ U(O , 1), then X can take on any value in the interval (0,1) , which
</p>
<p>consists of an uncountable number of values. We might expect that the average
</p>
<p>value is E[X] = 1/2 since the probability of X being in any equal length interval
in (0,1) is the same. To verify this conjecture we employ the same strategy used
</p>
<p>previously, that of approximating a uniform PDF by a uniform PMF, using a fine
</p>
<p>partitioning of the interval (0,1). Letting
</p>
<p>Xi = il::i.x
</p>
<p>for i = 1,2, . .. ,M and with l::i.x = I/M, we have from (11.1)
</p>
<p>E[X] t, x;PX[X;] = t,(i~X) (~)
M . 1 M
</p>
<p>= L~2 = M2Li.
i = l i = l
</p>
<p>But I: ~1 i = (M/2)(M + 1) so that
</p>
<p>E[X] = ~(M + 1) =! _1_
M2 2 + 2M
</p>
<p>(11.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3. DETERMINING THE EXPECTED VALUE 345
</p>
<p>and as M --+ 00 or the partition of (0,1) becomes infinitely fine, we have E[X] --+ 1/2,
as expected. To extend these results to more general PDFs we first note from (11.2)
</p>
<p>that
</p>
<p>M
</p>
<p>E[X] L XiP [Xi - b..x/2 ::; X ::; Xi + b..x/2]
i =1
</p>
<p>~ P[Xi - b..x/2 ::; X ::; Xi + b..x/2] J\
= L...J Xi b.. is&raquo;,
</p>
<p>i=1 X
</p>
<p>But
P[Xi - b..x/2 ::; X ::; Xi + b..x/2] = l/M = 1
</p>
<p>b..x b..x
</p>
<p>and as b..x --+ 0, this is the probability per unit length for all small intervals centered
about Xi, which is the PDF evaluated at X = Xi. In this example, PX(Xi) does not
depend on the interval center, which is Xi, so that the PDF is uniform or px(x) = 1
</p>
<p>for 0 &lt; X &lt; 1. Thus, as b..x --+ 0
</p>
<p>M
</p>
<p>E[X] --+ L XiPX(Xi)b..x
i=1
</p>
<p>and this becomes the integral
</p>
<p>E[X] = 11 xpx(x)dx
where px(x) = 1 for 0 &lt; X &lt; 1 and is zero otherwise. To confirm that this integral
produces a result consistent with our earlier value of E[X] = 1/2, we have
</p>
<p>E[X] = 11 xpx(x)dx
t' ai- 1dx = ~x211
</p>
<p>io 2 0
1
</p>
<p>2
</p>
<p>In general, the expected value for a continuous random variable X is defined as
</p>
<p>E[X] =I: xpx(x)dx (11.3)
where px(x) is the PDF of X. Another example follows.
</p>
<p>Example 11.1 - Expected value for random variable with a nonuniform
</p>
<p>PDF
</p>
<p>Consider the computation of the expected value for the PDF shown in Figure ll.la.
</p>
<p>From the PDF and some typical outcomes shown in Figure 11.1b the expected value</p>
<p/>
</div>
<div class="page"><p/>
<p>346 CHAPTER 11. EXPECTED VAL UES
</p>
<p>12 . - - - - r - - - ~ - - ~ - - ~ - ~ _ _ ,
px(x) = x / 2
</p>
<p>' " '"
.,
</p>
<p>' " .'
</p>
<p>11 T
302510 15 20
</p>
<p>Trial number
5
</p>
<p>o
o
</p>
<p>05
</p>
<p>2
</p>
<p>15
Cl)
</p>
<p>S 1.33
o
u...,
::I
o
</p>
<p>2
</p>
<p>~ r5
E[X] = 1.33
</p>
<p>05
</p>
<p>H OB
"-'"
</p>
<p>~
~ 0 . 6
</p>
<p>0.4
</p>
<p>02
</p>
<p>0
0
</p>
<p>(a) PDF (b) Typical outcomes and expected value of
</p>
<p>1.33
</p>
<p>Figure 11.1: Example of nonuniform PDF and its mean.
</p>
<p>should be between 1 and 2. Using (11.3) we have
</p>
<p>which appears to be reasonable.
</p>
<p>&lt;)
</p>
<p>As an analogy to the exp ected value we can revisit our Jarlsberg cheese first de-
</p>
<p>scribed in Section 10.3, and which is shown in Figure 11.2. The integral
</p>
<p>CM = 12 xm(x )dx (11.4)
is t he center of m ass, assuming that the total mass or J; m (x )dx, is one. Here ,
m(x) is the linear mass densi ty or mass per uni t length. The center of mass is the
</p>
<p>point at which one could balance the cheese on the point of a pencil. Recall t hat
</p>
<p>the linear mass density is m(x) = x / 2 for which CM = 4/ 3 from Example 11.1. To
show that CM is the balance point we first note that J02m(x)dx = 1 so that we can</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3. DETERMINING THE EXPECTED VALUE
</p>
<p>1
1
</p>
<p>1
/
</p>
<p>1
</p>
<p>center of mass
at x =4/3
</p>
<p>2
</p>
<p>Figure 11.2: Center of mass (CM) analogy to average value.
</p>
<p>write (11.4) as
</p>
<p>12 xm(x)dx - CM = 0
12 xm(x)dx - CM12 m(x)dx 0
</p>
<p>r
2
</p>
<p>(x - CM) m(x)dx = o.
J0 ------ '----"
</p>
<p>'-v-'moment arm mass
sum
</p>
<p>347
</p>
<p>Since the "sum" of the mass times moment arms is zero, the cheese is balanced at
</p>
<p>x = CM = 4/3.
By the same argument the expected value can also be found by solving
</p>
<p>i: (x - E[X])px(x)dx = 0 (11.5)
for E[X]. If, however, the PDF is symmetric about some point x = a, which is to
</p>
<p>say that px(a + u) = px(a - u) for -00 &lt; u &lt; 00, then (see Problem 11.2)
</p>
<p>i: (x - a)px(x)dx = 0 (11.6)
and therefore E[X] = a. Such was the case for X ,...., U(O, 1), whose PDF is symmetric
about a = 1/2. Another example is the Gaussian PDF which is symmetric about
a = J-L as seen in Figures lO.8a and lO.8c. Hence, E[X] = J-L for a Gaussian random
variable (see also the next section for a direct derivation). In summary, if the PDF
</p>
<p>is symmetric about a point, then that point is E[X]. However, the PDF need not
</p>
<p>be symmetric about any point as in Example 11.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>348 CHAPTER 11. EXPECTED VALUES
</p>
<p>L1h Not all PDFs have expected values.
Before computing the expected value of a random vari able using (11.3) we must
</p>
<p>make sure that it exists (see similar discussion in Section 6.4 for discrete random
</p>
<p>variables). Not all integrals of the form I ~ oo xpx (x )dx exist, even if I ~ oop x(x)d x =
</p>
<p>1. For example, if
</p>
<p>{
</p>
<p>1
-- x &gt; 1
</p>
<p>px (x) = 2x 3/ 2 -
o X &lt; 1
</p>
<p>then
</p>
<p>but
</p>
<p>1
00 1
</p>
<p>x -x- 3/ 2dx = y!xTXl -+ 00 .
1 2 1
</p>
<p>A more subtle and somewhat surprising example is the Cauchy PDF. Recall that it
</p>
<p>is given by
1
</p>
<p>px(x) = 7r(1 + x2 ) - 00 &lt; x &lt; 00.
</p>
<p>Since the PDF is symmetric about x = 0, we would expect that E[X] = O. However,
if we are careful about our definition of expected value by correctly interpreting the
</p>
<p>region of integration in a limiting sense , we would have
</p>
<p>E[X] = lim [0 xpx(x)dx + lim [ u xpx (x )dx.
L ~ - O O J L u ~ o o h
</p>
<p>But for a Cauchy PDF
</p>
<p>E[X] =
</p>
<p>=
</p>
<p>f o 1 l
U
</p>
<p>1lim x dx + lim x dx
L~- oo L 7r(1 + x 2 ) Us-vco &deg; 7r(1 + x 2 )
</p>
<p>1 1&deg; 1 I
U
</p>
<p>lim -In(1 + x 2 ) + lim -In(1 + x 2 )
L ~-oo 27r L U ~ oo 27r &deg;
</p>
<p>lim -~ In(1 + L 2 ) + lim ~ In(1 + U2 )
E--r-ccx: 27r U ~oo 27r
</p>
<p>-00 + 00 =?
</p>
<p>Hence , if the limits are taken independently, then the result is indeterminate. To
</p>
<p>make the expected value useful in practice the independent choice of limits (and not
</p>
<p>L = U) is necessary. The indeterminancy can be avoided, however, if we require
"absolute convergence" or
</p>
<p>i:Ixlpx (x )dx &lt; 00.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4. EXPECTED VALUES FOR IMPORTANT PDFS 349
</p>
<p>Hence , E[X] is defined to exist if E[IXIl &lt; 00. This surprising result can be "ver-
ified" by a computer simulation, the results of which are shown in Figure 11.3. In
</p>
<p>40 .---~--~--~-~-----, 10.---~--~--~-~-----,
</p>
<p>20 . 5 : -
</p>
<p>It-l
</p>
<p>2000 4000 6000 8000 10000
Total number of trials
</p>
<p>_ 1 0 L - . - ~ - - ~ - - ~ - ~ - - - - - - '
</p>
<p>o5010 20 30 40
Trial number
</p>
<p>- 4 0 L - . - ~ - - ~ - - ~ - ~ - - - '
</p>
<p>o
</p>
<p>(a) First 50 outcomes (b) Sample mean
</p>
<p>Figure 11.3: Illustration of nonexistence of Cauchy PDF mean.
</p>
<p>Figure 11.3a the first 50 outcomes of a total of 10,000 are shown. Because of the
</p>
<p>slow decay of the "tails" of the PDF or since the PDF decays only as 1/x2 , very
</p>
<p>large outcomes are possible. As seen in Figure 11.3b the sample mean does not
</p>
<p>converge to zero as might be expected because of these infrequent but very large
</p>
<p>outcomes (see also Problem 12.41). See also Problem 11.3 on the simulation used in
</p>
<p>this example and Problems 11.4 and 11.9 on how to make the sample mean converge
</p>
<p>by truncating the PDF.
</p>
<p>L1h
Finally, as for discrete random variables the expected value is the best guess of the
</p>
<p>outcome of the random variable. By "best" we mean that the use of b = E[X] as
our estimator. This estimator minimizes the mean square error, which is defined as
</p>
<p>mse = E[(X - b)2] (see Problem 11.5).
</p>
<p>11.4 Expected Values for Important PDFs
</p>
<p>We now determine the expected values for the important PDFs described in Chapter
</p>
<p>10. Of course, the Cauchy PDF is omitted.
</p>
<p>11.4.1 Uniform
</p>
<p>If X ,..., U(a,b), then it is easy to prove that E[X] = (a + b)/2 or the mean lies at
the midpoint of the interval (see Problem 11.8).</p>
<p/>
</div>
<div class="page"><p/>
<p>350 CHAPTER 11. EXPECTED VALUES
</p>
<p>11.4.2 Exponential
</p>
<p>If X", exp(X), then
</p>
<p>E[X] = 100 x&gt;..exp(-&gt;..x)dx
[-xexp(-&gt;..x) - ~exp(-&gt;..x)] [ =~. (11.7)
</p>
<p>Recall that the exponential PDF spreads out as &gt;.. decreases (see Figure 10.6) and
</p>
<p>hence so does the mean.
</p>
<p>11.4.3 Gaussian or Normal
</p>
<p>If X '" N(j.L, 0"2), then since the PDF is symmetric about the point x = u, we know
</p>
<p>that E[X] = u, A direct appeal to the definition of the expected value yields
</p>
<p>E[X]
</p>
<p>=
</p>
<p>Letting u = x - j.L in the first integral we have
</p>
<p>E[X] =100 u ~ exp [- 212u2] du +j.L100 ~ exp [-2\ (x - j.L)2] dx = p:
-00 27r0"2 0" -00 27r0"2 0", , , .,
</p>
<p>v v
</p>
<p>o =1
</p>
<p>The first integral is zero since the integrand is an odd function (g(-u) = - 9 (u), see
also Problem 11.6) and the second integral is one since it is the total area under the
</p>
<p>Gaussian PDF.
</p>
<p>11.4.4 Laplacian
</p>
<p>The Laplacian PDF is given by
</p>
<p>-oo&lt;x&lt;oo (11.8)
</p>
<p>and since it is symmetric about x = 0 (and the expected value exists - needed to
</p>
<p>avoid the situation of the Cauchy PDF), we must have E[X] = O.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5. EXPECTED VALUE FOR A FUNCTION OF A RAND. VAR. 351
</p>
<p>11.4.5 Gamma
</p>
<p>If X I'J r(a, x), then from (10.10)
</p>
<p>(X) &gt;,,0
E[X] = Jo xr(a) x
</p>
<p>o
-
</p>
<p>1
exp( -&gt;..x)dx.
</p>
<p>To evaluate this integral we attempt to modify the integrand so that it becomes the
</p>
<p>PDF of a r(a' , &gt;..') random variable. Then, we can immediately equate the integral
to one. Using this strategy
</p>
<p>&gt;,,0 roo &gt;..0+1 0 f(a + 1)
E[X] = r(a) Jo r(a + 1) x exp( -&gt;..x)dx &gt;"0+1
</p>
<p>f(a + 1)
&gt;"f(a)
</p>
<p>af(a)
=
</p>
<p>&gt;"f(a)
a
</p>
<p>&gt;..'
</p>
<p>(integrand is r(a + 1, x) PDF)
</p>
<p>(using Property 10.3)
</p>
<p>11.4.6 Rayleigh
</p>
<p>It can be shown that E[X] = J(-rra2 )j 2 (see Problem 11.16).
</p>
<p>The reader should indicate on Figures 10.6-10.10, 10.12, and 10.13 where the
</p>
<p>mean occurs.
</p>
<p>11.5 Expected Value for a Function of a Random Vari-
</p>
<p>able
</p>
<p>If Y = g(X), where X is a continuous random variable, then assuming that Y is
also a continuous random variable with PDF py(y), we have by the definition of
</p>
<p>expected value of a continuous random variable
</p>
<p>E[Y] = I: ypy(y)dy. (11.9)
Even if Y is a mixed random variable, its expected value is still given by (11.9) ,
</p>
<p>although in this case py (y) will contain impulses. Such would be the case if for
</p>
<p>example, Y = max(O, X) for X taking on values -00 &lt; x &lt; 00 (see Section 10.8).
As in the case of a discrete random variable, it is not necessary to use (11.9) directly,
</p>
<p>which requires us to first determine py(y) from px(x). Instead, we can use for
</p>
<p>Y = g(X) the formula
</p>
<p>E[g(X)] =I: g(x)px(x)dx. (11.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>352 CHAPTER 11. EXPECTED VALUES
</p>
<p>(integrand is symmetric about x = 0).
</p>
<p>A partial proof of this formula is given in Appendix llA. Some examples of its use
</p>
<p>follows.
</p>
<p>Example 11.2 - Expectation of linear (affine) function
</p>
<p>If Y = aX + b, then since g(x) = ax + b, we have from (11.10) that
</p>
<p>E[g(X)] = i:(ax + b)px(x)dx
ai:xpx (x )dx + bi:px(x)dx
aE[X] +b
</p>
<p>or equivalently
</p>
<p>E[aX + b] = aE[X] + b.
</p>
<p>It indicates how to easily change the expectation or mean of a random variable. For
</p>
<p>example, to increase the mean value by b just replace X by X + b. More generally,
</p>
<p>it is easily shown that
</p>
<p>This says that the expectat ion operator is lin ear.
</p>
<p>Example 11.3 - Power of N(O, 1) random variable
</p>
<p>If X ,...., N(o ,1) and Y = X 2, consider E[Y] = E[X2]. The quantity E[X2] is the
average squared value of X and can be interpreted physically as a power. If X is
</p>
<p>a voltage across a 1 ohm resistor , then X 2 is the power and therefore E[X2] is the
</p>
<p>average power. Now according to (11.10)
</p>
<p>E[X2] = 100 x2_1_ exp (_~x2) dx
-00...;2; 2
</p>
<p>= 2 roo x2_
1
_ exp (_~x2) dx
</p>
<p>io...;2; 2
</p>
<p>To evaluate this integral we use integration by parts UUdV = UV - JV dU, see
also Problem 11.7) with U = x, dU = dx , dV = (1/...;2;)xexp[-(1/2) x2]dx and
</p>
<p>therefore V = -(1/...;2;) exp[-(1/2)x2 ] to yield
</p>
<p>E[X2] = 2 [-x _1 exp (_~x 2) 1
00
</p>
<p>_ r oo __1 exp (_~x2) dX]
...;2; 2 0 io...;2; 2
</p>
<p>= 0+1=1.
</p>
<p>The first term is zero since
</p>
<p>1&middot; ( 1 2) l' x u 1im x exp - - x = im = im = 0
x-too 2 x-too exp Ux2) x-too Xexp ( ~ x 2 )</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5. EXPECTED VALUE FOR A FUNCTION OF A RAND. VAR. 353
</p>
<p>using L&rsquo;Hospital&rsquo;s rule and the second term is evaluated using
&int; &infin;
</p>
<p>0
</p>
<p>1&radic;
2
</p>
<p>exp
</p>
<p>(
</p>
<p>&minus;1
2
x2
</p>
<p>)
</p>
<p>dx =
1
</p>
<p>2
(Why?).
</p>
<p>&diams;
</p>
<p>Example 11.4 &ndash; Expected value of indicator random variable
</p>
<p>An indicator function indicates whether a point is in a given set. For example, if
the set is A = [3, 4], then the indicator function is defined as
</p>
<p>IA(x) =
</p>
<p>{
</p>
<p>1 3 &le; x &le; 4
0 otherwise
</p>
<p>and is shown in Figure 11.4. The subscript on I refers to the set of interest. The
</p>
<p>0 1 2 3 4 5 6
0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>2
</p>
<p>I A
(x
)
</p>
<p>x
</p>
<p>Figure 11.4: Example of indicator function for set A = [3, 4].
</p>
<p>indicator function may be thought of as a generalization of the unit step function
since if u(x) = 1 for x &ge; 0 and zero otherwise, we have that
</p>
<p>I[0,&infin;)(x) = u(x).
</p>
<p>Now if X is a random variable, then IA(X) is a transformed random variable that
takes on values 1 and 0, depending upon whether the outcome of the experiment lies
within the set A or not, respectively. (It is actually a Bernoulli random variable.)
On the average, however, it has a value between 0 and 1, which from (11.10) is
</p>
<p>E[IA(X)] =
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
IA(x)pX(x)dx
</p>
<p>=
</p>
<p>&int;
</p>
<p>{x:x&isin;A}
1 &middot; pX(x)dx (definition)
</p>
<p>=
</p>
<p>&int;
</p>
<p>{x:x&isin;A}
pX(x)dx
</p>
<p>= P [A].</p>
<p/>
</div>
<div class="page"><p/>
<p>354 CHAPTER 11. EXPECTED VALUES
</p>
<p>(11.11)
</p>
<p>Therefore, the expected value of the indicator random variable is the probability of the
</p>
<p>set or event. As an example of its utility, consider the estimation of P[3 ~ X ~ 4].
</p>
<p>But this is just E[IA(X)] when IA(x) is given in Figure 11.4. To estimate the
</p>
<p>expected value of a transformed random variable we first generate the outcomes of X ,
</p>
<p>say X l, X2, , X M, then transform each one to the new random variable producing
</p>
<p>for i = 1,2, , M
</p>
<p>I () {
1 3 ~ Xi ~ 4
</p>
<p>A Xi = o otherwise
and finally compute the sample mean for our estimat e using
</p>
<p>However, since P[A] = E[IA(X)], we have as our estimate of the probability
</p>
<p>But this is just what we have been using all along, since 2 : ~ 1 IA(Xi) counts all
</p>
<p>the outcomes for which 3 ~ x ~ 4. Thus, the indicator fun ction provides a means
</p>
<p>to connect the expected value with the probability. This is a very useful for later
</p>
<p>theoretical work in probability.
</p>
<p>Lastly, if the random variable is a mixed one with PDF
</p>
<p>00
</p>
<p>px(x) = Pc(x) + LPi8( X - Xi)
i= l
</p>
<p>where Pc(x) is the continuous part of the PDF, then the expected value becomes
</p>
<p>E[X] = i: x (p,(X) + t,PiO(X- Xi)) da:
i: xpc(x)dx +i:Xt,Pi8(X - xi)dx
</p>
<p>= i:x pc(x )dx + t,Pii:x8(x - x i)dx
i: xp, (x )dx +t, x,P,
</p>
<p>since J ~ o o g(x)8(x - x i)dx = g(x d for g(x) a function continuous at X = Xi. This
is known as the sifting property of a Dirac delta function (see Appendix D). A</p>
<p/>
</div>
<div class="page"><p/>
<p>11.6. VARIANCE AND MOMENTS 355
</p>
<p>Values PDF E [X ] var(X) 1Jx(w)
</p>
<p>Uniform a&lt;x &lt;b 1 ~( a+ b)
(b-a)2 exp(jwb)- exp(jw a)
</p>
<p>b-a -1-2- j w(b-a)
</p>
<p>Exponential x~ O ..\exp (- ..\x) 1 1
x
</p>
<p>x )0 ..\ - j w
</p>
<p>Gaussian - oo&lt;x &lt;oo
exp[ - ( I( 2a2))(X- /l)2j
</p>
<p>J.L
(J2 exp(jw JL- a2w2( 2]
</p>
<p>v'27ra 2
</p>
<p>Laplacian - oo&lt;x &lt;oo ~ex P(- V2( a2 Ixl) 0 (J2
2( a2
</p>
<p>2a w
2+2(a2
</p>
<p>Gamma x ~ O ..\" a - I a a 1r(a ) x exp( - ..\x) x )0 (l -jw( ..\)"
</p>
<p>{if
[Johnson
</p>
<p>Rayleigh x ~ O 5-exp[-x2(2a 2)J (2-7r(2 )a2 et a11994]a
</p>
<p>Table 11.1: Properties of cont inuous random variables.
</p>
<p>summary of the means for the important PDFs is given in Tab le 11.1. Lastly, note
</p>
<p>that the expected value of a random var iab le can also be determined from the CDF
</p>
<p>as shown in P roblem 11.28.
</p>
<p>11.6 Variance and Moments of a Continuous
</p>
<p>Random Variable
</p>
<p>The var iance of a cont inuous random variab le, as for a discrete random variable,
</p>
<p>measures the average squared deviation from the mean . It is defined as var (X ) =
</p>
<p>E[(X - E[X]) 2] (exactly the same as for a discret e random variab le) . To evaluate
</p>
<p>the var ian ce we use (11.10) to yield
</p>
<p>var(X) = i:(x - E [X]) 2pX( x)dx . (11.12)
As an example, consider a N(J.L , (J2) random variable. In Figure 10.9 we saw that
</p>
<p>the width of the P DF increases as (J2 increases. This is because the parameter (J2
</p>
<p>is actually the variance, as we now show. Using (11.12) and the definition of a
</p>
<p>Gaussian P DF
</p>
<p>var(X) =</p>
<p/>
</div>
<div class="page"><p/>
<p>356 CHAPTER 11. EXPECTED VALUES
</p>
<p>Let ting u = (x - J.L)/(j produces (recall that a = .Jdi&gt; 0)
</p>
<p>var(X) = / 00 (j2u2 ~ exp [- 2\ u2] odu
- 00 2w(j2 (j
</p>
<p>= (j2/ 00 u2_ 1_ exp [_~ u 2] du-00 .j2; 2
, '
</p>
<p>v
=1
</p>
<p>(see Example 11.3)
</p>
<p>Hence, we now know t hat a N(J.L , (j2) random variable has a mean of J.L and a var ian ce
of (j2.
</p>
<p>It is common to refer to the square-root of the vari ance as the standard deviat ion.
</p>
<p>For a N(J.L, (j2) random variable it is given by a, The standard deviation indicates
</p>
<p>how closely outcomes tend to cluster about the mean. (See Problem 11.29 for an
</p>
<p>alte rn at ive interpretation.) Again if the random variable is N(J.L, (j2), then 68.2%
</p>
<p>of the outcomes will be within the int erval [J.L - a, J.L + (j], 95.5% will be within
[J.L - 2(j, J.L + 2(j], and 99.8% will be within [J.L - 3(j, J.L + 3(j]. This is illustrated in
Figure 11.5. Of course, ot her PDFs will have concentrations that are different for
</p>
<p>E[X] &plusmn; k J var (X) . Another example follows.
</p>
<p>0.5,.-----.---.---.---.-----.,...-,. . . . . . . . .. .. : : : : : : : :?. . .
~ .
~0 .3 &middot; &middot; , : :. , :
</p>
<p>R. .'
0.2 ... .; ...;.
</p>
<p>0.1 .... ~ .... ~ .
</p>
<p>(a) 68.2% for 1 standard
</p>
<p>deviati on
</p>
<p>0.5,.-----.---.---.---.-----.,...-,
</p>
<p>:?. ...(...(.j... j ...;..
~0.3 .
</p>
<p>R.
0.2
</p>
<p>~~ -4--3 ~ - ~ 2 '-I- I
</p>
<p>(b) 95.5% for 2 standard
</p>
<p>deviations
</p>
<p>: : : : : : : : :. : : : : : : . :
:? ~ &middot; ..t&middot;r ..( .~ ... .t&middot; .. j""(&middot;
~0 .3 ; :.....:.&bull;: .... ;
</p>
<p>0.2 1 &middot; .. ; :&middot; .. &middot;,&middot;
</p>
<p>~ ~ - 4 - - 3 - ' - - " ' 2 --I
</p>
<p>(c) 99.8% for 3 standard
</p>
<p>deviations
</p>
<p>Figure 11.5: Percentage of outcomes of N(l , 1) random variable that are within
</p>
<p>k = 1,2, and 3 standard deviations from the mean. Shad ed regions denote area
within interval J.L - ko ::; x ::; J.L + ka .
</p>
<p>Example 11.5 - Variance of a uniform random variable
</p>
<p>If X "'-J U(a,b), then
</p>
<p>var(X ) i:(x - E [X ]) 2pX (x )dx
I
</p>
<p>b (1 )2 1x - - (a +b) --dx
a 2 b - a</p>
<p/>
</div>
<div class="page"><p/>
<p>11.6. VARIANCE AND MOMENTS
</p>
<p>and letting u = x - (a + b)/2, we have
</p>
<p>357
</p>
<p>var(X) = 1 c:----- u2du
b - a -(b-a}/2
</p>
<p>___1__ !
u 3
</p>
<p>1(b- a}/ 2
</p>
<p>b - a 3 -(b-a}/2
</p>
<p>(b - a)2
</p>
<p>12
</p>
<p>o
A summary of the variances for the important PDFs is given in Table 11.1. The
</p>
<p>variance of a continuous random variable enjoys the same properties as for a discrete
</p>
<p>random variable. Recall that an alternate form for variance computation is
</p>
<p>and if c is a constant then
</p>
<p>var(c)
</p>
<p>var(X + c)
var(cX)
</p>
<p>o
var(X)
</p>
<p>c2var(X). (11.13)
</p>
<p>Also, the variance is a nonlinear type of operation in that
</p>
<p>(see Problem 11.32). Recall from the discussions for a discrete random variable that
</p>
<p>E[X] and E[X2 ] are termed the first and second moments, respectively. In general,
</p>
<p>E[Xn] is termed the nth moment and it is defined to exist if E[!x!n] &lt; 00. If it
is known that E[XS] exists, then it can be shown that E[X r ] exists for r &lt; s (see
Problem 6.23). This also says that if E[X r ] is known not to exist, then E[XS] cannot
</p>
<p>exist for s &gt; r, An example is the Cauchy PDF for which we saw that E[X] does
not exist and therefore all the higher order moments do not exist. In particular,
</p>
<p>the Cauchy PDF does not have a second-order moment and therefore its variance
</p>
<p>does not exist. We next give an example of the computation of all the moments of
</p>
<p>a PDF.
</p>
<p>Example 11.6 - Moments of an exponential random variable
</p>
<p>Using (11.10) we have for X '" exp('x) that
</p>
<p>E[Xn] = 100 xn'xexp(-'xx)dx.
To evalute this we first show how the nth moment can be written recursively in terms
</p>
<p>of the (n - l)st moment. Since we know that E[X] = 1/,X, we can then determine</p>
<p/>
</div>
<div class="page"><p/>
<p>358 CHAPTER 11. EXPECTED VALUES
</p>
<p>all the moments using the recursion. We can begin to evaluate the integral using
</p>
<p>integration by parts. This will yield the recursive formula for the moments. Letting
</p>
<p>U = x n and dV = &gt;.exp(-&gt;.x)dx so that dU = nxn-1dx and V = -exp(-&gt;.x), we
have
</p>
<p>E[Xn] - x n exp( -&gt;,x)lgo -100 - exp( -&gt;.x)nxn-1dx
0+ n100 xn-1exp(-&gt;.x)dx
</p>
<p>= ~ 100 x n- 1&gt;. ex p(- &gt;.x )dx
~E[xn-l].
</p>
<p>Hence, the nth moment can be written in term of the (n - l)st moment. Since we
</p>
<p>know that E[X] = 1/&gt;', we have upon using the recursion that
</p>
<p>etc.
</p>
<p>and in general
</p>
<p>(11.14)
</p>
<p>The variance can be found to be var(X) = 1/&gt;.2 using these results.
</p>
<p>&lt;&gt;
In the next section we will see how to use characteristic functions to simplify the
</p>
<p>complicated integration process required for moment evaluation.
</p>
<p>Lastly, it is sometimes important to be able to compute moments about some
</p>
<p>point. For example, the variance is the second moment about the point E[X]. In
general, the nth central moment about the point E[X] is defined as E[(X - E[x])n].
The relationship between the moments and the central moments is of interest. For
</p>
<p>n = 2 the central moment is related to the moments by the usual formula E[ (X -
E[X])2] = E[X2] - E2[X]. More generally, this relationship is found using the
binomial theorem as follows.
</p>
<p>E[(X - E[X]t] E l~ G) X'(-EIXDn- , ]
n
</p>
<p>L (~) E[Xk](-E[X])n-k (linearity of expectation operator)
k=O</p>
<p/>
</div>
<div class="page"><p/>
<p>11.7. CHARACTERISTIC FUNCTIONS
</p>
<p>or finally we have that
</p>
<p>n
</p>
<p>E[(X - E[X)t ) = 2)-lt-k (~) (E[X])n-k E[Xk) .
k=O
</p>
<p>11.7 Characteristic Functions
</p>
<p>359
</p>
<p>(11.15)
</p>
<p>As first introduced for discrete random variables, the characteristic function is a
</p>
<p>valuable tool for the calculation of moments. It is defined as
</p>
<p>1Jx(w) = E[exp(jwX)) (11.16)
</p>
<p>and always exists (even though the moments of a PDF may not). For a continuous
</p>
<p>random variable it is evaluated using (11.10) for the real and imaginary parts of
</p>
<p>E[exp(jwX)), which are E[cos(wX)] and E[sin(wX)). This results in
</p>
<p>1Jx(w) = i: exp(jwx)px(x)dx
or in more familiar form as
</p>
<p>1Jx(w) =i:px(x) exp(jwx)dx. (11.17)
The characteristic function is seen to be the Fourier transform of the PDF, although
</p>
<p>with a +j in the definition as opposed to the more common - j. Once the charac-
teristic function has been found, the moments are given as
</p>
<p>1Jx(w)
</p>
<p>An example follows.
</p>
<p>Example 11.7 - Moments of the exponential PDF
</p>
<p>Using the definition of the exponential PDF (see (10.5)) we have
</p>
<p>100 Aexp( -AX) exp(jwx)dx
100 Aexp[-(A - jw)x)dx
A exp[-(A - jw)x) 1
</p>
<p>00
</p>
<p>-(A - jw) 0
</p>
<p>A
A . (exp[-(A - jw)oo) - 1) .
</p>
<p>-JW
</p>
<p>(11.18)</p>
<p/>
</div>
<div class="page"><p/>
<p>360 CHAPTER 11. EXPECTED VALUES
</p>
<p>But exp[-(&gt;. - jw)x] -+ 0 as x -+ 00 since&gt;' &gt; 0 and hence we have
</p>
<p>x
&cent;x(w) = &gt;. .
</p>
<p>-JW
(11.19)
</p>
<p>To find the moments using (11.18) we need to differentiate the characteristic function
</p>
<p>n times. Proceeding to do so
</p>
<p>and therefore
</p>
<p>d&cent;x(w)
</p>
<p>dw
= ~&gt;.(&gt;. _ jw)-l
</p>
<p>dw
&gt;'(-1)(&gt;' - jw)-2(_j)
</p>
<p>&gt;.( -1)(-2)(&gt;' - jw)-3( _j)2
</p>
<p>= &gt;.(-1)(-2) ... (-n)(&gt;. - jw)-n-l(_j)n
</p>
<p>= &gt;.jnn!(&gt;. _ jw)-n-l
</p>
<p>~ dn&cent;x(w) I
jn dwn w=o
</p>
<p>&gt;'n! (&gt;. - jw) -n-l L=o
n!
</p>
<p>&gt;.n
</p>
<p>which agrees with our earlier results (see (11.14)) .
</p>
<p>Moment formula only valid if moments exist
</p>
<p>Just because a PDF has a characterist ic function, and all do, does not mean that
</p>
<p>(11.18) can be applied. For example, the Cauchy PDF has the characteristic function
</p>
<p>(see Problem 11.40)
</p>
<p>&cent;x(w) = exp(-Iwl)
</p>
<p>(although the derivative does not exist at w = 0). However , as we have already
</p>
<p>seen , the mean does not exist and hence all higher order moments also do not exist.
</p>
<p>Thus, no moments exist at all for the Cauchy PDF.
</p>
<p>&amp;
The characteristic function has nearly the same properties as for a discrete random
</p>
<p>variable, namely</p>
<p/>
</div>
<div class="page"><p/>
<p>11.8. PROB., MOMENTS, AND THE CHEBYSHEV INEQUALITY 361
</p>
<p>1. The characteristic function always exists.
</p>
<p>2. The PDF can be recovered from the characteristic function by the inverse Fourier
</p>
<p>transform, which in this case is
</p>
<p>i" dw
px(x) = J-oo &lt;/&gt;x(w) exp(-jwx) 21f' (11.20)
</p>
<p>3. Convergence of a sequence of characteristic functions &lt;/&gt;c;) (w) for n = 1, 2, .. . to a
given characteristic function &lt;/&gt;(w) guarantees that the corresponding sequence
</p>
<p>of P DFs p ~ ) ( x ) for n = 1,2, . . . converges to p(x), where from (11.20)
</p>
<p>1
00 dw
</p>
<p>p(x) = &cent;(w) exp( -jwx) - .
-00 21f
</p>
<p>(See Problem 11.42 for an example.) This property is also essential for proving
</p>
<p>the central limit theorem described in Chapter 15.
</p>
<p>A slight difference from the characteristic function of a discrete random variable
</p>
<p>is that now &lt;/&gt;x(w) is not periodic in w. It does, however, have the usual proper-
</p>
<p>ties of the continuous-time Fourier transform [Jackson 1991]. A summary of the
</p>
<p>characteristic functions for the important PDFs is given in Table 11.1.
</p>
<p>11.8 P robability, Moments, and the Chebyshev Inequal-
</p>
<p>ity
</p>
<p>The mean and variance of a random variable indicate the average value and variabil-
</p>
<p>ity of the outcomes of a repeated experiment. As such, they summarize important
</p>
<p>information about the PDF. However, they are not sufficient to determine proba-
</p>
<p>bilities of events. For example, the PDFs
</p>
<p>px(x) = vh exp ( _ ~ x 2)
px(x) = ~ exp ( - h"lx l)
</p>
<p>(Gaussian)
</p>
<p>(Laplacian)
</p>
<p>both have E[X] = 0 (due to symmetry about x = 0) and var(X) = 1. Yet, the
probability of a given interval can be very different. Although the relationship
</p>
<p>between the mean and variance, and the probability of an event is not a direct one ,
</p>
<p>we can still obtain some information about the probabilities based on the mean and
</p>
<p>variance. In particular, it is possible to bound the probability or to be able to assert
</p>
<p>that
</p>
<p>P[IX - E [Xli &gt; ,] ~ B</p>
<p/>
</div>
<div class="page"><p/>
<p>362 CHAPTER 11. EXPECTED VALUES
</p>
<p>where B is a number less than one. This is especially useful if we only wish to
</p>
<p>make sure the probability is below a certain value, without explicitly having to find
</p>
<p>the probability. For example, if the probability of a speech signal of mean 0 and
</p>
<p>variance 1 exceeding a given magnitude, (see Section 10.10) is to be no more than
</p>
<p>1%, then we would be satisfied if we could determine a , so that
</p>
<p>P[lX - E[XJI &gt; ,] :::; 0.01.
</p>
<p>We now show that the probability for the event IX - E[XJI &gt; , can be bounded if
we know the mean and variance. Computation of the probability is not required and
</p>
<p>therefore the PDF does not need to be known. Estimating the mean and variance is
</p>
<p>much easier than the entire PDF (see Section 11.9). The inequality to be developed
</p>
<p>is called the Chebyshev inequality. Using the definition of the variance we have
</p>
<p>var(X) = i: (x - E[X])2pX (x )dx
r (x - E [X])2pX (x)dx + r (x - E[X])2pX (x )dx
</p>
<p>J{x:lx-E[XlI&gt;'Y} J{x:lx-E[XlI:S'Y}
</p>
<p>&gt; r (x - E[X])2pX (x )dx (omitted integral is nonnegative)
J{x:lx-E[XlI&gt;'Y}
</p>
<p>&gt; r ,2px (x )dx (since for each x, Ix - E[XJI &gt; ,)
J {x :lx-E[XlI&gt;'Y}
</p>
<p>,2 r px(x)dx
J{x:lx-E[XlI&gt;'Y}
</p>
<p>,2P[IX - E[X]I &gt;,]
so that we have the Chebyshev inequality
</p>
<p>P[lX - E[X] I &gt; ,] :::; var~X) ., (11.21)
Hence, the probability that a random variable deviates from its mean by more
</p>
<p>than, (in either direction) is less than or equal to var(X) /,2. This agrees with
our intuition in that the probability of an outcome departing from the mean must
</p>
<p>become smaller as the width of the PDF decreases or equivalently as the variance
</p>
<p>decreases. An example follows.
</p>
<p>Example 11.8 - Bounds for different PDFs
</p>
<p>Assuming E[X] = 0 and var(X) = 1, we have from (11.21)
</p>
<p>1
P[IXI &gt;,] :::; "2',
</p>
<p>If, = 3, then we have that P[IXI &gt; 3] :::; 1/9 ~ 0.11. This is a rather "loose"
bound in that if X "" N(O, 1), then the actual value of this probability is P[lXI &gt;</p>
<p/>
</div>
<div class="page"><p/>
<p>11.9. ESTIMATING THE MEAN AND VARIANCE 363
</p>
<p>3] = 2Q(3) = 0.0027. Hence , the actual probability is indeed less than or equal to
the bound of 0.11, but quite a bit less. In the case of a Laplacian random variable
</p>
<p>with mean 0 and variance 1, the bound is the same but the actual value is now
</p>
<p>P[lXI &gt; 3] = r-
3
</p>
<p>~ exp ( -hlxl) dx + I " ~ exp (-hlxl) dx
1-00 y2 13 y2
</p>
<p>(Xl 1
2 13 J2 exp ( -hx) dx (PDF is symmetric about x = 0)
</p>
<p>- exp (-hx) I:
exp ( -3h) = 0.0144.
</p>
<p>Once again the bound is seen to be correct but provides a gross overestimation of
</p>
<p>the probability. A graph of the Chebyshev bound as well as the actual probabilities
</p>
<p>of P[!X! &gt; ,] versus, is shown in Figure 11.6. The reader may also wish to consider
</p>
<p>4.5 53.5 4
</p>
<p>.. ...Chebyshev inequality ~ ~
&middot; .
&middot; .
&middot; .
</p>
<p>. . . . . . . . . . . . ..
</p>
<p>~0.7
</p>
<p>;D 0.6
ell
</p>
<p>.g 0.5
I-&lt;
</p>
<p>P-. 0.4
</p>
<p>03
Gaussian
</p>
<p>02 .., :, .. , , . .
</p>
<p>Laplacian ---f-'" ~
0.1 : , .- ~ .
</p>
<p>. ........-.-- ":"':.,--
</p>
<p>0.9
</p>
<p>OB
</p>
<p>Figure 11.6: Probabilities P[IXI &gt; ,] for Gaussian and Laplacian random variables
with zero mean and unity variance compared to Chebyshev inequality.
</p>
<p>what would happen if we used the Chebyshev inequality to bound P[!XI &gt; 0.5] if
X "J N(o,1).
</p>
<p>11.9 Estimating the Mean and Variance
</p>
<p>The mean and variance of a continuous random variable are estimated in exact ly
</p>
<p>the same way as for a discrete random variable (see Section 6.8). Assuming that we</p>
<p/>
</div>
<div class="page"><p/>
<p>364 CHAPTER 11. EXPECTED VALUES
</p>
<p>have the M outcomes {Xl, X2, . .. , XM} of a random variable X the mean estimate
</p>
<p>is
___ 1 M
</p>
<p>E[X] = -~X i
MLJ
</p>
<p>t=l
</p>
<p>(11.22)
</p>
<p>(11.23)
</p>
<p>and the variance estimate is
</p>
<p>var(X) E[X2j _ (E[Xj) 2
</p>
<p>= ~ t x1- (~ t x;)'
An example of the use of (11.22) was given in Example 2.6 for a N(o,1) random
variable. Some practice with the estimation of the mean and variance is provided
</p>
<p>in Problem 11.46.
</p>
<p>11.10 Real-World Example - Critical Software Testing
</p>
<p>Using Importance Sampling
</p>
<p>Computer software is a critical component of nearly every device used today. The
</p>
<p>failure of such software can range from being an annoyance, as in the outage of a
</p>
<p>cellular telephone, to being a catastrophe, as in the breakdown of the control system
</p>
<p>for a nuclear power plant. Testing of software is of course a prerequisite for reliable
</p>
<p>operation, but some events, although potentially catastrophic, will (hopefully) occur
</p>
<p>only rarely. Therefore, the question naturally arises as to how to test software that is
</p>
<p>designed to only fail once every 107 hours ( ~ 1400 years). In other words, although
</p>
<p>a theoretical analysis might predict such a low failure rate, there is no way to test
</p>
<p>the software by running it and waiting for a failure. A technique that is often used in
</p>
<p>other fields to test a system is to "stress" the system to induce more frequent failures,
</p>
<p>say by a factor of 105 , then estimate the probability of failure per hour, and finally
</p>
<p>readjust the probability for the increased stress factor. An analogous approach
</p>
<p>can be used for highly reliable software if we can induce a higher failure rate and
</p>
<p>then readjust our failure probability estimate by the increased factor. A proposed
</p>
<p>method to do this is to stress the software to cause the probability of a failure to
</p>
<p>increase [Hecht and Hecht 2000]. Conceivably we could do this by inputting data
</p>
<p>to the software that is suspected to cause failures but at a much higher rate than is
</p>
<p>normally encountered in practice. This means that if T is the time to failure, then
</p>
<p>we would like to replace the PDF of T so that P[T &gt; ,] increases by a significant
factor . Then, after estimating this probability by exercising the software we could
</p>
<p>adjust the estimate back to the original unstressed value. This probabilitic approach
</p>
<p>is called importance sampling [Rubinstein 1981].
</p>
<p>As an example of the use of importance sampling, assume that X is a continuous
</p>
<p>random variable and we wish to estimate P[X &gt; ,]. As usual, we could generate</p>
<p/>
</div>
<div class="page"><p/>
<p>(11.24)
</p>
<p>11.10. REAL-WORLD EXAMPLE - CRITICAL SOFTWARE TESTING 365
</p>
<p>realizations of X , count the number that exceed 'Y, and then divide this by the
</p>
<p>total number of realizations. But what if the probability sought is 1O-7? Then we
</p>
<p>would need about 109 realizations to do this. As a specific example, suppose that
</p>
<p>X ,....., N(O ,l) , although in practice we would not have knowledge of the PDF at
</p>
<p>our disposal, and that we wish to estimate P[X &gt; 5] based on observed realization
values. The true probability is known to be Q(5) = 2.86 x 10-7 . The importance
sampling approach first recognizes that the desired probability is given by
</p>
<p>and is equivalent to
</p>
<p>1
00 _1_ exp (- ! x 2 )
</p>
<p>I = ,j'j;i () 2 PX'(x)dx
s Px' x
</p>
<p>where PXI(X) is a more suitable PDF. By "more suitable" we mean that its prob-
</p>
<p>ability of X' &gt; 5 is larger, and therefore, generating realizations based on it will
produce more occurrences of the desired event. One possibility is X' ,....., exp(l) or
</p>
<p>px,(x) = exp( -x)u(x) for which P[X &gt; 5] = exp(-5) = 0.0067. Using this new
PDF we have the desired probability
</p>
<p>1
00 _1_ exp (_!x2 )
</p>
<p>I = ,j'j;i ( ~ exp( - x)dx
s exp -x
</p>
<p>or using the indicator function, this can be written as
</p>
<p>I =100 I(s,oo) (x) vb: exp ( _~ x2 + x) pXI(x)dx.
, ~
</p>
<p>v
</p>
<p>g(x)
</p>
<p>Now the desired probability can be interpreted as E[g(X')]' where X' ,....., expfl). To
</p>
<p>estimate it using a Monte Carlo computer simulation we first generate M realizations
</p>
<p>of an exp(l) random variable and then use as our estimate
</p>
<p>1 M
i = MLg(xd
</p>
<p>i=1
</p>
<p>= ~ t I(s,oo)(xd ~exp (_!x; + Xi) .
i=1 Y 27r 2 ,
</p>
<p>v
</p>
<p>weight with value &laquo; 1
for Xi &raquo; 5
</p>
<p>The advantage of the importance sampling approach is that the realizations whose
</p>
<p>values exceed 5, which are the ones contributing to the sum, are much more proba-
</p>
<p>ble. In fact, as we have noted P[X' &gt; 5] = 0.0067 and therefore with N = 10,000</p>
<p/>
</div>
<div class="page"><p/>
<p>366 CHAPTER 11. EXPECTED VALUES
</p>
<p>realizations we would expect about 67 realizations to contribute to the sum. Con-
</p>
<p>trast this with a N(O,l) random variable for which we would expect NQ(5) =
</p>
<p>(104)(2.86 x 10-7) ~ 0 realizations to exceed 5. The new PDF px! is called the
</p>
<p>importance function and hence the generation of realizations from this PDF, which
</p>
<p>is also called sampling from the PDF, is termed importance sampling. As seen from
</p>
<p>(11.24), its success requires a weighting factor that downweights the counting of
</p>
<p>threshold exceedances.
</p>
<p>In software testing the portions of software that are critical to the operation of
</p>
<p>the overall system would be exercised more often than in normal operation, thus
</p>
<p>effect ively replacing the operational PDF or px by the importance function PDF
or Px', The ratio of these two would be needed as seen in (11.24) to adjust the
</p>
<p>weight for each incidence of a failure. This ratio would also need to be estimated in
</p>
<p>practice. In this way a good estimate of the probability of failure could be obtained
</p>
<p>by exercising the software a reasonable number of times with different inputs. Oth-
</p>
<p>erwise, the critical software might not exhibit a failure a sufficient number of times
</p>
<p>to estimate its probability.
</p>
<p>As a numerical example, if X' "" exp(l), we can generate realizations using the
</p>
<p>inverse probability transformation method (see Section 10.9) via X' = -In(l - U) ,
</p>
<p>where U "" U(O, 1). A MATLAB computer program to estimate I is given below.
</p>
<p>rand('state',O) % sets random number generator to
</p>
<p>% initial value
</p>
<p>M=10000;gamma=5;% change Mfor different estimates
u=rand(M,l); % generates MU(O,l) realizations
</p>
<p>x=-log(l-u); % generates M exp(l) realizations
k=O;
</p>
<p>for i=l:M % computes estimate of P[X&gt;gamma]
</p>
<p>if x(i&raquo;gamma
</p>
<p>k=k+l;
</p>
<p>y ( k , 1 ) = ( 1 / s q r t ( 2 * p i ) ) * e x p ( - 0 . 5 * x ( i ) ~ 2 + x ( i ) ) ; % computes weights
</p>
<p>% for estimate
end
</p>
<p>end
</p>
<p>Qest=sum(y)/M % final estimate of P[X&gt;gamma]
</p>
<p>The results are summarized in Table 11.2 for different values of M , along with the
</p>
<p>true value of Q(5). Also shown are the number of times 'Y was exceeded. Without
</p>
<p>the use of importance sampling the number of exceedances would be expected to be
</p>
<p>MQ(5) ~ 0 in all cases.</p>
<p/>
</div>
<div class="page"><p/>
<p>REFERENCES
</p>
<p>M Estimated P[X &gt; 5]
</p>
<p>1.11 X 10-7
</p>
<p>2.96 X 10-7
</p>
<p>2.51 X 10-7
</p>
<p>2.87 X 10- 7
</p>
<p>True P[X &gt; 5]
</p>
<p>2.86 X 10-7
</p>
<p>2.86 X 10-7
</p>
<p>2.86 X 10-7
</p>
<p>2.86 X 10-7
</p>
<p>Exceedances
</p>
<p>4
</p>
<p>66
</p>
<p>630
</p>
<p>6751
</p>
<p>367
</p>
<p>Table 11.2: Importance sampling approach to estimation of small probabilities.
</p>
<p>References
</p>
<p>Hecht, M., H. Hecht, "Use of Importance Sampling and Related Techniques to
</p>
<p>Measure Very High Reliability Software," 2000 IEEE Aerospace Conference
</p>
<p>Proc., Vol. 4, pp. 533-546.
</p>
<p>Jackson, L.B., Signals, Systems, and Transforms, Addison-Wesley, Reading, MA,
</p>
<p>1991.
</p>
<p>Johnson, N.L., S. Kotz, N. Balakrishnan, Continuous Univariate Distributions,
</p>
<p>Vol. 1, see pp. 456-459 for moments, John Wiley &amp; Sons, New York, 1994.
</p>
<p>Parzen, E., Modern Probability Theory and its Applications, John Wiley &amp; Sons,
</p>
<p>New York, 1960.
</p>
<p>Rubinstein, R.Y., Simulation and the Monte Carlo Method, John Wiley &amp; Sons,
</p>
<p>New York, 1981.
</p>
<p>Problems
</p>
<p>11.1 C:..:.,) (f) The block shown in Figure 11.7 has a mass of 1 kg. Find the center
of mass for the block, which is the point along the x-axis where the block
</p>
<p>could be balanced (in practice the point would also be situated in the depth
</p>
<p>direction at 1/2).
</p>
<p>_ "-_ x
</p>
<p>Figure 11.7: Block for Problem 11.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>368 CHAPTER 11. EXPECTED VALUES
</p>
<p>11.2 (t) Prove that if the PDF is symmetric about a point x = a, which is to say
that it satisfies px(a+u) = px(a-u) for all-oo &lt; u &lt; 00, then the mean will
be a. Hint: Write the integral I ~ o o xpx(x)dx as I~oo xpx(x)dx+ Iaoo xpx(x)dx
and then let u = x - a in the first integral and u = a - x in the second integral.
</p>
<p>11.3 (c) Generate and plot 50 realizations of a Cauchy random variable. Do so by
</p>
<p>using the inverse probability integral transformation method. You should be
</p>
<p>able to show that X = tan(7r(U - 1/2&raquo;, where U '" U(O, 1), will generate the
Cauchy realizations.
</p>
<p>11.4 (c) In this problem we show via a computer simulation that the mean of
</p>
<p>a truncated Cauchy PDF exists and is equal to zero. A truncated Cauchy
</p>
<p>random variable is one in which the realizations of a Cauchy PDF are set to
</p>
<p>x = Xmax if x &gt; Xmax and x = -Xmax if x &lt; -Xm ax ' Generate realizations
of this random variable with Xmax = 50 and plot the sample mean versus the
</p>
<p>number of realizations. What does the sample mean converge to?
</p>
<p>11.5 (t) Prove that the best prediction of the outcome of a continuous random
</p>
<p>variable is its mean. Best is to be interpreted as the value that minimizes the
</p>
<p>mean square error mse(b) = E[(X - b)2].
</p>
<p>11.6 (t) An even function is one for which g(-x) = g(x) , as for example cos(x).
An odd function is one for which g( -x) = -g(x), as for example sin(x). First
prove that I ~ o o g(x)dx = 2 Iooo g(x)dx if g(x) is even and that I ~ o o g(x)dx = 0
if g(x) is odd. Next, prove that if px(x) is even, then E[X] = 0 and also that
Iooo px(x)dx = 1/2.
</p>
<p>11.7 (f) Many integrals encountered in probability can be evaluated using integra-
tion by parts. This useful formula is
</p>
<p>IUdV = UV - IV dU
where U and V are functions of x. As an example, if we wish to evaluate
</p>
<p>I xexp(ax)dx, we let U = x and dV = exp(ax)dx. The function U is easily
differentiated to yield dU = dx and the differential dV is easily integrated to
</p>
<p>yield V = (l/a) exp(ax). Continue the derivation to determine the integral of
</p>
<p>the function x exp (ax).
</p>
<p>11.8 (f) Find the mean for a uniform PDF. Do so by first using the definition and
</p>
<p>then rederive it using the results of Problem 11.2.
</p>
<p>11.9 (t) Consider a continuous random variable that can take on values Xmin ~
x ~ Xm ax ' Prove that the expected value of this random variable must satisfy
</p>
<p>Xmin ~ E[X] ~ Xm ax' Hint: Use the fact that if M i ~ g(x) ~ M 2 , then
</p>
<p>Mia ~ I: g(x)dx ~ M 2b.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 369
</p>
<p>11.10 C..:...) (w) The signal-to-noise ratio (SNR) of a random variable quantifies the
accuracy of a measurement of a physical quantity. It is defined as E 2 [X]/var(X)
</p>
<p>and is seen to increase as the mean, which represents the true value, increases
</p>
<p>and also as the variance, which represents the power of the measurement error,
</p>
<p>i.e., X - E [X], decreases. For example, if X rv N(I-", (J'2), then SNR = 1-"2 /(J'2 .
</p>
<p>Determine the SNR if the measurement is X = A + U, where A is the true
value and U is the measurement error with U rv U( -1/2,1/2). For an SNR of
</p>
<p>1000 what should A be?
</p>
<p>11.11 Coo:,,} (w) A toaster oven has a failure time that has an exponential PDF. If
the mean time to failure is 1000 hours, what is the probability that it will not
</p>
<p>fail for at least 2000 hours?
</p>
<p>11.12 (w) A bus always arrives late. On the average it is 10 minutes late. If the
lateness time is an exponential random variable, determine the probability
</p>
<p>that the bus will be less than 1 minute late.
</p>
<p>11.13 (w) In Section 1.3 we described the amount of time an office worker spends
</p>
<p>on the phone in a 10-minute period. From Figure 1.5 what is the average
</p>
<p>amount of time he spends on the phone?
</p>
<p>11.14 c.:..:...) (f) Determine the mean o f a x ~ PDF. See Chapter 10 for the definition
of this PDF.
</p>
<p>11.15 (f) Determine the mean of an Erlang PDF using the definition of expected
</p>
<p>value. See Chapter 10 for the definition of this PDF.
</p>
<p>11.16 (f) Determine the mean of a Rayleigh PDF using the definition of expected
</p>
<p>value. See Chapter 10 for the definition of this PDF.
</p>
<p>11.17 (w) The mode of a PDF is the value of x for which the PDF is maximum. It
</p>
<p>can be thought of as the most probable value of a random variable (actually
</p>
<p>most probable small interval). Find the mode for a Gaussian PDF and a
</p>
<p>Rayleigh PDF. How do they relate to the mean?
</p>
<p>11.18 (f) Indicate on the PDFs shown in Figures 10.7-10.13 the location of the
mean value.
</p>
<p>11.19 ( ~ ) (w) A dart is thrown at a circular dartboard. If the distance from the
</p>
<p>bullseye is a Rayleigh random variable with a mean value of 10, what is the
</p>
<p>probability that the dart will land within 1 unit of the bullseye?
</p>
<p>11.20 (f) For the random variables described in Problems 2.8-2.11 what are the
</p>
<p>means? Note that the uniform random variable is U(O , 1) and the Gaussian
</p>
<p>random variable is N(O , 1).</p>
<p/>
</div>
<div class="page"><p/>
<p>370 CHAPTER 11. EXPECTED VALUES
</p>
<p>11.21 C..:.,) (w) In Problem 2.14 it was asked whether the mean of v1J, where U '"
U(O, 1), is equal to Jmean of U. There we relied on a computer simulation to
</p>
<p>answer the question. Now prove or disprove this equivalence.
</p>
<p>11.22 L.:.,) (w) A sinusoidal oscillator outputs a waveform s(t) = cos(27rFot + &cent;),
where t indicates time, Fo is the frequency in Hz, and &cent; is a phase angle
that varies depending upon when the oscillator is turned on. If the phase is
</p>
<p>modeled as a random variable with &cent; '" U(O, 27r), determine the average value
</p>
<p>of s(t) for a given t = to. Also, determine the average power, which is defined
</p>
<p>as E[s2(t)] for a given t = to. Does this make sense? Explain your results.
</p>
<p>11.23 (f) Determine E[X2] for a N(/-L, (T2) random variable.
</p>
<p>11.24 (f) Determine E[(2X + 1)2] for a N(/-L, (T2) random variable.
</p>
<p>11.25 (f) Determine the mean and variance for the indicator random variable IA(X)
as a function of P[A].
</p>
<p>11.26 C..:...) (w) A half-wave rectifier passes a zero or positive voltage undisturbed
but blocks any negative voltage by outputting a zero voltage. If a noise sample
</p>
<p>with PDF N(o,(T2) is input to a half-wave rectifier, what is the average power
at the output? Explain your result.
</p>
<p>11.27 C..:...) (w) A mixed PDF is given as
</p>
<p>1 1 (1)px(x) = -2b"(x) + ~exp --2x2 u(x).
27r(T2 2(T
</p>
<p>What is E[X2 ] for this PDF? Can this PDF be interpreted physically? Hint:
</p>
<p>See Problem 11.26.
</p>
<p>11.28 (t) In this problem we derive an alternative formula for the mean of a non-
negative random variable. A more general formula exists for random variables
</p>
<p>that can take on both positive and negative values [Parzen 1960]. If X can
only take on values x ~ 0, then
</p>
<p>E[X] = 100 (1 - Fx(x)) dx.
First verify that this formula holds for X '" exp[A). To prove that the formula
</p>
<p>is true in general, we use integration by parts (see Problem 11.7) as follows.
</p>
<p>E[X] = 100 (1 - Fx(x)) dx
100100 p x ( t ) d t ~ .
O ~ d V
</p>
<p>u</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 371
</p>
<p>Finish the proof by using limx-too x Jxoo pX (t)dt = 0, which must be true if the
expected value exists (see if this holds for X rv exp(&gt;..)) .
</p>
<p>11.29 (t) The standard deviation a of a Gaussian PDF can be interpreted as the
distance from the mean at which the PDF curve goes through an inflection
</p>
<p>point. This means that at the points x = J.L &plusmn; a the second derivative of px(x)
is zero. The curve then changes from being concave (shaped like a n) to being
</p>
<p>convex (shaped like a U). Show that the second derivative is zero at these
</p>
<p>points.
</p>
<p>11.30 (...:.,:.,) (w) The office worker described in Section 1.3 will spend an average of
7 minutes on the phone in any lO-minute interval. However, the probability
</p>
<p>that he will spend exactly 7 minutes on the phone is zero since the length of
</p>
<p>this interval is zero. If we wish to assert that he will spend between Tmin and
</p>
<p>T max minutes on the phone 95% of the time, what should Tmin and T max be?
</p>
<p>Hint: There are multiple solutions - choose any convenient one.
</p>
<p>11.31 (w) A group of students is found to weigh an average of 150 lbs. with a stan-
dard deviation of 30 lbs. If we assume a normal population (in the probabilis-
</p>
<p>tic sense!) of students, what is the range of weights for which approximately
</p>
<p>99.8% of the students will lie? Hint: There are multiple solutions - choose
</p>
<p>any convenient one.
</p>
<p>11.32 (w) Provide a counterexample to disprove that var(gr(X) + g2(X)) =
var(gr(X)) + var(g2(X)) in general.
</p>
<p>11.33 (w) The SNR of a random variable was defined in Problem 11.10. Determine
the SNR for exponential random variable and explain why it doesn't increase
</p>
<p>as the mean increases. Compare your results to a N(J.L ,( 2 ) random variable
and explain.
</p>
<p>11.34 (f) Verify the mean and variance for a Laplacian random variable given in
Table 11.1.
</p>
<p>11.35 (...:.,:.,) (f) Determine E[X3 ] if X rv N(J.L, ( 2 ) . Next find the third central
moment.
</p>
<p>11.36 (f) An example of a Gaussian mixture PDF is
</p>
<p>1 1 [1 2] 1 1 [1 2]px(x) = -- exp --(x -1) +-- exp --(x + 1) .
2 J21r 2 2 J21r 2
</p>
<p>Determine its mean and variance.
</p>
<p>11.37 (t) Prove that if a PDF is symmetric about x = 0, then all its odd-order
moments are zero.</p>
<p/>
</div>
<div class="page"><p/>
<p>372 CHAPTER 11. EXPECTED VALUES
</p>
<p>11.38 C:...:.... ) (f) For a Laplacian PDF with (12 = 2 determine all the moments. Hint:
Let
</p>
<p>1 1 (1 1)
w2 + 1 = 2j w - j - w + j .
</p>
<p>11.39 (f) If X rv N(O, (12), determine E[X2 ] using the characteristic function ap-
proach.
</p>
<p>11.40 (t) To determine the characteristic function of a Cauchy random variable we
must evaluate the integral
</p>
<p>1
00 1
</p>
<p>( 2) exp(jwx )dx.
-00 7f 1 + x
</p>
<p>A result from Fourier transform theory called the duality theorem asserts that
</p>
<p>the Fourier transform and inverse Fourier transform are nearly the same if we
</p>
<p>replace x by wand w by x. As an example, for a Laplacian PDF with (12 = 2
</p>
<p>we have from Table 11.1 that
</p>
<p>1
00 100 1 1
</p>
<p>px(x) exp(jwx)dx = - exp(-Ixl) exp(jwx)dx = --2'-00 -00 2 1 + w
The inverse Fourier transform relationship is therefore
</p>
<p>/
</p>
<p>00 1 dw 1
1 2 exp( -jwx)-2 = - exp(-Ixl).
</p>
<p>-00 + w 7f 2
</p>
<p>Use the latter integral, with appropriate modifications (note that x and ware
</p>
<p>just variables which we can redefine as desired), to obtain the characteristic
</p>
<p>function of a Cauchy random variable.
</p>
<p>11.41 (f) If the characteristic function of a random variable is
</p>
<p>find the PDF. Hint: Recall that when we convolve two functions together the
</p>
<p>Fourier transform of the new function is the product of the individual Fourier
</p>
<p>transforms. Also, see Table 11.1 for the characteristic function of a U (-1, 1)
</p>
<p>random variable.
</p>
<p>11.42 C:...:....) (w) If x(n) rv N(f-L, lin), determine the PDF of the limiting random
variable X as n -+ 00. Use characteristic functions to do so.
</p>
<p>11.43 (f) Find the mean and variance ofaXJv random variable using the charac-
teristic function.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 373
</p>
<p>11.44 C:..:.-) (f) The probability that a random variable deviates from its mean by
an amount, in either direction is to be less than or equal to 1/2. What should
</p>
<p>, be?
</p>
<p>11.45 (f) Determine the probability that IXI &gt; , if X '" U[-a ,a]. Next compare
these results to the Chebyshev bound for a = 2.
</p>
<p>11.46 C:.:.-) (c) Estimate the mean and variance of a Rayleigh random variable with
a 2 = 1 using a computer simulation. Compare your estimated results to the
theoretical values.
</p>
<p>11.47 (c) Use the importance sampling method described in Section 11.10 to de-
</p>
<p>termine Q(7). If you were to generate M realizations of a N(O, 1) random
variable and count the number that exceed , = 7 as is usually done to esti-
</p>
<p>mate a right-tail probability, what would M have to be (in terms of order of
</p>
<p>magnitude)?</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix llA
</p>
<p>Partial Proof of Expected Value
</p>
<p>of Function of Continuous
</p>
<p>Random Variable
</p>
<p>For simplicity assume that Y = g(X) is a continuous random variable with PDF
</p>
<p>py(y) (having no impulses) . Also, assume that y = g(x) is monotonically increasing
</p>
<p>so that it has a single solution to the equation y = g(x) for all y as shown in Figure
</p>
<p>llA.1. Then
</p>
<p>g(x)
</p>
<p>--f-----r-- --f-- - - x
</p>
<p>Figure llA.1: Monotonically increasing function used to der ive E [g(X )].
</p>
<p>E[Y] = 1:ypy(y)dy
1:YPX(g-l(y)) Id9~~(Y) Idy (from (10.30).
</p>
<p>Next change variables from y to x using x = g-l(y) . Since we have assumed that
</p>
<p>g(x) is monotonically increasing, the limits for y of &plusmn;oo also become &plusmn;oo for x.</p>
<p/>
</div>
<div class="page"><p/>
<p>376 CHAPTER 11. EXPECTED VALUES
</p>
<p>(g is monotonically increasing,
</p>
<p>implies &laquo;: is monotonically increasing,
implies derivative is positive)
</p>
<p>Then, since x = g-l(y), we have that YPX(g-l(y)) becomes g(x)px(x) and
</p>
<p>Idg~~(y) Idy = dg~~(y) dy
</p>
<p>dx
= dydy = dx
</p>
<p>from which (11.10) follows. The more general result for nonmonotonic functions
</p>
<p>follows along these lines.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 12
</p>
<p>Multiple Continuous Random
</p>
<p>Variables
</p>
<p>12.1 Introduction
</p>
<p>In Chapter 7 we discussed multiple discrete random variables. We now proceed to
</p>
<p>parallel that discussion for multiple continuous random variables. We will consider
</p>
<p>in this chapter only the case of two random variables, also called bivariate random
</p>
<p>variables, with the extension to any number of continuous random variables to be
</p>
<p>presented in Chapter 14. In describing bivariate discrete random variables, we used
</p>
<p>the example of height and weight of a college student. Figure 7.1 displayed the
</p>
<p>probabilities of a student having a height in a given interval and a weight in a given
</p>
<p>interval. For example, the probability of having a height in the interval [5'8" ,6']
</p>
<p>and a weight in the interval [160,190] lbs. is 0.14 as listed in Table 4.1 and as seen
</p>
<p>in Figure 7.1 for the values of H = 70 inches and W = 175 lbs. For physical
measurements such as height and weight , however, we would expect to observe a
</p>
<p>continuum of values. As such, height and weight are more appropriately modeled
</p>
<p>by multiple continuous random variables. For example, we might have a population
</p>
<p>of college students, all of whose heights and weights lie in the intervals 60 ::; H ::; 80
</p>
<p>inches and 100 ::; W ::; 250 lbs. Therefore, the continuous random variables (H, W)
</p>
<p>would take on values in the sample space
</p>
<p>SH,W = {(h, w) : 60 ::; h ::; 80,100::; w ::; 250}
</p>
<p>which is a subset of the plane, i.e., R2 &bull; We might wish to determine probabilities
</p>
<p>such as P[61 ::; H ::; 67.5,98.5 ::; W ::; 154], which cannot be found from Figure 7.1.
</p>
<p>In order to compute such a probability we will define a joint PDF for the continuous
</p>
<p>random variables Hand W. It will be a two-dimensional function of hand w. In the
</p>
<p>case of a single random variable we needed to integrate to find the area under the
</p>
<p>PDF as the desired probability. Now integration ofthe joint PDF, which is a function
</p>
<p>of two variables, will produce the probability. However, we will now be determining</p>
<p/>
</div>
<div class="page"><p/>
<p>378 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>the volume under the joint PDF. All our concepts for a single continuous random
</p>
<p>variable will extend to the case of two random variables. Computationally, however,
</p>
<p>we will encounter more difficulty since two-dimensional integrals, also known as
</p>
<p>double integrals, will need to be evaluated. Hence, the reader should be acquainted
</p>
<p>with double integrals and their evaluation using iterated integrals.
</p>
<p>12.2 Summary
</p>
<p>The concept of jointly distributed continuous random variables is introduced in Sec-
</p>
<p>tion 12.3. Given the joint PDF the probability of any event defined on the plane
</p>
<p>is given by (12.2). The standard bivariate Gaussian PDF is given by (12.3) and is
</p>
<p>plotted in Figure 12.9. The concept of constant PDF contours is also illustrated
</p>
<p>in Figure 12.9. The marginal PDF is found from the joint PDF using (12.4). The
</p>
<p>joint CDF is defined by (12.6) and is evaluated using (12.7) . Its properties are
</p>
<p>listed in P12.1-P12.6. To obtain the joint PDF from the joint CDF we use (12.9).
</p>
<p>Independence of jointly distributed random variables is defined by (12.10) and can
</p>
<p>be verified by the factorization of either the PDF as in (12.11) or the CDF as in
</p>
<p>(12.12) . Section 12.6 addresses the problem of determining the PDF of a function
</p>
<p>of two random variables-see (12.13), and that of determining the joint PDF of
</p>
<p>a function which maps two random variables into two new random variables. See
</p>
<p>(12.18) for a linear transformation and (12.22) for a nonlinear transformation. The
</p>
<p>general bivariate Gaussian PDF is defined in (12.24) and some useful properties
</p>
<p>are discussed in Section 12.7. In particular, Theorem 12.7.1 indicates that a linear
</p>
<p>transformation of a bivariate Gaussian random vector produces another bivariate
</p>
<p>Gaussian random vector, although with different means and covariances. Exam-
</p>
<p>ple 12.14 indicates how a bivariate Gaussian random vector may be transformed to
</p>
<p>one with independent components. Also, a formula for computation of the expected
</p>
<p>value of a function of two random variables is given as (12.28) . Section 12.9 discusses
</p>
<p>prediction of a random variable from the observation of a second random variable
</p>
<p>while Section 12.10 summarizes the joint characteristic function and its properties.
</p>
<p>In particular, the use of (12.47) allows the determination of the PDF of the sum
</p>
<p>of two continuous and independent random variables. It is used to prove that two
</p>
<p>independent Gaussian random variables that are added together produce another
</p>
<p>Gaussian random variable in Example 12.15. Section 12.11 shows how to simulate
</p>
<p>on a computer a random vector with any desired mean vector and covariance ma-
</p>
<p>trix by using the Cholesky decomposition of the covariance matrix-see (12.53).
</p>
<p>If the desired random vector is bivariate Gaussian, then the procedure provides a
</p>
<p>general method for generating Gaussian random vectors on a computer. Finally, an
</p>
<p>application to optical character recognition is described in Section 12.12.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3. JOINTLY DISTRIBUTED RANDOM VARIABLES
</p>
<p>12.3 Jointly Distributed Random Variables
</p>
<p>379
</p>
<p>We consider two continuous random variables that will be denoted by X and Y. As
</p>
<p>alluded to in the introduction, they represent the functions that map an outcome s
of an experiment to a point in the plane. Hence , we have that
</p>
<p>[
X(S)] [ x ]
Y(s) y
</p>
<p>for all s E S . An example is shown in Figure 12.1 in which the outcome of a dart
toss S, which is a point within a unit radius circular dartboard, is mapped into a
</p>
<p>point in the plane, which is within the unit circle. The random variables X and Y
</p>
<p>y
</p>
<p>X(S) ,Y(s)
</p>
<p>S
</p>
<p>---l------::,.f----+~ x
</p>
<p>Figure 12.1: Mapping of the outcome of a thrown dart to the plane (example of
</p>
<p>jointly continuous random variables).
</p>
<p>are said to be jointly distributed continuous random variables. As before, we will
</p>
<p>denote the random variables as (X,Y) or [X Y]T , in either case referring to them as
</p>
<p>a random vector. Note that a different mapping would result if we chose to represent
</p>
<p>the point in SX,Y in polar coordinates (r,O). Then we would have
</p>
<p>SR,8 = {(r,O) : 0::; r::; 1,0::; 0 &lt; 27l"}.
</p>
<p>This is a different random vector but is of course related to (X, Y). Depending
</p>
<p>upon the shape of the mapped region in the plane, it may be more convenient to
</p>
<p>use either rectangular coordinates or polar coordinates for probability calculations
</p>
<p>(see also Problem 12.1).
</p>
<p>Typical outcomes of the random variables are shown in Figure 12.2 as points in
</p>
<p>SX,Y for two different players. In Figure 12.2a 100 outcomes for a novice dart player
</p>
<p>are shown while those for a champion dart player are displayed in Figure 12.2b. We
</p>
<p>might be interested in the probability that ";X2 + y2 ::; 1/4, which is the event
that a bullseye is attained. Now our event of interest is a two-dimensional region as
</p>
<p>opposed to a one-dimensional interval for a single continuous random variable. In</p>
<p/>
</div>
<div class="page"><p/>
<p>380 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>o 0.5
X
</p>
<p>-1 -0.5
</p>
<p>-1
</p>
<p>0.5
</p>
<p>. . .
-0.5 .. , :- , :- , . .
</p>
<p>. . . . .
</p>
<p>-1 -0.5 0 0.5
X
</p>
<p>- 1 ~ . .-....._.-:-
</p>
<p>;l) 0 &middot; &middot;
</p>
<p>(a) Novice (b) Champion
</p>
<p>Figure 12.2: Typical outcomes for novice and champion dart player.
</p>
<p>the case of the novice dart player the dart is equally likely to land anywhere in the
</p>
<p>unit circle and hence the probability is
</p>
<p>P[bullseye]
</p>
<p>=
</p>
<p>Area of bullseye
</p>
<p>Total area of dartboard
</p>
<p>7f(1/4)2 1
</p>
<p>7f(1)2 16
</p>
<p>However , for a champion dart player we see from Figure 12.2b that the probability of
</p>
<p>a bullseye is much higher. How should we compute this probability? For the novice
</p>
<p>dart player we can interpret the probability calculation geometrically as shown in
</p>
<p>Figure 12.3 as the volume of the inner cylinder since
</p>
<p>P[bullseye]
1
</p>
<p>7f(1/4)2 X -
7f
</p>
<p>= Area of bullseye x
, "....
</p>
<p>Area of event
</p>
<p>1
</p>
<p>7f
'-v-"
</p>
<p>Height
</p>
<p>If we define a function
</p>
<p>{
</p>
<p>I x 2 + y2 ~ 1
px,y(x ,y) = 07r
</p>
<p>otherwise
(12.1)
</p>
<p>then this volume is also given by
</p>
<p>P[A] = Jipx,Y(x ,Y)dXdY (12.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3. JOINTLY DISTRIBUTED RANDOM VARIABLES
</p>
<p>PX ,y(x,y)
</p>
<p>SX,Y
</p>
<p>bullseye = A
</p>
<p>y
</p>
<p>381
</p>
<p>Figure 12.3: Geometric interpretation of bullseye probability calculation for novice
</p>
<p>dart thrower.
</p>
<p>since then
</p>
<p>P[A]
</p>
<p>1
</p>
<p>16
</p>
<p>In analogy with the definition of the PDF for a single random variable X , we define
</p>
<p>PX ,y(x, y) as the joint PDF of X and Y. For this example, it is given by (12.1) and
</p>
<p>is used to evaluate the probability that (X, Y) lies in a given region A by (12.2).
</p>
<p>The region A can be any subset of the plane. Note that in using (12.2) we are
</p>
<p>determining the volume under PX,Y, hence the need for a double integral. Another
</p>
<p>example follows.
</p>
<p>Example 12.1 - Pyramid-like joint PDF
</p>
<p>A joint PDF is given by
</p>
<p>( ) {
4(1 -12x - 11)(1 -12y - 11)
</p>
<p>PX ,Y x ,y = 0
o ~ x ~ 1, 0 ~ y ~ 1
otherwise.
</p>
<p>We wish to first verify that the PDF integrates to one. Then, we consider the
</p>
<p>evaluation of P[I/4 ~ X ~ 3/4, 1/4 ~ Y ~ 3/4J. A three-dimensional plot of the
</p>
<p>PDF is shown in Figure 12.4 and appears pyramid-like. Since it is often difficult to
</p>
<p>visualize the PDF in 3-D, it is helpful to plot the contours of the PDF as shown</p>
<p/>
</div>
<div class="page"><p/>
<p>382 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>...
</p>
<p>. .
</p>
<p>xo 0
</p>
<p>. . . . .
</p>
<p>y
</p>
<p>- ~ -
</p>
<p>. . . . .
</p>
<p>..... .. . . . . - _. . ,
</p>
<p>o
1
</p>
<p>Figure 12.4: Three-dimensional plot of joint PDF.
</p>
<p>0.9 . . .. ..
</p>
<p>0.8 . . . .:..
</p>
<p>0 .7
</p>
<p>0.6
</p>
<p>;;Jl0.5
</p>
<p>0.4 . . .
</p>
<p>0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
</p>
<p>X
</p>
<p>Figure 12.5: Contour plot of joint PDF.
</p>
<p>in Figure 12.5. As seen in the contour plot (also called a topographical map) the
</p>
<p>innermost contour consists of all values of (x ,y) for which PX,Y(x, y) = 3.5. This
contour is obtained by slicing the solid shown in Figure 12.4 with a plane parallel
</p>
<p>to the x-y plane and at a height of 3.5 and similarly for the other contours. These
</p>
<p>contours are called contours of constant PDF.
</p>
<p>To verify that PX,Y is indeed a valid joint PDF, we need to show that the volume
</p>
<p>under the PDF is equal to one. Since the sample space is SX,Y = {(x ,y) : 0 ~ x ~</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3. JOINTLY DISTRIBUTED RANDOM VARIABLES
</p>
<p>1, 0 ~ y ~ I} we have that
</p>
<p>P[SX ,Y] = ~1 ~1 4(1-12x -11)(1-12y -ll)dxdy
</p>
<p>= ~1 2(1 -12x _ 11)dx ~1 2(1 -12y - 11)dy.
</p>
<p>383
</p>
<p>The two definite integrals are seen to be identical and hence we need only evaluate
</p>
<p>one of these. But each integral is the area under the function shown in Figure 12.6a
</p>
<p>which is easily found to be 1. Hence, P[SX,Y] = 1&middot;1 = 1, verifying that PX,Y is a
</p>
<p>&middot; . . . .
2 ; ; ; ; .
</p>
<p>&middot; . . . .&middot; . . . .&middot; . . . .&middot; . . . .&middot; . . . .&middot; . . . .
1.5 ; ;. : .; : .
</p>
<p>H........
01
</p>
<p>. . .
0.5 .. .. .. :.. . . . . .:--- ... .. :-- . . . ...;. . .. ... : . .. .. .
</p>
<p>&middot; . . . .&middot; . . . .&middot; . . . .&middot; . . . .&middot; . . . .&middot; . . .o '-----&lt;.-~---'--- - -'-- -~---'
o 0.25 0.5 0.75
</p>
<p>x
</p>
<p>(a)
</p>
<p>. . . . .
2 ; ; : ; .
.. "
:: :.:. . . .. . . .
</p>
<p>1.5 ; ;. .
</p>
<p>H........
01
</p>
<p>1 : .
</p>
<p>o 0.25 0.5 0.75
x
</p>
<p>(b)
</p>
<p>Figure 12.6: Plot of function g(x) = 2(1 - 12x - 11).
</p>
<p>valid PDF. Next to find P[1/4 ~ X ~ 3/4, 1/4 ~ Y ~ 3/4] we use (12.2) to yield
</p>
<p>i
3/4i3/4
</p>
<p>P[A] = 4(1 -12x - 11)(1 -12y - 11)dxdy.
1/4 1/4
</p>
<p>By the same argument as before we have
</p>
<p>[
</p>
<p>3/4 ] 2
</p>
<p>P[A] = 1/4 2(1 - 12x - 11)dx
</p>
<p>and referring to Figure 12.6b, we have that each unshaded triangle has an area of
</p>
<p>(1/2)(1/4)(1) = 1/8 and so
</p>
<p>[ 1 1] 2 (6) 2 9P[A] = 1 - 8 - 8 = 8 = 16'</p>
<p/>
</div>
<div class="page"><p/>
<p>384 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>In summary, a joint PDF has the expected properties of being a nonnegative two-
</p>
<p>dimensional function that integrates to one over R2 .
</p>
<p>For the previous example the double integral was easily evaluated since
</p>
<p>1. The integrand PX,Y(x, y) was separable (we will see shortly that this property
</p>
<p>will hold when the random variables are independent).
</p>
<p>2. The integration region in the x-y plane was rectangular.
</p>
<p>More generally this will not be the case. Consider, for example, the computation
</p>
<p>of P[Y :::; X]. We need to integrate PX,Y over the shaded region shown in Figure
</p>
<p>12.7. To do so we first integrate in the y direction for a fixed z , shown as the darkly
</p>
<p>1.2 r---'~-"""'------'---"-----'-- ""
</p>
<p>1 : ~ : ..; .
&middot; . . .&middot; . . .&middot; . . .
</p>
<p>0.8 : ~ : :.
</p>
<p>;::l)0.6 : ~ : ..
</p>
<p>0.2 ...... : .. .. .
</p>
<p>o '-- ~'-- --'-:.L-__-'-_....J
o 0.25 0.5 0.75
</p>
<p>x
</p>
<p>Figure 12.7: Integration region to determine P[Y :::; X].
</p>
<p>shaded region. Since 0 :::; y :::; x for a fixed x, we have the limits of a to x for the
integration over y and the limits of 0 to 1 for the final integration over x. This
</p>
<p>results in
</p>
<p>P[Y :::; X] = II lX PX,y(x, y)dy dx
II l X4(1 -12x - 11)(1 -12y - 11)dydx.
</p>
<p>Although the integration can be carried out, it is tedious. In this illustration the
</p>
<p>joint PDF is separable but the integration region is not rectangular.
</p>
<p>Zero probability events are more complex in two dimensions.
</p>
<p>Recall that for a single continuous random variable the probability of X attaining
</p>
<p>any value is zero. This is because the area under the PDF is zero for any zero length</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3. JOINTLY DISTRIBUTED RANDOM VARIABLES 385
</p>
<p>interval. Similarly, for jointly continuous random variables X and Y the probability
</p>
<p>of any event defined on the x-y plane will be zero if the region of the event in the
</p>
<p>plane has zero area. Then, the volume under the joint PDF will be zero. Some
</p>
<p>examples of these zero probability events are shown in Figure 12.8.
</p>
<p>y y y
</p>
<p>---+--_X--f---+---+-_x
</p>
<p>&bull;
</p>
<p>----+--_x
</p>
<p>(a) Point (b) Line (c) Curve
</p>
<p>Figure 12.8: Examples of zero probability events for jointly distributed continuous
</p>
<p>random variables X and Y. All regions in the x-y plane have zero area.
</p>
<p>~
An important joint PDF is the standard bivariate Gaussian or normal PDF, which
</p>
<p>is defined as
</p>
<p>-00 &lt; x &lt; 001 [1 2 2]
PX,Y(x, y) = 21rV1 _ p2 exp - 2(1 _ p2) (x - 2pxy + y )
</p>
<p>-00 &lt; Y &lt; 00
(12.3)
</p>
<p>where p is a parameter that takes on values -1 &lt; P &lt; 1. (The use of the term
standard is because as is shown later the means of X and Yare 0 and the variances
</p>
<p>are 1.) The joint PDF is shown in Figure 12.9 for various values of p. We will see
</p>
<p>shortly that p is actually the correlation coefficient PX,Y first introduced in Section
</p>
<p>7.9. The contours of constant PDF shown in Figures 12.9b,d,f are given by the
</p>
<p>values of (x ,y) for which
</p>
<p>where r is a constant. This is because for these values of (x, y) the joint PDF takes
</p>
<p>on the fixed value
</p>
<p>If p = 0, these contours are circular as seen in Figure 12.9d and otherwise they are
</p>
<p>elliptical. Note that our use of r2, which implies that x2 - 2pxy + y2 &gt; 0, is valid</p>
<p/>
</div>
<div class="page"><p/>
<p>386 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>3
</p>
<p>2 .. .
</p>
<p>~O
</p>
<p>-1 ..
</p>
<p>-2
</p>
<p>- 3
-3 -2 -1 0 2 3
</p>
<p>X
</p>
<p>(b) p = -0.9
</p>
<p>3
</p>
<p>2
</p>
<p>~O
</p>
<p>- 1
</p>
<p>-2
</p>
<p>-3
-3 - 2 -1 0 2 3
</p>
<p>X
</p>
<p>(d) p = 0
</p>
<p>3
</p>
<p>2 &middot;
</p>
<p>~ O
</p>
<p>- 1
</p>
<p>-2
</p>
<p>-3
- 3 - 2 - 1 0 2 3
</p>
<p>X
</p>
<p>(f) p = 0.9
</p>
<p>.; . . ..
</p>
<p>- 2
</p>
<p>-2
</p>
<p>(e) p = 0.9
</p>
<p>(a) p = - 0.9
</p>
<p>y
</p>
<p>y -2 X
</p>
<p>y
</p>
<p>o
</p>
<p>o
</p>
<p>o
</p>
<p>(c) p = 0
</p>
<p>;;;
</p>
<p>~
&gt;-'0.4 .
</p>
<p>~-
</p>
<p>l;:l.,0.2 .'
</p>
<p>...--.
~
</p>
<p>t-l
'-...' .. . " .:-.
</p>
<p>&gt;-'0.4 .. . . .
</p>
<p>~
l;:l.,0.2
</p>
<p>;;;
~ .
</p>
<p>&gt;-'0.4 . ' .. ' . '
</p>
<p>i
O
</p>
<p>.
2
</p>
<p>. .. . . . ...&bull; . .. ..
</p>
<p>Figure 12.9: Three-dimensional and constant PDF conto ur plots of standard bivari-
</p>
<p>at e Gaussian PDF.
</p>
<p>since in vector/matrix notation</p>
<p/>
</div>
<div class="page"><p/>
<p>12.4. MARGINAL PDFS AND THE JOINT CDF 387
</p>
<p>which is a quadratic form. Because -1 &lt; p &lt; 1, the matrix is positive definite
(its principal minors are all positive-see Appendix C) and hence the quadratic
</p>
<p>form is positive. We will frequently use the standard bivariate Gaussian PDF and
</p>
<p>its generalizations as examples to illustrate other concepts. This is because its
</p>
<p>mathematical tractability lends itself to easy algebraic manipulations.
</p>
<p>12.4 Marginal PDFs and the Joint CDF
</p>
<p>The marginal PDF px(x) of jointly distributed continuous random variables X and
</p>
<p>Y is the usual PDF which yields the probability of a :s; X :s; b when integrated over
the interval [a, b]. To determine px(x) if we are given the joint PDF PX,y(x , y), we
</p>
<p>consider the event .
</p>
<p>A = {(x ,y): a:S; x:S; b,-oo &lt; y &lt; oo}
</p>
<p>whose probability must be the same as
</p>
<p>Ax={x:a:S;x:S;b}.
</p>
<p>Thus, using (12.2)
</p>
<p>v
</p>
<p>px(x)
</p>
<p>P[a :s; X :s; b]
</p>
<p>Clearly then, we must have that
</p>
<p>P[Ax] = P[A]
</p>
<p>Ji PX,y(x, y)dx dy
i: l b PX,y(x, y)dx dy
</p>
<p>= l bi: px,y(x, y)dy dx.
,
</p>
<p>px(x) = i: PX,y(x , y)dy (12.4)
</p>
<p>PX(xo) =
</p>
<p>as the marginal PDF for X. This operation is shown in Figure 12.10. In effect, we
</p>
<p>"sum" the probabilites of all the y values associated with the desired x, much the
</p>
<p>same as summing along a row to determine the marginal PMF PX[Xi] from the joint
</p>
<p>PMF PX,y[Xi, yj]. The marginal PDF can also be viewed as the limit as b..x -7 0 of
</p>
<p>P[xo - b..x/2 :s; X :s; Xo + b..x/2 , -00 &lt; Y &lt; 00]
b..x
</p>
<p>f
xo+t&gt;. X/ 2 Joo ()
xo- t&gt;.x/2 -00 PX,y x ,Y dy dx
</p>
<p>b..x</p>
<p/>
</div>
<div class="page"><p/>
<p>388 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>..........
~
</p>
<p>~0.15
</p>
<p>&gt;-.
~ 0.1
</p>
<p>I;:l.,
</p>
<p>0.05
</p>
<p>o
</p>
<p>..- . . .. ., . .. ' . . '
</p>
<p>. . ..
</p>
<p>. . ' " . .
</p>
<p>y
</p>
<p>... . ..
;;;
~ 0 . 1 5
</p>
<p>'--""
</p>
<p>&gt;-.
~ 0.1
</p>
<p>I;:l.,
</p>
<p>0.05
. . :. ... .:. . . .
</p>
<p>y
</p>
<p>. . ' ,
</p>
<p>-,
' . .
</p>
<p>. . ..:.
</p>
<p>(a) Curveispx,Y(-l,y) (b) Area under curve is px (-1)
</p>
<p>Figure 12.10: Obtaining the marginal PDF of X from the joint PDF of (X, Y).
</p>
<p>for a small .6.x. An example follows.
</p>
<p>Example 12.2 - Marginal PDFs for Standard Bivariate Gaussian PDF
</p>
<p>From (12.3) and (12.4) we have that
</p>
<p>px(x) = i: 21l'R exp [- 2(1 ~ p2) (x 2 - 2pxy + y2)] dy. (12.5)
To carry out the integration we convert the integrand to one we recognize, i.e.,
</p>
<p>a Gaussian, for which the integral over (-00,00) is known. The trick here is to
"complete the square" in y as follows:
</p>
<p>Q y2 _ 2pxy + x 2
</p>
<p>y2 _ 2pxy + p2x2 + x 2 -lx2
</p>
<p>= (y_px)2+(1_ p2)x2.
</p>
<p>Substituting into (12.5) produces
</p>
<p>where J-L = px and (12 = 1 - p2, so that we have</p>
<p/>
</div>
<div class="page"><p/>
<p>12.4. MARGINAL PDFS AND THE JOINT CDF 389
</p>
<p>or X '" N(0,1) . Hence, the marginal PDF for X is a standard Gaussian PDF.
</p>
<p>By reversing the roles of X and Y, we will also find that Y '" N(O, 1). Note that
since the marginal PDFs are standard Gaussian PDFs, the corresponding bivariate
</p>
<p>Gaussian PDF is also referred to as a standard one.
</p>
<p>o
In the previous example we saw that the marginals could be found from the joint
</p>
<p>PDF. However, in general the reverse process is not possible-given the marginal
</p>
<p>PDFs we cannot determine the joint PDF. For example, knowing that X '" N(O, 1)
</p>
<p>and Y '" N(O, 1) does not allow us to determine p, which characterizes the joint
PDF. Furthermore, the marginal PDFs are the same for any p in the interval (-1,1).
</p>
<p>This is just a restatement of the conclusion that we arrived at for joint and marginal
</p>
<p>PMFs. In that case there were many possible two-dimensional sets of numbers, i.e.,
</p>
<p>specified by a joint PMF, that could sum to the same one-dimensional set, i.e.,
</p>
<p>specified by a marginal PMF.
</p>
<p>We next define the joint CDF for continuous random variables (X, Y). It is given
</p>
<p>by
</p>
<p>Fx,y(x, y) = P[X ~ x, Y ~ y].
</p>
<p>From (12.2) it is evaluated using
</p>
<p>Fx,y(x,y) = i~i:PX ,y(t ,U)dtdU.
</p>
<p>Some examples follow.
</p>
<p>Example 12.3 - Joint CDF for an exponential joint PDF
</p>
<p>If (X ,Y) have the joint PDF
</p>
<p>(12.6)
</p>
<p>(12.7)
</p>
<p>( )
_ { exp[-(x + y)]
</p>
<p>PX,y x ,y - 0
</p>
<p>then for x ~ 0, y ~ 0
</p>
<p>x ~ O,y ~ 0
</p>
<p>otherwise
</p>
<p>Fx,y(x ,y) ~ y ~ x exp[-(t + u)]dtdu
</p>
<p>~ y exp( -u) ~ x exp( -t)dt du
, ~
</p>
<p>V'
</p>
<p>l-exp(-x)
</p>
<p>~ y [1 - exp( -x)] exp( -u)du
</p>
<p>[1 - exp( -x)] ~ y exp(-u)du
</p>
<p>so that
F ( )_{ [1-exp(-x)][1 -exp(-y)] x ~ O , y ~ O
</p>
<p>X,y x, Y - 0 otherwise. (12.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>390 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>4
</p>
<p>x
y 0 0
</p>
<p>0.2
</p>
<p>o
4
</p>
<p>;;;1
~ 0 . 8
~
</p>
<p>~ 0 . 6
~
</p>
<p>0.4
</p>
<p>:" .
</p>
<p>. .... .
.. .
</p>
<p>: " .
</p>
<p>: " ..
</p>
<p>2
</p>
<p>x
</p>
<p>.: , ~ . . .: ..... : ..... ~
</p>
<p>: : -: . .: " .. : .... ;
</p>
<p>&bull; &bull; , &bull; -i ,
. .. . ....:. . .... . .~ . ..... .. .~ . ... . '. . ..
</p>
<p>... ..... ........ .. ....... ....... .
</p>
<p>y 0 0
</p>
<p>0.2
</p>
<p>O ~ _
</p>
<p>4
</p>
<p>;;;1
~ 0 . 8
~
</p>
<p>~ 0 . 6
</p>
<p>R.
0.4
</p>
<p>(a) PDF (b) CDF
</p>
<p>Figure 12.11: Joint exponential PDF and CDF.
</p>
<p>The joint CDF is shown in Figure 12.11 along with the joint PDF. Once the joint
</p>
<p>CDF is obtained the probability for any rectangular region is easily found.
</p>
<p>Example 12.4 - Probability from CDF for exponential random variables
</p>
<p>Consider the rectangular region A = {( x,y) : 1 &lt; x ~ 2, 2 &lt; y ~ 3}. Then referring
</p>
<p>y
</p>
<p>A = {(x, y) : 1 &lt; x ~ 2,2 &lt; y ~ 3}
</p>
<p>Figure 12.12: Evaluation of probability of rectangular region A using joint CDF.
</p>
<p>to Figure 12.12 we determine the probability of A by determining the probability of
</p>
<p>the shaded region, then subtracting out the probability of each cross-hatched region
</p>
<p>(one running from south-east to north-west and the other running from south-west
</p>
<p>to north-east) , and finally adding back in the probability of the double cross-hatched</p>
<p/>
</div>
<div class="page"><p/>
<p>12.4. MARGINAL PDFS AND THE JOINT CDF 391
</p>
<p>region (which has been subtracted out twice). This results in
</p>
<p>P[A] P[-oo &lt; X ::; 2, -00 &lt; Y ::; 3] - P[-oo &lt; X ::; 2, -00 &lt; Y ::; 2]
</p>
<p>-P[-oo &lt; X ::; 1, -00 &lt; Y ::; 3] + P[-oo &lt; X ::; 1, -00 &lt; Y ::; 2]
Fx,y[2, 3] - Fx,y[2, 2] - Fx,y[l, 3] + Fx,y[l, 2].
</p>
<p>For the joint CDF given by (12.8) this becomes
</p>
<p>P[A] = [1 - exp( -2)][1 - exp(-3)] - [1 - exp(_2)]2
</p>
<p>-[1- exp(-l)][l- exp(-3)] + [1- exp(-l)][l - exp(-2)].
</p>
<p>Upon simplication we have the result
</p>
<p>P[A] = [exp(-1) - exp(-2)][exp(-2) - exp( -3)]
</p>
<p>which can also be verified by a direct evaluation as
</p>
<p>P[A] = 1312 exp[-(x + y)]dxdy.
We see that the advantage here is that no integration is required. However, the
</p>
<p>event A must be a rectangular region.
</p>
<p>c
The joint PDF can be recovered from the joint CDF by partial differentiation as
</p>
<p>( ) _ a
2
Fx,y(x, y) (12.9)
</p>
<p>PX,y x, Y - axay
</p>
<p>which is the two-dimensional version of the fundamental theorem of calculus. As an
</p>
<p>example we continue the previous one.
</p>
<p>Example 12.5 - Obtaining the joint PDF from the joint CDF for expo-
</p>
<p>nential random variables
</p>
<p>Continuing with the previous example we have from (12.8) that
</p>
<p>{
</p>
<p>82[I-exp(-x)][l-exp(-y)] X &gt; 0 &gt; &deg;
PX,y(x, y) = 8x8y - ,~-
</p>
<p>o otherwise,
</p>
<p>For x &gt; O,y &gt; 0
</p>
<p>PX,y(x, y)
a a[l - exp( -x)][l - exp( -y)]
</p>
<p>ax ay
</p>
<p>a[l - exp( -x)] a[l - exp( -y)]
</p>
<p>ax ay
</p>
<p>exp(-x) exp(-y) = exp[-(x + y)].
</p>
<p>c
Finally, the properties of the joint CDF are for the most part identical to those for
</p>
<p>the CDF (see Section 7.4 for the properties of the joint CDF for discrete random
</p>
<p>variables). They are (see Figure 12.11b for an illustration):</p>
<p/>
</div>
<div class="page"><p/>
<p>392 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>P12.1 Fx,y(-00, -00) = 0
</p>
<p>P12.2 Fx,Y(+00 , +00) = 1
</p>
<p>P12.3 Fx,y(x , oo) = Fx(x)
</p>
<p>P12.4 Fx,y(oo , y) = Fy(y)
</p>
<p>P12.5 Fx,y(x, y) is monotonically increasing, which means that if Xz 2:: Xl and
in 2:: YI, then Fx,y(xz , yz) 2:: FX,Y(Xl ' yd&middot;
</p>
<p>P12.6 Fx,Y(x , y) is continuous with no jumps (assuming that X and Yare jointly
</p>
<p>continuous random variables). This property is different from the case of
</p>
<p>jointly discrete random variables.
</p>
<p>12.5 Independence of Multiple Random Variables
</p>
<p>The definition of independence of two continuous random variables is the same as for
</p>
<p>discrete random variables. Two continuous random variables X and Yare defined
</p>
<p>to be independent if for all events A E Rand B E R
</p>
<p>P[X E A,Y E B] = P[X E A]P[Y E B].
</p>
<p>Using the definition of conditional probability this is equivalent to
</p>
<p>(12.10)
</p>
<p>pry E BIX E A]
P[X EA,Y EB]
</p>
<p>=
P[XEA]
</p>
<p>P[YEB]
</p>
<p>and similarly P[X E AIY E B] = P[X E A]. It can be shown that X and Yare
independent if and only if the joint PDF factors as (see Problem 12.20)
</p>
<p>pX,Y(x, y) = px(x)py(y).
</p>
<p>Alternatively, X and Yare independent if and only if (see Problem 12.21)
</p>
<p>Fx,y(x, y) = Fx(x)Fy(y).
</p>
<p>An example follows.
</p>
<p>Example 12.6 - Independence of exponential random variables
</p>
<p>From Example 12.3 we have for the joint PDF
</p>
<p>(12.11)
</p>
<p>(12.12)
</p>
<p>( ) {
exp[-(x + y)]
</p>
<p>PX,y x,y = 0
X 2:: 0, y 2:: 0
otherwise.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5. INDEPENDENCE OF MULTIPLE RANDOM VARIABLES 393
</p>
<p>Recalling that the unit step function u(x) is defined as u(x) = 1 for x ~ 0 and
</p>
<p>u(x) = 0 for x &lt; 0, we have
</p>
<p>PX,y(x, y) = exp[-(x + y)]u(x)u(y)
</p>
<p>since u(x)u(y) = 1 if and only if u(x) = 1 and u(y) = 1, which will be true for
x ~ 0, y ~ O. Hence, we have
</p>
<p>PXy(x, y) = exp( -x)u(x) exp( -y)u(y).
, '-v-""-....-'
</p>
<p>px(x) py(y)
</p>
<p>To assert independence we need only factor PX,y(x,y) as g(x)h(y), where 9 and h
</p>
<p>are nonnegative functions . However, to assert that g(x) is actually px(x) and h(y)
</p>
<p>is actually py(y), each function, 9 and h, must integrate to one. For example, we
</p>
<p>could have factored PX,y(x, y) into (1/2) exp(-x)u(x) and 2 exp( -y)u(y), but then
</p>
<p>we could not claim that px(x) = (1/2) exp(-x)u(x) since it does not integrate to
</p>
<p>one. Note also that the joint CDF given in Example 12.3 is also factorable as given
</p>
<p>in (12.8) and in general, factorization of the CDF is also necessary and sufficient to
</p>
<p>assert independence.
</p>
<p>Assessing independence - careful with domain of PDF
</p>
<p>The joint PDF given by
</p>
<p>( )
_ { 2exp[-(x + y)]
</p>
<p>PX,y x,y - 0
x ~ 0, y ~ 0, and y &lt; x
otherwise
</p>
<p>is not factorable, although it is very similar to our previous example. The reason is
</p>
<p>that the region in the x-y plane where px,y(x,y) =I- 0 cannot be written as u(x)u(y)
or for that matter as any g(x)h(y). See Figure 12.13.
</p>
<p>y
</p>
<p>x ~ 0, y ~ 0, and y &lt; x
</p>
<p>x
</p>
<p>Figure 12.13: Nonfactorable region in x-y plane.</p>
<p/>
</div>
<div class="page"><p/>
<p>394 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>Example 12 .7 - Standard bivariate Gaussian PDF
</p>
<p>From (12.3) we see that PX,y(x, y) is only factorable if p = 0. From Figure 12.9d
this corresponds to the case of circular PDF contours. Specifically, for p = 0, we
</p>
<p>have
</p>
<p>PX,y(x, y) 2~ exp [_~(x2 +y2)] - 00 &lt; x &lt; 00, -00 &lt; Y &lt; 00
</p>
<p>vk exp [_~x2] vk exp [_~y2] .
, J' J
</p>
<p>"V 'Y
</p>
<p>px(x) py(y)
</p>
<p>Hence, we observe that if p = 0, then X and Yare independent. Furthermore, each
</p>
<p>marginal PDF is a standard Gaussian (normal) PDF, but as shown in Example 12.2
</p>
<p>this holds regardless of the value of p.
</p>
<p>o
Finally, note that if we can assume that X and Yare independent, then knowledge
</p>
<p>of px(x) and py(y) is sufficient to determine the joint PDF according to (12.11). In
</p>
<p>practice, the independence assumption greatly simplifies the problem of joint PDF
</p>
<p>estimation as we need only to estimate the two one-dimensional PDFs px(x) and
</p>
<p>py(y).
</p>
<p>12.6 Transformations
</p>
<p>We will consider two types of transformations. The first one maps two continuous
</p>
<p>random variables into a single continuous random variable as Z = g(X,Y) , and the
second one maps two continuous random variables into two -new continuous random
</p>
<p>variables as W = g(X, Y) and Z = h(X, Y) . The first type of transformation
</p>
<p>Z = g(X, Y) is now discussed. The approach is to find the CDF of Z and then
differentiate it to obtain the PDF. The CDF of Z is given as
</p>
<p>Fz( z) P[Z~ z]
</p>
<p>P[g(X, Y) ~ z]
</p>
<p>j"r PX,y(x, y)dx dy
} {(x ,y) :g(x ,y):Sz}
</p>
<p>(definition of CDF)
</p>
<p>(definition of Z)
</p>
<p>(from (12.2)). (12.13)
</p>
<p>We see that it is necessary to integrate the joint PDF over the region in the plane
</p>
<p>where g( x, y) ~ z. Depending upon the form of g, this may be a simple task or
</p>
<p>unfortunately a very complicated one. A simple example follows. It is the continuous
</p>
<p>version of (7.22), which yields the PMF for the sum of two independent discrete
</p>
<p>random variables.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.6. TRANSFORMATIONS 395
</p>
<p>Example 12.8 - Sum of independent U(O , 1) random variables
</p>
<p>In Section 2.3 we inquired as to the distribution of the outcomes of an experiment
</p>
<p>that added Ul, a number chosen at random from 0 to 1, to U2, another number
</p>
<p>chosen at random from 0 to 1. A histogram of the outcomes of a computer simulation
</p>
<p>indicated that there is a higher probability of the sum being near 1, as opposed to
</p>
<p>being near 0 or 2. We now know that U1 ,...., U(O,l), U2 ,...., U(O,l) . Also, in the
</p>
<p>experiment of Section 2.3 the two numbers were chosen independently of each other.
</p>
<p>Hence , we can determine the probabilities of the sum random variable if we first find
</p>
<p>the CDF of X = Ul + U2, where U; and U2 are independent, and then differentiate
it to find the PDF of X. We will use (12.13) and replace x, y, z, and g(x, y) by
</p>
<p>Ul ,U2,X, and g(Ul ,U2), respectively. Then
</p>
<p>To determine the possible values of X, we note that both Ul and U2 take on values in
</p>
<p>(0,1) and so 0 &lt; X &lt; 2. In evaluating the CDF we need two different intervals for x
as shown in Figure 12.14. Since U1 and U2 are independent, we have PU1,u2 = PU1PU2
</p>
<p>,
</p>
<p>(a) 0:::; x &lt; 1 (b) 1 s x s 2
</p>
<p>Figure 12.14: Shaded areas are regions of integration used to find CDF.
</p>
<p>and therefore PU1,u2(Ul, U2) = 1 for 0 &lt; Ul &lt; 1 and 0 &lt; U2 &lt; 1, which results in
</p>
<p>Hence , the CDF is given by
</p>
<p>Fx(x) ~ {
x &lt; O
</p>
<p>O:S;x&lt;l
</p>
<p>1:S;x:S;2
</p>
<p>x&gt; 2.</p>
<p/>
</div>
<div class="page"><p/>
<p>396 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>and the PDF is finally
</p>
<p>PX(x)
dFx(x)
</p>
<p>dx
</p>
<p>u- x
x&lt;O
</p>
<p>O:S;x&lt;l
</p>
<p>1:S;x:S;2
</p>
<p>x&gt; 2.
</p>
<p>This PDF is shown in Figure 12.15. This is in agreement with our computer results
</p>
<p>px(x)
</p>
<p>1
</p>
<p>-+---+--~~-- x
</p>
<p>1 2
</p>
<p>Figure 12.15: PDF for the sum of two independent U(O, 1) random variables.
</p>
<p>shown in Figure 2.2. The highest probability is at x = 1, which concurs with our
computer generated results of Section 2.3. Also, note that px(x) = PUj (x) *PU2(X),
where * denotes integral convolution (see Problem 12.28).
</p>
<p>&lt;:)
</p>
<p>More generally, we can derive a useful formula for the PDF of the sum of two
</p>
<p>independent continuous random variables. According to (12.13), we first need to
</p>
<p>determine the region in the plane for which x+y :s; z. This inequality can be written
as y :s; z - x, where z is to be regarded for the present as a constant. To integrate
PX,y(x, y) over this region, which is shown in Figure 12.16 as the shaded region, we
</p>
<p>can use an iterated integral. Thus,
</p>
<p>Fz(z) i: iZ~x PX,y(x, y)dy dx
i: iZ~x PX(x)py (y)dy dx
</p>
<p>= i:px(x) i Z ~ x py(y)dydx
i: px(x)Fy(z - x)dx
</p>
<p>(independence)
</p>
<p>(definition of CDF) .</p>
<p/>
</div>
<div class="page"><p/>
<p>12.6. TRANSFORMATIONS
</p>
<p>x
</p>
<p>397
</p>
<p>Figure 12.16: Iterated integral evaluation - shaded region is y ~ z - x. Integrate
</p>
<p>first in y direction for a fixed x and then integrate over -00 &lt; x &lt; 00.
</p>
<p>If we now differentiate the CDF, we have
</p>
<p>(chain rule with u = z - x )
</p>
<p>(assume interchange is valid)
</p>
<p>d 100pz(z) = dz -00 px(x)Fy(z - x)dx
1
</p>
<p>00 d
-00 px(x) dz Fy(z - x)dx
</p>
<p>I: px(x) d ~ Fy(u)lu=z_x ~~ dx
so that finally we have our formula
</p>
<p>pz(z) = I: px(x)py(z - x)dx. (12.14)
This is the analogous result to (7.22). It is recognized as a convolution integral,
</p>
<p>which we can express more succinctly as pz = vx * py , and thus may be more
easily evaluated by using characteristic functions. The latter approach is explored
</p>
<p>in Section 12.10.
</p>
<p>A second approach to obtaining the PDF of g(X,Y) is to let W = X , Z =
g(X,Y), find the joint PDF of Wand Z, i.e. , pw,z(w, z), and finally integrate
</p>
<p>out W to yield the desired PDF for Z. This method was encountered previously
</p>
<p>in Chapter 7, where it was used for discrete random variables, and was termed the
</p>
<p>method of auxiliary random variables. To implement it now requires us to determine
</p>
<p>the joint PDF of two new random variables that result from having transformed two
</p>
<p>random variables. This is the second type of transformation we were interested in.
</p>
<p>Hence, we now consider the more general transformation
</p>
<p>W g(X, Y)
</p>
<p>Z h(X,Y).</p>
<p/>
</div>
<div class="page"><p/>
<p>398 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>The final result will be a formula relating the joint PDF of (W,Z) to that of the
</p>
<p>given joint PDF of (X,Y). It will be a generalization of the single random variable
</p>
<p>transformation formula
</p>
<p>(12.15)
</p>
<p>for Y = g(X).
To understand what is involved, consider as an example the transformation
</p>
<p>[ ~: ] = [ (U, :~2)/2 ] (12.16)
where UI ,....., U(O, 1), U2 ,....., U(O, 1), and UI and U2 are independent. In Figure 2.13
</p>
<p>we plotted realizations of [UI U2]T and [Xl X2]T. Note that the original joint PDF
</p>
<p>PUl,U2 is nonzero on the unit square while the transformed PDF is nonzero on a
</p>
<p>parallelogram. In either case the PDFs appear to be uniformly distributed. Similar
</p>
<p>observations about the region for which the PDF of the transformed random variable
</p>
<p>is nonzero were made in the one-dimensional case for Y = g(X), where X ,....., U(O, 1),
in Figure 10.22. In general, a linear transformation will change the support area of
</p>
<p>the joint PDF, which is the region in the plane where the PDF is nonzero. In Figure
</p>
<p>2.13 it is seen that the area of the square is 1 while that for the parallelogram is 1/2.
</p>
<p>It can furthermore be shown that if we have the linear transformation (see Problem
</p>
<p>12.29)
</p>
<p>then
</p>
<p>(12.17)
</p>
<p>Area in w-z plane
</p>
<p>Area in x-y plane
Idet(G)1
</p>
<p>lad - bel&middot;
</p>
<p>It is always assumed that G is invertible so that det(G) i- 0. In the previous example
of (12.16) for which in our new notation we have W = X and Z = (X +Y)/2, the
linear transformation matrix is
</p>
<p>G=[~ !]
and it is seen that Idet(G)1 = 1/2. Thus, the PDF support region is decreased by
a factor of 2. We therefore expect the joint PDF of [X (X +Y)/2]T to be uniform
with a height of 2 (as opposed to a height of 1 for the original joint PDF). Hence,
</p>
<p>the transformed PDF should have a factor of 1/1det(G)1 to make it integrate to one.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.6. TRANSFORMATIONS 399
</p>
<p>This amplification factor, which is 1/1 det(G)1 = Idet(G-1)1 must be included in the
expression for the transformed joint PDF. Also, we have that [xyjT = G-1[wzjT.
Hence, it should not be surprising that for the linear transformation of (12.17) we
</p>
<p>have the formula for the transformed joint PDF
</p>
<p>PW,z(w, z) = PX,Y ( G-
1
</p>
<p>[ : ]) Idet(G-1)1&middot; (12.18)
</p>
<p>An example follows.
</p>
<p>Example 12.9 - Linear transformation for standard bivariate Gaussian
</p>
<p>PDF
</p>
<p>Assume that (X, Y) has the PDF of (12.3) and consider the linear transformation
</p>
<p>Then,
</p>
<p>and
</p>
<p>[
w/ow ]
</p>
<p>z/az
</p>
<p>1
</p>
<p>(12.20)
</p>
<p>so that from (12.3) and (12.18)
</p>
<p>pw,z(w,z)
</p>
<p>nexp [-2(1 1 2) ((w/aw)2-2pwz/(awaz) + (z/az)2)] _1_
211" 1 - p2 - P awaz
</p>
<p>1
</p>
<p>211"V(1 - p2)arva~
</p>
<p>. exp [ 2(1 ~ P') (c:)'-2p (;:) (:z) + (:J)]. (12.19)
Note that since -00 &lt; x &lt; 00, -00 &lt; y &lt; 00, we have that the region of support
for PW,z is -00 &lt; w &lt; 00, -00 &lt; z &lt; 00. Also, the joint PDF can be written in
vector/matrix form as (see Problem 12.31)
</p>
<p>pW,z(w,z) = 21rdet~/2(C) exp (-~ [ : rC-1 [ : ])</p>
<p/>
</div>
<div class="page"><p/>
<p>400 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>(12.21)c = [ a~ pa~az] .
pazaw az
</p>
<p>The matrix C will be shown later to be the covariance matrix of Wand Z (see Sec-
</p>
<p>tion 9.5 for the definition of the covariance matrix, which is also valid for continuous
</p>
<p>random variables).
</p>
<p>o
For nonlinear transformations a result similar to (12.18) is obtained. This is because
</p>
<p>a two-dimensional nonlinear function can be linearized about a point by replacing
</p>
<p>the usual tangent or derivative approximation for a one-dimensional function by a
</p>
<p>tangent plane approximation (see Problem 12.32). Hence, if the transformation is
</p>
<p>given by
</p>
<p>where
</p>
<p>W g(X,Y)
</p>
<p>Z h(X, Y)
</p>
<p>then a given point in the ui-z plane is obtained via w = g(x, y) , z = h(x, y). Assume
</p>
<p>that the latter set of equations has a single solution for all (w,z), say
</p>
<p>X g-l(w, z)
</p>
<p>Y h-1(w,z).
</p>
<p>Then it can be shown that
</p>
<p>(12.22)
</p>
<p>where
</p>
<p>8(x ,y)
</p>
<p>8(w,z)
(12.23)
</p>
<p>is called the Jacobian matrix of the inverse transformation from [w zjT to [x yjT and
is sometimes referred to as J- 1. It represents the compensation for the amplifica-
</p>
<p>tion/reduction of the areas due to the transformation. For a linear transformation
</p>
<p>G it is given by J = G (see also (12.15) for a single random variable). We now
illustrate the use of this formula.
</p>
<p>Example 12.10 - Affine transformation for standard bivariate Gaussian
</p>
<p>PDF
</p>
<p>Let (X, Y) have a standard bivariate Gaussian PDF and consider the affine trans-
</p>
<p>formation
</p>
<p>[: ] ~ [a~ :z] [; ]+ [ :: ] .</p>
<p/>
</div>
<div class="page"><p/>
<p>12.6. TRANSFORMATIONS
</p>
<p>Then using (12.22) we first solve for (x,y) as
</p>
<p>w - J.Lw
x
</p>
<p>o-w
z -J.Lz
</p>
<p>Y =
o-z
</p>
<p>The inverse Jacobian matrix becomes
</p>
<p>8(x, y) = [l/o-W 0 ]
8(w, z ) 0 l/o-z
</p>
<p>and therefore, since
</p>
<p>1 [1 2 2]
PX,Y(X, y) = 21rV1 _ p2 exp - 2(1 _ p2) (x - 2pxy + Y )
</p>
<p>we have from (12.22)
1
</p>
<p>pW,z(w,z) = ~
21rV1- p2
</p>
<p>401
</p>
<p>. exp [ - 2(1 ~ p') ( (w ~:w)' -2p ( w~w) (z :;z) + (z :;zn]a:az
or finally
</p>
<p>. exp [_ 2(1 ~ p') ( (w ~:w )' _2p ( w~w) (z :;z) + ( z:;z)')].
(12.24)
</p>
<p>This is called the bivariate Gaussian PDF. If J.Lw = J.Lz = 0 and o-w = oz = 1, then
it reverts back to the usual standard bivariate Gaussian PDF. If J.Lw = J.Lz = 0, we
</p>
<p>have the joint PDF in Example 12.9. An example of the PDF is shown in Figure
</p>
<p>12.17.
</p>
<p>c
The bivariate Gaussian PDF can also be written more compactly in vector/matrix
</p>
<p>form as
</p>
<p>( [
T )
</p>
<p>1 1 w - J.Lw w - J.Lw
pwz(w, z) = 1 2 exp -- C-1
</p>
<p>, 21l'det / (C) 2 Z-I'Z] [Z-I'z ] (12.25)
</p>
<p>where C is the covariance matrix given by (12.21) . It can also be shown that
</p>
<p>the marginal PDFs are W ""' N(J.Lw ,o-?v) and Z ""' N(J.Lz , o-~) (see Problem 12.36).</p>
<p/>
</div>
<div class="page"><p/>
<p>402 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>..
: .
</p>
<p>. . . . ...
. . .. ... .... . .
</p>
<p>. ... . . . ,'
</p>
<p>. . . .. .
. . .. ..
</p>
<p>(a) Joint PDF
</p>
<p>w
</p>
<p>-'.:.
</p>
<p>5
4 .
</p>
<p>3 &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; . ,
</p>
<p>2 .:. : .: Jj1fl . ~ ..:..:.:. :.:..
1 : .: :- : .. :-:~ : : : : : : : :
</p>
<p>NO &middot; &middot; &middot; . . . . . .
. .. . .
</p>
<p>-1
</p>
<p>-2
</p>
<p>-3
-4 ...
</p>
<p>-5
</p>
<p>-5-4-3-2-1 0 1 2 3 4 5
</p>
<p>'4J
</p>
<p>(b) Contours of constant PDF
</p>
<p>Figure 12.17: Example of bivariate Gaussian PDF with /-Lw = 1, /-Lz = 1, (T~ =
3, (T~ = 1, and p = 0.9.
</p>
<p>Hence, the marginal PDFs of the bivariate Gaussian PDF are obtained by inspection
</p>
<p>(see Problem 12.37).
</p>
<p>Example 12.11 - Transformation of independent Gaussian random vari-
</p>
<p>ables to a Cauchy random variable
</p>
<p>Let X rv N(O, 1), Y rv N(O, 1), and X and Y be independent. Then consider the
</p>
<p>transformation W = X, Z = YjX. To determine Sw,z note that w = x so that
-00 &lt; w &lt; 00 and since z = yjx with -00 &lt; x &lt; 00,-00 &lt; y &lt; 00, we have
-00 &lt; z &lt; 00. Hence, Sw,z is the entire plane. To find the joint PDF we first solve
for (x , y) as x = wand y = xz = wz. The inverse Jacobian matrix is
</p>
<p>8(x, y) = [1 0]
8(w ,z) z w
</p>
<p>so that Idet(8(x, y)j8(w, z))1 = Iwl. Using (12.22), we have
</p>
<p>pw,z(w,z) = 2~ exp [_~(x2 +y2)] Ix=w,y=wz Iwl
</p>
<p>= 2~ exp [_~(W2 + w2z2)J jwl
</p>
<p>2~ exp [-~(1 + z2)w2] Iwl&middot;
</p>
<p>It is of interest to determine the marginal PDFs. Clearly, the marginal of W = X
</p>
<p>is just the original PDF N(O , 1). The marginal PDF for Z , which is the ratio of two</p>
<p/>
</div>
<div class="page"><p/>
<p>12.6. TRANSFORMATIONS
</p>
<p>independent N(o,1) random variables, is found from (12.4) as
</p>
<p>403
</p>
<p>pz(z) = I: pW,z(w, z)dw
I: 2~ exp [-~(1 + z2)w2] Iwldw
~ 100 wexp [ - ~ ( 1 + z 2 ) w 2 ] dw
.!. exp[-(1/2)(1 + z2)w2] 100
</p>
<p>1f -(1 + z2) a
1
</p>
<p>(integrand is even function)
</p>
<p>-oo&lt;z&lt;oo
</p>
<p>which is recognized as the Cauchy PDF. Hence, the PDF of Y/X, where X and Y
</p>
<p>are independent standard Gaussian random variables is Cauchy. We have implicitly
</p>
<p>used the method of auxiliary random variables to derive this result . Finally, the
</p>
<p>observation that the denominator of Y / X is a standard Gaussian random variable,
</p>
<p>with significant probability of being near zero, may help to explain why the outcomes
</p>
<p>of a Cauchy random variable are as large as they are. See Figure 11.3.
</p>
<p>\/
The next example is of great importance in many fields of science and engineering.
</p>
<p>Example 12.12 - Magnitude and angle of jointly Gaussian distributed
</p>
<p>random variables
</p>
<p>Let X '" N(o,(72), Y '" N(o, (72), and X and Y be independent random variables.
Then, it is desired to find the joint PDF when X and Y, considered as Cartesian
</p>
<p>coordinates, are converted to polar coordinates via
</p>
<p>R
</p>
<p>e
JX2 + y2 R ~ &deg;
</p>
<p>Y
arctan X &deg;~ e &lt; 21f . (12.26)
</p>
<p>It is common in many engineering disciplines, for example, in radar, sonar, and
</p>
<p>communications, to transmit a sinusoidal signal and to process the received signal
</p>
<p>by a digital computer. The received signal will be given by s(t) = A cos(21fFat) +
B sin(21fFat) for a transmit frequency of Fa Hz. However, because the received signal
</p>
<p>is due to the sum of multiple reflections from an aircraft, as in the radar example,
</p>
<p>the values of A and B are generally not known. Consequently, they are modeled
</p>
<p>as continuous random variables with marginal PDFs A '" N(o, (72), B '" N(o, (72),
and where A and B are independent. Since the received signal can equivalently be
</p>
<p>written in terms of a single sinusoid as (see Problem 12.42)
</p>
<p>s(t) = JA2 + B2 cos(21fFat - arctan(B/A))</p>
<p/>
</div>
<div class="page"><p/>
<p>404 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>the amplitude is a random variable as is the phase angle. Thus, the transformation of
</p>
<p>(12.26) is of interest in order to determine the joint PDF of the sinusoid's amplitude
</p>
<p>and phase. This motivates our interest in this particular transformation.
</p>
<p>We first solve for (x,y) as x = rcos8, y = rsin8. Then using (12.22) and
replacing w by rand z by 8 we have the inverse Jacobian
</p>
<p>8(x,y) = [COS8 -rsin8]
8(r, 8) sin 8 r cos 8
</p>
<p>and thus
</p>
<p>(
8(x,y) )
</p>
<p>det 8(r,8) = r ~ O.
</p>
<p>Since
</p>
<p>1 [1 2 2]PX,Y(x, y) = px(x)py(y) = 21m2 exp - 2(12 (x +Y )
we have upon using (12.22)
</p>
<p>r ~ 0, 0::; 8 &lt; 211".
</p>
<p>r 2:: 0, 0 ::; 8 &lt; 211"PR,e(r,8) = _1_ exp [_ _1_r 2] r
21m2 2(12
</p>
<p>r: exp [_ _1_r 2] ~
(12 2(12 211"
</p>
<p>, J '-v-"
</p>
<p>PR(r) pe(O)
</p>
<p>Here we see that R has a Rayleigh PDF with parameter (12 , e has a uniform PDF,
and Rand e are independent random variables.
</p>
<p>12.7 Expected Values
</p>
<p>The expected value of two jointly distributed continuous random variables X and Y,
</p>
<p>or equivalently the random vector [X YjT, is defined as the vector of the expected
values. That is to say
</p>
<p>Ex Y [[ X ]] [ Ex[X] ] .
' y Ey[Y]
</p>
<p>Of course this is equivalent to the vector of the expected values of the marginal
</p>
<p>PDFs. As an example, for the bivariate Gaussian PDF as given by (12.24) with
</p>
<p>W, Z replaced by X ,Y, the marginals are N(/-lX, (1~) and N(/-lY, (1}) and hence the
expected value or equivalently the mean of the random vector is</p>
<p/>
</div>
<div class="page"><p/>
<p>12.7. EXPECTED VALUES 405
</p>
<p>as shown in Figure 12.17 for /-Lx = /-LY = l.
</p>
<p>We frequently require the expected value of a function of two jointly distributed
</p>
<p>random variables or of Z = g(X, Y). By definition this is
</p>
<p>E[Z] = i: zpz(z)dz.
</p>
<p>But as in the case for jointly distributed discrete random variables we can avoid the
</p>
<p>determination of pz(z) by employing instead the formula
</p>
<p>E[Z] = E[g(X, Y)] = i: i: g(x, y)PX,y(x, y)dx dy.
</p>
<p>To remind us that the averaging PDF is PX,y(x, y) we usually write this as
</p>
<p>Ex,y[g(X, Y)] = i: i: g(x, y)PX,y(x, y)dx dy.
</p>
<p>If the function 9 depends on only one of the variables, say X, then we have
</p>
<p>Ex,y[g(X)] = i: i: g(x)pX,y(x,y)dxdy
</p>
<p>i: g(x) i:PX,y(x,y)dYdX
</p>
<p>v
</p>
<p>px(x)
</p>
<p>Ex[g(X)].
</p>
<p>(12.27)
</p>
<p>(12.28)
</p>
<p>As in the case of discrete random variables (see Section 7.7), the expectation has
</p>
<p>the following properties:
</p>
<p>1. Linearity
</p>
<p>Ex,y[aX + bY] = aEx[X] + bEy[Y]
</p>
<p>and more generally
</p>
<p>Ex,y[ag(X, Y) + bh(X, Y)] = aEX,y[g(X, Y)] + bEx,y[h(X, Y)].
</p>
<p>2. Factorization for independent random variables
</p>
<p>If X and Yare independent random variables
</p>
<p>Ex,y[XY] = Ex[X]Ey[Y]
</p>
<p>and more generally
</p>
<p>Ex,y[g(X)h(Y)] = Ex[g(X)]Ey[h(Y)].
</p>
<p>(12.29)
</p>
<p>(12.30)</p>
<p/>
</div>
<div class="page"><p/>
<p>406 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>Also, in determining the variance for a sum of random variables we have
</p>
<p>var(X + Y ) = var(X) + var(Y ) + 2cov(X,Y ) (12.31)
</p>
<p>where cov(X,Y ) = EX,Y [(X &minus; EX [X])(Y &minus; EY [Y ])]. If X and Y are independent,
then by (12.30)
</p>
<p>cov(X,Y ) = EX,Y [(X &minus; EX [X])(Y &minus; EY [Y ])]
= EX [(X &minus; EX [X])]EY [(Y &minus; EY [Y ])]
= 0.
</p>
<p>The covariance can also be computed as
</p>
<p>cov(X,Y ) = EX,Y [XY ]&minus; EX [X]EY [Y ] (12.32)
where
</p>
<p>EX,Y [XY ] =
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
xypX,Y (x, y)dx dy. (12.33)
</p>
<p>An example follows.
</p>
<p>Example 12.13 &ndash; Covariance for standard bivariate Gaussian PDF
</p>
<p>For the standard bivariate Gaussian PDF of (12.3) we now determine cov(X,Y ).
We have already seen that the marginal PDFs are X &sim; N (0, 1) and Y &sim; N (0, 1) so
that EX [X] = EY [Y ] = 0. From (12.32) we have that cov(X,Y ) = EX,Y [XY ] and
using (12.33) and (12.3)
</p>
<p>cov(X,Y ) =
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
xy
</p>
<p>1
</p>
<p>2
&radic;
</p>
<p>1&minus; 2
exp
</p>
<p>[
</p>
<p>&minus; 1
2(1&minus; 2)(x
</p>
<p>2 &minus; 2xy + y2)
]
</p>
<p>dx dy.
</p>
<p>To evaluate this double integral we use iterated integrals and complete the square
in the exponent of the exponential as was previously done in Example 12.2. This
results in
</p>
<p>Q = y2 &minus; 2xy + x2 = (y &minus; x)2 + (1&minus; 2)x2
</p>
<p>and produces
cov(X,Y )
</p>
<p>=
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
xy
</p>
<p>1
</p>
<p>2
&radic;
</p>
<p>1&minus; 2
exp
</p>
<p>[
</p>
<p>&minus; 1
2(1&minus; 2)(y &minus; x)
</p>
<p>2
</p>
<p>]
</p>
<p>exp
</p>
<p>[
</p>
<p>&minus;1
2
x2
</p>
<p>]
</p>
<p>dx dy
</p>
<p>=
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
x
</p>
<p>1&radic;
2
</p>
<p>exp
</p>
<p>[
</p>
<p>&minus;1
2
x2
</p>
<p>]
&int; &infin;
</p>
<p>&minus;&infin;
y
</p>
<p>1
&radic;
</p>
<p>2(1&minus; 2)
exp
</p>
<p>[
</p>
<p>&minus; 1
2(1&minus; 2) (y &minus; x)
</p>
<p>2
</p>
<p>]
</p>
<p>dy dx.
</p>
<p>The inner integral over y is just EY [Y ] =
&int;&infin;
&minus;&infin; ypY (y)dy, where Y &sim; N (x, 1&minus; 2).
</p>
<p>Thus, EY [Y ] = x so that
</p>
<p>cov(X,Y ) =
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
x2
</p>
<p>1&radic;
2
</p>
<p>exp
</p>
<p>[
</p>
<p>&minus;1
2
x2
</p>
<p>]
</p>
<p>dx
</p>
<p>= EX [X
2]</p>
<p/>
</div>
<div class="page"><p/>
<p>12.7. EXPECTED VALUES 407
</p>
<p>where X "-' N(O, 1). But Ex[X2 ] = var(X) + E1-[X] = 1 + 02 = 1 and therefore we
have finally that
</p>
<p>cov(X, Y) = p.
</p>
<p>o
With the result of the previous example we can now determine the correlation
</p>
<p>coefficient between X and Y for the standard bivariate Gaussian PDF. Since the
</p>
<p>marginal PDFs are X rv N(O, 1) and Y rv N(O , 1), the correlation coefficient between
</p>
<p>X and Y is
</p>
<p>PX,y
cov(X, Y)
</p>
<p>vvar(X)var(Y)
</p>
<p>P
</p>
<p>VG1
= p.
</p>
<p>We have therefore established that in the standard bivariate Gaussian PDF, the pa-
</p>
<p>ram eter p is the correlation coefficient. This explains the orientation of the constant
</p>
<p>PDF contours shown in Figure 12.9. Also, we can now assert that if the correlation
</p>
<p>coefficient between X and Y is zero , i.e., p = 0, and X and Yare jointly Gaussian
</p>
<p>distributed (i.e. , a standard bivariate Gaussian PDF), then
</p>
<p>and X and Yare independent. This also holds for the general bivariate Gaussian
</p>
<p>PDF in which the marginal PDFs are X "-' N(/-Lx ,a1-) and Y "-' N(/-LY, an. This
result provides a partial converse to the theorem that if X and Yare independent,
</p>
<p>then the random variables are uncorrelated, but only for this particular joint PDF.
</p>
<p>Finally, since p = PX,Y we have from (12.21) upon replacing W by X and Z by
Y , that
</p>
<p>c [
a1- poxcv ]
</p>
<p>payaX a ~
</p>
<p>[
a1- oxroxov ]
</p>
<p>oxrovox a~
</p>
<p>[
var(X) cov(X, Y) ]
</p>
<p>cov(Y, X) var(Y)
</p>
<p>(12.34)</p>
<p/>
</div>
<div class="page"><p/>
<p>408 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>is the covariance matrix. We have now established that C as given by (12.34) is
</p>
<p>actually the covariance matrix. Hence, the general bivariate Gaussian PDF is given
</p>
<p>in succinct form as (see (12.24))
</p>
<p>(
</p>
<p>T ])
1 1 x - }.Lx x - tix
</p>
<p>PX,y(x,y) = 2?rdet l / 2(C) exp -2 [ y -I'y] c: [ y -I'Y (12.35)
</p>
<p>where C is given by (12.34) and is the covariance matrix (see Section also 9.5)
</p>
<p>C _ [var(x) cov(X, Y) ]
- cov(Y, X) var(Y) .
</p>
<p>(12.36)
</p>
<p>As previously mentioned, an extremely important property of the bivariate Gaussian
</p>
<p>PDF is that uncorrelated random variables implies independent random variables.
</p>
<p>Hence , if the covariance matrix in (12.36) is diagonal, then X and Yare inde-
</p>
<p>pendent. We have shown in Chapter 9 that it is always possible to diagonalize a
</p>
<p>covariance matrix by transforming the random vector using a linear transforma-
</p>
<p>tion. Specifically, if the random vector [X YV is transformed to a new random
vector yT[X YV, where Y is the modal matrix for the covariance matrix C, then
the transformed random vector will have a diagonal covariance matrix. Hence, the
</p>
<p>transformed random vector will have uncorrelated components. If furthermore, the
</p>
<p>transformed random vector also has a bivariate Gaussian PDF, then its component
</p>
<p>random variables will be independent. It is indeed fortunate that this is true-a
</p>
<p>linearly transformed bivariate Gaussian random vector produces another bivariate
</p>
<p>Gaussian random vector, as we now show. To do so it is more convenient to use a
</p>
<p>vector/matrix representation of the PDF. Let the linear transformation be
</p>
<p>where G is an invertible 2 x 2 matrix. Assume for simplicity that tix = }.Ly = O.
Then, from (12.35)
</p>
<p>PX,y(x,y) ~ 2 ? r d e t ~ / 2 ( C ) exp ( -~ [ : rC-1[ : ])
and using (12.18)
</p>
<p>pw,z(w,z) ~ PX,y ( G-1 [ : ]) Idet(G-1)1
</p>
<p>= ~ /2 exp (--21[ w ] T G-1TC-1G-1 [ W ]) Idet(G-1)I.
27f det (C) z z</p>
<p/>
</div>
<div class="page"><p/>
<p>12.7. EXPECTED VALUES 409
</p>
<p>But it can be shown that (see Section C.3 of Appendix C for matrix inverse and
</p>
<p>determinant formulas)
</p>
<p>and
</p>
<p>so that
</p>
<p>=
</p>
<p>1
</p>
<p>Idet(G)1
</p>
<p>1
</p>
<p>(det(G) det(G))1 /2
</p>
<p>1
</p>
<p>(det(G) det(GT))1/2
</p>
<p>!det(G-1)I
det1/2(C)
</p>
<p>1
</p>
<p>det1/2(C)(det(G) det(GT))1/2
</p>
<p>1
=
</p>
<p>(det(C) det(G) det(GT))l/2
</p>
<p>1
</p>
<p>(det(G) det(C) det(GT))1/2
</p>
<p>1
</p>
<p>Thus, we have finally that the PDF of the linearly transformed random vector is
</p>
<p>(
</p>
<p>T )
1 1 W W
</p>
<p>W Z - ex -- GCGT -1
PW,Z( , ) - 2ndct'/2(GCGT) p 2 [ z] ( ) [ z]
</p>
<p>which is recognized as a bivariate Gaussian PDF with zero means and a covariance
</p>
<p>matrix GCGT . This also agrees with Property 9.4. We summarize our results in a
</p>
<p>theorem.
</p>
<p>Theorem 12.7.1 (Linear transformation of Gaussian random variables)
</p>
<p>If (X, Y) has the bivariate Gaussian PDF
</p>
<p>(
</p>
<p>T )
1 1 x - ux x - /-lx
</p>
<p>PX,y(x, y) ~ 2n dct'/2(C) cxp -2 [ y _/ty ] c-1 [ Y -!'Y ]
and the random vector is linearly transformed as
</p>
<p>(12.37)</p>
<p/>
</div>
<div class="page"><p/>
<p>410 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>where G is invertible, then
</p>
<p>(
T [ ])
</p>
<p>1 1 W - /-Lw W - /-Lw
W z - ex -- GCGT -1
</p>
<p>PW,Z( , ) - 2ndet
'/
</p>
<p>2(GCGT) p 2 [ z -/'z ] ( ) z -/'z
</p>
<p>where
</p>
<p>is the transformed mean vector.
</p>
<p>The bivariate Gaussian PDF with mean vector p, and covariance matrix C is denoted
</p>
<p>by N(p" C). Hence, the theorem may be paraphrased as follows-if [X YjT f'V
</p>
<p>N(p" C), then G[X YjT f'V N(Gp" GCGT ) . An example, which uses results from
</p>
<p>Example 9.4, is given next.
</p>
<p>Example 12.14 - Transforming correlated Gaussian random variables to
</p>
<p>independent Gaussian random variables
</p>
<p>Let tix = /-Ly = 0 and
</p>
<p>c = [26 6]6 26
in (12.37) . The joint PDF and its constant PDF contours are shown in Figure 12.18.
</p>
<p>Now transform X and Y according to
</p>
<p>where G is the transpose of the modal matrix V, which is given in Example 9.4.
</p>
<p>Therefore
</p>
<p>so that
</p>
<p>We have that
</p>
<p>w
</p>
<p>z
</p>
<p>1 1
-X--Y
V'i V'i
1 1
</p>
<p>V'i X + V'iY.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.7. EXPECTED VALUES 411
</p>
<p>&minus;10
</p>
<p>0
</p>
<p>10
</p>
<p>&minus;10
</p>
<p>0
</p>
<p>10
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>x 10
&minus;3
</p>
<p>xy
</p>
<p>p
X
,Y
(x
,y
)
</p>
<p>(a) Joint PDF
</p>
<p>&minus;15 &minus;10 &minus;5 0 5 10 15
&minus;15
</p>
<p>&minus;10
</p>
<p>&minus;5
</p>
<p>0
</p>
<p>5
</p>
<p>10
</p>
<p>15
</p>
<p>0.001
</p>
<p>0.00
1
</p>
<p>0.
00
</p>
<p>2
</p>
<p>0
.0
</p>
<p>0
3
</p>
<p>0.00
4
</p>
<p>x
</p>
<p>y
</p>
<p>(b) Contours of constant PDF
</p>
<p>Figure 12.18: Example of joint PDF for correlated Gaussian random variables.
</p>
<p>&minus;10
</p>
<p>0
</p>
<p>10
</p>
<p>&minus;10
</p>
<p>0
</p>
<p>10
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>x 10
&minus;3
</p>
<p>wz
</p>
<p>p
W
,Z
(w
</p>
<p>,z
)
</p>
<p>(a) Joint PDF
</p>
<p>&minus;15 &minus;10 &minus;5 0 5 10 15
&minus;15
</p>
<p>&minus;10
</p>
<p>&minus;5
</p>
<p>0
</p>
<p>5
</p>
<p>10
</p>
<p>15
</p>
<p>0
.0
</p>
<p>0
1
</p>
<p>0
.0
</p>
<p>0
1
</p>
<p>0
.0
</p>
<p>0
2
</p>
<p>0
.0
</p>
<p>0
2 0.0
</p>
<p>03
0.004
</p>
<p>w
</p>
<p>z
</p>
<p>(b) Contours of constant PDF
</p>
<p>Figure 12.19: Example of joint PDF for transformed correlated Gaussian random
variables. The random variables are now uncorrelated and hence independent.
</p>
<p>pW,Z(w, z) =
1
</p>
<p>2
&radic;
20 &middot; 32
</p>
<p>exp
</p>
<p>
</p>
<p>&minus;1
2
</p>
<p>[
</p>
<p>w
</p>
<p>z
</p>
<p>]T
[
</p>
<p>1/20 0
0 1/32
</p>
<p>]
</p>
<p>[
</p>
<p>w
</p>
<p>z
</p>
<p>]
</p>
<p>
</p>
<p>
</p>
<p>=
1&radic;
</p>
<p>2 &middot; 20
exp
</p>
<p>[
</p>
<p>&minus;1
2
</p>
<p>w2
</p>
<p>20
</p>
<p>]
</p>
<p>&middot; 1&radic;
2 &middot; 32
</p>
<p>exp
</p>
<p>[
</p>
<p>&minus;1
2
</p>
<p>z2
</p>
<p>32
</p>
<p>]</p>
<p/>
</div>
<div class="page"><p/>
<p>412 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>which is the factorization of the joint PDF of Wand Z into the marginal PDFs
</p>
<p>W rv N(0,20) and Z rv N(0,32). Hence, Wand Z are now independent random
</p>
<p>variables, each with a marginal Gaussian PDF. The joint PDF Pw,z is shown in
</p>
<p>Figure 12.19. Note the rotation of the contour plots in Figures 12.18b and 12.19b.
</p>
<p>This rotation was asserted in Example 9.4 (see also Problem 12.48).
</p>
<p>12.8 Joint Moments
</p>
<p>For jointly distributed continuous random variables the k-lth joint moments are
</p>
<p>defined as EX,y[Xkyl]. They are evaluated as
</p>
<p>(12.38)
</p>
<p>An example for k = l = 1 and for a standard bivariate Gaussian PDF of Ex,y[XY]
was given Example 12.13. The k-lth joint central moments are defined as Ex,Y[(X -
</p>
<p>Ex [X])k(Y - Ey[y])l] and are evaluated as
</p>
<p>Ex,Y[(X-Ex [X])k(y _Ey[y])l] = i:i: (x-Ex [X])k (y-Ey[y])lpX,y(x, y)dx dy.
(12.39)
</p>
<p>Of course, the most important case is for k = l = 1 for which we have the cov(X, Y).
</p>
<p>For independent random variables the joint moments factor as
</p>
<p>and similarly for the joint central moments.
</p>
<p>12.9 Prediction of Random Variable Outcome
</p>
<p>In Section 7.9 we described the prediction of the outcome of a discrete random
</p>
<p>variable based on the observed outcome of another discrete random variable. We now
</p>
<p>examine the prediction problem for jointly distributed continuous random variables,
</p>
<p>and in particular, for the case of a bivariate Gaussian PDF. First we plot a scatter
</p>
<p>diagram of the outcomes of the random vector [X Y]T in the x-y plane. Shown in
</p>
<p>Figure 12.20 is the result for a random vector with a zero mean and a covariance
</p>
<p>matrix
</p>
<p>C = [1 0.9]
0.9 1 .
</p>
<p>Note that the correlation coefficient is given by
</p>
<p>(12.40)
</p>
<p>PX,y =
cov(X, Y)
</p>
<p>vvar(X)var(Y)
</p>
<p>0.9
r;--:;- = 0.9.
</p>
<p>v 1&middot;1</p>
<p/>
</div>
<div class="page"><p/>
<p>12.9. PREDICTION OF RANDOM VARIABLE OUTCOME 413
</p>
<p>3,..----~-~-~-~-~-_____,
</p>
<p>2 .... . . . : . .
. . .
</p>
<p>. ~ .. . .
..
</p>
<p>1 : . . ~:;; : ..&bull;.. . . : .
. -. .... :
</p>
<p>.0 . ' ... ::. .: . ~.. :
~ 0 : : ... &bull; , . . .&bull;;# :
</p>
<p>.~ : -. -:; -. :.. .
' . -.. :
</p>
<p>-1 . .. . . . ... . &bull;; .&bull; &bull; ' &bull;&bull;&bull;. . . . . . . . . .. . . . . . . . . . . .
. .~ .. .'.. .
</p>
<p>-2 .
</p>
<p>32o
X
</p>
<p>-1-2
-3 ' - - - ~ - ~ - ~ - ~ - ~ - - - - '
-3
</p>
<p>Figure 12.20: 100 outcomes of bivariate Gaussian random vector with zero means
</p>
<p>and covariance matrix given by (12.40). The best prediction of Y when X = x is
observed is given by the line.
</p>
<p>It is seen from Figure 12.20 that knowledge of X should allow us to predict the
</p>
<p>outcome of Y with some accuracy. To do so we adopt as our error criterion the
</p>
<p>minimum mean square error (MSE) and use a linear predictor or Y = aX +b. From
Section 7.9 the best linear prediction when X = x is observed is
</p>
<p>(12.41)
</p>
<p>For this example the best linear prediction is
</p>
<p>A 0.9
Y = 0 + T(x - 0) = 0.9x (12.42)
</p>
<p>and is shown as the line in Figure 12.20. Note that the error to = Y - 0.9X is also
</p>
<p>a random variable and can be shown to have the PDF to '" N(O,0.19) (see Problem
</p>
<p>12.49) . Finally, note that the predictor, which was constrained to be linear (actually
</p>
<p>affine but the use of the term linear is commonplace) , cannot be improved upon by
</p>
<p>resorting to a nonlinear predictor. This is because it can be shown that the optimal
</p>
<p>predic tor, among all predictors, is linear if (X, Y) has a bivariate Gaussian PDF
</p>
<p>(see Section 13.6). Hence, in this case the prediction of (12.42) is optimal among all
</p>
<p>linear and nonlinear predictors.</p>
<p/>
</div>
<div class="page"><p/>
<p>414 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>12.10 Joint Characteristic Functions
</p>
<p>T he joint characterist ic function for two jointly continuous random variables X and
</p>
<p>Y is defined as
</p>
<p>&cent;X,y (WX, wy) = Ex ,y[exp[j(wxX +wyY)].
</p>
<p>It is evaluated as
</p>
<p>&cent;X,y(wx ,wy) =i:i:PX,y(x,y) exp [j(wx x + wyy)] dxdy
(12.43)
</p>
<p>(12.44)
</p>
<p>(12.45)
</p>
<p>and is seen to be the two-dimensional Fourier transform of the PDF (with a +j
</p>
<p>instead of the more common -j in the exponential). As in the case of discrete
</p>
<p>random variables, the joint moments can be found from the characteristic function
</p>
<p>using the formula
</p>
<p>E [Xkyl] __1_ aHl&cent;X,y(wx,wy) I
X,y - 'Hl awk awl .
</p>
<p>J x y wx =wy=o
</p>
<p>Another important application is in determining the PDF for the sum of two inde-
</p>
<p>pendent cont inuous random variables. As shown in Section 7.10 for discrete random
</p>
<p>variables and also true for jointly continuous random variables, if X and Y are
</p>
<p>indep endent, then the charact eristic function of the sum Z = X + Y is
</p>
<p>&cent;z (w) = &cent;x(w)&cent;y (w). (12.46)
</p>
<p>If we were to take the inverse Fourier t ransform of both sides of (12.46) , then the
</p>
<p>PDF of X +Y would result. Hence, the pro cedure to det ermine the PDF of X +Y ,
where X and Yare independent random variables, is
</p>
<p>1. Find the characteristic function &cent;x(w) by evaluating the Fourier transform
</p>
<p>J ~ o o P x ( x ) exp(jwx)dx and similarly for &cent;y(w).
</p>
<p>2. Multiply &cent;x(w) and &cent;y(w) together to form &cent;x(w)&cent;y(w).
</p>
<p>3. Finally, find the inverse Fourier transform to yield the PDF for the sum Z =
</p>
<p>X+Yas
roo dw
</p>
<p>pz(z) = J- 00 &cent;x(w)&cent;y(w) exp( - jwz) 271"' (12.47)
</p>
<p>Alternatively, one could convolve the PDFs of X and Y using the convolution integral
</p>
<p>of (12.14) to yield the PDF of Z. However, the convolution approach is seldom easier.
</p>
<p>An example follows.
</p>
<p>Example 12.15 - PDF for sum of independent Gaussian random variables
</p>
<p>If X "J N(J.l x ,a3c) and Y "J N(J.lY ,a}) and X and Yare independent, we wish
</p>
<p>to det ermine the PDF of Z = X + Y. A convolut ion approach is explored in</p>
<p/>
</div>
<div class="page"><p/>
<p>12.11. COMPUTER SIMULATION 415
</p>
<p>Problem 12.51. Here we use (12.47) to accomplish the same task. First we need the
</p>
<p>characteristic function of a Gaussian PDF. From Table 11.1 if X ,....., N(J.L, 0'2), then
</p>
<p>Thus, the characteristic function for X + Y is
</p>
<p>(
. 1 2 2) (. 1 2 2)exp JWJ.Lx - 2"O'XW exp JWJ.Ly - 2"O'YW
</p>
<p>exp (iW(J.LX + J.LY) - ~(O'l + 0'~)w2) .
</p>
<p>Since this is again the characteristic function of a Gaussian random variable, albeit
</p>
<p>with different parameters, we have that X +Y ,....., N(J.Lx+J.LY, O'l+O'O. (Recognizing
that the characteristic function is that of a known PDF allows us to avoid inverting
</p>
<p>the characteristic function according to (12.47).) Hence, the PDF of the sum of
</p>
<p>independent Gaussian random variables is again a Gaussian random variable whose
</p>
<p>mean is J.L = J.Lx + J.Ly and whose variance is 0'2 = O'l + O'~. The Gaussian PDF
is therefore called a reproducing PDF. By the same argument it follows that the
</p>
<p>sum of any number of independent Gaussian random variables is again a Gaussian
</p>
<p>random variable with mean equal to the sum of the means and variance equal to the
</p>
<p>sum of the variances. In Problem 12.53 it is shown that the Gamma PDF is also a
</p>
<p>reproducing PDF.
</p>
<p>c
The result of the previous example could also be obtained by appealing to Theorem
</p>
<p>12.7.1. If we let
</p>
<p>then by Theorem 12.7.1, Wand Z = X +Yare bivariate Gaussian distributed. Also,
we know that the marginals of a bivariate Gaussian PDF are Gaussian PDFs and
</p>
<p>therefore the PDF of Z = X +Y is Gaussian. Its mean is J.Lx + J.Ly and its variance
is O'l + O'~, the latter because X and Yare independent and hence uncorrelated.
</p>
<p>12.11 Computer Simulation of Jointly Continuous
</p>
<p>Random Variables
</p>
<p>For an arbitrary joint PDF the generation of continuous random variables is most
</p>
<p>easily done using ideas from conditional PDF theory. In Chapter 13 we will see
</p>
<p>how this is done. Here we will consider only the generation of a bivariate Gaussian
</p>
<p>random vector. The approach is based on the following properties:</p>
<p/>
</div>
<div class="page"><p/>
<p>416 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>1. Any affine transformation of two jointly Gaussian random variables results in
</p>
<p>two new jointly Gaussian random variables. A special case, the linear trans-
</p>
<p>formation, was proven in Section 12.7 and the general result summarized in
</p>
<p>Theorem 12.7.1. We will now consider the affine transformation
</p>
<p>(12.48)
</p>
<p>2. The mean vector and,covariance matrix of [W zjT transform according to
</p>
<p>E [[ : ]] GE [[ ~ ]] + [ : ] (see Problem 9.22) (12.49)
Cw,z = GCX,yGT (see (Theorem 12.7.1))(12.50)
</p>
<p>where we now use subscripts on the covariance matrices to indicate the random
</p>
<p>variables.
</p>
<p>The approach to be described next assumes that X and Yare standard Gaussian and
</p>
<p>independent random variables whose realizations are easily generated. In MATLAB
</p>
<p>the command randn(1, 1) can be used . Otherwise, if only U(O , 1) random variables
</p>
<p>are available, one can use the Box-Mueller transform to obtain X and Y (see Problem
</p>
<p>12.54). Then, to obtain any bivariate Gaussian random variables (W, Z) with a given
</p>
<p>mean [tLw tLz]T and covariance matrix Cw,z, we use (12.48) with a suitable G and
</p>
<p>[a bjT so that
</p>
<p>Cw,z
pawaz]
</p>
<p>2 .
az
</p>
<p>(12.51)
</p>
<p>Since it is assumed that X and Yare zero mean, from (12.49) we choose a = tLw
</p>
<p>and b = tsz- Also, since X and Yare assumed independent, hence uncorrelated,
and with unit variances, we have
</p>
<p>CX,y = [~ ~] = I.
</p>
<p>It follows from (12.50) that Cw,z = GGT. To find G if we are given Cw,z, we could
use an eigendecomposition approach based on the relationship yTCw,zY = A (see
</p>
<p>Problem 12.55). Instead, we next explore an alternative approach which is somewhat
</p>
<p>easier to implement in practice. Let G be a lower triangular matrix</p>
<p/>
</div>
<div class="page"><p/>
<p>12.11. COMPUTER SIMULATION
</p>
<p>Then, we have that
</p>
<p>417
</p>
<p>[
a 0] [a b] [ a2 ab ]
b c 0 c = ab b2 + c2 . (12.52)
</p>
<p>The numerical procedure of decomposing a covariance matrix into a product such as
</p>
<p>GGT , where G is lower triangular, is called the Cholesky decomposition [Golub and
</p>
<p>Van Loan 1983]. Here we can do so almost by inspection. We need only equate the
</p>
<p>elements of Cw,z in (12.51) to those of GGT as given in (12.52). Doing so produces
</p>
<p>the result
</p>
<p>a aw
</p>
<p>b paz
</p>
<p>c azV1 - p2.
</p>
<p>Hence, we have that
</p>
<p>G = [ aw 0 ]
paz a z ~ .
</p>
<p>In summary, to generate a realization of a bivariate Gaussian random vector we first
</p>
<p>generate two independent standard Gaussian random variables X and Y and then
</p>
<p>transform according to
</p>
<p>(12.53)
</p>
<p>As an example, we let J.Lw = J.Lz = 1, aw = oz = 1, and p = 0.9. The constant
PDF contours as well as 500 realizations of [W zV are shown in Figure 12.21. To
verify that the mean vector and covariance matrix are correct, we can estimate these
</p>
<p>quantities using (9.44) and (9.46) which are
</p>
<p>~[[:]]=~j;[::]
</p>
<p>c;,; ~ ~ tl ([: ]-~ [[ :]]) ([:: ]-~ [[ :]]f
where [wmzmV is the mth realization of [W zV. The results and the true values
for M = 2000 are
</p>
<p>~ [[ &bull;&bull; [::::::] Ew,z [[ : ]] [ : ]
C;-,z = [0.9958 0.9077] C = [1 0.9]
</p>
<p>0.9077 1.0166 W,z 0.9 1 .
</p>
<p>The MATLAB code used to generate the realizations and to estimate the mean
</p>
<p>vector and covariance matrix is given next.</p>
<p/>
</div>
<div class="page"><p/>
<p>418 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>:
</p>
<p>-3
</p>
<p>234o
w
</p>
<p>-4 k . - ~ _ ~ _ - - , - _ , , - - - ~ _ ~ _ - - , - - - - J
</p>
<p>-4 -3 -2 -1
</p>
<p>Figure 12.21: 500 outcomes of bivariate Gaussian random vector with mean [lljT
and covariance matrix given by (12.40).
</p>
<p>randn('state',O) %set random number generator to initial value
G=[l 0;0.9 s q r t ( 1 - 0 . 9 ~ 2 ) ] ; %define G matrix
M=2000; %set number of realizations
for m=l:M
</p>
<p>x=randn(l,l);y=randn(l,l); %generate realizations of two
% independent N(O,l) random variables
</p>
<p>wz=G*[x y]'+[l 1]'; %transform to desired mean and covariance
WZ(:,m)=wz; %save realizations in 2 x Marray
</p>
<p>end
</p>
<p>Wmeanest=mean(WZ(l,:)); %estimate mean of W
Zmeanest=mean(WZ(2,:)); %estimate mean of Z
WZbar(l,:)=WZ(l,:)-Wmeanest; %subtract out mean of W
WZbar(2,:)=WZ(2,:)-Zmeanest; %subtract out mean of Z
Cest=[O 0;0 0];
</p>
<p>for m=l:M
</p>
<p>Cest=Cest+(WZbar(:,m)*WZbar(:,m)')/M; %compute estimate of
%covariance matrix
</p>
<p>end
</p>
<p>Wmeanest %write out estimate of mean of W
Zmeanest %write out estimate of mean of Z
Cest %write out estimate of covariance matrix</p>
<p/>
</div>
<div class="page"><p/>
<p>12.12. OPTICAL CHARACTER RECOGNITION
</p>
<p>12.12 Real-World Example - Optical Character
</p>
<p>Recognition
</p>
<p>419
</p>
<p>An important use of computers is to be able to scan a document and automatically
</p>
<p>read the characters. For example, bank checks are routinely scanned to ascertain
</p>
<p>the account numbers, which are usually printed on the bottom. Also, scanners are
</p>
<p>used to take a page of alphabetic characters and convert the text to a computer
</p>
<p>file that can later be edited in a computer. In this section we briefly describe how
</p>
<p>this might be done. A more comprehensive description can be found in [Trier, Jain,
</p>
<p>and Taxt 1996]. To simplify the discussion we consider recognition of the digits
</p>
<p>0,1 ,2, ... , 9 that have been generated by a printer (as opposed to handwritten, the
</p>
<p>recognition of which is much more complex due to the potential variations of the
</p>
<p>characters) . An example of these characters is shown in Figure 12.22. They were
</p>
<p>obtained by printing the characters from a computer to a laser printer and then
</p>
<p>scanning them back into a computer. Note that each digit consists of an 80 x 80
</p>
<p>20
</p>
<p>40
</p>
<p>60 o
20
</p>
<p>40
</p>
<p>60 1
20
</p>
<p>40
</p>
<p>60 2
20
</p>
<p>40
</p>
<p>60 3
80 80 80 80 L...- ~
</p>
<p>20 40 60 80 20 40 60 80 20 40 60 80 20 40 60 80
</p>
<p>20
</p>
<p>40
</p>
<p>60 4
20
</p>
<p>40
</p>
<p>60 5
20
</p>
<p>64060
20
</p>
<p>40
</p>
<p>60 7
80 80 80 80 L...- ~
</p>
<p>20 40 60 80 20 40 60 80 20 40 60 80 20 40 60 80
</p>
<p>20
</p>
<p>40
</p>
<p>60 8
20
</p>
<p>40
</p>
<p>60 9
80 80 L..- --'
</p>
<p>20 40 60 80 20 40 60 80
</p>
<p>Figure 12.22: Scanned digits for optical character recognition.
</p>
<p>ar ray of pixels and each pixel is eit her black or white. This is termed a binary image.
</p>
<p>A magnified version of the digit "I" is shown in Figure 12.23, where the "pixelat ion"
</p>
<p>is clearly evident . Also, some of the black pixels have been omitted due to errors in</p>
<p/>
</div>
<div class="page"><p/>
<p>420 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>the scanning process. In order for a computer to be able to recognize and decode
</p>
<p>10
</p>
<p>20
</p>
<p>30
</p>
<p>m40
</p>
<p>50
</p>
<p>60
</p>
<p>(12.54)
</p>
<p>m = 1,2, ... , 80jn = 1,2, ... , 80
</p>
<p>70
</p>
<p>n
</p>
<p>Figure 12.23: Magnified version of the digit "1" .
</p>
<p>the digits it is necessary to reduce each 80 x 80 image to a number or set of numbers
</p>
<p>that characterize the digit. These numbers are called the features and they compose
</p>
<p>a feature vector that must be different for each digit . This will allow a computer to
</p>
<p>distinguish between the digits and be less susceptible to noise effects as is evident
</p>
<p>in the "I" image. For our example, we will choose only two features, although in
</p>
<p>practice many more are used. A typical feature based on the geometric character of
</p>
<p>the digit images is the geometric moments. It attempts to measure the distribution
</p>
<p>of the black pixels and is completely analogous to our usual joint moments. (Recall
</p>
<p>our motivation of the expected value using the idea of the center of mass of an object
</p>
<p>in Section 11.3.) Let g[m, n] denote the pixel value at location [m, n] in the image,
where m = 1,2, ... ,80, n = 1,2, ... ,80 and either g[m, n] = 1 for a black pixel or
g[m, n] = 0 for a white pixel. Note from Figure 12.23 that the indices for the [m, n]
pixel are specified in matrix format, where m indicates the row and n indicates the
</p>
<p>column. The geometric moments are defined as
</p>
<p>,,80 ,,80 k I [ ]
'lk l] = Lim=l Lin=l m n 9 m, n
</p>
<p>J.L, ,,80 80
Lim=l I:n=l g[m,n]
</p>
<p>If we were to define
</p>
<p>g[m ,n]
p[m,n] = 80 80
</p>
<p>I:m=l I:n=l g[m, n]
</p>
<p>then p[m,n] would have the properties of a joint PMF, in that it is nonnegative and
sums to one. A somewhat better feature is obtained by using the central geometric</p>
<p/>
</div>
<div class="page"><p/>
<p>12.12. OPTICAL CHARACTER RECOGNITION 421
</p>
<p>moments which will yield the same number even as the digit is translated in the
</p>
<p>horizontal and vertical directions. This may be seen to be of value by referring to
</p>
<p>Figure 12.22, in which the center of the digits do not all lie at the same location. Us-
</p>
<p>ing central geometric moments alleviates having to center each digit. The definition
</p>
<p>is
</p>
<p>(12.55)
</p>
<p>where
</p>
<p>iii '[1 0] - 2:~-1 2:~~1 mg[m,n]
= J.L , - ",,80 80 [ ]
</p>
<p>wm=l 2:n=l 9 m, n
</p>
<p>n- '[0 1] _ 2:~=1 2:~~1 ng[m, n]J.L , - 80 80
2:m=l 2:n=l g[m, n]
</p>
<p>The coordinate pair (m, 71,) is the center of mass of the character and is completely
</p>
<p>analogous to the mean of the "joint PDF" p[m, n].
To demonstrate the procedure by which optical character recognition is accom-
</p>
<p>plished we will add noise to the characters. To simulate a "dropout", in which a
</p>
<p>black pixel becomes a white one (see Figure 12.23 for an example), we change each
</p>
<p>black pixel to a white one with a probability of 0.4, and make no change with prob-
</p>
<p>ability of 0.6. To simulate spurious scanning marks we change each white pixel to a
</p>
<p>black one with probability of 0.1, and make no change with probability of 0.9. An
</p>
<p>example of the corrupted digits is shown in Figure 12.24. As a feature vector we will
</p>
<p>use the pair (J.L [I, 1], J.L[2 , 2]). For the digits "I" and "8", 50 realizations of the feature
</p>
<p>vector are shown in Figure 12.25a. The black square indicates the center of mass
</p>
<p>(m,71,) for each digit 's feature vector. Note that we could distinguish between the
</p>
<p>two characters without error if we recognize an outcome as belonging to a "I" if we
</p>
<p>are below the line boundary shown and as a "8" otherwise. However, for the digits
</p>
<p>"I " and "3" there is an overlap region where the outcomes could belong to either
</p>
<p>character as seen in Figure 12.25b. For these digits we could not separate the digits
</p>
<p>without a large error. The latter is more typically the case and can only be resolved
</p>
<p>by using a larger dimension feature vector. The interested reader should consult
</p>
<p>[Duda, Hart, and Stork 2001] for a further discussion of pattern recognition (also
</p>
<p>called pattern classification). Also, note that the digits "3" and "8" would produce
</p>
<p>outcomes that would overlap greatly. Can you explain why? You might consider
</p>
<p>some typical scanned digits as shown in Figure 12.26 that have been designed to
</p>
<p>make recognition easier!</p>
<p/>
</div>
<div class="page"><p/>
<p>422 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>Figure 12.24: Realization of corru pted digits.
</p>
<p>X 10
5
</p>
<p>32 r - - - ~ - - - - - ' - - - - ~ - - ~
X 10
</p>
<p>5
</p>
<p>32 r - - - ~ - - - - - ' - - - - ~ - - ~
</p>
<p>3.1
~
</p>
<p>~
::t 3
</p>
<p>8
</p>
<p>. + +++\ + .
</p>
<p>-l:t*t~\ -+I-
... .. . : ~
</p>
<p>~ .o~~~ 0
2.9 ; '/ . ; 0 .
</p>
<p>3.1
~
</p>
<p>~
::t 3
</p>
<p>2.9
</p>
<p>o 5
1L[1 ,1]
</p>
<p>(b) Digit s 1 and 3
</p>
<p>-5
</p>
<p>2.8 L-__ ~ ~ ~ _ _ ---'
</p>
<p>-10 1010-5 o 5
1L[1 ,1]
</p>
<p>(a) Digit s 1 and 8
</p>
<p>2.8 L-__ ~ ~ ~ __---.J
</p>
<p>-10
</p>
<p>Figure 12.25: 50 realizations of feature vector for two competing digits.</p>
<p/>
</div>
<div class="page"><p/>
<p>REFERENCES
</p>
<p>01238
</p>
<p>423
</p>
<p>Figure 12.26: Some scanned digits typically used in optical character recognition.
</p>
<p>They were scanned into a computer, which accounts for the obvious errors.
</p>
<p>References
</p>
<p>Duda, R.O. , P.E. Hart, D.G. Stork, Pattern Classification, 2nd Ed., John Wiley &amp;
Sons, New York, 2001.
</p>
<p>Golub, G.H., C.F. Van Loan, Matrix Computations, Johns Hopkins University
</p>
<p>Press, Baltimore, MD, 1983.
</p>
<p>Trier, O.D., A.K. Jain, T. Taxt, "Feature Extraction Methods for Character Recog-
</p>
<p>nition - A Survey," Pattern Recognition, Vol. 29, No.4, pp. 641-662, 1996,
</p>
<p>Pergamon, Elsevier Science.
</p>
<p>Problems
</p>
<p>12.1 L . ~ ) (w) For the dartboard shown in Figure 12.1 determine the probability
that the novice dart player will land his dart in the outermost ring, which has
</p>
<p>radii 3/4 :S r :S 1. Do this by using geometrical arguments and also using
double integrals. Hint: For the latter approach convert to polar coordinates
</p>
<p>(r, B) and remember to use dx dy = rdr dB.
</p>
<p>12.2 (c) Reproduce Figure 12.2a by letting X rv U(-1 ,1) and Y rv U(-1,1),
</p>
<p>where X and Yare independent. Omit any realizations of (X, Y) for which
</p>
<p>J X2 + y2 &gt; 1. Explain why this produces a uniform distribution of points in
the unit circle. See also Problem 13.23 for a more formal justification of this
</p>
<p>procedure.
</p>
<p>12.3 C::-) (w) For the novice dart player is prO :S R :S 0.5] = 0.5 (R is the distance
from the center of the dartboard)? Explain your results.
</p>
<p>12.4 (w) Find the volume of a cylinder of height h and whose base has radius r by
using a double integral evaluation.
</p>
<p>12.5 L:-) (c) In this problem we estimate 1r using probability arguments. Let X rv
U( -1,1) and Y rv U( -1,1) for X and Y independent. First relate P[X2+Y2 :S
1] to the value of tt . Then generate realizations of X and Y and use them to
</p>
<p>estimate n,</p>
<p/>
</div>
<div class="page"><p/>
<p>424 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>12.6 (f) For the joint PDF
</p>
<p>{
</p>
<p>I X2 + y2 ~ 1
PX,y(x , y) = On:
</p>
<p>otherwise
</p>
<p>find P [lXJ ~ 1/2]. Hint: You will need
</p>
<p>J~ dx = ~ xJl - x2 + ~arc sin(x).
12.7 (...:..:.-) (f) If a joint PDF is given by
</p>
<p>{
</p>
<p>_c_ 0 ~ x ~ 1, 0 ~ Y ~ 1
PX,y(x , y) = oVXY
</p>
<p>otherwise
</p>
<p>find c.
</p>
<p>12.8 (w) A point is chosen at random from the sample space S = {(x ,y) : 0 ~ x ~
1,0 ~ Y ~ I}. Find pry ~ X].
</p>
<p>12.9 (f) For the joint PDF PX,y(x, y) = exp[-(x + y)]u(x)u(y) , find pry ~ X].
</p>
<p>12.10 (...:..:.-) (w,c) Two persons playa game in which the first person thinks of a
</p>
<p>number from 0 to 1, while the second person tries to guess player one 's number.
</p>
<p>The second player claims that he is telepathic and knows what number the
</p>
<p>first player has chosen. In reality the second player just chooses a number
</p>
<p>at random. If player one also thinks of a number at random, what is the
</p>
<p>probability that player two will choose a number whose difference from player
</p>
<p>one's number is less than 0.1? Add credibility to your solution by simulating
</p>
<p>the game and estimating the desired probability.
</p>
<p>12.11 (...:..:.-) (f) If (X, Y) has a standard bivariate Gaussian PDF, find P[X2 +y 2 =
10].
</p>
<p>12.12 (f,c) Plot the values of (x, y) for which x2 - 2pxy + y2 = 1 for p = -0.9,
p = 0, and p = 0.9. Hint: Solve for y in terms of x .
</p>
<p>12.13 (w,c) Plot the standard bivariate PDF in three dimensions for p = 0.9. Next
</p>
<p>examine your plot if p -+ 1 and determine what happens. As p -+ 1, can you
predict Y based on X = x?
</p>
<p>12.14 (f) If px,y(x , y) = exp[- (x + y)]u(x)u(y) , determine the marginal PDFs.
</p>
<p>12.15 (...:..:.-) (f) If
</p>
<p>PX,y(x , y) = { ~
</p>
<p>find the marginal PDFs.
</p>
<p>0&lt; x &lt; 1,0 &lt; Y &lt; x
otherwise</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 425
</p>
<p>12.16 (t) Assuming that (x , y) 1= (0,0), prove that x2 - 2pxy + y2 &gt; 0 for -1 &lt;
p&lt;l.
</p>
<p>12.17 (f) If px(x) = (1/2) exp[-(1/2)x]u(x) and py(y) = (1/4) exp[-(1/4)y]u(y),
find the joint PDF of X and Y.
</p>
<p>12.18 C....:.,) (f) Determine the joint CDF if X and Yare independent with
</p>
<p>px(x) = { J
</p>
<p>py(y) = { !
</p>
<p>0&lt;x&lt;2
</p>
<p>otherwise
</p>
<p>0&lt;y&lt;4
otherwise.
</p>
<p>x ~ O,y ~ 0
otherwise.
</p>
<p>12.19 (f) Determine the joint CDF corresponding to the joint PDF
</p>
<p>( )
_ { xyexp [ _ ~ ( x 2 +y2)]
</p>
<p>PX,y x,y - 0
</p>
<p>Next verify Properties 12.1-12.6 for the CDF.
</p>
<p>12.20 (t) Prove that (12.10) is true if (12.11) is true and vice versa. Hint: Let
</p>
<p>A = {a ~ x ~ b} and B = {y : c ~ y ~ d} for the first part and let A = {x :
</p>
<p>Xo - 6.x/2 ~ x ~ Xo +6.x/2} and B = {y : Yo - 6.y/2 ~ y ~ Yo + 6.y/2} with
Xo and Yo arbitrary for the second part.
</p>
<p>12.21 (t) Prove that (12.11) and (12.12) are equivalent.
</p>
<p>12.22 (w) Two independent speech signals are added together. If each one has a
Laplacian PDF with parameter (j2, what is the power of the resultant signal?
</p>
<p>12.23 C.":") (w) Lightbulbs fail with a time to failure modeled as an exponential
random variable with a mean time to failure of 1000 hours. If two lightbulbs
</p>
<p>are used to illuminate a room, what is the probability that both bulbs will fail
</p>
<p>before 2000 hours? Assume that the failure time of one bulb does not affect
</p>
<p>the failure time of the other bulb.
</p>
<p>12.24 (f) If a joint PDF is given as PX,Y(x, y) = 6 exp[-(2x + 3y)]u(x)u(y), what
is the probability of A = {(x,y) : 0 &lt; x &lt; 2,0 &lt; y &lt; I}? Are the two random
variables independent?
</p>
<p>12.25 C...:J (w) A joint PDF is uniform over the region {(x , y) : 0 ~ y &lt; x ,0 ~ x &lt;
I} and zero elsewhere. Are X and Y independent?
</p>
<p>12.26 C:...:.,) (w) The temperature in Antarctica is modeled as a random variable
X '" N(20 , 1500) degrees Fahrenheit, while that in Ecuador is modeled also
</p>
<p>as a random variable with Y '" N(lOO , 100) degrees Fahrenheit. What is the
</p>
<p>probability that it will be hotter in Antarctica than in Ecuador? Assume the
</p>
<p>random variables are independent.</p>
<p/>
</div>
<div class="page"><p/>
<p>426 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>12.27 (w,c) In Section 2.3 we discussed the outcomes resulting from adding to-
gether two random variables uniform on (0,1). We claimed that the proba-
</p>
<p>bility of 500 outcomes in the interval [0,0.5] and 500 outcomes in the interval
</p>
<p>[1.5, 2] resulting from a total of 1000 outcomes is
</p>
<p>(150~00) (~) 1000 ~ 2.2 X 10- 604 .
</p>
<p>Can you now justify this result? What assumptions are implicit in its calcula-
</p>
<p>tion? Hint: For each trial consider the 3 possible outcomes (0,0.5), [0.5,1.5),
</p>
<p>and [1.5,2). Also, see Problem 3.48 on how to evaluate expressions with large
</p>
<p>factorials.
</p>
<p>12.28 (f) Find the PDF of X = UI + U2, where UI '" U(O,l), U2 '" U(O, 1), and
UI, U2 are independent. Use a convolution integral to do this.
</p>
<p>12.29 (w) In this problem we show that the ratio of areas for the linear transfor-
</p>
<p>mation
</p>
<p>[:] [~:H:J
---------'-v--'"G e
</p>
<p>is Idet(G)I. To do so let e = [xyjT take on values in the region {(x,y)
</p>
<p>o ~ x ~ 1, 0 ~ y ~ 1} as shown by the shaded area in Figure 12.27. Then,
consider a point in the unit square to be represented as e = ael +{3e2, where
o ~ a ~ 1, 0 ~ {3 ~ 1, ei = [10jT, and e2 = [OljT. The transformed vector is
</p>
<p>Ge = G(ael + (3e2)
aGel + (3Ge2
</p>
<p>~ a[:]+p[:]
It is seen that the natural basis vectors ej , e2 map into the vectors [a ejT,
[b d]T, which appear as shown in Figure 12.27. The region in the w-z plane
that results from mapping the unit square is shown as shaded. The area of the
</p>
<p>parallelogram can be found from Figure 12.28 as BH. Determine the ratio of
</p>
<p>areas to show that
</p>
<p>Area ~ n w- z plane = ad _ be = det(G).
Area III x-y plane
</p>
<p>The absolute value is needed since if for example a &lt; 0, b &lt; 0, the parallelo-
gram will be in the second quadrant and its determinant will be negative. The
</p>
<p>absolute value sign takes care of all the possible cases.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS
</p>
<p>1
</p>
<p>y
</p>
<p>......_--.. x
1
</p>
<p>z
</p>
<p>427
</p>
<p>Figure 12.27: Mapping of areas for linear transformation.
</p>
<p>z
</p>
<p>()
</p>
<p>7r/2 - ()
</p>
<p>Figure 12.28: Geometry to determine area of parallelogram.
</p>
<p>12.30 L..:..) (w,c) The champion dart player described in Section 12.3 is able to
land his dart at a point (x, y) according to the joint PDF
</p>
<p>with some outcomes shown in Figure 12.2b. Determine the probability of a
</p>
<p>bullseye. Next simulate the game and plot the outcomes. Finally estimate the
</p>
<p>probability of a bullseye using the results of your computer simulation.</p>
<p/>
</div>
<div class="page"><p/>
<p>428 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>12.31 (t) Show that (12.19) can be written as (12.20).
</p>
<p>12.32 (t) Consi der the nonlinear transformation w = g(x ,y) , z = h(x ,y). Use a
tangent approximation to both functions about the point (xo,YO) to express
</p>
<p>[w zjT as an approximat e affine function of [x yjT, and use matrix/vector na-
tation. For example,
</p>
<p>Ogl Ogl
w = g(x,y) ~ g(xo,YO) + ox x =xo (x - xo) + oy x =xo (y - YO)
</p>
<p>Y=Yo Y= Yo
</p>
<p>and similarly for z = h(x, y). Compare the matrix to the J acobian matrix of
(12.23).
</p>
<p>12.33 (f) If a joint PDF is given as PX,y(x, y) = (1/4)2 e x p [ -~ ( Ix l + Iyl)] for
- 00 &lt; x &lt; 00, - 00 &lt; y &lt; 00, find the joint PDF of
</p>
<p>12.34 (f) If a joint PDF is given as pX,Y(x,y) = exp[-(x + y)]u(x)u(y) , find the
joint PDF of W = XY, Z = Y/X .
</p>
<p>12.35 (w,c) Consider the nonlinear transformation
</p>
<p>W X 2+5y2
</p>
<p>Z = -5X2 + y2 .
</p>
<p>Write a compute r program to plot in the x-y plane the points ( X i , Yj) for
</p>
<p>Xi = 0.95 + (i - 1)/100 for i = 1,2, .. . , 11 and Yj = 1.95 + (j - 1)/100 for
j = 1,2, . . . , 11. Next transform all these points into the w-z plane using
the given nonlinear t ransformation. What kind of figure do you see? Next
</p>
<p>calculate the area of the figure (you can use a rough approximat ion based on
</p>
<p>the computer generated figure output) and finally take the ratio of the areas
</p>
<p>of the figures in the two planes. Does this ratio agree with the Jacobian factor
</p>
<p>I
det (o(w,z&raquo;)I
</p>
<p>o(x,y)
</p>
<p>when evaluated at x = 1, y = 2?
</p>
<p>12.36 C:...:./ ) (f) Find the marginal PDFs of the joint PDF given in (12.25).
</p>
<p>12.37 (f) Determine the marginal PDFs for the joint PDF given by</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS
</p>
<p>12.38 t.:.:J (f) If X and Y have the joint PDF
</p>
<p>find the joint PDF of the transformed random vector
</p>
<p>429
</p>
<p>pz(z) = i: px(x)py(xz)lxldx.
12.39 (t) Prove that the PDF of Z = Y/X, where X and Y are independent, is
</p>
<p>given by
</p>
<p>12.40 (t) Prove that the PDF of Z = XY, where X and Yare independent is given
</p>
<p>by
reX) 1
</p>
<p>pz(z) = 1-00 px(x)py(z/x)r;rdx.
</p>
<p>12.41 (c) Generate outcomes of a Cauchy random variable using Y/ X, where X '"
</p>
<p>N(O,I) , Y ,...., N(O , 1) and X and Yare independent. Can you explain what
</p>
<p>happens when the Cauchy outcome becomes very large in magnitude?
</p>
<p>12.42 (t) Prove that set) = Acos(27fFot) + Bsin(27fFot) can be written as set) =
JA2 + B2cos(27fFot - arctan (B/A)). Hint: Convert (A,B) to polar coordi-
nates.
</p>
<p>12.43 L..:-) (w) A particle is subject to a force in a random force field. If the
velocity of the particle is modeled in the x and y directions as Vx '" N(O, 10)
</p>
<p>and Vy '" N(o, 10) meters/sec, and Vx and Vy are assumed to be independent,
how far will the particle move on the average in 1 second?
</p>
<p>12.44 (f) Prove that if X and Y are independent standard Gaussian random vari-
</p>
<p>ables, then X 2 + y 2 will have a X~ PDF.
</p>
<p>12.45 C:...:.-) (w,f) Two independent random variables X and Y have zero means and
variances of 1. If they are linearly transformed as W = X + Y, Z = X - Y,
find the covariance between the transformed random variables. Are Wand Z
</p>
<p>uncorrelated? Are Wand Z independent?
</p>
<p>12.46 (f) If
</p>
<p>determine the mean of X + Y and the variance of X + Y.</p>
<p/>
</div>
<div class="page"><p/>
<p>430 CHAPTER 12. MULTIPLE CONTINUOUS RANDOM VARIABLES
</p>
<p>12.47 c.:..:....) (w) The random vector [X YjT has a covariance matrix
</p>
<p>Find a 2 x 2 matrix G so that G[X y]T is a random vector with uncorrelated
</p>
<p>components.
</p>
<p>12.48 (t) Prove that if a random vector has a covariance matrix
</p>
<p>c = [ ~ ~ ]
</p>
<p>then the matrix
</p>
<p>G = [ ~ - ~ ]
Vi Vi
</p>
<p>can always be used to diagonalize it. Show that the effect of this matrix
</p>
<p>transformation is to rotate the point (x, y) by 45&deg; and relate this back to the
</p>
<p>contours of a standard bivariate Gaussian PDF.
</p>
<p>12.49 (f) Find the MMSE estimator of Y based on observing X = x if (X,Y) has
the joint PDF
</p>
<p>1 [1 2 2]PX,y(x, y) = 27fyl[19 exp - 2(0.19) (x - 1.8xy + y) .
Also, find the PDF of the error Y - Y = Y - (aX + b), where a, b are the
optimal values. Hint: See Theorem 12.7.1.
</p>
<p>12.50 (w,c) A random signal voltage V rv N(l, 1) is corrupted by an independent
noise sample N, where N rv N(O, 2), so that V + N is observed. It is desired
to estimate the signal voltage as accurately as possible using a linear MMSE
</p>
<p>estimator. Assuming that V and N are independent, find this estimator. Then
</p>
<p>plot the constant PDF contours for the random vector (V +N, V) and indicate
</p>
<p>the estimated values on the plot.
</p>
<p>12.51 (f) Using a convolution integral prove that if X and Yare independent stan-
</p>
<p>dard Gaussian random variables, then X + Y rv N(O, 2).
</p>
<p>12.52 C..:....) (f) If
</p>
<p>find P[X + Y &gt; 2].</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 431
</p>
<p>12.53 (t) Prove that if X rv r(ax, A) and Y rv I'(oy , A) and X and Yare inde-
</p>
<p>pendent, then X + Y rv r(ax +ay,A).
</p>
<p>12.54 (f) To generate two independent standard Gaussian random variables on a
computer one can use the Box-Mueller transform
</p>
<p>X =
Y
</p>
<p>J -21n Ul cOS(27l"U2)
J -21n o. sin(27l"U2)
</p>
<p>where U1 , U2 are both uniform on (0,1) and independent of each other. Prove
</p>
<p>that this result is true. Hint: To find the inverse transformation use a polar
</p>
<p>coordinate transformation.
</p>
<p>12.55 (t) Prove that by using the eigendecomposition of a covariance matrix or
</p>
<p>v"C'V = A that one can factor Case = GGT , where G = Y..fA, and ..fA
is defined as the matrix obtained by taking the positive square roots of all the
</p>
<p>elements. Recall that A is a diagonal matrix with positive elements on the
</p>
<p>main diagonal. Next find G for the covariance matrix
</p>
<p>C = [26 6]
6 26
</p>
<p>and verify that GGT does indeed produce C.
</p>
<p>12.56 (c) Simulate on the computer realizations of the random vector
</p>
<p>[ : ] ~ N ([ : ] , [-~.9 -~.9]) .
</p>
<p>Plot these realizations as well as the contours of constant PDF on the same
</p>
<p>graph.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 13
</p>
<p>Conditional Probability Density
</p>
<p>Functions
</p>
<p>13.1 Introduction
</p>
<p>A discussion of conditional probability mass functions (PMFs) was given in Chapter
</p>
<p>8. The motivation was that many problems are stated in a conditional format so
</p>
<p>that the solution must naturally accommodate this conditional structure. In addi-
</p>
<p>tion, the use of conditioning is useful for simplifying probability calculations when
</p>
<p>two random variables are statistically dependent. In this chapter we formulate the
</p>
<p>analogous approach for probability density functions (PDFs). A potential stum-
</p>
<p>bling block is that the usual conditioning event X = x has probability zero for a
continuous random variable. As a result the conditional PMF cannot be extended
</p>
<p>in a straightforward manner. We will see, however, that using care, a conditional
</p>
<p>PDF can be defined and will prove to be useful.
</p>
<p>13.2 Summary
</p>
<p>The conditional PDF is defined in (13.3) and can be used to find conditional proba-
</p>
<p>bilities using (13.4). The conditional PDF for a standard bivariate Gaussian PDF is
</p>
<p>given by (13.5) and is seen to retain its Gaussian form. The joint, conditional, and
</p>
<p>marginal PDFs are related to each other as summarized by Properties 13.1-13.5. A
</p>
<p>conditional CDF is defined by (13.6) and is evaluated using (13.7). The use of condi-
</p>
<p>tioning can simplify probability calculations as described in Section 13.5. A version
</p>
<p>of the law of total probability is given by (13.12) and is evaluated using (13.13). An
</p>
<p>optimal predictor for the outcome of a random variable based on the outcome of a
</p>
<p>second random variable is given by the mean of the conditional PDF as defined by
</p>
<p>(13.14). An example is given for the bivariate Gaussian PDF in which the predictor
</p>
<p>becomes linear (actually affine). To generate realizations of two jointly distributed</p>
<p/>
</div>
<div class="page"><p/>
<p>434 CHAPTER 13. CONDITIONAL PROBABILITY DENSITY FUNCTIONS
</p>
<p>continuous random variables the procedure based on conditioning and described in
</p>
<p>Section 13.7 can be used. Lastly, an application to determining mortality rates for
</p>
<p>retirement planning is described in Section 13.8.
</p>
<p>13.3 Conditional PDF
</p>
<p>(13.1)j = 1,2, ....
</p>
<p>Recall that for two jointly discrete random variables X and Y , the conditional PMF
</p>
<p>is defined as
</p>
<p>[ I ]
PX,y[Xi , Yj]
</p>
<p>PYIX Yj Xi = []
PX Xi
</p>
<p>This formula gives the probability of the event Y = Yj for j = 1,2, ... once we have
</p>
<p>observed that X = Xi. Since X = Xi has occurred, the only joint events with a
</p>
<p>nonzero probability are {( x , y) : x = Xi, Y = Yl, Y2,"'}' As a result we divide the
joint probability PX,y[Xi , Yj] = P[X = Xi, Y = Yj] by the probability of the reduced
sample space, which is PX[Xi] = P[X = Xi, Y = vil + P[X = Xi, Y = Y2] + ... =
I: ~l PX,Y[Xi , Yj]&middot; This division assures us that
</p>
<p>00
</p>
<p>LPYlx[Yjl xi]
j = l
</p>
<p>f PX,Y[Xi ,Yj]
j = l PX[Xi]
</p>
<p>I:~l PX,Y[Xi , Yj]
=
</p>
<p>PX[Xi]
</p>
<p>I:~l PX,y[Xi , Yj] = 1
I:~lP X , Y [ Xi ,Y j] .
</p>
<p>In the case of continuous random variables X and Y a problem arises in defining
</p>
<p>a conditional PDF. If we observe X = x, then since P[X = x] = 0, the use of a
formula like (13.1) is no longer valid due to the division by zero. Recall that our
</p>
<p>original definition of the conditional probability is
</p>
<p>P[AIB] = P[A nB]
P[B]
</p>
<p>which is undefined if P[B] = O. How should we then proceed to extend (13.1) for
continuous random variables?
</p>
<p>We will motivate a viable approach using the example of the circular dartboard
</p>
<p>described in Section 12.3. In particular, we consider a revised version of the dart
</p>
<p>throwing contest. Referring to Figure 12.2 the champion dart player realizes that the
</p>
<p>novice presents little challenge. To make the game more interesting the champion
</p>
<p>proposes the following modification. If the novice player's dart lands outside the
</p>
<p>region Ixi ~ t1x/2 , then the novice player gets to go again. He continues until his
dart lands within the region Ixi ~ t1x/2 as shown cross-hatched in Figure 13.1a.
The novice dart player even gets to pick the value of t1x. Hence, he reasons that it</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3. CONDITIONAL PDF
</p>
<p>(a) Dartboard
</p>
<p>435
</p>
<p>y
</p>
<p>-b.x/2
</p>
<p>- + - - - ~ ~ I - - - + - " X
</p>
<p>radius = 1/4
</p>
<p>(b) Sample space
</p>
<p>Figure 13.1: Revised dart throwing game. Only dart outcomes in the cross-hatched
</p>
<p>region are counted.
</p>
<p>should be small to exclude regions of the dart board that are outside the bullseye
</p>
<p>circle. As a result , he chooses a 6.x as shown in Figure 13.1b, which allows him
</p>
<p>to continue throwing darts until one lands within the cross-hatched region. The
</p>
<p>champion, however , has taken a course in probability and so is not worried. In fact,
</p>
<p>in Problem 12.30 the probability of the champion's dart landing in the bullseye area
</p>
<p>was shown to be 0.8646. To find the probability of the novice player obtaining a
</p>
<p>bullseye, we recall that his dart is equally likely to land anywhere on the dartboard.
</p>
<p>Hence , using conditional probability we have that
</p>
<p>P[b 11 I
_ A /2 X A /2] = P[bullseye, -6.x/2 ~ X ~ 6.x/2]
</p>
<p>u seye u X ~ ~ uX P[-6.x/2 ~ X ~ 6.x/2] .
</p>
<p>Since 6.x/2 is small, we can assume that it is much less than 1/4 as shown in Figure
</p>
<p>13.1b. Therefore, we have that the cross-hatched regions can be approximated by
</p>
<p>rectangles and so
</p>
<p>P[bullseyel - 6.x /2 ~ X ~ 6.x /2]
</p>
<p>P[double cross-hatched region]
</p>
<p>(probability = rectangle area/dartboard area)=
</p>
<p>P[double cross-hatched region] + P[single cross-hatched region]
</p>
<p>6.x{1/2)/7r
</p>
<p>. 6.x(2)/7r
</p>
<p>0.25 &lt; 0.865. (13.2)
</p>
<p>Hence , the revised strategy will still allow the champion to have a higher probability
</p>
<p>of winning for any 6.x , no matter how small it is chosen. Even though P[X = 0] = 0,</p>
<p/>
</div>
<div class="page"><p/>
<p>436 CHAPTER 13. CONDITIONAL PROBABILITY DENSITY FUNCTIONS
</p>
<p>the conditional probability is well defined even as .6.x --7 0 (but not equal 0). Some
</p>
<p>typical outcomes of this game are shown in Figure 13.2, where it is assumed the
</p>
<p>novice player has chosen .6.x/2 = 0.2. In Figure 13.2a are shown the outcomes of X
</p>
<p>OB &middot; .
</p>
<p>0.6
</p>
<p>0.4
</p>
<p>-0.6
</p>
<p>-0.4
</p>
<p>. .. ... . .
</p>
<p>!~: II tNrl'~ ~I
-0.8 .
</p>
<p>1008040 60
Trial number
</p>
<p>20
</p>
<p>-1 '--__--'-__----J. --'--__--'-__-"'
</p>
<p>o
</p>
<p>(a) All x outcomes - those with Ixl ::; 0.2 are shown as dark
lines
</p>
<p>0.8
</p>
<p>0.6 : .
</p>
<p>0.4
</p>
<p>o
</p>
<p>0.2
Q)
</p>
<p>&sect;
u...,
;::l
</p>
<p>o -0.2
&lt;:ll
</p>
<p>1008020
</p>
<p>-0.6
</p>
<p>-0.8 .
</p>
<p>-1 '-=-__--'-__----J. --'----'----_--'-__-"'
</p>
<p>o
</p>
<p>(b) Dark lines are y outcomes for which [z] ::; 0.2, b indicates
</p>
<p>a bullseye (";x 2 + y2 ::; 1/4) for the outcomes with [z] ::; 0.2
</p>
<p>Figure 13.2: Revised dart throwing game outcomes.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3. CONDITIONAL PDF 437
</p>
<p>for the novice player. Only those for which [z] :::; !:i.x/2 = 0.2, which are shown as
</p>
<p>the darker lines , are kept. In Figure 13.2b the outcomes of Yare shown with the
</p>
<p>kept outcomes shown as the dark lines. Those outcomes that resulted in a bullseye
</p>
<p>are shown with a "b" over them. Note that there were 27 out of 100 outcomes that
</p>
<p>had [z] values less than or equal to 0.2 (see Figure 13.2a), and of these, 8 outcomes
</p>
<p>resulted in a bullseye (see Figure 13.2b) . Hence, the estimated probability of landing
</p>
<p>in either the single or double cross-hatched region of Figure 13.1b is 27/100 = 0.27,
</p>
<p>while the theoretical probability is approximately !:i.x(2)/7r = 0.4(2)/7r = 0.254.
</p>
<p>Also, the estimated conditional probability of a bullseye is from Figure 13.2b , 8/27 =
0.30 while from (13.2) the theoretical probability is approximately equal to 0.25.
</p>
<p>(The approximations are due to the use of rectangular approximations to the cross-
</p>
<p>hatched regions, which only become exact as !:i.x --+ 0.) We will use the same
</p>
<p>strategy to define a conditional PDF. Let A = {(x , y) : Jx2+ y2 :::; 1/4}, which is
the bullseye region. Then
</p>
<p>P[AIIXI :::; !:i.x/2]
</p>
<p>=
</p>
<p>P[A , IXI :::; !:i.x/2]
P[IXI :::; !:i.x/2]
</p>
<p>P[{ (x, y) : Ixl :::; !:i.x/2, Iyl :::; J1/16 - x2}]
P[{(x,y) : Ixl :::; !:i.x/2, Iyl :::; I}]
</p>
<p>P[{( x , y) : Ixl :::; !:i.x/2, Iyl :::; J1/16 - x2}]
</p>
<p>P[{x : Ixl :::; !:i.x/2}]
</p>
<p>f l:&gt;.X/2 fJ1/16-x2 ()d d-l:&gt;.x/2 -J1/16-x2PX,Y x, Y Y x
</p>
<p>f~~~~2PX(x)dx
</p>
<p>(definition of condo prob.)
</p>
<p>(
double cross-hatched area)
</p>
<p>cross-hatched area
</p>
<p>As !:i.x --+ 0, we can write
</p>
<p>P[AIIXI :::; !:i.x/2]
</p>
<p>f
l:&gt;.X/2 f1/4
-l:&gt;.x/2 -1/4 PX,Y(x , y)dy dx
</p>
<p>f~~~~2PX(x)dx
</p>
<p>f
1/4
-1 /4PX,Y(O, y)!:i.xdy
</p>
<p>Px(O)!:i.x
</p>
<p>1
1/4 PX,y(O,y) d
</p>
<p>-1 /4 Px(O) y.
</p>
<p>(since J1/16 - x2 ~ 1/4 for Ixl :::; !:i.x/2)
</p>
<p>(since PX,y(x, y) ~ PX,y(O,y) for Ixl :::; !:i.x/2)
</p>
<p>We now define PX,y /px as the conditional PDF
</p>
<p>PX,y(x, y)
PYlx(ylx) = ().
</p>
<p>PX x
(13.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>438 CHAPTER 13. CONDITIONAL PROBABILITY DENSITY FUNCTIONS
</p>
<p>Note that it is well defined as long as px(x) i= 0. Thus, as ~x -+ &deg;
</p>
<p>1
1/ 4
</p>
<p>P[AIIXI ~ ~ x / 2 ] = PYlx(yIO)dy.
-1/4
</p>
<p>More generally, the conditional PDF allows us to compute probabilities as (see
</p>
<p>Problem 13.6)
</p>
<p>P[a ~ Y ~ blx - ~x/2 ~ X ~ x + ~x/2] = lb PYlx(ylx)dy.
This probability is usually written as
</p>
<p>P[a ~ y ~ blX = x]
</p>
<p>but the conditioning event should be understood to be {x : x-~x/2 ~ X ~ x+~x/2}
for ~x small. Finally, with this understanding, we have that
</p>
<p>P[a ~ y ~ blX = x] = lb PYlx(ylx)dy (13.4)
where PYIX is defined by (13.3) and is termed the conditional PDF. The condi-
</p>
<p>tional PDF PYlx(ylx) is the probability per unit length of Y when X = x (actually
</p>
<p>x - ~x/2 ~ X ~ x + ~x/2) is observed. Since it is found using (13.3), it is seen
to be a function of y and x . It should be thought of as a family of PDFs with y as
</p>
<p>the independent variable, and with a different PDF for each value x. An example
</p>
<p>follows.
</p>
<p>Example 13.1 - Standard bivariate Gaussian PDF
</p>
<p>Assume that (X, Y) have the joint PDF
</p>
<p>-00 &lt; x &lt; 00
-00 &lt; Y &lt; 00
</p>
<p>and note that the marginal PDF for X is given by
</p>
<p>1 [1 2]px(x) = V2ii exp -2"x .
The conditional PDF is found from (13.3) as
</p>
<p>21r~ exp [_~(x2 - 2pxy + y2)]
</p>
<p>vh- exp [-!x 2 ]
</p>
<p>J27f(~ _ p2) exp ( -~Q)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3. CONDITIONAL PDF
</p>
<p>where
</p>
<p>439
</p>
<p>Q =
x2 - 2pxy + y2
</p>
<p>1- p2
</p>
<p>y2 _ 2pxy + p2x2
1- p2
</p>
<p>(y _ px)2
</p>
<p>1- p2 .
</p>
<p>As a result we have that the conditional PDF is
</p>
<p>1 [1 2]
PYlx(ylx) = J27r(I _ p2) exp - 2(1 _ p2) (y - px) (13.5)
</p>
<p>and is seen to be Gaussian. This result, although of great importance, is not true
</p>
<p>in general. The form of the PDF usually changes from py(y) to PYlx(ylx). We will
</p>
<p>denote this conditional PDF in shorthand notation as YI(X = x) '" N(px, 1 _ p2).
</p>
<p>As expected, the conditional PDF depends on x, and in particular the mean of the
</p>
<p>conditional PDF is a function of x. It is a valid PDF in that for each x value, it is
</p>
<p>nonnegative and integrates to 1 over -00 &lt; Y &lt; 00. These properties are true in
general. In effect , the conditional PDF depends on the outcome of X so that we use a
</p>
<p>different PDF for each X outcome. For example, if p = 0.9 and we observe X = -1 ,
then to compute P[-I ::; Y ::; -0.8IX = -1] and P[-O.I ::; Y ::; O.IIX = -1], we
first observe from (13.5) that YI(X = -1) '" N( -0.9,0.19). Then
</p>
<p>P[-I ::; Y ::; -0.8IX = -1]
</p>
<p>P[-O.I ::; Y ::; O.I!X = -1]
</p>
<p>(
- 1 - (-0.9)) (-0.8 - (-0.9))
</p>
<p>Q 0U9 - Q 0U9 = 0.1815
0.19 0.19
</p>
<p>Q (-0.1- (-0.9)) _ Q (0.1- (-0.9)) = 0.0223.
0U9 JO.I9
</p>
<p>Can you explain the difference between these values? (See Figure I3.3b where the
</p>
<p>dark lines indicate y = 0 and y = -0.9.) In Figure I3.3b the cross-section of the
joint PDF is shown. Once the cross-section is normalized so that it integrates to
</p>
<p>one, it becomes the conditional PDF PYlx(y l - 1). This is easily verified since
</p>
<p>PYlx(yl- 1) =
px,y(-I,y)
</p>
<p>px(-1)
</p>
<p>px,y(-I,y)</p>
<p/>
</div>
<div class="page"><p/>
<p>440 CHAPTER 13. CONDITIONAL PROBABILITY DENSITY FUNCTIONS
</p>
<p>: ':-' ,
</p>
<p>. ~ . . -
</p>
<p>. . . .
. ~ - . .
</p>
<p>. ... . .
</p>
<p>.. .... ..
</p>
<p>o
</p>
<p>(a) (b)
</p>
<p>Figure 13.3: Standard bivari at e Gaussian PDF and its cross-sect ion at x = -1. The
normalized cross-sect ion is the condit ional PDF.
</p>
<p>13.4 J oint, Conditional, and Marginal PDFs
</p>
<p>T he relationships between the joint , conditional, and marginal PMFs as describ ed
</p>
<p>in Sect ion 8.4 also hold for the corresponding PDFs. Hence, we just summarize the
</p>
<p>properties and leave the proofs to the reader (see Problem 13.11).
</p>
<p>Property 13.1 - Joint PDF yields conditional PDFs.
</p>
<p>PXIY(xly )
</p>
<p>PX,y(x, y)
</p>
<p>J~ ooPX,y(x, y)dy
</p>
<p>PX,y(x, y)
</p>
<p>D
</p>
<p>P roperty 13.2 - Conditional PDFs are related.
</p>
<p>( I )
- PYlx(y lx )px (x)
</p>
<p>PXIY x y - ( )
PY Y
</p>
<p>D</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4. JOINT, CONDITIONAL, AND MARGINAL PDFS
</p>
<p>Property 13.3 - Conditional PDF is expressible using Bayes' rule.
</p>
<p>( I )
PXIY(xly)py(y)
</p>
<p>PYIX y x = foo
-00 PXIY(xly)py(y)dy
</p>
<p>441
</p>
<p>o
</p>
<p>Property 13.4 - Conditional PDF and its corresponding marginal PDF
</p>
<p>yields the joint PDF
</p>
<p>PX,y(x, y) = PYlx(ylx)px(x) = PXIY(xly)py(y)
</p>
<p>o
</p>
<p>Property 13.5 - Conditional PDF and its corresponding marginal PDF
</p>
<p>yields the other marginal PDF
</p>
<p>py(y) = i:PYIX (ylx)p x (x)dx
o
</p>
<p>A conditional CDF can also be defined. Based on (13.4) we have upon letting
</p>
<p>a = -00 and b = y
</p>
<p>pry s ylX = x] = i~ PYlx(tlx)dt.
</p>
<p>As a result the conditional CDF is defined as
</p>
<p>Fy1x(yl x) = pry s ylX = x]
</p>
<p>and is evaluated using
</p>
<p>(13.6)
</p>
<p>(13.7)
</p>
<p>As an example, if YI(X = x) '" N(px, 1 - p2) as was shown in Example 13.1, we
have that
</p>
<p>(
y-px )
</p>
<p>FYlx(ylx) = 1- Q ~ .
1 _p2
</p>
<p>(13.8)
</p>
<p>Finally, as previously mentioned in Chapter 12 two continuous random variables
</p>
<p>X and Yare independent if and only if the joint PDF factors as PX,y(x , y) =
</p>
<p>px(x)py(y) or equivalently if the joint CDF factors as Fx,y(x , y) = Fx(x)Fy(y) .</p>
<p/>
</div>
<div class="page"><p/>
<p>442 CHAPTER 13. CONDITIONAL PROBABILITY DENSITY FUNCTIONS
</p>
<p>T his is consistent with our definition of the conditional PDF since if X and Yare
</p>
<p>independent
</p>
<p>PX,y(x , y)
=
</p>
<p>px(x)
</p>
<p>px(x)p y(y) _ ()
= () - PY Y
</p>
<p>PX x
(13.9)
</p>
<p>(and similarly PXIY = px). Hence, the condit ional PDF no longer depends on the
</p>
<p>observed value of X , i.e., x. This means that the knowledge that X = x has occurred
</p>
<p>does not affect the PDF of Y (and thus does not affect the probability of events
</p>
<p>defined on Sy). Similarly, from (13.7), if X and Yare independent
</p>
<p>FYIX(ylx) = 1:00 PYlx(tl x)dt
</p>
<p>i ~ Py(t)dt (from (13.9))
</p>
<p>Fy(y).
</p>
<p>An example would be if p = 0 for the standard bivariate Gaussian PDF. Then since
Y I(X = x) "" N(px , 1 - p2) = N (O , 1), we have that PYlx(yl x) = py(y). Also, from
(13.8)
</p>
<p>1- Q ( y - px )
\/1- p2
</p>
<p>1 - Q(y) = Fy(y) .
</p>
<p>Another example follows.
</p>
<p>Example 13.2 - Lifetime PDF of spare lightbulb
</p>
<p>A professor uses the overhead projector for his class. The time to failure of a new
</p>
<p>bulb X has the exponential PDF px(x) = &gt;.exp(-&gt;.x)u(x), where x is in hours. A
</p>
<p>new spare bulb also has a time to failure Y that is modeled as an exponential PDF.
</p>
<p>However , the time to failure of the spare bulb depends upon how long the spare
</p>
<p>bulb sits unused. Assuming the spare bulb is act ivated as soon as the original bulb
</p>
<p>fails, the time to activation is given by X. As a result , the expected time to failure
</p>
<p>of the spare bulb is decreased as
</p>
<p>1 1
=----
</p>
<p>&gt;.y &gt;'(1 + ax)
</p>
<p>where 0 &lt; a &lt; 1 is some factor that indicates the degradation of the unused bulb
with st orage time. The expected t ime to failure of the spare bulb decreases as the</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4. JOINT, CONDITIONAL, AND MARGINAL PDFS 443
</p>
<p>1.5 r----,------,---.,.------.------y--,...-----,
</p>
<p>54
</p>
<p>:x = 1. .
</p>
<p>31 2
</p>
<p>Y (hours)
o
</p>
<p>I
</p>
<p>... . . 1.
</p>
<p>i
I
</p>
<p>I
</p>
<p>i
I
</p>
<p>... .... 1
</p>
<p>i
I
</p>
<p>-1
</p>
<p>oL-_--'-_---l.__-'---_----'--_.:::::::i=""""'......_-l
-2
</p>
<p>0.5
</p>
<p>Figure 13.4: Conditional PDF for lifetime of spare bulb. Dependence is on time to
</p>
<p>failure x of original bulb.
</p>
<p>original bulb is used longer (and hence the spare bulb must sit unused longer). Thus,
</p>
<p>we model the time to failure of the spare bulb as
</p>
<p>PYlx(ylx) = .xy exp( -.xyy)u(y)
</p>
<p>= .x(1+ ax) exp [-.x(1 + ax)y] u(y).
</p>
<p>This conditional PDF is shown in Figure 13.4 for 1/.x = 5 hours and a = 0.5. We
now wish to determine the unconditional PDF of the time to failure of the spare
</p>
<p>bulb which is Py(y). It is expected that the probability of failure of the spare bulb
</p>
<p>will increase than if the spare bulb were used rightaway or for x = O. Note that if
x = 0, then PYIX = Fx , which says that the spare bulb will fail with the same PDF
</p>
<p>as the original bulb. Using Property 13.5 we have
</p>
<p>py(y) = i: PYlx(yl x)px(x)dx
100 .x(1+ ax) exp [-.x(1 + ax)y].x exp( -.xx)dx
.x
</p>
<p>2
exp( -.xy)100 (1 + ax) exp [-.x(ay + 1)x]dx
</p>
<p>.x
2
</p>
<p>exp( -.xy) [100 exp(ax)dx + a 100 xexp(ax)dx] (let a = -.x(1 + ay))
.x
</p>
<p>2exp(-.xy) [ e x p ~ a x ) [ +a ( ~ x e x p ( a x ) - :2 exp(aX)) [ ]
</p>
<p>.x
2
</p>
<p>exp( -.xy) [ - ~ + ~]
</p>
<p>.x
2
</p>
<p>exp( -.xy) [.x(a:+ 1) + [.x(aya+ 1)]2] .</p>
<p/>
</div>
<div class="page"><p/>
<p>444 CHAPTER 13. CONDITIONAL PROBABILITY DENSITY FUNCTIONS
</p>
<p>or finally
</p>
<p>pY (y) = 
2 exp(&minus;y)
</p>
<p>[
</p>
<p>1
</p>
<p>(y + 1)
+
</p>
<p>
</p>
<p>[(y + 1)]2
</p>
<p>]
</p>
<p>u(y).
</p>
<p>This is shown in Figure 13.5 for 1/ = 5 hours and  = 0.5 along with the PDF
pX(x) of the time to failure of the original bulb. As expected the probability of the
</p>
<p>&minus;2 0 2 4 6 8 10
0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>pY (y)
</p>
<p>pX(x)
</p>
<p>Time to failure (hours)
</p>
<p>Figure 13.5: PDFs for time to failure of original bulb X and spare bulb Y .
</p>
<p>spare bulb failing before 2 hours is greatly increased.
&diams;
</p>
<p>Finally, note that the conditional PDF is obtained by differentiating the conditional
CDF. From (13.7) we have
</p>
<p>pY |X(y|x) =
&part;FY |X(y|x)
</p>
<p>&part;y
. (13.10)
</p>
<p>13.5 Simplifying Probability Calculations
</p>
<p>Using Conditioning
</p>
<p>Following Section 8.5 we can easily find the PDF of Z = g(X,Y ) if X and Y are
independent by using conditioning. We shall not repeat the argument other than to
summarize the results and give an example. The procedure is
</p>
<p>1. Fix X = x and let Z|(X = x) = g(x, Y ).
</p>
<p>2. Find the PDF of Z|(X = x) using the standard approach for a transformation
of a single random variable from Y to Z.
</p>
<p>3. Uncondition the conditional PDF to yield the desired PDF pZ(z).</p>
<p/>
</div>
<div class="page"><p/>
<p>13.5. SIMPLIFYING PROBABILITY CALCULATIONS 445
</p>
<p>Example 13.3 - PDF for ratio of independent random variables
</p>
<p>Consider the function Z = Y/X where X and Yare independent random variables.
In Problem 12.39 we asserted that
</p>
<p>PZ(z) = i: px(x)py(xz)lxldx.
We now derive this using the aforementioned approach. First recall that if Z = aY
for a a constant, then pz(z) = py(z/a)/Ial (see Example 10.5). Now we have that
</p>
<p>ZI(X = x) = ~ I(X = x) = ~ I(X = x)
</p>
<p>so that with a = l/x and noting the independence of X and Y, we have
</p>
<p>PZlx(zlx) = PYlx(zx)lxl = py(zx)lxl
</p>
<p>and thus
</p>
<p>(13.11)
</p>
<p>PZ(z) [: pZ,x(z,x)dx
</p>
<p>i:PZlx(zjx)px(x)dx
[: py(zx)lxlpx(x)dx
</p>
<p>[: px(x)py(xz)lxldx.
</p>
<p>(marginal PDF from joint PDF)
</p>
<p>(definition of conditional PDF)
</p>
<p>(from (13.11))
</p>
<p>Note that without the independence assumption, we could not assert that PYIX = py
</p>
<p>in (13.11).
</p>
<p>c
In general to compute probabilities of events it is advantageous to use conditioning
</p>
<p>arguments whether or not X and Yare independent. The analogous result to (8.28)
</p>
<p>is (see Problem 13.15)
</p>
<p>pry E A] = i: pry E AIX = x]px(x)dx. (13.12)
This is another form of the theorem of total probability. It can also be written as
</p>
<p>pry E A] = i: [i PY1x(Y1X)dY] px(x)dx (13.13)
where we have used (13.4) and replaced {y : a :s; Y :s; b} by the more general set
A. The formula of (13.13) is analogous to (8.27) for discrete random variables. An
</p>
<p>example follows.</p>
<p/>
</div>
<div class="page"><p/>
<p>446 CHAPTER 13. CONDITIONAL PROBABILITY DENSITY FUNCTIONS
</p>
<p>Example 13.4 - Probability of error for a digital communication system
</p>
<p>Consider the PSK communication system shown in Figure 2.14. The probability of
</p>
<p>error was shown in Section 10.6 to be
</p>
<p>P; = P[W ::; -A/2] = Q(A/2)
</p>
<p>since the noise sample W rv N(O, 1). In a wireless communication system such
</p>
<p>as is used in cellular telephone, the received amplitude A varies with time due to
</p>
<p>multipath propagation [Rappaport 2002]. As a result, it is usually modeled as a
</p>
<p>Rayleigh random variable whose PDF is
</p>
<p>()
{
</p>
<p>~ exp (- 2=21 a2 ) a 2: 0
PAa = (TA (TA
</p>
<p>o a &lt; O.
</p>
<p>We wish to determine the probability of error if A is a Rayleigh random variable.
</p>
<p>Thus, we need to evaluate P[W +A/2 ::; 0] if W rv N(O , 1), A is a Rayleigh random
variable, and we assume Wand A are independent. A straightforward approach is
</p>
<p>to first find the PDF of Z = W + A/2, and then to integrate pz(z) from -00 to O.
Alternatively, it is simpler to use (13.12) as follows.
</p>
<p>r; P[W ::; -A/2]i: P[W ::; -A/2IA = a]PA(a)da (from (13.12))
i: P[W ::; -a/2IA = a]PA(a)da (since A = a has occurred).
</p>
<p>But since Wand A are independent, P[W ::; -a/2IA = a] = P[W ::; -a/2] and
</p>
<p>thus
</p>
<p>r; = i: P[W ::; -a/2]PA(a)da.
Using P[W ::; -a/2] = Q(a/2) we have
</p>
<p>r; = roo Q(a/2)-; exp (_~a2) da .
io aA 2aA
</p>
<p>Unfortunately, this is not easily evaluated in closed form .
</p>
<p>13.6 Mean of Conditional PDF
</p>
<p>For a conditional PDF the mean is given by the usual mean definition except that
</p>
<p>the PDF now depends on x . We therefore have the definition
</p>
<p>(13.14)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.7. COMPUTER SIMULATION 447
</p>
<p>which is analogous to (8.29) for discrete random variables. We also expect and it
follows that (see Problem 13.19 and also the discussion in Section 8.6)
</p>
<p>EX [EY |X [Y |X]] = EY [Y ] (13.15)
</p>
<p>where EY |X [Y |X] is given by (13.14) except that the value x is now replaced by
the random variable X. Therefore, EY |X [Y |X] is viewed as a function of the ran-
dom variable X. As an example, we saw that for the bivariate Gaussian PDF
Y |(X = x) &sim; N (x, 1 &minus; 2). Hence, EY |X [Y |x] = x, but regarding the mean
of the conditional PDF as a function of the random variable X we have that
EY |X [Y |X] = X. To see that (13.15) holds for this example
</p>
<p>EX [EY |X [Y |X]] = EX [X] = EX [X] = 0
</p>
<p>since the marginal PDF of X for the standard bivariate Gaussian PDF was shown
to be N (0, 1). Also, since Y &sim; N (0, 1) for the standard bivariate Gaussian PDF,
EY [Y ] = 0, and we see that (13.15) is satisfied.
</p>
<p>The mean of the conditional PDF arises in optimal prediction, where it is proven
that the minimum mean square error (MMSE) prediction of Y given X = x has been
observed is EY |X [Y |x] (see Problem 13.17). This is optimal over all predictors, linear
and nonlinear. For the standard Gaussian PDF, however, the optimal prediction
turns out to be linear since EY |X [Y |x] = x. More generally, it can be shown that
if X and Y are jointly Gaussian with PDF given by (12.35), then
</p>
<p>EY |X [Y |x] = EY [Y ] +
cov(X,Y )
</p>
<p>var(X)
(x&minus; EX [X])
</p>
<p>= Y +
XY
2X
</p>
<p>(x&minus; X)
</p>
<p>= Y +
Y
X
</p>
<p>(x&minus; X).
</p>
<p>(See also Problem 13.20.)
</p>
<p>13.7 Computer Simulation of Jointly Continuous
</p>
<p>Random Variables
</p>
<p>In a manner similar to that described in Section 8.7 we can generate realizations of
a continuous random vector (X,Y ) using the relationship
</p>
<p>pX,Y (x, y) = pY |X(y|x)pX(x).
</p>
<p>(Of course, if X and Y are independent, we can generate X based on pX(x) and Y
based on pY (y)). Consider as an example the standard bivariate Gaussian PDF. We
know that Y |(X = x) &sim; N (x, 1 &minus; 2) and X &sim; N (0, 1). Hence, we can generate
realizations of (X,Y ) as follows.</p>
<p/>
</div>
<div class="page"><p/>
<p>448 CHAPTER 13. CONDITIONAL PROBABILITY DENSITY FUNCTIONS
</p>
<p>Step 1. Generate X = x according to N(o,1).
</p>
<p>Step 2. Generate Y!(X = x ) according to N(px , 1 - p2).
</p>
<p>This procedure is conceptually simpler than what we implemented in Section 12.11
</p>
<p>and much more general. There we used (12.53). Referring to (12.53) , if we let
</p>
<p>J1.w = J1.z = 0, (j ~ (j ~ = 1 and make the replacements of W, Z , X , Y with
X ,Y, U, V , we have
</p>
<p>(13.16)
</p>
<p>where U '" N(o,1), V'" N(O, 1), and U and V are independent. The transformation
of (13.16) can be used to generate realizations of a standard bivariate Gaussian
</p>
<p>random vector. It is interesting to note that in this special case the two procedures
</p>
<p>for generating bivariate Gaussian random vectors lead to the identical algorithm.
</p>
<p>Can you see why from (13.16)?
</p>
<p>As an example of the condit ional PDF approach, if we let p = 0.9, we have the
plot shown in Figure 13.6. It should be compared with Figure 12.21 (note that in
</p>
<p>Figure 12.21 the means of X and Yare 1). The MATLAB code used to generate
</p>
<p>. .. . .3 .
</p>
<p>. :
</p>
<p>1 : . . . .
</p>
<p>~ 0 : .
</p>
<p>~. . . . .
</p>
<p>-2
</p>
<p>2 : .
</p>
<p>-3 : .
</p>
<p>2 3 4
-4 ' - - - - - - ' - _ . . . L . - ~ _ ~ _ . L . . - - - ' - _ ~ - - - J
-4 -3 -2 -1 0
</p>
<p>X
</p>
<p>Figure 13.6: 500 outcomes of standard bivariate Gaussian random vector with p =
</p>
<p>0.9 generated using conditional PDF approach.
</p>
<p>realizations of a standard bivariate Gaussian random vector using conditioning is
</p>
<p>given below.
</p>
<p>randn('state',O) 'l. set random number generator to initial value
rho=O.9;
</p>
<p>M=500; 'l. set number of realizations to generate
for m=1:M
</p>
<p>x(m,1)=randn(1,1); 'l. generate realization of N(O,1) random
'l. variable (Step 1)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.8. REAL-WORLD EXAMPLE - RETIREMENT PLANNING
</p>
<p>ygx(m,1)=rho*x(m)+sqrt(1-rho-2)*randn(1,1)j %generate
%YI(X=x) (Step 2)
</p>
<p>end
</p>
<p>13.8 Real-World Example - Retirement Planning
</p>
<p>449
</p>
<p>Professor Staff, who teaches too many courses a semester , plans to retire at age 65.
</p>
<p>He will have accumulated a total of $500,000 in a retirement account and wishes to
</p>
<p>use the money to live on during his retirement years. He assumes that his money
</p>
<p>will earn enough to offset the decrease in value due to inflation. Hence , if he lives to
</p>
<p>age 75 he could spend $50,000 a year and if he lives to age 85, then he could spend
</p>
<p>only $25,000 a year. How much should he figure on spending per year?
</p>
<p>Besides the many courses Professor Staff has taught in history, English, math-
</p>
<p>ematics, and computer science, he has also taught a course in probability. He
</p>
<p>therefore reasons that if he spends s dollars a year and lives for Y years during his
</p>
<p>retirement, then the probability that 500,000 - sY &lt; 0 should be small. Here sY
is the total money spent during his retirement. In other words , he desires
</p>
<p>P[500 ,000 - sY &lt; 0] = 0.5. (13.17)
</p>
<p>He chooses 0.5 for the probability of outliving his retirement fund. This acknowl-
</p>
<p>edges the fact that choosing a lower probability will lead to an overly conservative
</p>
<p>approach and a small amount of expendable funds per year as we will see shortly.
</p>
<p>Equivalently, he requires that
</p>
<p>(13.18)
</p>
<p>As an example, if he spends s = 50,000 per year , then the probability he lives more
</p>
<p>than 500, 000/s = 10 years should be 0.5.
</p>
<p>It should now be obvious that (13.18) is actually the right-tail probability or
</p>
<p>complementary CDF of the years lived in retirement. This type of information is of
</p>
<p>great interest not only to retirees but also to insurance companies who pay annuities.
</p>
<p>An annuity is a payment that an insurance company pays annually to an investor for
</p>
<p>the remainder of his/her life. The amount of the payment depends upon how much
</p>
<p>the investor originally invest s, the age of the investor, and the insurance company's
</p>
<p>belief that the investor will live for so many years. To quantify answers to questions
</p>
<p>concerning years of life remaining, the mortality rate, which is the distribution of
</p>
<p>years lived past a given age is required. If Y is a continuous random variable that
</p>
<p>denotes the years lived past age X = X , then the mortality rate can be described by
</p>
<p>first defining the conditional CDF</p>
<p/>
</div>
<div class="page"><p/>
<p>450 CHAPTER 13. CONDITIONAL PROBABILITY DENSITY FUNCTIONS
</p>
<p>For example, the probability that a person will live at least 10 more years if he is
</p>
<p>cur rent ly 65 years old is given by
</p>
<p>P[Y&gt; lO!X = 65] = 1 - FY lx (10165)
</p>
<p>which is the complementary CDF or the right-tail probability of the conditional PDF
</p>
<p>PYlx(ylx). It has been shown that for Canadian citizens the conditional CDF is well
</p>
<p>modeled by [Milevsky and Robinson 2000]
</p>
<p>FYlx(ylx) = 1 - exp [exp (x ~ m) (1 - exp (&yen;))] y2:0 (13.19)
where m = 81.95, l = 10.6 for males and m = 87.8, l = 9.5 for females. As an
example, if FYlx(ylx) = 0.5, then you have a 50% chance of living more than y
years if you are currently x years old. In other words, 50% of the population who
</p>
<p>are x years old will live more than y years and 50% will live less than y years. The
</p>
<p>number of years y is the median number of years to live. (Recall that the median is
</p>
<p>the value at which the probability of being less than or equal to this value is 0.5.)
</p>
<p>From (13.19) this will be true when
</p>
<p>0.5 = exp [exp ( x ~ m) (1 - exp (T) )]
which results in the remaining number of years lived by 50% of the population who
</p>
<p>are currently x years old as
</p>
<p>y = lIn [1 - exp ( _ (x ~ m))In 0.5] . (13.20)
This is plotted in Figure 13.7a versus the current age x for males and females. In
</p>
<p>Figure 13.7a the median number of years left is shown while in Figure 13.7b the
</p>
<p>median life expectancy (which is x + y) is given.
Returning to Professor Staff, he can now determine how much money he can
</p>
<p>afford to spend each year. Since the probability of outliving one's retirement funds
</p>
<p>is a conditional probability based on current age, we rewrite (13.18) as
</p>
<p>where we allow the probability to be denoted in general by PL. Since he will retire
</p>
<p>at age x = 65, we have from (13.19) that he will live more than y years with a
probability of PL given by
</p>
<p>PL = exp [exp (65 ~ m) (1 - exp (&yen;))]. (13.21)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.8. REAL-WORLD EXAMPLE - RETIREMENT PLANNING 451
</p>
<p>95 r- ---,--- ~---~- -___,
</p>
<p>90 ; ; .
</p>
<p>~
</p>
<p>+ .
L"&gt; female :
q 85 .. . . . .
</p>
<p>80
</p>
<p>male
</p>
<p>9060 70 80
Current age, x
</p>
<p>75L-__ ~ __ ~ -'--__----'
</p>
<p>509060 70 80
Current age, x
</p>
<p>O L--- ~--~- --~---'
</p>
<p>50
</p>
<p>(a) Years to live, Y (b) Life expectancy, x + Y
</p>
<p>Figure 13.7: Mortality rates.
</p>
<p>302510 15 20
</p>
<p>Years to live, Y
5
</p>
<p>OL-- -'----'----'--- - '--- -'--- ---'
o
</p>
<p>0.9
</p>
<p>0.8
</p>
<p>0.7 .
</p>
<p>0.6
...:l
</p>
<p>c.., 0.5 .. .
</p>
<p>0.1
</p>
<p>0.3 .
</p>
<p>0.2 .
</p>
<p>0.4 .. . ' "
</p>
<p>Figure 13.8: Probability PL of exceeding Y years in retirement for male who retires
</p>
<p>at age 65.
</p>
<p>Assuming Professor Staff is a male , we use m = 81.95, l = 10.6 in (13.21) to produce
</p>
<p>a plot PL versus y as shown in Figure 13.8. If the professor is overly conservative,
</p>
<p>he may want to assure himself that the probability of outliving his retirement fund
</p>
<p>is only about 0.1. Then, he should plan on living another 27 years, which means
</p>
<p>that his yearly exp enses should not exceed $500,000/27 = $18,500. If he is less
conservat ive and chooses a probability of 0.5, then he can plan on living about 15
</p>
<p>years. Then his yearly expenses should not exceed $500,000/15 ::::; $33,000.</p>
<p/>
</div>
<div class="page"><p/>
<p>452 CHAPTER 13. CONDITIONAL PROBABILITY DENSITY FUNCTIONS
</p>
<p>References
</p>
<p>Milevsky, M.A., C. Robinson, "Self-Annuitization and Ruin in Retirement," North
</p>
<p>American Actuarial Journal, Vol. 4, pp. 113-129, 2000.
</p>
<p>Rappaport, T.S ., Wireless Communications, Principles and Practice, Prentice-
</p>
<p>Hall, Upper Saddle River, NJ, 2002.
</p>
<p>Problems
</p>
<p>13.1 (w,c) In this problem we simulate on a computer the dartboard outcomes of
the novice player for the game shown in Figure 13.1a. To do so, generate
</p>
<p>two independent U (-1, 1) random variables to serve as the x and y outcomes.
</p>
<p>Keep only the outcomes (x,y) for which J x2+ y2 :s; 1 (see Problem 13.23 for
why this produces a uniform joint PDF within the unit circle). Then, of the
</p>
<p>kept outcomes retain only the ones for which fj.xj2 :s; 0.2 (see Figure 13.2a) .
Finally, estimate the probability that the novice player obtains a bullseye and
</p>
<p>compare it to the theoretical value. Note that the theoretical value of 0.25
</p>
<p>as given by (13.2) is actually an approximation based on the areas in Figure
</p>
<p>13.1b being rectangular.
</p>
<p>13.2 C:..:..-) (w) Determine if the proposed conditional PDF
</p>
<p>P (Ix) = {cexp(-yjx) y 2: o,x &gt; 0
YIX Y 0 otherwise
</p>
<p>is a valid conditional PDF for some c. If so, find the required value of c.
</p>
<p>13.3 (w) Is the proposed conditional PDF
</p>
<p>1 [1 2]PYlx(ylx) = V27i exp -2(y - x ) - 00 &lt; y &lt; 00, -00 &lt; x &lt; 00
valid? If so, and if X '" N(o, 1), design an experiment that will produce the
random variables X and Y.
</p>
<p>13.4 C:..:,,) (f) If
</p>
<p>( )
_ { 2exp[-(x + y)]
</p>
<p>PX,Y x,y - 0
</p>
<p>find PYlx(ylx).
</p>
<p>13.5 (w) Plot the joint PDF
</p>
<p>o :s; y :s; x, x 2: 0
otherwise
</p>
<p>{
</p>
<p>2x
PX,y(x, y) = 0
</p>
<p>0&lt; x &lt; 1,0 &lt; y &lt; 1
otherwise.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 453
</p>
<p>Next determine by inspection the conditional PDF PYlx(ylx). Recall that the
</p>
<p>conditional PDF is just the normalized cross-section of the joint PDF.
</p>
<p>13.6 (t) In this problem we show that
</p>
<p>lim P[a:S Y :S blx - b.x/2 :S X :S x + b.x/2] = r PYlx(ylx)dy.
.6.x-tO l,
</p>
<p>To do so first show that
</p>
<p>l ~ o P[a :S Y :S blx - b.x/2 :S X :S x + b.x/2]
</p>
<p>f
X+ b.X/ 2 ( )
</p>
<p>j b . x-b.x/2 PX,Y x, Y dx= lim dy.
.6.x-tO f x+ .6.X/ 2 ()d
</p>
<p>a x - b. x / 2 PX X X
</p>
<p>13.7 (f) Determine pry &gt; ~IX = 0] if the joint PDF is given as
</p>
<p>{
2x 0 &lt; x &lt; 1,0 &lt; y &lt; 1
</p>
<p>PX,y(x ,y) = 0 otherwise.
</p>
<p>13.8 (..:.:,) (f) If X ~ U(O, 1) and YI(X = x) ~ U(O, x), find the joint PDF for X
and Y and also the marginal PDF for Y.
</p>
<p>13.9 (f,t) For the standard bivariate Gaussian PDF find the conditional PDFs PYIX
and PXIY and compare them. Explain your results. Are your results true in
</p>
<p>general?
</p>
<p>13.10 (..:.:,) (f) If the joint PDF PX,Y is uniform over the region 0 &lt; y &lt; x and
0&lt; x &lt; 1 and zero otherwise, find the conditional PDFs PYIX and PXIY'
</p>
<p>13.11 (t) Prove Properties P13.1-13.5.
</p>
<p>13.12 (f) Determine the PDF of Y/X if X ~ N(O, 1), Y ~ N(O, 1) and X and Y
are independent. Do so by using the conditioning approach.
</p>
<p>13.13 (t) Prove that the PDF of X +Y, where X and Yare independent is given as
a convolution integral (see (12.14&raquo;. Do so by using the conditioning approach.
</p>
<p>13.14 (..:.:,) (w) A game of darts is played using the linear dartboard shown in
</p>
<p>Figure 3.8. If two novice players throw darts at the board and each one's dart
</p>
<p>is equally likely to land anywhere in the interval (-1/2,1/2)' prove that the
</p>
<p>probability of player 2 winning is 1/2. Hint: Let Xl and X2 be the outcomes
</p>
<p>and use Y = IX21-IXII and X = Xl in (13.12).
</p>
<p>13.15 (t) Prove (13.12) by starting with (13.4).</p>
<p/>
</div>
<div class="page"><p/>
<p>454 CHAPTER 13. CONDITIONAL PROBABILITY DENSITY FUNCTIONS
</p>
<p>13.16 c . : ~ ) (w) A resistor is chosen from a bin of 10 ohm resistors whose distri-
bution satisfies R ,....., N(10, 0.25). A i = 1 amp cur rent source is applied to
</p>
<p>the resistor and the subsequent voltage V is measured with a voltmeter. The
</p>
<p>voltmeter has an error E that is modeled as E ,....., N(o, 1). Find the probability
that V &gt; 10 volts if an 11 ohm resistor is chosen. Note that V = iR + E.
What assumption do you need to make about the dependence between Rand
</p>
<p>E?
</p>
<p>13.17 (t) In this problem we prove that the minimum mean square error estimate
</p>
<p>of Y based on X = x is given by EYlx[ylx]. First let the estimate be denoted
</p>
<p>by Y( x) since it will depend in general on the outcome of X. Then note that
the mean square error is
</p>
<p>mse Ex,y[(Y - Y(X))2]
</p>
<p>= i:i: (y - Y(x))2pX,y(x, y)dx dy
i:i: (y - Y(x))2 py1x(y!x)px(x)dxdy
i: [I: (y - Y(X))2pYIX(YIX)dY] px(x)dx.
</p>
<p>, ,
v
</p>
<p>J(Y(x&raquo;
</p>
<p>Now we can minimize J(Y(x)) for each value of x since px(x) ~ O. Complete
</p>
<p>the derivation by differentiating J(Y(x)) and setting the result equal to zero.
</p>
<p>Consider Y(x) as a constant (since x is assumed fixed inside the inner integral)
</p>
<p>in doing so. Finally justify all the steps in the derivation.
</p>
<p>13.18 (f) For the joint PDF given in Problem 13.10 find the minimum mean square
</p>
<p>error estimate of Y given X = x. Plot the region in the x-y plane for which
</p>
<p>the joint PDF is nonzero and also the estimated value of Y versus x.
</p>
<p>13.19 (t) Prove (13.15).
</p>
<p>13.20 (w,c) If a bivariate Gaussian PDF has a mean vector [J,lx J,lyjT = [12jT and
a covariance matrix
</p>
<p>c = [ ~ ~ ]
plot the contours of constant PDF. Next find the minimum mean square error
</p>
<p>prediction of Y given X = x and plot it on top of the contour plot. Explain
the significance of the plot.
</p>
<p>13.21 ( ..:.:J (w) A random variable X has a Laplacian PDF with variance a 2 &bull; If
the variance is chosen according to a 2 ,....., U(O, 1), what is average variance of
</p>
<p>the random variable?</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 455
</p>
<p>13.22 (c) In this problem we use a computer simulation to illustrate the known
</p>
<p>result that E y lX [YlxJ = px for (X, Y) distributed according to a standard
bivariate Gaussian PDF. Using (13.16) generate M = 10,000 realizations of
</p>
<p>a standard bivariate Gaussian random vector with p = 0.9. Then let A =
{x : XQ - b..x/2 ::; x ::; XQ + b..x/2} and discard the realizations for which x
is not in A. Finally, estimate the mean of the conditional PDF by taking the
</p>
<p>sample mean of the remaining realizations. Choose b..x/2 = 0.1 and XQ = 1
</p>
<p>and compare the theoretical value of EYlx[YlxJ to the estimated value based
on your computer simulation.
</p>
<p>13.23 (t) We now prove that the procedure described in Problem 13.1 will produce
a random vector (X, Y) that is uniformly distributed within the unit circle.
</p>
<p>First consider the polar equivalent of (X, Y), which is (R, e), so that the
conditional CDF is given by
</p>
<p>P[R::; r, e ::; OIR ::; 1J
</p>
<p>But this is equal to
</p>
<p>P[R ::; r, R ::; 1, e ::; OJ
P[R::; 1J
</p>
<p>(Why?) Next show that
</p>
<p>o::; r ::; 1, 0 ::; 0 &lt; 21r.
</p>
<p>P[R ::; r, e ::; OJ
P[R::; 1J
</p>
<p>P[R ::; r,e ::; OIR ::; 1J = ~:
</p>
<p>and differentiate with respect to r and then 0 to find the joint PDF PR,e(r, 0)
</p>
<p>(which is actually a conditional joint PDF due to the conditioning on the value
</p>
<p>of R being r ::; 1). Finally, transform this PDF back to that of (X,Y) to verify
that it is uniform within the unit circle. Hint: You will need the result
</p>
<p>d (
8(r,O&raquo;) 1
</p>
<p>et - -----:--~:-
8(x, Y) - det (8(X,y ) .
</p>
<p>a r,e
</p>
<p>13.24 C:..:.,,) (f,c) For the conditional CDF of years left to live given current age,
which is given by (13.19) , find the conditional PDF. Plot the conditional PDF
</p>
<p>for a Canadian male who is currently 50 years old and also for one who is 75
</p>
<p>years old. Next find the average life span for each of these individuals. Hint:
</p>
<p>You will need to use a computer evaluation of the integral for the last part.
</p>
<p>13.25 (t) Verify that the conditional CDF given by (13.19) is a valid CDF.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 14
</p>
<p>Continuous N-Dimensional
</p>
<p>Random Variables
</p>
<p>14.1 Introduction
</p>
<p>This chapter extends the results of Chapters 10-13 for one and two continuous
</p>
<p>random variables to N continuous random variables. Our discussion will mirror
</p>
<p>Chapter 9 quite closely, the difference being the consideration of continuous rather
</p>
<p>than discrete random variables. Therefore, the descriptions will be brief and will
</p>
<p>serve mainly to extend the usual definitions for one and two jointly distributed con-
</p>
<p>t inuous random variables to an N-dimensional random vector. One new concept
</p>
<p>that is introduced is the orthogonality principle approach to prediction of the out-
</p>
<p>come of a random variable based on the outcomes of several other random variables.
</p>
<p>This concept will be useful later when we discuss prediction of random processes in
</p>
<p>Chapter 18.
</p>
<p>14.2 Summary
</p>
<p>The probability of an event defined on an N-dimensional sample space is given by
</p>
<p>(14.1). The most important example of an N-dimensional PDF is the multivariate
</p>
<p>Gaussian PDF, which is given by (14.2). If the components of the multivariate
</p>
<p>Gaussian random vector are uncorrelated, then they are also independent as shown
</p>
<p>in Example 14.2. Transformations of random vectors yield the transformed PDF
</p>
<p>given by (14.5). In particular, linear tranformations of Gaussian random vectors
</p>
<p>preserve the Gaussian nature but change the mean vector and covariance matrix as
</p>
<p>discussed in Example 14.3. Expected values are described in Section 14.5 with the
</p>
<p>mean and variance of a linear combination of random variables given by (14.8) and
</p>
<p>(14.10), respectively. The sample mean random variable is introduced in Example
</p>
<p>14.4. The joint moment is defined by (14.13) and the joint characteristic function</p>
<p/>
</div>
<div class="page"><p/>
<p>458 CHAPTER 14. CONTINUOUS N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>by (14.15). Joint moments can be found from the characteristic function using
</p>
<p>(14.17) . The PDF for a sum of independent and identically distributed random
</p>
<p>variables is conveniently determined using (14.22). The prediction of the outcome
</p>
<p>of a random variable based on a linear combination of the outcomes of other random
</p>
<p>variables is given by (14.24) . The linear prediction coefficients are found by solving
</p>
<p>the set of simultaneous linear equations in (14.27). The orthogonality principle
</p>
<p>is summarized by (14.29) and illustrated in Figure 14.3. Section 14.9 describes
</p>
<p>the computer generation of a multivariate Gaussian random vector. Finally, section
</p>
<p>14.10 applies the results of this chapter to the real-world problem of signal detection
</p>
<p>with the optimal detector given by (14.33).
</p>
<p>14.3 Random Vectors and PDFs
</p>
<p>An N-dimensional random vector will be denoted by either (Xl, X 2 , &bull; &bull; &bull; , XN) or
X = [Xl X 2... xNjT. It is defined as a mapping from the original sample space of
the experiment to a numerical sample space SXl ,X2,...,XN = R N . Hence, X takes on
values in the N-dimensional Euclidean space RN so that
</p>
<p>will have values
</p>
<p>[
~~~:~ ]X(s) =
XN(s)
</p>
<p>x = [ ~ U
where x is a point in RN . The number of possible values is uncountably infinite. As
</p>
<p>an example, we might observe the temperature on each of N successive days . Then,
</p>
<p>the elements ofthe random vector would be Xl (s) = temperature on day 1, X 2(s) =
</p>
<p>temperature on day 2, .. ., XN(s) = temperature on day N , and each temperature
</p>
<p>measurement would take on a continuum of values.
</p>
<p>To compute probabilities of events defined on SXl,X2" " ,XN we will define the
N -dimensional joint PDF (or more succinctly just the PDF) as
</p>
<p>and sometimes use the more compact notation px(x). The usual properties of a
</p>
<p>joint PDF must be valid</p>
<p/>
</div>
<div class="page"><p/>
<p>14.3. RANDOM VECTORS AND PDFS
</p>
<p>Then the probability of an event A defined on R N is given by
</p>
<p>459
</p>
<p>(14.1)
</p>
<p>The most important example of an N-dimensional joint PDF is the multivariate
</p>
<p>Gaussian PDF. This PDF is the extension of the bivariate Gaussian PDF described
</p>
<p>at length in Chapter 12 (see (12.35)). It is given in vector/matrix form as
</p>
<p>1 [ 1 T -1 ]
px(x) = (27r)N/2 det l / 2(C) exp -2"(x - JL) C (x - JL)
</p>
<p>where JL = [/-l1/-l2 .. . /-IN]T is the N x 1 mean vector so that
</p>
<p>(14.2)
</p>
<p>Ex[X] =
[
</p>
<p>EXl [Xl] ]
EX2[X2 ]
</p>
<p>EXN[XN]
</p>
<p>=JL
</p>
<p>and C is the N x N covariance matrix defined as
</p>
<p>[
</p>
<p>var(Xd
</p>
<p>C = COV(~2 ' Xl)
</p>
<p>COV(XN ,Xl)
</p>
<p>cov(Xl , X 2 )
</p>
<p>var(X2 )
cov(Xl , XN) ]
coV(X2,XN )
</p>
<p>. .
</p>
<p>var(XN )
</p>
<p>Note that C is assumed to be positive definite and so it is invertible and has det(C) &gt;
o (see Appendix C). If the random variables have the multivariate Gaussian PDF,
they are said to be jointly Gaussian distributed. Note that the covariance matrix
</p>
<p>can also be written as (see (Problem 9.21))
</p>
<p>To denote a multivariate Gaussian PDF we will use the notation N(JL, C). Clearly,
</p>
<p>for N = 2 we have the bivariate Gaussian PDF. Evaluation of the probability of
an event using (14.1) is in general quite difficult. Progress can, however, be made
</p>
<p>when A is a simple geometric region in RN and C is a diagonal matrix. An example
</p>
<p>follows.
</p>
<p>Example 14.1 - Probability of a point lying within a sphere
</p>
<p>Assume N = 3 and let X "-' N(O, a 2I). We will determine the probability that
</p>
<p>an outcome falls within a sphere of radius R. The event is then given by A =
{(Xl,X2 , X3) : xi + X&sect; + X~ ::; R2 } . This event might represent the probability that
a particle with mass m and random velocity components Vx , Vy , V; has a kinetic
</p>
<p>energy &pound; = (1/2)m(V; + V; + V}) less than a given amount. This modeling is</p>
<p/>
</div>
<div class="page"><p/>
<p>460 CHAPTER 14. CONTINUOUS N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>used in the kinetic theory of gases [Resnick and Halliday 1966] and is known as the
</p>
<p>Maxwellian distribution. From (14.2) we have with I.t = 0, C = (J2I, and N = 3
</p>
<p>P[A] =
</p>
<p>since det(CT2I) = ((J2) 3 and ((J2I)-1 = (lj(J2)I. Next we notice that the region of
integration is the inside of a sphere. As a result of this and the observation that
</p>
<p>the integrand only depends on the squared-distance of the point from the origin, a
</p>
<p>reasonable approach is to convert the Cartesian coordinates to spherical coordinates.
</p>
<p>Doing so produces the inverse transformation
</p>
<p>Xl rcosOsin&cent;
</p>
<p>X 2 r sin 0 sin &cent;
</p>
<p>X 3 rcos&cent;
</p>
<p>where r 2': 0, 0 ~ 0 &lt; 21l", 0 ~ &cent; ~ tt, We must be sure to include in the integral over
r, 0,&cent; the absolute value of the Jacobian determinant of the inverse transformation
</p>
<p>which is r 2sin&cent; (see Problem 14.5). Thus,
</p>
<p>To evaluate the integral
</p>
<p>l
R
</p>
<p>r2 (1 )1= -exp __r2 dr
o (J2 2(J2
</p>
<p>we use integration by parts (see Problem 11.7) with U = r and hence dU = dr and</p>
<p/>
</div>
<div class="page"><p/>
<p>14.3. RANDOM VECTORS AND PDFS
</p>
<p>I = - r exp [_~r 2 /a2] I~ + lR exp [_~r 2 /a2] dr
-R exp [_!R2/a2] + J21ra2 (R ~ exp [_!r2/a2] dr
</p>
<p>2 Jo 21ra2 2
</p>
<p>-Rexp [ _ ~ R 2 /a2] + J21ra2 [Q(O) - Q(R/a)].
</p>
<p>Finally, we have that
</p>
<p>P[A] = J1r~2 [-Rexp [_~R2 /a2] + J21ra2 [Q(O) - Q(R/a)]]
1- 2Q(R/a) - J1r~2Rexp (_~R2/a2) .
</p>
<p>461
</p>
<p>c
The marginal PDFs are found by integrating out the other variables. For exam-
</p>
<p>ple, if PXl (Xl) is desired, then
</p>
<p>As an example, for the multivariate Gaussian PDF it can be shown that X i
</p>
<p>N(J.Li ,al) , where al = var(Xd (see Problem 14.16). Also, the lower dimensional
joint PDFs are similarly found. To determinepx l,xN(XI,XN) for example, we use
</p>
<p>The random variables are defined to be independent if the joint PDF factors into
</p>
<p>the product of the marginal PDFs as
</p>
<p>(14.3)
</p>
<p>An example follows.
</p>
<p>Example 14.2 - Condition for independence of multivariate Gaussian
</p>
<p>random variables
</p>
<p>If the covariance matrix for a multivariate Gaussian PDF is diagonal, then the
</p>
<p>random variables are not only uncorrelated but also independent as we now show.
</p>
<p>Assume that
</p>
<p>C = diag (ai , a~ , ... ,a'Jv )</p>
<p/>
</div>
<div class="page"><p/>
<p>462 CHAPTER 14. CONTINUOUS N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>then it follows that
</p>
<p>N
</p>
<p>det(C) IIa;
i = l
</p>
<p>C-
1
</p>
<p>= diag(~ ,~ , ... , ~ ) .
a 1 a2 aN
</p>
<p>Using these results in (14.2) produces
</p>
<p>where X i rv N(J.Li,an. Hence , if a random vector has a multivariate Gaussian PDF
and the covariance matrix is diagonal, which means that the random variables are
</p>
<p>un correlated, then the random variables are also independent.
</p>
<p>L1h U ncorrelated implies independence only for multivariate Gaus-
sian PDF even if marginal PDFs are Gaussian!
</p>
<p>Consider the counterexample of a PDF for the random vector (X,Y) given by
</p>
<p>PX,y(x, y) 1 1 [1 2 2]2 27r~ exp - 2(1 _ p2) (x - 2pxy + y )
</p>
<p>1 1 [1 2 2]+2 27r~ exp - 2(1 _ p2) (x + 2pxy + y ) (14.4)
</p>
<p>for 0 &lt; P &lt; 1. This PDF is shown in Figure 14.1 for p = 0.9. Clearly, the random
variables are not independent. Yet, it can be shown that X rv N(O, 1), Y rv N(O, 1),
</p>
<p>and X and Yare uncorrelated (see Problem 14.7). The difference here is that the
</p>
<p>joint PDF is not a bivariate Gaussian PDF.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.4. TRANSFORMATIONS 463
</p>
<p>3 r--~----------'
</p>
<p>-3'--~---~--~----'
</p>
<p>-2 .&bull;&bull;&bull;&bull; &bull;&bull;
2
</p>
<p>&deg;X-2
</p>
<p>2 .
</p>
<p>-1
</p>
<p>~ 0 &middot;
</p>
<p>x
-2
</p>
<p>. . . , .
</p>
<p>y
</p>
<p>&deg;
</p>
<p>,..-...
~
</p>
<p>t-l
&gt;:'0.4
</p>
<p>~
&gt;:l..0.2 .: . . . . .
</p>
<p>(a) Joint PDF (b) Constant PDF contours
</p>
<p>Figure 14.1: Uncorrelated but not independent random variables with Gaussian
</p>
<p>marginal PDFs.
</p>
<p>A joint cumulative distribution function (CDF) can be defined in the N-dimensional
</p>
<p>case as
</p>
<p>It has the usual properties of being between 0 and 1, being monotonically increasing
</p>
<p>as any of the variables increases, and being "right continuous". Also,
</p>
<p>FX1 ,X2,... ,XN (-00, -00, , -00) 0
</p>
<p>FX1,X2,,,,,XN (+00 , +00, , +00) 1.
</p>
<p>The marginal CDFs are easily found by letting the undesired variables be evaluated
</p>
<p>at +00 . For example, to determine the marginal CDF for Xl, we have
</p>
<p>14.4 Transformations
</p>
<p>We consider the transformation from X to Y where
</p>
<p>YI 9I(XI , X2, , XN)
</p>
<p>Y2 92(XI, X2, , XN )</p>
<p/>
</div>
<div class="page"><p/>
<p>464 CHAPTER 14. CONTINUOUS N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>and the transformation is one-to-one. Hence Y is a continuous random vector having
</p>
<p>a joint PDF (due to the one-to-one property). If we wish to find the PDF of a subset
</p>
<p>of the Yi's, then we need only first find the PDF of Y and then integrate out the
undesired variables. The extension of (12.22) for obtaining the joint PDF of two
</p>
<p>transformed random variables is
</p>
<p>where
ax! ax! ax!
ay! aY2 aYN
</p>
<p>O(Xl,X2, ... ,XN) ~ ~
aX2
</p>
<p>ay! aY2 aYN
</p>
<p>O(Yl,Y2,&middot;&middot;&middot;,YN)
aXN aXN aXN
ay! aY2 aYN
</p>
<p>(14.5)
</p>
<p>is the inverse Jacobian matrix. An example follows.
</p>
<p>Example 14.3 - Linear transformation of multivariate Gaussian random
</p>
<p>vector
</p>
<p>If X", N(J-L, C) and Y = GX, where G is an invertible N x N matrix, then we
have from y = Gx that
</p>
<p>x
</p>
<p>ox
oy
</p>
<p>Hence, using (14.5) and (14.2)
</p>
<p>py(y)
</p>
<p>=
</p>
<p>(see Section 12.7 for details of matrix manipulations) so that Y '" N(GJ-L, GCGT ) .
</p>
<p>This result is the extension of Theorem 12.7 from 2 to N jointly Gaussian random
</p>
<p>variables. See also Problems 14.8 and 14.15 for the case where G is M x N with
</p>
<p>M &lt; N. It is shown there that the same result holds.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.5. EXPECTED VALUES
</p>
<p>14.5 Expected Values
</p>
<p>465
</p>
<p>The expected value of a random vector is defined as the vector of the expected values
</p>
<p>of the elements of the random vector. This says that we define
</p>
<p>II
Xl]] [EXl[XI]]X2 EX2[X2]
&middot; .&middot; .&middot; .
</p>
<p>X N EXN[XN]
</p>
<p>(14.6)
</p>
<p>We can view this definition as "passing" the expectation "through" the left bracket
</p>
<p>of the vector since EXl,X2"",XN[Xi ] = EXi[XiJ. A particular expectation of interest
is that of a scalar function of X I,X2, ... ,XN, say g(XI,X2, ... ,XN). Similar to
</p>
<p>previous results (see (12.28)) this is determined using
</p>
<p>EXl,X2,,,,,XN [g(XI , X 2, ... ,X N)]
</p>
<p>=100 100 ...100 g(XI,X2, ... ,XN)PXl,X2"" ,XN(XI,X2, . .. ,XN)dxldx2 . . . da:&raquo; ,
-00 -00 -00
</p>
<p>(14.7)
</p>
<p>Some specific results of interest are the linearity of the expectation operator or
</p>
<p>(14.8)
</p>
<p>and in particular if ai = 1 for all i , then we have
</p>
<p>(14.9)
</p>
<p>The variance of a linear combination of random variables is given by
</p>
<p>(14.10)
</p>
<p>where Cx is the covariance matrix of X and a = [al a2 .. . aNjT. The derivation of
</p>
<p>(14.10) is identical to that given in the proof of Property 9.2 for discrete random
</p>
<p>variables. If the random variables are uncorrelated so that the covariance matrix is
</p>
<p>diagonal or
</p>
<p>Cx = diag(var(XI), var(X2) ... , var(XN ) )
</p>
<p>then (see Problem 14.10)
</p>
<p>N
</p>
<p>= L arvar(Xi)'
i=l
</p>
<p>(14.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>466 CHAPTER 14. CONTINUOUS N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>If furthermore, ai = 1 for all i, then
</p>
<p>(14.12)
</p>
<p>An example follows.
</p>
<p>Example 14.4 - Sample mean of independent and identically distributed
</p>
<p>random variables
</p>
<p>Assume that X I, X 2, ... , X N are independent random variables and each ran-
</p>
<p>dom variable has the same marginal PDF. When random variables have the same
</p>
<p>marginal PDF, they are said to be identically distributed. Hence, we are assuming
</p>
<p>that the random variables are independent and identically distributed (lID). As a
</p>
<p>consequence of being identically distributed, Ex; [Xi] = fJ. and var(Xi ) = cr2 for all
i. It is of interest to examine the mean and variance of the random variable that we
</p>
<p>obtain by averaging the Xi'S together. This averaged random variable is
</p>
<p>1 N
X= - ~ Xi
</p>
<p>N~
t=l
</p>
<p>and is called the sample mean random variable. We have previously encountered
</p>
<p>the sample mean when referring to an average of a set of outcomes of a repeated
</p>
<p>experiment, which produced a number. Now, however, X is a function of the random
</p>
<p>variables Xl, X 2 , ... , XN and so is a random variable itself. As such we may consider
</p>
<p>its probabilistic properties such as its mean and variance. The mean is from (14.8)
</p>
<p>with ai = liN
_ 1 N
</p>
<p>EXl ,x2,...,x N[X] = N L Ex;[Xi ] = fJ.
i=l
</p>
<p>and the variance is from (14.11) with ai = liN (since Xi'S are independent and
hence uncorrelated)
</p>
<p>var(X)
N 1
</p>
<p>L N2 var(Xd
i=l
</p>
<p>1 N
</p>
<p>N2 Lcr2
i=l
</p>
<p>Note that on the average the sample mean random variable will yield the value fJ.,
</p>
<p>which is the expected value of each Xi. Also as N -+ 00, var(X) -+ 0, so that the
PDF of X will become more and more concentrated about u. In effect, as N -+ 00,
we have that X -+ u: This says that the sample mean random variable will converge</p>
<p/>
</div>
<div class="page"><p/>
<p>14.6. JOINT MOMENTS AND THE CHARACTERISTIC FUNCTION 467
</p>
<p>to the true expected value of Xi. An example is shown in Figure 14.2 in which the
</p>
<p>mar ginal PDF of each X i is N (2, 1). In the next chapter we will pr ove that X does
indeed converge to Ex ; [Xi] = J.L .
</p>
<p>0.5 : ~ .
</p>
<p>1 : ~ .
</p>
<p>. .
</p>
<p>2,....-~-~-~- -~-~ -,
</p>
<p>~ ..
Cl 1.5 : ~ : : : .
c,
"0
</p>
<p>Q)
</p>
<p>~
</p>
<p>.5...,
~
</p>
<p>~
Cl 1.5 : ~.. . .
c,
"0
</p>
<p>~ 1 : ~ .
</p>
<p>.5...,
~ 0.5 : ~ .
</p>
<p>o 2
X
</p>
<p>4 o
n
</p>
<p>2
X
</p>
<p>In
3 4
</p>
<p>(a) N = 10 (b) N = 100
</p>
<p>Figure 14.2: Esti mated PDF for sample mean random vari able, X.
</p>
<p>14.6 Joint Moments and the Characteristic Function
</p>
<p>The joint moments corresponding to an N-dimensional PDF are defined as
</p>
<p>= roo roo ... roo XilX~ 2 ... X ~ P X l , X 2 , ...,XN (X l,X2, .. . ,XN )dx l dx2 ... d x N.
i : t: i :
</p>
<p>(14.13)
</p>
<p>As usu al , if the random vari ables are independent, the joint PDF factors and there-
</p>
<p>fore
</p>
<p>(14.14)
</p>
<p>The joint charac te rist ic fun ction is defined as
</p>
<p>&cent;Xl ,X2" " ,XN (Wl , W2 ,' " , WN ) = E X l ,X 2"" ,X N [exp [j (w1X1+W2X2 + ... + WNXN)]]
(14.15)
</p>
<p>and is evaluated as</p>
<p/>
</div>
<div class="page"><p/>
<p>468 CHAPTER 14. CONTINUOUS N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>= 1:1:&middot;&middot;&middot;1:exp [j(WIXI +W2X2 + .. .+ WN X N )]
PX l ,X 2,.. .,XN (Xl , X2 , .. &middot; , XN )dXI d X 2 .. . d X N &middot;
</p>
<p>In particular, for independent random variables , we have (see Problem 14.13)
</p>
<p>Also, the joint PDF can be found from t he joint characteristic fun ction using the
</p>
<p>inverse Fourier transform as
</p>
<p>(14.16)
</p>
<p>All the proper ties of the 2-di mensional characterist ic funct ion extend to the general
</p>
<p>case . Note that once &lt;PXl,X2 " " ,XN (WI, W2 , . . . , WN ) is known, t he characteristic fun c-
</p>
<p>t ion for any subset of the X i 'S is found by setting Wi equal to zero for t he ones not in
</p>
<p>t he subset. For example, to find PX l ,X 2( X I , X2) , we can let W3 = W4 = ... = WN = 0
in the joint characteristic function to yield (see Problem 14.14)
</p>
<p>As seen previously, the joint mome nts can be obtained from t he characteristic fun c-
</p>
<p>t ion . T he general formula is
</p>
<p>EX 1,X 2" " ,X N [Xfl X~2 ... X ~ n
</p>
<p>1 a h +12+ &middot;&middot;+1N
</p>
<p>&middot;h + 12+ &middot;+ 1N a ha 12 a IN &lt;PX 1,X2,...,XN (W I, W2, ... , WN )
</p>
<p>J WI W2&middot;&middot;&middot; WN Wl=W2="'=WN = O
(14.17)
</p>
<p>An example follows.
</p>
<p>Example 14.5 - Second-order joint moments for multivariate Gaussian
</p>
<p>PDF
</p>
<p>In this example we derive t he second-order mome nts E X i X j [XiXj ] if X rv N(O, C ).
</p>
<p>T he charac teristic fun ction can be shown to be [Muirhead 1982]</p>
<p/>
</div>
<div class="page"><p/>
<p>14.6. JOINT MOMENTS AND THE CHARACTERISTIC FUNCTION 469
</p>
<p>where w = [WI W2 . . . WNV. We first let
</p>
<p>N N
</p>
<p>Q(w) = wTCw = L LWmwn[Clmn
m=1 n=1
</p>
<p>(14.18)
</p>
<p>and note that it is a quadratic form (see Appendix C) . Also, we let [Clmn = Cmn to
simplify the notation. Then from (14.17) with li = lj = 1 and the other l's equal to
</p>
<p>zero, we have
</p>
<p>Carrying out the partial differentiation produces
</p>
<p>But
</p>
<p>8 exp[-(1/2)Q(w )]
</p>
<p>8Wi
</p>
<p>82 exp[-(1/2)Q(w)]
</p>
<p>8Wi8wj
</p>
<p>18Q(w) (1 )-- exp --Q(w)
2 8Wi 2
</p>
<p>~ 8Q(w) 8Q(w) (_~Q())
4 8Wi 8wj exp 2 w
</p>
<p>18
2Q(w) (1 )- - exp --Q(w) .
</p>
<p>28wi8wj 2
(14.19)
</p>
<p>N N
</p>
<p>""' ""' 8wmwnLJ LJ 8Wi Cmn (from (14.18))
m=ln=1 w=o
</p>
<p>~ ~ [8wn 8Wm]
LJ LJ Wma:cmn +Wn ~Cmn
m=1 n=1 Wt Wt w=o
</p>
<p>o (14.20)
</p>
<p>and also
</p>
<p>But
</p>
<p>8
2
Q(w) I
</p>
<p>8Wi8wj w=o
</p>
<p>N N 82
</p>
<p>LL
wmwn
</p>
<p>cmn
8Wi8w '
</p>
<p>m=1 n=1 J w=o
</p>
<p>(14.21)
</p>
<p>where Oij is the Kronecker delta, which is defined to be 1 if i = j and 0 otherwise.
</p>
<p>Hence</p>
<p/>
</div>
<div class="page"><p/>
<p>(independence)
</p>
<p>470 CHAPTER 14. CONTINUOUS N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>and bmjbni equals 1 if (m, n) = (j, i) and equals 0 otherwise, and 8nj8mi equals 1 if
(m ,n) = (i, j) and equals 0 otherwise. Thus,
</p>
<p>~2~~W~ I = Cji + Cij (from (14.21))
W t wJ w=o
</p>
<p>= 2Cij (recall that C T = C).
</p>
<p>Finally, we have the expected result from (14.19) and (14.20) that
</p>
<p>Ex;,xj[XiXj] = ~ [_! 82Q(w)exp (_!Q(W))] I
)2 2 8Wi8wj 2 w=o
</p>
<p>f2 (-~) (2Cij ) = Cij = [C]ij.
o
</p>
<p>Lastly, we extend the characteristic function approach to determining the PDF for
</p>
<p>a sum of IID random variables. Letting Y = Z=~l Xi , the characteristic function of
Y is defined by
</p>
<p>ifJy(w) = Ey[exp(jwY)]
</p>
<p>and is evaluated using (14.7) with g(X1,X2 , ... ,XN) = exp[jwZ=~ lX i] (the real
. and imaginary parts are evaluated as separate integrals) as
</p>
<p>&cent;y(w) = Ex"x" ...x; [exp (iwt x;) ]
= Ex"x" ...,X N [fi eXP(jWX;)] .
</p>
<p>Now using the fact that the Xi'S are IID, we have that
</p>
<p>N
</p>
<p>epy(w) = IIEx;[exp(jwXd]
i=l
</p>
<p>(identically distributed)
</p>
<p>(14.22)
</p>
<p>where epx(w) is the common characteristic function of the random variables. To
</p>
<p>finally obtain the PDF of the sum random variable we use an inverse Fourier trans-
</p>
<p>form to yield
</p>
<p>1
00 dw
</p>
<p>py(y) = [epX(w)]N exp(-jwy)-.
-00 211"
</p>
<p>This formula will form the basis for the exploration of the PDF of a sum of IID
</p>
<p>random variables in Chapter 15. See Problems 14.17 and 14.18 for some examples
</p>
<p>of its use .</p>
<p/>
</div>
<div class="page"><p/>
<p>14.7. CONDITIONAL PDFS
</p>
<p>14.7 Conditional PDFs
</p>
<p>471
</p>
<p>The discussion of Section 9.7 of the definitions and properties of the conditional PMF
</p>
<p>also hold for the conditional PDF. To accommodate continuous random variables we
</p>
<p>need only replace the PMF notation of the "bracket" with that of the PDF notation
</p>
<p>of the "parenthesis." Hence, we do not pursue this topic further.
</p>
<p>14.8 Prediction of a Random Variable Outcome
</p>
<p>We have seen in Section 7.9 that the optimal linear prediction of the outcome of Y
</p>
<p>when X = x is observed to occur is
</p>
<p>(14.23)
</p>
<p>If (X,Y) has a bivariate Gaussian PDF, then the linear predictor is also the optimal
</p>
<p>predictor, amongst all linear and nonlinear predictors. We now extend these results
</p>
<p>to the prediction of a random variable after having observed the outcomes of several
</p>
<p>other random variables. In doing so the orthogonality principle will be introduced.
</p>
<p>Our discussions will assume only zero mean random variables, although the results
</p>
<p>are easily modified to yield the prediction for a nonzero mean random variable. To
</p>
<p>do so note that (14.23) can also be written as
</p>
<p>A cov(X,Y)
Y - Ey[Y] = var(X) (x - Ex [X)).
</p>
<p>But if X and Y had been zero mean, then we would have obtained
</p>
<p>y = cov(X,Y) x
var(X) .
</p>
<p>It is clear that the modification from the zero mean case to the nonzero mean case
</p>
<p>is to replace each Xi by Xi - EXi [Xi] and also Y by Y - Ey[Y].
Now consider the p + 1 continuous random variables {Xl, X2, .. . , X p , X p+l }
</p>
<p>and say we wish to predict X p+l based on the knowledge of the outcomes of
</p>
<p>X I,X2 ,""Xp , Letting Xl = XI,X2 = X2""Xp = xp be those outcomes, we
consider the linear prediction
</p>
<p>P
</p>
<p>Xp+ l = Laixi
i=l
</p>
<p>(14.24)
</p>
<p>where the ai's are the linear prediction coefficients, which are to be determined. The
</p>
<p>optimal coefficients are chosen to minimize the mean square error (MSE)</p>
<p/>
</div>
<div class="page"><p/>
<p>472 CHAPTER 14. CONTINUOUS N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>or written more explicitly as
</p>
<p>mse = Ex"x" ...,x ,+, [ ( X p+1 - t,a;x;) '] . (14.25)
We have used L:f=l aiXi , which is a random variable, as the predictor in order that
the error measure be the average over all predictions. If we now differentiate the
</p>
<p>MSE with respect to a1 we obtain
</p>
<p>oEX1,X2 ,...,Xp+l [(XP+l - L:f=l aiXi)2]
oa1
</p>
<p>(interchange integration
</p>
<p>and differentiation)[ o ~ (Xp+1 - t aiXd2]
1 i=l
</p>
<p>[- 2(
X p+l - t aiXi)X1] = O.
</p>
<p>2=1
</p>
<p>(14.26)
</p>
<p>This produces
</p>
<p>or
</p>
<p>EX1,X2 " ",Xp+l [X1Xp+1] = EX1,X2"" ,Xp+l [t aiX1 X i]
2=1
</p>
<p>k = 1,2, ... ,p
</p>
<p>p
</p>
<p>EX1,Xp+l[X1Xp+1] = L aiE X1,x ;[X 1X i].
i=l
</p>
<p>Letting Cij = Ex; ,xj [XiXj] denote the covariance (since the Xi'S are zero mean) we
have the equation
</p>
<p>p
</p>
<p>L C1i ai = C1 ,p+1'
i=l
</p>
<p>If we differentiate with respect to the other coefficients, similar equations are ob-
</p>
<p>tained. In all, there will be p simultaneous linear equations given by
</p>
<p>p
</p>
<p>L Ckiai = Ck,p+1
i=l
</p>
<p>that need to be solved to yield the ai's. These equations can be written in vec-
</p>
<p>tor/matrix form as
</p>
<p>[ ~ : ~ : ~:] [::] [~: :::: ]
Cp1 Cp2 Cpp ap Cp,p+l
</p>
<p>, v J ~
</p>
<p>C c
</p>
<p>(14.27)</p>
<p/>
</div>
<div class="page"><p/>
<p>14.8. PREDICTION OF A RANDOM VARIABLE OUTCOME 473
</p>
<p>We note that C is the covariance matrix of the random vector [Xl X 2 &bull;.&bull; xp]T and
c is the vector of covariances between X p+ l and each Xi used in the predictor.
</p>
<p>The linear prediction coefficients are found by solving these linear equations. An
</p>
<p>example follows.
</p>
<p>Example 14.6 - Linear prediction based on two random variable out-
</p>
<p>comes
</p>
<p>Consider the prediction of X3 based on the outcomes of Xl and X2 so that X3 =
alXI + a2X2, where p = 2. If we know the covariance matrix of X = [Xl X 2 x 3]T
say ex, then all the Cij'S needed for (14.27) are known. Hence, suppose that
</p>
<p>[
</p>
<p>cu Cl2 C13] [1 2/3 1/3]
Cx = C21 C22 C23 = 2/3 1 2/3 .
</p>
<p>C31 C32 C33 1/3 2/3 1
</p>
<p>Thus, X 3 is correlated with X 2 with a correlation coefficient of 2/3 and X3 is
</p>
<p>correlated with Xl but with a smaller correlation coefficient of 1/3. Using (14.27)
</p>
<p>with p = 2 we must solve
</p>
<p>[
1 2/3] [ al ] = [ 1/3 ] .
</p>
<p>2/3 1 a2 2/3
</p>
<p>By inverting the covariance matrix we have the solution
</p>
<p>1 [1 -2/3 ] [ 1/3 ]
1 - (2/3)2 -2/3 1 2/3
</p>
<p>[-n
Due to the larger correlation of X 3 with X 2, the prediction coefficient a2 is larger.
</p>
<p>Note that if the covariance matrix is Cx = (121, then Cl3 = C23 = 0 and alo pt =
</p>
<p>a2
0p
</p>
<p>t = O. This results in X3 = 0 or more generally for random variables with
nonzero means, X3 = EX3 [X3], as one might expect. See also Problem 14.24 to see
how to determine the minimum value of the MSE.
</p>
<p>\/
As another simple example, observe what happens if p = 1 so that we wish to
predict X 2 based on the outcome of Xl. In this case we have that X2 = alxl and
from (14.27), the solution for al is alo pt = CI2/Cll = cov(XI, X2)/var(Xd. Hence ,
X2 = [cov(XI, X2)/var(Xd]xI and we recover our previous results for the bivariate
case (see (14.23) and let Ex[X] = Ey[Y] = 0) by replacing Xl with X, Xl with X,
</p>
<p>and X 2 with Y.
</p>
<p>An interesting and quite useful interpretation of the linear prediction procedure
</p>
<p>can be made by reexamining (14.26). To simplify the discussion let p = 2 so that</p>
<p/>
</div>
<div class="page"><p/>
<p>474 CHAPTER 14. CONTINUOUS N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>the equations to be solved are
</p>
<p>EX1,X2,X3[(X3 - alXI - a2X2)XI]
</p>
<p>EX1,X2,X3[(X3 - alXI - a2X2)X2]
</p>
<p>o
O. (14.28)
</p>
<p>Let the predictor error be denoted by &euro;, which is explicitly &euro; = X 3 - alXI - a2X2.
</p>
<p>Then (14.28) becomes
</p>
<p>EX1,X2,X3[&euro;XI]
</p>
<p>EX1,X2,X3[&euro;X 2]
</p>
<p>o
o (14.29)
</p>
<p>which says that the optimal prediction coefficients aI, a2 are found by making the
</p>
<p>predictor error uncorrelated with the random variables used to predict X 3 &bull; Presum-
</p>
<p>ably if this were not the case, then some correlation would remain between the error
</p>
<p>and Xl , X 2 , and this correlation could be exploited to reduce the error further (see
</p>
<p>Problem 14.23).
</p>
<p>A geometric interpretation of (14.29) becomes apparent by considering Xl, X 2 ,
</p>
<p>and X 3 as vectors in a Euclidean space as depicted in Figure 14.3a. Since X3 =
</p>
<p>(a)
</p>
<p>X3
</p>
<p>(b)
</p>
<p>Figure 14.3: Geometrical interpretation of linear prediction.
</p>
<p>alXI +a2X2, X3 can be any vector in the shaded region, which is the XI-X2 plane,
depending upon the choice of al and a2. To minimize the error we should choose
</p>
<p>X3 as the orthogonal projection onto the plane as shown in Figure 14.3b. But this
is equivalent to making the error vector &euro; orthogonal to any vector in the plane. In
</p>
<p>particular, then we have the requirement that
</p>
<p>(14.30)</p>
<p/>
</div>
<div class="page"><p/>
<p>14.9. COMPUTER SIMULATION OF GAUSSIAN RANDOM VECTORS 475
</p>
<p>where..l denotes orthogonality. To relate these conditions back to those of (14.29) we
</p>
<p>define two zero mean random variables X and Y to be orthogonal if Ex,y[XY] = O.
</p>
<p>Hence, we have that (14.30) is equivalent to
</p>
<p>EX1,X2,X3[EXI] 0
</p>
<p>EXl ,X2,X3[EX2] 0
</p>
<p>or just the condition given by (14.29). (Since E depends on (Xl, X 2 , X 3 ) , the ex-
</p>
<p>pectation reflects this dependence.) This is called the orthogonality principle. It
</p>
<p>asserts that to minimize the MSE the error "vector" should be orthogonal to each of
</p>
<p>the "data vectors" used to predict the desired "vector". The "vectors" X and Yare
</p>
<p>defined to be orthogonal if Ex,y[XY] = 0, which is equivalent to being uncorrelated
</p>
<p>since we have assumed zero mean random variables. See also Problem 14.22 for the
</p>
<p>one-dimensional case of the orthogonality principle.
</p>
<p>14.9 Computer Simulation of Gaussian
</p>
<p>Random Vectors
</p>
<p>The method described in Section 12.11 for generating a bivariate Gaussian random
</p>
<p>vector is easily extended to the N-dimensional case. To generate a realization of
</p>
<p>X""' N(J.L , C) we proceed as follows:
</p>
<p>1. Perform a Cholesky decomposition of C to yield the N x N nonsingular matrix
</p>
<p>G , where C = GGT .
</p>
<p>2. Generate a realization u of an N x 1 random vector U whose PDF is N(o ,I).
</p>
<p>3. Form the realization of X as x = Gu + J.L.
</p>
<p>As an example, if J.L = 0 and
</p>
<p>then
</p>
<p>C = [ 2~3
1/3
</p>
<p>2/3
1
</p>
<p>2/3
</p>
<p>1/3 ]
2/3
</p>
<p>1
</p>
<p>(14.31)
</p>
<p>G = [0.6~67 0.7~54 ~ ] .
0.3333 0.5963 0.7303
</p>
<p>We plot 100 realizations of X in Figure 14.4. The MATLAB code is given next.</p>
<p/>
</div>
<div class="page"><p/>
<p>476 CHAPTER 14. CONTINUOUS N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>4
</p>
<p>2 : .
</p>
<p>2
-1 0
</p>
<p>Xl
-2-3
</p>
<p>.. :...
:. ~ . . ... - . .".". . .:. . &bull;.....
</p>
<p>&bull; . .:. I. ",tt &bull;&bull;
. ... ., .
</p>
<p>., ~.&middot;,~~u1;.:,::&middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot;
: . . . ~ ... ~~ .... : .
</p>
<p>&bull; &bull;&bull;&bull;&bull; ~ .e, L,&bull; .&bull; . .&bull;. .. : ................ .... .'. . ' . ~ ,.. ... ..... ... .. e:.. . . ~ . ~ . . : . .
.&bull;........ ..........: . .. . .
</p>
<p>-2
</p>
<p>-4
</p>
<p>4
</p>
<p>2
</p>
<p>X2 0
</p>
<p>~o
</p>
<p>Figure 14.4: Realizations of 3 x 1 multivariate Gaussian random vector.
</p>
<p>C=[1 2/3 1/3;2/3 1 2/3;1/3 2/3 1];
</p>
<p>G=chol(C)'; %perform Cholesky decomposition
%MATLAB produces C=A'*A so G=A'
</p>
<p>M=200;
</p>
<p>for m=1:M %generate realizations of x
u=[randn(1,1) randn(1,1) randn(1,1)]';
</p>
<p>x(:,m)=G*u; %realizations stored as columns of 3 x 200 matrix
end
</p>
<p>14.10 Real-World Example - Signal Detection
</p>
<p>An important problem in sonar and radar is to be able to determine when an object,
</p>
<p>such as a submarine in sonar or an aircraft in radar, is present. To make this decision
</p>
<p>a pulse is transmitted into the water (sonar) or air (radar) and one looks to see if a
</p>
<p>reflected pulse from the object is returned. Typically, a digital computer is used to
</p>
<p>sample the received waveform in time and store the samples in memory for further
</p>
<p>processing. We will denote the received samples as Xl, X 2 , . . . , X N. If there is no
</p>
<p>reflection, indicating no object is present, the received samples are due to noise only.
</p>
<p>If, however, there is a reflected pulse, also called an echo, the received samples will
</p>
<p>consist of a signal added to the noise. A standard model for the received samples is to
</p>
<p>assume that X i = Wi, where Wi""'" N(O, (T2) for noise only present and Xi = Si+Wi
for a signal plus noise present. The noise samples Wi are usually also assumed to be
</p>
<p>independent and hence they are IID. With this modeling we can formulate the signal</p>
<p/>
</div>
<div class="page"><p/>
<p>14.10. REAL-WORLD EXAMPLE - SIGNAL DETECTION 477
</p>
<p>detection problem as the problem of deciding between the following two hypotheses
</p>
<p>llw
</p>
<p>lls+w
</p>
<p>Xi =Wi
</p>
<p>Xi = Si + Wi
i = 1,2, ,N
</p>
<p>i = 1,2, ,N.
</p>
<p>It can be shown that a good decision procedure is to choose the hypothesis for which
</p>
<p>the received data samples have the highest probability of occurring. In other words,
</p>
<p>if the received data is more probable when lls+w is true than when llw is true,
</p>
<p>we say that a signal is present. Otherwise, we decide that noise only is present. To
</p>
<p>implement this approach we let px(x;llw) be the PDF when noise only is present
</p>
<p>and px(x; lls+w) be the PDF when a signal plus noise is present. Then we decide
</p>
<p>a signal is present if
</p>
<p>px(x; lls+w) &gt; px(x;llw) . (14.32)
</p>
<p>But from the modeling we have that X = W &lt;"V N(o ,(J2I) for no signal present and
X = s + W &lt;"V N(s, (J2I) when a signal is present. Here we have defined the signal
vector as s = [Sl S2 ... sN]T. Hence, (14.32) becomes from (14.2)
</p>
<p>1 N exp [-~(x -s)T(x - S)] &gt; 1 N exp [_~xT x]
(21f(J2) "2 2(J (21f(J2) "2 2(J
</p>
<p>An equivalent inequality is
</p>
<p>_(X-S)T(X-S) &gt; -xTx
</p>
<p>since the constant 1/(21f(J2)N/2 is positive and the exponential function increases
</p>
<p>with its argument. Expanding the terms we have
</p>
<p>_xTx + xTS + sTx - sTS &gt; _xTX
</p>
<p>and since sTx = xTs we have
</p>
<p>or finally we decide a signal is present if
</p>
<p>N 1 N
</p>
<p>LXiSi&gt; 2Lst.
i=l i=l
</p>
<p>(14.33)
</p>
<p>This detector is called a replica correlator [Kay 1998] since it correlates the data
</p>
<p>Xl , X2 , .. . , XN with a replica of the signal S1,S2, . . . , SN. The quantity on the right-
</p>
<p>hand-side of (14.33) is called the threshold. If the value of 2:~1 XiSi exceeds the
</p>
<p>threshold, the signal is declared as being present.
</p>
<p>As an example, assume that the signal is a "DC level" pulse or s; = A for
</p>
<p>i = 1,2, . . . ,N and that A &gt; O. Then (14.33) reduces to
</p>
<p>N 1
ALxi &gt; -NA2
</p>
<p>i=l 2</p>
<p/>
</div>
<div class="page"><p/>
<p>478 CHAPTER 14. CONTINUOUS N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>and since A &gt; 0, we decide a signal is present if
</p>
<p>Hence, the sample mean is compared to a threshold of A/2. To see how this detector
</p>
<p>performs we choose A = 0.5 and (J2 = 1. The received data samples are shown in
Figure 14.5a for the case of noise only and in Figure 14.5b for the case of a signal plus
</p>
<p>noise. A total of 100 received data samples are shown. Note that the noise samples
</p>
<p>3 r - - ~ - ~ - - ~ - ~ - - - - - - , 3 . - - - ~ - ~ - - ~ - ~ - - - - - - ,
</p>
<p>- 2 - 2 .
</p>
<p>10040 60 80
Sample, i
</p>
<p>20
- 3 L-_--'---_~ __....i..__ ____'__----'
</p>
<p>o1008040 60
Sample, i
</p>
<p>20
- 3 L-_~_~ __ ~ _ ~ _ - - - - '
</p>
<p>o
</p>
<p>(a) Noise only (b) Signal plus noise
</p>
<p>Figure 14.5: Received data samples. Signal is Si = A = 0.5 and noise consists of
IID standard Gaussian random vari ables.
</p>
<p>generated are different for each figure. The value of the sample mean (1/N) 2 : ~ 1 Xi
is shown in Figure 14.6 versus the number of dat a samples N used in the averaging.
</p>
<p>For example, if N = 10, then the value shown is (1/10) 2 : i ~ l Xi, where Xi is found
from the first 10 samples of Figure 14.5. To more easily observe the results they
</p>
<p>have been plotted as a cont inuous curve by connecting the points with straight lines.
</p>
<p>Also, the threshold of A/2 = 0.25 is shown as the dashed line. It is seen that as the
number of data samples averaged increases , the sample mean converges to the mean
</p>
<p>of X i (see also Example 14.4). When noise only is pr esent , this becomes Ex [X] = 0
</p>
<p>and when a signal is present , it becomes Ex [X] = A = 0.5. Thus by comparing the
</p>
<p>sample mean to the threshold of A/2 = 0.25 we should be able to decide if a signal
</p>
<p>is present or not most of the time (see also Problem 14.26).</p>
<p/>
</div>
<div class="page"><p/>
<p>REFERENCES 479
</p>
<p>0.5 ,..--~-~-,.---,.---,.------,
</p>
<p>O'---~-~-~-~-~---'
</p>
<p>ro ~ 00 ~ 00 M 100
Number of samples, N
</p>
<p>&sect; 0.4 .
Q)
</p>
<p>8
Q) 0.3 ..
</p>
<p>~ ..
&sect; 0.2 ; ; .
</p>
<p>if) ..
</p>
<p>100
</p>
<p>. - . ~ .
</p>
<p>noise only
</p>
<p>20 40 60 80
Number of sam ples, N
</p>
<p>-2'---~-~--~ -~ ------'
</p>
<p>o
</p>
<p>- 1.5
</p>
<p>0.5 :Signal pluS&middot;noise&middot; &middot; . . .-
~L:-F--
8
~ - 1 .
</p>
<p>(a) Total view (b) Expand ed view for 70 :::; N :::; 100
</p>
<p>Figure 14.6: Value of sample mean versus the number of dat a samples averaged.
</p>
<p>References
</p>
<p>Kay, S., Fundamentals of Statistical Signal Processing: Detection Th eory, Prentice-
</p>
<p>Hall , Englewood Cliffs, NJ , 1998.
</p>
<p>Muirhead, R.J ., Aspects of Multivariate Statistical Th eory, John Wiley &amp; Sons,
</p>
<p>New York, 1982.
</p>
<p>Resnick, R. , D. Halliday, Physics, Part I, John Wiley &amp; Sons, New York , 1966.
</p>
<p>Problems
</p>
<p>14.1 c.:.:...) (w ,f) If Y = Xl + X 2 + Xs, where X '" N(J-L , C) and
</p>
<p>C
</p>
<p>1/2
1
</p>
<p>1/2
</p>
<p>1/4 ]
1/2
</p>
<p>1
</p>
<p>find the mean and vari ance of Y.
</p>
<p>14.2 (w ,c) If [Xl x 2 ]T '" N (O ,0-21), find P [Xf + Xi &gt; R2 ]. Next , let 0-2 = 1 and
R = 1 and lend credence to your result by performing a computer simulation
to est imate the probabili ty.</p>
<p/>
</div>
<div class="page"><p/>
<p>480 CHAPTER 14. CONTINUOUS N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>14.3 (f) Find the PDF of Y = Xl + X~ +xj if X,...., N(O, I). Hint: Use the results
of Example 14.1. Note that you should obtain the PDF for a X~ random
</p>
<p>variable.
</p>
<p>14.4 (w) An airline has flights that depart according to schedule 95% of the time.
</p>
<p>This means that they depart late 1/2 hour or more 5% of the time due to
</p>
<p>mechanical problems, traffic delays, etc. (for less than 1/2 hour the plane is
</p>
<p>considered to be "on time"). The amount of time that the plane is late is
</p>
<p>modeled as an exp(X] random variable. If a person takes a plane that makes
two stops at intermediate destinations, what is the probability that he will
</p>
<p>be more than 1 1/2 hours late? Hint: You will need the PDF for a sum of
</p>
<p>independent exponential random variables.
</p>
<p>14.5 (f) Consider the transformation from spherical to Cartesian coordinates. Show
that the Jacobian has a determinant whose absolute value is equal to r 2 sin e.
</p>
<p>14.6 C..:....) (w) A large group of college students have weights that can be modeled
as a N(150, 30) random variable. If 4 students are selected at random, what
</p>
<p>is the probability that they will all weigh more than 150 lbs?
</p>
<p>14.7 (t) Prove that the joint PDF given by (14.4) has N(O, 1) marginal PDFs and
</p>
<p>that the random variables are uncorrelated. Hint: Use the known properties
</p>
<p>of the standard bivariate Gaussian PDF.
</p>
<p>14.8 (t) Assume that X,...., N(O, C) for X an N x 1 random vector and that Y =
</p>
<p>GX, where G is an M x N matrix with M &lt; N. If the characteristic function
of X is &cent;x (w) = exp (- ~wT Cw), find the characteristic function of Y. Use
the following
</p>
<p>Based on your results conclude that Y ,...., N(O, GCGT ) .
</p>
<p>14.9 C.:.,) (f) If Y = Xl + X 2 + X 3 , where X ,...., N(o, C) and C = diag(ar, ai, aj),
find the PDF of Y. Hint: See Problem 14.8.
</p>
<p>14.10 (f) Show that if Cx is a diagonal matrix, then aTCxa = 2:~1 a[Var(Xi)'
</p>
<p>14.11 (c) Simulate a single realization of a random vector composed of IID random
</p>
<p>variables with PDF Xi ,...., N(l, 2) for i = 1,2, ... ,N. Do this by repeating an
experiment that successively generates X ,...., N(l, 2). Then, find the outcome
of the sample mean random variable and discuss what happens as N becomes
</p>
<p>large.
</p>
<p>14.12 c.:.:.,,) (w,c) An Nx 1 random vector X has EX;[Xi] = J-l and var(Xi ) = ia2 for
i = 1,2, ... ,N. The components of X are independent. Does the sample mean</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 481
</p>
<p>random variable converge to /-L as N becomes large? Carry out a computer
</p>
<p>simulation for this problem and explain your results.
</p>
<p>14.13 (t) Prove that if X 1,X2 , ... ,XN are independent random variables, then
</p>
<p>&lt;PX1,X2 ,...,XN(W1,W2,'" ,WN) = rr~l &lt;Px;(wd&middot;
</p>
<p>14.15 (t) If X'" N(p" C) with X an N x 1 random vector, prove that the charac-
</p>
<p>teristic function is
</p>
<p>&lt;Px(w) = exp (jWTp, - ~wTCw ) .
</p>
<p>To do so note that the characteristic function of a random vector distributed
</p>
<p>according to N(o, C) is exp (_~wTCw). With these results show that the
PDF of Y = GX for G an M x N matrix with M &lt; N is N(Gp" GCGT ) .
</p>
<p>14.16 (t) Prove that if X '" N(p" C) for X an N x 1 random vector, then the
</p>
<p>marginal PDFs are Xi '" N(/-Li, an. Hint: Examine the PDF of Y = eTX,
where e, is the N x 1 vector whose elements are all zeros except for the ith
</p>
<p>element, which is a one. Also, make use of the results of Problem 14.15.
</p>
<p>14.17 (f) Prove that if Xi '" N(O, 1) for i = 1,2 ... ,N and the Xi'S are IID, then
</p>
<p>L ~ l Xl '" X~&middot; To do so first find the characteristic function of Xl. Hint:
You will need the result that
</p>
<p>/
</p>
<p>00 1 (1 x 2 )--exp --- dx = 1
-00 ..j21fc 2 C
</p>
<p>for c a complex number. Also, see Table 11.1.
</p>
<p>14.18 (t) Prove that if Xi '" exp().) and the Xi'S are IID, then L ~ l Xi has an
Erlang PDF. Hint: See Table 11.1.
</p>
<p>14.19 (...:.:,) (w,c) Find the mean and variance of the random variable
</p>
<p>12
</p>
<p>Y = l:(Ui - 1/2)
i=l
</p>
<p>where U; '" U(O, 1) and the Ui'S are IID. Estimate the PDF of Y using a
computer simulation and compare it to a standard Gaussian PDF. See Section
</p>
<p>15.5 for a theoretical justification of your results.</p>
<p/>
</div>
<div class="page"><p/>
<p>482 CHAPTER 14. CONTINUOUS N-DIMENSIONAL RANDOM VARIABLES
</p>
<p>14.20 (w) Three different voltmeters measure the voltage of a 100 volt source. The
</p>
<p>measurements can be modeled as random variables with
</p>
<p>lfl }{(100,1)
</p>
<p>lf2 f'.J }{(100,1O)
</p>
<p>lf3 f'.J }{(100, 5).
</p>
<p>Is it better to average the results or just use the most accurate voltmeter?
</p>
<p>14.21 C:.:J (f) If a 3 x 1 random vector has mean zero and covariance matrix
</p>
<p>find the optimal prediction of X 3 given that we have observed X I
</p>
<p>X 2 = 2.
</p>
<p>1 and
</p>
<p>14.22 (t) Consider the prediction of the random variable Y based on observing that
</p>
<p>X = x. Assuming (X, Y) is a zero mean random vector, we propose using the
linear prediction Y = ax. Determine the optimal value of a (being the value
that minimizes the MSE) by using the orthogonality principle. Explain your
</p>
<p>results by drawing a diagram.
</p>
<p>14.23 (f) If a 3 x 1 random vector X has a zero mean and covariance matrix
</p>
<p>determine the optimal linear prediction of X3 based on the observed outcomes
</p>
<p>of Xl and X2. Why is alopt = O? Hint: Consider the covariance between
</p>
<p>E = X 3- pX2 , which is the predictor error for X 3 based on observing only X2 ,
and Xl.
</p>
<p>14.24 C...:...) (t,f) Explain why the minimum MSE of the predictor X3 = aloptXI +
a2optX2 is
</p>
<p>msemin = E X 1,X2,X3 [(X 3 - aloptXI - a2optX 2)2]
</p>
<p>= EXI ,X2,X3 [(X3 - aloptXI - a2optX2)X3 ]
</p>
<p>C33 - aloptcl3 - a2optC23&middot;
</p>
<p>Next use this result to find the minimum MSE for Example 14.6.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 483
</p>
<p>14.25 (...:....:..-) (c) Use a computer simulation to generate realizations of the random
</p>
<p>vector X described in Example 14.6. Then, predict X 3 based on the outcomes
</p>
<p>of Xl and X2 and plot the true realizations and the predictions. Finally,
</p>
<p>estimate the average predictor error and compare your results to the theoretical
</p>
<p>minimum MSE obtained in Problem 14.24.
</p>
<p>14.26 (w) For the signal detection example described in Section 14.9 prove that
</p>
<p>the probability of saying a signal is present when indeed there is one goes to
</p>
<p>1 as A -+ 00.
</p>
<p>14.27 (c) Generate on a computer 1000 realizations of the two different random
</p>
<p>variables Xw rv N{o, 1) and X s+w rv N{0.5, 1). Next plot the outcomes of
the sample mean random variable versus N, the number of successive samples
</p>
<p>averaged, or XN = (liN) L ~ l Xi&middot; What can you say about the sample means
as N becomes large? Explain what this has to do with signal detection.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 15
</p>
<p>Probability and Moment
</p>
<p>Approximations Using Limit
</p>
<p>Theorems
</p>
<p>15.1 Introduction
</p>
<p>So far we have described the methods for determining the exact probability of events
</p>
<p>using probability mass functions (PMFs) for discrete random variables and proba-
</p>
<p>bility density functions (PDFs) for continuous random variables. Also of importance
</p>
<p>were the methods to determine the moments of these random variables. The proce-
</p>
<p>dures employed were all based on knowledge of the PMF/PDF and the implementa-
</p>
<p>tion of its summation/integration. In many practical situations the PMF/PDF may
</p>
<p>be unknown or the summation/integration may not be easily carried out. It would
</p>
<p>be of great utility, therefore, to be able to approximate the desired quantities using
</p>
<p>much simpler methods. For random variables that are the sum of a large number of
</p>
<p>independent and identically distributed random variables this can be done. In this
</p>
<p>chapter we focus our discussions on two very powerful theorems in probability-the
</p>
<p>law of large numbers and the central limit theorem. The first theorem asserts that
</p>
<p>the sample mean random variable, which is the average of lID random variables and
</p>
<p>which was introduced in Chapter 14, converges to the expected value, a number, of
</p>
<p>each random variable in the average. The law of large numbers is also known collo-
</p>
<p>quially as the law of averages. Another reason for its importance is that it provides
</p>
<p>a justification for the relative frequency interpretation of probability. The second
</p>
<p>theorem asserts that a properly normalized sum of lID random variables converges
</p>
<p>to a Gaussian random variable.
</p>
<p>The theorems are actually the simplest forms of much more general results.
</p>
<p>For example, the theorems can be formulated to handle sums of nonidentically
</p>
<p>distributed random variables [Rao 1973] and dependent random variables [Brockwell
</p>
<p>and Davis 1987].</p>
<p/>
</div>
<div class="page"><p/>
<p>486
</p>
<p>15.2
</p>
<p>CHAPTER 15. PROBABILITY AND MOMENT APPROXIMATIONS
</p>
<p>Summary
</p>
<p>The Bernoulli law of large number is introduced in Section 15.4 as a prelude to the
</p>
<p>more general law of large numbers. The latter is summarized in Theorem 15.4.1 and
</p>
<p>asserts that the sample mean random variable of IID random variables will converge
</p>
<p>to the expected value of a single random variable. The central limit theorem is
</p>
<p>described in Section 15.5 where it is demonstrated that the repeated convolution of
</p>
<p>PDFs produces a Gaussian PDF. For continuous random variables the central limit
</p>
<p>theorem, which asserts that the sum of a large number of lID random variables has a
</p>
<p>Gaussian PDF, is summarized in Theorem 15.5.1. The precise statement is given by
</p>
<p>(15.6) . For the sum of a large number ofIID discrete random variables it is the CDF
</p>
<p>that converges to a Gaussian CDF. Theorem 15.5.2 is the central limit theorem for
</p>
<p>discrete random variables. The precise statement is given by (15.9). The concept
</p>
<p>of confidence intervals is introduced in Section 15.6. A 95% confidence interval for
</p>
<p>the sample mean estimate of the parameter p of a Ber(p) random variable is given
</p>
<p>by (15.14). It is then applied to the real-world problem of opinion polling.
</p>
<p>15.3 Convergence and Approximation of a Sum
</p>
<p>Since we will be dealing with the sum of a large number of random variables, it is
</p>
<p>worthwhile first to review some concepts of convergence. In particular, we need to
</p>
<p>understand the role that convergence plays in approximating the behavior of a sum
</p>
<p>of terms. As an illustrative example, consider the determination of the value of the
</p>
<p>sum
</p>
<p>for some large value of N. We have purposedly chosen a sum that may be evaluated
</p>
<p>in closed form to allow a comparison to its approximation. The exact value can be
</p>
<p>found as
</p>
<p>Examples of SN versus N are shown in Figure 15.1. The values of SN have been
</p>
<p>connected by straight lines for easier viewing. It should be clear that as N ~ 00,
</p>
<p>SN ~ 1 if lal &lt; 1. This means that if N is sufficiently large, then SN will differ from 1
by a very small amount. This small amount, which is the error in the approximation
</p>
<p>of SN by 1, is given by</p>
<p/>
</div>
<div class="page"><p/>
<p>15.4. LAW OF LARGE NUMBERS 487
</p>
<p>2.-- - ----.---.---- - ---.---...--- -----,
</p>
<p>- - - - ~ - - - - ~ - - - - ~ - - - - ~ - - - -
0.8 .
</p>
<p>0.6 " ' ; ' " .
</p>
<p>0.4 : .
</p>
<p>10080604020
</p>
<p>0.2 ..
</p>
<p>O'--- - ----'----..L..-------'------'---- ----'
o
</p>
<p>N
</p>
<p>Figure 15.1: Convergence of sum to 1.
</p>
<p>and will depend on a as well as N . For example, if we wish to claim that the error
</p>
<p>is less than 0.1, then N would have to be 10 for a = 0.5 but N would need to be 57
</p>
<p>for a = 0.85, as seen in Figure 15.1. Thus, in general the error of the approximation
</p>
<p>will depend upon the particular sequence (value of a here). We can assert , without
</p>
<p>actually knowing the value of a as long as lal &lt; 1 and hence the sum converges, that
S N will eventually become close to 1. The error can be quite large for a fixed value
</p>
<p>of N (consider what would happen if a = 0.999). Such are the advantages (sum will
be close to 1 for aUlal &lt; 1) and disadvantages (how large does N have to be?) of
limit theorems. We next describ e the law of large numbers.
</p>
<p>15 .4 Law of Large N umbers
</p>
<p>When we began our study of probability, we argued that if a fair coin is tossed N
</p>
<p>times in succession, then the relative frequency of heads, i.e., the number of heads
</p>
<p>observed divided by the number of coin tosses, should be close to 1/2. This was
</p>
<p>why we intuitively accepted the assignment of a probability of 1/2 to the event that
</p>
<p>the outcome of a fair coin toss would be a head. If we continue to toss the coin,
</p>
<p>then as N -T 00, we expect the relative frequency to approach 1/2. We can now
</p>
<p>prove that this is indeed the case under certain assumptions. First we model the
</p>
<p>repeated coin toss experiment as a sequence of N Bernoulli subexperiments (see
</p>
<p>also Section 4.6.2). The result of the it h subexperiment is denoted by the discrete
</p>
<p>random variable X i, where
</p>
<p>1 if heads
</p>
<p>o if tails.</p>
<p/>
</div>
<div class="page"><p/>
<p>(15.1)
</p>
<p>488 CHAPTER 15. PROBABILITY AND MOMENT APPROXIMATIONS
</p>
<p>We can then model the overall experimental output by the random vector X =
</p>
<p>[Xl X 2 ... XNV. We next assume that the discrete random variables Xi are lID
</p>
<p>with marginal PMF
</p>
<p>{
</p>
<p>~ k = 0
px[k] = ~ k = 1
</p>
<p>or the experiment is a sequence of independent and identical Bernoulli subexperi-
</p>
<p>ments. Finally, the relative frequency is given by the sample mean random variable
</p>
<p>1 N
</p>
<p>XN= NLXi
i=l
</p>
<p>which was introduced in Chapter 14, although there it was used for the average of
</p>
<p>continuous random variables. We subscript the sample mean random variable by N
</p>
<p>to remind us that N coin toss outcomes are used in its computation. Now consider
</p>
<p>what happens to the mean and variance of XN as N --+ 00. The mean is
</p>
<p>1 N
Ex [XN ] = N L Ex [Xi]
</p>
<p>i=l
</p>
<p>1 N
</p>
<p>= NLEx;[Xi]
i=l
</p>
<p>1 N 1
</p>
<p>NL 2
i=l
</p>
<p>1
= for all N .
</p>
<p>2
</p>
<p>The variance is
</p>
<p>var ( ~ tXi)
1 N
</p>
<p>N2 L var(Xd (Xi'S are independent ~ uncorrelated)
i = l
</p>
<p>(Xi'S are identically distributed
</p>
<p>~ have same variance).
</p>
<p>But for a Bernoulli random variable, Xi '" Ber(p), the variance is var(Xd = p(l-p).
Since p = 1/2 for a fair coin,
</p>
<p>p(l- p)
</p>
<p>N
1
</p>
<p>4N --+ 0 as N --+ 00.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.4. LAW OF LARGE NUMBERS 489
</p>
<p>Therefore the width of the PMF of XN must decrease as N increases and eventually
</p>
<p>go to zero . Since the variance is defined as
</p>
<p>we must have that as N --+ 00, X N --+ Ex [XN] = 1/2. In effect the random
variable XN becomes not random at all but a constant. It is called a degenerate
</p>
<p>random variable. To further verify that the PMF becomes concentrated about its
</p>
<p>mean, which is 1/2, we note that the sum of N IID Bernoulli random variables is a
</p>
<p>binomial random variable. Thus,
</p>
<p>and therefore the PMF is
</p>
<p>k = 0,1, ... ,N.
</p>
<p>- - N -
To find the PMF of XN we let X N = (1/N) Li=l Xi = SN/ N and note that XN
can take on values Uk = kiN for k = 0,1 , . . . ,N. Therefore, using the formula for
the transformation of a discrete random variable, the PMF becomes
</p>
<p>Uk = kiN; k = 0,1, ... , N (15.2)
</p>
<p>which is plotted in Figure 15.2 for various values of N. Because as N increases XN
</p>
<p>0,05 ...
</p>
<p>01~0-~~
</p>
<p>0.25.-----------,
</p>
<p>'-::;; 0.2 . . . . .. . . , .. . &bull; .. .. ,.
</p>
<p>2..
, ~. 1 5 ..
R.
</p>
<p>0,1 f ....;....... .. :... .... ..;.... &middot;.... &middot;: &middot;.... &middot; &middot; 1
</p>
<p>0.05 . ...
</p>
<p>O.lf&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;:&middot;&middot;&middot;&middot; &middot;&middot;&middot;&middot;&middot;'HIIII
</p>
<p>0.25.----,---,-- .,-----,---,
</p>
<p>~ O . 2 ,
</p>
<p>2..
,~15 ..
R.
</p>
<p>,l T,
</p>
<p>0.1
</p>
<p>o
o 0.2 0.4 0,6 0.8
</p>
<p>Uk
</p>
<p>0.25
</p>
<p>0.05
</p>
<p>'-::;; 0.2
</p>
<p>2..
</p>
<p>,~ .15
R.
</p>
<p>(a) N = 10 (b) N = 30 (c) N = 100
</p>
<p>Figure 15.2: PMF for sample mean random variable of N IID Bernoulli random
</p>
<p>variables with p = 1/2. It models the relative frequency of heads obtained for N
fair coin tosses.</p>
<p/>
</div>
<div class="page"><p/>
<p>490 CHAPTER 15. PROBABILITY AND MOMENT APPROXIMATIONS
</p>
<p>takes on values more densely in the interval [0, 1], we do not obtain a PMF with
</p>
<p>all its mass concentrated at 0.5, as we might expect. Nonetheless, the probability
</p>
<p>that the sample mean random variable will be concentrated about 1/2 increases.
</p>
<p>As an example, the probability of being within the interval [0.45,0.55] is 0.2461 for
</p>
<p>N = 10, 0.4153 for N = 30, and 0.7287 for N = 100, as can be verified by summing
the values of the PMF over this interval. Usually it is better to plot the CDF since
</p>
<p>as N -+ 00, it can be shown to converge to the unit step beginning at u = 0.5
(see Problem 15.1). Also, it is interesting to note that the PMF appears Gaussian,
</p>
<p>although it changes in amplitude and width for each N. This is an observation that
</p>
<p>we will focus on later when we discuss the central limit theorem. The preceding
</p>
<p>results say that for large enough N the sample mean random variable will always
</p>
<p>yield a number, which in this case is 1/2. By "always" we mean that every time we
</p>
<p>perform a repeated Bernoulli experiment consisting of N independent and fair coin
</p>
<p>tosses, we will obtain a sample mean of 1/2, for N large enough. As an example,
</p>
<p>we have plotted in Figure 15.3 five realizations of the sample mean random variable
</p>
<p>or XN versus N. The values of XN have been connected by straight lines for easier
</p>
<p>viewing. We see that
</p>
<p>0.9 .
</p>
<p>0.8 . . .. . . . . . . . . . . . . . . .
</p>
<p>0.3
</p>
<p>500400300200100
</p>
<p>0.2
</p>
<p>0.1
</p>
<p>o"-__----'- -'--__----L --'-__---'
o
</p>
<p>N
</p>
<p>Figure 15.3: Realizations of sample mean random variable of N lID Bernoulli ran-
</p>
<p>dom variables with p = 1/2 as N increases.
</p>
<p>- 1
XN -+"2 = Ex [X] . (15.3)
</p>
<p>This is called the B ernoulli law of large numbers, and is known to the layman as
</p>
<p>the law of averages. More generally for a Bernoulli subexperiment with probability
</p>
<p>p , we have that
</p>
<p>XN -+P = Ex [X].
</p>
<p>The sample mean random variable converges to the expected value of a single ran-
</p>
<p>dom variable. Note that since XN is the relative frequency of heads and p is the</p>
<p/>
</div>
<div class="page"><p/>
<p>15.4. LAW OF LARGE NUMBERS 491
</p>
<p>probability of heads, we have shown that the probability of a head in a single coin
</p>
<p>toss can be interpreted as the value obtained as the relative frequency of heads in a
</p>
<p>large number of independent and identical coin tosses. This observation also justifies
</p>
<p>our use of the sample mean random variable as an estimator of a moment since
</p>
<p>__ 1 N
</p>
<p>Ex[X] = N z=Xi -+ Ex [X]
i=l
</p>
<p>asN-+oo
</p>
<p>and more generally, justifies our use of (liN) L:f::l xr as an estimate of the nth
moment E[Xn ] (see also Problem 15.6).
</p>
<p>A more general law of large numbers is summarized in the following theorem. It
</p>
<p>is valid for the sample mean of IID random variables, either discrete, continuous, or
</p>
<p>mixed.
</p>
<p>Theorem 15.4.1 (Law of Large Numbers) If Xl, X 2 , . .. , XN are lID random
</p>
<p>variables with mean Ex[X] and var(X) = (72 &lt; 00, then limN-tooXN = Ex [X].
</p>
<p>Proof:
</p>
<p>Consider the probability of the sample mean random variable deviating from the
</p>
<p>expected value by more than E, where E is a small positive number. This probability
</p>
<p>is given by
</p>
<p>P [IXN - Ex [X]I &gt; E] = P [IXN - Ex [XN]! &gt; E] .
</p>
<p>Since var(XN) = (721N, we have upon using Chebyshev's inequality (see Section
</p>
<p>11.8)
</p>
<p>and taking the limit of both sides yields
</p>
<p>2
</p>
<p>lim P [IXN - Ex [X] I &gt; E] :S lim ~ = O.
N-too N-too N E
</p>
<p>Since a probability must be greater than or equal to zero, we have finally that
</p>
<p>lim P [IXN - Ex [X]I&gt; E] = 0
N-too
</p>
<p>(15.4)
</p>
<p>which is the mathematical statement that the sample mean random variable con-
</p>
<p>verges to the expected value of a single random variable.
</p>
<p>o
The limit in (15.4) says that for large enough N, the probability of the error
</p>
<p>in the approximation of XN by Ex[X] exceeding E (which can be chosen as small
as desired) will be exceedingly small. It is said that XN -+ Ex[X] in probability
[Grimmett and Stirzaker 2001].</p>
<p/>
</div>
<div class="page"><p/>
<p>492 CHAPTER 15. PROBABILITY AND MOMENT APPROXIMATIONS
</p>
<p>&amp; Convergence in probability does not mean all realizations will
converge.
</p>
<p>Referring to Figure 15.3 it is seen that for all realizations except the top one, the
</p>
<p>error is small. The statement of (15.4) does allow some realizations to have an
</p>
<p>error greater than E for a given large N. However, the probability of this happening
</p>
<p>becomes very small but not zero as N increases. For all practical purposes, then, we
</p>
<p>can ignore this occurrence. Hence , convergence in probability is somewhat different
</p>
<p>than what one may be familiar with in dealing with convergence of deterministic
</p>
<p>sequences. For deterministic sequences, all sequences (since there is only one) will
</p>
<p>have an error less than E for all N 2:: N f , where N, will depend on E (see Figure 15.1).
The interested reader should consult [Grimmett and Stirzaker 2001] for further
</p>
<p>details. See also Problem 15.8 for an example.
</p>
<p>We conclude our discussion with an example and some further comments.
</p>
<p>Example 15.1 - Sample mean for lID Gaussian random variables
</p>
<p>Recall from the real-world example in Chapter 14 that when a signal is present we
</p>
<p>have
</p>
<p>X S+W i '" N(A, a
2
</p>
<p>) i = 1,2, ... ,N.
</p>
<p>Since the random variables are lID, we have by the law of large numbers that
</p>
<p>XN -+ Ex[X] = A.
</p>
<p>Thus, the upper curve shown in Figure 14.6 must approach A = 0.5 (with high
probability) as N -+ 00.
</p>
<p>\/
In applying the law of large numbers we do not need to know the marginal PDF.
</p>
<p>If in the previous example, we had XS+Wi '" U(O, 2A), then we also conclude that
</p>
<p>XN -+ A. As long as the random variables are lID with mean A and a finite
variance, XN -+ A (although the error in the approximation will depend upon the
marginal PDF-see Problem 15.3).
</p>
<p>15.5 Central Limit Theorem
</p>
<p>By the law of large numbers the PMF/PDF of the sample mean random variable
</p>
<p>decreases in width until all the probability is concentrated about the mean. The
</p>
<p>theorem, however, does not say much about the PMF/PDF itself. However, by con-
</p>
<p>sidering a slightly modified sample mean random variable, we can make some more
</p>
<p>definitive assertions about its probability distribution. To illustrate the necessity
</p>
<p>of doing so we consider the PDF of a continuous random variable that is the sum</p>
<p/>
</div>
<div class="page"><p/>
<p>15.5. CENTRAL LIMIT THEOREM 493
</p>
<p>of N continuous liD random variables. A particularly illustrative example is for
</p>
<p>Xi rvU(-1/2 ,1/2).
</p>
<p>Example 15.2 - PDF for sum of lID U(-1/2 , 1/2) random variables
</p>
<p>Consider the sum
N
</p>
<p>8N = 2:Xi
i=l
</p>
<p>where the X i'S are liD random variables with Xi rv U( -1/2,1/2). If N = 2, then
82 = Xl + X 2 and the PDF of 82 is easily found using a convolution integral as
described in Section 12.6. Therefore,
</p>
<p>PS2(X) = px(x) *px(x) = i:px(u)px(x - u)du
where * denotes convolution. The evaluation of the convolution integral is most
easily done by plotting px(u) and px(x - u) versus u as shown in Figure 15.4a.
</p>
<p>This is necessary to determine the regions over which the product of px(u) and
</p>
<p>px(x - u) is nonzero and so contributes to the integral. The reader should be able
</p>
<p>to show, based upon Figure 15.4a, that the PDF of 8 2 is that shown in Figure 15.4b.
</p>
<p>More generally, we have from (14.22) that
</p>
<p>1
__.L...-__ I - - _ ~ __ ~ x
</p>
<p>1-xl+x
</p>
<p>_ _ ___ u
</p>
<p>(a) Cross-hatched region con-
</p>
<p>tributes to integral
(b) Result of convolution
</p>
<p>Figure 15.4: Determining the PDF for the sum of two independent uniform random
</p>
<p>variables using a convolution integral evaluation.
</p>
<p>PS
N
</p>
<p>(x) = (Xi &lt;p~(w) exp( -jwx) dw
1-00 2~
</p>
<p>= px(x) *px(x) * ... *px(x).
, #
</p>
<p>v
</p>
<p>(N-I) convolutions
</p>
<p>Hence to find ps3(x) we must convolve ps2(x) with px(x) to yield px(x) *px(x) *
px(x) since PS2(X) = px(x) *Px(x). This is
</p>
<p>PS3(X) = 1:ps2(u)px(x - u)du</p>
<p/>
</div>
<div class="page"><p/>
<p>494 CHAPTER 15. PROBABILITY AND MOMENT APPROXIMATIONS
</p>
<p>but since px(-x) = px(x), we can express this in the more convenient form as
</p>
<p>The integrand may be determined by plotting P82 (u) and the right-shifted version
</p>
<p>px(u - x) and multiplying these two functions. The different regions that must be
</p>
<p>considered are shown in Figure 15.5. Hence, referring to Figure 15.5 we have
</p>
<p>J~ &bull; u -~C----f--'----+-'''''--'--'' u
1
</p>
<p>x
-11-1
</p>
<p>&bull; u
</p>
<p>1-1
</p>
<p>(a) -3/2::; x ::; -1/2 (b) -1/2 &lt; x ::; 1/2 (c) 1/2 &lt; x ::; 3/2
</p>
<p>Figure 15.5: Determination of limits for convolution integral.
</p>
<p>rX +I / 2
i-I P82(U) &middot;ldu
1 2 3 9
-x + -x+-
2 2 8
</p>
<p>.:P82(U) . Idux - I/2
2 3
</p>
<p>-x +-
4
</p>
<p>t' P82(U) . Idu
iX-I /2
</p>
<p>1 2 3 9
-x - -x+-
2 2 8
</p>
<p>3 1
- - &lt; x &lt;--
</p>
<p>2 - - 2
</p>
<p>1 1
--&lt;x&lt;-
</p>
<p>2 - 2
</p>
<p>1 3
-&lt;x&lt;-
2 - 2
</p>
<p>and P83 (x) = &deg;otherwise. This is plotted in Figure 15.6 versus the PDF of a
N(0,3/12) random variable. Note the close agreement. We have chosen the mean
</p>
<p>and variance of the Gaussian approximation to match that of P83(X) (recall that
</p>
<p>var(X) = (b - a)2/12 for X "" U(a, b) and hence var(Xi) = 1/12). If we continue
the convolution process , the mean will remain at zero but the variance of SN will
</p>
<p>be N/12.
</p>
<p>o
A MATLAB program that implements a repeated convolution for a PDF that is
</p>
<p>nonzero over the interval (0,1) is given in Appendix 15A. It can be used to verify</p>
<p/>
</div>
<div class="page"><p/>
<p>15.5. CENTRAL LIMIT THEOREM 495
</p>
<p>solid ':' exact PDF
</p>
<p>0.9 : d ~ ~ h ~ d &middot; ~&middot; N(b",&middot;3ji2) approximation
M . . .. . . ~ , . . .. . . ..
</p>
<p>I . \
</p>
<p>21.50.5o
x
</p>
<p>oL.--......."'------'------'---'----'------'-----"''''''''----'
- 2 -1.5 -1 -0.5
</p>
<p>0.4 .
</p>
<p>0.1
</p>
<p>0.3
</p>
<p>0.2 .
</p>
<p>0.7
</p>
<p>~0 .6 .
~
~O .5
</p>
<p>Figure 15.6: PDF for sum of 3 lID U( -1/2,1/2) random variables and Gaussian
</p>
<p>approximation.
</p>
<p>analytical results and also to tryout other PDFs. An example of its use is shown in
</p>
<p>Figure 15.7 for the repeated convolution of a U(O, 1) PDF. Note that as N increases
</p>
<p>the PDF moves to the right since E[SN] = NEx[X] = N/2 and the variance also
increases since var(SN) = Nvar(X) = N/12. Because of this behavior it is not
</p>
<p>possible to state that the PDF converges to any PDF. To circumvent this problem
</p>
<p>it is necessary to normalize the sum so that its mean and variance are fixed as N
</p>
<p>increases. It is convenient, therefore, to have the mean fixed at &deg;and the variance
fixed at 1, resulting in a standardized sum. Recall from Section 7.9 that this is easily
</p>
<p>accomplished by forming
</p>
<p>SN - E[SN]
</p>
<p>y'var(SN)
</p>
<p>SN-NEx[X]
</p>
<p>y'Nvar(X)
(15.5)
</p>
<p>By doing so, we can now assert that this standardized random variable will converge
</p>
<p>to a N(O , 1) random variable. An example is shown in Figure 15.8 for Xi rv U(O, 1)
</p>
<p>and for N = 2,3,4. This is the famous central limit theorem, which says that the
PDF of the standardized sum of a large number of continuous IID random variables
</p>
<p>will converge to a Gaussian PDF. Its great importance is that in many practical
</p>
<p>situations one can model a random variable as having arisen from the contributions
</p>
<p>of many small and similar physical effects. By making the lID assumption we can
</p>
<p>assert that the PDF is Gaussian. There is no need to know the PDF of each random
</p>
<p>variable or even if it is known, to determine the exact PDF of the sum, which may
</p>
<p>not be possible. Some application areas are:</p>
<p/>
</div>
<div class="page"><p/>
<p>496 CHAPTER 15. PROBABILITY AND MOMENT APPROXIMATIONS
</p>
<p>PDF for SN PDF for SN
</p>
<p>N=2
0.8 0.8 , .
</p>
<p>0.6 .
</p>
<p>0.4
</p>
<p>0.2
</p>
<p>\
2 4 6
</p>
<p>x
8 10 12 6
</p>
<p>x
8 10 12
</p>
<p>(a) p x
</p>
<p>PDF for SN PDF for SN
</p>
<p>0.8 . . . .
N=8 N = 12 :
</p>
<p>0.8 , : . .
</p>
<p>0.6 . . .
</p>
<p>(J)
</p>
<p>Co
</p>
<p>0.4 .
</p>
<p>0.2
</p>
<p>0
10 12 0 2 4 6 8 10 12
</p>
<p>x
86
</p>
<p>x
42
</p>
<p>O'---"---~-----"'"-~-~----'
</p>
<p>o
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6 . . .
</p>
<p>(J)
</p>
<p>Co
</p>
<p>(c) PSa (d) PS 12
</p>
<p>Figure 15.7: PDF of sum of N lID U(O, 1) random variables. The plots were obtained
</p>
<p>using clLdemo.m listed in Appendix 15A.
</p>
<p>1. Polling (see Section 15.6) [Weisburg, Krosnick, Bowen 1996]
</p>
<p>2. Noise characterization [Middleton 1960]
</p>
<p>3. Scattering effects modeling [Urick 1975]
</p>
<p>4. Kinetic theory of gases [Reif 1965]
</p>
<p>5. Economic modeling [Harvey 1989]</p>
<p/>
</div>
<div class="page"><p/>
<p>15.5. CENTRAL LIMIT THEOREM 497
</p>
<p>432
</p>
<p>~ .
.. ~ ~ . . .. . . . . . . . ..&bull; . .
</p>
<p>o
X
</p>
<p>-1-2-3
</p>
<p>o L-_~:..L---'--_----'-_----'- _ ___I.__ L . . . . . . . . l ~ " ' - - - - - '
-4
</p>
<p>0.05 , .
</p>
<p>0.1 .
</p>
<p>05 ,---,-----.------.----,-----,-----,r---.------,
</p>
<p>0.45
</p>
<p>0.4
</p>
<p>0.35
</p>
<p>~ 03
~
c, 0.25
</p>
<p>02
</p>
<p>0.15
</p>
<p>Figure 15.8: PDF of standardized sum of N IID U(O, 1) random variables.
</p>
<p>(15.6)
</p>
<p>and many more.
</p>
<p>We now state the theorem for continuous random variables.
</p>
<p>Theorem 15.5.1 (Central limit theorem for continuous random variables)
</p>
<p>If Xl , X2, ... , XN are continuous lID random variables, each with mean Ex [X] and
</p>
<p>variance var(X) , and SN = 2:i:1 Xi , then as N -+ 00
</p>
<p>SN - E[SN] _ 2:i:1 Xi - NEx[X] N( )
- -+ 0,1 .
</p>
<p>..jvar(SN) ..jNvar(X)
</p>
<p>(15.7)
</p>
<p>Equivalently, the CDF of the standardized sum converges to &lt;p(x) or
</p>
<p>p [SN - E[SN] :s; X] -+ [X ~ exp (_~t2) dt = &lt;p(x).
Jvar(SN) J-OOV21f 2
</p>
<p>The proof is given in Appendix 15B and is based on the properties of characteristic
</p>
<p>functions and the continuity theorem. An example follows.
</p>
<p>Example 15.3 - PDF of sum of squares of independent N(O,1) random
</p>
<p>variables
</p>
<p>Let Xi rv N(O, 1) for i = 1,2, .. . , N and assume that the Xi'S are independent.
</p>
<p>We wish to determine the approximate PDF of YN = 2:i:1 Xl as N becomes large.
Note that the exact PDF for YN is a X'jy PDF so that we will equivalently find
</p>
<p>an approximation to the PDF of the standardized X'jy random variable. To apply
</p>
<p>the central limit theorem we first note that since the Xi'S are IID so are the Xl's
</p>
<p>(why?). Then as N -+ 00 we have from (15.6)
</p>
<p>2:i:1 xs : NEx[X2] N( )
-+ 0,1 .
</p>
<p>..jNvar(X2)</p>
<p/>
</div>
<div class="page"><p/>
<p>498 CHAPTER 15. PROBABILITY AND MOMENT APPROXIMATIONS
</p>
<p>But X 2 rv xi so that Ex[X2 ] = 1 and var(X2 ) = 2 (see Section 10.5.6 and Table
11.1 for a x~ = f(N/2, 1/2) PDF) and therefore
</p>
<p>I:f--l X?- N -+ N(O 1).
V2N '
</p>
<p>Noting that for finite N this result can be viewed as an approximation, we can use
</p>
<p>the approximate result
N
</p>
<p>YN = LX? rv N(N,2N)
i=l
</p>
<p>in making probability calculations. The error in the approximation is shown in
</p>
<p>Figure 15.9, where the approximate PDF (shown as the solid curve) of YN, which
</p>
<p>is a N(N, 2N), is compared to the exact PDF, which is a x~ (shown as the dashed
</p>
<p>curve). It is seen that the approximation becomes better as N increases.
</p>
<p>0.1 ,---:--~--~--~--_____, 0.05,----~--~--~--_____,
</p>
<p>806040
X
</p>
<p>20
</p>
<p>-'I
. .. J
</p>
<p>I
I
</p>
<p>.... . . .. . . .: /
/
t
</p>
<p>0.04 .
</p>
<p>~
0 0.03
</p>
<p>0...
</p>
<p>0.02
</p>
<p>0.01
</p>
<p>0
80 06040
</p>
<p>X
</p>
<p>20
</p>
<p>(a) N = 10 (b) N = 40
</p>
<p>Figure 15.9: x~ PDF (dashed curve) and Gaussian PDF approximation of
</p>
<p>N(N,2N) (solid curve).
</p>
<p>&lt;&gt;
For the previous example it can be shown directly that the characteristic function of
</p>
<p>the standardized x ~ random variable converges to that of the standardized Gaussian
</p>
<p>random variable, and hence so do their PDFs by the continuity theorem (see Section
</p>
<p>11.7 for third property of characteristic function and also Problem 15.17). We next
</p>
<p>give an example that quantifies the numerical error of the central limit theorem
</p>
<p>approximation.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.5. CENTRAL LIMIT THEOREM 499
</p>
<p>Example 15.4 - Central limit theorem and computation of probabilities-
</p>
<p>numerical results
</p>
<p>Recall that the Erlang PDF is the PDF of the sum of N lID exponential random
</p>
<p>variables, where Xi ,....., exp(&gt;.) for i = 1,2, ... , N (see Section 10.5.6). Hence, letting
YN = L:~l Xi the Erlang PDF is
</p>
<p>y;:::o
</p>
<p>y &lt; O.
(15.8)
</p>
<p>Its mean is N / &gt;. and its variance is N / &gt;.2 since the mean and variance of an exp (&gt;')
random variable is 1/&gt;. and 1/&gt;.2, respectively. If we wish to determine P[YN &gt; 10],
then from (15.8) we can find the exact value for&gt;. = 1 as
</p>
<p>] F" 1 N-lP[YN&gt; 10 = 110 (N _ I)! Y exp(-y)dy.
</p>
<p>But using
</p>
<p>J
n k
</p>
<p>yn exp( -y)dy = -n! exp( -y) 2:: ~!
k=O
</p>
<p>[Gradshteyn and Ryzhik 1994], we have
</p>
<p>P[YN &gt; 10] = (N ~ I)! [-(N - I)! exp( _y) ~ ~ ~ 00]
k=O 10
</p>
<p>N-l 10k
</p>
<p>= exp(-lO) 2:: kf'
k=O
</p>
<p>A central limit theorem approximation would yield YN ,....., N(N/&gt;' ,N/ &gt;.2) = N(N,N)
so that
</p>
<p>where the P denotes the approximation of P. The true and approximate values
for this probability are shown in Figure 15.10. The probability values have been
</p>
<p>connected by straight lines for easier viewing.
</p>
<p>o
For the sum of lID discrete random variables the situation changes markedly. Con-
</p>
<p>sider the sum of N lID Ber(p) random variables. We already know that the PMF
</p>
<p>is binomial so that
</p>
<p>k = 0,1 , ... , N - 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>500 CHAPTER 15. PROBABILITY AND MOMENT APPROXIMATIONS
</p>
<p>30252015
N
</p>
<p>. : .. .
</p>
<p>105
</p>
<p>0.3 .
</p>
<p>0.2
</p>
<p>0.1
</p>
<p>OL--.-L..---'-----'-----L...---'---------'
o
</p>
<p>0.4 .
</p>
<p>0.8
</p>
<p>0.7 .
</p>
<p>;&gt;, 0.6 ....
</p>
<p>~
~ 0.5
</p>
<p>..c
o...
p..
</p>
<p>0.9 ...
</p>
<p>Figure 15.10: Exact and approximate calculation of probability that YN &gt; 10 for YN
an Erlang PDF. Exact value shown as dashed cur ve and Gaussian approximation
</p>
<p>as solid cur ve.
</p>
<p>Hence, this example will allow us to compare t he true P MF against any approxima-
</p>
<p>tio n . For reasons already explained we need to consider the P MF of the standardized
</p>
<p>sum or
SN -E[SN]
</p>
<p>v var(SN)
</p>
<p>SN - Np
</p>
<p>VN p(l - p)
The P MF of t he standardized binomial random variable P MF with p = 1/2 is shown
</p>
<p>in Figure 15.11 for various values on N. Note that it does not converge to any given
</p>
<p>0l..._1lIIIlU
- 5
</p>
<p>0.1 .
</p>
<p>0.3.--- - - ,.----- - -----,
</p>
<p>0.05 ..
</p>
<p>0.25 ..
</p>
<p>~ 0.2 "
</p>
<p>p..0.15 . . ..
</p>
<p>0.1 .
</p>
<p>0.3r-- - --,---------,
</p>
<p>0.25 .
</p>
<p>~~ 0.2 .
</p>
<p>p..0.15 ..
</p>
<p>o
X
</p>
<p>3
</p>
<p>5 .. .... ..... ....
</p>
<p>2 ..
</p>
<p>5 ..
</p>
<p>1
</p>
<p>5 .r
.. .. .. ..
</p>
<p>i..
0
-5
</p>
<p>o.
</p>
<p>o.
</p>
<p>0.0
</p>
<p>0.2
</p>
<p>(a) N = 10 (b) N = 30 (c) N = 100
</p>
<p>Fi gure 15.11: P MF for standardized binomial random variable with p = 1/2.
</p>
<p>P MF, alt hough the "envelope" , whose amplit ude decreases as N increases, appears</p>
<p/>
</div>
<div class="page"><p/>
<p>15.5. CENTRAL LIMIT THEOREM 501
</p>
<p>to be Gaussian. The lack of convergence is because the sample space or values that
</p>
<p>the standardized random variable can take on changes with N . The possible values
</p>
<p>k = 0,1 , ... , N
k - N p k - N/2
</p>
<p>Xk = JNp(1 - p) = IN/4
</p>
<p>which become more dense as N increases. However , what does converge is the CDF
</p>
<p>as shown in Figure 15.12. Now as N --+ 00 we can assert that the CDF converges,
</p>
<p>are
</p>
<p>12,-------,----- -, 1.2,--------,------ - ----, 1.2,---- - - -,--------,
</p>
<p>1 &middot;
</p>
<p>Ii&lt; 0 8 ..
Q'
0 0.6 .. . .
</p>
<p>0.4 . . . .
</p>
<p>0.2 ..
</p>
<p>1 &middot;
</p>
<p>~ 08
</p>
<p>. 0 0.6 "
</p>
<p>0.4 . . . .
</p>
<p>0.2 .. ..
</p>
<p>1 &middot;
</p>
<p>~ 0 8
. . 0 0.6 "
</p>
<p>0.4 . .
</p>
<p>0.2 ..
</p>
<p>OL....--..........- --'--- -- ~
- 5
</p>
<p>~5 '-- - "'""""- - --'-- ---~ ~ 5L....--~- --'---------'
</p>
<p>(a) N = 10 (b) N = 30 (c) N = 100
</p>
<p>Figure 15.12 : CDF for standardized binomial random vari able with p = 1/2.
</p>
<p>and fur thermore it converges to the CDF of a N (O, 1) random variable. Hence, the
</p>
<p>cent ral limit t heorem for discrete random variables is stated in terms of its CDF. It
</p>
<p>says that as N --+ 00
</p>
<p>[
SN - E[SN] ] j X 1 (1 2)P ~ x --+ ;;=;-= exp - -t dt = ll&gt;(x)
J var(SN) - ooy211" 2
</p>
<p>and is also known as the DeMoivre-Laplace theorem. We summarize the central
</p>
<p>limit theorem for discret e random variables next .
</p>
<p>Theorem 15.5.2 (Central limit theorem for discrete random variables)
</p>
<p>If Xl , X 2 , .. . , X N are lID discrete random variables, each with mean Ex[X] and
</p>
<p>variance var (X ), and SN = L: ~ l X i, then as N --+ 00
</p>
<p>[
SN - E [SN] ] I. 1 (1 2)P ~ x --+ ;;=;-= exp --t dt = ll&gt;(x)
Jvar(SN) - 00 y 211" 2
</p>
<p>(15.9)
</p>
<p>An example follows.
</p>
<p>Example 15.5 - Computation of binomial probability
</p>
<p>Assume that YN ,...., bin(N, 1/2), which may be viewed as the PMF for the number of
</p>
<p>heads obtained in N fair coin tosses , and consider the probability P [k l ~ YN ~ k2 ].</p>
<p/>
</div>
<div class="page"><p/>
<p>(15.10)
</p>
<p>502 CHAPTER 15. PROBABILITY AND MOMENT APPROXIMATIONS
</p>
<p>Then the exact probability is
</p>
<p>P[k 1 ~ YN ~ k2] = I: (~) (~) N
k=kl
</p>
<p>A central limit theorem approximation yields
</p>
<p>P [ k l ~ Y N ~ k 2 ] = p[k1-N/2&lt;YN-N/2&lt;k2-N/2]
IN/4 - IN/4 - IN/4
</p>
<p>~ &lt;p (k2 - N/2) _ &lt;p (k1 - N/2) (from (10.25))
IN/4 IN/4
</p>
<p>sInce
</p>
<p>P[490 ~ YN ~ 510]
</p>
<p>(15.11)
</p>
<p>is the standardized random variable for p = 1/2. For example, if we wish to compute
</p>
<p>the probability of between 490 and 510 heads out of N = 1000 tosses, then
</p>
<p>~ &lt;p (510 - 500) _ &lt;p (490 - 500)
)250 )250
</p>
<p>1 - Q ( )~~O) - ( 1 - Q ( ;21~0) )
</p>
<p>1 - 2Q ()~~O) = 0.4729.
</p>
<p>The exact value, however, is from (15.10)
</p>
<p>P[490 ~ YN ~ 510] = ~ (~) (~) N = 0.4933
k=490
</p>
<p>(see Problem 15.24 on how this was computed). A slightly better approximation
</p>
<p>using the central limit theorem can be obtained by replacing P[490 ~ Y ~ 510] with
</p>
<p>P[489.5 ~ Y ~ 510.5], which will more closely approximate the discrete random
</p>
<p>variable CDF by the continuous Gaussian CDF. This is because the binomial CDF
</p>
<p>has jumps at the integers as can be seen by referring to Figure 15.12. By taking a
</p>
<p>slighter larger interval to be used with the Gaussian approximation, the area under
</p>
<p>the Gaussian CDF more closely approximates these jumps at the endpoints of the
</p>
<p>interval. With this approximation we have
</p>
<p>P[489.5 &lt; Y &lt; 510.5] ~ Q (489.5 - 500) _ Q (510.5 - 500)
- - )250 )250
</p>
<p>= 0.4934
</p>
<p>which is quite close to the true value!</p>
<p/>
</div>
<div class="page"><p/>
<p>15.6. REAL-WORLD EXAMPLE - OPINION POLLING
</p>
<p>15.6 Real-World Example - Opinion Polling
</p>
<p>503
</p>
<p>(15.12)
</p>
<p>A frequent news topic of interest is the opinion of people on a major issue. For
</p>
<p>example, during the year of a presidential election in the United States, we hear
</p>
<p>almost on a daily basis the percentage of people who would vote for candidate A,
</p>
<p>with the remaining percentage voting for candidate B. It may be reported that 75%
</p>
<p>of the population would vote for candidate A and 25% would vote for candidate
</p>
<p>B. Upon reflection, it does not seem reasonable that a news organization would
</p>
<p>contact the entire population of the United States, almost 294,000,000 people, to
</p>
<p>determine their voter preferences. And indeed it is unreasonable! A more typical
</p>
<p>number of people contacted is only about 1000. How then can the news organization
</p>
<p>report that 75% of the population would vote for candidate A? The answer lies in
</p>
<p>the polling error - the results are actually stated as 75% with a margin of error
</p>
<p>of &plusmn;3%. Hence , it is not claimed that exactly 75% of the population would vote
</p>
<p>for candidate A, but between 72% and 78% would vote for candidate A. Even so,
</p>
<p>this seems like a lot of information to be gleaned from a very small sample of the
</p>
<p>population.
</p>
<p>An analogous problem may help to unravel the mystery. Let's say we have a
</p>
<p>coin with an unknown probability of heads p. We wish to estimate p by tossing the
</p>
<p>coin N times. As we have already discussed, the law of large numbers asserts that
</p>
<p>we can determine p without error if we toss the coin an infinite number of times
</p>
<p>and use as our est imate the relative frequency of heads. However, in practice we are
</p>
<p>limited to only N coin tosses. How much will our estimate be in error? Or more
</p>
<p>precisely, how much can the true value deviate from our estimate? We know that
</p>
<p>the number of heads observed in N independent coin tosses can be anywhere from
</p>
<p>o to N . Hence, our estimate of p for N = 1000 can take on the possible values
A 1 2
p = 0, 1000' 1000' ... , l.
</p>
<p>Of course, most of these estimates are not very probable. The probability that the
</p>
<p>estimate will take on these values is
</p>
<p>P[p = k/1000] = (10;0) pk(l_ p)lOOO-k k = 0,1, ... , 1000
</p>
<p>which is shown in Figure 15.13 for p = 0.75. The probabilities for p outside the
interval shown are approximately zero . Note that the maximum probability is for
</p>
<p>the true value p = 0.75. To assess the error in the estimate of p we can determine the
</p>
<p>interval over which say 95% of the p's will lie. The interval is chosen to be centered
about p = 0.75. In Figure 15.13 it is shown as the interval contained within the
dashed vertical lines and is found by solving
</p>
<p>t (10;0) (0.75)k (0.25) lOOO-k = 0.95
k=kl' Jv
</p>
<p>P[k heads]</p>
<p/>
</div>
<div class="page"><p/>
<p>504 CHAPTER 15. PROBABILITY AND MOMENT APPROXIMATIONS
</p>
<p>yielding kl = 724 and k2 = 776, which results in PI = 0.724 and P2 = 0.776. Hence,
</p>
<p>0.03,-------.-----,-----,-------,----,
</p>
<p>0.01
</p>
<p>0.005
</p>
<p>0.80.78
</p>
<p>I
</p>
<p>1
</p>
<p>. .. . .1
</p>
<p>1
</p>
<p>1
</p>
<p>. .1
</p>
<p>1
</p>
<p>I
</p>
<p>0.760.74
</p>
<p>0.02 .
</p>
<p>O !IIIlU
</p>
<p>0.7 0.72
</p>
<p>0.025 .
</p>
<p>~ : I
:::E : I
0..0.015 . . .. . . . .: .1 . .
</p>
<p>: 1
: 1
</p>
<p>.. . . . .. . : .1 ..
</p>
<p>: 1
: I
</p>
<p>Figure 15.13: PMF for estimate of p for a binomial random variable. Also, shown
</p>
<p>as the dashed vertical lines are the boundaries of the interval within which 95% of
</p>
<p>the estimates will lie.
</p>
<p>for p = 0.75 we see that 95% of the time (if we kept repeating the 1000 coin toss
</p>
<p>experiment), the value of P would be in the interval [0.724, 0.776]. We can assert
that we are 95% confident that for p = 0.75
</p>
<p>p - 0.026 ~ P ~ p + 0.026
</p>
<p>or
</p>
<p>-p + 0.026 ~ -P ~ -p - 0.026
</p>
<p>or finally
</p>
<p>P- 0.026 ~ p ~ P+ 0.026.
</p>
<p>The interval [p-0.026,p+0 .026] is called the 95% confidence interval. It is a random
</p>
<p>interval that covers the true value of p = 0.75 for 95% of the time. As an example
</p>
<p>a MATLAB simulation is shown in Figure 15.14. For each of 50 trials the estimate
</p>
<p>of p is shown by the dot while the confidence interval is indicated by a vertical line.
</p>
<p>Note that only 3 of the intervals fail to cover the true value of p = 0.75. With 50
</p>
<p>trials and a probability of 0.95 we expect 2.5 intervals not to cover the true value.
</p>
<p>Instead of having to compute k l and k2 using (15.12), it is easier in practice to
</p>
<p>use the central limit theorem. Since P= 2:~1 (Xi/N), with Xi '" Ber(p), is a sum
of IID random variables we can assert from Theorem 15.5.2 that
</p>
<p>P [-b ~ ~ ~ b] ~ &lt;I&gt;(b) - &lt;I&gt; (-b).
var(p)</p>
<p/>
</div>
<div class="page"><p/>
<p>15.6. REAL-WORLD EXAMPLE - OPINION POLLING 505
</p>
<p>0.9r------,-----,------,---.,------n
</p>
<p>0.85
</p>
<p>0.65 &middot; .
</p>
<p>504010
</p>
<p>0.6 L-__--'--__----'-__--'- '--__--'-'
</p>
<p>o
</p>
<p>Figure 15.14: 95% confidence interval for estimate of p = 0.75 for a binomial random
variable. The estimates are shown as dots.
</p>
<p>Noting that Xi '" Ber(p), E[P] = E[L:f::l Xi/N] = Np/N = p and var(p) =
var(L:f::l Xi/N) = Np(l - p)/N2 = p(l - p)/N, we have
</p>
<p>P [-b &lt; P- p &lt; b] ~ &lt;I&gt;(b) - &lt;I&gt; (-b).
- Vp(l - p)/N -
</p>
<p>For a 95% confidence interval or &lt;I&gt;(b) - &lt;I&gt; (-b) = 0.95, we have b= 1.96, as may be
easily verified . Hence, we can use the approximation
</p>
<p>-1.96 &lt; P- p &lt; 1.96
- Vp(l- p)/N -
</p>
<p>which after the same manipulation as before yields the confidence interval
</p>
<p>p - 1.96JP(1;; p) ~ p ~ p+ 1.96JP(l;; p). (15.13)
</p>
<p>The only difficulty in applying this result is that we don't know the value of p, which
</p>
<p>arose from the variance of p. To circumvent this there are two approaches. We can
replace p by its estimate to yield the confidence interval
</p>
<p>p- 1.96Jp(1;; p) ~ p ~ p+ 1.96Jp(1;; p). (15.14)
</p>
<p>A more conservative approach is to note that p(l - p) is maximum for p = 1/2.
Using this number yields a larger interval than necessary. However, it allows us to</p>
<p/>
</div>
<div class="page"><p/>
<p>506 CHAPTER 15. PROBABILITY AND MOMENT APPROXIMATIONS
</p>
<p>determine before the experiment is performed and the value of p revealed, the length
of the confidence interval. This is useful in planning how large N must be in order
</p>
<p>to have a confidence interval not exceeding a given length (see Problem 15.25). If
</p>
<p>we adopt the latter approach then the confidence interval becomes
</p>
<p>p&plusmn; 1.96VP(1;; p) = p&plusmn; 1.96V1~4 ~ P&plusmn; ~.
In summary, if we toss a coin with a probability p of heads N times, then the interval
</p>
<p>[p - l/VN,p + l/VN] will contain the true value of p more than 95% of the time.
It is said that the error in our estimate of pis &plusmn;l/VN.
</p>
<p>Finally, returning to our polling problem we ask N people if they will vote for
</p>
<p>candidate A. The probability that a person chosen at random will say "yes" is p,
</p>
<p>because the proportion of people in the population who will vote for candidate A
</p>
<p>is p. We liken this to tossing a single coin and noting if it comes up a head (vote
</p>
<p>"yes") or a tail (vote "no"). Then we continue to record the responses of N people
</p>
<p>(continue to toss the coin N times). Assume, for example, 750 people out of 1000 say
</p>
<p>"yes". Then p= 750/1000 = 0.75 and the margin of error is &plusmn;l/VN ~ 3%. Hence,
we report the results as 75% of the population would vote for candidate A with a
</p>
<p>margin of error of 3%. (Probabilistically speaking, if we continue to poll groups of
</p>
<p>1000 voters, estimating p for each group, then about 95 out of 100 groups would
</p>
<p>cover the true value of lOOp % by their estimated interval [lOOp - 3, lOOp + 3]%.)
We needn't poll 294,000,000 people since we assume that the percentage of the 1000
</p>
<p>people polled who would vote for candidate A is representative of the percentage of the
</p>
<p>entire population. Is this true? Certainly not if the 1000 people were all relatives of
</p>
<p>candidate A. Pollsters make their living by ensuring that their sample (1000 people
</p>
<p>polled) is a representative cross-section of the entire United States population.
</p>
<p>References
</p>
<p>Brockwell, P.J., R.A. Davis, Time Series: Theory and Methods, Springer-Verlag,
</p>
<p>New York, 1987.
</p>
<p>Gradshteyn, 1.S., 1.M. Ryzhik, Tables of Integrals , Series, and Products, Fifth Ed.,
</p>
<p>Academic Press, New York, 1994.
</p>
<p>Grimmett, G., D. Stirzaker, Probability and Random Processes, Third Ed., Oxford
</p>
<p>University Press, New York, 2001.
</p>
<p>Harvey, A.C., Forecasting, Structural Time Series Models and the Kalman Filter,
</p>
<p>Cambridge University Press, New York, 1989.
</p>
<p>Middleton, D., An Introduction to Statistical Communication Theory, McGraw-
</p>
<p>Hill, New York, 1960.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 507
</p>
<p>Rao, C.R., Linear Statistical Inference and Its Applications, John Wiley &amp; Sons,
</p>
<p>New York, 1973.
</p>
<p>Reif, F. , Fundamentals of Statistical and Thermal Physics, McGraw-Hill, New
</p>
<p>York, 1965.
</p>
<p>Urick , R.J. , Principles of Underwater Sound, McGraw-Hill, New York, 1975.
</p>
<p>Weisburg, H.F. , J.A. Krosnick, B.D. Bowen, An Introduction to Survey Research,
</p>
<p>Polling and Data Analysis, Sage Pubs., CA, 1996.
</p>
<p>Problems
</p>
<p>15.1 (f) For the PMF given by (15.2) plot the CDF for N = 10, N = 30, and
N = 100. What function does the CDF appear to converge to?
</p>
<p>15.2 (c) If Xi '" N(l, 1) for i = 1,2 ... ,N are lID random variables, plot a real-
ization of the sample mean random variable versus N. Should the realization
</p>
<p>converge and if so to what value?
</p>
<p>15.3 (w,c) Let Xli'" U(O , 2) for i = 1,2 . . . , N be lID random variables and let
X 2i '" N(l, 4) for i = 1,2 ... ,N be another set of lID random variables. If the
sample mean random variable is formed for each set of lID random variables,
</p>
<p>which one should converge faster? Implement a computer simulation to check
</p>
<p>your results.
</p>
<p>15.4 (-:.:.,) (w) Consider the weighted sum of N lID random variables YN = 2:f::1 &laquo;.x;
If Ex[XJ = 0 and var(X) = 1, under what conditions will the sum converge
to a number? Can you give an example, other than cq = liN, of a set of ai's
</p>
<p>which will result in convergence?
</p>
<p>15.5(w) A random walk is defined as XN = XN-I + UN for N = 2,3, ... and
Xl = UI , where the Ui'S are lID random variables with P[Ui = -lJ = P[Ui =
+lJ = 1/2. Will X N converge to anything as N --+ oo?
</p>
<p>15.6 (w) To estimate the second moment of a random variable it is proposed to
use (liN) 2:f::1 Xl. Under what conditions will the estimate converge to the
true value?
</p>
<p>15.7 C:..:J (w) If Xi for i = 1,2 ... ,N are lID random variables, will the random
variable (l/VN) 2 : ~ 1 Xi converge to a number?
</p>
<p>15.8 (t,c) In this problem we attempt to demonstrate that convergence in prob-
</p>
<p>ability is different than standard convergence of a sequence of real numbers.</p>
<p/>
</div>
<div class="page"><p/>
<p>508 CHAPTER 15. PROBABILITY AND MOMENT APPROXIMATIONS
</p>
<p>Consider the sequence of random variables
</p>
<p>YN =
XN&radic;
N
</p>
<p>+ u
</p>
<p>(
</p>
<p>XN&radic;
N
</p>
<p>&minus; 0.1
)
</p>
<p>where the XN &rsquo;s are IID, each with PDF XN &sim; N (0, 1) and u(x) is the unit
step function. Prove that P [|YN | &gt; ] &rarr; 0 as N &rarr; &infin; by using the law of
total probability as
</p>
<p>P [|YN | &gt; ] = P [|YN | &gt; |XN/&radic;N &gt; 0.1]P [XN/&radic;N &gt; 0.1]
+ P [|YN | &gt; |XN/&radic;N &le; 0.1]P [XN/&radic;N &le; 0.1].
</p>
<p>This says that YN &rarr; 0 in probability. Next simulate this sequence on the
computer for N = 1, 2, . . . , 200 to generate 4 realizations of {Y1, Y2, . . . , Y200}.
Examine whether for a given N all realizations lie within the &ldquo;convergence
band&rdquo; of [&minus;0.2, 0.2]. Next generate an additional 6 realizations and overlay all
10 realizations. What can you say about the convergence of any one realiza-
tion?
</p>
<p>15.9 (w) There are 1000 resistors in a bin labeled 10 ohms. Due to manufacturing
tolerances, however, the resistance of the resistors are somewhat different.
Assume that the resistance can be modeled as a random variable with a mean
of 10 ohms and a variance of 2 ohms 2. If 100 resistors are chosen from the
bin and connected in series (so the resistances add together), what is the
approximate probability that the total resistance will exceed 1030 ohms?
</p>
<p>15.10 (w) Consider a sequence of random variablesX1,X1,X2,X2,X3,X3, . . ., where
X1,X2,X3 . . . are IID random variables. Does the law of large numbers hold?
How about the central limit theorem?
</p>
<p>15.11 (w) Consider an Erlang random variable with parameter N . If N increases,
does the PDF become Gaussian? Hint: Compare the characteristic functions
of the exponential random variable and the (N,) random variable in Table
11.1.
</p>
<p>15.12 (f) Find the approximate PDF of Y =
&sum;100
</p>
<p>i=1 X
2
i , if the Xi&rsquo;s are IID with
</p>
<p>Xi &sim; N (&minus;4, 8).
</p>
<p>15.13 (

</p>
<p>. . ) (f) Find the approximate PDF of Y =
&sum;1000
</p>
<p>i=1 Xi, if the Xi&rsquo;s are IID
with Xi &sim; U(1, 3).
</p>
<p>15.14 (f) Find the approximate probability that Y =
&sum;10
</p>
<p>i=1 Xi will exceed 7, if the
Xi&rsquo;s are IID with the PDF
</p>
<p>pX(x) =
</p>
<p>{
</p>
<p>2x 0 &lt; x &lt; 1
0 otherwise.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 509
</p>
<p>15.15 (c) Modify the computer program clt_demo.m listed in Appendix 15A to
</p>
<p>display the repeated convolution of the PDF
</p>
<p>px(x) = { O~ sin(7rx) 0 &lt; x &lt; 1
otherwise.
</p>
<p>and examine the results.
</p>
<p>15.16 (c) Use the computer program clt_demo.m listed in Appendix 15A to display
</p>
<p>the repeated convolution of the PDF U(O, 1). Next modify the program to
</p>
<p>display the repeated convolution of the PDF
</p>
<p>{
12 - 4xl 0 &lt; x &lt; 1
</p>
<p>px(x) = 0 otherwise.
</p>
<p>Which PDF results in a faster convergence to a Gaussian PDF and why?
</p>
<p>15.17 (t) In this problem we prove that the PDF of a standardized X7v random
variable converges to a Gaussian PDF as N -+ 00. To do so let YN ,...., X7v and
</p>
<p>show that the characteristic function is
</p>
<p>1
&lt;/JYN(W) = (1- 2jw)N/2
</p>
<p>by using Table 11.1. Next define the standardized random variable
</p>
<p>ZN = YN - E[YN]
Jvar(YN)
</p>
<p>and note that the mean and variance ofax7v random variable is Nand 2N,
</p>
<p>respectively. Show the characteristic function of ZN is
</p>
<p>exp( -jw..jNfi)
</p>
<p>&lt;/JZN(w) = (1 _ jwJ2/N)N/2'
</p>
<p>Finally, take the natural logarithm of &lt;/JzN(w) and note that for a complex
</p>
<p>variable x with Ixl &laquo; 1, we have that lnf l - x) ~ -x - x2 /2. You should be
able to show that as N -+ 00, In&lt;/JzN(w) -+ -w2/2.
</p>
<p>15.18 (w) A particle undergoes collisions with other particles. Each collision causes
its horizontal velocity to change according to a N(O,0.1) em/sec random vari-
</p>
<p>able. After 100 independent collisions what is the probability that the parti-
</p>
<p>cle's velocity will exceed 5 em/sec if it is initially at rest? Is this result exact
</p>
<p>or approximate?
</p>
<p>15.19 C:..:.-) (f) The sample mean random variable of N IID random variables with
Xi ,...., U(O , 1) will converge to 1/2. How many random variables need to be
</p>
<p>averaged before we can assert that the approximate probability of an error of
</p>
<p>not more than 0.01 in magnitude is 0.99?</p>
<p/>
</div>
<div class="page"><p/>
<p>510 CHAPTER 15. PROBABILITY AND MOMENT APPROXIMATIONS
</p>
<p>15.20 t:...:J (w) An orange grove produces oranges whose weights are uniformly
distributed between 3 and 'i oz&laquo;. If a truck can hold 4000 lbs. of oranges, what
</p>
<p>is the approximate probability that it can carry 15,000 oranges?
</p>
<p>15.21 (w) A sleeping pill is effective for 75% of the population. If in a hospital 160
</p>
<p>patients are given a sleeping pill, what is the approximate probability that 125
</p>
<p>or more of them will sleep better?
</p>
<p>15.22 (...:..:.-) (w) For which PDF will a sum of IID random variables when added
together have a PDF that converges to a Gaussian PDF the fastest?
</p>
<p>15.23 ( . . ~ . : . - ) (w) A coin is tossed 1000 times, producing 750 heads. Is this a fair
coin?
</p>
<p>15.24 (f,c) To compute the probability of (15.11) we can use the following approach
</p>
<p>to compute each term in the summation. Each term can be written as
</p>
<p>[k] = (N) ( ~ ) N = N(N - 1)&middot; &middot;&middot; (N - k + 1) (~) N
PYN k 2 1(2)(3)&middot;&middot;&middot; (k) 2
</p>
<p>Taking the natural logarithm produces
</p>
<p>N k
</p>
<p>InpYN[k] = L In(i) - Lln(i) - Nln(2)
i=N-k+l i=l
</p>
<p>which is easily done on a computer. Next, exponentiate to find PYN [k] and add
</p>
<p>each of the terms together to finally implement the summation. Carry this
</p>
<p>out to verify the result given in (15.11). What happens if you try to compute
</p>
<p>each term directly?
</p>
<p>15.25 (f) In a poll of candidate preferences for two candidates, we wish to report
that the margin of error is only &plusmn;1%. What is the maximum number of people
whom we will need to poll?
</p>
<p>15.26 t:...:... ) (w) A clinical trial is performed to determine if a particular drug is
effective . A group of 100 people is split into two equal groups at random. The
</p>
<p>drug is administered to group 1 while group 2 is given a placebo. As a result
</p>
<p>of the study, 40 people in group 1 show a marked improvement while only
</p>
<p>30 people in group 2 do so. Is the drug effective? Hint: Find the confidence
</p>
<p>intervals (using (15.14&raquo; for the percentage of the people in each group who
</p>
<p>show an improvement.</p>
<p/>
</div>
<div class="page"><p/>
<p>This program demonstrates the central limit theorem. It determines
</p>
<p>the PDF for the sum S_N of N IID random variables . Each marginal PDF
</p>
<p>is assumed to be nonzero over the interval (0,1). The repeated
</p>
<p>convolution integral is implemented using a discrete convolution. The
</p>
<p>plots of the PDF of S_N as N increases are shown successively
</p>
<p>(press carriage return for next plot).
</p>
<p>Appendix 15A
</p>
<p>MATLAB Program to Compute
</p>
<p>Repeated Convolution of PDFs
</p>
<p>%
%
</p>
<p>%
%
</p>
<p>%
</p>
<p>%
%
%clt_demo .m
clear all
</p>
<p>delu=0.005;
</p>
<p>u=[O:delu:l-delu]'; %p_X defined on interval [0,1)
p_X=ones(length(u),l); %try p_X=abs(2-4*u) for really strange PDF
x=[u;u+l]; % increase abcissa values since repeated
</p>
<p>%convolution increases nonzero width of output
p_S=zeros(length(x),l);
</p>
<p>N=12; %number of random variables summed
for j=l:length(x) %start discrete convolution approximation
</p>
<p>%to continuous convolution
for i=l:length(u)
</p>
<p>if j-i&gt;O&amp;j-i&lt;=length(p_X)
</p>
<p>p_S(j)=p_S(j)+p_X( i)*p_X(j-i)*delu;
</p>
<p>end
</p>
<p>end
</p>
<p>end
</p>
<p>plot(x,p_S) %plot results for N=2
grid</p>
<p/>
</div>
<div class="page"><p/>
<p>512 CHAPTER 15. PROBABILITY AND MOMENT APPROXIMATIONS
</p>
<p>axis([O N 0 1]) ;. set axes lengths for plotting
xlabel( , x ")
</p>
<p>ylabel( 'p_S')
</p>
<p>title('PDF for S_N')
</p>
<p>text(0.75*N,0.85,'N = 2') ;. label plot with the
</p>
<p>;. number of convolutions
</p>
<p>for n=3:N
</p>
<p>pause
x=[xju+n-1] j ;. increase abcissa values since
</p>
<p>;. repeated convolution increases
</p>
<p>;. nonzero width of output
</p>
<p>p_S=[p_Sjzeros(length(u),1)] j
</p>
<p>g=zeros(length(p_S),1)j
</p>
<p>for j=1:length(x) ;. start discrete convolution
</p>
<p>for i=1:length(u)
</p>
<p>if j-i&gt;O
</p>
<p>g(j,1)=g(j,1)+p_X(i)*p_S(j -i)*deluj
</p>
<p>end
</p>
<p>end
</p>
<p>end
</p>
<p>p_S=gj ;. plot results for N=3,4, ... ,12
</p>
<p>plot(x,p_S)
</p>
<p>grid
</p>
<p>axis ([0 N 0 1])
</p>
<p>xlabel ( , x ' )
</p>
<p>ylabel ('p_S')
</p>
<p>title('PDF for S_N')
</p>
<p>text(0.75*N,0.85,['N = , num2str(n)])
</p>
<p>end</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 15B
</p>
<p>Proof of Central Limit Theorem
</p>
<p>In this appendix we prove the central limit theorem for continuous random variables.
</p>
<p>Consider the characteristic function of the standardized continuous random variable
</p>
<p>ZN = SN - NEx[X]
</p>
<p>JNvar(X)
</p>
<p>where SN =: I:~1 Xi and the Xi 'S are IID. By definition of ZN the characteristic
function becomes
</p>
<p>EZN [exp(jwZN)]
</p>
<p>Ex [exp (jwl:~~:ar~:;[XI)]
</p>
<p>Ex [IT exp (jW Xi - Ex [X] )]
i=l JNvar(X)
</p>
<p>rr
N
</p>
<p>E [ (. Xi - EX[X])]x. exp JW---;========;:~
i=l' JNvar(X)
</p>
<p>[
Ex [exp (jW X - Ex [X] )]] N
</p>
<p>JNvar(X)
</p>
<p>(independence of Xi'S)
</p>
<p>(identically distributed Xi'S).
</p>
<p>(see Problem 5.22).
</p>
<p>But for a complex variable ~ we can write its exponential as a Taylor series yielding
</p>
<p>00 ~ k
</p>
<p>exp(e) = I: k!
k=O
</p>
<p>Thus,</p>
<p/>
</div>
<div class="page"><p/>
<p>514 CHAPTER 15. PROBABILITY AND MOMENT APPROXIMATIONS
</p>
<p>Ex [f (jw)k (X - EX[Xl)k]
k=O k! JNvar(X)
</p>
<p>= f (jw)kEx [(X - EX[Xl)k] (assume interchange valid)
k=O k! JNvar(X)
</p>
<p>= l+jwEx [ ~ - E x [ X l ] +-21(jw)2Ex [(~-EX[Xl)2] + Ex[R(X)l
Nvar(X) Nvar(X)
</p>
<p>where R(X) is the third-order and higher terms of the Taylor expansion. But
</p>
<p>and so
</p>
<p>E [X - Ex[Xl] =
x JNvar(X)
</p>
<p>E [(X - Ex[Xl) 2]
x JNvar(X)
</p>
<p>Ex[Xl - Ex [Xl = 0
</p>
<p>JNvar(X)
</p>
<p>Ex[(X - EX[X])2] 1
</p>
<p>Nvar(X) N
</p>
<p>&lt;/JZN(w) = [1 _;~ + Ex [R(X)]] N
The terms comprising R(X) are
</p>
<p>R(X) = (jW)3 Ex [(X - EX[Xl)3] + ...
3! JNvar(X)
</p>
<p>[( )
3]1 (jw)3 E X - Ex [X]
</p>
<p>N3/23! x Jvar(X) + ...
</p>
<p>(see Problem 5.15)
</p>
<p>which can be shown to be small, due to the division of the successive terms by
</p>
<p>N 3/2, N 2, ... , relative to the _w2/(2N) term. Hence as N -+ 00, they do not
contribute to &lt;/JZN (w) and therefore
</p>
<p>&lt;/JZN(W) -+ (1- ;~) N
-+ exp ( _~w2) = &lt;/Jz(w)
</p>
<p>where Z '" N(o,1). Since the characteristic function of ZN converges to the char-
acteristic function of Z, we have by the continuity theorem (see Section 11.7) that
</p>
<p>the PDF of ZN must converge to the PDF of Z. Therefore, we have finally that as
</p>
<p>N-+oo
</p>
<p>1 (1 2)PZN (z) -+ Pz(z) = .../2if exp -'2z .</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 16
</p>
<p>Basic Random Processes
</p>
<p>16.1 Introduction
</p>
<p>So far we have studied the probabilistic description of a finite number of random
</p>
<p>variables. This is useful for random phenomena that have definite beginning and
</p>
<p>end times. Many physical phenomena, however, are more appropriately modeled as
</p>
<p>ongoing in time. Such is the case for the annual summer rainfall in Rhode Island
</p>
<p>as shown in Figure 1.1 and repeated for convenience in Figure 16.1. This physical
</p>
<p>20 ,.........- - - -r-- - --r-- - ----,r--- - -,.-- - ---r&gt;
</p>
<p>200019801940 1960
Year
</p>
<p>1920
2
</p>
<p>1900
</p>
<p>4
</p>
<p>Average = 9.76 inches :
18 . . ; , , -: ; -:. . . . . .&middot; . . . . .&middot; . . . . .
16 .. : : : : : :
</p>
<p>&middot; . . . . .&middot; . . . . .. . . . .
" ,14 .. : .... .... .
</p>
<p>~ 12
-5
.s 10 Ftr."fIt-rl"-'f-II:t""it'1H
</p>
<p>8
</p>
<p>6
</p>
<p>Figure 16.1: Annual summer rainfall in Rhode Island from 1895 to 2002.
</p>
<p>process has been ongoing for all time and will undoubtedly continue into the future.
</p>
<p>It is only our limited ability to measure the rainfall over several lifetimes that has
</p>
<p>produced the data shown in Figure 16.1. It therefore seems more reasonable to
</p>
<p>attempt to study the probabilistic characteristics of the annual summer rainfall in</p>
<p/>
</div>
<div class="page"><p/>
<p>516 CHAPTER 16. BASIC RANDOM PROCESSES
</p>
<p>Rhode Island for all time. To do so let X[n] be a random variable that denotes
</p>
<p>the annual summer rainfall for year n. Then, we will be interested in the behav-
</p>
<p>ior of the infinite tuple of random variables (... , X [- l ], X [O], X [l ], ... ), where the
</p>
<p>corresponding year for n = 0 can be chosen for convenience (maybe according to
the Christian or Hebrew calendars, as examples). Note that we cannot employ our
</p>
<p>previous probabilistic methods directly since the number of random variables is not
</p>
<p>finite or N-dimensional.
</p>
<p>Given our interest in the annual summer rainfall, what types of questions are
</p>
<p>pertinent? A meterologist might wish to determine if the rainfall totals are increas-
</p>
<p>ing with time. Hence , he may question if the average rainfall is really constant. If it
</p>
<p>is not constant with time, then our estimate of the average, obtained by taking the
</p>
<p>sample mean of the values shown in Figure 16.1, is meaningless. As an example,
</p>
<p>we would also have obtained an average of 9.76 inches if the rainfall totals were in-
</p>
<p>creasing linearly with time, starting at 7.76 inches and ending at 11.76 inches. The
</p>
<p>meterologist might argue that due to global warming the rainfall totals should be
</p>
<p>increasing. We will return to this question in Section 16.8. Another question might
</p>
<p>be to assess the probability that the following year the rainfall will be 12 inches or
</p>
<p>more if we know the entire past history of rainfall totals. This is the problem of
</p>
<p>prediction, which is a fundamental problem in many scientific disciplines.
</p>
<p>A second example of a random process, which is of intense interest, is a man-
</p>
<p>made one: the Dow-Jones industrial average (DJIA) for stocks. At the end of each
</p>
<p>trading day the average of the prices of a representative group of stocks is computed
</p>
<p>to give an indication of the health of the U.S. stock market. Its usefulness is that
</p>
<p>this value also gives an indication of the overall health of the U.S. economy. Some
</p>
<p>recent weekly values are shown in Figure 16.2. The overall trend beginning at week
</p>
<p>10 is upward until about week 60, at which point it fluctuates up and down. Some
</p>
<p>questions of interest are whether the index will go back up again after week 92
</p>
<p>and to what degree is it possible to predict the movement of the stock market, of
</p>
<p>which the DJIA is an indicator. The financial industry and in fact the health of the
</p>
<p>U.S. economy depends in a large degree upon the answers to these questions! In the
</p>
<p>remaining chapters we will describe the theory and application of random processes.
</p>
<p>As always, the theory will serve as a foundation upon which we will be able to analyze
</p>
<p>random processes. In any practical situation, however, the ideal theoretical analysis
</p>
<p>must be tempered with the constraints and additional complexities of the real world.
</p>
<p>16.2 Summary
</p>
<p>A random process is defined in Section 16.3. Four different types of random pro-
</p>
<p>cesses are described in Section 16.4. They are classified according to whether they
</p>
<p>are defined for all time or only for uniformly spaced time samples, and also accord-
</p>
<p>ing to their possible values as being discrete or continuous. Figure 16.5 illustrates
</p>
<p>the various types. A stationary random process is one for which its probabilistic</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3. WHAT IS A RANDOM PROCESS?
</p>
<p>12000 rr---,----,-----.---.----.----,--,------,.------.-,
</p>
<p>11000 :.... .: . .
</p>
<p>10 20 30 40 50 60 70 80 90
Week
</p>
<p>517
</p>
<p>Figure 16.2: Dow-Jones industrial average at the end of each week from January 8,
</p>
<p>2003 to September 29, 2004 [DowJones.com 2004] .
</p>
<p>description does not change with the chosen time origin, which is expressed mathe-
</p>
<p>matically by (16.3) . An lID random process is stationary as shown in Example 16.3.
</p>
<p>The concept of a random process having stationary and independent increments is
</p>
<p>described in Section 16.5 with an illustration given in Example 16.5. Some more
</p>
<p>examples of random processes are given in Section 16.6. The most useful moments
</p>
<p>of a random process, the mean sequence and the covariance sequence, are defined
</p>
<p>by (16.5) and (16.7) , respectively. Finally, in Section 16.8 an application of the
</p>
<p>est imat ion of the mean sequence to predicting average rainfall totals is described.
</p>
<p>The least squares estimator of t he slope and intercept of a straight line is found
</p>
<p>using (16.9) and is commonly used in data analysis problems.
</p>
<p>16.3 What Is a Random Process?
</p>
<p>To define the concept of a random process we will begin by considering our usual
</p>
<p>example of a coin tossing experiment. Assume that at some start time we toss
</p>
<p>a coin and then repeat this subexperiment at one second intervals for all time.
</p>
<p>Letting n denote the time in seconds, we therefore generate successive outcomes
</p>
<p>at times n = 0,1 ,... . The experiment continues indefinitely. Since there are
</p>
<p>two possible outcomes for each coin toss and we will assume that the tosses are
</p>
<p>independent, we have an infinite sequence of Bernoulli trials. This is termed a
</p>
<p>Bernoulli random process and extends the finite Bernoulli set of random variables
</p>
<p>first introduced in Section 4.6.2, in which a finite number of trials were carried
</p>
<p>out. As usual, we let the probability of a head (X = 1) be p and the prob-</p>
<p/>
</div>
<div class="page"><p/>
<p>518 CHAPTER 16. BASIC RANDOM PROCESSES
</p>
<p>ability of a tail (X = 0) be 1 - p for each trial. With this setup, a random
process can be defined as a mapping from the original experimental sample space
</p>
<p>S = {(H,H,T, ),(H,T,H, . . .),(T,T,H, ), ... } to the numerical sample space
</p>
<p>Sx = {(I , 1,0, ), (1,0,1 , .. .) , (0,0,1 , ... ), }. Note that each simple event or el-
ement of S is an infinite sequence of H's and T 's which is then mapped into an
</p>
<p>infinite sequence of 1's and O's , which is the corresponding simple event in SX . One
</p>
<p>may picture a random process as being generated by the "random process gener-
</p>
<p>ator" shown in Figure 16.3. The random process is composed of the infinite (but
</p>
<p>..
r
</p>
<p>Random process
</p>
<p>generator
r
</p>
<p>+ PMF description
</p>
<p>(X [0], X[I], ... )
.....;..-...~
</p>
<p>Figure 16.3: A conceptual random process generator. The input is an infinite se-
</p>
<p>quence of random variables with their probabilistic description and the output is an
</p>
<p>infinite sequence of numbers.
</p>
<p>countable) "vector" of random variables (X[O] ,X[I] , .. .), each of which is a Bernoulli
random variable, and each outcome of the random process is given by the infinite
</p>
<p>sequence of numerical values (x[O] ,x[I] , .. .). As usual, uppercase letters are used for
</p>
<p>the random variables and lowercase letters for the values they take on . Some typical
</p>
<p>outcomes of the Bernoulli random process are shown in Figure 16.4. They were
</p>
<p>generated in MATLAB using x=floor(rand(31,1)+O.5) for each outcome. Each
</p>
<p>sequence in Figure 16.4 is called an outcome or by its synonyms of realization or
</p>
<p>sample sequence. We will prefer the use of the term "realizat ion" . Each realization
</p>
<p>is an infinite sequence of numbers. Hence , the random process is a mapping from S,
</p>
<p>which is a set of infinite sequential experimental outcomes, to Sx , which is a set of
infinite sequences of 1's and O's or realizations. The total number of realizations is
</p>
<p>not countable (see Problem 16.3) . The set of all realizations is sometimes referred
</p>
<p>to as the ensemble of realizations. Just as for the case of a single random variable,
</p>
<p>which is a mapping from S to SX and therefore is represented as the set function
</p>
<p>X(5) , a similar notation is used for random processes. Now, however, we will use
</p>
<p>X[n ,5] to represent the mapping from an element of S to a realization x[n]. In
</p>
<p>Figure 16.4 we see the result of the mapping for 5 = 51, which is X[n ,51] = xdn] ,
as well as others. It is important to note that if we fix n at n = 18, for example,
then X[18 ,5] is a random variable that has a Bernoulli PMF. Three of its outcomes
are shown highlighted in Figure 16.4 with dashed boxes. Hence, all the methods
</p>
<p>developed for a single random variable are applicable. Likewise, if we fix two sam-
</p>
<p>ples at n = 20 and n = 22, then X[20,5] and X[22, 5] becomes a bivariate random
vector. Again all our previous methods for two-dimensional random vectors apply.
</p>
<p>To summarize, a random process is defined to be an infinite sequence of random</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3. WHAT IS A RANDOM PROCESS? 519
</p>
<p>X[n ,sd = xdn]
2,.------.------,---,------l-+----.----,-----", ., ,, ,, ,
</p>
<p>" 0 JJ1..lJIIT.TIIUI.L.I.l.. I
. .
</p>
<p>30252015105
-1L.----'-----'-----"-----l-+---'------'---...L.....J
</p>
<p>o
2,.------.------,---,------&lt;--+----.----,-----"
</p>
<p>: X[n ,S2] = X2[n]
</p>
<p>~ :.1.Jl.lLl.lIL... .1].1.111]
. . . I ' . .
</p>
<p>,,,,
</p>
<p>30252015105
</p>
<p>-1 L.-__-'-__--'- -"-----&lt;--+---'-__---''--__...L.....J
o
</p>
<p>2.------,------r---...,....---ic-+----r-----,r----,-,
</p>
<p>3025105
</p>
<p>. .
</p>
<p>~ :.IIL'.lLn&bull;.&bull;II.JI1Ill.T.IILJ&middot;. .&middot;. .&middot;.&middot;.&middot; ,&middot; ,15 ., 20
n
</p>
<p>Figure 16.4: Typical outcomes of Bernoulli random process with p = 0.5. The
</p>
<p>realization starts at n = 0 and continues indefinitely. The dashed box indicates the
</p>
<p>realizations of the random variable X[18,s].
</p>
<p>variables (X(O), X(1) , . . .), with one random variable for each time instant, and
</p>
<p>each realization of the random process takes on a value that is represented as an
</p>
<p>infinite sequence of numbers or (x[O], x[I]' ... ). We will denote the random process
</p>
<p>more succinctly by X[n] and the realization by x[n] but it is understood that the n
</p>
<p>denotes the values n = 0,1, .... If we wish to indicate the random process at a fixed
</p>
<p>time instant, then we will use n = no or n = nI, etc. so that X[no] is the random
</p>
<p>process at n = no (which is just a random variable) and its realization at that time
</p>
<p>is x [no] (which is a number). Finally, we have used the [.] notation to remind us
</p>
<p>that X[n] is defined only for discrete integer times. This type of random process is
</p>
<p>known as a discrete-time random process. In the next section the continuous-time
</p>
<p>random process will be discussed. Before continuing, however , we look at a typical
</p>
<p>probability calculation for a random process.
</p>
<p>Example 16.1 - Bernoulli random process
</p>
<p>For the infinite coin tossing example, we might ask for the probability of the first</p>
<p/>
</div>
<div class="page"><p/>
<p>520 CHAPTER 16. BASIC RANDOM PROCESSES
</p>
<p>5 tosses coming up all heads. Thus, we wish to evaluate
</p>
<p>P[X[O] = 1, X[I] = 1, X[2] = 1, X[3] = 1, X [4] = 1, X[5] = 0 or 1, X[6] = 0 or 1, ...].
</p>
<p>It would seem that since we don 't care what the outcomes of X[n] for n = 5,6, ...
</p>
<p>are, then the probability expression could be replaced by
</p>
<p>P[X[O] = I ,X[I] = I ,X[2] = I ,X[3] = I,X[4] = 1]
</p>
<p>and indeed this is the case, although it is not so easy to prove [Billingsley 1986].
</p>
<p>Then, by using the assumption of independence of a Bernoulli random process we
</p>
<p>have
</p>
<p>4
</p>
<p>P[X[O] = I,X[I] = I,X[2] = I,X[3] = 1, X [4] = 1] = II P[X[n] = 1] =p5.
n=O
</p>
<p>A related question is to determine the probability that we will ever observe 5 ones
</p>
<p>in a row. Intuitively, we expect this probability to be 1, but how do we prove this?
</p>
<p>It is not easy! Such is the difficulty encountered when we make the leap from a
</p>
<p>random vector, having a finite number of random variables, to a random process,
</p>
<p>having an infinite number of random variables.
</p>
<p>16.4 Types of Random Processes
</p>
<p>The previous example of an infinite number of coin tosses produced a random process
</p>
<p>X[n] for n = 0,1 , .... In some cases , however, we wish to think of the random
</p>
<p>process as having started sometime in the infinite past. If X[n] is defined for n =
</p>
<p>... , -1 ,0,1 , ... or equivalently -00 &lt; n &lt; 00, where it is assumed that n is an
integer, then X[n] is called an infinite random process. In contrast, the previous
</p>
<p>example is referred to as a semi-infinite random process. Another categorization
</p>
<p>of random processes involves whether the times at which the random variables are
</p>
<p>defined and the values that they take on are either discrete or continuous. The
</p>
<p>infinite coin toss example is a discrete-time random process, since it is defined for n =
</p>
<p>0,1 , ... , and is a discrete-valued random process, since it takes on values 0 and 1 only.
</p>
<p>It is referred to as a discrete-time/discrete valued (DTDV) random process. Other
</p>
<p>types ofrandom processes are discrete-time/continuous-valued (DTCV) , continuous-
</p>
<p>time/discrete-valued (CTDV), and continuous-time/continuous-valued (CTCV). A
</p>
<p>realization of each type is shown in Figure 16.5. In Figure 16.5a a realization of
</p>
<p>the Bernoulli random process, as previously described, is shown while in Figure
</p>
<p>16.5b a realization of a Gaussian random process with Y[n] rv N(O,I) is shown.
The Bernoulli random process is defined for n = 0,1 , ... (semi-infinite) while the
Gaussian random process is defined for -00 &lt; n &lt; 00 and n an integer (infinite).</p>
<p/>
</div>
<div class="page"><p/>
<p>16.4. TYPES OF RANDOM PROCESSES 521
</p>
<p>3 3 .-------r---~---,..___--_,
</p>
<p>2 -- 2 . .
</p>
<p>~ :'.1.11"..1.11 1.1 ....Il.1
- 1 .
</p>
<p>- 2 . - 2 .
</p>
<p>2010 15
</p>
<p>Sample, n
5
</p>
<p>_3L----'----~-- ~-----J
</p>
<p>o2010 15
Sample, n
</p>
<p>5
</p>
<p>- 3 L-_ _ ~ ~ __ ~ __----'
</p>
<p>o
</p>
<p>(a) Discrete-time/discret e-valued (DTDV)
</p>
<p>Bern oulli random pro cess
</p>
<p>(b) Discrete-time/ continuous-valued
</p>
<p>(DTCV) Gaussian random process
</p>
<p>3
</p>
<p>2 .. . . ...
</p>
<p>,.-......,
'-'
~ 0
</p>
<p>- 1
</p>
<p>. . . . .
</p>
<p>- 3
20 0 5 10 15 20
</p>
<p>Time, t (sec)
</p>
<p>o'-- --~---~--~-- ----'
o 5 10 15
</p>
<p>Time, t (sec)
</p>
<p>5 .
</p>
<p>15 .-----~ ---~--~ -----,
</p>
<p>(c) Continuous-time/ discrete-valued
</p>
<p>(CT DV) binomial random process
</p>
<p>(d) Continuous-time/ cont inuous-valued
</p>
<p>(CTCV) Gaussian random process
</p>
<p>Figure 16.5: Typical realizations of different types of random processes.
</p>
<p>Both these random processes are discrete-time with the first one taking on only the
</p>
<p>values 0 and 1 and the second one taking on all real values. In Figure 16.5c is shown
</p>
<p>a random process, also known as a cont inuous-time binomial random process, which
</p>
<p>is defined as W(t) = E ~ ~o X [n]' where X[n] is a Bernoulli random process and [t]
denotes the largest integer less than or equal to t . This process effectively counts
</p>
<p>the number of successes or ones of the Bernoulli random process (compare Figure
</p>
<p>16.5c with Figure 16.5a) . It is defined for all ti me; hence, it is a continuous-time
</p>
<p>random process, and it takes on only integer values in the range {O, 1, ... }; hence,
it is discrete-valued. Finally, in Figure 16.5d is shown a realization of anot her</p>
<p/>
</div>
<div class="page"><p/>
<p>522 CHAPTER 16. BASIC RANDOM PROCESSES
</p>
<p>Gaussian process but with Z(t) rv N(O, 1) for all time t. This is a continuous-time
</p>
<p>random process that takes on all real values; hence, it is continuous-valued. We
</p>
<p>will generally use a discrete-time random process, with either discrete or continuous
</p>
<p>values, to introduce new concepts. This is because a continuous-t ime random process
</p>
<p>introduces a host of mathematical subtleties which in many cases are beyond the
</p>
<p>scope of this text. When possible, however, we will quote the analogous results for
</p>
<p>continuous-time random processes. Note finally that a realization of X[n], which is
</p>
<p>x [n], is also called a sample sequence, while a realization of X(t), which is x(t) , is
</p>
<p>also called a sample function. We will , however , reserve the use of the word sample
</p>
<p>to refer to a time sample of the random process. Hence, a time sample will refer
</p>
<p>to either the random variable X[no] (X(to&raquo; or the realization x [no] (x(to&raquo; of the
</p>
<p>random process, with the meaning determined by the context of the discussion. We
</p>
<p>next revisit the random walk of Example 9.5.
</p>
<p>Example 16.2 - Random walk (continued from Example 9.5)
</p>
<p>Recall that
n
</p>
<p>n = 1,2, . . .
</p>
<p>where
</p>
<p>Pu[kJ ~ {
</p>
<p>n
</p>
<p>X[n] = LU[i]
i= O
</p>
<p>k =-1
</p>
<p>k=1
</p>
<p>n = 0,1, ...
</p>
<p>(16.1)
</p>
<p>where the U[i]'s are IID random variables having the PMF of (16.1). We also
</p>
<p>assume that the random walk starts at time n = 0. The U[i]'s comprise the random
variables of a Bernoulli random process but with values of &plusmn;1, instead of the usual
</p>
<p>&deg;and 1. As such, we can view the U[i]'s as comprising a Bernoulli random process
Urn] for n = 0,1, .... Realizations of Urn] and X[n] are shown in Figure 16.6. One
question that comes to mind is the behavior of the random walk for large n. For
</p>
<p>example, we might be interested in the PDF of X[n] for large n. Relying on the
</p>
<p>central limit theorem (see Chapter 15), we can assert that the PDF is Gaussian,
</p>
<p>and therefore we need only determine the mean and variance. This easily follows
</p>
<p>from the definition of the random walk as
</p>
<p>n
</p>
<p>E[X[n]] = L E[U[i]] = (n + I)E[U[O]] =&deg;
i=O
n
</p>
<p>var(X[n]) = L var(U[i]) = (n + l)var(U[O]) = n + 1
i=O</p>
<p/>
</div>
<div class="page"><p/>
<p>16.5. THE IMPORTANT PROPERTY OF STATIONARITY 523
</p>
<p>302520105
</p>
<p>5.--~-~-~-----,----:"---,
</p>
<p>-3
-4 , , , , .
</p>
<p>-5'---~-~-~-~-~-~
</p>
<p>o30252015
n
</p>
<p>10
</p>
<p>. . .. . . . . . . . . . . . . . . . . . . : .
</p>
<p>5
</p>
<p>5c---~-~-~-~-~--""
</p>
<p>4
</p>
<p>3
</p>
<p>2
</p>
<p>*~ :ttl
1
&middot;.111'nu
</p>
<p>I
&middot;
1
&lt;tl
</p>
<p>1
1
1
:111'1'1:'1' '1'.11'
</p>
<p>-1 1.1. . ...
-2
</p>
<p>-3 .
</p>
<p>-4
</p>
<p>- 5 ' - - - ~ - ~ - ~ - ~ - ~ - - - - ' - - '
</p>
<p>o
</p>
<p>(a) Realization of Bernoulli random pro-
</p>
<p>cess Urn]
(b) Realization of random walk X[n]
</p>
<p>Figure 16.6: Typical realization of a random walk.
</p>
<p>since E[U[i]] = &deg;and var{U[i]) = 1. (Note that since the U[i]'s are identically
distributed, they all have the same mean and variance. We have arbitrarily chosen
</p>
<p>U[O] in the expression for the mean and variance of a single sample.) Hence, for
large n we have approximately that X[n] I'V N{o,n+ 1). Does this appear to explain
the behavior of x[n] shown in Figure 16.6b?
</p>
<p>16.5 The Important Property of Stationarity
</p>
<p>The simplest type of random process is an IID random process. The Bernoulli
</p>
<p>random process is an example of this. Each random variable X[no] is independent
of all the others and each random variable has the same marginal PMF. As such,
</p>
<p>the joint PMF of any finite number of samples can immediately be written as
</p>
<p>N
</p>
<p>PX[nI) ,X[n2],...,X [nN][Xl , X2,&middot;&middot;&middot;, XN] = IIPX[ni] [Xi]
i= l
</p>
<p>(16.2)
</p>
<p>and used for probability calculations. For example, for a Bernoulli random process
</p>
<p>with values 0,1 the probability of the first 10 samples being 1,0,1,0,1,0,1,0,1,&deg;is
p5(1 - p)5. Note that we are able to specify the joint PMF for any finite number
</p>
<p>of sample times. This is sometimes referred to as being able to specify the finite
</p>
<p>dimensional distribution (FDD) . It is the most complete probabilistic description
</p>
<p>that we can manage for a random process and reduces the analysis of a random
</p>
<p>process to the analysis of a finite but arbitrary set of random variables.</p>
<p/>
</div>
<div class="page"><p/>
<p>524 CHAPTER 16. BASIC RANDOM PROCESSES
</p>
<p>A generalization of the lID random process is a random process for which the
</p>
<p>FDD does not change with the time origin. This is to say that the PMF or PDF
</p>
<p>of the samples {X[nl], X[n2],"" X[nNn is the same as for {X[ni + no] , X[n2 +
no], ... ,X[nN + non , where no is an arbitrary integer. Alternatively, the set of
samples can be shifted in time, with each one being shifted the same amount, without
</p>
<p>affecting the joint PMF or joint PDF. Mathematically, for the FDD not to change
</p>
<p>with the time origin, we must have that
</p>
<p>(16.3)
</p>
<p>for all no, and for any arbitrary choice of Nand nl, n2, ... , nN. Such a random
</p>
<p>process is said to be stationary. It is implicit from (16.3) that all joint and marginal
</p>
<p>PMFs or PDFs must have probabilities that do not depend on the time origin. For
</p>
<p>example, by letting N = 1 in (16.3) we have that PX [nl +no] = PX[nI] and setting
ni = 0, we have that PX[no] = PX[O] for all no. This says that the marginal PMF or
</p>
<p>PDF is the same for every sample in a stationary random process. We next prove
</p>
<p>that an lID random process is stationary.
</p>
<p>Example 16.3 - lID random process is stationary.
</p>
<p>To prove that the lID random process is a special case of a stationary random
</p>
<p>process we must show that (16.3) is satisfied. This follows from
</p>
<p>N
</p>
<p>IlPx[ni+no]
</p>
<p>i= 1
</p>
<p>N
</p>
<p>IlPX[ni]
</p>
<p>i=1
</p>
<p>(by independence)
</p>
<p>(by identically distributed)
</p>
<p>(by independence).
</p>
<p>(;
</p>
<p>If a random process is stationary, then all its joint moments and more generally all
</p>
<p>expected values of functions of the random process, must also be stationary since
</p>
<p>EX[nl+no] ,...,X[nN+no][&middot;] = EX[nl], ...,X [nN] [&middot;]
</p>
<p>which follows from (16.3). Examples then of random processes that are not station-
</p>
<p>ary are ones whose means and/or variances change in time, which implies that the
</p>
<p>marginal PMF or PDF change with time. In Figure 16.7 we show typical realiza-
</p>
<p>tions ofrandom processes whose mean in Figure 16.7a and whose variance in Figure
</p>
<p>16.7b change with time. They were generated using the MATLAB code:
</p>
<p>randn('state',O)
</p>
<p>N=51;
</p>
<p>x=randn(N,1)+O.1*[O:N-1]'; %for Figure 16.7a
y=sqrt(O.95.-[O :50] ').*randn(N,1); %for Figure 16.7b</p>
<p/>
</div>
<div class="page"><p/>
<p>16.5. THE IMPORTANT PROPERTY OF STATIONARITY 525
</p>
<p>5040302010
</p>
<p>3 .------~--~--~-~--_,
</p>
<p>_3L--~--~--~-~-----'
</p>
<p>o
</p>
<p>-2 .
</p>
<p>2
</p>
<p>~ _: Illltd(~~Lr~t.~i~T+I~I.1
</p>
<p>50
</p>
<p>11
</p>
<p>40302010
</p>
<p>1
</p>
<p>5 .------~----r---~-:::-A~___r....__~
</p>
<p>-2
</p>
<p>-3
</p>
<p>-4
_ 5 L - - ~ - - ~ - - ~ - ~ - - - - - '
</p>
<p>o
n n
</p>
<p>(a) Mean increasing with n (b) Variance decreasing with n
</p>
<p>Figure 16.7: Random processes that are not stationary.
</p>
<p>In Figure 16.7a the true mean increases linearly from 0 to 5 while in Figure 16.7b the
</p>
<p>variance decreases exponentially as 0.95n . It is clear then that the samples all have
</p>
<p>different moments and therefore PX[nl+no] -=I PX[nll which violates the condition for
stationarity.
</p>
<p>.ffi It is impossible to determine if a random process is stationary
from a single realization.
</p>
<p>A realization of a random process is a single outcome of the random process. This is
</p>
<p>analogous to observing a single outcome of a coin toss. We cannot determine if the
</p>
<p>coin is fair by observing that the outcome was a head. What is required are multiple
</p>
<p>realizations of the coin tossing experiment. So it is with random processes. In Figure
</p>
<p>16.7b, although we generated the realization using a variance that decreased with
</p>
<p>time, and hence the random process is not stationary, the realization shown could
</p>
<p>have been generated with a constant variance. Then, the values of the realization
</p>
<p>near n = 50 just happen to be smaller than the ones near n = 0, which is possible,
although maybe not very probable. To better discern whether a random process is
</p>
<p>stationary we require multiple realizations.
</p>
<p>Another example of a random process that is not stationary follows.
</p>
<p>Example 16.4 - Sum random process
</p>
<p>A sum random process is a slight generalization of the random walk process of</p>
<p/>
</div>
<div class="page"><p/>
<p>526 CHAPTER 16. BASIC RANDOM PROCESSES
</p>
<p>Example 16.2. As before, X[n] = L:?=o U[i], where the U[i]'s are lID but for the
general sum process, the U[i]'s can have any, although the same, PMF or PDF.
</p>
<p>Thus, the sum random process is not stationary since
</p>
<p>E[X[n]]
</p>
<p>var(X[n])
</p>
<p>= (n + l)Eu[U[O]]
(n + l)var(U[O])
</p>
<p>both of which change with n. Hence, it violates the condition for stationarity.
</p>
<p>&lt;&gt;
A random process that is not stationary is said to be nonstationary. In light of
</p>
<p>the fact that an lID random process lends itself to simple probability calculations,
</p>
<p>it is advantageous, if possible, to transform a nonstationary random process into a
</p>
<p>stationary one (see Problem 16.12 on transforming the random processes of Figure
</p>
<p>16.7 into stationary ones). As an example, for the sum random process this can be
</p>
<p>done by "reversing" the summing operation. Specifically, we difference the random
</p>
<p>process. Then X[n] - X[n - 1] = U[n] for n 2: 0, where we define X[-l] = O.
This is an lID random process. The differences or increment random variables U[n]
</p>
<p>are independent and identically distributed. More generally, for the sum random
</p>
<p>process any two increments of the form
</p>
<p>n2
</p>
<p>L U[i]
i=nl+l
</p>
<p>n4
</p>
<p>X[n4] - X[n3] = L U[i]
i= n 3+ l
</p>
<p>are independent if n4 &gt; n3 2: n2 &gt; nl. Thus, nonoverlapping increments for a sum
random process are independent. (Recall that functions of independent random
</p>
<p>variables are themselves independent.) If furthermore, n4 - n3 = n2 - nl, then they
also have the same PMF or PDF since they are composed of the same number of lID
</p>
<p>random variables. It is then said that for the sum random process, the increments
</p>
<p>are independent and stationary (equivalent to being identically distributed) or that
</p>
<p>it has stationary independent increments. The reader may wish to ponder whether
</p>
<p>a random process can have independent but nonstationary increments (see Problem
</p>
<p>16.13). Many random processes (an example of which follows) that we will encounter
</p>
<p>have this property and it allows us to more easily analyze the probabilistic behavior.
</p>
<p>Example 16.5 - Binomial counting random process
</p>
<p>Consider the repeated coin tossing experiment where we are interested in the num-
</p>
<p>ber of heads that occurs. Letting U[n] be a Bernoulli random process with U[n] = 1
</p>
<p>with probability p and U[n] = 0 with probability 1-p , the number of heads is given</p>
<p/>
</div>
<div class="page"><p/>
<p>16.5. THE IMPORTANT PROPERTY OF STATIONARITY
</p>
<p>by the binomial counting or sum process
</p>
<p>527
</p>
<p>n
</p>
<p>X[n] = 2:Uri]
i=O
</p>
<p>or equivalently
</p>
<p>n = 0,1, .. .
</p>
<p>X n _ { U[O] n = 0
[ ] - X[n - 1] + Urn] n ~ 1.
</p>
<p>A typical realization is shown in Figure 16.8. The random process has stationary
</p>
<p>20 .----.----~-~-~-~-,...,
</p>
<p>15 .
</p>
<p>~
</p>
<p>t'l10 .
</p>
<p>:"rrl11I
o 5 10 15 20 25 30
</p>
<p>n
</p>
<p>Figure 16.8: Typical realization of binomial counting random process with p = 0.5.
</p>
<p>and independent increments since the changes over two nonoverlapping intervals
</p>
<p>are composed of different sets of identically distributed U[i]'s. We can use this
property to more easily determine probabilities of events. For example, to determine
</p>
<p>PX[lj,x[2J[1,2] = P[X[l] = 1, X[2] = 2]' we can note that the event X[l] = 1, X[2] =
2 is equivalent to the event Yl = X[l] - X[-l] = 1, Y2 = X[2] - X[l] = 1, where
X[-l] is defined to be identically zero. But Yl and Y2 are nonoverlapping increments
(but of unequal length), making them independent random variables. Thus,
</p>
<p>P[X[l] = 1, X[2] = 2] = P[Yl = 1,Y2 = 1] = P[Yl = I]P[Y2 = 1]
</p>
<p>P[U[O] + U[I] = I]P[U[2] = 1]
'--.---"
</p>
<p>bin(2 ,p)
</p>
<p>(~) pl(1 _ p)l . P
</p>
<p>2p2(1 - p).</p>
<p/>
</div>
<div class="page"><p/>
<p>528 CHAPTER 16. BASIC RANDOM PROCESSES
</p>
<p>16.6 Some More Examples
</p>
<p>We continue our discussion by examining some random processes of practical interest.
</p>
<p>Example 16.6 - White Gaussian noise
</p>
<p>A common model for physical noise, such as resistor noise due to electron motion
</p>
<p>fluctuations in an electric field, is termed white Gaussian noise (WGN) . It is assumed
</p>
<p>that the noise has been sampled in time to yield a DTCV random process X[n]. The
WGN random process is defined to be an IID one whose marginal PDF is Gaussian
</p>
<p>so that X[n] rv N(O,O'2 ) for -00 &lt; n &lt; 00. Each random variable X[no] has a
mean of zero , consistent with our notion of a noise process, and the same variance
</p>
<p>or because the mean is zero, the same power E[X 2 [no]] . A typical realization is
shown in Figure 16.5b for 0'2 = 1. The WGN random process is stationary since it
is an IID random process. Its joint PDF is
</p>
<p>=
</p>
<p>N
</p>
<p>IIPX[n;J(xi)
i=l
</p>
<p>N 1 (1 2)II ~ e x p --22 xi
. 21m2 a
~=l
</p>
<p>1 (1 N )
(21rO'2)N/2 exp - 20'2 ~ x; . (16.4)
</p>
<p>Note that the joint PDF is N(o, 0' 21), which is a special form of the multivariate
Gaussian PDF (see Problem 16.15). The terminology of "white" derives from th e
</p>
<p>property that such a random process may be synthesized from a sum of different
</p>
<p>frequency random sinusoids each having the same power, much the same as white
</p>
<p>light is composed of equal contributions of each visible wavelength of light. We will
</p>
<p>justify this property in Chapter 17 when we discuss the power spectral density.
</p>
<p>Example 16.7 - Moving average random process
</p>
<p>The moving average (MA) random process is a DTCV random process defined as
</p>
<p>X[n] = ~(U[n] + Urn - 1]) -oo&lt;n&lt;oo
</p>
<p>where Urn] is a WGN random process with variance O ' ~ . (To avoid confusion with
the variance of other random variables we will sometimes use a subscript on 0'2 , in
</p>
<p>this case o'~, to refer to the variance of the U[no] random variable.) The terminology
of moving average refers to the averaging of the current random variable Urn] with
the previous random variable urn - 1] to form the current moving average random</p>
<p/>
</div>
<div class="page"><p/>
<p>16.6. SOME MORE EXAMPLES 529
</p>
<p>variable. Also, this averaging "moves" in time, as for example,
</p>
<p>X[O] =
</p>
<p>X[I] =
</p>
<p>X[2]
</p>
<p>~(U[O] + U[-I])
~ ( U [ I ] + U[O])
~(U[2] + U[I])
</p>
<p>etc.
</p>
<p>A typical realization of X[n] is shown in Figure 16.9 and should be compared to
the realization of Urn] shown in Figure 16.5b. It is seen that the moving average
random process is "smoother" than the WGN random process, from which it was
</p>
<p>obtained. Further smoothing is possible by averaging more WGN samples together
</p>
<p>(see Problem 16.17). The MATLAB code shown below was used to generate the
</p>
<p>realization.
</p>
<p>randn('state' ,0)
</p>
<p>u=randn(21,1);
</p>
<p>for i=1:21
</p>
<p>if i==1
</p>
<p>x(i,1)=0.5*(u(1)+randn(1,1)); %needed to initialize sequence
else
</p>
<p>x(i,1)=0.5*(u(i)+u(i-1)) ;
</p>
<p>end
</p>
<p>end
</p>
<p>3 . - - - - ~ - - ~ - - ~ - - - - - - - ,
</p>
<p>2 &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; . .. . .
</p>
<p>~ _:II'r.lr.,.,.n.l1.r
-2 .
</p>
<p>20155
_3L-.--~--~--~ __-----l
</p>
<p>o
</p>
<p>Figure 16.9: Typical realization of moving average random process. The realization
</p>
<p>of the Urn] random process is shown in Figure 16.5b.
</p>
<p>The joint PDF of X[n] can be determined by observing that it is a linearly trans-
formed version of Urn]. As an example, to determine the joint PDF of the random</p>
<p/>
</div>
<div class="page"><p/>
<p>530 CHAPTER 16. BASIC RANDOM PROCESSES
</p>
<p>vector [X [0] X[I]]T, we have from the definition of the MA random process
</p>
<p>[
</p>
<p>U[-I] ]
</p>
<p>[
X[O] ] = [! ! 0] U[O]
X[I] O!!
</p>
<p>U[I]
</p>
<p>or in matrix/vector notation X = GU. Now recalling that U is a Gaussian random
</p>
<p>vector (see (16.4)) and that a linear transformation of a Gaussian random vector
</p>
<p>produces another Gaussian random vector, we have from Example 14.3 that
</p>
<p>Explicitly, since each sample of Urn] is zero mean with variance a ~ and all samples
are independent, we have that E[U] = 0 and Cu = ap. This results in
</p>
<p>X = [ X[O] ] ""' N(o, a~GGT)
X[I]
</p>
<p>where
</p>
<p>GG
T
</p>
<p>= [: n-
It can furthermore be shown that the MA random process is stationary (see Example
</p>
<p>20.2 and Property 20.2).
</p>
<p>Example 16.8 - Randomly phased sinusoid (or sine wave)
</p>
<p>Consider the DTCV random process given as
</p>
<p>X[n] = cos(27f(0.I)n + 8) -oo&lt;n&lt;oo
</p>
<p>where 8 ""' U(O, 27f). Some typical realizations are shown in Figure 16.10. The MAT-
</p>
<p>LAB statements n= [0: 31] , and x=cos (2*pi*0 .1*n+2*pi*rand(1, 1)) can be used
</p>
<p>to generate each realization. This random process is frequently used to model an
</p>
<p>analog sinusoid whose phase is unknown and that has been sampled by an analog-to-
</p>
<p>digital convertor. It is nearly a deterministic signal, except for the phase uncertainty,
</p>
<p>and is therefore perfectly predictable. This is to say that once we observe two suc-
</p>
<p>cessive samples, then all the remaining ones are known (see Problem 16.20). This is
</p>
<p>in contrast to the WGN random process, for which regardless of how many samples
</p>
<p>we observe, we cannot predict any of the remaining ones due to the independence
</p>
<p>of the samples. Because of the predictability of the randomly phased sinusoidal
</p>
<p>process, the joint PDF can only be represented using impulsive functions. As an ex-
</p>
<p>ample, you might try to find the PDF of (X,Y) if (X,Y) has the bivariate Gaussian</p>
<p/>
</div>
<div class="page"><p/>
<p>16.6. SOME MORE EXAMPLES 531
</p>
<p>1.5.---~~-.,.----,-...,.,
</p>
<p>~ ~: 11.j]1,&middot;1111'11],.1111'1]]jJI ~ ~: [111,1111'11]1.1111,[ljl,IIII, ~ ~~ III111jljllllll,Il!l1111111111
10 15 20 25 30
</p>
<p>n
10 15 20 25 30 -1 .50'--~~~-::----:~~----:-:
</p>
<p>n
-1.5 '---:':---:':-~~---:':~~
</p>
<p>10 15 20 25 30 0
n
</p>
<p>(a) () = 5.9698 (b) ()= 1.4523 (c) () = 3.8129
</p>
<p>Figure 16.10: Typical realizations for randomly phased sinusoid.
</p>
<p>PDF with p = 1. We will not pursue this further . However, we can determine the
marginal PDF PX[n]' To do so we use the transformation formula of (10.30), where
</p>
<p>the Y random variable is X[noJ (considering the random process at a fixed time)
and the X random variable is 8. The transformation is shown in Figure 16.11 for
</p>
<p>no = O. Note that there are two solutions for any given x[noJ = y (except for the
</p>
<p>15 r-----,------r-----,--------,
</p>
<p>1
Y -- 1 - ~ - - - - - : - - - - - - - - : - - - - - - - : - - - " ' 7 I ' " ' - - - - 1
</p>
<p>05
</p>
<p>-1
</p>
<p>7r
()
</p>
<p>1
-1 .5 '---'---+----'------:t------'----;:;!
</p>
<p>o
</p>
<p>Figure 16.11: Function transforming 8 into X[noJ for the value no
X[noJ = cos(27r(0.1)no + 8).
</p>
<p>0, where
</p>
<p>point at () = 7r, which has probability zero). We denote the solutions as () = Xl,X2.
Using our previous notation of y = g(x) for a transformation of a single random</p>
<p/>
</div>
<div class="page"><p/>
<p>532 CHAPTER 16. BASIC RANDOM PROCESSES
</p>
<p>variable we have that
</p>
<p>y = cos(27f(0.1)no + x)
</p>
<p>so that the solutions are
</p>
<p>X l = arccos(y) - 27f(0.1)no = 911 (y)
X2 = 27f - [arccos(y) - 27f(0.1)no] = 92"I(y)
</p>
<p>for -1 &lt; y &lt; 1 and thus 0 &lt; arccos(y) &lt; 7f. Using darccos(y)/dy = 1/~, we
have
</p>
<p>py(y) PX(91
1(y)) Id9~~(Y) 1+ PX(92"I(y)) Id9~~(Y) I
</p>
<p>1 1 1 1
= - +-
</p>
<p>27f ~ 27f ~
</p>
<p>1
</p>
<p>7f~'
</p>
<p>Finally, in our original notation we have the marginal PDF for X[n] for any n
</p>
<p>{
</p>
<p>I -1 &lt; x &lt; 1
PX[ ](x ) = 1rVI -x2
</p>
<p>n 0 otherwise.
</p>
<p>This PDF is shown in Figure 16.12. Note that the values of X[n] that are most
probable are near x = &plusmn;1. Can you explain why? (Hint: Determine the values of ()
for which 0.9 &lt; cos() &lt; 1 and also 0 &lt; cos() &lt; 0.1 in Figure 16.11.)
</p>
<p>2.5r----,-----.-------.------,
</p>
<p>2 .
</p>
<p>0.5o
x
</p>
<p>-0.5
O'------'-------L-------'--------'
-1
</p>
<p>Figure 16.12: Marginal PDF for randomly phased sinusoid.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.7. JOINT MOMENTS
</p>
<p>16.7 Joint Moments
</p>
<p>533
</p>
<p>The first and second moments or equivalently the mean and variance of a random
</p>
<p>process at a given sample time are of great practical importance since they are easily
</p>
<p>determined. Also, the covariance between two samples of the random process at two
</p>
<p>different times is easily found. At worst, the first and second moments can always
</p>
<p>be estimated in practice. This is in contrast to the joint PMF or joint PDF, which
</p>
<p>in practice may be difficult to determine. Hence , we next define and give some
</p>
<p>examples of the mean, variance, and covariance sequences for a DTCV random
</p>
<p>process. The mean sequence is defined as
</p>
<p>J-tx[n] = E[X[n]]
</p>
<p>while the variance sequence is defined as
</p>
<p>CTk[n] = var(X[n])
</p>
<p>-oo&lt;n&lt;oo
</p>
<p>-oo&lt;n&lt;oo
</p>
<p>(16.5)
</p>
<p>(16.6)
</p>
<p>and finally the covariance sequence is defined as
</p>
<p>cov(X[nlJ, X [n2])
</p>
<p>E[(X[nl] - ux [nl]) (X[n2] - J-tx[n2])] -00 &lt; nl &lt; 00 (16.7)
-00 &lt; n2 &lt; 00.
</p>
<p>The expectations for the mean and variance are taken with respect to the PMF or
</p>
<p>PDF PX[n] for a particular value of n. Similarly, the expectation needed for the
</p>
<p>evaluation of the covariance is with respect to the joint PMF or PDF PX[nl],X[n2]
for particular values of nl and n2. Since the required PMF or PDF should be clear
</p>
<p>from the context, we henceforth do not subscript the expectation operator as we
</p>
<p>have done so previously. Note that the usual symmetry property of the covariance
</p>
<p>holds, which results in cx[n2, nl] = cx[nl, n2]. Also, it follows from the definition
</p>
<p>of the covariance sequence that cx[n,n] = CTk[n]. The actual evaluation of the
</p>
<p>moments proceeds exactly the same as for random variables.
</p>
<p>If the random process is a continuous-time one, then the corresponding defini-
</p>
<p>tions are
</p>
<p>p x (t)
</p>
<p>CTk(t)
</p>
<p>CX(tl' t2)
</p>
<p>E[X(t)]
</p>
<p>= var(X(t))
</p>
<p>E[(X(td - J-tx(td) (X(t2) - J-tX(t2))]'
</p>
<p>These are called the mean junction, variance junction, and covariance junction,
</p>
<p>respectively. We next examine the moments for the examples of the previous section.
</p>
<p>Noting that the variance is just the covariance sequence evaluated at nl = n2 = n,
we need only determine the mean and covariance sequences.</p>
<p/>
</div>
<div class="page"><p/>
<p>534 CHAPTER 16. BASIC RANDOM PROCESSES
</p>
<p>Example 16.9 - White Gaussian noise
</p>
<p>Since X[n] ,...., N(O, a2) for all n, we have that
</p>
<p>flx[n] = 0 - 00 &lt; n &lt; 00
</p>
<p>ak[n] = a2 - 00 &lt; n &lt; 00.
</p>
<p>The covariance sequence for nl i= n2 must be zero since the random variables are
all independent. Recalling that the covariance between X[n] and itself is just the
</p>
<p>variance, we have that
</p>
<p>This can be written in more succinct form by using the discrete delta function as
</p>
<p>In summary, for a WGN random process we have that flx[n] = 0 for all nand
</p>
<p>cx[nl ,n2] = a28[n2 - nl]'
</p>
<p>Example 16.10 - Moving average random process
</p>
<p>The mean sequence is
</p>
<p>flx[n] = E[X[n]] = E[!(U[n] + Urn - 1])] = 0 -oo&lt;n&lt;oo
</p>
<p>since Urn] is white Gaussian noise, which has a zero mean for all n. To find the
</p>
<p>covariance sequence using X[n] = (U[n] + Urn - 1])/2, we have
</p>
<p>ex[nl ,n2] E[(X[nl] - ux [nl])(X[n2] - ux [n2])]
E[X[nl]X[n2]]
1
</p>
<p>= :4E[(U[nl] + U[nl - 1])(U[n2]+ U[n2 - 1])]
</p>
<p>1
= :4 (E[U[nl]U[n2]] + E[U[nl]U[n2 - 1]]
</p>
<p>+E[U[nl - I]U[n2]] + E[U[nl - I]U[n2 - 1]]) .
</p>
<p>But E[U[k]U[l]] = a~8[l - k] since Urn] is WGN , and as a result
</p>
<p>ex [nl ,n2] ~ (a~8[n2 - nd + a~8[n2 - 1 - nd + a~8[n2 - nl + 1] + a~8[n2 - nd)
</p>
<p>a2 a2 a2
~ 8[n2 - nl] + : 8[n2 - nl - 1] + : 8[n2 - nl + 1].
</p>
<p>This is plotted in Figure 16.13 versus tln = n2 - nl. It is seen that the covariance
</p>
<p>sequence is zero unless the two samples are at most one unit apart or tln = n2-nl =</p>
<p/>
</div>
<div class="page"><p/>
<p>16.7. JOINT MOMENTS
</p>
<p>-3 -2 2 3
</p>
<p>535
</p>
<p>Figure 16.13: Covariance sequence for moving average random process.
</p>
<p>&plusmn;l. Note that the covariance between any two samples spaced one unit apart is the
</p>
<p>same. Thus, for example, X[I] and X[2] have the covariance ex[I ,2] = ab/4,
as do X[9] and X[10] since ex [9, 10] = ab/4, and as do X[-3] and X[-2] since
ex [- 3, -2] = ab/4 (see Figure 16.13). Any samples that are spaced more than one
unit apart are uneorrelated. This is because for In2 - nIl&gt; 1, X[nI] and X[n2]
are independent, being composed of two sets of different WGN samples (recall that
</p>
<p>functions of independent random variables are independent). In summary, we have
</p>
<p>that
</p>
<p>I-tx[n] 0
</p>
<p>{
a 2:::rL ni = n22
</p>
<p>ex[nI, n2] a 2 In2 - nIl = 1:::rL4
0 In2 -nIl&gt; 1.
</p>
<p>and the variance is ex[n, n] = ab/2 for all n. Also, note from Figure 16.13 that the
covariance sequence is symmetric about tln = O.
</p>
<p>Example 16.11 - Randomly phased sinusoid
</p>
<p>Recalling that the phase is uniformly distributed on (0,27f) we have that the mean</p>
<p/>
</div>
<div class="page"><p/>
<p>536 CHAPTER 16. BASIC RANDOM PROCESSES
</p>
<p>sequence is
</p>
<p>(use (11.10))
</p>
<p>J-lx[n] E[X[n]] = E[cos(211"(0.I)n + 8)]
</p>
<p>121r cos(211"(0.I)n + B) 2~ dB
1 121r
</p>
<p>- sin(211"(0.I)n + B) = 0
211" 0
</p>
<p>for all n. Noting that the mean sequence is zero , the covariance sequence becomes
</p>
<p>Once again the covariance sequence depends only on the spacing between the two
</p>
<p>0,6 r-----,-----...,..-------.-------,
</p>
<p>0,5
</p>
<p>::: ... . .. . . .. . '. :. "
-0.4 . . .
</p>
<p>10
</p>
<p>-0,5 .. , ...
</p>
<p>_0.6l...----...1....------'----------'------'
-10
</p>
<p>Figure 16.14: Covariance sequence for randomly phased sinusoid.
</p>
<p>samples or on n2 - nl. The covariance sequence is shown in Figure 16.14. The
</p>
<p>reader should note the symmetry of the covariance sequence about 6.n = O. Also,
</p>
<p>the variance follows as a-;',[n] = cx [n,n] = 1/2 for all n. It is interesting to observe
that in this example the fact that the mean sequence is zero makes intuitive sense.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.7. JOINT MOMENTS 537
</p>
<p>To see this we have plotted 50 realizations of the random process in an overlaid
</p>
<p>fashion in Figure 16.15. This representation is called a scatter diagram. Also is
</p>
<p>1.5r-----.-----.-----,---,---,---,-,
</p>
<p>302520105
</p>
<p>-1.5 L-__'--__.L..-__.L..-__.L..-__.L..-__.!-J
</p>
<p>o
</p>
<p>Figure 16.15: Fifty realizations of randomly phased sinusoid plotted in an overlaid
</p>
<p>format with one realization shown with its points connected by straight lines.
</p>
<p>plotted the first realization with the values connected by straight lines for easier
</p>
<p>viewing. The difference in the realizations is due to the different values of phase
</p>
<p>realized. It is seen that for a given time instant the values are nearly symmetric
</p>
<p>about zero, as is predicted by the PDF shown in Figure 16.12 and that the majority
</p>
<p>of the values are near &plusmn;1, again in agreement with the PDF. The MATLAB code
</p>
<p>used to generate Figure 16.15 (but omitting the solid curve) is given below.
</p>
<p>clear all
</p>
<p>rand ( 'state' ,0)
</p>
<p>n=[0:31]' ;
</p>
<p>nreal=50;
</p>
<p>for i=l:nreal
</p>
<p>x(:,i)=cos(2*pi*0.1*n+2*pi*rand(1,1));
</p>
<p>end
</p>
<p>plot (n , x ( : , 1) &bull; ' . ')
</p>
<p>grid
</p>
<p>hold on
</p>
<p>for i=2:nreal
</p>
<p>plot(n,x(: ,i),'. ')
</p>
<p>end
</p>
<p>axis([O 31 -1.5 1.5])</p>
<p/>
</div>
<div class="page"><p/>
<p>538 CHAPTER 16. BASIC RANDOM PROCESSES
</p>
<p>In these three examples the covariance sequence only depends on In2 - nIl. This is
</p>
<p>not always the case, as is illustrated in Problem 16.26. Also, another counterexample
</p>
<p>is the random process whose realization is shown in Figure 16.7b . This random
</p>
<p>process has var(X[n]) = cx[n, n] which is not a function of n2 - nl = n - n = &deg;
since otherwise its variance would be a constant for all n,
</p>
<p>16.8 Real-World Example - Statistical Data Analysis
</p>
<p>It was mentioned in the introduction that some meterologists argue that the annual
</p>
<p>summer rainfall totals are increasing due to global warming. Referring to Figure
</p>
<p>16.1 this supposition asserts that if X[n] is the annual summer rainfall total for year
</p>
<p>n, then J.Lx[n2] &gt; {Lx[nl] for n2 &gt; nl ' One way to attempt to confirm or dispute
this supposition is to assume that J.Lx [n] = an + b and then determine if a &gt; 0, as
would be the case if the mean were increasing. From the data shown in Figure 16.1
</p>
<p>we can estimate a. To do so we let the year 1895, which is the beginning of our data
</p>
<p>set , be indexed as n = &deg;and note that an + b when plotted versus n is a straight
line. We estimate a by fitting a straight line to the data set using a least squares
</p>
<p>procedure [Kay 1993]. The least squares estimate chooses as estimates of a and b
</p>
<p>the values that minimize the least squares error
</p>
<p>N-l
</p>
<p>J(a ,b) = L (x[n] - (an + b))2
n=O
</p>
<p>(16.8)
</p>
<p>where N = 108 for our data set. This approach can be shown to be an optimal
one under the condition that the random process is actually given by X[n] = an +
b+ Urn], where Urn] is a WGN random process [Kay 1993]. Note that if we did
not suspect that the mean rainfall totals were changing, then we might assume that
</p>
<p>J.Lx[n] = b and the least squares estim?,te of b would result from minimizing
</p>
<p>N-l
</p>
<p>J(b) = L (x[n] - b)2 .
n=O
</p>
<p>If we differentiate J(b) with respect to b, set the derivative equal to zero, and solve
</p>
<p>for b, we obtain (see Problem 16.32)
</p>
<p>N-l
A 1 ~
b= N LJx[n]
</p>
<p>n=O
</p>
<p>or b = x, where x is the sample mean, which for our data set is 9.76. Now, however,
we obtain the least squares estimates of a and b by differentiating (16.8) with respect</p>
<p/>
</div>
<div class="page"><p/>
<p>16.8. REAL-WORLD EXAMPLE - STATISTICAL DATA ANALYSIS 539
</p>
<p>to b and a to yield
</p>
<p>oj
ob
</p>
<p>oj
oa
</p>
<p>N-1
</p>
<p>-2 :L (x[n] - an - b) = 0
n=O
</p>
<p>N-1
</p>
<p>-2:L (x[n] - an - b)n = O.
n=O
</p>
<p>This results in two simultaneous linear equations
</p>
<p>N-1
</p>
<p>bN+a :Ln
n=O
</p>
<p>N-1 N-1
</p>
<p>b :Ln+a :Ln2
n=O n=O
</p>
<p>In vectorjmatrix form this is
</p>
<p>N-1
</p>
<p>:Lx[n]
n=O
</p>
<p>N-1
</p>
<p>:Lnx[n].
n=O
</p>
<p>(16.9)[":-1 ~5:~1 n2 ] [ b ] = [ L5~~1 x[n] ]
L..m=O n L..m=O n a Ln:=:o nx[n]
</p>
<p>which is easily solved to yield the estimates band a. For the data of Figure 16.1
the estimates are a = 0.0173 and b = 8.8336. The data along with the estimated
mean sequence itx[n] = 0.0173n + 8.8336 are shown in Figure 16.16. Note that the
</p>
<p>20 r-r- - - ,-- - ,..--- - --,-- - ---r- - ---n
</p>
<p>&lt;Il
QJ 12 .
</p>
<p>-B
</p>
<p>..5 10tllJL!JI~~ttlflHlhtlinl1Jl
8 ; .
</p>
<p>6
</p>
<p>4
</p>
<p>2
1900 1920 1940 1960
</p>
<p>Year
1980 2000
</p>
<p>Figure 16.16: Annual summer rainfall in Rhode Island and the estimated mean
</p>
<p>sequence, itx[n] = 0.0173n + 8.8336, where n = 0 corresponds to the year 1895.
</p>
<p>mean indeed appears to be increasing with time. The least squares error sequence,</p>
<p/>
</div>
<div class="page"><p/>
<p>540 CHAPTER 16. BASIC RANDOM PROCESSES
</p>
<p>which is defined as ern] = x[n] - (im + b), is shown in Figure 16.17. It is sometimes
referred to as the fitting error. Note that the error can be quite large. In fact, we
</p>
<p>10.--------r-----"T---,----,-------r---,
</p>
<p>2
</p>
<p>&middot; . . . .&middot; . . . .
8 ; ; : : : . .
</p>
<p>&middot; . . . .&middot; . . . .
6 ~ : ~ ~.
</p>
<p>. . . ,
&middot; .
</p>
<p>4 &lt; , . . ; ' :'"
&middot; .&middot; .
</p>
<p>- 4 . . ': : ~ ~ .
&middot; . . . .&middot; . . . .
</p>
<p>-6 ; ; : : :. .
&middot; . . . .&middot; . . . .
</p>
<p>- 8 : : : : : . .
&middot; . . . .&middot; . . . .&middot; . . . .
</p>
<p>1008060
n
</p>
<p>4020
</p>
<p>-10 '--__.........__---'- .L.-__--'-__----I.----'
</p>
<p>o
</p>
<p>Figure 16.17: Least squares error sequence for annual summer rainfall in Rhode
</p>
<p>Island fitted with a straight line.
</p>
<p>have that (liN) ~ ~ ' : O l e2[n] = 10.05.
Now the real question is whether the estimated mean increase in rainfall is
</p>
<p>significant. The increase is it = 0.0173 per year for a total increase of about 1.85
</p>
<p>inches over the course of 108 years. Is it possible that the true mean rainfall has
</p>
<p>not changed, or that it is really J.tx[n] = b with the true value of a being zero?
In effect, is the value of it = 0.0173 only due to estimation error? One way to
</p>
<p>answer this question is to hypothesize that a = 0 and then determine the probability
density function of it as obtained from (16.9). This can be done analytically by
</p>
<p>assuming X[n] = b+Urn], where Urn] is white Gaussian noise (see Problem 16.33).
However, we can gain some quick insight into the problem by resorting to a computer
</p>
<p>simulation. To do so we assume that the true model for the rainfall data is X[n] =
b+ Urn] = 9.76 + Urn] , where Urn] is white Gaussian noise with variance (72. Since
we do not know the value of (72, we estimate it by using the results shown in Figure
</p>
<p>16.17. The least squares error sequence ern], which is the original data with its
estimated mean sequence subtracted, should then be an estimate of Urn]. Therefore,
</p>
<p>we use ;2 = (liN) ~ ~ ' : O l e2[n] = 10.05 in our simulation. In summary, we generate
20 realizations of the random process X[n] = 9.76 + Urn]' where Urn] is WGN with
(72 = 10.05. Then, we use (16.9) to estimate a and b and finally we plot our mean
sequence estimate, which is flx[n] = itn+b for each realization. Using the MATLAB
code shown at the end of this section, the results are shown in Figure 16.18. It is
</p>
<p>seen that even though the true value of a is zero , the estimated value will take on
</p>
<p>nonzero values with a high probability. Since some of the lines are decreasing, some</p>
<p/>
</div>
<div class="page"><p/>
<p>16.8. REAL-WORLD EXAMPLE - STATISTICAL DATA ANALYSIS 541
</p>
<p>15,------.------,----.,-----,------,-----,
</p>
<p>14
</p>
<p>13
</p>
<p>12 .
</p>
<p>~ 11 ;;~~:~ .~~~~ . : .~ .~~ .~ :~ .::~.;.;.; .== .=~
( ~ 1 0 iii 'iIi " &middot;:&middot;1&middot;.&middot;. G . ., .J.: ..;:: . ~ . ... ._._.- ...-e ..... ~
</p>
<p>iii .;: = :Iii ;; &bull; &bull; -:::. _'ii 11 C iii ~ -: ; !i! - :::
9 __ . : . . : . _ . - . : : : . . ~ . ~ . ~ : ~ .':"':~ .~ .~~ .5 :E ..
</p>
<p>~:::: :: ==:- . . . : -
8 .
</p>
<p>1008060
n
</p>
<p>4020
</p>
<p>7
</p>
<p>6
</p>
<p>5'------'-------'-----'------'---..........---'
o
</p>
<p>Figure 16.18: Twenty realizations of the estimated mean sequence {tx[n] = an + b
based on the random process X[n] = 9.76 + Urn] with Urn] being WGN with 0'2 =
10.05. The realizations are shown as dashed lines . The estimated mean sequence
</p>
<p>from Figure 16.16 is shown as the solid line .
</p>
<p>of the estimated values of a are even negative. Hence, we would be hard pressed to
</p>
<p>say that the mean rainfall totals are indeed increasing. Such is the quandry that
</p>
<p>scientists must deal with on an everyday basis. The only way out of this dilemma is
</p>
<p>to accumulate more data so that hopefully our estimate of a will be more accurate
</p>
<p>(see also Problem 16.34).
</p>
<p>clear all
</p>
<p>randn('state' ,0)
</p>
<p>years=[1895:2002]';
</p>
<p>N=length(years);
</p>
<p>n=[0:N-1]';
</p>
<p>A=[N sum(n);sum(n) sum(n.-2)]; %precompute matrix (see (16 .9))
B=inv(A); % invert matrix
for i=1:20
</p>
<p>xn=9.76+sqrt(10 .05)*randn(N,1); %generate realizations
baest=B*[sum(xn);sum(n .*xn)]; %estimate a and busing (16.9)
aest=baest(2);best=baest(1);
</p>
<p>meanest(:,i)=aest*n+best; %determine mean sequence estimate
end
</p>
<p>figure %plot mean sequence estimates and overlay
plot(n,meanest(:,l))
</p>
<p>grid
</p>
<p>xlabel('n')</p>
<p/>
</div>
<div class="page"><p/>
<p>542
</p>
<p>ylabel('Estimated mean')
</p>
<p>axis([O 107 5 15])
</p>
<p>hold on
</p>
<p>for i=2:20
</p>
<p>plot(n,meanest(:,i))
</p>
<p>end
</p>
<p>References
</p>
<p>CHAPTER 16. BASIC RANDOM PROCESSES
</p>
<p>Billingsley, P., Probability and Measure , John Wiley &amp; Sons, New York, 1986.
</p>
<p>DowJones.com, "DowJones Averages," http://averages.dowjones.com/jsp/
</p>
<p>uiHistoricalIndexRep.jsp, 2004.
</p>
<p>Kay, S., Fundamentals of Statistical Signal Processing: Estimation Theory, Prentice-
</p>
<p>Hall, Englewood Cliffs , NJ, 1993.
</p>
<p>Problems
</p>
<p>16.1 C:..:.-) (w) Describe a random process that you are likely to encounter in the
following situations:
</p>
<p>a. listening to the daily weather forecast
</p>
<p>b. paying the monthly telephone bill
</p>
<p>c. leaving for work in the morning
</p>
<p>Why is each process a random one?
</p>
<p>16.2 (w) A single die is tossed repeatedly. What are Sand Sx? Also, can you
determine the joint PMF for any N sample times?
</p>
<p>16.3 (t) An infinite sequence of O's and l 's, denoted as bl , b2,"" can be used to
represent any number x in the interval [0, 1] using the binary representation
</p>
<p>formula
00
</p>
<p>x = 2:biTi .
</p>
<p>i=1
</p>
<p>For example, we can represent 3/4 as 0.bIb2 ..&bull; = 0.11000 ... and 1/16 as
</p>
<p>0.bIb2 &bull; . &bull; = 0.0001000 .... Find the representations for 7/8 and 5/8. Is the
</p>
<p>total number of infinite sequences of O's and l 's countable?
</p>
<p>16.4 C:...:...) (w) For a Bernoulli random process determine the probability that we
will observe an alternating sequence of 1's and O's for the first 100 samples
</p>
<p>with the first sample being a 1. What is the probability that we will observe
</p>
<p>an alternating sequence of l's and O's for all n?</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 543
</p>
<p>16.5 (w) Classify the following random processes as either DTDV, DTCV, CTDV,
</p>
<p>or CTCV:
</p>
<p>a. temperature in Rhode Island
</p>
<p>b. outcomes for continued spins of a roulette wheel
</p>
<p>c. daily weight of person
</p>
<p>d. number of cars stopped at an intersection
</p>
<p>16.6 (c) Simulate a realization of the random walk process described in Example
</p>
<p>16.2 on a computer. What happens as n becomes large?
</p>
<p>16.7 C:...:..-) (c,f) A biased random walk process is defined as X[n] = L:?=o Uri], where
Uri] is a Bernoulli random process with
</p>
<p>{
t k =-1
</p>
<p>pu[k] = i k = 1.
</p>
<p>What is E[X[n]] and var(X[nJ) as a function of n? Next, simulate on a
computer a realization of this random process. What happens as n -+ 00 and
</p>
<p>why?
</p>
<p>16.8 (w) A random process X[n] is stationary. If it is known that E[X[10J] = 10
</p>
<p>and var(X[10J) = 1, then determine E[X[100J] and var(X[100J).
</p>
<p>16.9 (..:.:.,,) (f) The IID random process X[n] has the marginal PDF
</p>
<p>px(x) = exp( -x)u(x). What is the probability that X [0],X[I], X[2] will all
be greater than I?
</p>
<p>16.10 (w) If an IID random process X[n] is transformed to the random process
Y[n] = X 2[n], is the transformed random process also IID?
</p>
<p>16.11 (w) A Bernoulli random process X[n] that takes on values 0 or 1, each with
probability of p = 1/2, is transformed using Y[n] = (_I)nX[n]. Is the random
process Y[n] IID?
</p>
<p>16.12 (w,f) A nonstationary random process is defined as X[n] = a1nIU[n] , where
o&lt; a &lt; 1 and Urn] is WGN with variance crb. Find the mean and covariance
sequences of X[n]. Can you transform the X[n] random process to make it
</p>
<p>stationary?
</p>
<p>16.13 (..:.:.,,) (w) Consider the random process X[n] = L:?=o Uri], which is defined
for n ~ O. The Urn] random process consists of independent Gaussian ran-
</p>
<p>dom variables with marginal PDF Urn] ,...... N(O, (1/2)n). Are the increments
independent? Are the increments stationary?</p>
<p/>
</div>
<div class="page"><p/>
<p>544 CHAPTER 16. BASIC RANDOM PROCESSES
</p>
<p>16.14 (c) Plot 50 realizations of a WGN random process X[n] with (J2 = 1 for
n = 0, 1, . . . , 49 using a scatter diagram (see Figure 16.15 for an example). Use
</p>
<p>the MATLAB commands plot (x , y , ' . ') and hold on to plot each realization
</p>
<p>as dots and to overlay the realizations on the same graph, respectively. For a
</p>
<p>fixed n can you explain the observed distribution of the dots?
</p>
<p>16.15 (f) Prove that
1
</p>
<p>---------:--;::-- exp (_1x T C -Ix)
(21r)N/2 detI/2(C) 2
</p>
<p>where x = [Xl X2 &bull;&bull;&bull; xN]T and C = (J21 for I an N x N identity matrix, reduces
to (16.4).
</p>
<p>16.16 C..:.,) (f) A "white" uniform random process is defined to be an IID random
process with X[n] rv U( -y'3, y'3) for all n. Determine the mean and covari-
ance sequences for this random process and compare them to those of the
</p>
<p>WGN random process. Explain your results.
</p>
<p>16.17 (w) A moving average random process can be defined more generally as one
</p>
<p>for which N samples of WGN are averaged, instead of only N = 2 samples as
</p>
<p>in Example 16.7. It is given by X[n] = (liN) I:{':c/ Urn - i] for all n, where
Urn] is a WGN random process with variance ( J ~ . Determine the correlation
coefficient for X[O] and X[I] . What happens as N increases?
</p>
<p>16.18 C..:.,,) (f) For the moving average random process defined in Example 16.7
determine P[X[n] &gt; 3] and compare it to P[U[n] &gt; 3]. Explain the difference
in terms of "smoothing". Assume that ( J ~ = 1.
</p>
<p>16.19 (c) For the randomly phased sinusoid defined in Example 16.8 determine the
</p>
<p>mean sequence using a computer simulation.
</p>
<p>16.20 (t) For the randomly phased sinusoid of Example 16.8 assume that the real-
ization x[n] = cos(21r(0.I)n+0) is generated. Prove that if we observe only the
</p>
<p>samples x[O] = 1 and x[l] = cos(21r(0.1)) = 0.8090, then all the future samples
can be found by using the recursive formula x[n] = 2cos(21r(0.1))x[n - 1] -
</p>
<p>x[n - 2] for n ~ 2. Could you also find the past samples or x[n] for n :s; -I?
See also Problem 18.25 for prediction of a sinusoidal random process.
</p>
<p>16.21 (c) Verify the PDF of the randomly phased sinusoid given in Figure 16.12
</p>
<p>by using a computer simulation.
</p>
<p>16.22 C:..:,/) (f,c) A continuous-time random process known as the random am-
plitude sinusoid is defined as X(t) = A cos(21rt) for -00 &lt; t &lt; 00 and
A rv N(O,I). Find the mean and covariance functions. Then, plot some
</p>
<p>realizations of X (t) in an overlaid fashion.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 545
</p>
<p>16.23 (f) A random process is the sum of WGN and a deterministic sinusoid and is
</p>
<p>given as X[n] = Urn] +sin(27rfon) for all n, where Urn] is WGN with variance
O"b. Determine the mean and covariance sequences.
</p>
<p>16.24 ( . . . : . ~ ) (w) A random process is IID with samples X[n] '" N(J.L , 1). It is desired
to remove the mean of the random process by forming the new random process
</p>
<p>Y [n] = X [n] - X [n- 1]. First determine the mean sequence of Y [n]. Next find
cov(Y[O], Y[l]). Is Y[n] an lID random process with a zero mean sequence?
</p>
<p>16.25 (f) If a random process is defined as X[n] = h[0]U[n]+h[1]U[n-1], where h[O]
and h[l] are constants and Urn] is WGN with variance O"b , find the covariance
for X[O] and X[l]. Repeat for X[9] and X[lO]. How do they compare?
</p>
<p>16.26 C : . ~ ) (f) If a sum random process is defined as X[n] = 2::~=0 Uri] for n ~ 0,
where E[U[i]] = 0 and var(U[i]) = O"b for i ~ 0 and the Uri] are lID, find the
mean and covariance sequences of X [n].
</p>
<p>16.27 C:...:J (c) For the MA random process defined in Example 16.7 find cx[l, 1],
cx[1,2] and cx[1,3] if O"b = 1. Next simulate on a computer M = 10,000
realizations of the random process X[n] for n = 0,1, . . . ,10 . Estimate the pre-
vious covariance sequence samples using cx[n1' n2] = (11M) 2::~ 1 xi[n1]xi[n2],
where xi [n] is the ith realization of X[n]. Note that since X[n] is zero mean,
</p>
<p>cx[n1' n2] = E[X[n1]X[n2]].
</p>
<p>16.28 (w) For the randomly phased sinusoid described in Example 16.11 determine
</p>
<p>the minimum mean square estimate of X[10] based on observing x[O]. How
accurate do you think this prediction will be?
</p>
<p>16.29 (f) For a random process X[n] the mean sequence J.Lx[n] and covariance
sequence cx[n1,n2] are known. It is desired to predict k samples into the
future. If x[no] is observed, find the minimum mean square estimate of X[no+
k]. Next assume that J.Lx[n] = cos(27rfon) and cx[n1' n2] = 0.9 In 2- n l! and
evaluate the estimate. Finally, what happens to your prediction as k -+ 00
and why?
</p>
<p>16.30 (f) A random process is defined as X[n] = As[n] for all n, where A", N(o, 1)
and s[n] is a deterministic signal. Find the mean and covariance sequences.
</p>
<p>16.31 C:..:J (f) A random process is defined as X[n] = AU[n] for all n, where A '"
N(O, O"~) and Urn] is WGN with variance O"b, and A is independent of Urn] for
all n . Find the mean and covariance sequences. What type of random process
</p>
<p>is X[n]?
</p>
<p>16.32 (f) Verify that by differentiating 2::::01(x[n] - b)2 with respect to b, setting
the derivative equal to zero , and solving for b, we obtain the sample mean.</p>
<p/>
</div>
<div class="page"><p/>
<p>546 CHAPTER 16. BASIC RANDOM PROCESSES
</p>
<p>16.33 (t) In this problem we show how to obtain the variance of a as obtained
by solving (16.9). The variance of a is derived under the assumption that
X[n] = b+ Urn], where Urn] is WGN with variance a 2 . This says that we
assume the true value of a is zero. The steps are as follows:
</p>
<p>a. Let
1 0 X [0]
</p>
<p>1 1 X[l]
</p>
<p>H= 1 2 X= X[2]
</p>
<p>1 N-1 X[N-1]
</p>
<p>where H is an N x 2 matrix and X is an N x 1 random vector. Now
</p>
<p>show that that the equations of (16.9) can be written as
</p>
<p>b. The solution for b and a can now be written symbolically as
</p>
<p>Since X is a Gaussian random vector, show that [ba]T is also a Gaussian
random vector with mean [b 0]T and covariance matrix a 2 (HTH) -1 .
</p>
<p>c. As a result we can assert that the marginal PDF of a is Gaussian with
mean zero and variance equal to the (2,2) element of a 2(H T H )- 1. Show
</p>
<p>then that a rv N(o, var(a)), where
</p>
<p>Next assume that a 2 = 10.05, N = 108 and find the probability that a &gt;
0.0173. Can we assert that the estimated mean sequence shown in Figure
</p>
<p>16.16 is not just due to estimation error?
</p>
<p>16.34 C:..:...) (f) Using the results of Problem 16.33 determine the required value of
N so that the probability that a&gt; 0.0173 is less than 10-6 .</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 17
</p>
<p>Wide Sense Stationary Random
</p>
<p>Processes
</p>
<p>17.1 Introduction
</p>
<p>Having introduced the concept of a random process in the previous chapter, we
</p>
<p>now wish to explore an important subclass of stationary random processes. This is
</p>
<p>motivated by the very restrictive nature of the stationarity condition, which although
</p>
<p>mathematically expedient, is almost never satisfied in practice. A somewhat weaker
</p>
<p>type of stat ionarity is based on requiring the mean to be a constant in time and
</p>
<p>the covariance sequence to depend only on the separation in time between the two
</p>
<p>samples. We have already encountered these types of random processes in Examples
</p>
<p>16.9-16.11. Such a random process is said to be stationary in the wide sense or wide
</p>
<p>sense stationary (WSS). It is also termed a weakly stationary random process to
</p>
<p>distinguish it from a stationary process, which is said to be strictly stationary. We
</p>
<p>will use the form er terminology to refer to such a process as a WSS random process.
</p>
<p>In addition, as we will see in Chapter 19, if the random process is Gaussian, then
</p>
<p>wide sense stationarity implies stationarity. For this reason alone , it makes sense
</p>
<p>to explore WSS random processes since the use of Gaussian random processes for
</p>
<p>modeling is ubiquitous.
</p>
<p>Once we have discussed the concept of a WSS random process, we will be able
</p>
<p>to define an extremely important measure of the WSS random process-the power
</p>
<p>spectral density (PSD). This function extends the idea of analyzing the behavior of a
</p>
<p>deterministic signal by decomposing it into a sum of sinusoids of different frequencies
</p>
<p>to that of a random process. The difference now is that the amplitudes and phases
</p>
<p>of the sinusoids will be random variables and so it will be convenient to quantify the
</p>
<p>average power of the various sinusoids. This description of a random phenomenon
</p>
<p>is important in nearly every scientific field that is concerned with the analysis of
</p>
<p>time series data such as systems control [Box and Jenkins 1970], signal processing
</p>
<p>[Schwartz and Shaw 1975], economics [Harvey 1989], geophysics [Robinson 1967],</p>
<p/>
</div>
<div class="page"><p/>
<p>548 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>vibration testing [McConnell 1995], financial analysis [Taylor 1986], and others. As
</p>
<p>an example, in Figure 17.1 the Wolfer sunspot data [Tong 1990] is shown, with the
</p>
<p>data points connected by straight lines for easier viewing. It measures the average
</p>
<p>number of sunspots visually observed through a telescope each year. The importance
</p>
<p>of the sunspot number is that as it increases, an increase in solar flares occurs. This
</p>
<p>has the effect of disrupting all radio communications as the solar flare particles reach
</p>
<p>the earth. Clearly from the data we see a periodic type property. The estimated
</p>
<p>PSD of this data set is shown in Figure 17.2. We see that the distribution of power
</p>
<p>versus frequency is highest at a frequency of about 0.09 cycles per year. This means
</p>
<p>that the random process exhibits a large periodic component with a period of about
</p>
<p>1/0.09 ~ 11 years per cycle, as is also evident from Figure 17.1. This is a powerful
</p>
<p>prediction tool and therefore is of great interest. How the PSD is actually estimated
</p>
<p>will be discussed in this chapter, but before doing so, we will need to lay some
</p>
<p>groundwork.
</p>
<p>200,----.----.----.----.----.--------,
</p>
<p>180
</p>
<p>160 .
</p>
<p>1il
"S 140
a120 . . ..
...,
8. 100
o:
</p>
<p>&sect; 80
00
</p>
<p>60
</p>
<p>40
</p>
<p>20
</p>
<p>o \J
1700 1750
</p>
<p>\J
</p>
<p>1800 1850 1900
Year
</p>
<p>1950 2000
</p>
<p>17.2
</p>
<p>Figure 17.1: Annual number of sunspots - Wolfer sunspot data.
</p>
<p>Summary
</p>
<p>A less restrictive form of stationarity, termed wide sense stationarity, is defined by
</p>
<p>(17.4) and (17.5). The conditions require the mean to be the same for all n and the
</p>
<p>covariance sequence to depend only on the time difference between the samples. A
</p>
<p>random process that is stationary is also wide sense stationary as shown in Section
</p>
<p>17.3. The autocorrelation sequence is defined by (17.9) with n being arbitrary. It
</p>
<p>is the covariance between two samples separated by k units for a zero mean WSS
</p>
<p>random process. Some of its properties are summarized by Properties 17.1-17.4.
</p>
<p>Under certain conditions the mean of a WSS random process can be found by using</p>
<p/>
</div>
<div class="page"><p/>
<p>17.3. DEFINITION OF WSS RANDOM PROCESS
</p>
<p>X 10
4
</p>
<p>7r-------.----,--------r-----,--------,
</p>
<p>6
</p>
<p>5 . .
</p>
<p>0 4 . -...
sr:
0..
</p>
<p>3 .
</p>
<p>2
</p>
<p>o 0.1 0.2 0.3 0.4 0.5
Frequency (cycles per year)
</p>
<p>549
</p>
<p>Figure 17.2: Estimated power spectral density for Wolfer sunspot data of Figure
</p>
<p>17.1. The sample mean has been computed and removed from the data prior to
</p>
<p>estimation of the PSD.
</p>
<p>the temporal average of (17.25). Such a process is said to be ergodic in the mean. For
</p>
<p>this to be true the variance of the temporal average given by (17.28) must converge
</p>
<p>to zero as the number of samples averaged becomes large. The power spectral
</p>
<p>density (PSD) of a WSS random process is defined by (17.30) and can be evaluated
</p>
<p>more simply using (17.34). The latter relationship says that the PSD is the Fourier
</p>
<p>transform of the autocorrelation sequence. It measures the amount of average power
</p>
<p>per unit frequency or the distribution of average power with frequency. Some of its
</p>
<p>properties are summarized in Properties 17.7-17.12. From a finite segment of a
</p>
<p>realization of the random process the autocorrelation sequence can be estimated
</p>
<p>using (17.43) and the PSD can be estimated by using the averaged periodogram
</p>
<p>estimate of (17.44) and (17.45). The analogous definitions for a continuous-time
</p>
<p>WSS random process are given in Section 17.8. Also, an important example is
</p>
<p>described that relates sampled continuous-time white Gaussian noise to discrete-
</p>
<p>time white Gaussian noise. Finally, an application of the use of PSDs to random
</p>
<p>vibration testing is given in Section 17.9.
</p>
<p>17.3 Definition of WSS Random Process
</p>
<p>Consider a discrete-time random process X[n] , which is defined for -00 &lt; n &lt; 00
with n an integer. Previously, we defined the mean and covariance sequences of</p>
<p/>
</div>
<div class="page"><p/>
<p>X[n] to be
</p>
<p>JLx[n] E[X[n]] - 00 &lt; n &lt; 00
</p>
<p>cx[nI' n2] = E[(X[nI] - JLx [nI])(X[n2] - JLx[n2])]
</p>
<p>550 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>(17.1)
</p>
<p>-00 &lt; ni &lt; 00 (17.2)
-00 &lt; n2 &lt; 00
</p>
<p>where nI, n2 are integers. Having knowledge of these sequences allows us to assess
important characteristics of the random process such as the mean level and the
</p>
<p>correlation between samples. In fact, based on only this information we are able to
</p>
<p>predict X[n2] based on observing X[nI] = x[nI] as
</p>
<p>(17.3)
</p>
<p>which is just the usual linear prediction formula of (7.41) with x replaced by x[nI]
and Y replaced by X[n2], and which makes use of the mean and covariance sequences
defined in (17.1) and (17.2), respectively. However, since in general the mean and
</p>
<p>covariance change with time, i.e., they are nonstationary, it would be exceedingly
</p>
<p>difficult to estimate them in practice. To extend the practical utility we would like
</p>
<p>the mean not to depend on time and the covariance only to depend on the separation
</p>
<p>between samples or on In2 - nIl. This will allow us to estimate these quantities as
</p>
<p>described later. Thus, we are led to a weaker form of stationarity known as wide
</p>
<p>sense stationarity. A random process is defined to be WSS if
</p>
<p>JLx[n]
</p>
<p>cx[nI ,n2] =
</p>
<p>JL (a constant)
</p>
<p>g(ln 2 - nIl)
</p>
<p>- 00 &lt; n &lt; 00 (17.4)
</p>
<p>- 00 &lt; ni &lt; 00, -00 &lt; n2 &lt; 00 (17.5)
</p>
<p>for some function g. Note that since
</p>
<p>these conditions are equivalent to requiring that X[n] satisfy
</p>
<p>E[X[n]] =
</p>
<p>E[X[nI]X[n2]]
</p>
<p>-oo&lt;n&lt;oo
</p>
<p>- 00 &lt; ni &lt; 00, -00 &lt; n2 &lt; 00
</p>
<p>for some function h. The mean should not depend on time and the average value
</p>
<p>of the product of two samples should depend only upon the time interval between
</p>
<p>the samples. Some examples of WSS random processes have already been given in
</p>
<p>Examples 16.9-16.11. For the MA process of Example 16.10 we showed that
</p>
<p>JLx[n] 0 -oo&lt;n&lt;oo
</p>
<p>{
1(J2 In2 - nIl = 02 U
</p>
<p>cx[nI , n2] = 1(J2 In2 - nIl = 14 U
0 jn2 - nIl&gt; 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>17.3. DEFINITION OF WSS RANDOM PROCESS 551
</p>
<p>It is seen that every random variable X[n] for -00 &lt; n &lt; 00 has a mean of zero
and the covariance for two samples depends only on the time interval between the
</p>
<p>samples, which is In2 - nIl . Also, this implies that the variance does not depend
</p>
<p>on time since var(X[n]) = cx [n, n] = (j~/2 for -00 &lt; n &lt; 00. In contrast to this
behavior consider the random processes for which typical realizations are shown in
</p>
<p>Figure 16.7. In Figure 16.7a the mean changes with time (with the variance being
</p>
<p>constant) and in Figure 16.7b the variance changes with time (with the mean being
</p>
<p>constant). Clearly, these random processes are not WSS.
</p>
<p>A WSS random process is a special case of a stationary random process. To see
</p>
<p>this recall that if X[n] is stationary, then from (16.3) with N = 1 and nl = n, we
have
</p>
<p>PX[n+no] = PX[n] for all n and for all no.
</p>
<p>As a consequence, if we let n = 0, then
</p>
<p>PX[no] = PX[O] for all no
</p>
<p>and since the PDF does not depend on the particular time no, the mean must not
</p>
<p>depend on time. Thus,
</p>
<p>/-lx[n] = /-l
</p>
<p>Next, using (16.3) with N = 2, we have
</p>
<p>- 00 &lt; n &lt; 00. (17.6)
</p>
<p>PX[nl +no],X[n2+nol = PX[nI] ,X[n2]
</p>
<p>Now ifno = -nl we have from (17.7)
</p>
<p>(17.7)
</p>
<p>and if no = -n2, we have
</p>
<p>This results in
</p>
<p>which leads to
</p>
<p>PX[nI],X[n21
</p>
<p>PX[nI] ,X[n21
</p>
<p>PX[O] ,X[n2-nI]
</p>
<p>PX[nl-n2] ,X[O]
</p>
<p>E[X[nl]X[n2]]
</p>
<p>E[X[nl]X[n2]]
</p>
<p>E[X[0]X[n2 - nl]]
</p>
<p>E[X[nl - n2]X[0]] = E[X[O]X[nl - n2]].
</p>
<p>Finally, these two conditions combine to give
</p>
<p>(17.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>552 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>which along with the mean being constant with t ime yields the second condit ion for
</p>
<p>wide sense stat ionarity of (17.5) that
</p>
<p>cx [n l ' nz] = E [X [n l ]X [nz]] - E[X[nl]]E[X[nz]] = E[X[O]X[lnz - n Il] - J-Lz .
</p>
<p>This proves the assert ion that a st ationary random process is WSS but the converse
</p>
<p>is not generally true (see Problem 17.5).
</p>
<p>17.4 Autocorrelation Sequence
</p>
<p>If X[n] is WSS , then as we have seen E[X[nl]X[nz]] depends only on the separa t ion
</p>
<p>in t ime between the samples. We can therefore define a new joint moment by letting
</p>
<p>nl = n and nz = n + k to yield
</p>
<p>r x [k] = E[X[n]X[n + k]] (17.9)
</p>
<p>which is called the autocorrelation sequence (ACS). It depends only on the t ime
</p>
<p>difference between samples which is Inz -nIl = I(n+k) -nl = Ikl so that the value
</p>
<p>of n used in the definition is arbitrary. It is termed the autocorrelation sequence
</p>
<p>(ACS) since it measures the correlation between two samples of the same random
</p>
<p>process. Later we will have occasion to define correlat ion between two different
</p>
<p>random processes (see Section 19.3). Note that the time interval between samples
</p>
<p>is also called the lag. An example of the computation of the ACS is given next .
</p>
<p>Example 17.1 - A Differencer
</p>
<p>Define a random pro cess as X[n] = Urn] - urn - 1]' where Urn] is an IID random
</p>
<p>process with mean J-L and vari ance 1Jf; . A realization of this random process for which
U rn] is a Gaussian random vari able for all n is shown in Figure 17.3. Although
</p>
<p>U rn] was chosen here to be a sequence of Gaussian random variables for the sake
</p>
<p>of displaying the realization in Figure 17.3, the ACS to be found will be the same
</p>
<p>regardless of the PDF of Urn] . This is because it relies on only thefirst two moments
</p>
<p>of Urn] and not its PDF. The ACS is found as
</p>
<p>r x [k] E[X[n]X[n + k]]
E[(U[n] - Urn - l]) (U[n + k] - Urn + k - 1])]
E[U[n]U[n + k]] - E[U[n]U[n + k - 1]]
</p>
<p>- E[U[n - l]U[n + k]] + E[U[n - l] U[n + k - 1]] .
</p>
<p>and for nl = nz = n
</p>
<p>E[U[nl]]E[U[nz]]
</p>
<p>J-Lz
</p>
<p>(independence)
</p>
<p>E[U[n l]U[nz]] = E[Uz[n]] = E[Uz[O]] = 1Jf; + J-Lz (identic ally distributed) .</p>
<p/>
</div>
<div class="page"><p/>
<p>17.4. AUTOCORRELATION SEQUENCE 553
</p>
<p>3,-----..,.-----,------,----.-----.------,
</p>
<p>2
</p>
<p>-1
</p>
<p>-2 .
</p>
<p>302520105
-3 '--_----'__--L__----'-__-'-__-'--.::....--l
</p>
<p>o
</p>
<p>Figure 17.3: Typical realization of a differenced IID Gaussian random process with
</p>
<p>U[n] '" N(l , 1).
</p>
<p>Combining these results we have that
</p>
<p>and therefore the ACS becomes
</p>
<p>(17.10)
</p>
<p>This is shown in Figure 17.4. Several observations can be made. The only nonzero
</p>
<p>correlat ion is between adjacent samples and this correlation is negative. This ac-
</p>
<p>counts for the observation that the realization shown in Figure 17.3 exhibits many
</p>
<p>adjacent samples that are opposite in sign. Some other observations are that
</p>
<p>rx[O] &gt; 0, Irx[k]I ~ rx[O] for all k, and finally rx[-k] = rx[k]. In words, the
ACS has a maximum at k = 0, which is positive, and is a symmetric sequence about
k = 0 (also called an even sequence). These properties hold in general as we now
prove.
</p>
<p>Property 17.1 - ACS is positive for the zero lag or rx[O] &gt; O.
Proof:
</p>
<p>rx[k] = E[X[n]X[n + k]] (definition)
</p>
<p>so that with k = 0 we have rx[O] = E[X2[n]] &gt; O.
D
</p>
<p>Note that rx[O] is the average power of the random process at all sample times
</p>
<p>n. One can view X[n] as the voltage across a 1 ohm resistor and hence x2 [n]/ 1</p>
<p/>
</div>
<div class="page"><p/>
<p>554 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>[ .
-ab
</p>
<p>-5 -4 -3 -2 -1 0
k
</p>
<p>2 3 4 5
</p>
<p>Figure 17.4: Autocorrelation sequence for differenced random process.
</p>
<p>is the power for any particular realization of X[n] at time n. The average power
</p>
<p>E[X2[n]] = r x [O] does not change with time.
</p>
<p>Property 17.2 - ACS is an even sequence or rx[-k] = r x [k].
</p>
<p>Proof:
</p>
<p>r x [k]
</p>
<p>r x [- k]
</p>
<p>E[X[n]X[n + k]]
E[X[n]X[n - k]]
</p>
<p>(definition)
</p>
<p>and letting n = m + k since the choice of n in the definit ion of the ACS is arbit rary,
we have
</p>
<p>r x [- k] E[X[m + k]X[m]]
E[X[m]X[m + k]]
E[X[n]X[n + k]] (ACS not dependent on n)
rx[k] .
</p>
<p>D
</p>
<p>Property 17.3 - Maximum absolute value of ACS is at k = 0 or Irx[k]l :S
r x [O] .
</p>
<p>Note that it is possible for some values of rx[k] for k =1= 0 to also equal r x [O]. As an
</p>
<p>example, for the randomly phased sinusoid of Example 16.11 we had ex [nI, n2] =
! cos[21T(0.1)(n2 - nl)] with a mean of zero. Thus, rx[k] = ! cos [21T(0.1)k] and
therefore rx[lO] = r x [O]. Hence, the property says that no value of the ACS can
exceed rx [0], although there may be multiple values of the ACS that are equal to
rx [O].</p>
<p/>
</div>
<div class="page"><p/>
<p>17.4. AUTOCORRELATION SEQUENCE 555
</p>
<p>Proof: The proof is based on the Cauchy-Schwarz inequality, which from Appendix
</p>
<p>7A is
</p>
<p>IEv,w[VW]I::; J EV[V2]JEw[W2]
</p>
<p>with equality holding if and only if W = cV for c a constant. Letting V = X[n] and
W = X[n + k], we have
</p>
<p>IE[X[n]X[n + k]]1 ::; JE[X2[n]]JE[X2[n + k]]
</p>
<p>from which it follows that
</p>
<p>Irx[k]1 &lt; Jrx[O]Jrx[O] = Irx [0] I = rx[O] (since rx[O] &gt; 0).
</p>
<p>Note that equality holds if and only if X[n + k] = cX[n] for all n. This implies
perfect predictability of a sample based on the realization of another sample spaced
</p>
<p>k units ahead or behind in time (see Problem 17.10 for an example involving periodic
</p>
<p>random processes).
</p>
<p>o
</p>
<p>Property 17.4 - ACS measures the predictability of a random process.
</p>
<p>The correlation coefficient for two samples of a zero mean WSS random process is
</p>
<p>rx[k]
PX[n],X[n+k) = rx[O] (17.11)
</p>
<p>For a nonzero mean the expression is easily modified (see Problem 17.11).
</p>
<p>Proof: Recall that the correlation coefficient for two random variables V and W
</p>
<p>is defined as
cov(V, W)
</p>
<p>pv: w = -r=:;:::::'=.=:::;::::===.=
, vvar(V)var(W)
</p>
<p>Assuming that V and Ware zero mean, this becomes
</p>
<p>_ Ev,w[VW]
pv,w - J EV[V2]Ew[W2]
</p>
<p>and letting V = X[n] and W = X[n + k], we have
</p>
<p>(from Property 17.1).
</p>
<p>PX[n),X[n+k] =
E[X[n]X[n + k]]
</p>
<p>JE[X2[n]]E[X2[n + k]]
rx[k]
</p>
<p>.s: [O]rx [0]
rx[k]
</p>
<p>Irx [0] I
rx[k]
</p>
<p>rx[O]
</p>
<p>o</p>
<p/>
</div>
<div class="page"><p/>
<p>556 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>As an example, for the differencer of Example 17.1 we have from Figure 17.4
</p>
<p>{
</p>
<p>I k = 0
</p>
<p>PX[n],X[n+k] = -~ k = &plusmn;1
o otherwise.
</p>
<p>As mentioned previously, the adjacent samples are negatively correlated and the
</p>
<p>magnitude of the correlation coefficient is now seen to be 1/2.
</p>
<p>We next give some more examples of the computation of the ACS.
</p>
<p>Example 17.2 - White noise
</p>
<p>White noise is defined as a WSS random process with zero mean, identical variance
</p>
<p>0'2, and uncorrelated samples. It is a more general case of the white noise random
</p>
<p>process first described in Example 16.9. There we assumed the stronger condition
</p>
<p>of zero mean lID samples (hence they must have the same variance due to the
</p>
<p>identically distributed assumption and also be uncorrelated due to the independence
</p>
<p>assumption) . In addition, it was assumed there that each sample had a Gaussian
</p>
<p>PDF. Note, however, that the definition given above for white noise does not specify
</p>
<p>a particular PDF. To find the ACS we note that from the definition of the white
</p>
<p>noise random process
</p>
<p>rx[k] E[X[n]X[n + k]]
= E[X[n]]E[X[n + k]] = 0 k =I 0
</p>
<p>Therefore, we have that
</p>
<p>rx[k] = O'28[k].
</p>
<p>Could you predict X[l] from a realization of X[O]?
</p>
<p>(uncorrelated and
</p>
<p>zero mean samples)
</p>
<p>(equal variance samples).
</p>
<p>(17.12)
</p>
<p>o
As an aside, for WSS random processes, we can find the covariance sequence from
</p>
<p>the ACS and the mean since
</p>
<p>E[X[nl]X[n2]] - tsx [nl]J,tx [n2]
</p>
<p>rx[n2 - nIl - J,t2. (17.13)
</p>
<p>Another property of the ACS that is evident from (17.13) concerns the behavior of
</p>
<p>the ACS as k -t 00. Letting nl = nand n2 = n + k, we have that
</p>
<p>rx[k] = cx[n,n + k] + J,t2. (17.14)
</p>
<p>If two samples becomes uncorrelated or cx[n, n + k] -t 0 as k -t 00, then we see
that rx[k] -+ J,t2 as k -t 00. Thus, as another property of the ACS we have the
following.</p>
<p/>
</div>
<div class="page"><p/>
<p>17.4. AUTOCORRELATION SEQUENCE 557
</p>
<p>Property 17.5 - ACS approaches J.L2 as k -t 00
</p>
<p>This assumes that the samples become uncorrelated for large lags, which is usually
</p>
<p>the case.
o
</p>
<p>If the mean is zero , then from (17.14)
</p>
<p>rx[k] = cx[n,n + k] (17.15)
</p>
<p>and the ACS approaches zero as the lag increases. We continue with some more
</p>
<p>examples.
</p>
<p>Example 17.3 - MA random process
</p>
<p>This random process was shown in Example 16.10 to have a zero mean and a
</p>
<p>covariance sequence
</p>
<p>{
</p>
<p>(72
</p>
<p>T nI = n2
cx[nl ,n2] = ~ In2 - nIl = 1
</p>
<p>o otherwise.
</p>
<p>(17.16)
</p>
<p>Since the covariance sequence depends only on In2 - nIl, X[n] is WSS from (17.15).
Specifically, the ACS follows from (17.15) and (17.16) with k = n2 - ni as
</p>
<p>{
</p>
<p>~ k=O
</p>
<p>rx[k] = O~ k = &plusmn;1
otherwise.
</p>
<p>See Figure 16.13 for a plot of the ACS (replace Don with k.) Could you predict X[l]
from a realization of X[O]?
</p>
<p>Example 17.4 - Randomly phased sinusoid
</p>
<p>This random process was shown in Example 16.11 to have a zero mean and a covari-
</p>
<p>ance sequence cx[nI ' n2] = ~ cos[21r(O.1)(n2 - nI)J. Since the covariance sequence
</p>
<p>depends only on In2 - nIl, X[n] is WSS. Hence , from (17.15) we have that
</p>
<p>1
rx[k] = 2" cos[21r(O.1)k].
</p>
<p>See Figure 16.14 for a plot of the ACS (replace Don with k.) Could you predict X[l]
</p>
<p>from a realization of X[O]?</p>
<p/>
</div>
<div class="page"><p/>
<p>558 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>In determining predictability of a WSS random process, it is convenient to consider
</p>
<p>the linear predictor, which depends only on the first two moments. Then, the MMSE
</p>
<p>linear prediction of X[no + k] given x[no] is from (17.3) and (17.13) with nl = no
and n2 = no + k
</p>
<p>A rx[k] - /-L2
X[no + k] = /-L + [0] 2 (x[no] - /-L) for all k and no&middot;
</p>
<p>r x -/-L
</p>
<p>For a zero mean random process this becomes
</p>
<p>X[no + k]
rx[k]
rx[O] x[no]
</p>
<p>PX[noJ,X[no+k]x[no] for all k and no.
</p>
<p>One last example is the autoregressive random process which we will use to illustrate
</p>
<p>several new concepts for WSS random processes.
</p>
<p>Example 17.5 - Autoregressive random process
</p>
<p>An autoregressive (AR) random process X[n] is defined to be a WSS random process
</p>
<p>with a zero mean that evolves according to the recursive difference equation
</p>
<p>X[n] = aX[n - 1] + Urn] -oo&lt;n&lt;oo (17.17)
</p>
<p>where lal &lt; 1 and Urn] is WGN. The WGN random process Urn] (see Example
16.6) , has a zero mean and variance ab for all nand its samples are all independent
with a Gaussian PDF. The name autoregressive is due to the regression of X[n] upon
</p>
<p>X[n - 1]' which is another sample of the same random process, hence, the prefix
</p>
<p>auto. The evolution of X[n] proceeds, for example, as
</p>
<p>X[O] aX[-I] + U[O]
X[I] aX[O] + U[I]
X[2] aX [1] + U[2]
</p>
<p>Note that X[n] depends only upon the present and past values of Urn] since for
</p>
<p>example
</p>
<p>X[2] = aX[I] + U[2] = a(aX[O] + U[I]) + U[2] = a2X[O] + aU[I] + U[2]
a
2(aX[-I] + U[O]) + aU [1] + U[2] = a3X[-I] + a2U[0] + aU [1] + U[2]
</p>
<p>(17.18)</p>
<p/>
</div>
<div class="page"><p/>
<p>17.4. AUTOCORRELATION SEQUENCE 559
</p>
<p>where the term involving ak U[2 - k] decays to zero as k -t 00 since lal &lt; 1. We
see that X[2] depends only on {U[2],U[l], ... } and it is therefore uncorrelated with
{U[3], U[4], ... }. More generally, it can be shown that (see also Problem 19.6)
</p>
<p>E[X[n]U[n + k]] = 0 k ~ 1. (17.19)
</p>
<p>It is seen from (17.18) that in order for the recursion to be stable and hence X[n] to
be WSS it is required that lal &lt; 1. The AR random process can be used to model
a wide variety of physical random processes with various ACSs, depending upon
</p>
<p>the choice of the parameters a and ( J ~ . Some typical realizations of the AR random
</p>
<p>process for different values of a are shown in Figure 17.5. The WGN random process
</p>
<p>Urn] has been chosen to have a variance ( J ~ = 1 - a2 . We will soon see that this
choice of variance results in rx[O] = 1 for both AR processes shown in Figure 17.5.
The MATLAB code used to generate the realizations shown is given below.
</p>
<p>3 r - - ~ - ~ - ~ - ~ - ~ - - - , 3 r - - - ~ - ~ - ~ - ~ - ~ - - - ,
</p>
<p>2 2
</p>
<p>~ ~: -rlr'r).HI\pll'lll'jl&middot;l1. ~"_: llUFUWljIlI.III.&middot;&bull;&bull;&bull;.T...
. . . . .
</p>
<p>-2 .
</p>
<p>30252015
n
</p>
<p>105
_3L-..-~-~-~-~--,"----'
</p>
<p>o30252015
n
</p>
<p>105
-3 ' - - - ~ - ~ - ~ - ~ - ~ - - - - '
</p>
<p>o
</p>
<p>(a) a = 0.25, u~ = 1 - a2 (b) a = 0.98, u~ = 1 - a 2
</p>
<p>Figure 17.5: Typical realizations of autoregressive random process with different
</p>
<p>parameters.
</p>
<p>clear all
</p>
<p>randn C'state' ,0)
</p>
<p>al=0.25;a2=0.98;
varul=1-al-2;varu2=1-a2-2;
</p>
<p>varxl=varul/(1-al-2);varx2=varu2/(1-a2-2); 'l. this is r_X[O]
xl(l ,l)=sqrt(varxl)*randn(l,l); 'l. set initial condition X[-l]
</p>
<p>'l. see Problems 17.17, 17.18
x2(1,1)=sqrt(varx2)*randn(1,1);
</p>
<p>for n=2:31
</p>
<p>xl(n,l)=al*xl(n-l)+sqrt(varul)*randn(l,l);
</p>
<p>x2(n,1)=a2*x2(n-l)+sqrt(varu2)*randn(1,1);
</p>
<p>end</p>
<p/>
</div>
<div class="page"><p/>
<p>560 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>(17.20)
</p>
<p>We next derive the ACS. In Chapter 18 we will see how to alternatively obtain
</p>
<p>the ACS using results from linear systems theory. Using (17.17) we have for k ~ 1
</p>
<p>r x[k] = E[X[n]X[n + k]]
= E[X [n](aX[n + k -1] + Urn + k])]
</p>
<p>aE[X[n]X[n + k - 1]] (using (17.19))
arx[k - 1] .
</p>
<p>(17.21)- 00 &lt; k &lt; 00.
</p>
<p>(52
</p>
<p>r x [O] = -1 U 2
-a
</p>
<p>so that for all k ~ 0, rx[k] = rx[O]ak becomes
</p>
<p>2
</p>
<p>[ ]
(5u k
</p>
<p>rx k = -1--2a .
-a
</p>
<p>Finally, noting that rx[-k] = r x[k] from Property 17.2, we obtain the ACS as
</p>
<p>(52
</p>
<p>rx [k] = __U_ alkl
1 - a2
</p>
<p>(See also Problem 17.16 for an alternative derivation of the ACS.) The ACS is
</p>
<p>plotted in Figure 17.6 for a = 0.25 and a = 0.98 and ( 5 ~ = 1- a2 &bull; For both values of
a the value of (5~ has been chosen to ensure that r x[O] = 1. Note that for a = 0.25
</p>
<p>the ACS dies off very rapidly which means that the random process samples quickly
</p>
<p>become uncorrelated as the separation between them increases. This is consistent
</p>
<p>with the typical realization shown in Figure 17.5a. For a = 0.98 the ACS decays
very slowly, indicating a strong positive correlation between samples, and again
</p>
<p>being consistent with the typical realization shown in Figure 17.5b . In either case
</p>
<p>the samples become uncorrelated as k -+ 00 since lal &lt; 1 and therefore, rx[k] -+ 0
as k -+ 00 in accordance with Property 17.5. However, the random process with the
</p>
<p>slower decaying ACS is more predictable.
</p>
<p>c
One last property that is necessary for a sequence to be a valid ACS is the property
</p>
<p>of positive definiteness. As its name implies, it is related to , the positive definite
</p>
<p>property of the covariance matrix. As an example, consider the random vector
</p>
<p>X = [X [0] X[I]jT. Then we know from the proof of Property 9.2 (covariance matrix
is positive semidefinite) that if Y = aoX[O] + a1X[I] cannot be made equal to a
constant by any choice of ao and aI , then
</p>
<p>The solution of this recursive linear difference equation is readily seen to be rx[k] =
</p>
<p>cak , for c any constant and for k ~ 1. For k = 1 we have that rx [1] = ca and so
from (17.20) rx[l] = arx[O], which implies c = rx[O]. In Problem 17.15 it is shown
that
</p>
<p>_ [ ] [ cov(X [0],X [0]) cov(X [0],X[I]) ] [ ao ]
var(Y) - ~, cov(X[I],X[O]) cov(X[I],X[I]) J al &gt; O.
</p>
<p>aT '" ~
ex a</p>
<p/>
</div>
<div class="page"><p/>
<p>17.4. AUTOCORRELATION SEQUENCE 561
</p>
<p>105-5
oL..L...L..JL..L...L...JL..J-...L...J--L--'---l--L-.............--L..-.L......L--'---I
</p>
<p>-10
</p>
<p>0.4
</p>
<p>0.2
</p>
<p>0.8
</p>
<p>::se
~0 .6
</p>
<p>h
</p>
<p>105o
k
</p>
<p>-5
</p>
<p>, ,
</p>
<p>&bull; T T Ao
-10
</p>
<p>0.2
</p>
<p>0.8
</p>
<p>0.4
</p>
<p>::se
~0.6
</p>
<p>h
</p>
<p>(a) a = 0.25, a~ = 1 - a2 (b) a = 0.98, a~ = 1 - a2
</p>
<p>Figure 17.6: The autocorrelation sequence for autoregressive random processes with
</p>
<p>different parameters.
</p>
<p>Since this holds for all a i= 0, the covariance matrix C x is by definition positive
definite (see Appendix C). (If it were possible to choose ao and al so that Y = c,
</p>
<p>for c a constant, then X[I] would be perfectly predictable from X[O] as X[I] =
-(aolal)X[O] + (clal)' Therefore, we could have var(Y) = aTCxa = 0, and Cx
would only be positive semidefinite.) Now if X[n] is a zero mean WSS random
</p>
<p>process
</p>
<p>and the covariance matrix becomes
</p>
<p>Cx = [ rx[O]
rx[-I]
</p>
<p>rx[l] ] = [rx[o] rx[I]]
rx[O] rx[l] rx[O] .
</p>
<p>, .,
v
</p>
<p>Rx
</p>
<p>Therefore, the covariance matrix, which we now denote by Rx and which is called
</p>
<p>the autocorrelation matrix, must be positive definite. This implies that all the
</p>
<p>principal minors (see Appendix C) are positive. For the 2 x 2 case this means that
</p>
<p>rx[O] &gt; 0
</p>
<p>r~[O] - r~[I] &gt; 0 (17.22)
</p>
<p>with the first condition being consistent with Property 17.1 and the second condition
</p>
<p>producing rx[O] &gt; Irx[I]I. The latter condition is nearly consistent with Property
17.3 with the slight difference, that Irx[lJ1 may equal rx[O] being excluded. This is
because we assumed that X[I] was not perfectly predictable from knowledge of X[O].
</p>
<p>If we allow perfect predictability, then the autocorrelation matrix is only positive</p>
<p/>
</div>
<div class="page"><p/>
<p>562 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>semidefinite and the&gt; sign in the second equation of (17.22) would be replaced
</p>
<p>with 2. In general the N x N autocorrelation matrix Rx is given as the covariance
</p>
<p>matrix of the zero mean random vector X = [X [0] X[I] ... X[N - I]V as
</p>
<p>Rx=
[
</p>
<p>rx[O]
rx[l]
</p>
<p>r x [ ~ -1]
</p>
<p>rx[l]
rx[O]
</p>
<p>rx[N - 2]
</p>
<p>rx[2] rx[N - 1] ]
rx[l] rx[N - 2]
</p>
<p>&middot; . .&middot; . .&middot; . .
rx[N - 3] ... rx[O]
</p>
<p>(17.23)
</p>
<p>For a sequence to be a valid ACS the N x N autocorrelation matrix must be positive
</p>
<p>semidefinite for all N = 1,2, ... and positive definite if we exclude the possibility of
</p>
<p>perfect predictability [Brockwell and Davis 1987]. This imposes a large number of
</p>
<p>constraints on rx[k] and hence not all sequences satisfying Properties 17.1-17.3 are
valid ACSs (see also Problem 17.19). In summary, for our last property of the ACS
</p>
<p>we have the following.
</p>
<p>Property 17.6 - ACS is a positive semidefinite sequence.
</p>
<p>Mathematically, this means that rx[k] must satisfy
</p>
<p>aTRxa 20
</p>
<p>for all a = lao al ... aN_dT and where Rx is the N x N autocorrelation matrix
given by (17.23). This must hold for all N 2 1.
</p>
<p>o
</p>
<p>17.5 Ergodicity and Temporal Averages
</p>
<p>When a random process is WSS, its mean does not depend on time. Hence, the
</p>
<p>random variables ... , X [-1], X [0], X [1], ... all have the same mean. Then, at least
</p>
<p>as far as the mean is concerned, when we observe a realization of a random process,
</p>
<p>it is as if we are observing multiple realizations of the same random variable. This
</p>
<p>suggests that we may be able to determine the value of the mean from a single
</p>
<p>infinite length realization. To pursue this idea further we plot three realizations of
</p>
<p>an lID random process whose marginal PDF is Gaussian with mean J.Lx[n] = J.L = 1
and a variance o}[n] = a2 = 1 in Figure 17.7. If we let xi[18] denote the ith
realization at time n = 18, then by definition of E[X[18]]
</p>
<p>1 M
lim - L x m [18] = E[X[18]] = J.Lx[18] = J.L = 1.
</p>
<p>M-too M
m=l
</p>
<p>(17.24)
</p>
<p>This is because as we observe all realizations of the random variable X[18] they will
conform to the Gaussian PDF (recall that X[n] rv N(I ,1)). In fact , the original
definition of expected value was based on the relationship given in (17.24). This</p>
<p/>
</div>
<div class="page"><p/>
<p>17.5. ERGODICITY AND TEMPORAL AVERAGES 563
</p>
<p>4,.------.-------,---,----_7"_---,--------,,----,...-,
I
</p>
<p>: : I : : : : :. . . I &middot; . .
~ _2
</p>
<p>0-
~ ~ ~T f-..-l-IiJi-H-;-&bull;JThtTJI-Jl; jf--- t~o'lli averaging
r .. .&bull;&middot;
</p>
<p>30252015105
-2 L-----'------.1----'--~_7"_--'----'------'---'
</p>
<p>o
4.-------,------,----,----.----,-----,----,...-,
</p>
<p>~ : .nlll.l'jJ.l.IJ.1,.TIII.T.tllt
30252015105
</p>
<p>-2 '--__--'-__---.1 -'--_-.----'- '--__--'---'
</p>
<p>o
4.------,------,----,----i----,-----,-----,-,
</p>
<p>3025105
</p>
<p>~ :&middot;jtIllrII,J,
j
,iIll!Jl&bull;.&bull;11 .&bull;&bull;1111t
</p>
<p>I.
I
</p>
<p>15 : 20
n I
</p>
<p>!
ensemble averaging
</p>
<p>Figure 17.7: Several realizations of WSS random process with j.tx[n] = j.t = 1.
</p>
<p>Vertical dashed line indicates "ensemble averaging" while horizontal dashed line
</p>
<p>indicates "temporal averaging."
</p>
<p>type of averaging is called "averaging down the ensemble" and consequently is just a
</p>
<p>restatement of our usual notion of the expected value of a random variable. However,
</p>
<p>if we are given only a single realization such as xI[n], then it seems reasonable that
</p>
<p>N-l
</p>
<p>P,N = ~ L xI[n]
n=O
</p>
<p>should also converge to j.t as N -+ 00. This type of averaging is called "temporal
</p>
<p>averaging" since we are averaging the samples in time. If it is true that the temporal
</p>
<p>average converges to u, then we can state that
</p>
<p>1 N-l 1 M
lim - L xl[n] = j.t = E[X[18]] = lim - L xm [18]
</p>
<p>N-+oo N M-+oo M
n=O m=l</p>
<p/>
</div>
<div class="page"><p/>
<p>564 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>and it is said that temporal averaging is equivalent to ensem ble averaging or that
</p>
<p>the random process is ergodic in the mean. This property is of great practical
</p>
<p>importance since it assures us that by averaging enough samples of the realization,
</p>
<p>we can determine the mean of the random process. For the case of an lID random
</p>
<p>process ergodicity holds due to the law of large numbers (see Chapter 15). Recall
</p>
<p>that if Xl , X2 , &bull;&bull;&bull; , XN are lID random variables with mean J.L and variance 0-
2
</p>
<p>, then
</p>
<p>the sample mean random variable has the property that
</p>
<p>1 N
N L Xi ---+ E[X] = J.L
</p>
<p>i= l
</p>
<p>as N ---+ 00.
</p>
<p>Hence, if X[n] is an lID random process, the conditions required for the law of large
numbers to hold are satisfied, and we can immediately conclude that
</p>
<p>N-I
</p>
<p>fi,N = ~ L X[n] ---+ J.L.
n=O
</p>
<p>(17.25)
</p>
<p>Now the assumptions required for a random process to be lID are overly restrictive
</p>
<p>for (17.25) to hold. More generally, if X[n] is a WSS random process, then since
</p>
<p>E[X[n]] = J.L , it follows that E[fi,N] = (l/N) ' E - ~ : O l E[X[n]] = J.L. Therefore, the only
fur ther condition required for ergodicity in the mean is that
</p>
<p>lim var(fi,N) = O.
N -+oo
</p>
<p>In the case of the lID random process it is easily shown that var(fi,N) = 0-2 / N ---+ 0
as N ---+ 00 and the condition is satisfied. More generally, however , the random
process samples are correlated so that evaluation of this variance is slightly more
</p>
<p>complicated. We illustrate this computation next.
</p>
<p>Example 17.6 - General MA random process
</p>
<p>Consider the general MA random process given as X[n] = (U[n] + U[n - 1])/2,
where E[U[n]] = J.L and var(U[n]) = o-~ for -00 &lt; n &lt; 00 and the U[nJ's are
all uncorrelated. This is similar to the MA process of Example 16.10 but is more
</p>
<p>general in that the mean of U[n] is not necessarily zero, the samples of U[n] are only
uncorrelated, and hence , not necessarily independent, and the PDF of each sample
</p>
<p>need not be Gaussian. The general MA process X[n] is easily shown to be WSS
and to have a mean sequence JLx[n] = JL (see Problem 17.20) . To determine if it is
</p>
<p>ergodic in the mean we must compute the var(fi,N) and show that it converges to
</p>
<p>zero as N ---+ 00. Now
</p>
<p>(
</p>
<p>N -I )
</p>
<p>var(fi,N) = var ~ ~ X[n] .</p>
<p/>
</div>
<div class="page"><p/>
<p>17.5. ERGODICITY AND TEMPORAL AVERAGES 565
</p>
<p>Since the X[n]'s are now correlated, we use (9.26), where a = lao al ... aN-IV with
an = liN, to yield
</p>
<p>The covariance matrix has (i ,j) element
</p>
<p>(17.26)
</p>
<p>[CX]ij = E[(X[i]-E[X[i]])(X[j]-E[X[j]])]
</p>
<p>But
</p>
<p>i = 0,1, ... ,N-1jj = 0, 1, ... ,N-l.
</p>
<p>X[n] - E[X[n]]
1 1
</p>
<p>= 2(U[n] + Urn - 1]) - 2(J.L + J.L)
1
</p>
<p>= 2[(U[n] - J.L) + (U[n - 1] - J.L)]
1 - -
2[U[n] + Urn -1]]
</p>
<p>where Urn] is a zero mean random variable for each value of n. Thus,
</p>
<p>1 - - - -
4E[(U[i] + Uri - 1])(U[j] + U[j - 1])]
</p>
<p>l (E[U[i]U[j]] + E[U[i]U[j - 1]] + E[U[i - l]U[j]] + E[U[i - l]U[j - 1]])
and since E[U[nl]U[n2]] = cov(U[nlJ, U[n2]) = ab8[n2 - nl] (all the U[n]'s are
uncorrelated), we have
</p>
<p>1
[CX]ij = 4 (ab8[j - i] + ab8[j - 1 - i] + ab8[j - i + 1] + ab8[j - in&middot;
</p>
<p>Finally, we have the required covariance matrix
</p>
<p>%=J
</p>
<p>Ii - jl = 1
otherwise.
</p>
<p>(17.27)
</p>
<p>Using this in (17.26) produces</p>
<p/>
</div>
<div class="page"><p/>
<p>566 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>=
</p>
<p>=
</p>
<p>u2 U2
0 0 0 0 0::IL ::IL2 4
</p>
<p>u 2 u2 u 2
0 0 0 0 [I]
</p>
<p>::IL ::IL ::IL
4 2 4
</p>
<p>1 1 1
N N N
</p>
<p>0 0 0
u 2 u 2 u 2
</p>
<p>0 ::IL ::IL ::IL4 2 4
</p>
<p>0 0 0 0 0
u 2 u2
::IL ::IL
</p>
<p>4 2
</p>
<p>N-l 2 N-2 2 N-l 2
1 L (Ju 1 L (Ju 1 L (Ju
</p>
<p>N2 T+N2 4""+N2 4""
i=O i=O i=l
</p>
<p>(J2 (J2 N 1 (J2 N 1u u - u - 0 as N -7 00 .
2N +4""N2+4""N2 -7
</p>
<p>Finally, we see that the genera l MA random process is ergodic in the mean.
</p>
<p>&lt;:;
In general, it can be shown that for a WSS random process to be ergodic in the
</p>
<p>mean, the variance of the sample mean
</p>
<p>1 N-l ( Ikl)
var(flN) = N L 1 - N (rx[k] - J-L2)
</p>
<p>k=-(N-l)
</p>
<p>(17.28)
</p>
<p>must converge to zero as N -t 00 (see Problem 17.23 for the derivation of (17.28)).
</p>
<p>For this to occur, the covariance sequence rx[k] - J-L2 must decay to zero at a fast
</p>
<p>enough rate as k -7 00, which is to say that as the samples are spaced further
</p>
<p>and further apart , they must eventually become uncorrelated. A little reflection on
</p>
<p>the part of the reader will reveal that ergodicity requires a single realization of the
</p>
<p>random process to display the behavior of the entire ensemble of realizations. If not,
</p>
<p>ergodicity will not hold. Consider the following simple nonergodic random process.
</p>
<p>Example 17 .7 - Random DC level
</p>
<p>Define a random process as X [n] = A for -00 &lt; n &lt; 00, where A rv N (O, 1). Some
</p>
<p>realizations are shown in Figure 17.8. T his random process is WSS since
</p>
<p>J-Lx[n]
</p>
<p>rx[k]
</p>
<p>- E[X[n]] = E[A] = 0 = J-L
E[X[n]X[n + k]] = E[A2] = 1
</p>
<p>-oo &lt;n &lt;oo (not dependent on n)
</p>
<p>(not dependent on n).
</p>
<p>However , it should be clear that flN will not converge to J-L = O. Referring to the
</p>
<p>realization xdn] in Figure 17.8, the sample mean will produce - 0.43 no matter how
large N becomes. In addition, it can be shown that var(flN) = 1 (see Problem
</p>
<p>17.24) . Each rea lization is not representative of the ensem ble of realizations.</p>
<p/>
</div>
<div class="page"><p/>
<p>17.6. THE POWER SPECTRAL DENSITY 567
</p>
<p>30252015105
</p>
<p>. . .. . . . . . .. . . ... .. ... . .. ~ . . . . . . ... - . ~ .
&middot; . . .
&middot; .. ... . ... . .. . .. . . . .. - _ .
&middot; . . .
</p>
<p>&middot; . .
, . .
</p>
<p>3,-----.-------,.---.-------r------,,---,.-,
</p>
<p>2
</p>
<p>&gt;::: 1
</p>
<p>~ 0 &middot;1 1&middot; ' &middot;1&middot; 1 1 1&middot;&middot;1 1&middot;&middot;1 1' 1 1 1 ' 1''1 1&middot;1 &middot;1&middot;1&middot; ' &middot;1&middot;&middot;1 1&middot;1'1&middot;1&middot;&middot;1
&middot; , . .
</p>
<p>-1
</p>
<p>-2
-3 L-__...L.__---'- -'---__-'-__---' --'--'
</p>
<p>o
3.----,------r---,-----,------,---,-,
</p>
<p>2
</p>
<p>&gt;::: 1
N ' .
</p>
<p>H _~ :1.1'1:11:111.1'1&middot;111&middot;1'1.11:11111:11:111.1' 1.1
-2
</p>
<p>30252015105
</p>
<p>-3 L-__-'-__---'- -'---__--'--__---'L-__--'--'
</p>
<p>o
3,----.-------,.---.-------r------,,---,.-,
</p>
<p>2
</p>
<p>&gt;::: 1
&middot; . .H 0 . .&bull; .&bull; . .&bull;. &bull; .. &bull; . &bull; .. &bull;..&bull; . &bull;.. ~ . &bull; . .&bull; . &bull;. &bull; . .~ . &bull; . .&bull; . &bull; . .&bull; . ~ .&bull; o.e . &bull; . .&bull; . ~ .. &bull; ..&bull; .&bull; . .&bull; o ~ .
&middot; .
</p>
<p>, .
~ &middot;&middot;&middot; 7&middot;&middot; &middot; &middot; &middot;&middot;&middot; &middot; &middot; &middot; &middot; : &middot;
</p>
<p>-2 . .. .. . . . . . .... ... . . . . .. .... . ..... .&bull; . . . . . . . ... .: : .
</p>
<p>302520105
</p>
<p>-3 L-__-'-__--'- -'--__-'-__----' -'--'
</p>
<p>o
</p>
<p>Figure 17.8: Several realizations of the random DC level process.
</p>
<p>17.6 The Power Spectral Density
</p>
<p>The ACS measures the correlat ion between samples of a WSS random process. For
</p>
<p>example, the AR random process was shown to have the ACS
</p>
<p>a 2rx [kJ= __U_ alkJ
1 - a2
</p>
<p>which for a = 0.25 and a = 0.98 is shown in Figure 17.6, along with some typical
realizations in Figure 17.5. Note that when the ACS dies out rapidly (see Figure
</p>
<p>17.6a) , the realization is more rapidly varying in time (see Figure 17.5a). In contrast,
</p>
<p>when the ACS decays slowly (see Figure 17.6b) , the realization varies slowly (see
</p>
<p>Figure 17.5b). It would seem that the ACS is related to the rate of change of the
</p>
<p>random process. For deterministic signals the rate of change is usually measured
</p>
<p>by examining a discrete-time Fourier transform [Jackson 1991J. Signals with high
</p>
<p>frequency content exhibit rapid fluctutations in time while signals with only low
</p>
<p>frequency content exhibit slow variations in time. For WSS random processes we
</p>
<p>will be interested in the pow er at the various frequencies. In particular , we will
</p>
<p>introduce the measure known as the pow er spectral density (PSD) and show that it</p>
<p/>
</div>
<div class="page"><p/>
<p>568 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>quantifies the distribution of power with frequency. Before doing so, however, we
</p>
<p>consider the following deterministically motivated measure of power with frequency
</p>
<p>based on the discrete-time Fourier transform
</p>
<p>N-l 2
A I ",
</p>
<p>Px(f) = N L.J X[n]exp(-j27fJn)
n=O
</p>
<p>(17.29)
</p>
<p>This is a normalized version of the magnitude-squared discrete-time Fourier trans-
</p>
<p>form of the random process over the time interval 0 ~ n ~ N - 1. It is called the
</p>
<p>periodogram since its original purpose was to find periodicities in random data sets
</p>
<p>[Schuster 1898]. In (17.29) J denotes the discrete-time frequency, which is assumed
to be in the range -1/2 ~ J ~ 1/2 for reasons that will be elucidated later. The l/N
factor is required to normalize Px (f) to be interpretable as a power spectral density
or power per unit frequency. The use of a "hat" is meant to convey the notion that
</p>
<p>this quantity is an estimator. As we now show, the periodogram is not a suitable
</p>
<p>measure of the distribution of power with frequency, although it would be for some
</p>
<p>deterministic signals (such as periodic discrete-time signals with period N) . As an
</p>
<p>example, we plot Px(f) in Figure 17.9 for the realizations given in Figure 17.5. We
</p>
<p>8 . 8 .
</p>
<p>4 ... .:.
</p>
<p>.........-.
~ 6 .... :.
</p>
<p>&lt;Q., :
</p>
<p>4 . . . . :. . . . .... ... . . ... . . . ... . . ... .. . .
</p>
<p>2
</p>
<p>o'-"""'............L.JoL.JL.~~--..:e'-~~ ........~ ...........LJ
-0.5 -0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4 0.5
</p>
<p>J
</p>
<p>(a) a = 0.25, u~ = 1 - a 2
</p>
<p>2 ... .; ....:... ..:.
</p>
<p>-0.5 -0.4 -0.3 -0.2 - 0.1 0 0.1 0.2 0.3 0.4 0.5
</p>
<p>J
</p>
<p>(b) a = 0.98, u~ = 1 - a2
</p>
<p>Figure 17.9: Periodogram for autoregressive random process with different param-
</p>
<p>eters. The realizations shown in Figure 17.5 were used to generate these estimates.
</p>
<p>see that the periodogram in Figure 17.9a exhibits many random fluctuations. Other
</p>
<p>realizations will also produce similar seemingly random curves. However, it does
</p>
<p>seem to produce a reasonable result-for the periodogram in Figure 17.9a there is
</p>
<p>more high frequency power than for the periodogram in Figure 17.9b. The reason
</p>
<p>for the random nature of the plot is that (17.29) is a function of N random variables
</p>
<p>and hence is a random variable itself for each frequency. As such, it exhibits the</p>
<p/>
</div>
<div class="page"><p/>
<p>17.6. THE POWER SPECTRAL DENSITY 569
</p>
<p>variability of a random process for which the usual dependence on time is replaced
</p>
<p>by frequency. What we would actually like is an average measure of the power dis-
</p>
<p>tribution with frequency, suggesting the need for an expected value. Also, to ensure
</p>
<p>that we capture the entire random process behavior, an infinite length realization is
</p>
<p>required. We are therefore led to the following more suitable definition of the PSD
</p>
<p>(17.30)
</p>
<p>The function Px(J) is called the power spectral density (PSD) and when integrated
</p>
<p>provides a measure of the average power within a band of frequencies. It is com-
</p>
<p>pletely analogous to the PDF in that to find the average power of the random process
</p>
<p>in the frequency band h ::; f ::; 12 we should find the area under the PSD curve.
</p>
<p>Fourier analysis of a random process yields no phase information.
</p>
<p>In our definition of the PSD we are using the magnitude-squared of the Fourier
</p>
<p>transform. It is obvious then, that the PSD does not tell us anything about the
</p>
<p>phases of the Fourier transform of the random process. This is in contrast to a
</p>
<p>Fourier transform of a deterministic signal. There the inverse Fourier transform can
</p>
<p>be viewed as a decomposition of the signal into sinusoids of different frequencies
</p>
<p>with deterministic amplitudes and phases. For a random process a similar decom-
</p>
<p>position called the spectral representation theorem [Brockwell and Davis 1987] yields
</p>
<p>sinusoids of different frequencies with random amplitudes and random phases. The
</p>
<p>PSD is essentially the expected value of the power of the random sinusoidal ampli-
</p>
<p>tudes per unit of frequency. No phase information is retained and therefore no phase
</p>
<p>information can be extracted from knowledge of the PSD.
</p>
<p>We next give an example of the computation of a PSD.
</p>
<p>Example 17.8 - White noise
</p>
<p>Assume that X[n] is white noise (see Example 17.2) and therefore, has a zero mean</p>
<p/>
</div>
<div class="page"><p/>
<p>570 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>and ACS rx[k] = 0-28[k]. Then,
</p>
<p>Px(f) ~ , J i ~ = 2M\ 1E [ntM X[n] exp(j2~fnl mtMX[mJexp(-j2n f m l]
1 M M
</p>
<p>= lim 2M '" '" E[X[n]X[m]] exp[-j27rj(m - n)] (17.31)
M~oo +1 LJ LJ ~
</p>
<p>n=-Mm=-M [ I
r x m -n
</p>
<p>=
</p>
<p>=
</p>
<p>M M
</p>
<p>lim 1 '" '" 0-28[m - n]exp[-j27rj(m - n)]
M ~oo 2M + 1 LJ LJ
</p>
<p>n=-Mm=-M
</p>
<p>1 M
</p>
<p>lim '" 0-
2
</p>
<p>M~oo 2M + 1 LJ
n=-M
</p>
<p>lim 0-2 = 0-2. (17.32)
M~oo
</p>
<p>Hence , for white noise the PSD is
</p>
<p>- 1/2 ~ j ~ 1/2.
</p>
<p>As first mentioned in Chapter 16 white noise contains equal contributions of average
</p>
<p>power at all frequencies.
</p>
<p>c
A more straightforward approach to obtaining the PSD is based on knowledge of
</p>
<p>the ACS. From (17.31) we see that
</p>
<p>1 M M
</p>
<p>Px(J) = lim 2M 1 2: 2: rx[m - n]exp[-j27rj(m - n)].
M~oo +
</p>
<p>n=-Mm=-M
</p>
<p>This can be simplified using the formula (see Problem 17.26)
</p>
<p>M M 2M
</p>
<p>2: 2: g[m - n] = 2: (2M + 1 -Ikl)g[k]
n=-M m=-M k=-2M
</p>
<p>(17.33)
</p>
<p>which results from considering g[m - n] as an element of the (2M + 1) x (2M + 1)
matrix G with elements [G]mn = g[m-n] for m = -M, ... , M and n = -M, ... , M
and then summing all the elements. Using this relationship in (17.33) produces
</p>
<p>Px(J) =
1 2M
</p>
<p>lim 2M 1 2: (2M + 1 -Ikl)rx[k] exp( -j27rjk)
M ~ o o +
</p>
<p>k=-2M
</p>
<p>lim ~ (1- 2 ~ k l ) rx[k]exp(-j27rjk) .
M~oo + 1
</p>
<p>k=-2M</p>
<p/>
</div>
<div class="page"><p/>
<p>17.6. THE POWER SPECTRAL DENSITY 571
</p>
<p>Assuming that L ~ = - o o Irx[kJl &lt; 00, the limit can be shown to produce the final
result (see Problem 17.27)
</p>
<p>00
</p>
<p>Px(f) = L rx[k] exp (-j27rfk)
k=-oo
</p>
<p>(17.34)
</p>
<p>which says that the power spectral density is the discrete-tim e Fourier transform
</p>
<p>of the ACS. This relationship is known as the Wiener-Khinchin e theorem. Some
</p>
<p>examples follow.
</p>
<p>Example 17.9 - White noise
</p>
<p>From Example 17.2 rx[k] = a28[k] and so
</p>
<p>00
</p>
<p>Px(f) = L rx[k]exp(-j27rfk)
k=-oo
</p>
<p>00
</p>
<p>L a 28[k] exp( -j27rfk)
k=-oo
</p>
<p>= a 2 .
</p>
<p>This is shown in Figure 17.10. Note that the total average power in X[n], which is
rx [O] = a2 , is given by the area under the PSD curve.
</p>
<p>Px(f)
</p>
<p>_ . . . L . . - _ ~ __---L._", f
1
"2
</p>
<p>Figure 17.10: PSD of white noise.
</p>
<p>Example 17.10 - AR random process
</p>
<p>From (17.21) we have that
</p>
<p>-oo&lt;k&lt;oo</p>
<p/>
</div>
<div class="page"><p/>
<p>572 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>and from (17.34)
</p>
<p>00
</p>
<p>Px(J) = L rx[k] exp( -j21fjk)
k=-oo
</p>
<p>2 00
</p>
<p>---.!!.JL ~ a1k1exp(-j21fjk)
1- a2 LJ
</p>
<p>k=-oo
</p>
<p>Since laexp(&plusmn;j21fJ)1 = lal &lt; 1, we can use the formula 2 : ~ k o zk = zko /(1 - z) for
z a complex number with Izi &lt; 1 to evaluate the sums. This produces
</p>
<p>(17.35)
11 - aexp(-j21fj)J2.
</p>
<p>(1[; ( a exp(j21fJ) 1)
1- a2 1- aexp(j21fJ) + 1 - aexp(-j21fJ)
</p>
<p>(1[; aexp(j21fj)(l- aexp(-j21fJ)) + (1- aexp(j21fJ))
1- a2 (1 - aexp(j21fJ))(1 - aexp(-j21fJ))
</p>
<p>(1[; 1 - a2
</p>
<p>1- a2 11 - a exp(-j21fJ) 12
</p>
<p>(12
U=
</p>
<p>Px(J) =
</p>
<p>This can also be written in real form as
</p>
<p>2
</p>
<p>Px(J) = (1u
1 + a2 - 2a cos(21fJ) - 1/2 '5:. j '5:. 1/2.
</p>
<p>(17.36)
</p>
<p>For a = 0.25 and a = 0.98 and (1[; = 1 - a2 , the PSDs are plotted in Figure
17.11. Note that the total average power in each PSD is the same, being rx[O] =
</p>
<p>(1[;/(1 - a2 ) = 1. As expected the more noise-like random process has a PSD (see
</p>
<p>Figure 17.11a) with more high frequency average power than the slowly varying
</p>
<p>random process (see Figure 17.11b) which has all its average power near j = 0 (or
at DC).
</p>
<p>o
From the previous example, we observe that the PSD exhibits the properties of
</p>
<p>being a real nonnegative function of frequency, consistent with our notion of power
</p>
<p>as a nonnegative physical quantity, of being symmetric about j = 0, and of being
</p>
<p>periodic with period one (see (17.36)). We next prove that these properties are true
</p>
<p>in general.</p>
<p/>
</div>
<div class="page"><p/>
<p>17.6. THE POWER SPECTRAL DENSITY 573
</p>
<p>Px(O) = 99
8
</p>
<p>4
</p>
<p>2
</p>
<p>OL.-~~~-~~~~-~~---J
</p>
<p>-0.5 -0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4 0.5
j
</p>
<p>(a) a = 0.25, ab = 1 - a2
</p>
<p>8
</p>
<p>4 .
</p>
<p>2 . . . .
</p>
<p>(b) a = 0.98, ab = 1 - a2
</p>
<p>Figure 17.11: Power spectral densities for autoregressive random process with differ-
</p>
<p>ent parameters. The periodograms, which are estimated PSDs, were given in Figure
</p>
<p>17.9.
</p>
<p>Property 17.7 - PSD is a real function.
</p>
<p>The PSD is also given by the real function
</p>
<p>Proof:
</p>
<p>Px(f)
</p>
<p>00
</p>
<p>Px(f) = L rx[k]cos(27fjk) .
k=-oo
</p>
<p>00
</p>
<p>L rx[k] exp( -j2n}k)
k=-oo
</p>
<p>00
</p>
<p>L rx[k](cos(27fjk) - jsin(27fjk))
k=-oo
</p>
<p>(17.37)
</p>
<p>00 00
</p>
<p>But
</p>
<p>L rx[k] cos(27fjk) - j L rx[k] sin(27fjk) .
k=-oo k=-oo
</p>
<p>00 -1 00
</p>
<p>L rx[k] sin(27fjk) = L rx[k] sin(27fjk) +L rx[k] sin(27fjk)
k=-oo k=-oo k=l</p>
<p/>
</div>
<div class="page"><p/>
<p>574 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>since the k = 0 term is zero , and letting l = -k in the first sum we have
</p>
<p>00
</p>
<p>L rx [k] sin(21rfk)
</p>
<p>k=-oo
</p>
<p>from which (17.37) follows.
</p>
<p>00 00
</p>
<p>L r x [-l ]sin(21rf (-l )) + L r x[k]sin(21rfk)
1=1 k=l
</p>
<p>00
</p>
<p>L r x [k](-sin(21rfk) + sin(21rf k)) = 0 (r x [-l ] = rx[l])
k= l
</p>
<p>o
</p>
<p>Property 17.8 - PSD is nonnegative.
</p>
<p>Px(J) ~ 0
</p>
<p>Proof: Follows from (17.30) but can also be shown to follow from the positive
</p>
<p>semidefinite property of the ACS [Brockwell and Davis 1987]. (See also Problem
</p>
<p>17.19.)
</p>
<p>o
</p>
<p>Property 17.9 - PSD is symmetric about f = O.
</p>
<p>Pxi: - J) = Px(J )
</p>
<p>Proof: Follows from (17.37).
</p>
<p>o
</p>
<p>Property 17 .10 - PSD is periodic with period one.
</p>
<p>Px(J + 1) = Px(J)
</p>
<p>Proof: From (17.37) we have
</p>
<p>00
</p>
<p>Px(J + 1) = L r x[k] cos(21r(J + 1)k)
</p>
<p>k= -oo
</p>
<p>00
</p>
<p>= L rx [k] cos(21rfk + 21rk)
</p>
<p>k= - oo
</p>
<p>00
</p>
<p>L r x [k] cos(21rfk)
</p>
<p>k=-oo
</p>
<p>Px (J )
</p>
<p>(cos(21rk) = 1, sin(21rk) = 0)
</p>
<p>o</p>
<p/>
</div>
<div class="page"><p/>
<p>17.6. THE POWER SPECTRAL DENSITY 575
</p>
<p>Property 17.11 - ACS recovered from PSD using inverse Fourier trans-
</p>
<p>form
</p>
<p>1
</p>
<p>rx[k] t.Px(f) exp(j21fjk)dj - 00 &lt; k &lt; 00
2
</p>
<p>1
</p>
<p>= i: Px(f) cos(21fjk)dj - 00 &lt; k &lt; 00
2
</p>
<p>(17.38)
</p>
<p>(17.39)
</p>
<p>(17.40)
</p>
<p>Average physical power in [0,1/2] =
</p>
<p>Proof: (17.38) follows from properties of discrete-time Fourier transform [Jackson
</p>
<p>1991]. (17.39) follows from Property 17.9 (see Appendix B.5 and also Problem
</p>
<p>17.49).
</p>
<p>o
</p>
<p>Property 17.12 - PSD yields average power over band of frequencies.
</p>
<p>To obtain the average power in the frequency band II ::; j ::; 12 we need only find
the area under the PSD curve for this band. The average physical power is obtained
</p>
<p>as twice this area since the negative frequencies account for half of the average power
</p>
<p>(recall Property 17.9). Hence,
</p>
<p>Average physical power in [II,12] = 2 rh Px (f)dj.
1ft
</p>
<p>The proof of this property requires some concepts to be described in the next chapter ,
</p>
<p>and thus, we defer the proof until Section 18.4. Note, however , that if II = 0 and
12 = 1/2, then the average power in this band is
</p>
<p>r1/ 2
2 10 Px(f)dj
</p>
<p>1i: Px(J)dj (due to symmetry of PSD)
2
</p>
<p>1
</p>
<p>i: Px(J) exp(j21fj (0))dj
2
</p>
<p>= rx[O] (from (17.38))
</p>
<p>which we have already seen yields the total average power since r x[O] = E[X2[n]] .
</p>
<p>Hence , we see that the total average power is obtained by integrating the PSD over
</p>
<p>all frequencies to yield
1
</p>
<p>r x[O] = i 21 Px(J)dj.
2
</p>
<p>(17.41)</p>
<p/>
</div>
<div class="page"><p/>
<p>576 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>o
</p>
<p>.Lit. Definitions of PSD are not consistent.
In some texts, especially ones describing the use of the PSD for physical measure-
</p>
<p>ments, the definition of the PSD is slightly different . The alternative definition relies
</p>
<p>on the relationship of (17.40) to define the PSD as Gx(J) = 2Px(J). It is called the
one-sided PSD and its advantage is that it yields directly the average power over a
</p>
<p>band when integrated over the band. As can be seen from (17.40)
</p>
<p>~
h
</p>
<p>Average physical power in [!l ,12] = GX (J)df.
h
</p>
<p>.Lit.
A final comment concerns the periodicity of the PSD. We have chosen the fre-
</p>
<p>quency interval [-1/2,1/2] over which to display the PSD. The rationale for this
</p>
<p>choice arises from the practical situation in which a continuous-time WSS random
</p>
<p>process (see Section 17.8) is sampled to produce a discrete-time WSS random pro-
</p>
<p>cess. Then, if the continuous-time random process X (t) has a PSD that is bandlim-
</p>
<p>ited to W Hz and is sampled at Fs samples/sec, the discrete-time PSD Px(J) will
have discrete-time frequency units of W / F s &bull; For Nyquist rate sampling of F; = 2W,
</p>
<p>the maximum discrete-time frequency will be f = W/Fs = 1/2. Hence, our choice
of the frequency interval [-1/2,1/2] corresponds to the continuous-time frequency
</p>
<p>interval of [-W , W] Hz. The discrete-time frequency is also referred to as the nor-
</p>
<p>mali zed frequency, the normalizing factor being Fs .
</p>
<p>17.7 Estimation of the ACS and PSD
</p>
<p>Recall from our discussion of ergodicity that in the problem of mean estimation
</p>
<p>for a WSS random process, we were restricted to observing only a finite number of
</p>
<p>samples of one realization of the random process. If the random process is ergodic
</p>
<p>in the mean, then we saw that as the number of samples increases to infinity, the
</p>
<p>temporal average flN will converge to the ensemble average u, To apply this result
to estimation of the ACS consider the problem of estimating the ACS for lag k = ko
which is
</p>
<p>rx[ko] = E[X[n]X[n + ko]].
</p>
<p>Then by defining the product random process Y[n] = X[n]X[n + ko] we see that
</p>
<p>rx [ko] = E[Y[n]] -oo&lt;n&lt;oo</p>
<p/>
</div>
<div class="page"><p/>
<p>17.7. ESTIMATION OF THE ACS AND PSD 577
</p>
<p>or the desired quantity to be estimated is just the mean of the random process Y[n].
The mean of Y[n] does not depend on n. This suggests that we replace the observed
values of X[n] with those of Y[n] by using y[n] = x[n]x[n + ko], and then use a
temporal average to estimate the ensemble average. Hence, we have the temporal
</p>
<p>average estimate
</p>
<p>rx[ko]
1 N-l
</p>
<p>NLy[n]
n=O
</p>
<p>1 N-l
</p>
<p>= N L x[n]x[n + ko].
n=O
</p>
<p>(17.42)
</p>
<p>(17.43)k = 0,1, ... , N - 1.
</p>
<p>Also, since rx[-k] = rx[k], we need only estimate the ACS for k ~ O. There
is one slight modification that we need to make to the estimate. Assuming that
</p>
<p>{x[O] ,x[I] , ... ,x[N -I]} are observed, we must choose the upper limit on the sum-
mation in (17.42) to satisfy the constraint n +ko :s; N -1. This is because x[n + ko]
is unobserved for n + ko &gt; N - 1. With this modification we have as our estimate
of the ACS (and now replacing the specific lag of ko by the more general lag k)
</p>
<p>A 1 N-l-k
</p>
<p>rx[k] = N _ k L x[n]x[n + k]
n=O
</p>
<p>rx[O]
</p>
<p>rx[l]
</p>
<p>rx[2]
</p>
<p>rx[3]
</p>
<p>We have also changed the liN averaging factor to I/(N - k). This is because the
number of terms in the sum is only N - k. For example, if N = 4 so that we observe
{x[0],x[l],x[2],x[3]}, then (17.43) yields the estimates
</p>
<p>1
4" (x2[0] + x2[1] + x2[2] + x2[3))
</p>
<p>1
3(x[O]x[l] + x[l]x[2] + x[2]x[3))
1
"2(x[0]x[2] + x[l]x[3))
</p>
<p>x[0]x[3].
</p>
<p>k=O,I, ...
</p>
<p>As k increases, the distance between the samples increases and so there are less
</p>
<p>products available for averaging. In fact, for k &gt; N -1, we cannot estimate the value
of the ACS at all. With the estimate given in (17.43) we see that E[rx[k)) = rx[k]
for k = 0,1 , .. . , N - 1. In order for the estimate to converge to the true value as
N -+ 00, i.e, for the random process to be ergodic in the autocorrelation or
</p>
<p>N-l-k
</p>
<p>lim rx[k] = lim N 1 k '" x[n]x[n + k] = rx[k]
N -too N -too - L.J
</p>
<p>n=O
</p>
<p>we require that var(rx[k)) -+ 0 as N -+ 00. This will generally be true if rx[k] -+ 0
as k -+ 00 for a zero mean random process but see Problem 17.25 for a case where</p>
<p/>
</div>
<div class="page"><p/>
<p>578 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>this is not required. To illustrat e the est imation performance consider the AR
</p>
<p>random process described in Example 17.5. The true ACS and the estimated one
</p>
<p>using (17.43) and based on the realizations shown in Figure 17.5 are shown in
</p>
<p>Figure 17.12. The est imated ACS is shown as the dark lines while the true ACS as
</p>
<p>given by (17.21) is shown as light lines, which are slightly displ aced to the right for
</p>
<p>eas ier viewing. Note that in Figure 17.12 the est imated values for k large exhibit
</p>
<p>0.5 . ~ 0: j,jjWJJWJJjjjjJJjjjjUJJI
-0.5 -0.5 .
</p>
<p>15 20 25 30
k
</p>
<p>105
-1 '--_~_~_~_~_~_---.....J
</p>
<p>o15 20 25 30
k
</p>
<p>105
-1 L...-_~_~_~_~_~_-'-'
</p>
<p>o
</p>
<p>(a) a = 0.25, a ~ = 1 - a2 (b) a = 0.98, a ~ = 1 - a2
</p>
<p>Figure 17.12: Estimated ACSs (dark lines) and the t rue ACSs given in Figure 17.6
</p>
<p>(light lines) for the AR random process realizations shown in Figure 17.5.
</p>
<p>a large error. This is due to the fewer number of products, i.e., N - k = 31 - k ,
that are available for averaging in (17.43). In the case of k = 30 the estimate is
fx [30] = x [0]x[30], which as you might expect is very poor since there is no averaging
at all! Clearly, for accurate est imates of the ACS we require that km ax &laquo; N . The
MATLAB code used to est imate the ACS for Figure 17.12 is given below.
</p>
<p>n=[O:30]';N=length(n);
</p>
<p>a1=O.25;a2=O.98;
v a r u 1 = 1 - a 1 ~ 2 ; v a r u 2 = 1 - a 2 ~ 2 ;
</p>
<p>r 1 t r u e = ( v a r u 1 / ( 1 - a 1 ~ 2 &raquo; * a l . ~ n ; %see (17.21)
r 2 t r u e = ( v a r u 2 / ( 1 - a 2 ~ 2 &raquo; * a 2 . ~ n ;
</p>
<p>for k=O:N-1
</p>
<p>r1est(k+1,1)=(1/(N-k&raquo;*sum(xl(1:N-k).*x1(1+k :N&raquo;;
</p>
<p>r2est(k+1,1)=(1/(N-k&raquo;*sum(x2(1:N-k).*x2(1+k:N&raquo;;
</p>
<p>end
</p>
<p>To est imate the PSD requires somewhat more care than the ACS. We have
</p>
<p>already seen that the periodogra m est imate of (17.29) is not suitable. There are
</p>
<p>many ways to est imate the PSD based on eit her (17.30) or (17.34). We illus trate</p>
<p/>
</div>
<div class="page"><p/>
<p>17.7. ESTIMATION OF THE ACS AND PSD 579
</p>
<p>one approach based on (17.30). Others may be found in [Jenkins and Watts 1968,
</p>
<p>Kay 1988]. Since we only have a segment of a single realization of the random
</p>
<p>process, we cannot implement the expectation operation required in (17.30). Note
</p>
<p>that the operation of E[&middot;] represents an average down the ensemble or equivalently
an average over multiple realizations. To obtain some averaging, however , we can
</p>
<p>break up the data {x[O] , x[I], . . . , x [N -I]} into I nonoverlapping blocks, with each
</p>
<p>block having a total of L samples. We assume for simplicity that there is an integer
</p>
<p>number of blocks so that N = I L. The implicit assumption in doing so is that each
</p>
<p>block exhibit s the statistical characteristics of a single realization and so we can
</p>
<p>mimic the averaging down the ensemble by averaging temporally across successive
</p>
<p>blocks of data. Once again, the assumption of ergodicity is being employed. Thus,
</p>
<p>we first break up the data set into the I nonoverlapping data blocks
</p>
<p>Yi[n] = x[n + iLl n = 0, 1, . .. ,L - 1; i = 0, 1, ... ,I - 1
</p>
<p>where each data block has a length of L samples. Then, for each data block we
</p>
<p>compute a periodogram as
</p>
<p>L-l 2
</p>
<p>A (i) 1 '"Px (J) = L L.J Yi[n] exp( -j21rIn)
n=O
</p>
<p>(17.44)
</p>
<p>and then average all the periodograms together to yield the final PSD estimate as
</p>
<p>1-1
A _ 1 '" A (i)
</p>
<p>Pav(J) - I L.J Px (J) .
i= O
</p>
<p>(17.45)
</p>
<p>This est imate is called the averaged periodogram. It can be shown that under some
</p>
<p>condit ions, limN-too Av(J) = Px(J). Once again we are calling upon an ergodicity
type of property in that we are averaging the periodograms obtained in time instead
</p>
<p>of the theoretical ensemble averaging. Of course , for convergence to hold as N -+ 00,
we must have L -+ 00 and I -+ 00 as well.
</p>
<p>As an example, we examine the averaged periodogram estimates for the two AR
</p>
<p>processes whose PSDs are shown in Figure 17.ll. The number of data samples was
</p>
<p>N = 310, which was broken up into I = 10 nonoverlapping blocks of data with
L = 31 samples in each one. By comparing the spectral estimates in Figure 17.13
with those of Figure 17.9, it is seen that the averaging has yielded a better estimate.
</p>
<p>Of course, the price paid is that the data set needs to be I = 10 times as long!
The MATLAB code used to implement the averaged periodogram estimate is given
</p>
<p>next. A fast Fourier transform (FFT) is used to compute the Fourier transform of
</p>
<p>the Yi[n] sequences at the frequencies I = -0.5 + k.6.j, where k = 0,1 , ... , 1023 and
.6. j = 1/1024 (see [Kay 1988] for a more detailed description).</p>
<p/>
</div>
<div class="page"><p/>
<p>580 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>8 .,.
</p>
<p>-----....-..
'---"' 6 .
</p>
<p>&lt; ~
4 . . -,
</p>
<p>8
</p>
<p>4 .
</p>
<p>2 . 2
</p>
<p>OL-~~~-~~~-~~~---1
</p>
<p>-0.5 -0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4 0.5
</p>
<p>f
</p>
<p>(a) a = 0.25, a ~ = 1 - a 2 (b) a = 0.98, a~ = 1 - a 2
</p>
<p>Figure 17.13: Power spectral density estimates using the averaged periodogram
</p>
<p>method for autoregressive processes with different parameters. The true PSDs are
</p>
<p>shown in Figure 17.11.
</p>
<p>Nfft=1024; % set FFT size
Pav1=zeros(Nfft,1);Pav2=Pav1; %set up arrays with desired dimension
f=[O:Nfft-1] '/Nfft-O.5; % set frequencies for later plotting
</p>
<p>%of PSD estimate
for i=0:I-1
</p>
<p>nstart=1+i*L;nend=L+i*L; %set up beginning and end points
%of ith block of data
</p>
<p>y1=x1(nstart:nend);
</p>
<p>y2=x2(nstart:nend);
</p>
<p>%take FFT of block, since FFT outputs samples of Fourier
%transform over frequency range [0,1), must shift FFT outputs
%for [1/2,1) to [-1/2, 0), then take complex magnitude-squared,
%normalize by L and average
P a v 1 = P a v 1 + ( 1 / ( I * L ) ) * a b s ( f f t s h i f t ( f f t ( y 1 , N f f t ) ) ) . ~ 2 ;
</p>
<p>P a v 2 = P a v 2 + ( 1 / ( I * L ) ) * a b s ( f f t s h i f t ( f f t ( y 2 , N f f t ) ) ) . ~ 2 ;
</p>
<p>end
</p>
<p>17.8 Continuous-Time WSS Random Processes
</p>
<p>In this section we give the corresponding definitions and formulas for continuous-
</p>
<p>time WSS random processes. A more detailed description can be found in [Papoulis
</p>
<p>1965]. Also, an important example is described to illustrate the use of these formulas.
</p>
<p>A continuous-time random process X(t) for -00 &lt; t &lt; 00 is defined to be WSS</p>
<p/>
</div>
<div class="page"><p/>
<p>17.8. CONTINUOUS-TIME WSS RANDOM PROCESSES
</p>
<p>if the mean fun ction J.Lx (t) sat isfies
</p>
<p>581
</p>
<p>J.L x(t) = E[X(t )] = J.L -oo &lt;t &lt;oo (17.46)
</p>
<p>which is to say it is constant in t ime and an autocorrelation function (ACF) can be
</p>
<p>defined as
</p>
<p>rX(7) = E [X(t)X(t + 7)] - 00&lt;7&lt; 00 (17.47)
</p>
<p>which is not dependent on the value of t. Thus, E[X(tr)X(t2)] depends only on
</p>
<p>It2 - ttl . Not e the use of the "parentheses" indicates that the argument of the ACF
is cont inuous and serves to distinguish rx[k]from rX(7 ). The ACF has the following
</p>
<p>prop erties.
</p>
<p>Property 17.13 - ACF is positive for the zero lag or rx(O) &gt; o.
The total average power is rx(O) = E[X2(t)].
</p>
<p>o
</p>
<p>Property 17.14 - ACF is an even function or rx(-7) = rX(7).
</p>
<p>o
</p>
<p>Property 17.15 - Maximum value of ACF is at 7 = 0 or IrX(7) 1 ~ rx(O).
</p>
<p>o
</p>
<p>Property 17.16 - ACF measures the predictability of a random process.
</p>
<p>The correlat ion coefficient for two samples of a zero mean WSS random process is
</p>
<p>rX(7)
PX(t),X(t+r) = rX (0) .
</p>
<p>o
</p>
<p>Property 17.17 - ACF approaches J.L2 as 7 -+ 00 .
</p>
<p>This assumes that the samples become uncorrelat ed for large lags , which is usually
</p>
<p>the case .
</p>
<p>o
</p>
<p>Property 17.18 - rX (7) is a positive semidefinite function.
</p>
<p>See [Pap oulis 1965] for the definition of a positive semidefinite func tion. This
</p>
<p>property assumes that the some samples of X(t) may be perfectly predictable. If it
</p>
<p>is not, then the ACF is pos it ive definite.
</p>
<p>o</p>
<p/>
</div>
<div class="page"><p/>
<p>582 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>The PSD is defined as
</p>
<p>[
</p>
<p>T/2 2]
Px(F) = lim T
</p>
<p>1
E f X(t) exp( -j27rFt)dt
</p>
<p>T-too -T/2
-oo&lt;F&lt;oo (17.48)
</p>
<p>where F is the frequency in Hz. We use a capital F to denote continuous-time
</p>
<p>or analog frequency. By the Wiener-Khinchine theorem this is equivalent to the
</p>
<p>continuous-time Fourier transform of the ACF
</p>
<p>Px(F) = i: rx(r) exp(-j27rFr)dr
i: rx(r) cos (27rFr)dr.
</p>
<p>(17.49)
</p>
<p>(17.50)
</p>
<p>(See also Problem 17.49.) The PSD has the usual interpretation as the average
</p>
<p>power distribution with frequency. In particular, it is the average power per Hz.
</p>
<p>The average physical power in a frequency band [Fl , F2] is given by
</p>
<p>j
F2
</p>
<p>Average physical power in [Fl , F2 ] = 2 Px(F)dF
Fl
</p>
<p>where again the 2 factor reflects the additional contribution of the negative frequen-
</p>
<p>cies. The properties of the PSD are as follows:
</p>
<p>Property 17.19 - PSD is a real function.
</p>
<p>The PSD is given by the real function
</p>
<p>o
</p>
<p>Property 17.20 - PSD is nonnegative.
</p>
<p>Px(F) 2:: 0
</p>
<p>o
</p>
<p>Property 17.21 - PSD is symmetric about F = O.
</p>
<p>Px(-F) = Px(F)
</p>
<p>o</p>
<p/>
</div>
<div class="page"><p/>
<p>17.8. CONTINUOUS-TIME WSS RANDOM PROCESSES 583
</p>
<p>Property 17.22 - ACF recovered from PSD using inverse Fourier trans-
</p>
<p>form
</p>
<p>i: Px(F) exp(j27fP7)dF
i: Px(F) cos(21rF7)dF
</p>
<p>(See also Problem 17.49.)
</p>
<p>-00&lt;7&lt;00
</p>
<p>- 00 &lt; 7 &lt; 00.
</p>
<p>(17.51)
</p>
<p>(17.52)
</p>
<p>o
Unlike the PSD for a discrete-time WSS random process, the PSD for a continuous-
</p>
<p>time WSS random process is not periodic. We next illustrate these definitions and
</p>
<p>formulas with an example of practical importance.
</p>
<p>Example 17.11 - Obtaining discrete-time WGN from continuous-time
</p>
<p>WGN
</p>
<p>A common model for a continuous-time noise random process X(t) in a physical
</p>
<p>system is a WSS random process with a zero mean. In addition, due to the origin of
</p>
<p>noise as microscopic fluctuations of a large number of electrons, or molecules, etc.,
</p>
<p>a central limit. theorem can be employed to assert that X(t) is a Gaussian random
</p>
<p>variable for all t. The average power of the noise in a band of frequencies is observed
</p>
<p>to be the same for all bands up to some upper frequency limit, at which the average
</p>
<p>power begins to decrease. For instance, consider thermal noise in a conductor due to
</p>
<p>random fluctuations of the electrons about some mean velocity. The average power
</p>
<p>versus frequency is predicted by physics to be constant until a cutoff frequency of
</p>
<p>about Fc = 1000 GHz at room temperature [Bell Telephone Labs 1970]. Hence,
</p>
<p>we can assume that the PSD of the noise has a PSD shown in Figure 17.14 as
</p>
<p>the true PSD. To further simplify the mathematically modeling without sacrificing
</p>
<p>the realism of the model, we can observe that all physical systems will only pass
</p>
<p>frequency components that are much lower than Fc-typically the bandwidth of
</p>
<p>the system is W Hz as shown in Figure 17.14. Any frequencies above W Hz will
</p>
<p>be cut off by the system. Therefore, the noise output of the system will be the
</p>
<p>same whether we use the true PSD or the modeled one shown in Figure 17.10. The
</p>
<p>modeled PSD is given by
</p>
<p>Px(F) = No
2
</p>
<p>- 00 &lt; F &lt; 00.
</p>
<p>This is dearly a physically impossible PSD in that the total average power is
</p>
<p>rx(O) = J~oo Px(F)df = 00. However, its use simplifies much systems analysis
(see Problem 17.50). The corresponding ACF is from (17.51) the inverse Fourier
</p>
<p>transform, which is
</p>
<p>(17.53)</p>
<p/>
</div>
<div class="page"><p/>
<p>584 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>Px(F)
</p>
<p>system passband
</p>
<p>__ L_
modeled PSD
</p>
<p>"'~~..::..:r=-=-=-=~;.......-
-""'--+----L.--~f----...L--_1_~-____.. F
</p>
<p>-Fe -W W Fe
</p>
<p>Figure 17.14: True and modeled PSDs for continuous-time white Gaussian noise.
</p>
<p>and is seen to be an impulse at T = O. Again the nonphysical nature of this model
is manifest by the value rx(O) = 00. A continuous-time WSS Gaussian random
</p>
<p>process with zero mean and the ACF given by (17.53) is called continuous-time
</p>
<p>white Gaussian noise (WGN) (see also Example 20.6) . It is a standard model in
</p>
<p>many disciplines.
</p>
<p>Now as was previously mentioned, all physical systems are bandlimited to W Hz,
</p>
<p>which is typically chosen to ensure that a desired signal with a bandwidth of W Hz
</p>
<p>is not distorted. Modern signal processing hardware first bandlimits the continuous-
</p>
<p>time waveform to a maximum of W Hz using a lowpass filter and then samples the
</p>
<p>output of the filter at the Nyquist rate of F, = 2W samples/sec. The samples are
</p>
<p>then input into a digital computer. An important question to answer is: What are
</p>
<p>the statistical characterist ics of the noise samples that are input to the computer?
</p>
<p>To answer this question we let ~t be the time interval between successive samples.
</p>
<p>Also, let X(t) be the noise at the output of an ideal lowpass filter (H(F) = 1 for
</p>
<p>IFI ~ Wand H(F) = 0 for IFI &gt; W) over the system passband shown in Figure
17.14. Then, the noise samples can be represented as
</p>
<p>X(t)lt=nllt = X[n] for - 00 &lt; n &lt; 00.
</p>
<p>Since X(t) is bandlimited to W Hz and prior to filtering had the modeled PSD
</p>
<p>shown in Figure 17.14, its PSD is
</p>
<p>P (F) = {~ IFI ~ W
x 0 IFI &gt; W.
</p>
<p>The noise samples X[n] comprise a discrete-time random process. Its characteristics
</p>
<p>follow those of X(t). Since X(t) is Gaussian, then so is X[n] (being just a sample).
</p>
<p>Also, since X(t) is zero mean, so is X[n] for all n. Finally, we inquire as to whether
</p>
<p>X[n] is WSS , i.e. , can we define an ACS? To answer this we first note that X[n] =
</p>
<p>X(n~d and recall that X(t) is WSS. Then from the definition of the ACS
</p>
<p>E[X[n]X[n + k]] = E[X(n~dX((n + k)~d]
= rx(k~d (definition of continuous-time ACF)</p>
<p/>
</div>
<div class="page"><p/>
<p>17.8. CONTINUOUS-TIME WSS RANDOM PROCESSES 585
</p>
<p>which does not depend on n, and so X[n] is a zero mean discrete-time WSS random
process with ACS
</p>
<p>rx[k] = rx(k.6.t}. (17.54)
</p>
<p>It is seen to be a sampled version of the continuous-time ACF. To explicitly evaluate
</p>
<p>the ACS we have from (17.51)
</p>
<p>rX(T) = i: Px(F) exp(j21rFT)dF
(w No exp(j21rFT)dF
</p>
<p>J-w 2
N, jW~ cos(21rFT)dF
2 -w
</p>
<p>No sin(21rFT) IW
2 21rT -w
</p>
<p>W
sin(21rWT)
</p>
<p>No 21rWT
</p>
<p>(sine component is odd function)
</p>
<p>(17.55)
</p>
<p>which is shown in Figure 17.15. Now since rx[k] = rx(k.6.t} = rx(k/(2W)), we
</p>
<p>1 .
</p>
<p>0.5
</p>
<p>a
</p>
<p>. .. . .. . . . . . . . . . ..
</p>
<p>-0.5 . . ... , . . . , ... . . , . . .. . , . . . .. , ...
</p>
<p>-1
</p>
<p>o
T
</p>
<p>1 2 3
2W 2W 2W
</p>
<p>Figure 17.15: ACF for bandlimited continuous-time WGN with NoW = 1.
</p>
<p>see from Figure 17.15 that for k = &plusmn;1, &plusmn;2, ... the ACS is zero, being the result of
</p>
<p>sampling the continuous-time ACF at its zeros. The only nonzero value is for k = 0,
</p>
<p>which is r x [0] = r x (0) = NoW from (17.55). Therefore, we finally observe that the
ACS of the noise samples is
</p>
<p>rx[k] = NoWJ[k]. (17.56)</p>
<p/>
</div>
<div class="page"><p/>
<p>586 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>The discrete-time noise random process is indeed WSS and has the ACS of (17.56).
</p>
<p>The PSD corresponding to this ACS has already been found and is shown in Figure
</p>
<p>17.10, where (]"2 = NoW. Therefore, X[n] is a discrete-time white Gaussian noise
</p>
<p>random process. This example justifies the use of the WGN model for discrete-time
</p>
<p>systems analysis.
</p>
<p>&amp;. Sampling faster gives only marginally better performance.
It is sometimes argued that by sampling the output of a system lowpass filter whose
</p>
<p>cutoff frequency is W Hz at a rate greater than 2W, we can improve the performance
</p>
<p>of a signal processing system. For example, consider the estimation of the mean /-L
</p>
<p>based on samples Y[n] = /-L + X[n] for n = 0,1, ... , N - 1 where E[X[n]] = 0,
var(X[n]) = (]"2, and the X[n1 samples are uncorrelated. The obvious estimate is
the sample mean or (l/N) Ln':Ol Y[n], whose expectation is /-L and whose variance
is (]"2/N. Clearly, if we could increase N, then the variance could be reduced and a
</p>
<p>better estimate would result. This suggests sampling the continuous-time random
</p>
<p>process at a rate faster than 2W samples/sec. The fallacy, however, is that as
</p>
<p>the sampling rate increases, the noise samples become correlated as can be seen by
</p>
<p>considering a sampling rate of 4W for which the time interval between samples
</p>
<p>becomes T = b..tl2 = 1/(4W). Then, as observed from Figure 17.15, the correlation
</p>
<p>between successive samples is rx(1/(4W)) = 0.6. In effect, by sampling faster we
are not obtaining any new realizations of the noise samples but nearly repetitions
</p>
<p>of the same noise samples. As a result , the variance will not decrease as l/N but at
</p>
<p>a slower rate (see also Problem 17.51).
</p>
<p>17.9 Real-World Example - Random Vibration Testing
</p>
<p>Anyone who has ever traveled in a jet knows that upon landing, the cabin can
</p>
<p>vibrate greatly. This is due to the air currents outside the cabin which interact with
</p>
<p>the metallic aircraft surface. These pressure variations give rise to vibrations which
</p>
<p>are referred to as turbulent boundary layer noise. A manufacturer that intends to
</p>
<p>attach an antenna or other device to an aircraft must be cognizant of this vibration
</p>
<p>and plan for it. It is customary then to subject the antenna to a random vibration
</p>
<p>test in the lab to make sure it is not adversely affected in flight [McConnell 1995] .
</p>
<p>To do so the antenna would be mounted on a shaker table and the table shaken in
</p>
<p>a manner to simulate the turbulent boundary layer (TBL) noise. The problem the
</p>
<p>manufacturer faces is how to provide the proper vibration signal to the table, which</p>
<p/>
</div>
<div class="page"><p/>
<p>17.9. REAL-WORLD EXAMPLE - RANDOM VIBRATION TESTING 587
</p>
<p>presumably will then be transmitted to the antenna. We now outline a possible
</p>
<p>solution to this problem.
</p>
<p>The National Aeronautics and Space Administration (NASA) has determined
</p>
<p>PSD models for the TBL noise through physical modeling and experimentation.
</p>
<p>A reasonable model for the one-sided PSD of TBL noise upon reentry of a space
</p>
<p>vehicle , such as the space shuttle, into the earth's atmosphere is given by [NASA
</p>
<p>2001]
</p>
<p>{
</p>
<p>Gx(500) 0 ~ F &lt; 500 Hz
Gx(F) = 9 xlO14r2 500 &lt; F &lt; 50000 Hz
</p>
<p>F+11364 --
</p>
<p>where r represents a reference value which is 20J.lPa. A J.lPa is a unit of pressure
</p>
<p>equal to 10-6 nt/m2 &bull; This PSD is shown in Figure 17.16 referenced to the standard
unit so that r = 1. Note that it has a lowpass type of characteristic. In order
</p>
<p>X 10'0
8 ,------,.-------,-----.------r--------,
</p>
<p>2 .. .&bull;. . . ...... .
</p>
<p>7 .
</p>
<p>6 . . . .
</p>
<p>k;'
'--"
</p>
<p>~ 5
Cj
</p>
<p>4 .
</p>
<p>3 ..
</p>
<p>. ,', ,' .
</p>
<p>42 3
F (Hz)
</p>
<p>1'---------'--------'----'--------1.-------.J
o
</p>
<p>Figure 17.16: Continuous-time one-sided PSD for TBL noise.
</p>
<p>to provide a signal to the shaker table that is random and has the PSD shown
</p>
<p>in Figure 17.16, we will assume that the signal is produced in a digital computer
</p>
<p>and then converted via a digital-to-analog convertor to a continuous-time signal.
</p>
<p>Hence , we need to produce a discrete-time WSS random process within the computer
</p>
<p>that has the proper PSD. Recalling our discussion in Section 17.8 we know that
</p>
<p>rx[k] = rx(k~d and since the highest frequency in the PSD is W = 50,000 Hz, we
choose ~t = 1/(2W) = 1/100,000. This produces the discrete-time PSD shown in
Figure 17.17 and is given by Px(J) = (1/(2~t))Gx(J I ~t). (We have divided by two
to obtain the usual two-sided PSD. Also, the sampling operation introduces a factor
</p>
<p>of II ~ t [Jackson 1991].) To generate a realization of a discrete-time WSS random
process with PSD given in Figure 17.17 we will use the AR model introduced in</p>
<p/>
</div>
<div class="page"><p/>
<p>588 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>3.5 .
</p>
<p>3 ,
</p>
<p>2 .
</p>
<p>1.5
</p>
<p>0.5 '-------'----'---'----'----'-------''----'----'-----'------'
-0.5 -0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4 0.5
</p>
<p>f (cycles/sample)
</p>
<p>Figure 17.17: Discrete-time PSD for TBL noise.
</p>
<p>Example 17.5. From the ACS we can determine values of a and ( 7 ~ if we know rx [0]
and rx[l] since
</p>
<p>a
rx[l]
</p>
<p>rx[O]
</p>
<p>2 [(rx[l ] ) 2]rx[O](l - a ) = rx[O] 1 - rx[O] .
</p>
<p>(17.57)
</p>
<p>(17.58)
</p>
<p>Knowing a and ( 7 ~ will allow us to use the defining recursive difference equation,
</p>
<p>X[n] = aX[n -1] + Urn], of an AR random process to generate the realization. To
obtain the first two lags of the ACS we use (17.39)
</p>
<p>1
</p>
<p>rx[O] = [21Px(J)df
2
</p>
<p>1
</p>
<p>rx[l] = [21Px(J) cos(21rf)df
2
</p>
<p>where Px(J) is given in Figure 17.17. These can be evaluated numerically by re-
</p>
<p>placing the integrals with approximating sums to yield rx[O] = 1.5169 x 1015 and
</p>
<p>rx[l] = 4.8483 x 1014 . Then, using (17.57) and (17.58) , we have the AR parame-
</p>
<p>ters a = 0.3196 and ( 7 ~ = 1.362 X 1015 . With these parameters the AR PSD (see
(17.36)) and the true PSD (shown in Figure 17.17) are plotted in Figure 17.18. The
</p>
<p>agreement between them is fairly good except near f = O. Hence, with these values
of the parameters a random process realization could be synthesized within a digital
</p>
<p>computer and then converted to analog form to drive the shaker table.</p>
<p/>
</div>
<div class="page"><p/>
<p>REFERENCES
</p>
<p>3.5 .
</p>
<p>3 ..
</p>
<p>Q .
UJ. 2.5
c,
</p>
<p>2 .
</p>
<p>1.5 ..
</p>
<p>1 ..
</p>
<p>0.5 '----'-----'---'---'----'----'------'---'---'-----'
-0.5 -0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4 0.5
</p>
<p>f (cycles/sample)
</p>
<p>589
</p>
<p>Figure 17.18: Discrete-time PSD and its AR PSD model for TBL noise. The true
</p>
<p>PSD is shown as the dashed line and the AR PSD model as the solid line.
</p>
<p>References
</p>
<p>Bell Telephone Laboratories, Transmission Systems for Communications, Western
</p>
<p>Electric Co, Winston-Salem, NC, 1970.
</p>
<p>Box, E.P.G. , G.M. Jenkins, Time Series Analysis, Forecasting and Control, Holden-
</p>
<p>Day, San Francisco, 1970.
</p>
<p>Brockwell, P.J., R.A. Davis, Time Series: Theory and Methods, Springer-Verlag,
</p>
<p>New York, 1987.
</p>
<p>Harvey, A.C ., Forecasting, Structural Time Series Models and the Kalman Filter,
</p>
<p>Cambridge University Press, New York, 1989.
</p>
<p>Jackson, L.B. , Signals, Systems, and Transforms, Addison-Wesley, Reading, MA
</p>
<p>1991.
</p>
<p>Jenkins, G.M., D.G . Watts, Spectral Analysis and Its Applications, Holden-Day,
</p>
<p>San Francisco, 1968.
</p>
<p>Kay, S., Modern Spectral Estimation, Theory and Applications, Prentice-Hall, En-
</p>
<p>glewood Cliffs, NJ, 1988.
</p>
<p>McConnell, K.G. , Vibration Testing, John Wiley &amp; Sons, New York, 1995.
</p>
<p>NASA, Dynamic Environmental Criteria, NASA Technical Handbook, pp. 50-51,
</p>
<p>NASA-HDBK-7005, March 13, 2001.</p>
<p/>
</div>
<div class="page"><p/>
<p>590 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>Papoulis, A., Probability, Random Variables, and Stochastic Processes, McGraw-
</p>
<p>Hill , New York, 1965.
</p>
<p>Robinson, E.A., Multichannel Time Series Analysis with Digital Computer Pro-
</p>
<p>grams, Holden-Day, San Francisco, 1967.
</p>
<p>Schuster, A., "On the Investigation of Hidden Periodicities with Applications to a
</p>
<p>Supposed 26 Day Period of Meterological Phenomena," Terr. Magn., Vol. 3,
</p>
<p>pp. 13-41, March 1898.
</p>
<p>Schwartz, M., L. Shaw, Signal Processing, Discrete Spectral Analysis, Detection,
</p>
<p>and Estimation, McGraw-Hill, New York, 1975.
</p>
<p>Taylor, S., Modelling Financial Time Series , John Wiley &amp; Sons, New York, 1986.
</p>
<p>Tong, H. , Non-linear Tim e Series , Oxford University Press, New York, 1990.
</p>
<p>Problems
</p>
<p>17.1 C..:..) (w) A Bernoulli random process X[n] for -00 &lt; n &lt; 00 consists of
independent random variables with each random variable taking on the values
</p>
<p>+1 and -1 with probabilities p and 1 - p , respectively. Is this random process
WSS? If it is WSS, find its mean sequence and autocorrelation sequence.
</p>
<p>17.2 (w) Consider the random process defined as X[n] = aoU[n] + alU[n - 1] for
-00 &lt; n &lt; 00, where ao and al are constants, and Urn] is an IID random
process with each Urn] having a mean of zero and a variance of one. Is this
random process WSS? If it is WSS, find its mean sequence and autocorrelation
</p>
<p>sequence.
</p>
<p>17.3 (w) A sinusoidal random process is defined as X[n] = A cos(27110n) for -00 &lt;
n &lt; 00, where 0 &lt; fo &lt; 0.5 is a discrete-time frequency, and A rv N(O, 1). Is
this random process WSS? If it is WSS, find its mean sequence and autocor-
</p>
<p>relation sequence.
</p>
<p>17.4 (f) A WSS random process has E[X[O]] = 1 and a covariance sequence cx[nl' n2] =
2&lt;5[n2 - nl]' Find the ACS and plot it.
</p>
<p>17.5 t.:.:') (w) A random process X[n] for -00 &lt; n &lt; 00 consists of independent
random variables with
</p>
<p>X[n] rv { N(O,l) for n even
U(-V3,V3) for n odd.
</p>
<p>Is this random process WSS? Is it stationary?</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 591
</p>
<p>(17.59)
</p>
<p>17.6 (w) The random processes X[n] and Y[n] are both WSS. Every sample of
X [n] is independent of every sample of Y[n]. Is Z[n] = X[n] + Y[n] WSS? If
it is WSS , find its mean sequence and autocorrelation sequence.
</p>
<p>17.7 (w) The random processes X[n] and Y[n] are both WSS. Every sample of
X[n] is independent of every sample of Y[n]. Is Z[n] = X[n]Y[n] WSS? If it
is WSS , find its mean sequence and autocorrelation sequence.
</p>
<p>17.8 (f) For the ACS rx[k] = {1/2)k for k ~ 0 and rx[k] = (1/2)-k for k &lt; 0,
verify that Properties 17.1-17.3 are satisfied.
</p>
<p>17.9 L..:.J (w) For the sequence rx[k] = ab1kl for -00 &lt; k &lt; 00, determine the
values of a and b that will result in a valid ACS.
</p>
<p>17.10 (w) A periodic WSS random process with period P is defined to be a random
process X[n] whose ACS satisfies rx[k + P] = rx[k] for all k, An example
is the randomly phased sinusoid of Example 17.10 for which P = 10. Show
</p>
<p>that the correlation coefficient for two samples of a zero mean periodic random
</p>
<p>process that are separated by P samples is one. Comment on the predictability
</p>
<p>of X[n + P] based on X[n] = x[n].
</p>
<p>17.11 (w) A WSS random process has an ACS rx[k] and mean J.L. Find the corre-
lation coefficient for two samples of the random process that are separated by
</p>
<p>k samples.
</p>
<p>17.12 c.:.:..-) (w) Which of the sequences in Figure 17.19 cannot be valid ACSs? If
the sequence cannot be an ACS, explain why not.
</p>
<p>17.13 (w) For the randomly phased sinusoid described in Example 17.4 find the
</p>
<p>optimal linear prediction of X[l] based on observing X[O] = x[O], and also of
X[10] based on observing X[O] = x[O]. Can either of these samples be perfectly
predicted? Explain why or why not.
</p>
<p>17.14 (w) For the AR random process described in Example 17.10 find the optimal
linear prediction of X[no+ko] based on observing X[no] = x[no]. How accurate
is your prediction in terms of MSE as ko increases?
</p>
<p>17.15 (t) In this problem we derive rx[O] for the AR random process described in
Example 17.5. To do so assume that X[n] can be written as
</p>
<p>00
</p>
<p>X[n] = I: akU[n - k].
k=O
</p>
<p>This was shown to be true in Example 17.5. Then verify that rx[O] can be
</p>
<p>written as
00 00
</p>
<p>rx[O] = I:I: akaIE[U[n - k]U[n -I]]
k=OI=O</p>
<p/>
</div>
<div class="page"><p/>
<p>592 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>. . . . . , .. ......., .
</p>
<p>I
. . ; . .. . ; .. .. , ..
</p>
<p>. . .. . .. .
</p>
<p>J J
. .. . . . . . . ....
</p>
<p>. . .. .. ...... .... ... -....
; ; :
</p>
<p>- 1
</p>
<p>0.5
</p>
<p>-0.5
</p>
<p>. .&bull; &bull; &bull; . &bull; &bull;&bull;&bull; &bull;&bull; &bull;&bull; &bull; &bull; . , &bull; &bull; &bull;&bull; &bull; &bull; &bull; &bull;&bull;&bull; , &bull;&bull; &bull;&bull; , &bull;&bull; &bull;&bull; j &bull;&bull;
</p>
<p>- 1
</p>
<p>o.s &bull; ..&bull; ..'I , .
-0.5
</p>
<p>- 1
</p>
<p>05 .: r '.&bull;1 1.&bull;&bull;&bull;
1
T&middot;
</p>
<p>- 0.5 .
</p>
<p>-4 -3 - 2 - 1 -4 -3 -2 - 1 0
k
</p>
<p>1 2 3 4 -4 -3 -2 -1 ~ 1 2 3 4
</p>
<p>(a) (b) (c)
</p>
<p>f &middot; &middot; &middot; &middot; &middot; ; &middot; &middot; &middot; &middot; &middot; ; &middot; &middot; &middot; &middot; &middot; ; &middot; &middot; &middot; &middot; &middot; ; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; &middot; , &middot; ... , ..
</p>
<p>0.5
</p>
<p>- 0.5
</p>
<p>-1 .
</p>
<p>-0.5
</p>
<p>- 1 .
</p>
<p>0.5
</p>
<p>-0 .5 .
</p>
<p>- 1 .
</p>
<p>-4 -3 -2 - 1 ~ 1 2 3 4 -4 -3 -2 -1 0
k
</p>
<p>1 2 3 4 -4 -3 -2 -1 ~ 1 2 3 4
</p>
<p>(d) (e) (f )
</p>
<p>Figure 17.19: Possible ACSs for Problem 17.12.
</p>
<p>and use the properties of the Urn] random pro cess to finish the derivation.
</p>
<p>17.16 (t) Using a similar approach to the one used in Problem 17.15 derive the
ACS for the AR random process described in Example 17.5. Hint: Start with
</p>
<p>the definition of the ACS and use (17.59).
</p>
<p>17.17 C.:,) (w) To generate a realization of an AR process on the computer we
can use the recursive difference equation X[n] = aX[n - 1] + Urn] for n ;:::
O. However, in doing so, we soon realiz e that the initial condition X[-I]
is required. Assume that we set X[-I] = 0 and use the recursion X[O] =
U[O], X[I] = aX[O] + U[I] , .... Determine the mean and variance of X[n] for
n ~ 0, where as usual Urn] consists of uncorrelated random variables with
zero mean and variance (7[;. Does the mean depend on n? Does the variance
depend on n? What happens as n -+ oo? Hint: First show that X[n] can be
written as X[n] = L:~=o akU[n - k] for n ~ O.
</p>
<p>17.18 (w) This problem continues Problem 17.17. Instead ofletting X [-I] = 0, set
X[-I] equal to a random variable with mean 0 and a variance of (7[;/(1- a2 )
and that is uncorrelated with Urn] for n ;::: O. Find the mean and variance of</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 593
</p>
<p>X[O] . Explain your results and why this makes sense.
</p>
<p>17.19 C.:J (w) An example of a sequence that is not positive semidefinite is r[O] =
1, r[-l] = r[l] = -7/8 and equals zero otherwise. Compute the determinant
of the 1 x 1 principal minor, the 2 x 2 principal minor, and the 3 x 3 principal
</p>
<p>minor of the 3 x 3 autocorrelation matrix Rx using these values. Also, plot
</p>
<p>the discrete-time Fourier transform of r[k]. Why do you think the positive
</p>
<p>semidefinite property is important?
</p>
<p>17.20 C.':") (w) For the general MA random process of Example 17.6 show that the
</p>
<p>process is WSS.
</p>
<p>17.21 (f) Use (17.28) to show that the MA random process defined in Example
17.6 is ergodic in the mean.
</p>
<p>17.22 (t,f) Show that a WSS random process whose ACS satisfies rx[k] = f-l2 for
k &gt; ko ~ 0 must be ergodic in the mean.
</p>
<p>17.23 (t) Prove (17.28) by using the relationship
</p>
<p>N-IN-l N-l
</p>
<p>L L g[i - j] = L (N -Ikl)g[k].
i=O j=O k=-(N-l)
</p>
<p>Try verifying this relationship for N = 3.
</p>
<p>17.24 (f) For the random DC level defined in Example 17.7 prove that var(ji,N) = 1.
</p>
<p>17.25 (f) Explain why the randomly phased sinusoid defined in Example 17.4 is
</p>
<p>ergodic in the mean. Next show that it is ergodic in the ACS in that
</p>
<p>N-l-k
</p>
<p>lim rx[k] = lim N 1 k '" X[n]X[n+k] = ! cos(21l"(O.1)k) = rx[k] k ~ 0
N -too N -too - L....J 2
</p>
<p>n=O
</p>
<p>by computing rx[k] directly. Hint: Use the fact that
</p>
<p>limN-too(1/(N - k&raquo; L:,:':Ol-k cos(21l"fn + &cent;) = 0 for any 0 &lt; f &lt; 1 and any
phase angle &cent;. This is because the temporal average of an infinite duration
sinusoid is zero.
</p>
<p>17.26 (t) Show that the formula
</p>
<p>M M 2M
</p>
<p>L L g[m -n] = L (2M + 1-lkl)g[k]
m=-M n=-M k=-2M
</p>
<p>is true for M = 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>594 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>17.27 (t) Argue that
</p>
<p>lim ~ (1- Ikl ) rx[k]exp(-j271-jk) = ~ rx[k]exp(-j27l"fk)
M-+oo LJ 2M + 1 LJ
</p>
<p>k=-2M, , k=-oo
v
</p>
<p>w[k]
</p>
<p>by drawing pictures of rx[k], which decays to zero, and overlay it with w[k]
</p>
<p>as M increases.
</p>
<p>17.28 C:.:") (w) For the differenced random process defined in Example 17.1 deter-
mine the PSD. Explain your results.
</p>
<p>17.29 (f) Determine the PSD for the randomly phased sinusoid described in Exam-
ple 17.4. Is this result reasonable? Hint: The discrete-time Fourier transform
</p>
<p>of exp(j27l"fon) for -1/2 &lt; fa &lt; 1/2 is o(f - fa) over the frequency interval
-1/2 ::; f ::; 1/2.
</p>
<p>17.30 C:.:J (w) A random process is defined as X[n] = AU[n], where A rv N(O, O"~)
and Urn] is white noise with variance O"b. The random variable A is indepen-
dent of all the samples of Urn]. Determine the PSD of X[n].
</p>
<p>17.31 (w) Find the PSD of the random process X[n] = (1/2)lnIU[n] for -00 &lt; n &lt;
00, where Urn] is white noise with variance O"b.
</p>
<p>17.32 (w) Find the PSD of the random process X[n] = aoU[n] +alU[n-1]' where
ao ,al are constants and Urn] is white noise with variance O"b = 1.
</p>
<p>17.33 (w) A Bernoulli random process consists of lID Bernoulli random variables
</p>
<p>taking on values +1 and -1 with equal probabilities. Determine the PSD and
explain your results.
</p>
<p>17.34 C:..:..-) (w) A random process is defined as X[n] = Urn] + f-l for -00 &lt; n &lt; 00,
where Urn] is white noise with variance O"b. Find the ACS and PSD and plot
your results.
</p>
<p>17.35 (w,c) Consider the AR random process defined in Example 17.5 and de-
</p>
<p>scribed further in Example 17.10 with -1 &lt; a &lt; 0 and for some O"b &gt; O. Plot
the PSD for several values of a and explain your results.
</p>
<p>17.36 (f,c) Plot the corresponding PSD for the ACS
</p>
<p>{
</p>
<p>1 k = 0
1/2 k = &plusmn;1
</p>
<p>rx[k] = 1/4 k = &plusmn;2
</p>
<p>o otherwise.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 595
</p>
<p>17.37 (w) If a random process has the PSD Px (J) = 1+cos(27fJ) , are the samples
of the random process uncorrelated?
</p>
<p>17.38 C...:....) (f) If a random process has the PSD Px(J) = 11 + exp(-j27fJ) +
(1/2) exp( -j47fJ)12 , determine the ACS.
</p>
<p>17.39 (c) For the AR random processes whose ACSs are shown in Figure 17.6
</p>
<p>generate a realization of N = 2000 samples for each process. Use the MATLAB
code segment given in Section 17.4 to do this. Then, estimate the ACS for
</p>
<p>k = 0,1 , ... , 30 and plot the results. Compare your results to those shown in
</p>
<p>Figure 17.12 and explain.
</p>
<p>17.40 C..:.,,) (w) A PSD is given as Px(J) = a+bcos(27fJ) for some constants a and
b. What values of a and b will result in a valid PSD?
</p>
<p>17.41 (f) A PSD is given as
</p>
<p>Px(J) = { 2
0
</p>
<p>- 8f os f :S 1/4
1/4 &lt; f :S 1/2.
</p>
<p>Plot the PSD and find the total average power in the random process.
</p>
<p>17.42 C:_:J (c) Plot 50 realizations of the randomly phased sinusoid described in
Example 17.4 with N = 50, and overlay the samples in a scatter diagram plot
such as shown in Figure 16.15. Explain the results by referring to the PDF of
</p>
<p>Figure 16.12. . Next est imate the following quantities: E[X[10]],E[X[12]],
E[X[10]X[12]],E[X[12]X[14]] by averaging down the ensemble, and compare
your simulated results to the theoretical values.
</p>
<p>17.43 (c) In this problem we support the results of Problem 17.18 by using a com-
</p>
<p>puter simulation. Specifically, generate M = 10,000 realizations of the AR
</p>
<p>random process X[n] = 0.95X[n -1] +Urn] for n = 0,1 , ... ,49, where Urn] is
WGN with &lt;7D = 1. Do so two ways: for the first set ofrealizations let X[-I] =
oand for the second set of realizations let X [-1] '"'" N (0, &lt;7D / (1 - a2 ) ) , using a
different random variable for each realization. Now estimate the variance for
</p>
<p>each sample time n, which is rx[O], by averaging X 2[n] down the ensemble of
realizations. Do you obtain the theoretical result of rx[O] = &lt;7D/(l - a2 )?
</p>
<p>17.44 C..:.,,) (c) Generate a realization of discrete-time white Gaussian noise with
variance &lt;7i = 1. For N = 64, N = 128, and N = 256, plot the periodogram.
What is the true PSD? Does your estimated PSD get closer to the true PSD
</p>
<p>as N increases? If not , how could you improve your estimate?
</p>
<p>17.45 (c) Generate a realization of an AR random process of length N = 31,000
</p>
<p>with a = 0.25 and &lt;7D = 1-a2 . Break up the data set into 1000 nonoverlapping
blocks of data and compute the periodogram for each block. Finally, average</p>
<p/>
</div>
<div class="page"><p/>
<p>596 CHAPTER 17. WIDE SENSE STATIONARY RANDOM PROCESSES
</p>
<p>the periodograms together for each point in frequency to determine the final
</p>
<p>averaged periodogram estimate. Compare your results to the theoretical PSD
</p>
<p>shown in Figure 17.11a.
</p>
<p>17.46 (f) A continuous-time randomly phased sinusoid is defined by X(t) =
cos(211"Fot + 8), where 8 '" U(O , 211"). Determine the mean function and ACF
for this random process.
</p>
<p>17.47 c.:...:J (f) For the PSD Px(F) = exp( -IF!), determine the average power in
the band [10, 100] Hz.
</p>
<p>17.48 (w) If a PSD is given as Px(F) = exp( -IF/Fo!), what happens to the ACF
as Fo increases and also as Fo --7 oo?
</p>
<p>17.49 (t) Based on (17.49) derive (17.50), and also based on (17.51) derive (17.52).
</p>
<p>17.50 k.:...) (w) A continuous-time white noise random process U(t) whose PSD is
given as Pu(F) = No/2 is integrated to form the continuous-time MA random
process
</p>
<p>X(t) = T
1 r
</p>
<p>t
</p>
<p>U(~)d~.
It-T
</p>
<p>Determine the mean function and the variance of X(t) . Does X(t) have infinite
</p>
<p>total average power?
</p>
<p>17.51 c.:...:.-) (w,c) Consider a continuous-time random process X(t) = J-L + U(t),
where U(t) is zero mean and has the ACF given in Figure 17.15. If X(t) is
</p>
<p>sampled at twice the Nyquist rate, which is F, = 4W, determine the ACS of
X[n]. Next using (17.28) find the variance of the sample mean estimator [J,N
for N = 20. Is it half of the variance of the sample mean estimator if we had
</p>
<p>sampled at the Nyquist rate and used N = 10 samples in our estimate? Note
that in either case the total length of the data interval in seconds is the same,
</p>
<p>which is 20/(4W) = 10/(2W).
</p>
<p>17.52 (f) A PSD is given as
</p>
<p>Model this PSD by using an AR PSD as was done in Section 17.9. Plot the
</p>
<p>true PSD and the AR model PSD.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 18
</p>
<p>Linear Systems and Wide Sense
</p>
<p>Stationary Random Processes
</p>
<p>18.1 Introduction
</p>
<p>Most physical systems are conveniently modeled by a linear system. These include
</p>
<p>electrical circuits, mechanical machines, human biological functions, and chemical
</p>
<p>reactions, just to name a few. When the system is capable of responding to a
</p>
<p>continuous-time input , its effect can be described using a linear differential equation.
</p>
<p>For a system that responds to a discrete-time input a linear difference equation
</p>
<p>can be used to characterize the effect of the system. Furthermore, for systems
</p>
<p>whose characteristics do not change with time, the coefficients of the differential or
</p>
<p>difference equation are constants. Such a system is termed a linear time invariant
</p>
<p>(LTI) system for continuous-time inputs/outputs and a linear shift invariant (LSI)
</p>
<p>system for discrete-time inputs/outputs. In this chapter we explore the effect of these
</p>
<p>systems on wide sense stationary (WSS) random process inputs. The reader who is
</p>
<p>unfamiliar with the basic concepts of linear systems should first read Appendix D for
</p>
<p>a brief introduction. Many excellent books are available to supplement this material
</p>
<p>[Jackson 1991, Oppenheim, Willsky, and Nawab 1997, Poularikas and Seely 1985].
</p>
<p>We will now consider only discrete-time systems and discrete-time WSS random
</p>
<p>processes. A summary of the analogous concepts for the continuous-time case is
</p>
<p>given in Section 18.6.
</p>
<p>The importance of LSI systems is that they maintain the wide sense stationarity
</p>
<p>of the random process. That is to say, if the input to an LSI system is a WSS
</p>
<p>random process, then the output is also a WSS random process. The mean and ACS,
</p>
<p>or equivalently the PSD, however, are modified by the action of the system. We will
</p>
<p>be able to obtain simple formulas yielding these quantities at the system output. In
</p>
<p>effect, the linear system modifies the first two moments of the random process but
</p>
<p>in an easily determined and intuitively pleasing way. This allows us to assess the
</p>
<p>effect of a linear system on a WSS random process and therefore provides a means</p>
<p/>
</div>
<div class="page"><p/>
<p>598 CHAPTER 18. LINEAR SYSTEMS AND WSS RANDOM PROCESSES
</p>
<p>to produce a WSS random process at the output with some desired characteristics.
</p>
<p>Furthermore, the theory is easily extended to the case of multiple random processes
</p>
<p>and multiple linear systems as we will see in the next chapter.
</p>
<p>18.2 Summary
</p>
<p>For the linear shift invariant system shown in Figure 18.1 the output random process
</p>
<p>is given by (18.2). If the input random process is WSS, then the output random
</p>
<p>process is also WSS. The output random process has a mean given by (18.9), an ACS
</p>
<p>given by (18.10), and a PSD given by (18.11). If the input WSS random process
</p>
<p>is white noise , then the output random process has the ACS of (18.15). In Section
</p>
<p>18.4 the PSD is interpreted, using the results of Theorem 18.3.1, as the average
</p>
<p>power in a narrow frequency band divided by the width of the frequency band. The
</p>
<p>application of discrete-time linear systems to estimation of samples of a random
</p>
<p>process is explored in Section 18.5. Generically known as Wiener filtering, there are
</p>
<p>four separate problems defined, of which the smoothing and prediction problems
</p>
<p>are solved. For smoothing of a random process signal in noise the estimate is given
</p>
<p>by (18.20) and the optimal filter has the frequency response of (18.25). A specific
</p>
<p>application is given in Example 18.4 to estimation of an AR signal that has been
</p>
<p>corrupted by white noise. The minimum MSE of the optimal Wiener smoother
</p>
<p>is given by (18.27) . One-step linear prediction of a random process sample based
</p>
<p>on the current and all past samples as given by (18.21) leads to the optimal filter
</p>
<p>impulse response satisfying the infinite set of linear equations of (18.28). The general
</p>
<p>solution is summarized in Section 18.5.2 and then illustrated in Example 18.6. For
</p>
<p>linear prediction based on the current sample and a finite number of past samples
</p>
<p>the optimal impulse response is given by the solution of the Wiener-Hopf equations
</p>
<p>of (18.36). The corresponding minimum MSE is given by (18.37). In particular, if
</p>
<p>the random process is an AR random process of order p, the Wiener-Hopf equations
</p>
<p>are the same as the Yule-Walker equations of (18.38) and the minimum mean square
</p>
<p>error equation of (18.37) is the same as for the white noise variance of (18.39). In
</p>
<p>Section 18.6 the corresponding formulas for a continuous-time random process that
</p>
<p>is input to a linear time invariant system are summarized. The mean at the output
</p>
<p>is given by (18.40), the ACF is given by (18.41), and the PSD is given by (18.42).
</p>
<p>Example 18.7 illustrates the use of these formulas. In Section 18.7 the application
</p>
<p>of AR random process modeling to speech synthesis is described. In particular, it
</p>
<p>is shown how a segment of speech can first be modeled, and then how for an actual
</p>
<p>segment of speech, the parameters of the model can be extracted. The model with
</p>
<p>its estimated parameters can then be used for speech synthesis.
</p>
<p>18.3 Random Process at Output of Linear System
</p>
<p>We wish to consider the effect of an LSI system on a discrete-time WSS random
</p>
<p>process. We will from time to time refer to the linear system as a filter, a term that</p>
<p/>
</div>
<div class="page"><p/>
<p>18.3. RANDOM PROCESS AT OUTPUT OF LINEAR SYSTEM 599
</p>
<p>is synonomous. In Section 18.6 we summarize the results for a continuous-time WSS
</p>
<p>random process that is input to an LTI system. To proceed, let Urn] be the WSS
</p>
<p>random process input and X[n] be the random process output of the system. We
</p>
<p>generally represent an LSI system schematically with its input and output as shown
</p>
<p>in Figure 18.1. Previously, in Chapters 16 and 17 we have seen several examples
</p>
<p>Linear shift
Urn] invariant
</p>
<p>n= ... ,-I,O,I, ... system
</p>
<p>1-----1~ X [n]
</p>
<p>n= ... ,-I,O,I, .. .
</p>
<p>(18.1)
</p>
<p>Figure 18.1: Linear shift invariant system with random process input and output.
</p>
<p>of LSI systems with WSS random process inputs. One example is the MA random
</p>
<p>process (see Example 16.7) for which X[n] = (U[n] +Urn -1])/2, with Urn] a white
Gaussian noise process with variance ( J ~ . (Recall that discrete-time white noise is
</p>
<p>a zero mean WSS random process with ACS ru[k] = (J~8[k].) We may view the
</p>
<p>MA random process as the output X[n] of an LSI filter excited at the input by the
</p>
<p>white Gaussian noise random process Urn]. (In this chapter we will be considering
</p>
<p>only the first two moments of X[n]. That Urn] is a random process consisting of
</p>
<p>Gaussian random variables is of no consequence to these discussions. The same
</p>
<p>results are obtained for any white noise random process Urn] irregardless of the
</p>
<p>marginal PDFs. In Chapter 20, however, we will consider the joint PDF of samples
</p>
<p>of X[n] , and in that case, the fact that Urn] is white Gaussian noise will be very
</p>
<p>important.) The averaging operation can be thought of as a filtering by the LSI
</p>
<p>filter having an impulse response
</p>
<p>{
</p>
<p>~ k = &deg;
h[k] = ~ k = 1
</p>
<p>&deg; otherwise.
(Recall that the impulse response h[n] is the output of the LSI system when the
</p>
<p>input urn] is a unit impulse 8[n].) This is because the output of an LSI filter is
</p>
<p>obtained using the convolution sum formula
</p>
<p>00
</p>
<p>X[n] = L h[k]U[n - k]
k=-OQ
</p>
<p>so that upon using (18.1) in (18.2) we have
</p>
<p>X[n] h[O]U[n] + h[I]U[n - 1]
1 1
2"U[n] + 2"U[n - 1]
1
2"(U[n] + Urn - 1]).
</p>
<p>(18.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>600 CHAPTER 18. LINEAR SYSTEMS AND WSS RANDOM PROCESSES
</p>
<p>In general, the LSI system will be specified by giving its impulse response h[k] for
</p>
<p>-00 &lt; k &lt; 00 or equivalently by giving its system function, which is defined as the
z-t ransform of the impulse response. The system function is thus given by
</p>
<p>00
</p>
<p>1i(z) = L h[k]z-k.
k=-oo
</p>
<p>(18.3)
</p>
<p>(18.4)
</p>
<p>In addition, we will have need for the frequency responseof the LSI system, which is
</p>
<p>defined as the discrete-time Fourier transform of the impulse response. It is therefore
</p>
<p>given by
00
</p>
<p>H(J) = L h[k]exp(-j27ffk).
k=-oo
</p>
<p>This function assesses the effect of the system on a complex sinusoidal input sequence
</p>
<p>urn] = exp(j27f fan) for -00 &lt; n &lt; 00. It can be shown that the response of the
system to this input is x[n] = H(Jo) exp(j27f fan) = H(Jo)u[n] (use (18.2) with the
deterministic input urn] = exp(j27f fan)). Hence, its name derives from the fact that
the system action is to modify the amplitude of the complex sinusoid by IH(Jo)I and
the phase of the complex sinusoid by L.H(Jo), but otherwise retains the complex
</p>
<p>sinusoidal sequence. It should also be noted that the frequency response is easily
</p>
<p>obtained from the system function as H(J) = 1i(exp(j27fJ)) . For the MA random
</p>
<p>process we have upon using (18.1) in (18.3) that the system function is
</p>
<p>1i(z) = ~ + ~z-l
2 2
</p>
<p>and the frequency response is the system function when z is replaced by exp(j27f J) ,
</p>
<p>yielding
1 1
</p>
<p>H(J) ="2 + "2 exp (- j 27fJ) .
</p>
<p>It is said that the system function has been evaluated "on the unit circle in the
</p>
<p>z-plane" .
</p>
<p>We next give an example to determine the characteristics of the output random
</p>
<p>process of an LSI system with a WSS input random process. The previous example
</p>
<p>is generalized slightly to prepare for the theorem to follow.
</p>
<p>Example 18.1 - Output random process characteristics
</p>
<p>Let Urn] be a WSS random process with mean J.Lu and ACS ru[k]. This random
</p>
<p>process is input to an LSI system with impulse response
</p>
<p>{
</p>
<p>h[O] k = 0
</p>
<p>h[k] = h[l] k = 1
o otherwise.
</p>
<p>This linear syst em is called a finite impulse response (FIR) filter since its impulse
</p>
<p>response has only a finite number of nonzero samples. We wish to determine if</p>
<p/>
</div>
<div class="page"><p/>
<p>18.3. RANDOM PROCESS AT OUTPUT OF LINEAR SYSTEM 601
</p>
<p>a. the output random process is WSS and if so
</p>
<p>b. its mean sequence and ACS.
</p>
<p>The output of the linear system is from (18.2)
</p>
<p>X[n] = h[O]U[n] + h[I]U[n - 1] .
</p>
<p>The mean sequence is found as
</p>
<p>E[X[n]] h[O]E[U[n]] + h[I]E[U[n - 1]]
= h[O]/-Lu + h[I]/-Lu
= (h[O] + h[I])/-Lu
</p>
<p>so that the mean is constant with time and is given by
</p>
<p>ux = (h[O] + h[I])/-Lu.
</p>
<p>It can also be written from (18.4) as
</p>
<p>00
</p>
<p>p-x = L h[k]exp(-j21rfk) uo = H(O)/-Lu.
k= - oo 1=0
</p>
<p>The mean at the output of the LSI system is seen to be modified by the frequency
</p>
<p>response evaluated at f = O. Does this seem reasonable? Next, if E[X[n]X[n+k]] is
found not to depend on n, we will be able to conclude that X[n] is WSS. Continuing
we have
</p>
<p>E[X[n]X[n + k]] E[(h[O]U[n] + h[I]U[n - 1])(h[O]U[n + k] + h[I]U[n + k - 1])]
h2[0]E[U[n]U[n + k]] + h[O]h[I]E[U[n]U[n + k - 1]]
</p>
<p>+ h[l]h[O]E[U[n - I]U[n + k]] + h2[I]E[U[n - I]U[n + k - 1]]
(h2[0] + h2 [1]) ru [k] + h[O]h[l]ru[k - 1] + h[l]h[O]ru[k + 1]
</p>
<p>and is seen not to depend on n. Hence , X[n] is WSS and its ACS is
</p>
<p>rx[k] = (h2[0] + h2 [1])ru[k] + h[O]h[l]ru[k - 1] + h[l]h[O]ru[k + 1]. (18.5)
</p>
<p>c
Using the previous example for sake of illustration, we next show that the ACS of the
</p>
<p>output random process of an LSI system can be written as a multiple convolution
</p>
<p>of sequences. To do so consider (18.5) and let
</p>
<p>g[O] h2[0] + h2[1]
</p>
<p>g[l] h[O]h[l]
</p>
<p>g[-1] h[1]h[O]</p>
<p/>
</div>
<div class="page"><p/>
<p>(18.8)
</p>
<p>602 CHAPTER 18. LINEAR SYSTEMS AND WSS RANDOM PROCESSES
</p>
<p>and zero otherwise. Then
</p>
<p>rx[k] = g[O]ru[k] + g[l]ru[k - 1] + g[-l]ru[k + 1]
1
</p>
<p>L g[j]ru[k - j]
j=-l
</p>
<p>g[k] * ru[k] (definition of convolution sum) (18.6)
where * denotes convolution. Also, it is easily shown by direct computation that
</p>
<p>o
</p>
<p>g[k] L h[-j]h[k - j]
j=-l
</p>
<p>h[-k] * h[k] (18.7)
and therefore from (18.6) and (18.7) we have the final result
</p>
<p>rx[k] = (h[-k] * h[k])* ru[k]
= h[-k] * h[k] * ru[k].
</p>
<p>The parentheses can be omitted in (18.8) since the order in which the convolu-
</p>
<p>tions are carried out is immaterial (due to associative and commutative property of
</p>
<p>convolution) .
</p>
<p>To find the PSD of X[n] we note from (18.4) that the Fourier transform of the
</p>
<p>impulse response is the frequency response and therefore
</p>
<p>F{h[k]} = H(J)
</p>
<p>F{h[-k]} = H*(J)
</p>
<p>where F indicates the discrete-time Fourier transform. Fourier transforming (18.8)
</p>
<p>produces
</p>
<p>Px(J) = H*(J)H(J)Pu(J)
</p>
<p>or finally we have
</p>
<p>Px(J) = IH(J)12 Pu(J).
</p>
<p>This is the fundamental relationship for the PSD at the output of an LSI system-the
</p>
<p>output PSD is the input PSD multiplied by the magnitude-squared of the frequency
</p>
<p>response. We summarize the foregoing results in a theorem.
</p>
<p>Theorem 18.3.1 (Random Process Characteristics at LSI System Output)
</p>
<p>If a WSS random process Urn] with mean J.Lu and ACS ru[k] is input to an LSI
</p>
<p>system which has an impulse response h[k] and frequency response H(J), then the
</p>
<p>output random process X[n] = 2:~-00 h[k]U[n - k] is also WSS and
</p>
<p>00
</p>
<p>rx[k]
</p>
<p>Px(J)
</p>
<p>L h[k]J.Lu = H(O)J.Lu
k=-oo
</p>
<p>= h[-k] * h[k] * ru[k]
IH(J)12Pu(J).
</p>
<p>(18.9)
</p>
<p>(18.10)
</p>
<p>(18.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>18.3. RANDOM PROCESS AT OUTPUT OF LINEAR SYSTEM 603
</p>
<p>Proof: The mean sequence at the output is
</p>
<p>~x[n] = E[X[n]] ~ E li=oo h[k]U[n - k]]
00
</p>
<p>= L h[k]E[U[n - k]]
k=-oo
</p>
<p>00
</p>
<p>L h[k]j.lu = H(O)j.lu
k=-oo
</p>
<p>(U[n] is WSS)
</p>
<p>Tu[k-j+i]
i=-ooj=-oo
</p>
<p>and is not dependent on n. To determine if an ACS can be defined, we consider
</p>
<p>E[X[n]X[n + k]]. This becomes
</p>
<p>E[X[n]X[n + k]] ~ E [,too h[i]U[n - i] ;tooh[j]U[n + k - j]]
00 00
</p>
<p>L L h[i]h[j] ~[U[n - i]U[n + k - jn
v
</p>
<p>since Urn] was assumed to be WSS. It is seen that there is no dependence on nand
hence X[n] is WSS. The ACS is
</p>
<p>00 00
</p>
<p>rx[k] = L L h[i]h[j]ru[(k + i) - j]
i=-ooj=-oo
</p>
<p>00 00
</p>
<p>= L h[i] L h[j]ru[(k + i) - j]
v
</p>
<p>g[k+i]
</p>
<p>i=-oo j=-oo
'''---------
</p>
<p>where
</p>
<p>g[m] = h[m] * ru[m]. (18.12)
Now we have
</p>
<p>00
</p>
<p>rx[k] = L h[i]g[k + i]
i=-oo
</p>
<p>00
</p>
<p>L h[-l]g[k -l]
1=-00
</p>
<p>(let l = -i)
</p>
<p>h[-k] *g[k].
</p>
<p>But from (18.12) g[k] = h[k]* ru[k] and therefore
</p>
<p>rx[k] h[-k] * (h[k] * ru[k])
= h[-k] * h[k] * ru[k] (18.13)</p>
<p/>
</div>
<div class="page"><p/>
<p>604 CHAPTER 18. LINEAR SYSTEMS AND WSS RANDOM PROCESSES
</p>
<p>due to the associate and commutative properties of convolution. The last re-
</p>
<p>sult of (18.11) follows by taking the Fourier transform of (18.13) and noting that
</p>
<p>F{h[-k]} = H*(J).
6
</p>
<p>A special case of particular interest occurs when the input to the system is white
</p>
<p>noise. Then using Pu(J) = (]" ~ in (18.11), the output PSD becomes
</p>
<p>(18.14)
</p>
<p>Using r u[k] = (]"~8[k] in (18.10), the output ACS becomes
</p>
<p>rx[k] = h[-k] * h[k] * (]" ~8[k]
</p>
<p>and noting that h[k] * 8[k] = h[k]
</p>
<p>r x[k] = (]"~h[-k] * h[k]
00
</p>
<p>( ] " ~ L h[-i]h[k - i].
i=-oo
</p>
<p>Finally, letting m = - i we have the result
</p>
<p>00
</p>
<p>rx[k] = (]" ~ L h[m]h[m + k]
m=-oo
</p>
<p>- 00 &lt; k &lt; 00. (18.15)
</p>
<p>This formula is useful for determining the output ACS, as is illustrated next.
</p>
<p>Example 18.2 - AR random process
</p>
<p>In Examples 17.5 and 17.10 we derived the ACS and PSD for an AR random
</p>
<p>process. We now rederive these quantities using the linear systems concepts just
</p>
<p>described. Recall that an AR random process is defined as X[n] = aX[n -1] +Urn]
and can be viewed as the output of an LSI filter with system function
</p>
<p>1
1-l(z) = 1 - 1
</p>
<p>-az
</p>
<p>with white Gaussian noise Urn] at the input. This is shown in Figure 18.2 and
follows from the definition of the system function 1-l(z) as the z-transform of the
</p>
<p>output sequence divided by the z-transform of the input sequence. To see this
</p>
<p>let urn] be a deterministic input sequence with z-transform U(z) and x[n] be the
corresponding deterministic output sequence with z-transform X(z). Then we have
</p>
<p>by the definition of the system function
</p>
<p>1-l(z) = X( z)
U(z)</p>
<p/>
</div>
<div class="page"><p/>
<p>18.3. RANDOM PROCESS AT OUTPUT OF LINEAR SYSTEM 605
</p>
<p>1-l(z)---i.~I ----1~~ X[n]Urn]
</p>
<p>1-l(z) = I-dz- I
</p>
<p>Figure 18.2: Linear system model for AR random pro cess. The input random
</p>
<p>process Urn] is white Gaussian noise with variance (J&amp;.
</p>
<p>and therefore for the given system function
</p>
<p>X( z) 1-l(z)U(z)
</p>
<p>1
1 _ az-1 U(z) .
</p>
<p>Thus,
</p>
<p>X(z) - az-1X( z) = U(z)
</p>
<p>and taking the inverse z-t ransform yields the recursive difference equation
</p>
<p>x[n] - ax[n - 1] = urn] (18.16)
</p>
<p>which is equivalent to our AR random process definition when the input and output
</p>
<p>sequences are replaced by random processes.
</p>
<p>The output PSD is now found by using (18.14) to yield
</p>
<p>Px(J)
</p>
<p>11- aexp(-j27fjW
(18.17)
</p>
<p>which agrees with our previous results. To determine the ACS we can either take the
</p>
<p>inverse Fourier transform of (18.17) or use (18.15). The latter approach is generally
</p>
<p>easier . To find the impulse response we can use (18.16) with the input set to 6[n] so
that the output is by definition h[n]. Since the LSI system is assumed to be causal,
we need to determine the solution of the difference equation h[n] = ah[n - 1]+ 6[n]
for n ~ 0 with initial condit ion h[-1] = O. The reason that the initial condition is
set equal to zero is our assumption that the LSI system is causal. A causal system
</p>
<p>cannot produce an output which is nonzero, in this case h[-1] , before the input is
applied, in this case at n = 0 since the input is 6[n]. This produces h[n] = anus[n] ,
where we now use us[n] to denote the unit step in order to avoid confusion with
the random process realization urn] (see Appendix D.3). Thus, (18.15) becomes for</p>
<p/>
</div>
<div class="page"><p/>
<p>606 CHAPTER 18. LINEAR SYSTEMS AND WSS RANDOM PROCESSES
</p>
<p>00
</p>
<p>rx[k] = a~ L amus[m]am+kus[m + k]
m=-oo
</p>
<p>00
</p>
<p>= a~ak L a2m
m=O
</p>
<p>2 a
k
</p>
<p>aU 1 _ a2
</p>
<p>and therefore for all k
</p>
<p>(m 2: 0 and m + k 2: 0 for nonzero term in sum)
</p>
<p>(since lal &lt; 1)
</p>
<p>2 a1kl
rx[k] = aU--2.
</p>
<p>I-a
</p>
<p>Again the ACS is in agreement with our previous results. Note that the linear
</p>
<p>system shown in Figure 18.2 is called an infinite impulse response (IIR) filter. This
</p>
<p>is because the impulse response h[n] = anus[n] is infinite in length.
</p>
<p>&amp; Fourier and z-transforms of WSS random process don't exist.
To determine the system function in the previous example we assumed the input
</p>
<p>to the linear system was a deterministic sequence urn]. The corresponding output
</p>
<p>x[n], therefore, was also a deterministic sequence. This is because formally the z-
transform (and also the Fourier transform) cannot exist for a WSS random process.
</p>
<p>Existence requires the sequence to decay to zero as time becomes large. But of
</p>
<p>course if the random process is WSS, then we know that E[X2 [n]] is constant as
n --+ &plusmn;oo and so we cannot have IX[n]1 --+ 0 as n --+ &plusmn;oo.
</p>
<p>Example 18.3 - MA random process
</p>
<p>In Example 17.3 we derived the ACS for an MA random process. We now show
</p>
<p>how to accomplish this more easily using (18.15). Recall the definition of the MA
</p>
<p>random process in Example 17.3 as X[n] = (U[n] + Urn - 1])/2, with Urn] being
white Gaussian noise. This may be interpreted as the output of an LSI filter with
</p>
<p>white Gaussian noise at the input. In fact , it should now be obvious that the system
</p>
<p>function is ll(z) = 1/2 + (1/2)z-1 and therefore the impulse response is h[m] = 1/2</p>
<p/>
</div>
<div class="page"><p/>
<p>18.4. INTERPRETATION OF THE PSD
</p>
<p>for m = 0,1 and zero otherwise. Using (18.15) we have
</p>
<p>00
</p>
<p>rx[kJ = a ~ L h[mJh[m + kJ
m=-oo
</p>
<p>1
</p>
<p>a ~ L h[mJh[m+ kJ
m = O
</p>
<p>and so 'for k 2: 0
</p>
<p>607
</p>
<p>k=O
</p>
<p>k=l
</p>
<p>k 2: 2.
</p>
<p>Finally, we have
</p>
<p>k=O
</p>
<p>k=l
</p>
<p>k2:2
</p>
<p>which is the same as previously obtained.
</p>
<p>18.4 Interpretation of the PSD
</p>
<p>We are now in a position to prove that the PSD, when integrated over a band of
</p>
<p>fr equen cies yields th e average power within that band. In doing so, the PSD may
</p>
<p>then be interpreted as the average power per unit frequency. We next consider
</p>
<p>a method to measure the average power of a WSS random process within a very
</p>
<p>narrow band of frequencies. To do so we filter the random process with an ideal
</p>
<p>narrowband filter whose frequency response is
</p>
<p>{
f Y- f f, Y- f, Y- f f !=1H(f) = 1 - 0 - . 2 ~ ~ - 0 + 2' 0 - 2 ~ ~ 0 + 2
</p>
<p>o otherwise
</p>
<p>and which is shown in Figure 18.3a. The width of the passband of the filter /).f is
</p>
<p>assumed to be very small. If a WSS random process X[nJ is input to this filter , then
</p>
<p>the output WSS random process Y[nJ will be composed of frequency components
</p>
<p>within the /).f frequency band, the remaining ones having been "filtered out" . The
</p>
<p>total average power in the output random process Y[nJ (which is WSS by Theorem
</p>
<p>18.3.1) is ry[O] and represents the sum of the average powers in X[n] within the
</p>
<p>bands [- fo - /).f /2, - fo + /).f /2] and [fo - /).f /2, fo + /).f /2J. It can be found from
1
</p>
<p>r y[O] = i: Py(f)df (from (17.38)).
2</p>
<p/>
</div>
<div class="page"><p/>
<p>608 CHAPTER 18. LINEAR SYSTEMS AND WSS RANDOM PROCESSES
</p>
<p>H(J)
</p>
<p>1
</p>
<p>X[n]
</p>
<p>f
1 -fo 1- 2 2
</p>
<p>fo-!&yen;- fo+!&yen;-
</p>
<p>(a)
</p>
<p>-~'I H(f) ~ Yin]
</p>
<p>(b)
</p>
<p>Figure 18.3: Narrowband filtering of random process to measure power within a
</p>
<p>band of frequencies.
</p>
<p>Now using (18.11) and the definition of the narrowband filter frequency response we
</p>
<p>have
</p>
<p>1
</p>
<p>ry[O] i :Py(J)df
2
</p>
<p>1
</p>
<p>i : IH(J)12Px(J)df (from (18.11))
2
</p>
<p>1
-10+6.1/2 ~/ 0+6.1 /2
</p>
<p>= 1&middot; Px(J)df + 1&middot; Px(J)df
- 10-6.// 2 10-6.//2
</p>
<p>~
/ 0 + 6. 1 / 2
</p>
<p>= 2 1&middot; Px(J)df (since Px'; - J) = Px(J)).
10-6.//2
</p>
<p>If we let ilf --+ 0, so that Px(J) --+ Px(Jo) within the integration interval, this
becomes approximately
</p>
<p>ry[O] = 2Px(Jo)ilf
</p>
<p>or
</p>
<p>P (f) = ~ ry [0]
X 0 2 ilf .
</p>
<p>Since ry [0] is the total average power due to the frequency components within the
bands shown in Figure 18.3a, which is twice the total average power in the positive
</p>
<p>frequency band, we have that
</p>
<p>P (+) = Total average power in band [fo - ilf /2, fo + ilf /2]
x JO ilf . (18.18)</p>
<p/>
</div>
<div class="page"><p/>
<p>18.5. WIENER FILTERING 609
</p>
<p>This says that the PSD Px(Jo) is the average power of X[n] in a small band of
</p>
<p>frequencies about f = fo divided by the width of the band. It justifies the name of
power spectral density. Furthermore, to obtain the average power within a frequency
</p>
<p>band from knowledge of the PSD, we can reverse (18.18) to obtain
</p>
<p>Total average power in band [/0 - b..f /2, fo + b..f /2] = Px(Jo)b..f
</p>
<p>which is the area under the PSD curve. More generally, we have for an arbitrary
</p>
<p>frequency band
</p>
<p>~
h
</p>
<p>Total average power in band [II,12] = Px(J)df
t.
</p>
<p>which was previously asserted.
</p>
<p>18.5 Wiener Filtering
</p>
<p>Armed with the knowledge of the mean and ACS or equivalently the mean and
</p>
<p>PSD of a WSS random process, there are several important problems that can be
</p>
<p>solved . Because the required knowledge consists of only the first two moments of
</p>
<p>the random process (which in practice can be estimated), the solutions to these
</p>
<p>problems have found widespread application. The generic approach that results is
</p>
<p>termed Wiener filtering, although there are actually four slightly different problems
</p>
<p>and corresponding solutions. These problems are illustrated in Figure 18.4 and are
</p>
<p>referred to as filtering, smoothing, prediction, and interpolation [Wiener 1949]. In
</p>
<p>the filtering problem (see Figure 18.4a) it is assumed that a signal S[n] has been
</p>
<p>corrupted by additive noise W[n] so that the observed random process is X[n] =
S[n]+W[n]. It is desired to estimate S[n] by filtering X[n] with an LSI filter having
an impulse response h[k]. The filter will hopefully reduce the noise but pass the
</p>
<p>signal. The filter estimates a particular sample of the signal, say S[noJ, by processing
</p>
<p>the current data sample X[no] and the past data samples {X[no -1], X[no - 2], ... }.
</p>
<p>Hence, the filter is assumed to be causal with an impulse response h[k] = 0 for
k &lt; O. This produces the estimator
</p>
<p>00
</p>
<p>S[no] = L h[k]X[no - k]
k=O
</p>
<p>(18.19)
</p>
<p>which depends on the current sample, containing the signal sample of interest, and
</p>
<p>past observed data samples. Presumably, the past signal samples are correlated
</p>
<p>with the present signal sample and hence the use of past samples of X[n] should
</p>
<p>enhance the estimation performance. This type of processing is called filtering and
</p>
<p>can be implemented in real time.</p>
<p/>
</div>
<div class="page"><p/>
<p>610 CHAPTER 18. LINEAR SYSTEMS AND WSS RANDOM PROCESSES
</p>
<p>&bull;
</p>
<p>I
</p>
<p>I
I
</p>
<p>&bull;
</p>
<p>x[n] = s[n] + w[n] Estimate s[noJ
</p>
<p>,/... , ...
I
</p>
<p>___ " T " T _ + L _ _ _ L . L . . - T ' T " _ _ _ L . L . . - . . L L _ ~ n
</p>
<p>Estimate s[noJ
</p>
<p>'/I
</p>
<p>no
</p>
<p>&bull;&bull;
</p>
<p>x[n] = s[n] + w[n]
</p>
<p>&bull; &bull; &bull;
</p>
<p>---"T"T-+L---LL.-TT"........L.------l~ n
</p>
<p>II------ Data used ------.--- Data used ---
</p>
<p>(a) Filtering (true signal shown dashed and
</p>
<p>displaced to right)
</p>
<p>(b) Smoothing (true signal shown dashed and
</p>
<p>displaced to right)
</p>
<p>I
Data _
used
</p>
<p>Estimate x[noJ
</p>
<p>/&bull; &bull; &bull;&bull;
I
</p>
<p>no
</p>
<p>x[n]
</p>
<p>___ _ _ _ L _ + - ~ - .......---L-....... -~ n
</p>
<p>------ Data I
used
</p>
<p>no + 1
</p>
<p>&bull; &bull; &bull;
</p>
<p>7 mate x[no+lJ
,
</p>
<p>x[n]
</p>
<p>...
___ _ _ _ L _ + - ~- . L - ........-.L-----l~ n
</p>
<p>--- Data used ------1
</p>
<p>(c) Prediction (d) Interpolation
</p>
<p>Figure 18.4: Definition of Wiener "filtering" problems .
</p>
<p>W hat are we really estimating here?
</p>
<p>In Section 7.9 we attempted to estimate the outcome of a random variable, which
</p>
<p>was unobserved, based on the outcome of another random var iab le, which was ob-
</p>
<p>served. The correlation between the two random variables allowed us to do this.
</p>
<p>Here we have essentially the same problem, except that the outcome of interest to
</p>
<p>us is of the random variable S[no]. The random variables that are observed are
{X[no],X[no -I]' . .. } or we have access to the realization (another name for out-
come) {x[no ],x[no - 1], ... }. Thus, we are attempting to estimate the realization of
S[no] based on the realization {x[no],x[no -I], .. .}. This should be kept in mind
since our notation of S[no] = L:~o h[k]X[no - k] seems to indicate that we are
attempting to est imate a random variable S[no] based on other random variables
</p>
<p>{X[no],X[no - 1]' . . .}. What we are actually trying to accomplish is a procedure of</p>
<p/>
</div>
<div class="page"><p/>
<p>18.5. WIENER FILTERING 611
</p>
<p>estimating a realization of a random variable based on realizations of other random
</p>
<p>variables that will work for all realizations. Hence, we employ the capital letter
</p>
<p>notation for random variables to indicate our interest in all realizations and to allow
</p>
<p>us to employ expectation operations on the random variables.
</p>
<p>Lfl
The second problem is called smoothing (see Figure 18.4b). It differs from
</p>
<p>filtering in that the filter is not constrained to be causal. Therefore, the estimator
</p>
<p>becomes
00
</p>
<p>S[no] = L h[k]X[no - k]
k=-oo
</p>
<p>(18.20)
</p>
<p>(18.21)
</p>
<p>where S[no] now depends on present, past, and future samples of X[n]. Clearly,
</p>
<p>this is not realizable in real time but can be approximated if we allow a delay
</p>
<p>before determining the estimate. The delay is necessary to accumulate the samples
</p>
<p>{X[no + 1], X[no + 2], ... } before computing S[no]. Within a digital computer we
would store these "future" samples.
</p>
<p>For problems three and four we observe samples of the WSS random process X[n]
</p>
<p>and wish to estimate an unobserved sample. For prediction, which is also called ex-
</p>
<p>trapolation and forecasting, we observe the current and past samples {X [no]' X[no-
</p>
<p>I], ... } and wish to estimate a future sample, X[no + L], for some positive integer
L. The prediction is called an L-step prediction. We will only consider one-step
</p>
<p>prediction or L = 1 (see Figure 18.4c). The reader should see [Yaglom 1962] for
</p>
<p>the more general case and also Problem 18.26 for an example. The predictor then
</p>
<p>becomes
00
</p>
<p>X[no + 1] = L h[k]X[no - k]
k=O
</p>
<p>which of course uses a causal filter. For interpolation (see Figure 18.4d) we observe
</p>
<p>samples {... , X [no- l ], X [no+ l ], . . .} and wish to estimate X[no]. The interpolator
then becomes
</p>
<p>00
</p>
<p>X[no] = L h[k]X[no - k]
k=-oo
</p>
<p>k,eO
</p>
<p>(18.22)
</p>
<p>which is a noncausal filter. For practical implementation of (18.19)-(18.22) we must
</p>
<p>truncate the impulse responses to some finite number of samples.
</p>
<p>To determine the optimal filter impulse responses we adopt the mean square error
</p>
<p>(MSE) criterion. Estimators that consist of LSI filters whose impulses are chosen
</p>
<p>to minimize a MSE are generically referred to as Wiener filters [Wiener 1949]. Of
</p>
<p>the four problems mentioned, we will solve the smoothing and prediction problems.
</p>
<p>The solution for the filtering problem can be found in [Orfanidis 1985] while that for
</p>
<p>the interpolation problem is described in [Yaglom 1962] (see also Problem 18.27).</p>
<p/>
</div>
<div class="page"><p/>
<p>612 CHAPTER 18. LINEAR SYSTEMS AND WSS RANDOM PROCESSES
</p>
<p>18.5.1 Wiener Smoothing
</p>
<p>We observe X[n] = S[n] + W[n] for -00 &lt; n &lt; 00 and wish to estimate S[no]
using (18.20). It is assumed that S[n] and W[n] are both zero mean WSS random
processes with known ACSs (PSDs). Also, since there is usually no reason to assume
otherwise, we assume that the signal and noise random processes are uncorrelated.
</p>
<p>This means that any sample of S[n] is uncorrelated with any sample of W[n] or
E[S[nl]W[n2]] = 0 for all nl and n2. The MSE for this problem is defined as
</p>
<p>mse = E[E2[no]] = E[(S[no] - S[no])2]
</p>
<p>where E[nO] = S[no] - S[no] is the error. To minimize the MSE we utilize the
orthogonality principle described in Section 14.7 which states that the error should
</p>
<p>be orthogonal, i.e., uncorrelated, with the data. Since the data consists of X[n] for
all n, the orthogonality principle produces the requirement
</p>
<p>Thus, we have that
</p>
<p>E[E[no]X[no - l]] = 0 - 00 &lt; l &lt; 00.
</p>
<p>E[(S[no] - S[no])X[no - l]]
</p>
<p>E [ (s[no] - .f;oo h[k]X[no - k]) X[no - I]]
which results in
</p>
<p>00
</p>
<p>o
</p>
<p>o (from (18.20))
</p>
<p>But
</p>
<p>E[S[no]X[no -l]] = L h[k]E[X[no - k]X[no -l]].
k=-oo
</p>
<p>(18.23)
</p>
<p>and
</p>
<p>E[S[no]X[no - l]] E[S[no](S[no -l] + W[no -l])]
E[S[no]S[no -l]] (S[n] and W[n] are
</p>
<p>uncorrelated and zero mean)
</p>
<p>= rs[l]
</p>
<p>E[X[no - k]X[no -l]] E[(S[no - k] + W[no - k])(S[no -l] + W[no -l])]
E[S[no - k]S[no - l]] + E[W[no - k]W[no - l]]
</p>
<p>= rs[l - k] + rw[l - k].
</p>
<p>The infinite set of simultaneous linear equations becomes from (18.23)
</p>
<p>00
</p>
<p>rs[l] = L h[k](rs[l - k] + rw[l- k])
k=-oo
</p>
<p>- 00 &lt; l &lt; 00. (18.24)</p>
<p/>
</div>
<div class="page"><p/>
<p>18.5. WIENER FILTERING 613
</p>
<p>Note that the equations do not depend on no and therefore the solution for the
</p>
<p>optimal impulse response is the same for any no. This is due to the WSS assumption
</p>
<p>coupled with the LSI assumption for the estimator, which together imply that a shift
</p>
<p>in the sample to be estimated results in the same filtering operation but shifted. To
</p>
<p>solve this set of equations we can use transform techniques since the right-hand side
</p>
<p>of (18.24) is seen to be a discrete-time convolution. It follows then that
</p>
<p>rs[l] = h[l] * (rs[l] + rw[l])
</p>
<p>and taking Fourier transforms of both sides yields
</p>
<p>Ps(f) = H(f)(Ps(f) + Pw(f))
</p>
<p>or finally the frequency response of the optimal Wiener smoothing filter is
</p>
<p>Ps(f)
Hopt(f) = Ps(f) + Pw(f)' (18.25)
</p>
<p>The optimal impulse response can be found by taking the inverse Fourier transform
</p>
<p>of (18.25). We next give an example.
</p>
<p>Example 18.4 - Wiener smoother for AR signal in white noise
</p>
<p>Consider a signal that is an AR random process corrupted by additive white noise
</p>
<p>with variance a?v. Then, the PSDs are
</p>
<p>Ps(f)
</p>
<p>Pw(f)
</p>
<p>11- a exp(-j27fJ) 12
</p>
<p>2= aw&middot;
</p>
<p>The PSDs and corresponding Wiener smoother frequency responses are shown in
</p>
<p>Figure 18.5. In both cases the white noise variance is the same, a?v = 1, and the
AR input noise variance is the same, a ~ = 0.5, but the AR filter parameter a has
been chosen to yield a wide PSD and a very narrow PSD. As an example, consider
</p>
<p>the case of a = 0.9, which results in a lowpass signal random process as shown in
Figure 18.5b. Then, the results of a computer simulation are shown in Figure 18.6.
</p>
<p>In Figure 18.6a the signal realization s[n] is shown as the dashed curve and the
noise corrupted signal realization x[n] is shown as the solid curve. The points have
been connected by straight lines for easier viewing. Applying the Wiener smoother
</p>
<p>results in the estimated signal shown in Figure 18.6b as the solid curve. Once
</p>
<p>again the true signal realization is shown as dashed. Note that the estimated signal
</p>
<p>shown in Figure 18.6b exhibits less noise fluctuations but having been smoothed,
</p>
<p>also exhibits a reduced ability to follow the signal when the signal changes rapidly
</p>
<p>(see the estimated signal from n = 25 to n = 35). This is a standard tradeoff in
</p>
<p>that noise smoothing is obtained at the price of poorer signal following dynamics.
</p>
<p>&lt;:;</p>
<p/>
</div>
<div class="page"><p/>
<p>614 CHAPTER 18. LINEAR SYSTEMS AND WSS RANDOM PROCESSES
</p>
<p>50 : ; : : : : : ..
</p>
<p>20 _ : - _ _ : ; .
</p>
<p>; Pw(J)
</p>
<p>o L--'----'-~~~--'-__'__~~__'____J
-0.5 - 0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4 0.5
</p>
<p>J
</p>
<p>. .
40 .... ; : ."
</p>
<p>Q .
U) . :
</p>
<p>p., 30 &middot;&middot; ...... .... &middot; .. . &middot;Ps(f ) &middot;.. .. &middot;.... ..
</p>
<p>(a) a = 0.2
</p>
<p>----.0.8 . . . .:..
</p>
<p>................
g.0.6 . . . -: . . . -:. . . ...... ..
</p>
<p>""'-i . .
</p>
<p>0.4 .... :.. .. -:.. . ..
</p>
<p>0.2 " &gt; '
</p>
<p>oL--'----'-~_~~__'__~~__'_____l
- 0.5 -0.4 - 0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4 0.5
</p>
<p>J
</p>
<p>(c) a = 0.2
</p>
<p>(b) a = 0.9
</p>
<p>----.0.8 . ....
</p>
<p>................
g.0.6 .
</p>
<p>i:J.:j
</p>
<p>0.4 .. .. ;
</p>
<p>o L--'----'-~~~--'-~_~~~___J
-0.5 -0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4 0.5
</p>
<p>J
</p>
<p>(d ) a = 0.9
</p>
<p>Figure 18.5: Power spectral densities of the signal and noise and corresponding
</p>
<p>frequency responses of Wiener smoother.
</p>
<p>In order to implement the Wiener smoother for the previous example the data was
</p>
<p>filtered in the frequency domain and converted back into the time domain. This was
</p>
<p>done using the inverse discrete-time Fourier transform
</p>
<p>1
</p>
<p>A 12 Ps(J) .
s[n] = 1 P (J) 2 XN(J) exp(J27fJn)dJ
</p>
<p>- 2 S +O'w
n = 0,1 , ... , N - 1
</p>
<p>where X N(J) is the Fourier transform of the available data {x[0], x[I ], ... ,x[N -In,</p>
<p/>
</div>
<div class="page"><p/>
<p>18.5. WIENER FILTERING 615
</p>
<p>4 4
</p>
<p>,
&gt;:: 2 &gt;:: 2
</p>
<p>1\
</p>
<p>H &lt;'F.&gt;
</p>
<p>n
40302010
</p>
<p>n
</p>
<p>-4 L-_----'__ ~ __ ~ __ ~ _ _____'
</p>
<p>o40302010
-4 L - _ ~ __ ~ __ ~ __ ~ _ - - - - - '
</p>
<p>o
</p>
<p>(a) True (dashed) and noisy (solid) signal (b) True (dashed) and estimated (solid)
</p>
<p>signal
</p>
<p>Figure 18.6: Example of Wiener smoother for additive noise corrupted AR signal.
</p>
<p>The true PSDs are shown in Figure 18.5b. In a) the true signal is shown as the
</p>
<p>dashed curve and the noisy signal as the solid curve and in b) the true signal is
</p>
<p>shown as the dashed curve and the Wiener smoothed signal estimate (using the
</p>
<p>Wiener smoother shown in Figure 18.5d) as the solid curve.
</p>
<p>which is
N-l
</p>
<p>XN(f) = :L x[n]exp(-j21fJn)
n=O
</p>
<p>(N = 50 for the previous example). The actual implementation used an inverse FFT
to approximate the integral as is shown in the MATLAB code given next. Note that
</p>
<p>in using the FFT and inverse FFT to calculate the Fourier transform and inverse
</p>
<p>Fourier transform, respectively, the frequency interval has been changed to [0, 1].
</p>
<p>Because the Fourier transform is periodic with period one, however, this will not
</p>
<p>affect the result.
</p>
<p>clear all
</p>
<p>r andnf ' state' ,0)
a=0.9;varu=0.5;vars=varu/(1-a-2);varw=1;N=50; %set up parameters
for n=0:N-1 %generate signal realization
</p>
<p>nn=n+1;
</p>
<p>if n==O %use Gaussian random processes
s(nn,1)=sqrt(vars)*randn(1,1); %initialize first sample
</p>
<p>%to avoid transient
else
</p>
<p>s(nn,1)=a*s(nn-1)+sqrt(varu)*randn(1,1);</p>
<p/>
</div>
<div class="page"><p/>
<p>616 CHAPTER 18. LINEAR SYSTEMS AND WSS RANDOM PROCESSES
</p>
<p>end
</p>
<p>end
x=s+sqrt(varw)*randn(N,l)j %add white Gaussian noise
Nfft=1024j %set up FFT length
%compute PSD of signal, frequency interval is [0,1]
Ps=varu./(abs(1-a*exp(-j*2*pi*[0:Nfft-l]'/Nfft)).A2)j
</p>
<p>Hf=Ps./(Ps+varw)j %form Wiener smoother
sestf=Hf.*fft(x,Nfft)j %signal estimate in frequency domain,
</p>
<p>%frequency interval is [0,1]
sest=real(ifft(sestf,Nfft))j %inverse Fourier transform
</p>
<p>One can also determine the minimum MSE to assess how well the smoother
</p>
<p>performs. This is
</p>
<p>msemin E[(S[no] - S[no])2]
</p>
<p>E[(S[no] - S[no])S[no]] - E[(S[no] - S[no])S[no]] .
</p>
<p>k=-oo
</p>
<p>But the second term is zero since by the orthogonality principle
</p>
<p>00
</p>
<p>L hopt[k] E[&euro;[no]X[no - k]] = O., '....
=0
</p>
<p>Thus, we have
</p>
<p>k=-oo
</p>
<p>msemin E[(S[no] - S[no])S[no]]
</p>
<p>rs[O] - E l~= hopt[k]X[no - kls[no]]
00
</p>
<p>rs[O] - L hopdk]E[(S[no - k] + W[no - k])S[no]]
, "
</p>
<p>v
</p>
<p>=E[S[no-k]S[no]]=rs[k]
</p>
<p>since S[nl] and W[n2] are uncorrelated for all nl and n2 and also are zero mean.
The minimum MSE becomes
</p>
<p>00
</p>
<p>msemin = rs[O] - L hopt[k]rs[k].
k=-oo
</p>
<p>(18.26)
</p>
<p>This can also be written in the frequency domain by using Parseval's theorem to</p>
<p/>
</div>
<div class="page"><p/>
<p>18.5. WIENER FILTERING
</p>
<p>yield
</p>
<p>1 1
</p>
<p>msemin i: Ps(f)df - i: Hopt(f)Ps(f)df
2 2
</p>
<p>1
</p>
<p>i: (1 - Hopt(f))Ps(f)df
2
</p>
<p>= t. (1 - Ps(ff:~w(f)) Ps(f)df
2
</p>
<p>1
</p>
<p>( 2 Pw(f)
J_1 Ps(f) + Pw(f) Ps(f)df
</p>
<p>2
</p>
<p>((17.38) and Parseval)
</p>
<p>617
</p>
<p>and finally letting p(f) = Ps (f) / Pw (f) be the signal-to-noise ratio in the frequency
</p>
<p>domain we have
1
</p>
<p>(2 Ps(f)
msemin = J_1 1 + p(f) df.
</p>
<p>2
</p>
<p>(18.27)
</p>
<p>It is seen that the frequency bands for which the contribution to the minimum MSE
</p>
<p>is largest, are the bands for which the signal-to-noise ratio is smallest or for which
</p>
<p>p(f) &laquo; 1.
</p>
<p>18.5.2 Prediction
</p>
<p>We consider only the case of L = 1 or one-step prediction. The more general case
</p>
<p>can be found in [Yaglom 1962] (see also Problem 18.26). As before, the criterion of
</p>
<p>MSE is used to design the predictor so that from (18.21)
</p>
<p>mse E[(X[no + 1] - X[no + 1])2]
</p>
<p>E [ ( X [no + n- t, h[k]X [no _ k]) ']
is to be minimized over h[k] for k 2:: 0. Invoking the orthogonality principle leads
to the infinite set of simultaneous linear equations
</p>
<p>l = 0,1, ....
</p>
<p>These equations become
</p>
<p>00
</p>
<p>E[X[no + l]X[no -i]] = z= h[k]E[X[no - k]X[no -i]]
k=O</p>
<p/>
</div>
<div class="page"><p/>
<p>618 CHAPTER 18. LINEAR SYSTEMS AND WSS RANDOM PROCESSES
</p>
<p>or finally
00
</p>
<p>rx[l + 1] = L h[k]rx[l - k] l = 0,1 ,. .. . (18.28)
k=O
</p>
<p>Note that once again the optimal impulse response does not depend upon no so
that we obtain the same predictor for any sample. Although it appears that we
</p>
<p>should be able to solve these simultaneous linear equations using the previous Fourier
</p>
<p>transform approach, this is not so. Because the equations are only valid for l 2: 0
and not for l &lt; 0, a z-transform cannot be used. Consider forming the z-transform
of the left-hand-side as 2:~o rx[l + 1]z-l and note that it is not equal to zP(z).
(See also Problem 18.15 to see what would happen if we blindly went ahead with
</p>
<p>this approach.)
</p>
<p>The minimum MSE is evaluated by using a similar argument as for the Wiener
</p>
<p>smoother
</p>
<p>msemin
</p>
<p>00
</p>
<p>= rx[O] - L hoptlk]rx[k + 1]
k=O
</p>
<p>(18.29)
</p>
<p>where hopt[k] is the impulse response solution from (18.28). A simple example for
</p>
<p>which the equations of (18.28) can be solved is given next.
</p>
<p>Example 18.5 - Prediction of AR random process
</p>
<p>Consider an AR random process for which the ACS is given by rx[k] = (a~/(1 -
a2))a1k\ = rx[0]a1kl. Then from (18.28)
</p>
<p>00
</p>
<p>rx[0]all+11= L h[k]rx[0]a1l- kl
k=O
</p>
<p>l = 0,1, ...
</p>
<p>and if we let h[k] = 0 for k 2: 1, we have
</p>
<p>ailH I = h[O]a ll l l = 0,1 , . . ..
</p>
<p>Since l 2: 0, the solution is easily seen to be
</p>
<p>al+1
hopt[O] = -l = a
</p>
<p>a
</p>
<p>or finally
</p>
<p>X[no + 1] = aX[no].
</p>
<p>Also, since this is true for any no, we can replace the specific sample by a more
general sample by replacing no by n - 1. This results in
</p>
<p>X[n] = aX[n - 1]. (18.30)</p>
<p/>
</div>
<div class="page"><p/>
<p>18.5. WIENER FILTERING 619
</p>
<p>Recalling that the AR random process is defined as X[n] = aX[n - 1]+ Urn], it is
now seen that the optimal one-step linear predictor is obtained from the definition
</p>
<p>by ignoring the term Urn]. This is because Urn] cannot be predicted from the
past samples {X[n -1],X[n - 2] , ... }, which are uncorrelated with Urn] (see also
Example 17.5). Furthermore, the prediction error is E[n] = X[n] - X[n] = X[n] -
aX[n-l] = Urn]. Finally, note that the prediction only depends on the most recent
sample and not on the past samples of X[n]. In effect, to predict X[no + 1] all
the past information of the random process is embodied in the sample X[no]. To
illustrate the prediction solution consider the AR random process whose parameters
</p>
<p>and realizations were shown in Figure 17.5. The realizations, along with the one-step
</p>
<p>predictions, shown as the "*"s, are given in Figure 18.7. Note the good predictions
</p>
<p>3 .---~-~-~-~-~------, 3 .---~-~-~-~-~------,
</p>
<p>2 2
</p>
<p>1 . . . . . . . . . . .
</p>
<p>~
</p>
<p>"_: HlWUl!f!!U1ftn:&bull;&bull;&bull;&bull;tt&bull;&bull;
-2 : . . ... . : .
</p>
<p>30252015
n
</p>
<p>105
-3 ' - - - - ~ - ~ - ~ - ~ - ~ - - - - '
</p>
<p>o302515 20
n
</p>
<p>105
-3 ' - - - - - - ~ - ~ - ~ - ~ - ~ - - - '
</p>
<p>o
</p>
<p>(a) a = 0.25, a ~ = 1 - a 2 (b) a = 0.98, a~ = 1 - a2
</p>
<p>Figure 18.7: Typical realizations of autoregressive random process with different
</p>
<p>parameters and their one-step linear predictions indicated by the ,,*"s as X[n+ 1] =
ax[n].
</p>
<p>for the AR random process with a = 0.98 but the relatively poor ones for the AR
random process with a = 0.25. Can you justify these results by comparing the
</p>
<p>minimum MSEs? (See Problem 18.17.)
</p>
<p>&lt;)
</p>
<p>The general solution of (18.28) is fairly complicated. The details are given in Ap-
</p>
<p>pendix 18A. We now summarize the solution and then present an example.
</p>
<p>1. Assume that the z-transform of the ACS, which is
</p>
<p>00
</p>
<p>'Px(z) = I: rx[k]z-k
k=-oo</p>
<p/>
</div>
<div class="page"><p/>
<p>620 CHAPTER 18. LINEAR SYSTEMS AND WSS RANDOM PROCESSES
</p>
<p>can be written as
</p>
<p>(18.31)
</p>
<p>where
00
</p>
<p>A(z) = 1- La[k]z-k.
k=l
</p>
<p>It is required that A(z) have all its zeros inside the unit circle of the z-plane,
</p>
<p>i.e., the filter with z-t ransform 1/A(z) is a stable and causal filter [Jackson
</p>
<p>1991].
</p>
<p>2. The solution of (18.28) for the impulse response is
</p>
<p>hopdk] = ark + 1]
</p>
<p>and the minimum MSE is
</p>
<p>k = 0,1, . ..
</p>
<p>A 2 2
msemin = E[(X[no + 1] - X[no + 1]) ] = (1u.
</p>
<p>3. The optimal linear predictor becomes from (18.21)
</p>
<p>00
</p>
<p>X[no+ 1] = L ark + l]X [no - k]
k=O
</p>
<p>and has the minimum MSE , msemin = (1 ~ .
</p>
<p>(18.32)
</p>
<p>Clearly, the most difficult part of the solution is putting Px(z) into the required
</p>
<p>form of (18.31) . In terms of the PSD the requirement is
</p>
<p>Px(J) = Px(exp(j27ff)) =
</p>
<p>=
</p>
<p>A(exp(j27ff) )A(exp(- j27ff))
</p>
<p>( 1 ~
</p>
<p>A(exp(j27ff))A*(exp(j27ff))
</p>
<p>( 1 ~
</p>
<p>IA(exp(j27ff)) 12
</p>
<p>(1 ~
=
</p>
<p>11- 2 : ~ 1 ark] exp(-j27fjk) 12.
</p>
<p>But the form of the PSD is seen to be a generalization of the PSD for the AR
</p>
<p>random process. In fact , if we truncate the sum so that the required PSD becomes
</p>
<p>(12
</p>
<p>Px(J) = u 2
11 - 2:~=1 a[k] exp(-j27fjk) I</p>
<p/>
</div>
<div class="page"><p/>
<p>18.5. WIENER FILTERING 621
</p>
<p>then we have the PSD of what is referred to as an AR random process of order p,
</p>
<p>which is also denoted by the symbolism AR(P). In this case, the random process is
defined as
</p>
<p>p
</p>
<p>X[n] = L a[k]X[n - k] + Urn] (18.33)
k=l
</p>
<p>where as usual Urn] is white Gaussian noise with variance a ~ . Of course, for p = 1 we
have our previous definition of the AR random process, which is an AR(l) random
</p>
<p>process with a[l] = a. Assuming an AR(p) random process so that all] = 0 for
l &gt; p, the solution for the optimal one-step linear predictor is from (18.32)
</p>
<p>p-l
</p>
<p>X[no + 1] = La[l + l]X[no -l]
1=0
</p>
<p>and letting k = l + 1 produces
p
</p>
<p>X[no + 1] = L a[k]X[no + 1 - k]
k=l
</p>
<p>(18.34)
</p>
<p>and the minimum MSE is a~. Another example follows.
</p>
<p>Example 18.6 - One-step linear prediction of MA random process
</p>
<p>Consider the zero mean WSS random process given by X[n] = Urn] - bUrn - 1],
where Ibl &lt; 1 and Urn] is white Gaussian noise with variance a ~ (also called an MA
random process). This random process is a special case of that used in Example
</p>
<p>18.1 for which h[O] = 1 and h[l] = -b and Urn] is white Gaussian noise. To find the
optimal linear predictor we need to put the a-t ransform of the ACS into the required
</p>
<p>form. First we determine the PSD. Since the system function is easily shown to be
</p>
<p>ll(z) = 1- bz-1, the frequency response follows as H(J) = 1- bexp( -j21r f). From
</p>
<p>(18.14) the PSD becomes
</p>
<p>Px(J) = H(J)H*(J)a~ = (1- bexp(-j21rf))(1 - bexp(j21rf))a~
</p>
<p>and hence replacing exp(j21r f) by z, we have
</p>
<p>Px(z) = (1 - bz-1)(1 - bz)a~. (18.35)
</p>
<p>By equating (18.35) to the required form for Px(z) given in (18.31) we have
</p>
<p>1
A(z) = 1 _ bz-1 .
</p>
<p>To convert this to 1 - E~l a[k]z-k, we take the inverse z-transform, assuming a
</p>
<p>stable and causal sequence, to yield</p>
<p/>
</div>
<div class="page"><p/>
<p>622 CHAPTER 18. LINEAR SYSTEMS AND WSS RANDOM PROCESSES
</p>
<p>and so ark] = _bk for k ~ 1. (Note why Ibl &lt; 1 is required or else a[n] would not be
stable.) The optimal predictor is from (18.32)
</p>
<p>00
</p>
<p>X[no + 1] = L ark + l]X[no - k]
k=O
00
</p>
<p>L( -bk+l)X[no - k]
k=O
</p>
<p>-bX[no] - b2X [no - 1] - b3X[no - 2] - ...
</p>
<p>and the minimum MSE is
2msemin = (7U.
</p>
<p>c
As a special case of practical interest, we next consider a finite length one-step
</p>
<p>linear predictor. By finite length we mean that the prediction can only depend
</p>
<p>on the present sample and past M - 1 samples. In a derivation similar to the
</p>
<p>infinite length predictor it is easy to show (see the discussion in Section 14.8 and
</p>
<p>also Problem 18.20) that if the predictor is given by
</p>
<p>M-l
</p>
<p>X[no + 1] = L h[k]X[no - k]
k=O
</p>
<p>which is just (18.21) with h[k] = 0 for k ~ M , then the optimal impulse response
satisfies the M simultaneous linear equations
</p>
<p>M - l
</p>
<p>rx[l + 1] = L h[k]rx[l- k]
k=O
</p>
<p>1= 0,1 , ... , M - 1.
</p>
<p>(If M -+ 00, these equations are identical to (18.28)). The equations can be written
in vector/matrix form as
</p>
<p>rx[l]
</p>
<p>rx[O]
</p>
<p>rx[M -2]
v
</p>
<p>Rx
</p>
<p>(18.36)
</p>
<p>The corresponding minimum MSE is given by
</p>
<p>M-l
</p>
<p>msemin = rx[O] - L hopdk]rx[k + 1].
k=O
</p>
<p>(18.37)</p>
<p/>
</div>
<div class="page"><p/>
<p>18.6. CONTINUOUS-TIME DEFINITIONS AND FORMULAS 623
</p>
<p>These equations are called the Wiener-Hop! equations. In general, they must be
</p>
<p>solved numerically but there are many efficient algorithms to do so [Kay 1988].
</p>
<p>The algorithms take advantage of the structure of the matrix which is seen to be
</p>
<p>an autocorrelation matrix Rx as first described in Section 17.4. As such, it is
</p>
<p>symmetric, positive definite, and has the Toeplitz property. The Toeplitz property
</p>
<p>asserts that the elements along each northwest-southeast diagonal are identical.
</p>
<p>Another important connection between the linear prediction equations and an AR(p)
</p>
<p>random process is made by letting M = p in (18.36). Then, since for an AR(p)
</p>
<p>process, we have that h[n] = a[n+ 1] for n = 0,1 , ... ,p -1 (recall from (18.34) that
X[no + 1] = 2:~=1 a[k]X[no + 1 - k]) the Wiener-Hopf equations become
</p>
<p>[
</p>
<p>rx[O]
rx[I]
</p>
<p>rX~-I]
</p>
<p>rx[l]
</p>
<p>rx[O]
</p>
<p>rx[p - 2]
</p>
<p>... rx[p - 1] ] [a[l] ] [ rx[l] ]... rx[p - 2] a[2] rx[2]
- .. . . .. . . .'. . .
</p>
<p>... rx[O] a[p] rx[P]
</p>
<p>(18.38)
</p>
<p>It is important to note that for an AR(p) random process, the optimal one-step linear
</p>
<p>predictor based on the infinite number of samples {X [no], X[no -I], } is the same
as that based on only the finite number of samples {X[no],X [no -1]' , X [no- (p-
I)]} [Kay 1988]. The equations of (18.38) are now referred to as the Yule- Walker
</p>
<p>equations. In this form they relate the ACS samples {rx[O], rx[I], . . . rx[P]} to the
AR filter parameters {a[I] , a[2] , .. . ,a[p] }. If the ACS samples are known, then the
AR filter parameters can be obtained by solving the equations. Furthermore, once
</p>
<p>the filter parameters have been found from (18.38) , the variance of the white noise
</p>
<p>random process Urn] is found from
</p>
<p>p
</p>
<p>a'fJ = msemin = rx[O] - La[k]rx[k]
k=l
</p>
<p>(18.39)
</p>
<p>which follows by letting hopt[k] = ark+ 1] with M = pin (18.37). In the real-world
example of Section 18.7 we will see how these equations can provide a method to
</p>
<p>synthesize speech.
</p>
<p>18.6 Continuous-Time Definitions and Formulas
</p>
<p>For a continuous-time WSS random process as defined in Section 17.8 the linear
</p>
<p>system of interest is a linear time invariant (LTI) system. It is characterized by its
</p>
<p>impulse response h(T). If a random process U(t) is input to an LTI system with
impulse response h(T), the output random process X(t) is
</p>
<p>X(t) = i: h(T)U(t - T)dT.</p>
<p/>
</div>
<div class="page"><p/>
<p>624 CHAPTER 18. LINEAR SYSTEMS AND WSS RANDOM PROCESSES
</p>
<p>The integral is referred to as a convolution integral and in shorthand notation the
</p>
<p>output is given by X(t) = h(t) * U(t). If U(t) is WSS with constant mean J.Lu
and ACF rU(T), then the output random process X(t) is also WSS. It has a mean
f u o c t i ~ .
</p>
<p>where
</p>
<p>J.Lx = (1: h(T)dT) J.Lu = H(O)J.Lu (18.40)
</p>
<p>H(F) = 1: h(T) exp( -j21rFT)dT
is the frequency response of the LTI system. The ACF of the output random process
</p>
<p>X(t) is
</p>
<p>and therefore the PSD becomes
</p>
<p>Px(F) = IH(F)12Pu(F).
</p>
<p>(18.41)
</p>
<p>(18.42)
</p>
<p>An example follows.
</p>
<p>Example 18.7 - Inteference rejection filter
</p>
<p>A signal, which is modeled as a WSS random process S(t), is corrupted by an
</p>
<p>additive interference I(t) , which can be modeled as a randomly phased sinusoid
</p>
<p>with a frequency of Fo = 60 Hz. The corrupted signal is X(t) = S(t) + I(t) . It
is desired to filter out the interference but if possible, to avoid altering the PSD
</p>
<p>of the signal due to the filtering. Since the sinusoidal interference has a period of
</p>
<p>T = 1/Fo = 1/60 seconds, it is proposed to filter X(t) with the differencing filter
</p>
<p>Y(t) = X(t) - X(t - T). (18.43)
</p>
<p>The motivation for choosing this type of filter is that a periodic signal with period
</p>
<p>T will have the same value at any two time instants separated by T seconds. Hence,
</p>
<p>the difference should be zero for all t . We wish to determine the PSD at the filter
</p>
<p>output. We will assume that the interference is uncorrelated with the signal. This
</p>
<p>assumption means that the ACF of X(t) is the sum of the ACFs of S(t) and I(t)
</p>
<p>and consequently the PSDs sum as well (see Problem 18.33). The differencing filter
</p>
<p>is an LTI system and so its output can be written as
</p>
<p>Y(t) = 1: h(T)X(t - T)dT (18.44)
for the appropriate choice of the impulse response. The impulse response is obtained
</p>
<p>by equating (18.44) to (18.43) from which it follows that
</p>
<p>h(T) = 8(T) - 8(T - T) (18.45)</p>
<p/>
</div>
<div class="page"><p/>
<p>18.6. CONTINUOUS-TIME DEFINITIONS AND FORMULAS 625
</p>
<p>as can easily be verified. By taking the Fourier transform, the frequency response
</p>
<p>becomes
</p>
<p>H(F) = 1:(8(7) -8(7 - T)) exp(-j21rF7)d7
1 - exp(-j21rFT) . (18.46)
</p>
<p>To determine the PSD at the filter output we use (18.42) and note that for the
</p>
<p>randomly phased sinusoid with amplitude A and frequency Fa, the ACF is (see
</p>
<p>Problem 17.46)
</p>
<p>and therefore its PSD, which is the Fourier transform, is given by
</p>
<p>The PSD at the filter input is Px(F) = Ps(F) + P[(F) (the PS Ds add due to the
uncorrelated assumption) and therefore the PSD at the filter output is
</p>
<p>Py(F) IH(F)12Px(F) = IH (F W (Ps (F ) + P[(F))
= 11- exp(-j21rFTW(Ps(F) + P[(F)) .
</p>
<p>The magnitude-squared of the frequency response of (18.46) can also be written in
</p>
<p>real form as
</p>
<p>IH(F)1 2 = 2 - 2 cos(21rFT)
</p>
<p>and is shown in Figure 18.8. Note that it exhibits zeros at multiples of F = liT =
</p>
<p>5 .--~-~--~ - ~----r----,
</p>
<p>N 4 . . . .:..
</p>
<p>&amp;;
~3 &middot; &lt; &middot; &middot;
</p>
<p>2 .
</p>
<p>1 ..
</p>
<p>T T
</p>
<p>Figure 18.8: Magnitude-squared frequency response of interference canceling filter
</p>
<p>with Fa = liT.</p>
<p/>
</div>
<div class="page"><p/>
<p>626 CHAPTER 18. LINEAR SYSTEMS AND WSS RANDOM PROCESSES
</p>
<p>Fo. Hence, IH(FoW = 0 and so the interfering sinusoid is filtered out. The PSD at
</p>
<p>the filter output then becomes
</p>
<p>Py(F) IH(F)12Ps(F)
</p>
<p>2(1 - cos(27f-FT))Ps(F).
</p>
<p>Unfortunately, the signal PSI? has also been modified. What do you think would
</p>
<p>happen if the signal were periodic with period 1/(2Fo)?
</p>
<p>18.7 Real-World Example - Speech Synthesis
</p>
<p>It is commonplace to hear computer generated speech when asking for directory
</p>
<p>assistance in obtaining telephone numbers, in using text to speech conversion pro-
</p>
<p>grams in computers, and in playing with a multitude of children's toys. One of the
</p>
<p>earliest applications of computer speech synthesis was the Texas Instruments Speak
</p>
<p>and Spelll . The approach to producing intelligible, if not exactly human sounding,
</p>
<p>speech, is to mimic the human speech production process. A speech production
</p>
<p>model is shown in Figure 18.9 [Rabiner and Schafer 1978]. It is well known that
</p>
<p>speech sounds can be delineated into two classes-voiced speech such as a vowel
</p>
<p>sound and unvoiced speech such as a consonant sound. A voiced sound such as
</p>
<p>"ahhh" (the 0 in "lot" for example) is produced by the vibration of the vocal cords,
</p>
<p>while an unvoiced sound such as "sss" (the s in "runs" for example) is produced
</p>
<p>by passing air over a constriction in the mouth. In either case, the sound is the
</p>
<p>output of the vocal tract with the difference being the excitation sound and the
</p>
<p>subsequent filtering of that sound. For voiced sounds the excitation is modeled as
</p>
<p>a train of impulses to produce a periodic sound while for an unvoiced sound it is
</p>
<p>modeled as white noise to produce a noise-like sound (see Figure 18.9). The excita-
</p>
<p>&yen;h
voiced
</p>
<p>-I 1l y (z)
1
</p>
<p>x[n]
</p>
<p>~ D/A 1
~
</p>
<p>~ speech, x(t)
unvoiced
</p>
<p>-I
1
</p>
<p>x[n] I
1lUY (z)
</p>
<p>Figure 18.9: Speech production model.
</p>
<p>tion is modified by the vocal tract, which can be modeled by an LSI filter. Knowing
</p>
<p>1Registered trademark of Texas Instruments</p>
<p/>
</div>
<div class="page"><p/>
<p>18.7. REAL-WORLD EXAMPLE - SPEECH SYNTHESIS 627
</p>
<p>1
</p>
<p>lluAz) = 1 - L:~=l a[k]z-k
which is an all-pole filter. Typically, the order of the filter p, which is the number
of poles, is chosen to be p = 12. The output of the filter X[n] for a white Gaussian
noise random process input Urn] with variance CTb is given as the WSS random
process
</p>
<p>the excitation waveform and the vocal tract system function allows us to synthesize
</p>
<p>speech. For the unvoiced sound we pass discrete white Gaussian noise through an
</p>
<p>LSI filter with system function llU Y (z). We next concentrate on the synthesis of
</p>
<p>unvoiced sounds with the synthesis of voiced sounds being similar.
</p>
<p>It has been found that a good model for the vocal tract is the LSI filter with
</p>
<p>system function
</p>
<p>(18.47)
</p>
<p>... rx[p - 1] ] [a[l] ] [ rx[l] ]... rx[p - 2] a[2] rx[2]
</p>
<p>. . . .. . . .
'. . .
</p>
<p>... rx[O] a[p] rx[P]
</p>
<p>rx[l]
rx[O]
</p>
<p>p
</p>
<p>X[n] = L a[k]X[n - k] + Urn]
k=l
</p>
<p>which is recognized as the defining difference equation for an AR(P) random process.
</p>
<p>Hence , unvoiced speech sounds can be synthesized using this difference equation for
</p>
<p>an appropriate choice of the parameters {a[l] ,a[2] , ... ,a[p] ,CTb }. The parameters
will be different for each unvoiced sound to be synthesized. To determine the pa-
</p>
<p>rameters for a given sound, a segment of the target speech sound is used to estimate
</p>
<p>the ACS. Estimation of the ACS was previously described in Section 17.7. Then,
</p>
<p>the parameters ark] for k = 1,2, ... ,p can be obtained by solving the Yule-Walker
equations (same as Wiener-Hopf equations). The theoretical ACS samples required
</p>
<p>are replaced by estimated ones to yield the set of simultaneous linear equations from
</p>
<p>(18.38) as
</p>
<p>[
~~f~~
</p>
<p>rx[p: - 1] rx[p - 2]
</p>
<p>which are solved to yield the a[k]'s. Then, the white noise variance estimate is found
from (18.39) as
</p>
<p>(18.49)k=O,l, ... ,p
</p>
<p>p
</p>
<p>a-b = rx[O] - L a[k]rx[k] (18.48)
k=l
</p>
<p>where ark] is given by the solution of the Yule-Walker equations of (18.47). Hence, we
estimate the ACS for lags k = 0,1 , ... ,p based on an actual speech sound and then
</p>
<p>solve the equations of (18.47) to obtain {a[l] ,0,[2], ... ,a[p]} and finally, determine
a-b using (18.48). The only modification that is commonly made is to the ACS
estimate, which is chosen to be
</p>
<p>N -l-k
</p>
<p>rx[k] = ~ L x[n]x[n + k]
n=O</p>
<p/>
</div>
<div class="page"><p/>
<p>628 CHAPTER 18. LINEAR SYSTEMS AND WSS RANDOM PROCESSES
</p>
<p>and which differs from the one given in Section 17.7 in that the normalizing factor
</p>
<p>is N instead of N - k . For N &raquo; p this will have minimal effect on the parameter
estimates but has the benefit of ensur ing a stable filter est imate, i.e. , the poles of
</p>
<p>1&pound;uv(z) will lie inside the unit circle [Kay 1988]. This method of estimating the
</p>
<p>AR parameters is called the autocorrelation method oj linear prediction. The entire
</p>
<p>pro cedure of modeling speech by an AR(p) model is referred to as linear predictive
</p>
<p>coding (LPC). The name originated with the connection of (18.47) as a set of linear
</p>
<p>prediction equations, although the ultimate goal here is not linear prediction but
</p>
<p>speech modeling [Makhoul 1975].
</p>
<p>To demonstrate the modeling of an unvoiced sound consider the spoken word
</p>
<p>"seven" shown in Figure 18.10. A portion of the "sss" utterance is shown in Figure
</p>
<p>OB
</p>
<p>0.6
</p>
<p>0.4
</p>
<p>I
-0.2 &middot; &middot; &middot; &middot; &middot; &middot; : "1'1
</p>
<p>-0.4 ;.1..1 .
: I I
</p>
<p>-0.6 .
</p>
<p>-0.8 ' :'5 : n
</p>
<p>0.60503 0.4
t (sec)
</p>
<p>020.1
-1 L.---'----'-__...1...L..__-'---l-_J.l--_..l.......L__--l--J
</p>
<p>o
</p>
<p>Figure 18.10: Waveform for the utterance "seven" [Allu 2005].
</p>
<p>(18.50)
</p>
<p>18.11 and as expected is noise-like. It is composed of the samples indicated between
</p>
<p>the dashed vertical lines in Figure 18.10. Typically, in analyzing speech sounds to
</p>
<p>estimate its AR parameters, we sample at 8 KHz and use a block of data 20 msec
</p>
<p>(about 160 samples) in length. The samples of x(t) in Figure 18.10 from t = 115
</p>
<p>msec to t = 135 msec are shown in Figure 18.11. With a model order ofp = 12 we use
</p>
<p>(18.49) to estimate the ACS lags and then solve the Yule-Walker equations of (18.47)
</p>
<p>and also use (18.48) to yield the estimated parameters {a[l], 0,[2], ... ,a[P] , a~}. If
the model is reasonably accur ate, then the synthesized sound should be perceived
</p>
<p>as being similar to the original sound. It has been found through experimentation
</p>
<p>that if the PSDs are similar, then this will be the case. Hence , the estimated PSD
</p>
<p>A2
</p>
<p>?x(J) = (Ju
11 - I:~=l a[k] exp (- j 27rj k) 1
</p>
<p>2
</p>
<p>should be a good match to the normalized and squar ed-magnitude of the Fourier</p>
<p/>
</div>
<div class="page"><p/>
<p>18.7. REAL-WORLD EXAMPLE - SPEECH SYNTHESIS 629
</p>
<p>0.2,--------.--------,------,---,
</p>
<p>0.15
</p>
<p>0.1
</p>
<p>0.05
</p>
<p>~
</p>
<p>t'1 0
</p>
<p>-0.05
</p>
<p>-0.1
</p>
<p>I~ &middot; . ~. ~ &bull;.&middot; l ~ ~ U~j~W J}&bull;&bull;~ l~ &middot;
lml~~ &bull;.. ...llr!11p ~lI
. ... . . . . . . . . . ... . . . . . . . . . . . . . . . . ... . . .. . . .. . . .... . . . . . .. ....
</p>
<p>-0.15 .
</p>
<p>15010050
</p>
<p>-0.2 '-- --'- ----L ...L--J
</p>
<p>o
n
</p>
<p>Figure 18.11: A 20 msec segment of the waveform for "sss". See Figure 18.10 for
</p>
<p>segment extracted as indicated by the vertical dashed lines.
</p>
<p>transform of the speech sound. The latter is of course the periodogram. We need
</p>
<p>only consider the match in power since it is well known that the ear is relatively
</p>
<p>insensitive to the phase of the speech waveform [Rabiner and Schafer 1978].
</p>
<p>As an example, for the portion of the "sss" sound shown in Figure 18.11 a
</p>
<p>periodogram as well as the AR PSD model of (18.50), is compared in Figure 18.12.
</p>
<p>Both PSDs are plotted in dB quantities, which is obtained by taking 10 loglO of the
</p>
<p>PSD. Note that the resonances, i.e., the portions of the PSD that are large and
</p>
<p>which are most important for intelligibility, are well matched by the model. This
</p>
<p>verifies the validity of the AR model. Finally, to synthesize the "sss" sound we
</p>
<p>compute
p
</p>
<p>x[n] = L a[k]x[n - k] + urn]
k=l
</p>
<p>where urn] is a pseudorandom Gaussian noise sequence [Knuth 1981] with variance
a-b, for a total of about 20 msec. Then, the samples are converted to an analog
sound using a digital-to-analog (D/A) convertor (see Figure 18.9). The TI Speak
</p>
<p>and Spell used p = 10 and stored the AR parameters in memory for each sound.
</p>
<p>The MATLAB code used to generate Figure 18.12 is given below.
</p>
<p>N=length(xseg); %xseg is the data shown in Figure 18.11
Nfft=1024; %set up FFT length for Fourier transforms
freq=[O:Nfft-1] '/Nfft-O.5; %PSD frequency points to be plotted
P_per=(1/N)*abs(fftshift(fft(xseg,Nfft))).-2j %compute periodogram
p=12; %dimension of autocorrelation matrix
for k=1:p+1 %estimate ACS for k=O,1, ... ,p (MATLAB indexes</p>
<p/>
</div>
<div class="page"><p/>
<p>630 CHAPTER 18. LINEAR SYSTEMS AND WSS RANDOM PROCESSES
</p>
<p>-20
</p>
<p>~ -30
'-"
</p>
<p>~
&lt;c.. -35
</p>
<p>o......
b.O..sa -40
o
.-i - 45 .
</p>
<p>-50 : : . . . . _ ~ . . . . . . . . . . . .. .
</p>
<p>-55 L---L_--'-_-'-----'_'--'--'----'--_.l-----1._--'------'
- 0.5 -0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4 0.5
</p>
<p>f
</p>
<p>Figure 18.12: Periodogram, shown as the light line , and AR PSD model, shown as
</p>
<p>the darker line for sp eech segment of Figure 18.11.
</p>
<p>i. must start at 1)
rX(k,1)=(1/N)*sum(xseg(1:N-k+1).*xseg(k:N));
</p>
<p>end
</p>
<p>r=rX(2:p+1); i. fill in right-hand-side vector
for i=1:p i. fill in autocorrelation matrix
</p>
<p>for j=1:p
</p>
<p>R(i,j)=rX(abs(i-j)+1);
</p>
<p>end
</p>
<p>end
</p>
<p>a=inv(R)*r; i. solve linear equations to find AR filter parameters
varu=rX(1)-a'*r; i. find excitation noise variance
den=abs(fftshift(fft([1;-a],Nfft))).-2; i. compute denominator of AR PSD
P_AR=varu./den; i. compute AR PSD
</p>
<p>See also Problem 18.34 for an application of AR modeling to spectral est imat ion
</p>
<p>[Kay 1988].
</p>
<p>References
</p>
<p>Allu, G. , personal communication of speech data, 2005.
</p>
<p>Jackson, L.B., Signals , Systems, and Transforms, Addison-Wesley, Reading, MA,
</p>
<p>1991.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 631
</p>
<p>Kay, S., Modern Spectral Estimation: Theory and Application, Prentice-Hall, En-
</p>
<p>glewood Cliffs, NJ , 1988.
</p>
<p>Knuth, D.E. , The Art of Computer Programming, Vol. 2, Addison-Wesley, Read-
</p>
<p>ing , MA, 1981.
</p>
<p>Makhoul, J. , "Linear Prediction: A Tutorial Review," IEEE Proceedings, Vol. 63,
</p>
<p>pp. 561-580, 1975.
</p>
<p>Oppenheim, A.V. , A.S. Willsky, S.H. Nawab , Signal and Systems, Prentice-Hall,
</p>
<p>Upper Saddle River, NJ, 1997.
</p>
<p>Orfanidis, S.J., Optimum Signal Processing, An Introduction, Macmillan, New
</p>
<p>York, 1985.
</p>
<p>Poularikas, A.D. , S. Seely, Signals and Systems, PWS, Boston, 1985.
</p>
<p>Rabiner, L.R. , R.W. Schafer, Digital Processing of Speech Signals , Prentice-Hall,
</p>
<p>Englewood Cliffs, NJ , 1978.
</p>
<p>Wiener, N., Extrapolation, Interpolation, and Smoothing of Stationary Time Series,
</p>
<p>MIT Press, Cambridge, MA, 1949.
</p>
<p>Yaglom, A.M ., An Introduction to the Theory of Stationary Random Functions,
</p>
<p>Dover , New York , 1962.
</p>
<p>Problems
</p>
<p>18.1 C:.:... ) (f) An LSI system with system function 1i(z) = 1 - z-l - z-2 is used
to filter a discrete-time white noise random process with variance a ~ = 1.
</p>
<p>Det ermine the ACS and PSD of the output random process.
</p>
<p>18.2 (f) A discrete-time WSS random process with mean /-Lu = 2 is input to an LSI
</p>
<p>system with impulse response h[n] = (1/2)n for n 2: &deg;and h[n] = &deg;for n &lt; 0.
Find the mean sequence at the system output.
</p>
<p>18.3 (w) A discrete-time white noise random process Urn] is input to a system to
produce the output random process X[n] = a1nIU[n] for [c] &lt; 1. Determine
the output PSD.
</p>
<p>18.4 c.:..:..-) (w) A randomly phased sinusoid X[n] = cos(21f(0.25)n + 8) with 8 "-J
U(0,21f) is input to an LSI system with system function 1i(z) = 1 - b1 z-1 -
</p>
<p>b2z-
2. Determine the filter coefficients bl , b2 so that the sinusoid will have
</p>
<p>zero power at the filter output.</p>
<p/>
</div>
<div class="page"><p/>
<p>632 CHAPTER 18. LINEAR SYSTEMS AND WSS RANDOM PROCESSES
</p>
<p>18.5 (f,c) A discrete-time WSS random process X[n] is defined by the difference
equation X[n] = aX[n - 1] + Urn] - bUrn - 1], where Urn] is a discrete-time
white noise random process with variance (J'b = 1. Plot the PSD of X[n] if
a = 0.9, b = 0.2 and also if a = 0.2, b = 0.9 and explain your results.
</p>
<p>18.6 (f) A discrete-time WSS random process X[n] is defined by the difference
equation X[n] = O.5X[n - 1] + Urn] - O.5U[n - 1], where Urn] is a discrete-
time white noise random process with variance (J'b = 1. Find the ACS and
PSD of X[n] and explain your results.
</p>
<p>18.7 (..:..:.,) (f) A differencer is given by X[n] = Urn] - U[n-1] . If the input random
process Urn] has the PSD Pu(f) = 1- cos(271}), determine the ACS and PSD
at the output of the differencer.
</p>
<p>18.8 (t) Verify that the discrete-time Fourier transform of rx[k] given in (18.15) is
(J'bI H (f )12 .
</p>
<p>18.9 (w) A discrete-time white noise random process is input to an LSI system
which has h[O] = 1 with all the other impulse response samples nonzero. Can
the output power of the filter ever be less than the input power?
</p>
<p>18.10 (w) A random process with PSD
</p>
<p>1
Px(f) = 2
</p>
<p>11 - ~ exp( - j21rJ) I
</p>
<p>is to be filtered with an LSI system to produce a white noise random process
</p>
<p>Urn] with variance (J'b = 4 at the output. What should the difference equation
of the LSI system be?
</p>
<p>18.11 (w,c) An AR random process of order 2 is given by the recursive difference
</p>
<p>equation X[n] = 2r cos(21rfo)X[n -1] - r2X[n - 2]+Urn], where Urn] is white
Gaussian noise with variance (J'b = 1. For r = 0.7, fo = 0.1 and also for
r = 0.95, fo = 0.1 plot the PSD of X[n]. Can you explain your results? Hint:
Determine the pole locations of ll(z).
</p>
<p>18.12 (w) A signal, which is bandlimited to B cycles/sample with B &lt; 1/2, is
modeled as a WSS random process with zero mean and PSD Ps(f). If white
</p>
<p>noise is added to the signal with ( J ~ = 1, find the frequency response of the
optimal Wiener smoother. Explain your results.
</p>
<p>18.13 (..:..:.,) (f,c) A zero mean signal with PSD Ps(f) = 2 - 2 cos(21rJ) is embedded
in white noise with variance ( J ~ = 1. Plot the frequency response of the
optimal Wiener smoother. Also, compute the minimum MSE. Hint: For the
</p>
<p>MSE use a "sum" approximation to the integral (see Problem 1.14).</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 633
</p>
<p>18.14 (c) In this problem we simulate the Wiener smoother. First generate N = 50
</p>
<p>samples of a signal 8[n], which is an AR random process (assumes that U[n]
is white Gaussian noise) with a = 0.25 and a ~ = 0.5. Remember to set the
initial condition 8[-1] "-' N(O, a~j(1 - a2 ) . Next add white Gaussian noise
W[n] with a ~ = 1 to the AR random process realization. Finally, use the
MATLAB code in the chapter to smooth the noise-corrupted signal. Plot the
</p>
<p>true signal and the smoothed signal. How well does the smoother perform?
</p>
<p>18.15 (w) To see that the linear prediction equations of (18.28) cannot be solved
directly using z-transforms, take the z-transform of both sides of the equation.
</p>
<p>Next solve for ll(z) = Z{h[k]}. Explain why the solution for the predictor
cannot be correct.
</p>
<p>18.16 (t) In this problem we rederive the optimal one-step linear predictor for the
</p>
<p>AR random process of Example 18.5. Assume that X[no+1] is to be predicted
based on observing the realization of {X[no] ,X[no - 1], ...}. The random
process X[n] is assumed to be an AR random process described in Example
18.5. Prove that X[no + 1] = aX[no] satisfies the orthogonality principle,
making use of the result that E[U[no + l]X[no - k]] = 0 for k = 0,1, .... The
latter result says that "future" samples of U[n] must be uncorrelated with the
present and past samples of X[n]. Explain why this is true. Hint: Recall that
for an AR random process X[n] can be rewritten as X[n] = 2:~o alU[n -l].
</p>
<p>18.17 (w) For the AR random process described in. Example 18.5 show that the
minimum MSE for the optimal predictor X[no + 1] = aX[no] is given by
msemin = rx[O](l - a2 ) . Use this to explain why the results shown in Figure
18.7 are reasonable.
</p>
<p>18.18 C:....:J (w) Express the minimum MSE given in the previous problem in terms
of rx[O] and the correlation coefficient between X[no] and X[no + 1]. What
happens to the minimum MSE if the correlation coefficient magnitude ap-
</p>
<p>proaches one and also if it is zero?
</p>
<p>18.19 (c) Consider an AR(2) random process given by X[n] = _r2X[n - 2]+U[n],
where Urn] is white Gaussian noise with variance a ~ and 0 &lt; r &lt; 1. This
random process follows from (18.33) with p = 2 and a[l] = 0, a[2] = _r2 .
The ACS for this random process can be shown to be rx[k] = (a~j(1 -
r 4&raquo;r1kl cos(k1rj2) [Kay 1988]. Find the optimal one-step linear predictor based
</p>
<p>on the present and past samples of X[n]. Next perform a computer simulation
to see how the predictor performs. Consider the two cases r = 0.5, a~ = 1- r 4
</p>
<p>and r = 0.95, a~ = 1 - r 4 so that the average power in each case is the
same (rx[O] = 1). Generate 150 samples of each process and discard the first
100 samples to make sure the generated samples are WSS. Then, plot the
</p>
<p>realization and its predicted values for each case. Which value of r results in
</p>
<p>a more predictable process?</p>
<p/>
</div>
<div class="page"><p/>
<p>634 CHAPTER 18. LINEAR SYSTEMS AND WSS RANDOM PROCESSES
</p>
<p>18.20 (t) Derive the Wiener-Hopf equations given by (18.36) and the resulting min-
</p>
<p>imum MSE given by (18.37) for the finite length predictor.
</p>
<p>18.21 (f) For M = 1 solve the Wiener-Hopf equations given by (18.36) to find h[O].
Relate this to cov(X,Y)jvar(X) used in the prediction of Y given X = x .
</p>
<p>18.22 C:..:...) (f) The MA random process described in Example 18.6 and given by
X[n] = Urn] - bUrn - 1] has as its ACS for ab = 1
</p>
<p>{
</p>
<p>l+b2 k=O
</p>
<p>rx[k] = -s k = 1
o k ~ 2.
</p>
<p>For M = 2 solve the Wiener-Hopf equations to find this finite length predictor
</p>
<p>and then determine the minimum MSE. Compare this minimum MSE to that
</p>
<p>of the infinite length predictor given in Example 18.6.
</p>
<p>18.23 (f) It is desired to predict white noise. Solve the Wiener-Hopf equations for
</p>
<p>rx[k] = ai-o[k] and explain your results.
</p>
<p>18.24 C:..:...) (f,c) For the MA random process X[n] = Urn] - iU[n - 1] where Urn]
is white Gaussian noise with ab = 1 find the optimal finite length predictor
X[no + 1] = h[O]X[no] + h[I]X[no -1] and the corresponding minimum MSE.
Next simulate the random process and compare the estimated minimum MSE
</p>
<p>with the theoretical one. Hint: Use your results from Problem 18.22.
</p>
<p>18.25 (f) Consider the prediction of a randomly phased sinusoid whose ACS is
</p>
<p>rx[k] = cos(21rfok). For M = 2 solve the Wiener-Hopfequations to determine
the optimal linear predictor and also the minimum MSE. Hint: You should be
</p>
<p>able to show that the minimum MSE is zero. Use the trigonometric identity
</p>
<p>cos(20) = 2cos2(O) - 1.
</p>
<p>18.26 (t) In this problem we consider the L-step infinite length predictor of an AR
random process. Let the predictor be given as
</p>
<p>00
</p>
<p>X[no + L] = L h[k]X[no - k]
k=O
</p>
<p>and show that the linear equations to be solved to determine the optimal h[kl's
are
</p>
<p>00
</p>
<p>rx[l + L] = L h[k]rx[l - k]
k=O
</p>
<p>Next show that the minimum MSE is
</p>
<p>l = 0,1, ....
</p>
<p>00
</p>
<p>msemin = rx[O]- L hopdk]rx[k + L].
k=O</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 635
</p>
<p>Finally, for an AR random process with ACS rx[k] = (O"bl(l - a2&raquo;a1kl show
</p>
<p>that
</p>
<p>X[no + L] = aL X [no]
msemin = rx[O](l - a2L )
</p>
<p>for a predictor based on {X[no] ,X[no -1], ... } . To do so assume that h[k] = 0
for k 2: 1 and show that the equations can be satisfied by choosing h[O).
Explain what happens to the quality of the prediction as L increases and why.
</p>
<p>18.27 (..:.:,) (t) In this problem we consider the interpolation of a random process
using a sample on either side of the sample to be interpolated. We wish to
</p>
<p>estimate or interpolate X[no] using X[no] = h[-l)X[no +1] +h[l]X[no -1] for
some impulse response values h[-1], h[l]. Find the optimal impulse response
values by minimizing the MSE of the interpolated sample if X[n] is the AR
</p>
<p>random process given by X[n] = aX[n - 1) + Urn). Does your interpolator
average the samples on either side of X[no)? What happens as a -+ 1 and as
</p>
<p>a -+ O?
</p>
<p>18.28 (f) An LTI system has the impulse response h(r) = exp(-r) for r 2: 0 and is
zero for r &lt; O. If continuous-time white noise with ACF ru(r) = (No/2)5(r)
is input to the system, what is the PSD of the output random process? Sketch
</p>
<p>the PSD.
</p>
<p>18.29 (..:.:,) (f) An LTI system has the impulse response h(r) = 1 for 0 :::; r :::; T
and is zero otherwise. If continuous-time white noise with ACF ru(r) =
(No/2)5(r) is input to the system, what is the PSD of the output random
process? Sketch the PSD.
</p>
<p>18.30 (f) A filter with frequency response H(F) = exp( -j21rFro) is used to filter a
</p>
<p>WSS random process with PSD Px(F). What is the PSD at the filter output
and why?
</p>
<p>18.31 (t) Prove that if a continuous-time white noise random process with ACF
</p>
<p>ru(r) = (No/2)5(r) is input to an LTI system with impulse response h(r),
then the ACF of the output random process is
</p>
<p>IV, roo
rx(r) = T J-oo h(t)h(t + r)dt.
</p>
<p>18.32 (..:.:,) (w) An RC electrical circuit with frequency response
</p>
<p>H(F) _ 11RC
- 11RC+ j21rF
</p>
<p>is used to filter a white noise random process with ACF ru(r) = (No/2)5(r).
</p>
<p>Find the total average power at the filter output. Is it infinite? Hint: See
</p>
<p>previous problem.</p>
<p/>
</div>
<div class="page"><p/>
<p>636 CHAPTER 18. LINEAR SYSTEMS AND WSS RANDOM PROCESSES
</p>
<p>18.33 (t) Two continuous-time WSS zero mean random processes X(t) and Y(t)
</p>
<p>are uncorrelated, which means that E[X(tl)Y(t2)] = 0 for all tl and ta- Is the
sum random process Z(t) = X(t) +Y(t) also WSS , and if so, what is its ACF
and PSD?
</p>
<p>18.34 (c) In this problem we compare the periodogram spectral estimator to one
</p>
<p>based on an AR(2) model. This assumes, however, that the AR model is
</p>
<p>an accurate one for the random process. First generate N = 50 samples of
a realization of the AR(2) random process described in Problem 18.19 with
</p>
<p>r = 0.5 and ab = 1 - r4 &bull; Next plot the periodogram of the realization (see
Section 17.6). Using the estimate of the ACS given in (18.49) solve the Yule-
</p>
<p>Walker equations of (18.47) for p = 2 and then find a-b from (18.48). Finally,
plot the estimated PSD given by (18.50) and compare it to the periodogram
</p>
<p>as well as the true PSD. You may also wish to print out a[l ] and a[2] and
compare them to the theoretical values of a[l] = 0 and a[2] = _r2 = - 0.25.
Hint: You can use the MATLAB code given in Section 18.7.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 18A
</p>
<p>Solution for Infinite Length
</p>
<p>Predictor
</p>
<p>The equations to be solved for the one-step predictor are from (18.28)
</p>
<p>00
</p>
<p>rx [l + 1] = L h[k]rx[l - k]
k=O
</p>
<p>l = 0,1 , . .. (18A.1)
</p>
<p>and the minimum MSE can be writ t en from (18.29) as
</p>
<p>00
</p>
<p>msemin = rx[O] - L hopdk]rx [-l - k].
k=O
</p>
<p>Now let n = l + 1 in (18A.1) so that
</p>
<p>(18A.2)
</p>
<p>00
</p>
<p>rx[n] = Lh[k]rx[n -1- k]
k=O
00
</p>
<p>Lh[j -l]rx[n - j]
j= l
</p>
<p>and also let j = k + 1 in (18A.2) to yield
00
</p>
<p>n=1 ,2, ...
</p>
<p>(let j = k + 1) (18A.3)
</p>
<p>(18A.4)rx [O] = L h[j - l]rx[-j] + msemin
j=l
</p>
<p>where we drop the "opt" on hopdk] since h[k] and msemin are unknowns that we
wish to solve for. Then combining (18A.3) and (18A.4) we have
</p>
<p>00
</p>
<p>rx[n] = L h[j - l ]rx [n - j ] + msemin8nO
j=l
</p>
<p>n = 0,1 , ...</p>
<p/>
</div>
<div class="page"><p/>
<p>638 CHAPTER 18. LINEAR SYSTEMS AND WSS RANDOM PROCESSES
</p>
<p>where 8no = 1 for n = 0 and 8no = 0 for n ~ 1. Next divide both sides by msemin to
yield
</p>
<p>Let
</p>
<p>rx[n] _ ~ h[j - 1] [ .] s
--=-=- - L...J rx n - J + UnO
msemin 0 mSemin
</p>
<p>3=1
</p>
<p>n = 0, 1, . ...
</p>
<p>[0] {l /msem in j = 0
9 J = -h[j - 1]/msem in j = 1,2, ...
</p>
<p>so that the equations become
</p>
<p>00
</p>
<p>rx[n]g[O] = - Lg[j]rx[n - j] + 8no
j=1
</p>
<p>or
</p>
<p>(18A.5)
</p>
<p>(18A.6)
</p>
<p>00
</p>
<p>Lg[j]rx[n - j] = 8no n = 0,1, ....
j=O
</p>
<p>Now if (18A.6) can be solved for g[j], then h[j], msemin can then be found from
(18A.5). Note that (18A.6) is a discrete-time convolution that holds for n ~ O. We
</p>
<p>therefore need to find a causal sequence g[n] (since the sum in (18A.6) is only over
</p>
<p>j ~ 0), which when convolved with rx[n] yields 1 for n = 0 and 0 for n &gt; O. Note
that the values of g[n]* rx[n] for n &lt; 0 are unspecified by the equations. Hence,
g[n] * rx[n] must be an anticausal sequence to be a solution of (18A.6). This can
easily be solved if
</p>
<p>00
</p>
<p>Px(z) = L rx[n]z-n
k=-oo
</p>
<p>can be written as
</p>
<p>(18A.7)
</p>
<p>where
00
</p>
<p>A(z) = 1 - L a[k]z-k
k=1
</p>
<p>has all its zeros within the unit circle of the z plane. Now IIA(z) is the z-transform
of a causal sequence. This is because if all the zeros of A(z) are within the unit
</p>
<p>circle, then all the poles of l/A(z) are within the unit circle. Thus, the z-transform
</p>
<p>1IA( z) must converge on and outside of the unit circle. Also, then 1IA(z-1) is the
z-transform of an anticausal sequence. Assuming this is possible (18A.6) becomes
</p>
<p>Z-1{9(z)Px(z)} = { 2}1 (7uZ- 9(z) A(z)A(z-1)
{
</p>
<p>I n = 0
</p>
<p>On&gt; 0</p>
<p/>
</div>
<div class="page"><p/>
<p>APPENDIX 18A. SOLUTION FOR INFINITE LENGTH PREDICTOR 639
</p>
<p>where 9(z) is the z-t ransform of g[n] and Z-l denotes the inverse z-t ransform. Now
</p>
<p>if we choose
</p>
<p>then
</p>
<p>9(z) = A(z)
a 2U
</p>
<p>{ 2}1 auZ- 9(z) A(z)A(z-l) Z-l { A ( ~ - l ) }
{
</p>
<p>I n = 0
On&gt; 0
</p>
<p>(18A.8)
</p>
<p>since 1/A(z-l) is the z-transform of an anticausal sequence , and the equations are
</p>
<p>satisfied. The inverse z-t ransform for n = 0 has been obtained by using the initial
</p>
<p>value theorem [Jackson 1991] which says that for an anticausal sequence x[n]
</p>
<p>Z -l { t x[n]z- n} = lim t x[n]z- n = x[O].
z-tO
</p>
<p>n=-oo n=O n=-oo
</p>
<p>Therefore , we have that
</p>
<p>Z-l { A ( ~ - l ) } In=o = l~ A(~-l ) = 1.
The solution for g[n] is from (18A.8)
</p>
<p>g[n] = Z-l {Aa~ u Z)} = { l/a ~ n = 0
-a[ n]/a ~ n ~ 1
</p>
<p>and using (18A.5)
</p>
<p>1
</p>
<p>msemin
</p>
<p>h[j -1]
</p>
<p>msemin
</p>
<p>Finally, we have the result that
</p>
<p>1
g[0]=2
</p>
<p>au
</p>
<p>g[j] = _ aU]
a 2U
</p>
<p>j ~ 1.
</p>
<p>h[n]
</p>
<p>msemin
</p>
<p>= a[n + 1]
2
</p>
<p>au&middot;
</p>
<p>n = 0,1 , . . .</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 19
</p>
<p>Multiple Wide Sense Stationary
</p>
<p>Random Processes
</p>
<p>19 .1 Introduction
</p>
<p>In Chapters 7 and 12 we defined multiple random variables X and Y as a mapping
from the sample space S of the experiment to a point (x, y) in the x-y plane. We
</p>
<p>now extend that definition to be a mapping from S to a point in the x-y plane that
</p>
<p>evolves with time, and denote that point as (x[n],y[n]) for -00 &lt; n &lt; 00. The
mapping, denoted either by (X[n], Y[n]) or equivalently by [X[n] Y[n]V , is called a
jointly distributed random process. An example is the mapping from a point at some
</p>
<p>geographical location, where the possible choices for the location constitute S, to the
</p>
<p>daily temperature and pressure at that point or (T[n], P[n]). Instead of treating the
random processes, which describe temperature and pressure, separately, it makes
</p>
<p>more sense to analyze them jointly. This is especially true if the random processes
</p>
<p>are correlated. For example, a drop in barometric pressure usually indicates the
</p>
<p>onset of a storm, which in turn will cause a drop in the temperature. Another
</p>
<p>example of great interest is the effect of a change in the Federal Reserve discount
</p>
<p>rate, which is the percentage interest charged to banks by the federal government,
</p>
<p>on the rate of job creation. It is generally assumed that by lowering the discount
</p>
<p>rate, companies can borrow money more cheaply and thus invest in new products
</p>
<p>and services, thus increasing the demand for labor. The jointly distributed random
</p>
<p>processes describing this situation are I[n], the daily discount interest rate, and
J[n], the daily number of employed Americans. Many other examples are possible,
encompassing a wide range of disciplines.
</p>
<p>In this chapter we extend the concept of a wide sense stationary (WSS) ran-
</p>
<p>dom process to two jointly distributed WSS random processes. The extension to
</p>
<p>any number of WSS random processes can be found in [Bendat and Piersol 1971,
</p>
<p>Jenkins and Watts 1968, Kay 1988, Koopmans 1974, Robinson 1967]. Multiple
</p>
<p>random process theory is known by the synonymous terms multivariate random</p>
<p/>
</div>
<div class="page"><p/>
<p>642 CHAPTER 19. MULTIPLE WSS RANDOM PROCESSES
</p>
<p>processes, multichannel random processes, and vector random processes. Also, the
</p>
<p>characterization of the random processes at the input and output of an LSI system
</p>
<p>is explored. We will find that the extensions are much the same as in going from a
</p>
<p>single random variable to two random variables, especially since the definitions are
</p>
<p>based on samples of the random process, which themselves are random variables.
</p>
<p>As in previous chapters our focus will be on discrete-time random processes but
</p>
<p>the analogous concepts and formulas for continuous-time random processes will be
</p>
<p>summarized later.
</p>
<p>19.2 Summary
</p>
<p>Two random processes are jointly WSS if they are individually WSS (satisfy (19.1)-
</p>
<p>(19.4)) and also the cross-correlation given by (19.5) does not depend on n. The se-
</p>
<p>quence given by (19.5) is called the cross-correlation sequence. The cross-correlation
</p>
<p>sequence has the properties given in Property 19.1-19.4, which differ from those of
</p>
<p>the ACS. Jointly WSS random processes are defined to be uncorrelated if (19.12)
</p>
<p>holds. The cross-power spectral density is defined by (19.13) and is evaluated using
</p>
<p>(19.14). It has the properties given by Property 19.5-19.9, which differ from those
</p>
<p>of the PSD. The correlation between two jointly WSS random processes can be mea-
</p>
<p>sured in the frequency domain using the coherence function defined in (19.20). The
</p>
<p>ACS and PSD for the sum of two jointly distributed WSS random processes is given
</p>
<p>in Section 19.5. If the random processes are uncorrelated, then the ACS and PSD
</p>
<p>of the sum random process are given by (19.25) and (19.26), respectively. For the
</p>
<p>filtering operation shown in Figure 19.2a the cross-correlation sequence is given by
</p>
<p>(19.27) and the cross-power spectral density by (19.28). For the filtering operation
</p>
<p>shown in Figure 19.2b the cross-correlation sequence is given by (19.29) and the
</p>
<p>cross-power spectral density by (19.30). The corresponding definitions and formulas
</p>
<p>for continuous-time random processes are given in Section 19.6. Estimation of the
</p>
<p>cross-correlat ion sequence is discussed in Section 19.7 with the estimate given by
</p>
<p>(19.46). Finally, an application of cross-correlation to brain physiology research is
</p>
<p>described in Section 19.8.
</p>
<p>19.3 Jointly Distributed WSS Random Processes
</p>
<p>We will denote the two discrete-time random processes by X[n] and Y[n] for -00 &lt;
n &lt; 00. Of particular interest will be the extension of the concept of wide sense
stationarity from one to two random processes. To do so we first assume that each
</p>
<p>random process is individually WSS, which is to say that
</p>
<p>px [n]
</p>
<p>rx[k]
</p>
<p>J-ly [n]
</p>
<p>ry[k] =
</p>
<p>E[X[n]] = ux
E[X[n]X[n + k]]
E[Y[n]] = J-ly
</p>
<p>E[Y[n]Y[n + k]]
</p>
<p>(19.1)
</p>
<p>(19.2)
</p>
<p>(19.3)
</p>
<p>(19.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>19.3. JOINTLY DISTRIBUTED WSS RANDOM PROCESSES 643
</p>
<p>or the first two moments do not depend on n. For the concept of wide sense sta-
</p>
<p>tionarity to be useful in the context of two random processes, we require a further
</p>
<p>definition. To motivate it , consider the situation in which we add the two ran-
</p>
<p>dom processes together and wish to determine the overall average power. Then, if
</p>
<p>Z[n] = X[n] + Y[n] , we need to find E[Z2[n]]. Proceeding we have
</p>
<p>E[Z2[n]] E[(X[n] + Y[n])2]
</p>
<p>= E[X2[n]] + E[X[n]Y[n]] + E[Y[n]X[n]] + E[y2[n]]
</p>
<p>= rx[O] + 2E[X [n]Y[n]] + ry[O].
</p>
<p>To complete the calculation we require knowledge of the joint moment E[X[n]Y[n]].
If it does not depend on n, then E[Z2[n]] will likewise not depend on n. More
generally, if we were to compute E[Z[n]Z[n + k]], then we would require knowledge
of E[X[n]Y[n + k]] and so we will assume that the latter does not depend on n.
Therefore, with this assumption we can now define
</p>
<p>rx,y[k] = E[X[n]Y[n + k]] k= ... , - I ,O, I , .... (19.5)
</p>
<p>This new sequence is called the cross-correlation sequence (eeS). Returning to our
average power computation we can now write that
</p>
<p>E[Z2[n]] = rx[O] + 2rx,Y[O] + ry[O]
</p>
<p>and the average power is seen not to depend on n. Note also from the definition of
</p>
<p>the ees, that the ACS is just rx,x[k] .
If X[n ] and Y[n] are WSS random processes and a ces can be defined
</p>
<p>(E[X[n]Y[n + k]] not dependent on n), then the random processes are said to be
jointly wide sense stationary. In summary, for the two random processes to be
jointly WSS we require the conditions (19.1)-(19.5) to hold. An example follows.
</p>
<p>Example 19.1 - ecs for WSS random processes delayed with respect to
each other
</p>
<p>Let X[n] be a WSS random process and let Y[n] be a delayed version of X[n] so
that Y[n] = X[n - no]. Then, to determine if the random processes are jointly WSS
we have
</p>
<p>E[X[n]]
</p>
<p>E[Y[n]]
</p>
<p>E[X[n]X[n + k]] =
E[Y[n]Y[n + k]] =
E[X[n]Y[n + k]]
</p>
<p>/-Lx
</p>
<p>E[X[n - no]] = ux
rx[k]
</p>
<p>E[X[n - no]X[n + k - no]] = rx[k]
E[X[n]X[n + k - no]] = rx[k - no] (19.6)
</p>
<p>all of which follow from our definition of Y[n] and the assumption that X[n] is WSS.
Note that E[X[n]Y[n + k]] does not depend on n and so a ees can be defined. It
is given by (19.6) as
</p>
<p>rx,y[k] = rx[k - no]. (19.7)</p>
<p/>
</div>
<div class="page"><p/>
<p>644 CHAPTER 19. MULTIPLE WSS RANDOM PROCESSES
</p>
<p>Since all the first and second moments do not depend on n, the random processes
</p>
<p>are jointly WSS.
</p>
<p>&lt;&gt;
We will henceforth assume that X[n] and Y[n] are jointly WSS unless stated oth-
</p>
<p>erwise. From the previous example it is observed that the ees has very different
</p>
<p>properties than the AeS. Unlike the AeS, the CCS does not necessarily have its
</p>
<p>maximum value at k = O. In the previous example, the maximum of the ees oc-
</p>
<p>curs at k = no (see (19.7)). Also, in general we do not have rx,y[-k] = rx,y[k]
</p>
<p>or the CCS is not symmetric about k = O. In the previous example, we have from
(19.7)
</p>
<p>rx,y[-k] rx[-k - no]
</p>
<p>= rx[k + no] i= rx[k - no] = rx,y[k].
</p>
<p>Furthermore, even though the ees is symmetric about k = no in the previous
</p>
<p>example, it need not be symmetric at all.
</p>
<p>ees asymmetry requires vigilance.
</p>
<p>Since the ees is not symmetric, in contrast to the AeS, one must be careful.
</p>
<p>The cross-second moment E[X[m]Y[n]] , where X[n] and Y[n] are jointly WSS, is
</p>
<p>expressed in terms of the ees as rx,y[n - m], not rx,y[m - n]. To determine the
</p>
<p>argument k of the ees for rx,y[k], always take the index of the Y random variable
</p>
<p>and subtract the index of the X random variable. For example, E[X[3]Y[I]] =
rx,y[1 - 3] = rx,y[-2]. This is especially important in light of the fact that the
</p>
<p>definition of the ees is not standard. Some authors use rx,y[k] = E[X[n]Y[n - k]],
which will produce a ees that is "flipped around" in k, relative to our definition.
</p>
<p>We give one more example and then summarize the properties of the ees.
</p>
<p>Example 19.2 - Another calculation of the CCS
</p>
<p>Assume that X[n] = Urn] and Y[n] = Urn] + 2U[n - 1], where Urn] is white noise
with variance ( J ~ = 1. Thus, X[n] is a white noise random process and Y[n] is a
</p>
<p>general MA random process, i.e. , no Gaussian assumption is made. Then, it is easily
</p>
<p>shown that JLx[n] = JLy[n] = 0, rx[k] = 8[k], and
</p>
<p>{
</p>
<p>5 k = 0
ry[k] = 2 k = &plusmn;1
</p>
<p>o otherwise
</p>
<p>so that X[n] and Y[n] are individually WSS. Now computing the cross-second mo-</p>
<p/>
</div>
<div class="page"><p/>
<p>19.3. JOINTLY DISTRIBUTED WSS RANDOM PROCESSES
</p>
<p>ment, we have
</p>
<p>645
</p>
<p>E[X[n]Y[n + k]] E[U[n](U[n + k] + 2U[n+ k - 1])]
ru[k] + 2ru[k - 1]
8[k] + 28[k - 1]
</p>
<p>and it is seen to be independent of n. Hence, the CCS is
</p>
<p>rx,y[k] = 8[k] + 28[k - 1]
</p>
<p>and the random processes are jointly WSS. The ACSs and the CCS are shown in
</p>
<p>Figure 19.1. We observe that rx,y[-k] i- rx,y[k] and that the maximum does not
occur at k = O. We can assert, however , that the maximum must be less than or
equal to vrx[O]ry[O] since by the Cauchy-Schwarz inequality (see Appendix 7A)
</p>
<p>Irx,y[k]l IE[X[n]Y[n + k]]1
&lt; VE[X2 [n]]E[y2[n+ k]]
</p>
<p>Vrx[O]ry[O].
</p>
<p>For this example we see that
</p>
<p>Irx,y[k]j ::; ~ = vIS.
</p>
<p>o
We now summarize the properties (or more appropriately the nonproperties) of the
</p>
<p>CCS.
</p>
<p>Property 19.1 - CCS is not necessarily symmetric.
</p>
<p>rx,y[-k] i- rx,y[k] (19.8)
</p>
<p>o
</p>
<p>Property 19.2 - The maximum of the CCS can occur for any value of k.
</p>
<p>o
</p>
<p>Property 19.3 - The maximum value of the CCS is bounded.
</p>
<p>Irx,y[k] I ::; vrx[O]ry[O] (19.9)
</p>
<p>o</p>
<p/>
</div>
<div class="page"><p/>
<p>646
</p>
<p>5
</p>
<p>4
</p>
<p>CHAPTER 19. MULTIPLE WSS RANDOM PROCESSES
</p>
<p>5
</p>
<p>4
</p>
<p>o I... .. ... . . . .~ .. .. &bull; . . .. ~ .. o .. ...&bull;. ...~ ...II. . ~ . . . .~ ..
-1 .
</p>
<p>-4 -3 -2 -1
</p>
<p>(a)
</p>
<p>o
k
</p>
<p>2 3 4
</p>
<p>-1 . . ..... . . .&bull;..
</p>
<p>-4 -3 -2 -1
</p>
<p>(b)
</p>
<p>023
k
</p>
<p>4
</p>
<p>5 . .. .. , .. ... , ... . . , . . . . . , . . ... , .. .
</p>
<p>4
</p>
<p>IFo .. ...~ . ...&bull; ....~ . &bull; . ..&bull;.. ..
-1 .
</p>
<p>-4 -3 - 2 -1 0
</p>
<p>k
</p>
<p>(c)
</p>
<p>234
</p>
<p>Figure 19.1: Autocorrelation and cross-correlation sequences for Example 19.2.
</p>
<p>A fourth property that is useful arises by considering E[Y[n]X[n + k]], which is
the cross-second moment with X[n] and Y[n] interchanged. Assuming jointly WSS
random processes, this moment becomes
</p>
<p>E[Y[n]X[n + k]] E[X[n + k]Y[n]]
= E[X[m]Y[m - k]]
</p>
<p>rx,y[-k]
</p>
<p>(let m = n + k)
(from definition of CCS).
</p>
<p>Therefore, E[Y[n]X[n + k]] does not depend on n and so we can define another
cross-correlation sequence as
</p>
<p>rY,x[k] = E[Y[n]X[n + k]] (19.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>19.4. THE CROSS-POWER SPECTRAL DENSITY 647
</p>
<p>and it is seen to be equal to rx,y[-k]. Thus, as our last property we have
</p>
<p>Property 19.4 - Interchanging X[n] and Y[n] flips the ecs about k = O.
</p>
<p>rY,x[k] = rx,y[-k] (19.11)
</p>
<p>o
Next, we define the concept of un correlat ed jointly WSS random processes. Two
</p>
<p>zero m ean jointly WSS random processes are said to be uncorrelated if
</p>
<p>rx,y[k] = 0 for - 00 &lt; k &lt; 00 (19.12)
</p>
<p>including k = O. (For nonzero mean random processes the definition of uncorrelated
random processes is that rx,y[k] = /-Lx/-Ly for -00 &lt; k &lt; 00.) Of course, if the
random processes are independent so that E[X[n]Y[n + k]] = 0 does not depend on
n, then they must be jointly WSS as well. It also follows from Property 19.4 that
</p>
<p>if the random processes are uncorrelated, then rY,x[k] = 0 for all k. An example
follows.
</p>
<p>Example 19.3 - Uncorrelated sinusoidal random processes
</p>
<p>Let X[n] = cos(21rfon + 8 1) and Y[n] = cos(21rfon + 8 2 ) , where 81 '" U(O , 21r) ,
8 2 '" U(O, 21r), and 8 1 and 8 2 are independent random variables. Then, we have
</p>
<p>seen previously that X[n] and Y[n] are individually WSS (see Example 17.4) and
</p>
<p>E[X[n]Y[n + k]]
Eel ,e2[COs(21rfon + 8 1) cos(21rfo(n + k) + 8 2 ) ]
Eel [cos(21rfon + 8d]Ee2 [cos(21r fo(n + k) + 8 2 ) ] (independent random
</p>
<p>variables and (12.30))
</p>
<p>= 0
</p>
<p>since each random sinusoid has a zero mean (see Example 16.11). Thus, the random
</p>
<p>processes are uncorrelated and jointly WSS. Can you interpret this result physically?
</p>
<p>o
</p>
<p>19.4 The Cross-Power Spectral Density
</p>
<p>The PSD of a WSS random process was seen earlier to describe the distribution of
</p>
<p>average power with frequency. Also, the average power of the random process in a
</p>
<p>band of frequencies is obtained by integrating the PSD over that frequency band.
</p>
<p>In a similar vein to the definition of the PSD, we can define the cross-power spectral
</p>
<p>density (CPSD) of two jointly WSS random processes as
</p>
<p>PX,Y (I) ~ , , ) ~ = 2M!+ ! E [ CtM X[n] exp( - j21rfn)rCtM Yin] exp( -j21r f n)) ]
(19.13)</p>
<p/>
</div>
<div class="page"><p/>
<p>648 CHAPTER 19. MULTIPLE WSS RANDOM PROCESSES
</p>
<p>which results in the usual PSD if Y[n] = X[n]. Using a similar derivation as the one
</p>
<p>that resulted in the Wiener-Khinchine theorem, it can be shown that (see Problem
</p>
<p>19.8)
00
</p>
<p>PX,y(J) = 2: rx,y[k] exp(-j27l"fk).
k=-oo
</p>
<p>(19.14)
</p>
<p>It is less clear than for the PSD what the physical significance of the CPSD is. From
</p>
<p>(19.13) it appears that the CPSD will be large when the Fourier transforms of X[n]
</p>
<p>and Y[n] at a given frequency are large and are in phase. Conversely, when the
</p>
<p>Fourier transforms are either small or out of phase, the CPSD will be small. This is
</p>
<p>confirmed by the results of Example 19.3 in which the sinusoidal processes have all
</p>
<p>their power at f = &plusmn;fo since Px(J) = Py(J) = ~o(J + fa) + ~o(J - fa). However,
because they have phases that are independent of each other and can take on values
</p>
<p>in (0,27l") uniformly, rx,y[k] = 0 and therefore, PX,y(J) = O. On the other hand,
</p>
<p>if the phase random variables were statistically dependent, say 8 1 = 8 2 , then the
CPSD would be large (see Problem 19.9). Another example follows.
</p>
<p>Example 19.4 - ees for WSS random processes delayed with respect to
each other (continued)
</p>
<p>We continue Example 19.1 in which Y[n] = X[n - no] and X[n] is WSS. We saw
that the CCS is given by rx,y[k] = rx[k - no]. Using (19.14) the CPSD is
</p>
<p>00
</p>
<p>PX,y(J) = 2: rx[k - no]exp(-j27l"fk)
k=-oo
</p>
<p>and letting l = k - no produces
</p>
<p>00
</p>
<p>PX,y(J) = 2: rx[l] exp[-j27l"f(l + no)]
1=-00
</p>
<p>00
</p>
<p>2: r x [l] exp( - j27l"ft) exp( -j27l"fno)
1=-00
</p>
<p>Px(J) exp( -j27l"fno).
</p>
<p>It is seen that the CPSD is a complex function and that PX,y( - f) =I PX,y(J).
It does appear, however, that PX,y(- f) = PX,y(J) so that it has the symmetry
properties
</p>
<p>IPx,Y(-f) I
L.PX,y(- f) =
</p>
<p>IPx,y(J)!
</p>
<p>-L.PX,y(J) (19.15)
</p>
<p>or the magnitude of the CPSD is an even function and the phase of the CPSD is an
</p>
<p>odd function. This result is indeed true as we will prove in Property 19.6.</p>
<p/>
</div>
<div class="page"><p/>
<p>19.4. THE CROSS-POWER SPECTRAL DENSITY 649
</p>
<p>(19.16)
</p>
<p>One way to think about the CPSD is as a correlation between the normalized Fourier
</p>
<p>transforms of X[n] and Y[n] at a given frequency. From (19.13) we see that if
</p>
<p>1 M
L X[n] exp( -j27rfn)
</p>
<p>.j2M + 1 n=-M
</p>
<p>1 M
L Y[n] exp( -j27rfn)
</p>
<p>.j2M + 1 n=-M
</p>
<p>then
</p>
<p>PXy(J) = lim E[X2M+1(J)Y2M+l (J)] ., M--+oo (19.17)
</p>
<p>This is a correlation between the two complex random variables X2M+l (J) and
</p>
<p>Y2M+l (J). In fact, a normalized version of the CPSD is a complex correlation coef-
</p>
<p>ficient. Indeed, from the Cauchy-Schwarz inequality for complex random variables
</p>
<p>(see Appendix 7A for real random variables) , we have that (recall that if X = U+jV,
then E[X] is defined as E[X] = E[U] + jE[V])
</p>
<p>(19.18)
</p>
<p>and therefore as M --7 00, this becomes from (19.17) and (17.30)
</p>
<p>(19.19)
</p>
<p>Thus, if we normalize the CPSD to form the complex function of frequency
</p>
<p>( )
PX,y(J)
</p>
<p>'Yx,y f = Jpx(J)Py(J) (19.20)
</p>
<p>then we have that l'Yx,y(J) I ~ 1. The complex function of frequency 'Yx,y(J) is
called the coherence function and it is a complex correlation coefficient. It measures
</p>
<p>the correlation between the Fourier transforms of two jointly WSS random processes
</p>
<p>at a given frequency. As an example, consider the random processes of Example
</p>
<p>19.4. Then
</p>
<p>Jpx(J)Py(J)
</p>
<p>Px(J) exp( -j27rfno)
=
</p>
<p>Jpx(J)Px(J)
</p>
<p>exp( -j27rfno) (since Px(J) ~ 0).
</p>
<p>The magnitude of the coherence is unity for all frequencies , meaning that the Fourier
</p>
<p>transform of Y[n] at a given frequency can be perfectly predicted from the Fourier
</p>
<p>transform of X[n] at the same frequency since Y[n] = X[n - no]. It follows that</p>
<p/>
</div>
<div class="page"><p/>
<p>650 CHAPTER 19. MULTIPLE WSS RANDOM PROCESSES
</p>
<p>Y2M+l(f) = exp(-j2n}nO)X2M+l(f) and therefore Y2M+l(f) = ')'x,y(f)X2M+l(f)
for all f. Furthermore, since the coherence magnitude is unity for all frequencies,
the prediction of the frequency component of Y[n] is perfect for all frequencies as
</p>
<p>well. This says finally that Y[n] can be perfectly predicted from X[n]. To do so
</p>
<p>just let Y[n] = X[n + no]. In general, we will see later that if Y[n] is the output of
an LSI system whose input is X[n], then the coherence magnitude is always unity.
</p>
<p>Can you interpret Y[n] = X[n - no] as the action of an LSI system? Finally, in
contrast to perfect prediction, consider the CPSD if X[n] and Y[n] are zero mean
</p>
<p>and uncorrelated. Then since rx,y[k] = 0, we have that PX,y(f) = 0 for I , and of
course the coherence will be zero as well. We now summarize the properties of the
</p>
<p>CPSD.
</p>
<p>Property 19.5 - CPSD is Fourier transform of the ecs.
</p>
<p>00
</p>
<p>PX,y(f) = L rx,y[k] exp(-j21rfk)
k=-oo
</p>
<p>Proof: See Problem 19.8.
</p>
<p>o
</p>
<p>Property 19.6 - CPSD is a hermitian function.
</p>
<p>A complex function g(f) is hermitian if its real part is an even function and its
</p>
<p>imaginary part is an odd function about f = O. This is equivalent to saying that
g(- 1) = g*(f). Thus,
</p>
<p>PX,y( - 1) = PX,y(f)
</p>
<p>(see also (19.15) which is valid for a hermitian function).
</p>
<p>Proof:
</p>
<p>(19.21)
</p>
<p>00
</p>
<p>L rx,y[k]exp(-j21rfk)
k=-oo
</p>
<p>00
</p>
<p>Px ,y(-1) = L rx,y[k]exp(j21rfk)
k=-oo
</p>
<p>00 00
</p>
<p>L rx,y[k] cos(21rfk) + j L rx,y[k] sin(21rfk)
~-oo ~-oo
</p>
<p>C~ rx,y[k] cos(2~ f k) - j k~OO rx,y [k] sin(2~ f k)) &bull;
</p>
<p>= C~oorx,vlk]exP(-jhfk))'
PX,y(f)
</p>
<p>o</p>
<p/>
</div>
<div class="page"><p/>
<p>19.4. THE CROSS-POWER SPECTRAL DENSITY
</p>
<p>Property 19.7 - CPSD is bounded.
</p>
<p>IPx,y(f)! :::; y'Px(f)Py(f)
</p>
<p>Proof: See argument leading to (19.20).
</p>
<p>651
</p>
<p>(19.22)
</p>
<p>o
</p>
<p>Property 19.8 - CPSD is zero for zero mean uncorrelated random pro-
</p>
<p>cesses.
</p>
<p>If X[n] and Y[n] are jointly WSS random processes that are zero mean and uncor-
related, then PX,y(f) = 0 for all j.
</p>
<p>Proof: Since the random processes are zero mean and uncorrelated, rx,y[k] = 0 by
definition. Hence, the CPSD is zero as well, being the Fourier transform of the CCS.
</p>
<p>o
</p>
<p>Property 19.9 - CPSD of (Y[n], X[n]) is the complex conjugate of the
CPSD of (X[n] ,Y[n]).
</p>
<p>Proof:
</p>
<p>pY,x(f)
</p>
<p>pY,x(f) = PX,y(f)
</p>
<p>00
</p>
<p>L rY,x[k] exp( -j27l"jk)
k=-oo
</p>
<p>00
</p>
<p>L rx,y[-k] exp(-j27l"jk)
k=-oo
</p>
<p>00
</p>
<p>L rx,y[1]exp(j27l"jl)
1=-00
</p>
<p>= PX,y(-j)
</p>
<p>PX,y(f)
</p>
<p>(using (19.11))
</p>
<p>(let 1= -k)
</p>
<p>(using (19.21))
</p>
<p>(19.23)
</p>
<p>o
We conclude this section with one more example.
</p>
<p>Example 19.5 - MA Random Process
</p>
<p>Let Y[n] = X[n] - bX[n -1], where X[n] is white Gaussian noise with variance (Tk.
We wish to determine the CPSD between the input X[n] and output Y[n] random
processes (assuming they are jointly WSS, which will be borne out shortly). (Are
</p>
<p>X[n] and Y[n] individually WSS?) To do so we first find the CCS and then take the</p>
<p/>
</div>
<div class="page"><p/>
<p>652 CHAPTER 19. MULTIPLE WSS RANDOM PROCESSES
</p>
<p>Fourier transform of it. Proceeding we have
</p>
<p>rx ,y[k] E[X[n]Y [n + k]]
E[X[n](X[n + k] - bX[n + k - 1])]
</p>
<p>= E [X[n ]X[n + k]] - bE[X[n]X [n + k - 1]]
rx[k] - brx[k - 1]
</p>
<p>which does not depend on n and hence X[n] and Y[n] are jointly WSS with the
</p>
<p>CCS
</p>
<p>rx,y[k] = aJe8[k] - baJe8[k - 1].
</p>
<p>The CPSD is found as the Fourier transform to yield
</p>
<p>PX,y(J) aJe - baJe exp( - j21rf)
</p>
<p>aJe(1- bexp(-j21rf)).
</p>
<p>&lt;I
Note that in the previous example we can view Y[n] as the output of an LSI filter
</p>
<p>with frequency response H (J) = 1 - bexp( - j21rf). Therefore, we have the result
</p>
<p>Px,Y(f) = H(J)aJe. (19.24)
</p>
<p>More generally, we will prove in the next section that if X[n] is the input to an LSI
</p>
<p>system with Y [n] as its corresponding output, then X[n] and Y[n] are jointly WSS
</p>
<p>and PX,y(J) = H(J)Px(J). As an application note, if the input to the LSI system is
white noise with aJe = 1, then PX,y(J) = H(J). To measure the frequency response
of an unknown LSI syst em one can input white noise with a variance equal to one
</p>
<p>and then estimate the CCS from the input and observed output (see Section 19.7).
</p>
<p>Up on Fourier transforming that est imate one obtains an estimate of the frequency
</p>
<p>response. Lastly, since PX,y(J) = H(J) for Px(J) = 1, it is clear that the properties
of the CPSD should mirror those of a frequency response, i.e., complex in general,
</p>
<p>hermitian, etc.
</p>
<p>19.5 Transformations of Multiple Random Processes
</p>
<p>We now consider the effect of some transformations on jointly WSS random pro-
</p>
<p>cesses. As a simple first example, we add the two random processes together.
</p>
<p>Hence, assume X[n] and Y[n] are jointly WSS random processes, and Z[n] =
</p>
<p>X[n] + Y[n]. We next compute the first two moments. Clearly, we will have
flz[n] = fl x[n] + fl y[n] = tix + fly and
</p>
<p>r z [k] = E[Z[n]Z[n + k]]
E[(X[n] + Y[n]) (X[n + k] +Y[n + k])]
</p>
<p>= rx[k] + rx,y [k] + rY,x [k] + ry[k] (assumed jointly WSS)</p>
<p/>
</div>
<div class="page"><p/>
<p>19.5. TRANSFORMATIONS OF MULTIPLE RANDOM PROCESSES 653
</p>
<p>and hence Z[n] is a WSS random process. Its PSD is found by taking the Fourier
transform of the ACS to yield
</p>
<p>Pz(f) = Px(f) + PX,y(f) + PY,x(f) + Py(f).
</p>
<p>If in particular X[n] and Y[n] are zero mean and uncorrelated, so that rx,y[k] = 0
and hence rY,x[k] = rx,y[-k] = 0 as well, we have
</p>
<p>rz[k]
</p>
<p>Px(f)
</p>
<p>rx[k] + ry[k]
Px(f) + Py(f) .
</p>
<p>(19.25)
</p>
<p>(19.26)
</p>
<p>Another frequently encountered transformation is that due to filtering of a WSS
</p>
<p>random process by one or two LSI filters. These transformations are shown in Figure
</p>
<p>19.2. For the transformation shown in Figure 19.2a we already know from Chapter
</p>
<p>x [ n ] - ~ &middot; I H(f) ~ Yin]
</p>
<p>(a)
</p>
<p>Urn]
</p>
<p>HI(J)
</p>
<p>-
</p>
<p>H 2(f)
</p>
<p>(b)
</p>
<p>X[n]
</p>
<p>Y[n]
</p>
<p>Figure 19.2: Common filtering operations.
</p>
<p>18 that if X[n] is WSS, then Y[n] is also WSS and its mean and ACS are easily
</p>
<p>found. The question arises, however, as to whether X[n] and Y[n] are jointly WSS.
</p>
<p>To answer this we compute E[X[n]Y[n + k]] to see if it depends on n. Proceeding,
we have for the filtering operation shown in Figure 19.2a with h[k] denoting the
</p>
<p>impulse response
</p>
<p>E[X[n]Y[n + k]] E [xln] ,t;oo hll]Xln + k -IJ]
00
</p>
<p>L h[l]E[X[n]X[n + k -l]]
1=-00
</p>
<p>00
</p>
<p>L h[l]rx[k -l]
1=-00
</p>
<p>and we see that it does not depend on n . Hence, if X[n] is the input to an LSI
</p>
<p>system and Y[n] is its corresponding output, then X[n] and Y[n] are jointly WSS.</p>
<p/>
</div>
<div class="page"><p/>
<p>654 CHAPTER 19. MULTIPLE WSS RANDOM PROCESSES
</p>
<p>Also , we have for the ees
</p>
<p>00
</p>
<p>rx,y[k] = L h[l]rx[k -I]
1=-00
</p>
<p>(19.27)
</p>
<p>which can be seen to be a discrete convolution or rx,y[k] = h[k]*rx[k]. As a result,
by Fourier transforming the ees we obtain the epSD as
</p>
<p>PX,y(f) = H(f)Px(f) (19.28)
</p>
<p>which agrees with our earlier result of (19.24). As previously asserted, we can also
</p>
<p>now prove that if X[n] is a WSS random process that is input to an LSI system and
Y[n] is the output random process, then the coherence magnitude is one. This says
</p>
<p>that Y[n] is perfectly predictable from X[n], which upon reflection just says that to
predict Y[n] we need only pass X[n] through the same filter! To verify the assertion
about the coherence magnitude
</p>
<p>(using (19.28) and (18.11))
</p>
<p>Px,y(j)
</p>
<p>Jpx(f)Py(f)
</p>
<p>H(f)Px(f)
</p>
<p>J Px(f)IH(f)12Px(f)
</p>
<p>H(f)
</p>
<p>IH(f)1
</p>
<p>= exp(j&cent;(f))
</p>
<p>=
</p>
<p>,X,y(f) =
</p>
<p>where &cent;(f) is the phase response of the LSI system or &cent;(f) = LH(f) Thus,
</p>
<p>I,x,y(f)! = 1 (assuming H(f) f:. 0) and Y[n] is perfectly predictable from X[n]
as
</p>
<p>00
</p>
<p>Y[n] = L h[k]X[n - k]
k=-oo
</p>
<p>for all n
</p>
<p>where h[k] is the impulse response of H(f). Also , X[n] can be perfectly predicted
</p>
<p>from Y[n] as one might expect from the analogous result of the symmetry of the
</p>
<p>correlation coefficient, which is PX,y = PY,X (see Problem 19.21).
</p>
<p>Next consider the transformation depicted in Figure 19.2b. The input random
</p>
<p>process Urn] is WSS so that X[n] and Y[n] are individually WSS according to Theo-
</p>
<p>rem 18.3.1. To determine if they are jointly WSS we again compute E[X[n]Y[n +k]]
to see if it depends on n. Therefore, with hI [k], h2 [k] denoting the impulse responses,</p>
<p/>
</div>
<div class="page"><p/>
<p>19.5. TRANSFORMATIONS OF MULTIPLE RANDOM PROCESSES 655
</p>
<p>and H1(J) , H2(J) denoting the corresponding frequency responses
</p>
<p>E[X[n]Y [n+ kIJ ~ E [,t=h,[i]U[n - i]j'f;= h2[i]U[n+ k - j]]
00 00
</p>
<p>= L L hI[i]h2[j]E[U[n - i]U[n + k - j]]
i= -ooj= -oo
</p>
<p>00 00
</p>
<p>= L L hI[i]h2[j]ru[k + i - j]
i= - oo j = - oo
</p>
<p>and does not depend on n. Hence, X[n] and Y[n] are jointly WSS and the CCS is
</p>
<p>00 00
</p>
<p>rx,y[k] = L h1[i] L h2[j]ru[k + i - j]
i=-oo j=-oo,
</p>
<p>V'
</p>
<p>g[k+i]
</p>
<p>where g[n] = h2[n] * ru[n]. Continuing we have
00
</p>
<p>rx ,y[k] = L hI [i]g[k + i]
i= - oo
</p>
<p>00
</p>
<p>L h1[-l]g[k -l]
1= - 00
</p>
<p>so that
</p>
<p>(let 1= -i)
</p>
<p>rx,y[k] = hI[-k] * h2[k] *ru[k] (19.29)
(this should be reminscent of another relationship that results if h1[k] = h2[k] =
h[k]). Upon Fourier transforming both sides we have the CPSD
</p>
<p>PX,y(J) = H;(J)H2(J)PU(J). (19.30)
</p>
<p>An int eresting observation from (19.30) is that if the two filters have nonoverlapping
</p>
<p>passbands, as shown in Figure 19.3, then
</p>
<p>-1&lt;1&lt;1
2 - - 2
</p>
<p>and PX,y(J) = O. Taking the inverse Fourier transform of the CPSD produces the
</p>
<p>CCS , which is rx,y[k] = 0 for all k. Hence, for nonoverlapping passband filters as
shown in Figure 19.3 the X[n] and Y [n] random processes are uncorrelated. (Note
that because of the nonoverlapping passbands we must have J.Lx = 0 or J.Ly = 0.)
Since this holds for any filters satisfying the nonoverlapping constraint , it also holds</p>
<p/>
</div>
<div class="page"><p/>
<p>656 CHAPTER 19. MULTIPLE WSS RANDOM PROCESSES
</p>
<p>Figure 19.3: Nonoverlapping passband filters.
</p>
<p>in particular for any narrowband filters with nonoverlapping passbands. What this
</p>
<p>says is that the the Fourier transform of a WSS random process Urn] is uncorre-
</p>
<p>lated at two different frequencies. (Actually, it is the truncated Fourier transform
</p>
<p>or U2M+l(f) = (I/J2M + 1) L~-M Urn] exp(-j2nJn) , which is required for ex-
istence, that is uncorrelated at different frequencies as M -+ 00.) This is because
the Fourier transform can be thought of as resulting from filtering the random pro-
</p>
<p>cess with a narrowband filter and then determining the amplitude and phase of the
</p>
<p>resulting sinusoidal output. The spectral representation of a WSS random process
</p>
<p>is based upon this interpretation (see [Brockwell and Davis 1987] and also Problem
</p>
<p>19.22).
</p>
<p>L1h
wss.
</p>
<p>Two random processes can be individually WSS but not jointly
</p>
<p>All the examples thus far of individually WSS random processes have also resulted
</p>
<p>in jointly WSS random processes. To dispel the notion that this is true in general
</p>
<p>consider the following example. Let X[n] = A and Y[n] = (_1)n A, where A is a
</p>
<p>random variable with E[A] = 0 and var(A) = 1. Then, I-lx[n] = I-ly[n] = 0 and it is
easily shown that rx[k] = 1 for all k and ry[k] = (-I)k for all k. Therefore, X[n]
and Y[n] are individually WSS random processes but they are not jointly WSS since
</p>
<p>which depends on n. For example, since X[O] = Y[2] = A and X[I] = -Y[3] = A,
we have that
</p>
<p>E[X[O]Y[2]]
</p>
<p>E[X[I]Y[3]]
</p>
<p>E[A2 ] = 1
</p>
<p>E[A(-A)] =-1
</p>
<p>so that the cross-correlation between two samples spaced two units apart depends
</p>
<p>on n.</p>
<p/>
</div>
<div class="page"><p/>
<p>19.6. CONTINUOUS-TIME DEFINITIONS AND FORMULAS 657
</p>
<p>19.6 Continuous-Time Definitions and Formulas
</p>
<p>Two continuous-time random processes X(t) and Y (t) for &minus;&infin; &lt; t &lt; &infin; are jointly
WSS if X(t) is WSS, Y (t) is WSS, and we can define the cross-correlation function
(CCF) as
</p>
<p>rX,Y () = E[X(t)Y (t+ )] &minus;&infin; &lt;  &lt; &infin; (19.31)
which does not depend on t. Some properties (actually nonproperties) of the CCF
are
</p>
<p>Property 19.10 &ndash; CCF is not necessarily symmetric about  = 0.
</p>
<p>rX,Y () 	= rX,Y (&minus;) (19.32)

</p>
<p>Property 19.11 &ndash; The maximum of the CCF can occur for any value
of  .
</p>
<p>
</p>
<p>Property 19.12 &ndash; The maximum value of the CCF is bounded.
</p>
<p>|rX,Y ()| &le;
&radic;
</p>
<p>rX(0)rY (0) (19.33)
</p>
<p>
</p>
<p>Property 19.13 &ndash; Interchanging X(t) and Y (t) flips the CCF about  = 0.
</p>
<p>rY,X() = rX,Y (&minus;) (19.34)

</p>
<p>Two zero mean jointly WSS continuous random processes are said to be uncorrelated
if rX,Y () = 0 for &minus;&infin; &lt;  &lt; &infin;.
</p>
<p>The CPSD for two jointly WSS random processes is defined as
</p>
<p>PX,Y (F ) = lim
T&rarr;&infin;
</p>
<p>1
</p>
<p>T
E
</p>
<p>[(
</p>
<p>&int; T/2
</p>
<p>&minus;T/2
X(t) exp(&minus;j2Ft)dt
</p>
<p>)&lowast; (
&int; T/2
</p>
<p>&minus;T/2
Y (t) exp(&minus;j2Ft)dt
</p>
<p>)]
</p>
<p>(19.35)
and is evaluated as
</p>
<p>PX,Y (F ) =
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
rX,Y () exp(&minus;j2F)d. (19.36)
</p>
<p>Some properties of the CPSD follow. The proofs are similar to those for the discrete-
time case.</p>
<p/>
</div>
<div class="page"><p/>
<p>658 CHAPTER 19. MULTIPLE WSS RANDOM PROCESSES
</p>
<p>Property 19.14 &ndash; CPSD is a complex and hermitian function.
</p>
<p>The hermitian property is
</p>
<p>PX,Y (&minus;F ) = P &lowast;X,Y (F ) (19.37)
</p>
<p>
</p>
<p>Property 19.15 &ndash; CPSD is bounded.
</p>
<p>|PX,Y (F )| &le;
&radic;
</p>
<p>PX(F )PY (F ) (19.38)
</p>
<p>
</p>
<p>Property 19.16 &ndash; CPSD of (Y (t),X(t)) is the complex conjugate of the
CPSD of (X(t), Y (t)).
</p>
<p>PY,X(F ) = P
&lowast;
X,Y (F ) (19.39)
</p>
<p>
</p>
<p>The formulas for the linear system configuration corresponding to that shown in Fig-
ure 19.2a are (continuous-time system is assumed to be LTI with impulse response
h() and frequency response H(f))
</p>
<p>rX,Y () = h()  rX() (19.40)
</p>
<p>PX,Y (F ) = H(F )PX (F ) (19.41)
</p>
<p>and for the configuration of Figure 19.2b (continuous-time systems are assumed to
be LTI with impulse responses h1(), h2(), and corresponding frequency responses
H1(f), H2(f))
</p>
<p>rX,Y () = h1(&minus;)  h2()  rU () (19.42)
PX,Y (F ) = H
</p>
<p>&lowast;
1 (F )H2(F )PU (F ). (19.43)
</p>
<p>An example of great practical importance is given next to illustrate the concepts
and formulas.
</p>
<p>Example 19.6 &ndash; Measurement of Channel Delay
</p>
<p>It is frequently of interest to be able to measure the propagation time of a signal
through a channel. This allows one to determine distance if the speed of propaga-
tion is known. This idea forms the basis for the global positioning system (GPS)
[Hofmann-Wellenhof, Lichtenegger, Collins 1992]. See also Problem 19.28 for an-
other application. To do so we transmit a WSS random process X(t), that is ban-
dlimited to W Hz (meaning that PX(F ) = 0 for |F | &gt; W ) through a channel and</p>
<p/>
</div>
<div class="page"><p/>
<p>19.6. CONTINUOUS-TIME DEFINITIONS AND FORMULAS 659
</p>
<p>observe the output of the channel Y(t). We furthermore assume that the channel is
</p>
<p>modeled as an LTI system with frequency response
</p>
<p>H(F) = exp(-j21TP to)
1+j27fF .
</p>
<p>(19.44)
</p>
<p>Note that the numerator term represents a delay of to seconds, sometimes called the
</p>
<p>propagation or bulk delay, and the term HLP (F) = 1/(1 + j27fF) represents a low-
</p>
<p>pass filter response since HLP (0) = 1 and HLP (F) -+ 0 as F -+ 00. A question arises
as to how to choose the transmit random process X (t) so that we can accurately
</p>
<p>measure the delay to through the channel. In the ideal case in which Y(t) is just a
</p>
<p>delayed replica of X(t) or Y(t) = X(t - to), we know that the CCF is
</p>
<p>E[X(t)Y(t + r)]
E[X(t)X(t + r - to)]
rx(r - to).
</p>
<p>Since the ACF has a maximum at lag zero, there will be maximum of rx,y(r)
</p>
<p>at r = to, suggesting that the location of this maximum can be used to measure
</p>
<p>the delay. But when the channel has the frequency response given by (19.44) the
</p>
<p>maximum of the CCF may no longer be located at r = to. To see why, first compute
</p>
<p>the CCF as
</p>
<p>i: Px,y(F) exp(j27fFr)dF
i: H(F)Px(F) exp(j27fFr)dF
1
</p>
<p>00 exp(-j27fF to)
1 '2 F Px(F) exp(j27fFr)dF
</p>
<p>-00 + J 7f
</p>
<p>1
00 1
</p>
<p>1 '2 FPx(F) exp(j27fF(r - to))dF
-00 + J 7f
</p>
<p>(inverse Fourier transform)
</p>
<p>(from (19.41))
</p>
<p>(from (19.44))
</p>
<p>and since X(t) is assumed to be bandlimited to W Hz, we have
</p>
<p>l
w 1
</p>
<p>rx,y(r) = 1 '2 FPx(F) exp(j27fF(r - to))dF.
-w + J 7f
</p>
<p>(19.45)
</p>
<p>If, as an example, we choose X(t) to be bandlimited white noise (see Example 17.11)
</p>
<p>or Px(F) = No/2 for IFI ~ Wand Px(F) = 0 for IFI &gt; W, then
</p>
<p>N, l w 1rx,y(r) = -f -w 1 + j27fF exp(j27fF(r - to))dF.</p>
<p/>
</div>
<div class="page"><p/>
<p>660 CHAPTER 19. MULTIPLE WSS RANDOM PROCESSES
</p>
<p>To evaluate this we first note that
</p>
<p>{ I } 1
00 1 .;:-1 . = . F exp(J21rFt)dF = exp( -t)u(t)
</p>
<p>1 + J21rF -00 1 + J21r
</p>
<p>so that if we define the frequency window function
</p>
<p>G(F) = {I IFI:::; W
o IFI &gt; W
</p>
<p>(convolution in time yields
</p>
<p>multiplication in frequency).
</p>
<p>then
</p>
<p>No100 G(F) : exp(j21rF(T - to))dF
2 -00 1 + J21rF
</p>
<p>No
= 2 g(t) *exp(-t)u(t)lt=T_to
</p>
<p>where g(t) is the inverse Fourier transform of G(F). We have chosen to express
</p>
<p>the integral in the time domain since its physical significance becomes clearer. In
</p>
<p>particular, note that the convolution in time results in a wider pulse. But
</p>
<p>(t ) = 2W
sin(21rWt)
</p>
<p>9 21rWt
(see Example 17.11)
</p>
<p>so that using a convolution integral, we have
</p>
<p>This is shown in Figure 19.4 for the case when W = 1 and to = 2 as the light line
</p>
<p>and has been normalized to have a maximum value of 1. The integral has been
</p>
<p>evaluated numerically. Note that the maximum does not occur at to = 2 because
</p>
<p>the phase response of the channel has added a time delay. To remedy this problem
</p>
<p>we can insert an equalizing filter at the channel output whose frequency response is
</p>
<p>H (F) = { 1 + j21rF IFI:::; W
eq 0 IFI &gt; W.
</p>
<p>Then, we have for the CPSD between the input X(t) and output random process of
the equalizer Y (t)
</p>
<p>P (F) = H (F)H(F)P (F) = { J&yen;o- exp(-j21rF to) IFI:::; W
X,Y eq X 0 IFI &gt; W.
</p>
<p>The CCF is found as before by using the inverse Fourier transform</p>
<p/>
</div>
<div class="page"><p/>
<p>19.7. CROSS-CORRELATION SEQUENCE ESTIMATION 661
</p>
<p>0.8 .
I
</p>
<p>. . I
</p>
<p>~ 0.6
</p>
<p>~
&gt;..
~ 0.4
r,...
</p>
<p>0.2 .
</p>
<p>-0.2 .
</p>
<p>105o 2
T
</p>
<p>-5
-0.4 '-------'---------'---'---'----------'
</p>
<p>-10
</p>
<p>Figure 19.4: Cross-correlation functions for W = 1 and to = 2. Both curves are
normalized to yield one at their peak. The light line is for no equalization while the
</p>
<p>dark line incorporates equalization. The dashed line indicates T = 2, the true delay.
</p>
<p>j
w N,
</p>
<p>~ exp(j2n-F(T - to))dF
-w 2
</p>
<p>N, W sin(27fW(T - to))
o 27fW(T - to)
</p>
<p>which is shown in Figure 19.4 as the dark line. As before it has been normalized to
</p>
<p>yield one at its peak. Note that the maximum now occurs at the correct location
</p>
<p>and also the width of the maximum peak is narrower. This allows a better location
</p>
<p>of the maximum in the presence of noise.
</p>
<p>19.7 Cross-correlation Sequence Estimation
</p>
<p>The estimation of the CCS is similar to that for the ACS (see Section 17.7). The
</p>
<p>main difference between the two stems from the fact that the ACS is guaranteed to
</p>
<p>have a maximum at k = 0 while the maximum for the CCS can be located anywhere.
Furthermore, two samples of a WSS random process tend to become less correlated
</p>
<p>as the spacing between them increases. This implies that it is only necessary to
</p>
<p>estimate the ACS r x [k] for k = 0,1, ... ,M if we assume that r x [k] ~ 0 for k &gt; M.
For the CCS , however, we must estimate rx,Y[k] for k = -MI , ... , 0, ... , M 2 (recall
</p>
<p>that rX'y[-k] =1= rx,y[k]) for which rx,Y[k] ~ 0 if k &lt; -MI or k &gt; M 2&bull; In
practice, it is not clear how M I and M 2 should be chosen. Frequently, a preliminary
</p>
<p>est imate of rx,y[k] is made, followed by a search for the maximum location. Then,
</p>
<p>the data records used to estimate the CCS are shifted relative to each other to
</p>
<p>place the maximum at k = O. This is called time alignment [Jenkins and Watts</p>
<p/>
</div>
<div class="page"><p/>
<p>&middot; 662 CHAPTER 19. MULTIPLE WSS RANDOM PROCESSES
</p>
<p>(19.46)
</p>
<p>1968]. We assume that this has already been done. Then , we est imate the CCS for
</p>
<p>Ikl ~ M assuming that we have observed the realizations for X[n] and Y [n], both
for n = 0,1, .. . ,N - 1. The est imate becomes
</p>
<p>A { N ~k Lt: ';Ol-k x[n]y[n + k] k = 0,1 , . . . , M
rx,y[k] = 1 N -l
</p>
<p>N- Ikl Ltn=lklx[n]y[n + k] k = - M ,-(M - 1), . . . , - 1.
</p>
<p>rx,Y[O]
</p>
<p>rX,y[ I ]
</p>
<p>rx,y[- I ]
</p>
<p>rx ,y [- 2]
</p>
<p>rX,y[2] =
</p>
<p>Note that the summation limi ts have been chosen to make sure that all the available
</p>
<p>products x[n]y[n + k] are used. Similar to the est imation of the ACS, there will
be a different number of products for each k . For example, if N = 4 so that
</p>
<p>{x [0], x[I] ,x [2],x[3]} and {y[O], y[l ]' y[2], y[3]} are observed, and we wish to compute
the CCS est imate for Ikl ~ M = 2, we will have
</p>
<p>1 3 1
2L x[n]y[n - 2] = 2(x[2]y[0] + x[3]y[l ])
</p>
<p>n=2
</p>
<p>1 3 1
3L x[n]y[n - 1] = 3(x[l ]y[0] + x[2]y[l ] + x[3]y[2])
</p>
<p>n =l
</p>
<p>1 3 1
4L x[n]y[n] = 4(x[0]y[0] + x[l ]y[l ] + x[2]y[2] + x[3]y[3])
</p>
<p>n=O
</p>
<p>1 2 1
3L x[n]y[n + 1] = 3(x[0] y[l] + x[l ]y[2] + x[2]y[3])
</p>
<p>n=O
</p>
<p>1 1 1
"2 L x[n]y[n + 2] = "2 (x[0]y[2] + x[l ]y[3]).
</p>
<p>n=O
</p>
<p>As an example, consider the jointly WSS random processes described in Example
</p>
<p>19.2, where X[ n] = Urn], Y [n] = Urn] + 2U[n - 1] and Urn] is white noise with
vari ance (j ~ = 1. We fur ther assume that Urn] has a Gaussian PDF for each n
for the purpose of computer simulation (although we could use any PDF or PMF).
</p>
<p>Recall that the theoretical CCS is rx,y[k] = 8[k] + 28[k - 1]. The estimated CCS
using N = 1000 data samples is shown in Figure 19.5. The MATLAB code used
</p>
<p>to est imate the CCS is given below.
</p>
<p>%assume realizations are x[n] , yEn] for n=1,2, ... ,N
for k=O:M %compute zero and positive lags, see (19.46)
% compute values for k=O,1, ... ,M
</p>
<p>rxypos(k+1,1)=(1!(N-k))*sum(x(1 :N-k).*y(1+k:N));
</p>
<p>end
</p>
<p>for k=1:M %compute negative lags, see (19 .46)
%compute values for k=-M,-(M-1), ... ,-1
</p>
<p>rxyneg(k+1,1)=(1!(N-k))*sum(x(k+1:N) .*y(1:N-k));
</p>
<p>end
</p>
<p>rxy=[flipud(rxyneg(2:M+1,1));rxypos]; %arrange values from k=-M to k=M</p>
<p/>
</div>
<div class="page"><p/>
<p>19.8. REAL-WORLD EXAMPLE - BRAIN PHYSIOLOGY RESEARCH 663
</p>
<p>1.5
</p>
<p>~
;:... , .
</p>
<p>&gt;&lt;
&lt; ~
</p>
<p>. . .0.5 "' 0&middot; &middot; &middot; &middot;&middot;&middot; &middot;&middot; &middot; &middot; &middot;&middot; .
</p>
<p>0 . . . . i' ..... . . ~ &bull;&bull; . . . .. .&bull; . . ... &bull;.... ,.
-0.5
</p>
<p>-5 -4 -3 -2 -1 0 2 3 4 5
k
</p>
<p>Figure 19.5: Estimated CCS using realizations for X[n] and Y[n] with N = 1000
samples. The theoretical CCS is rx,y[k] = 8[k] + 28[k - 1].
</p>
<p>Finally, we note that estimation of the CPSD is more difficult and so we refer the
</p>
<p>interested reader to [Jenkins and Watts 1968, Kay 1988].
</p>
<p>19.8 Real-World Example - Brain Physiology Research
</p>
<p>Understanding the operation of the human brain is one of the most important goals
</p>
<p>of physiological research. Currently, there is an enormous effort to decipher its
</p>
<p>inner workings. At a very fundamental level is the study of its cells or neurons,
</p>
<p>which when working in unison form the basis for our behavior. Their electrical
</p>
<p>activity and the transmission of that activity to neighboring neurons yields clues
</p>
<p>as to the brain's operation. When an individual neuron "fires" it produces a spike
</p>
<p>or electrical pulse that propagates to nearby neurons. The connections between
</p>
<p>the neurons that allow this propagation to occur are called synapses and it is this
</p>
<p>connectivity that is the focus of much research. A typical spike train that might
</p>
<p>be recorded is shown in Figure 19.6a for a neuron at rest and in Figure 19.6b for a
</p>
<p>neuron that has been excited by some stimulus. Clearly, the firing rate increases in
</p>
<p>response to a stimulus. The model used to produce this figure is an IID Bernoulli
</p>
<p>random process with p = Pq = 0.1 for Figure 19.6a and p = Ps = 0.6 for Figure
19.6b. The subscripts "q" and "s" are meant to indicate the state of the neuron,
</p>
<p>eit her quiescent or stimulated. Now consider the question of whether two neurons
</p>
<p>are connected via a synapse. If they are , and a stimulus is applied to the first
</p>
<p>neuron, then the electrical pulse will propagate to the second neuron and appear
</p>
<p>some time later. Then, we would expect the second neuron electrical activity to
</p>
<p>change from that in Figure 19.6a to that in Figure 19.6b. It would be fairly simple</p>
<p/>
</div>
<div class="page"><p/>
<p>664 CHAPTER 19. MULTIPLE WSS RANDOM PROCESSES
</p>
<p>1 . 2 , . . . - - - ~ - ~ - ~ - - " - - - ~ - - - , 1.2 r----.---~-~--~---.-----,
</p>
<p>1 "
</p>
<p>0.8 .
</p>
<p>~ 0.6.,
0.8
</p>
<p>~ 0.6.,
0.4 . . 0.4
</p>
<p>0.2 .
</p>
<p>o &bull;&bull;.&bull;&bull;&bull;&bull;.&bull;&bull;&bull;&bull; .&bull; &bull; &bull; .. .&bull;&bull;&bull;&bull;&bull;.&bull;&bull;&bull;&bull;&bull;&bull;&bull;
</p>
<p>0.2
</p>
<p>o . .... .&bull;. ... ..........&bull;...
30252015
</p>
<p>n
105
</p>
<p>-0.2 ' - - - ~ - - ' - - ~ - - - ' - - - - - ~ - - - - '
o30252015
</p>
<p>n
105
</p>
<p>-0.2 l - . . _ - - ' - - _ - - - ' - _ ~ __-'----_--'--_-'
o
</p>
<p>(a) Quiescent, pq = 0.1 (b) Stimulated, P&raquo; = 0.6
</p>
<p>Figure 19.6: Typical spike trains for neurons.
</p>
<p>then to estimate the p for each possible connected neuron and choose the neuron or
</p>
<p>neurons (there may be multiple connections with the stimulated neuron) for which
</p>
<p>p is large. Unfortunately, it is not easy to stimulate a single neuron so that when
</p>
<p>a st imulus is applied, many neurons may be activated. Thus, we need a method
</p>
<p>to associate one stimulated neuron with its connected ones. Ideally, if we record
</p>
<p>the elect rical activity at two neurons under considerat ion, denoted by Xdn] and
</p>
<p>X2[n], then for connected neurons X2[n] = X1[n - no]. Since we have assumed that
the spike train for the first neuron X1[n] is an IID random process, it is therefore
</p>
<p>WSS and we know from Example 19.1, the two random processes are jointly WSS.
</p>
<p>Therefore, we have as before
</p>
<p>rX l ,X2[k] E[Xdn]X2[n + k]]
E[Xdn]Xdn - no + k]]
</p>
<p>= rXl[k-no]
</p>
<p>and therefore the CCS will exhibit a maximum at k = no. Otherwise, if the neu-
</p>
<p>rons ar e not connected, we would expect a much smaller value of the maximum or
</p>
<p>no discernible maximum at all. For example, for unconnected but simultaneously
</p>
<p>stimulated neurons it is reasonable to assume that X1[n] and X2[n] are uncorrelated
</p>
<p>and hence rXl ,X2[k] = E[Xdn]]E[X2[n + k]] = p; ,which presumably will be less
than rXl [k - no] at its peak. Note that for connected neurons
</p>
<p>if the covariance is positive.
</p>
<p>Specifically, we assume that a neuron output is modeled as an IID Bernoulli
</p>
<p>random process that takes on the values 1 and 0 with probabilities Ps and 1 - Ps,</p>
<p/>
</div>
<div class="page"><p/>
<p>19.8. REAL-WORLD EXAMPLE - BRAIN PHYSIOLOGY RESEARCH 665
</p>
<p>resp ectively. For two neurons that are connected we have that rXl ,X2[k] = rXl[k-
no] . But
</p>
<p>rXl[k] E[Xdn]Xdn + k]]
</p>
<p>{
E[Xf[n]]
E [X1[n]]E[X1 [n + k]]
</p>
<p>= {ps k = 0
p ~ k:l: 0
</p>
<p>= Ps(l - Ps)&lt;5[k] + p; .
</p>
<p>k=O
k:l:0
</p>
<p>Hence, for two connected neurons the CCS is
</p>
<p>For two neurons that are not connected, so that their outputs are uncorrelated (even
</p>
<p>if both are stimulated) , the CCS is
</p>
<p>E[Xdn]]X2[n + k]]
E[Xdn]]E[X2[n + k]]
P; for all k.
</p>
<p>As a result , t he maximum is p ~ for unconnected neurons but Ps(1-ps)+P; = Ps &gt; P;
for connected neurons. The two different cess are shown in Figure 19.7 for Ps = 0.6
</p>
<p>and no = 2. As an example, for Ps = 0.6 we show realizations of three neuron
</p>
<p>2 34
</p>
<p>,
</p>
<p>o
-4 -3 -2 -1 0
</p>
<p>k
</p>
<p>0.4
</p>
<p>0.2
</p>
<p>~ 0. 8
</p>
<p>&gt;&lt;:
~ 0 .6
...
</p>
<p>2 3 4
</p>
<p>:
</p>
<p>, ,
</p>
<p>:
</p>
<p>o
-4 -3 - 2 -1 0
</p>
<p>k
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>~ 0 . 8
</p>
<p>&gt;&lt;:
~ 0. 6
...
</p>
<p>(a) Unconnected (b) Connected with no = 2
</p>
<p>Figure 19.7: CCS for unconnected and connected stimulated neurons with Ps = 0.6.
</p>
<p>outputs in Figure 19.8, where only neuron 1 and neuron 3 are connected. There is</p>
<p/>
</div>
<div class="page"><p/>
<p>666 CHAPTER 19. MULTIPLE WSS RANDOM PROCESSES
</p>
<p>1 .
</p>
<p>o &bull; &bull; ' . .&bull; .. &bull;&bull; - ~ &bull;&bull;&bull;
</p>
<p>&gt;:: 0.8 ..
</p>
<p>~ 0.6 "
</p>
<p>0.4 ..
</p>
<p>02 &middot;&middot;
</p>
<p>o . . . &bull; .. . .
</p>
<p>&gt;:: 0.8
</p>
<p>r; 0.6
0.4
</p>
<p>0.2
</p>
<p>.. .. . . . .. . ;
</p>
<p>.., ~ .. . ~ .
</p>
<p>~0 .8 &middot; &middot;
</p>
<p>~ 0.6 " .
</p>
<p>0.4
</p>
<p>02 &middot;' .
</p>
<p>... .; .....
</p>
<p>10 15 20 25 30
</p>
<p>n
10 15 20 25 30
</p>
<p>n
</p>
<p>(a) Neuron 1
</p>
<p>-0.2'----'---'-------'----'----'----'
o
</p>
<p>(b) Neuro n 2
</p>
<p>10 15 20 25 30
</p>
<p>n
</p>
<p>(c) Neuron 3
</p>
<p>Figure 19.8: Spike trains for three neurons with neuron 1 connected to neuron 3
</p>
<p>with a delay of two samples. The spike train of neuro n 2 is uncorrelated with those
</p>
<p>for neurons 1 and 3.
</p>
<p>a two sample delay between neurons 1 and 3. Neuron 2 is not connected to either
</p>
<p>of the other neurons and hence its spike train is uncorrelated with the others. The
</p>
<p>theoretical CCS between neurons 1 and 2 is given in Figure 19.7a while that between
</p>
<p>neurons 1 and 3 is given in Figure 19.7b. The est imated CCS for the spike trains
</p>
<p>shown in Figure 19.8 and based on the est imate of (19.46) is shown in Figure 19.9.
</p>
<p>2 3 4 5 2 3 4 5
</p>
<p>~ 0.8 .. . . ~ .
</p>
<p>~ 0.6
</p>
<p>,B::.11111FFII
-0.2 L - ~ ~ _ ~ ~ _ ~ ~ ~ _ ~ ~ - - - . J
</p>
<p>-5 -4 -3 -2 -1 0
k
</p>
<p>(a) Unconnected neur ons 1 an d 2
</p>
<p>.::s 0.8 .
</p>
<p>'"~ 0.6 .
</p>
<p>,B:: J[II[L 11
-0.2 L . . - - , - ~ _ ~ ~ _ ~ ~ ~ _ ~ ~ - - - . J
</p>
<p>-5 -4 -3 -2 -1 0
k
</p>
<p>(b) Connected neurons 1 and 3 wit h no = 2
</p>
<p>Figure 19.9: Estimated CCS for unconnected and connected stimulated neurons
</p>
<p>wit h Ps = 0.6.
</p>
<p>It is seen that as expected there is a maximum at k = no = 2. The interested reader
should consult [Univ . Pennsylvannia 2005] for further details.</p>
<p/>
</div>
<div class="page"><p/>
<p>REFERENCES
</p>
<p>References
</p>
<p>667
</p>
<p>Bendat, J.S., A.G . Piersol, Random Data: Analysis and Measurement Procedures,
</p>
<p>Wiley-Interscience, New York, 1971.
</p>
<p>Brockwell, P.J. , R.A. Davis, Time Series: Theory and Methods, Springer-Verlag,
</p>
<p>New York, 1987.
</p>
<p>Hofmann-Wellenhof, H. Lichtenegger, J. Collins, Global Positioning System: The-
</p>
<p>ory and Practice, Springer-Verlag, New York, 1992.
</p>
<p>Jenkins, G.M., D.G. Watts, Spectral Analysis and Its Applications, Holden-Day,
</p>
<p>San Francisco, 1968.
</p>
<p>Kay, S., Modern Spectral Estimation: Theory and Application, Prentice Hall, En-
</p>
<p>glewood Cliffs, NJ, 1988.
</p>
<p>Koopmans, L.H. ,The Spectral Analysis of Time Series, Academic Press, New York,
</p>
<p>1974.
</p>
<p>Robinson, E.A. , Multichannel Time Series Analysis with Digital Computer Pro-
</p>
<p>grams, Holden-Day, San Francisco, 1967.
</p>
<p>University of Pennsylvannia, Multiple Unit Laboratory,
</p>
<p>http://mulab.physiol.upenn.edu/analysis.html
</p>
<p>Problems
</p>
<p>19.1 t.:..:,,) (w) Two discrete-time random processes are defined as X[n] = Urn] and
Y[n] = (-l)nU[n] for -00 &lt; n &lt; 00, where Urn] is white noise with variance
abo Are the random processes X[n] and Y[n] jointly WSS?
</p>
<p>19.2 (w) Two discrete-time random processes are defined as X[n] = aiUi[n] +
a2U2[n] and Y[n] = biUdn] +b2U2[n] for -00 &lt; n &lt; 00, where Ui[n] and U2[n]
are jointly WSS and ai, a2,bi , b: are constants. Are the random processes X[n]
</p>
<p>and Y[n] jointly WSS?
</p>
<p>19.3 (f) If the CCS is given as rx,y[k] = (1/2) lk- il for -00 &lt; k &lt; 00, plot it and
describe which properties are the same or different from an ACS.
</p>
<p>19.4 (f) IfY[n] = X[n] +W[n], where X[n] and W[n] are jointly WSS , find rx,y[k]
and PX,y(J).
</p>
<p>19.5 C:.:..-) (w) A discrete-time random process is defined as Y[n] = X[n]W[n] ,
where X[n] is WSS and W[n] is an lID Bernoulli random process that takes on
</p>
<p>values &plusmn;1 with equal probability. The random processes X[n] and W[n] are</p>
<p/>
</div>
<div class="page"><p/>
<p>668 CHAPTER 19. MULTIPLE WSS RANDOM PROCESSES
</p>
<p>independent of each other, which means that X[ntJ is independent of W[n2]
for all n1 and n2. Find rx,y[k] and explain your results.
</p>
<p>19.6 (...:.:,) (w) In this problem we show that for the AR random process X[n] =
aX[n - 1] + Urn]' which was described in Example 17.5, the cross-correlation
sequence E[X[n]U[n + k]] = 0 for k &gt; O. Do so by evaluating E[X[n](X[n +
k] - aX[n + k -1])]. Determine and plot the CCS rX ,u[k] for -00 &lt; k &lt; 00 if
a = 0.5 and CT&amp; = 1. Hint: Refer back to Example 17.5 for the ACS of an AR
</p>
<p>random process.
</p>
<p>19.7 (f) If X[n] and Y[n] are jointly WSS with ACSs
</p>
<p>(1)lk
l
</p>
<p>rx[k] = 4 2"
</p>
<p>ry[k] = 38[k] + 28[k + 1] + 28[k - 1]
</p>
<p>determine the maximum possible value of rx,y[k].
</p>
<p>19.8 (t) Derive (19.14). To do so use the relationship ~ ~ = - M ~ ~ - M g[m - n] =
</p>
<p>~%~_2M(2M + 1 - Ikl)g[k].
</p>
<p>19.9 (f) For the two sinusoidal random processes X[n] = cos(27rJon + 8d and
Y[n] = cos(21rfon + 82) , where 8 1 = 8 2 '" U(0,21r) find the CPSD and
explain your results versus the case when 8 1 and 8 2 are independent random
variables.
</p>
<p>19.10 (...:..:.-) (f,c) If rX,y[k] = 8[k] + 28[k - 1] , plot the magnitude and phase of the
CPSD. You will need a computer to do this.
</p>
<p>19.11 (f) For the random processes X[n] = Urn] and Y[n] = Urn] - bUrn - 1],
where Urn] is discrete white noise with variance CT&amp; = 1, find the CPSD and
explain what happens as b -t O.
</p>
<p>19.12 (...:..:.-) (w) If a random process is defined as Z[n] = X[n] - Y[n], where X[n]
and Y[n] are jointly WSS, determine the ACS and PSD of Z[n].
</p>
<p>19.13 (w) For the random processes X[n] and Y[n] defined in Problem 19.11 find
the coherence function. Explain what happens as b -t O.
</p>
<p>19.14 (f) Determine the CPSD for two jointly WSS random processes if rx,y[k] =
8[k] - 8[k - 1]. Also, explain why the coherence function at f = 0 is zero.
Hint: The random processes X[n] and Y[n] are those given in Problem 19.11
if b = 1.
</p>
<p>19.15 (...:..:.-) (f) If Y[n] = -X[n] for -00 &lt; n &lt; 00, determine the coherence func-
tion and relate it to the predictability of Y[no] based on observing X[n] for
-00 &lt; n &lt; 00.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS
</p>
<p>19.16 (t) A cross-spectral matrix is defined as
</p>
<p>[
Px(f) PX,y(j)]
</p>
<p>PY,x(f) Py(f) .
</p>
<p>669
</p>
<p>Prove that the cross-spectral matrix is positive semidefinite for all f. Hint:
Show that the principal minors of the matrix are all nonnegative (see Appendix
</p>
<p>C for the definition of principal minors). To do so use the properties of the
</p>
<p>coherence function.
</p>
<p>19.17 (w) The random processes X[n] and Y[n] are zero mean jointly WSS and
are uncorrelated with each other. If rx[k] = 2t5[k] and ry[k] = (1/2)lkl for
-00 &lt; k &lt; 00, find the PSD of X[n] +Y[n].
</p>
<p>19.18 C:...:..) (t) In this problem we derive an extension of the Wiener smoother (see
Section 18.5.1). We consider the problem of estimating Y[no] based on ob-
serving X[n] for -00 &lt; n &lt; 00. To do so we use the linear estimator
</p>
<p>00
</p>
<p>Y[no] = L h[k]X[no - k].
k=-oo
</p>
<p>To find the optimal impulse response we employ the orthogonality principle
</p>
<p>to yield the infinite set of simultaneous linear equations
</p>
<p>E [(Y[nol- ,t;oo h[kIX[no - k])X[no -II] ~ 0 - 00 &lt; 1&lt; 00.
Assuming that X[n] and Y[n] are jointly WSS random processes, determine
the frequency response of the optimal Wiener estimator. Then, show how the
</p>
<p>Wiener smoother, where Y[n] represents the signal S[n] and X[n] represents
the signal S[n] plus noise W[n] (recall that S[n] and W[n] are zero mean and
uncorrelated random processes), arises as a special case of this solution.
</p>
<p>19.19 (f) For the random processes defined in Example 19.2 determine the CPSD.
</p>
<p>Next, find the optimal Wiener smoother for Y[no] based on the realization of
X[n] for -00 &lt; n &lt; 00.
</p>
<p>19.20 (t) Prove that if X[n] is a WSS random process that is input to an LSI system
and Y[n] is the corresponding random process output, then the coherence
function between the input and output has a magnitude of one.
</p>
<p>19.21 (t) Consider a WSS random process X[n] that is input to an LSI system with
frequency response H(f), where H(f) t= 0 for IfI ~ 1/2, and let Y[n] be the
corresponding random process output. It is desired to predict X[no] based on
observing Y[n] for -00 &lt; n &lt; 00. Draw a linear filtering diagram (similar to
that shown in Figure 19.2) to explain why X[no] is perfectly predictable by
passing Y[n] through a filter with frequency response 1/H(f).</p>
<p/>
</div>
<div class="page"><p/>
<p>670 CHAPTER 19. MULTIPLE WSS RANDOM PROCESSES
</p>
<p>19.22 (t) In this problem we argue that a Fourier transform is actually a narrow-
band filtering operation. First consider the Fourier transform at f = fo for
the truncated random process X[n], n = -M, . . . ,0, . . . , M which is X(Jo) =
L:~-M X [k] exp (-j27110k). Next show that this may be written as
</p>
<p>00
</p>
<p>X(Jo) = L X[k]h[n - k]
k=-oo n =O
</p>
<p>where
</p>
<p>h[k] = { exp(j21f fok) k = -M, ... , 0, . .. , M
o Ikl &gt;M.
</p>
<p>Notice that this is a convolution sum so that h[k] can be considered as the
impulse response, although a complex one, of an LSI filter. Finally, find and
</p>
<p>plot the frequency response of this filter. Hint: You will need
</p>
<p>~ . ( .kB) = sin&laquo;2M + 1)B/2)
k ~ M exp J sin(B/2)&middot;
</p>
<p>19.23 c.:.:.... ) (w) Consider the continuous-time averager
</p>
<p>Y(t) = ~it X(~)d~
t-T
</p>
<p>where the random process X(t) is continuous-time white noise with PSD
</p>
<p>Px(F) = No/2 for -00 &lt; F &lt; 00. Determine the CCF rX,y(7) and show
that it is zero for 7 outside the interval [0,T]. Explain why it is zero outside
this interval.
</p>
<p>19.24 (f) Ifa continuous-time white noise process X(t) with ACF rX(7) = (No/2)o(7)
is input to an LTI system with impulse response h(7) = exp( -7)u(7), deter-
</p>
<p>mine rX,Y(7) .
</p>
<p>19.25 (t) Can the CPSD ever have the same properties as the PSD in terms of being
real and symmetric? If so, give an example. Hint: Consider the relationship
</p>
<p>given in (19.43).
</p>
<p>19.26 c.:..:...) (f,c) Consider the random processes X[n] = U[n] and Y[n] = U[n] -
bU[n - 1]' where U[n] is white Gaussian noise with variance ( j ~ = 1. Find
r x ,y [k] and then to verify your results perform a computer simulation. To do
</p>
<p>so first generate N = 1000 samples of X[n] and Y[n]. Then, estimate the CCS
for b = -0.1 and b = -1. Explain your results.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 671
</p>
<p>19.21 (f,c) An AR random process is given by X[n] = aX[n -1] +Urn], where Urn]
is white Gaussian noise with variance ( ] ' ~ . Find the ees rx,u[k] and then to
</p>
<p>verify your results perform a computer simulation using a = 0.5 and ( ] ' ~ = 1.
</p>
<p>To do so first generate N = 1000 samples of Urn] and X[n]. Then, estimate the
ees. Hint: Remember to set the initial condition X[-I] f',J N(O, (]'~/(1- a2&raquo;.
</p>
<p>19.28 (w) In this problem we explore the use of the eeF to determine the direction
of arrival of a sound source. Referring to Figure 19.10, a sound source emits a
</p>
<p>pulse that propagates to a set of two receivers. Because the distance from the
</p>
<p>source to the receivers is large, it is assumed that the wavefronts are planar
</p>
<p>as shown. If the source has the angle () with respect to the x axis as shown,
</p>
<p>it first reaches receiver 2 and then reaches receiver 1 at a time to = dcos((})/c
seconds later, where d is the distance between receivers and c is the propagation
</p>
<p>speed. Assume that the received signal at receiver 2 is a WSS random process
</p>
<p>X 2(t) = U(t) with a PSD
</p>
<p>Pu(F) = { /V,o0/2 IFI:::; W
IFI &gt; W
</p>
<p>and therefore the received signal at receiver 1 is Xl (t) = U(t - to). Determine
</p>
<p>the eeF rXl ,X2(T) and describe how it could be used to find the arrival angle
().
</p>
<p>planar
</p>
<p>wavefronts
</p>
<p>dcos((}) ",7 ..,...,
</p>
<p>Figure 19.10: Geometry for sound source arrival angle measurement (figure for
</p>
<p>Problem 19.28).</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 20
</p>
<p>Gaussian Random Processes
</p>
<p>20.1 Introduction
</p>
<p>There are several types of random processes that have found wide application be-
</p>
<p>cause of their realistic physical modeling yet relative mathematical simplicity. In
</p>
<p>this and the next two chapters we describe these important random processes. They
</p>
<p>are the Gaussian random process, the subject of this chapter; the Poisson random
</p>
<p>process, described in Chapter 21; and the Markov chain, described in Chapter 22.
</p>
<p>Concentrating now on the Gaussian random process, we will see that it has many
</p>
<p>important properties. These properties have been inherited from those of the N-
</p>
<p>dimensional Gaussian PDF, which was discussed in Section 14.3. Specifically, the
</p>
<p>important characteristics of a Gaussian random process are:
</p>
<p>1. It is physically motivated by the central limit theorem (see Chapter 15).
</p>
<p>2. It is a mathematically tractable model.
</p>
<p>3. The joint PDF of any set of samples is a multivariate Gaussian PDF, which
</p>
<p>enjoys many useful properties (see Chapter 14).
</p>
<p>4. Only the first two moments, the mean sequence and the covariance sequence, are
</p>
<p>required to completely describe it. As a result,
</p>
<p>a. In practice the joint PDF can be estimated by estimating only the first two
</p>
<p>moments.
</p>
<p>b. If the Gaussian random process is wide sense stationary, then it is also
</p>
<p>stationary.
</p>
<p>5. The processing of a Gaussian random process by a linear filter does not alter
</p>
<p>its Gaussian nature, but only modifies the first two moments. The modified
</p>
<p>moments are easily found.</p>
<p/>
</div>
<div class="page"><p/>
<p>674 CHAPTER 20. GAUSSIAN RANDOM PROCESSES
</p>
<p>In effect, the Gaussian random process has so many useful properties that it is always
</p>
<p>the first model to be proposed in the solution of a problem. It finds application
</p>
<p>as a model for electronic noise [Bell Labs 1970], ambient ocean noise [Urick 1975],
</p>
<p>scattering phenomena such as reverberation of sound in the ocean or electromagnetic
</p>
<p>clutter in the atmosphere [Van Trees 1971], and financial time series [Taylor 1986],
</p>
<p>just to name a few. Any time a random process can be modeled as due to the sum of
</p>
<p>a large number of independent and similar type effects, a Gaussian random process
</p>
<p>results due to the central limit theorem. One example that we will explore in detail
</p>
<p>is the use of the scattering of a sound pulse from a school of fish to determine their
</p>
<p>numbers (see Section 20.9). In this case, the received waveform is the sum of a large
</p>
<p>number of scattered pulses that have been added together. The addition occurs
</p>
<p>because the leading edge of a pulse that is reflected from a fish farther away will
</p>
<p>coincide in time with the trailing edge of the pulse that is reflected from a fish that
</p>
<p>is nearer (see Figure 20.14). If the fish are about the same size and type, then the
</p>
<p>average intensity of the returned echos will be relatively constant. However, the
</p>
<p>echo amplitudes will be different due to the different reflection characteristics of
</p>
<p>each fish, i.e., its exact position, orientation, and motion will all determine how the
</p>
<p>incoming pulse is scattered. These characteristics cannot be predicted in advance
</p>
<p>and so the amplitudes are modeled as random variables. When overlapped in time,
</p>
<p>these random echos are well modeled by a Gaussian random process. As an example,
</p>
<p>consider a transmitted pulse s(t) = cos(21rFot) , where Fo = 10 Hz, over the time
interval 0 ~ t ~ 1 second as shown in Figure 20.1. Assuming a single reflection
</p>
<p>0.5
</p>
<p>- 0.5 .
</p>
<p>0.2 0.4 0.6
t (sec)
</p>
<p>0.8 2 4 6
t (sec)
</p>
<p>8 10
</p>
<p>(a) Transmit pulse (b) Transmit pulse shown in receive wave-
</p>
<p>form observation window
</p>
<p>Figure 20.1: Transmitted sinusoidal pulse.
</p>
<p>for every 0.1 second interval with the starting time being a uniformly distributed
</p>
<p>random variable within the interval and an amplitude A that is a random variable</p>
<p/>
</div>
<div class="page"><p/>
<p>20.2. SUMMARY 675
</p>
<p>with A '" U{O, 1) to account for the unknown reflection coefficient of each fish, a
typical received waveform is shown in Figure 20.2. If we now estimate the marginal
</p>
<p>108642
</p>
<p>-3 L.-__--'- -'-__----'- --'-__----l
</p>
<p>o
t (sec)
</p>
<p>Figure 20.2: Received waveform consisting of many randomly overlapped and ran-
</p>
<p>dom amplitude echos.
</p>
<p>PDF for x{t) as shown in Figure 20.2 by assuming that each sample has the same
</p>
<p>marginal PDF, we have the estimated PDF shown in Figure 20.3 (see Section 10.9
</p>
<p>on how to estimate the PDF). Also shown is the Gaussian PDF 'with its mean
</p>
<p>and variance estimated from uniformly spaced samples of x{t). It is seen that the
</p>
<p>Gaussian PDF is very accurate as we would expect from the central limit theorem.
</p>
<p>The MATLAB code used to generate Figure 20.2 is given in Appendix 20A. In
</p>
<p>Section 20.3 we formally define the Gaussian random process.
</p>
<p>20.2 Summary
</p>
<p>Section 20.1 gives an example of why the Gaussian random process arises quite
</p>
<p>frequently in practice. The discrete-time Gaussian random process is defined in
</p>
<p>Section 20.3 as one whose samples comprise a Gaussian random vector as charac-
</p>
<p>terized by the PDF of (20.1). Also, some examples are given and are shown to
</p>
<p>exhibit two important properties, which are summarized in that section. Any linear
</p>
<p>transformation of a Gaussian random process produces another Gaussian random
</p>
<p>process. In particular for a discrete-time WSS Gaussian random process that is
</p>
<p>filtered by an LSI filter , the output random process is Gaussian with PDF given
</p>
<p>in Theorem 20.4.1. A nonlinear transformation does not maintain the Gaussian
</p>
<p>random process but its effect can be found in terms of the output moments using
</p>
<p>(20.12). An example of a squaring operation on a discrete-time WSS Gaussian ran-
</p>
<p>dom process produces an output random process that is still WSS with moments</p>
<p/>
</div>
<div class="page"><p/>
<p>676 CHAPTER 20. GAUSSIAN RANDOM PROCESSES
</p>
<p>0.5,-----,-----,-----,------,
&middot; . .&middot; . .
</p>
<p>0.45 : : : .
&middot; .&middot; .
</p>
<p>-2
</p>
<p>0.4 : .
</p>
<p>0.35 ; .
</p>
<p>0.3 : .
</p>
<p>0.25 : .
</p>
<p>0.2 : .
</p>
<p>0.15 : ..
</p>
<p>0.05 .
</p>
<p>o
-4 o
</p>
<p>X
</p>
<p>2 4
</p>
<p>Figure 20.3: Marginal PDF of samples of received waveform shown in Figure 20.2
</p>
<p>and Gaussian PDF fit.
</p>
<p>given by (20.14). A continuous-time Gaussian random process is defined in Sec-
</p>
<p>tion 20.6 and examples are given. An important one is the Wiener random process
</p>
<p>examined in Example 20.7. Its covariance matrix is found using (20.16). Some
</p>
<p>special continuous-time Gaussian random processes are described in Section 20.7.
</p>
<p>The Rayleigh fading sinusoid is described in Section 20.7.1. It has the ACF given
</p>
<p>by (20.17) and corresponding PSD given by (20.18). A continuous-time bandpass
</p>
<p>Gaussian random process is described in Section 20.7.2. It has an ACF given by
</p>
<p>(20.21) and a corresponding PSD given by (20.22). The important example of band-
</p>
<p>pass "white" Gaussian noise is discussed in Example 20.8. The computer generation
</p>
<p>of a discrete-time WSS Gaussian random process realization is described in Section
</p>
<p>20.8. Finally, an application of the theory to estimating fish populations using a
</p>
<p>sonar is the subject of Section 20.9.
</p>
<p>20.3 Definition of the Gaussian Random Process
</p>
<p>We will consider here the discrete-time Gaussian random process, an example of
</p>
<p>which was given in Figure 16.5b as the discrete-time/continuous-valued (DTCV)
</p>
<p>random process. The continuous-time/continuous-valued (CTCV) Gaussian ran-
</p>
<p>dom process, an example of which was given in Figure 16.5d, will be discussed in
</p>
<p>Section 20.6. Before defining the Gaussian random process we briefly review the
</p>
<p>N-dimensional multivariate Gaussian PDF as described in Section 14.3. An N X 1
</p>
<p>random vector X = [Xl X2 . . . XNV is defined to be a Gaussian random vector if</p>
<p/>
</div>
<div class="page"><p/>
<p>20.3. DEFINITION OF THE GAUSSIAN RANDOM PROCESS
</p>
<p>its joint PDF is given by the multivariate Gaussian PDF
</p>
<p>1 [ 1 T -1 )]
PX(x) = (2rr) N/2 det1/ 2(C) exp -2(x - p,) C (x - p,
</p>
<p>where p, = [JL1 JL2 ... JLN]T is the mean vector defined as
</p>
<p>677
</p>
<p>(20.1)
</p>
<p>p, = Ex[X] =
[
</p>
<p>EXl[Xtl ]
EX2[X2]
</p>
<p>EXN[XN]
</p>
<p>(20.2)
</p>
<p>and C is the N x N covariance matrix defined as
</p>
<p>[
</p>
<p>var(Xt}
</p>
<p>C = COV(~2 ' Xl)
</p>
<p>COV(XN, X t}
</p>
<p>cov(X1 , X 2 )
</p>
<p>var(X2)
cov(X1, XN) ]
coV(X2,XN)
</p>
<p>. .
</p>
<p>var(XN)
</p>
<p>(20.3)
</p>
<p>In shorthand notation X '" N(p" C). The important properties of a Gaussian
</p>
<p>random vector are:
</p>
<p>1. Only the first two moments p, and C are required to specify the entire PDF.
</p>
<p>2. If all the random variables are uncorrelated so that [C]ij = 0 for i f:. i . then they
are also independent.
</p>
<p>3. A linear transformation of X produces another Gaussian random vector. Specif-
</p>
<p>ically, if Y = GX, where G is an M x N matrix with M ::; N , then
Y '" N(Gp" GCGT ) .
</p>
<p>Now we consider a discrete-time random process X[n] , where n 2: 0 for a semi-
infinite random process and -00 &lt; n &lt; 00 for an infinit e random process. The
random process is defined to be a Gaussian random process if all finite sets of sam-
</p>
<p>ples have a multivariate Gaussian PDF as per (20.1). Mathematically, if X =
</p>
<p>[X[n1]X[n2] ... X[nK]]T has a multivariate Gaussian PDF (given in (20.1) with N
</p>
<p>replaced by K) for all {n1 ' n2, . . . ,nK} and all K , then X[n] is said to be a Gaussian
random process. Some examples follow.
</p>
<p>Example 20.1 - White Gaussian noise
</p>
<p>White Gaussian noise was first introduced in Example 16.6. We revisit that exam-
</p>
<p>ple in light of our formal definition of a Gaussian random process. First recall that
</p>
<p>discrete-time white noise is a WSS random process X[n] for which E[X[n]] = JL = 0
</p>
<p>for -00 &lt; n &lt; 00 and rx [k] = (/28[k]. This says that all the samples are zero mean,
uncorrelated with each other , and have the same variance (/2. If we now further-
</p>
<p>more assume that the samples are also independent and each sample has a Gaussian</p>
<p/>
</div>
<div class="page"><p/>
<p>678 CHAPTER 20. GAUSSIAN RANDOM PROCESSES
</p>
<p>PDF, then X[n] is a Gaussian random process. It is referred to as white Gaussian
noise (WGN). To verify this we need to show that any set of samples has a mul-
</p>
<p>tivariate Gaussian PDF. Let X = [X[nl] X[n2]'" X[nK]V and note that the joint
K-dimensional PDF is the product of the marginal PDFs due to the independence
</p>
<p>assumpt ion. Also, each marginal PDF is X[n i] rv N(O ,a2 ) by assumption. This
produces the joint PDF
</p>
<p>K
</p>
<p>px(x) = IIPX[nil(x[ni])
i= 1
</p>
<p>or X rv N(O , a 21) , where 1 is the K x K identity matrix. Note also that since WGN
</p>
<p>is an lID random process, it is also stationary (see Example 16.3).
</p>
<p>Example 20.2 - Moving average random process
</p>
<p>Consider the MA random process X[n] = (U[n] + U[n -1])/2, where U[n] is WGN
with variance ab. Then, X[n] is a Gaussian random process. This is because U[n]
is a Gaussian random process (from previous example) and X[n] is just a linear
transformation of U[n]. For instance, if K = 2, and nl = 0, n2 = 1, then
</p>
<p>and thus X rv N(o ,GCuGT) = N(O,abGGT). The same argument applies to any
number of samples K and any samples times nl , n2 , ... ,nK. Note here that the MA
random process is also stationary. If we were to change the two samples to nl = no
and n2 = no + 1, then
</p>
<p>[
X[no] ] [1
</p>
<p>X[no + 1] = ~
I &deg; [u[no- 1] ]
~ !] Urn,]
</p>
<p>U[no + 1]</p>
<p/>
</div>
<div class="page"><p/>
<p>20.3. DEFINITION OF THE GAUSSIAN RANDOM PROCESS 679
</p>
<p>and the joint PDF will be the same since the U vector has the same PDF. Again
</p>
<p>this result remains the same for any number of samples and sample times. We will
</p>
<p>see shortly that a Gaussian random process that is WSS is also stationary. Here,
</p>
<p>the Urn] random process is WSS and hence X[n] is WSS, being the output of an
LSI filter (see Theorem 18.3.1).
</p>
<p>As a typical probability calculation let (J~ = 1 and determine P[X[l] - X[O] &gt;
1]. We would expect this to be less than P[U[l] - U[O] &gt; 1] = Q(l/v'2) (since
U[l] - U[O] '" N(O,2)) due to the smoothing effect of the filter (ll(z) = ! + !z-l).
Thus, let Y = X[l] - X[O] or
</p>
<p>Y = [-1 1] [ X[O] ] .
~ X[l]
</p>
<p>A '"'-...-'
X
</p>
<p>Then, since Y is a linear transformation of X, we have Y '" N(O,var(Y)), where
var(Y) = ACAT . Thus,
</p>
<p>var(Y) = [-I I] C [ ~ 1 ]
</p>
<p>[-I I] GGT [ ~I ] (C = ,,~GGT = GGT)
</p>
<p>[-I I] [ ~ : ;] [! t][ ~I ]
1
</p>
<p>2
</p>
<p>so that Y '" N(o ,1/2). Therefore,
</p>
<p>P[X[I]- X[O] &gt; I] ~ Q ("':/2) = Q(V2) = 0.0786 &lt; Q (~) = 0.2398
</p>
<p>and is consistent with our notion of smoothing.
</p>
<p>Example 20.3 - Discrete-time Wiener random process or Brownian mo-
</p>
<p>tion
</p>
<p>This random process is basically a random walk with Gaussian "steps" or more
</p>
<p>specifically the sum process (see also Example 16.4)
</p>
<p>n
</p>
<p>X[n] = 2:U[i] n ~ 0
i=O</p>
<p/>
</div>
<div class="page"><p/>
<p>680 CHAPTER 20. GAUSSIAN RANDOM PROCESSES
</p>
<p>where Urn] is WGN with variance a ~ . Note that the increments X[n2] - X[nl] are
</p>
<p>independent and stationary (why?). As in the previous example, any set of samples
</p>
<p>of X[n] is a linear transformation of the U[i]'s and hence has a multivariate Gaussian
</p>
<p>PDF. For example,
</p>
<p>[ ~:~~ ] = [; ~ ] [ ~:~; ]
'--v--'
</p>
<p>G
</p>
<p>and therefore the Wiener random process is a Gaussian random process. It is clearly
</p>
<p>nonstationary, since , for example, the variance increases with n (recall from Example
</p>
<p>16.4 that var(X[nD = (n + l)a~).
c
</p>
<p>In Example 20.1 we saw that if the samples are uncorrelated, and the random
</p>
<p>process is Gaussian and hence the multivariate Gaussian PDF applies, then the
</p>
<p>samples are also independent. In Examples 20.1 and 20.2, the random processes
</p>
<p>were WSS but due to the fact that they are also Gaussian random processes, they
</p>
<p>are also stationary. We summarize and then prove these two properties next.
</p>
<p>Property 20 .1 - A Gaussian random process with uncorrelated samples
</p>
<p>has independent samples.
</p>
<p>Proof:
</p>
<p>Since the random process is Gaussian, the PDF of (20.1) applies for any set of
</p>
<p>samples. But for uncorrelated samples, the covariance matrix is diagonal and hence
</p>
<p>the joint PDF factors into the product of its marginal PDFs. Hence, the samples
</p>
<p>are independent.
</p>
<p>o
</p>
<p>Property 20.2 - A WSS Gaussian random process is also stationary.
</p>
<p>Proof:
</p>
<p>Since the random process is Gaussian, the PDF of (20.1) applies for any set of
</p>
<p>samples. But if X[n] is also WSS , then for any no
</p>
<p>and
</p>
<p>E[X[ni + no]] = J.lx[ni + no] = J.l i = 1,2, ... , K
</p>
<p>[GJij = cov(X[ni + no],X[nj + noD
E[X[ni + no]X[nj + no]] - E[X[ni + no]]E[X[nj + no]]
</p>
<p>= rx[nj - nil - J.l2 (due to WSS)
</p>
<p>for i = 1,2, ... , K and j = 1,2, ... , K. Since the mean vector and the covariance
</p>
<p>matrix do not depend on no, the joint PDF also does not depend on no. Hence, the
</p>
<p>WSS Gaussian random process is also stationary.
</p>
<p>o</p>
<p/>
</div>
<div class="page"><p/>
<p>20.4. LINEAR TRANSFORMATIONS 681
</p>
<p>20.4 Linear Transformations
</p>
<p>Any linear transformation of a Gaussian random process produces another Gaus-
</p>
<p>sian random process. In Example 20.2 the white noise random process Urn] was
</p>
<p>Gaussian, and the MA random process X[n], which was the result of a linear
</p>
<p>transformation, is another Gaussian random process. The MA random process
</p>
<p>in that example can be viewed as the output of the LSI filter with system function
</p>
<p>H(z) = 1/2 + (1/2)z-1 whose input is Urn]. This result, that if the input to an
LSI filter is a Gaussian random process, then the output is also a Gaussian random
</p>
<p>process, is true in general. The random processes described by the linear difference
</p>
<p>equations
</p>
<p>aX[n - 1] + Urn]
Urn] - bUrn -1]
</p>
<p>aX[n - 1] + Urn] - bUrn - 1]
</p>
<p>X[n]
</p>
<p>X[n]
</p>
<p>X[n]
</p>
<p>AR random process (see Example 17.5)
</p>
<p>MA random process (see Example 18.6)
</p>
<p>ARMA random process
</p>
<p>(This is the definition.)
</p>
<p>can also be viewed as the outputs of LSI filters with respective system functions
</p>
<p>H(z)
</p>
<p>H(z)
</p>
<p>H(z)
</p>
<p>1
</p>
<p>1 - az-1
</p>
<p>= 1 - bz-1
</p>
<p>1 - bz-1
</p>
<p>1 - az-1 '
</p>
<p>As a result , since the input Urn] is a Gaussian random process, they are all Gaussian
</p>
<p>random processes. Furthermore, since it is only necessary to know the first two
</p>
<p>moments to specify the joint PDF of a set of samples of a Gaussian random process,
</p>
<p>the PDF for the output random process of an LSI filter is easily found. In particular,
</p>
<p>assume we are interested in the filtering of a WSS Gaussian random process by an
</p>
<p>LSI filter with frequency response H(J) . Then, if the input to the filter is the WSS
</p>
<p>Gaussian random process X[n], which has a mean of ux and an ACS of rx[k], then
we know from Theorem 18.3.1 that the output random process Y[n] is also WSS and
</p>
<p>its mean and A CS are
</p>
<p>f..Ly
</p>
<p>Py(J)
</p>
<p>f..LxH(O)
</p>
<p>IH(J)1
2
Px(J)
</p>
<p>(20.4)
</p>
<p>(20.5)
</p>
<p>and furthermore Y[n] is a Gaussian random process (and is stationary according to
</p>
<p>Property 20.2). (See also Problem 20.7.) The joint PDF for any set of samples of
</p>
<p>Y[n] is found from (20.1) by using (20.4) and (20.5). An example follows.
</p>
<p>Example 20.4 - A differencer
</p>
<p>A WSS Gaussian random process X[n] with mean ux and ACS rx[k] is input to
</p>
<p>a differencer. The output random process is defined to be Y[n] = X[n] - X[n - 1].</p>
<p/>
</div>
<div class="page"><p/>
<p>682 CHAPTER 20. GAUSSIAN RANDOM PROCESSES
</p>
<p>What is the PDF of two successive output samples? To solve this we first note that
</p>
<p>the output random process is Gaussian and also WSS since a differencer is just an
</p>
<p>LSI filter whose system function is ll(z) = 1- z-l. We need only find the first two
</p>
<p>moments of Y[n]. The mean is
</p>
<p>E[Y[n]] = E[X[n]] - E[X[n - 1]] = tsx - ux = 0
</p>
<p>and the ACS can be found as the inverse Fourier transform of Py(f). But from
</p>
<p>(20.5) with H(f) = 1l(exp(j21rf) = 1- exp(-j21rf), we have
</p>
<p>Py(f) H(f)H*(f)Px(f)
</p>
<p>[1 - exp( -j21r f)][1 - exp(j21r f)]Px (f)
</p>
<p>2Px(f) - exp(j21rf)Px(f) - exp( -j21rf)Px(f).
</p>
<p>Taking the inverse Fourier transform produces
</p>
<p>ry[k] = 2rx[k] - rx[k + 1] - rx[k - 1]. (20.6)
</p>
<p>For two successive samples, say Y[O] and Y[1], we require the covariance matrix of
</p>
<p>Y = [Y[O] Y[1]]T. Since Y[n] has a zero mean, this is just
</p>
<p>c = [ry[O] rY[1]]
y ry[1] ry[O]
</p>
<p>and thus using (20.6), it becomes
</p>
<p>Cy - [ 2(rx[0] - rx[1]) 2rx[1] - rx[2] - rx[O] ]
- 2rx[1] - rx[2] - rx[O] 2(rx[0] - rx[1]) .
</p>
<p>The joint PDF is then
</p>
<p>PY[O] ,Y[l] (y[OJ, y[1]) = 21r det11/2 (Cy) exp( _~yTCyly)
</p>
<p>where y = [y[O] y[1]jT. See also Problem 20.5.
</p>
<p>We now summarize the foregoing results in a theorem.
</p>
<p>Theorem 20.4.1 (Linear filtering of a WSS Gaussian random process)
</p>
<p>Suppose that X[n] is a WSS Gaussian random process with mean ux and ACS rx[k]
that is input to an LSI filter with frequency response H(f). Then, the PDF of N
</p>
<p>successive output samples Y = [Y[O] Y[1] ... Y[N - 1]jT is given by
</p>
<p>(20.7)</p>
<p/>
</div>
<div class="page"><p/>
<p>20.5. NONLINEAR TRANSFORMATIONS
</p>
<p>where
</p>
<p>[
</p>
<p>~X~. (0) ]
/-Ly =
</p>
<p>~xH(O)
</p>
<p>683
</p>
<p>(20.8)
</p>
<p>[CY]mn = ry[m - n] - (~XH(0)) 2 (20.9)
1i : IH(f)12Px(f) exp(j21rf(m - n))df - (~XH(0))2
2
</p>
<p>(20.10)
</p>
<p>for m = 1,2, ... ,N; n = 1,2, ... ,N. The same PDF is obtained for any shifted set
</p>
<p>of successive samples since Y[n] is stationary.
</p>
<p>Note that in the preceding theorem the covariance matrix is a symmetric Toeplitz
</p>
<p>matrix (all elements along each northwest-southeast diagonal are the same) due to
</p>
<p>the assumption of successive samples (see also Section 17.4) .
</p>
<p>Another transformation that occurs quite frequently is the sum of two inde-
</p>
<p>pendent Gaussian random processes. If X[n] is a Gaussian random process and
</p>
<p>Y[n] is another Gaussian random process, and X[n] and Y [n] are independent, then
</p>
<p>Z [n] = X[n] + Y [n] is a Gaussian random process (see Problem 20.9). By inde-
pendence of two random processes we mean that all sets of samples of X[n] or
</p>
<p>{X[nl ], X [n2], ... , X [nK]} and of Y[n] or {Y[ml ],Y[m2]," " Y[mL]} are indepen-
dent of each other. This must hold for all n I , . .. , n K , m I , ... , m L and for all K and
</p>
<p>L. If this is the case then the PDF of the entire set of samples can be written as
</p>
<p>the product of the PDFs of each set of samples.
</p>
<p>20.5 Nonlinear Transformations
</p>
<p>The Gaussian random process is one of the few random processes for which the
</p>
<p>moments at the output of a nonlinear transformation can easily be found . In par-
</p>
<p>ticular, a polynomial transformation lends itself to output moment evaluation. This
</p>
<p>is because the higher-order joint moments of a multivariate Gaussian PDF can be
</p>
<p>expressed in terms of first- and second-order moments. In fact , this is not sur-
</p>
<p>prising in that the multivariate Gaussian PDF is characterized by its first- and
</p>
<p>second-order moments. As a result , in computing the joint moments, any integral of
</p>
<p>the form J ~ oo" . J ~ oo xil ... x~ PXl ,...,XN (Xl, . .. , XN)dXI . .. dXN must be a fun ction
of the mean vector and covariance matrix. Hence , the joint moments must be a
</p>
<p>function of the first- and second-order moments. As a particular case of interest,
</p>
<p>consider the fourth-order moment E[XIX2X3X4] for X = [Xl X2 X3 X 4]T a zero</p>
<p/>
</div>
<div class="page"><p/>
<p>684 CHAPTER 20. GAUSSIAN RANDOM PROCESSES
</p>
<p>mean Gaussian random vector. Then, it can be shown that (see Problem 20.12)
</p>
<p>E[XIX2X3X4] = E[XIX2]E[X3X4] + E[XIX3]E[X2X4] + E[XIX4]E[X2X3]
(20.11)
</p>
<p>and this holds even if some of the random variables are the same (try Xl = X2 =
X3 = X4 and compare it to E[X
</p>
<p>4] for X", N(O,1)). It is seen that the fourth-order
</p>
<p>moment is expressible as the sum of products of the second-order moments, which
</p>
<p>are found from the covariance matrix. Now if X[n] is a Gaussian random process
with zero mean, then we have for any four samples (which by the definition of a
</p>
<p>Gaussian random process has a fourth-order Gaussian PDF)
</p>
<p>E[X[ntJX[n2]X[n3]X[n4]] = E[X[ntJX[n2]]E[X[n3]X[n4]]
</p>
<p>+E[X[nl]X[n3]]E[X[n2]X[n4]]
</p>
<p>+E[X[nl]X[n4]]E[X[n2]X[n3]] (20.12)
</p>
<p>and iffurthermore, X[n] is WSS, then this reduces to
</p>
<p>E[X[nl]X[n2]X[n3]X[n4]] = rx[n2 - nl]rx[n4 - n3] + rx[n3 - nl]rx[n4 - n2]
+rx[n4 - ntJrx[n3 - n2]' (20.13)
</p>
<p>This formula allows us to easily calculate the effect of a polynomial transformation
</p>
<p>on the moments of a WSS Gaussian random process. An example follows.
</p>
<p>Example 20.5 - Effect of squaring WSS Gaussian random process
</p>
<p>Assuming that X[n] is a zero mean WSS Gaussian random process, we wish to
determine the effect of squaring it to form Y[n] = X 2[n]. Clearly, Y[n] will no
longer be a Gaussian random process since it can only take on nonnegative values
</p>
<p>(see also Example 10.8). We can, however, show that Y[n] is still WSS. To do so
we calculate the mean as
</p>
<p>E[Y[n]] = E[X2[n]] = rx[O]
</p>
<p>which does not depend on n, and the covariance sequence as
</p>
<p>E[Y[n]Y[n + k]] = E[X2[n]X2[n + k]]
d&middot;[O] + 2r~[k] (using nl = n2 = n
</p>
<p>and n3 = n4 = n + kin (20.13))
</p>
<p>which also does not depend on n. Thus, at the output of the squarer the random
</p>
<p>process is WSS with
</p>
<p>/-Ly
</p>
<p>ry[k]
</p>
<p>= rx[O]
</p>
<p>r~[O] + 2r~[k]. (20.14)</p>
<p/>
</div>
<div class="page"><p/>
<p>20.5.&middot; NONLINEAR TRANSFORMATIONS 685
</p>
<p>Note that if the PSD at the input to the squarer is Px(f), then the output PSD is
</p>
<p>obtained by taking the Fourier transform of (20.14) to yield
</p>
<p>Py(f) = d&middot;[0]8(f) + 2Px(f) *Px(f) (20.15)
where
</p>
<p>1
</p>
<p>Px(f) *Px(f) = i: Px(v)Px(f - v)dv
2
</p>
<p>is a convolution integral. As a specific example, consider the MA random process
</p>
<p>X[n] = (U[n]+U[n-1])/2, where Urn] is WGN with variance ( 1 ~ = 1. Then, typical
realizations of X[n] and Y[n] are shown in Figure 20.4. The MA random process
</p>
<p>3 .----~-~-~-~-~-----,
</p>
<p>2 .
</p>
<p>; : r&bull;&bull;&bull;L...J1.II&bull;&bull;&bull;,J&bull;.I1
-1 .
</p>
<p>-2 -2
</p>
<p>10 15 20 25 30
</p>
<p>n
5
</p>
<p>-3'---'---~-~-~-~----'
</p>
<p>o10 15 20 25 30
n
</p>
<p>5
_3L--~-~-~-~-~----'
</p>
<p>o
</p>
<p>(a) MA random process (b) Squared MA random process
</p>
<p>Figure 20.4: Typical realization of a Gaussian MA random process and its squared
</p>
<p>realization.
</p>
<p>has a zero mean and ACS rx[k] = (1/2)8[k] + (1/4)8[k + 1] + (1/4)8[k - 1] (see
Example 17.3). Because of the squaring, the output mean is E[Y[n]] = rx[O] = 1/2.
The PSD of X[n] can easily be shown to be Px(f) = (1 +COS(21TJ))/2 and the PSD
of Y[n] follows most easily by taking the Fourier transform of ry[k]. From (20.14)
we have
</p>
<p>ry[k] r1-[O] + 2r1-[k]
</p>
<p>1 (1 1 1 )24 + 2 2"8[k] + 48[k + 1] + 48[k - 1]
1 (1 1 1 )4 + 2 48[k] + 16 8[k + 1]+ 16 8[k -1]
</p>
<p>since all the cross-terms must be zero and 82[k - ko] = 8[k - ko]. Thus, we have
</p>
<p>1 1 1 1
ry[k] = 4+ 2"8[k] + 88[k + 1] + 88[k - 1]</p>
<p/>
</div>
<div class="page"><p/>
<p>686 CHAPTER 20. GAUSSIAN RANDOM PROCESSES
</p>
<p>and taking the Fourier transform produces the PSD as
</p>
<p>1 1 1
Py(J) = "4 c5(J ) + 2+ "4 cos(27fJ).
</p>
<p>The PSDs are shown in Figure 20.5. Note that the squaring has produced an impulse
</p>
<p>ol...----'----'-----'_~----'--~_~~---'-____J
-0.5 -0.4 -0.3 -0.2 -0.1 0 0.1 02 03 0.4 05
</p>
<p>f
</p>
<p>1 :..
</p>
<p>1
4:
</p>
<p>. .;.. . . ; . . .. :. . .. :.. ...:. . . . .:....
</p>
<p>. _ .
</p>
<p>0.4
</p>
<p>02
</p>
<p>12 .
</p>
<p>O'""""---'----'-----'-~~~-~~~~
</p>
<p>-0.5 -0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4 0.5
</p>
<p>f
</p>
<p>0.4
</p>
<p>1.2 . . . -:
</p>
<p>----.
~0.8 .
</p>
<p>&gt;&lt;
0.. 0.6.
</p>
<p>(a) MA random process (b) Squared MA random process
</p>
<p>Figure 20.5: PSDs of Gaussian MA random process and the squared random process.
</p>
<p>at f = 0 of strength 1/4 that is due to the nonzero mean of the Y[n] random process.
Also, the squaring has "widened" the PSD, the usual consequence of a convolution
</p>
<p>in frequency.
</p>
<p>20.6 Continuous-Time Definitions and Formulas
</p>
<p>A continuous-time random process is defined to be a Gaussian random process if the
</p>
<p>random vector X = [X(tl) X(t2)' " X(tK)V has a multivariate Gaussian PDF for
</p>
<p>all {tl' t2, .. . , tK} and all K. The properties of a continuous-time Gaussian random
process are identical to those for the discrete-time random process as summarized
</p>
<p>in Properties 20.1 and 20.2. Therefore, we will proceed directly to some examples
</p>
<p>of interest.
</p>
<p>Example 20.6 - Continuous-time WGN
</p>
<p>The continuous-time version of discrete-time WGN as defined in Example 20.1
</p>
<p>is a continuous-time Gaussian random process X(t) that has a zero mean and an
</p>
<p>ACF rX(T) = (No/2)c5(T). The factor of No/2 is customarily used, since it is the
level of the corresponding PSD (see Example 17.11). The random process is called
</p>
<p>continuous-time white Gaussian noise (WGN). This was previously described in</p>
<p/>
</div>
<div class="page"><p/>
<p>20.6. CONTINUOUS-TIME DEFINITIONS AND FORMULAS 687
</p>
<p>t 2: o.
</p>
<p>0.4 0.6 0.8
t (sec)
</p>
<p>Example 17.11. Note that in addition to the samples being uncorrelated (since
</p>
<p>rX(T) = 0 for T # 0), they are also independent because of the Gaussian assump-
tion. Unfortunately, for continuous-time WGN , it is not possible to explicitly write
</p>
<p>down the multivariate Gaussian PDF since rx(O) -+ 00. Instead, as explained in
Example 17.11 we use continuous-time WGN only as a model, reserving any proba-
</p>
<p>bility calculations for the random process at the output of some filter , whose input
</p>
<p>is WGN. This is illustrated next.
</p>
<p>Example 20.7 - Continuous-time Wiener random process or Brownian
</p>
<p>motion
</p>
<p>Let U(t) be WGN and define the semi-infinite random process
</p>
<p>X(t) = it U(~)d~
This random process is called the Wiener random process and is often used as a
</p>
<p>model for Brownian motion. It is the continuous-time equivalent of the discrete-
</p>
<p>time random process of Example 20.3. A typical realization of the Wiener random
</p>
<p>process is shown in Figure 20.6 (see Problem 20.18 on how this was done). Note that
</p>
<p>0.8
</p>
<p>0.6 .
</p>
<p>0.4
</p>
<p>_____ 0.2 . ....,
'-"
</p>
<p>~ 0
</p>
<p>-0.2 .
</p>
<p>-0.4
</p>
<p>-0.6
</p>
<p>-0.8 .
</p>
<p>-1 L-__---'- "--__----'- --'----__-----l
</p>
<p>o 0.2
</p>
<p>Figure 20.6: Typical realization of the Wiener random process.
</p>
<p>because of its construction as the "sum" of independent and identically distributed
</p>
<p>random variables (the U(t)'s), the increments are also independent and stationary.
</p>
<p>To prove that X(t) is a Gaussian random process is somewhat tricky in that it is
</p>
<p>an uncountable "sum" of independent random variables U ( ~ ) for 0 :::; ~ :::; t. We will
</p>
<p>take it on faith that any integral, which is a linear transformation, of a continuous-
</p>
<p>time Gaussian random process produces another continuous-time Gaussian random</p>
<p/>
</div>
<div class="page"><p/>
<p>688 CHAPTER 20. GAUSSIAN RANDOM PROCESSES
</p>
<p>process (see also Problem 20.16 for a heuristic proof). As such, we need only deter-
</p>
<p>mine the mean and covariance functions. These are found as
</p>
<p>E[X(t)] = E [I t U(Od~]
= I t E[U(~)]d~ = 0
</p>
<p>E[X(td X(t2)] = E [l t 1 U(6)d61t2U(6)d6]
= l t1lt 2 !J[U(6JU(6)l d6d6
</p>
<p>ru(6-{I)=(No/2)5(6 - ~ l )
</p>
<p>~ O l t 1 (l t28(6 - 6)d6) d6.
To evaluate the double integral we first examine the inner integral and assume that
</p>
<p>t2 &gt; tl. Then, the function 8(6 - 6) with 6 fixed is integrated over the interval
o ~ 6 ~ t2 as shown in Figure 20.7. It is clear from the figure that if we fix 6
</p>
<p>integrate along here first
</p>
<p>Figure 20.7: Evaluation of double integral of Dirac delta function for the case of
</p>
<p>t2 &gt; ti -
</p>
<p>and integrate along 6 , then we will include the impulse in the inner integral for all
6 . (This would not be the case if t2 &lt; tl as one can easily verify by redrawing the
rectangle for this condition.) As a result , if t2 &gt; t l , then
</p>
<p>for all 0 ~ 6 ~ tl
</p>
<p>and therefore</p>
<p/>
</div>
<div class="page"><p/>
<p>20.7. SPECIAL CONTINUOUS-TIME GAUSSIAN RANDOM PROCESSES 689
</p>
<p>and similarly ift2 &lt; tl , we will have E[X(tl)X(t2)] = (No/2)t2. Combining the two
results produces
</p>
<p>(20.16)
</p>
<p>which should be compared to the discrete-time result obtained in Problem 16.26.
</p>
<p>Hence, the joint PDF of the samples of a Wiener random process is a multivariate
</p>
<p>Gaussian PDF with mean vector equal to zero and covariance matrix having as its
</p>
<p>(i ,j)th element
</p>
<p>[Ch = ~o min(ti, tj).
</p>
<p>Note that from (20.16) with tl = t2 = t, the PDF of X(t) is N(o, (No/2)t). Clearly,
the Wi en er random process is a nonstationary correlated random process whose mean
</p>
<p>is zero, variance increases with time, and marginal PDF is Gaussian.
</p>
<p>&lt;:;
In the next section we explore some other important continuous-time Gaussian ran-
</p>
<p>dom processes often used as models in practice.
</p>
<p>20.7 Special Continuous-Time
</p>
<p>Gaussian Random Processes
</p>
<p>20.7.1 Rayleigh Fading Sinusoid
</p>
<p>In Example 16.11 we studied a discrete-time randomly phased sinusoid. Here we
</p>
<p>consider the continuous-time equivalent for that random process, which is given by
</p>
<p>X(t) = A cos (27rFot + 8) , where A &gt; 0 is the amplitude, Fo is the frequency in Hz,
and 8 is the random phase with PDF U(0 ,27r). We now further assume that the
</p>
<p>amplitude is also a random variable. This is frequently a good model for a sinu-
</p>
<p>soidal signal that is subject to multipath fading. It occurs when a sinusoidal signal
</p>
<p>propagates through a medium, e.g., an electromagnetic pulse in the atmosphere or a
</p>
<p>sound pulse in the ocean, and reaches its destination by several different paths. The
</p>
<p>constructive and destructive interference of several overlapping sinusoids causes the
</p>
<p>received waveform to exhibit amplitude fluctuations or fading. An example of this
</p>
<p>was given in Figure 20.2. However, over any short period of time, say 5 ::; t ::; 5.5
seconds, the waveform will have approximately a constant amplitude and a constant
</p>
<p>phase as shown in Figure 20.8. Because the amplitude and phase are not known in
</p>
<p>advance, we model them as realizations of random variables. That the waveform
</p>
<p>does not maintain the constant amplitude level and phase outside of the small inter-
</p>
<p>val will be of no consequence to us if we are only privy to observing the waveform
</p>
<p>over a small time interval. Hence, a reasonable model for the random process (over
</p>
<p>the small time interval) is to assume a random amplitude and random phase so that
</p>
<p>X(t) = Acos(27rFot+8), where A and 8 are random variables. A more convenient</p>
<p/>
</div>
<div class="page"><p/>
<p>690 CHAPTER 20. GAUSSIAN RANDOM PROCESSES
</p>
<p>3,.....-------.-------,,------,--------r---,
</p>
<p>2 .
</p>
<p>-1
</p>
<p>-2
</p>
<p>5.55.45.2 5.3
t (sec)
</p>
<p>5.1
</p>
<p>-3 L-__--'-__----J'--__-'-__---'__---'
</p>
<p>5
</p>
<p>Figure 20.8: Segment of waveform shown in Figure 20.2 for 5 ::; t ::; 5.5 seconds .
</p>
<p>form is obtained by expanding the sinusoid as
</p>
<p>X(t) Acos(27TFot + 8)
= A cos(8) cos(27TFot ) - Asin(8) sin(27TFot )
</p>
<p>= U COS(27TFot) - V sin(27TFot)
</p>
<p>where we have let A cos(8 ) = U, Asin(8) = V. Clearly, since A and 8 are random
</p>
<p>variables, so are U and V. Since the physical waveform is due to the sum of many
</p>
<p>sinusoids, we once again use a central limit theorem argument to assume that U and
</p>
<p>V are Gaussian. Furthermore, if we assume that they are independent and have the
</p>
<p>same PDF of N(o, (12), we will obtain PDFs for the amplitude and phase which are
found to be valid in practice. With the Gaussian assumptions for U and V, the
</p>
<p>random amplitude becomes a Rayleigh distributed random variable, the random
</p>
<p>phase becomes a uniformly distributed random variable, and the amplitude and
</p>
<p>phase random variables are independent of each other. To see this note that since
</p>
<p>U = Acos(8), V = Asin(8), we have A = JU2 + V 2 and 8 = arctan(V/U). It
was shown in Example 12.12 that if X f',J N(O, (12), Y f',J N(O, (12), and X and Yare
independent , then R = JX2 + y2 is a Rayleigh random variable, 8 = arctan(Y/X)
is a uniformly distributed random variable, and Rand 8 are independent. Hence , we
</p>
<p>have that for the random amplit ude/ random phase sinusoid X(t) = Acos{27TFot +
8) , the amplitude has the PDF
</p>
<p>PA(a) = { ~exp (-~~) a ~ 0
o a&lt;O
</p>
<p>and the phase has the PDF 8 f',J U{0,27T) , and A and 8 are independent. This
</p>
<p>model is usually referred to as the Rayleigh fading sinusoidal model. It is also a</p>
<p/>
</div>
<div class="page"><p/>
<p>20.7. SPECIAL CONTINUOUS-TIME GAUSSIAN RANDOM PROCESSES 691
</p>
<p>Gaussian random process since all sets of K samples can be written as
</p>
<p>[
</p>
<p>X(tl) ]
X(t2)
</p>
<p>X(tK)
[
</p>
<p>cos(27rFotl) - sin(27rFotd ]
cos(27rFot2) - sin(27rFot2)
</p>
<p>= &middot; .&middot; .&middot; .
cos(27rFotK) -sin(27rFotK)
</p>
<p>[ ~ ]
which is a linear transformation of the Gaussian random vector [Uvy, and so has
a multivariate Gaussian PDF. (For K &gt; 2 the covariance matrix will be singular,
so that to be more rigorous we would need to modify our definition of the Gaussian
</p>
<p>random process. This would involve the characteristic function which exists even
</p>
<p>for a singular covariance matrix.) Furthermore, X(t) is WSS, as we now show. Its
</p>
<p>mean is zero since E[U] = E[V] = 0 and its ACF is
</p>
<p>rX(T)
</p>
<p>E[X(t)X(t + T)]
= E[[Ucos(27rFot) - V sin(27rFot)][U cos(27rFo(t + T)) - V sin(27rFo(t + T))))
</p>
<p>E[U2] cos (27rFot) cos(27rFo(t +T)) + E[V 2] sin(27rFot) sin(27rFo(t + T))
(J2[cos(27rFot) cos(27rFo(t + T)) + sin(27rFot) sin(27rFo(t + T))]
(J2 cos(27rFoT) (20.17)
</p>
<p>where we have used E[UV] = E[U]E[V] = 0 due to independence. Its PSD is
</p>
<p>obtained by taking the Fourier transform to yield
</p>
<p>(20.18)
</p>
<p>and it is seen that all its power is concentrated at F = Fo as expected.
</p>
<p>20.7.2 Bandpass Random Process
</p>
<p>The Rayleigh fading sinusoid model assumed that our observation time was short.
</p>
<p>Within that time window, the sinusoid exhibits approximately constant amplitude
</p>
<p>and phase. If we observe a longer time segment of the random process whose typ-
</p>
<p>ical realization is shown in Figure 20.2, then the constant in time (but random
</p>
<p>amplitude/random phase) sinusoid is not a good model. A more realistic but more
</p>
<p>complicated model is to let both the amplitude and phase be random processes so
</p>
<p>that they vary in time. As such, the random process will be made up of many fre-
</p>
<p>quencies, although they will be concentrated about F = Fo. Such a random process
is usually called a narrowband random process. Our model, however, will actually
</p>
<p>be valid for a bandpass random process whose PSD is shown in Figure 20.9. Hence ,
</p>
<p>we will assume that the bandpass random process can be represented as
</p>
<p>X(t) A(t) cos(27rFot + 8(t))
A(t) cos(8(t)) cos (27rFot) - A(t) sin(8(t)) sin(27rFot)</p>
<p/>
</div>
<div class="page"><p/>
<p>692 CHAPTER 20. GAUSSIAN RANDOM PROCESSES
</p>
<p>Px(F)
</p>
<p>- - - - - - + - - - J I I - - + - - ~ ~ - - - - ' " " ' r _ - .. F
</p>
<p>-Fo
</p>
<p>Figure 20.9: Typical PSD for bandpass random process. The PSD is assumed to be
</p>
<p>symmetric about F = Fo and also that Fo &gt; W /2.
</p>
<p>where A(t) and 8(t) are now random processes. As before we let
</p>
<p>U(t) = A(t) cos(8(t))
</p>
<p>V(t) = A(t) sin(8(t))
</p>
<p>so that we have as our model for a bandpass random process
</p>
<p>X(t) = U(t) cos(21rFot) - V(t) sin(21rFot). (20.19)
</p>
<p>The X(t) random process is seen to be a modulated version of U(t) and V(t) (mod-
</p>
<p>ulation meaning that U(t) and V(t) are multiplied by cos(21rFot) and sin(21rFot),
</p>
<p>resp ectively). This modulation shifts the PSD of U(t) and V(t) to be centered about
</p>
<p>F = Fo. Therefore, U(t) and V(t) must be slowly varying or lowpass random pro-
cesses. As a suitable description of U(t) and V(t) we assume that they are each zero
</p>
<p>mean lowpass Gaussian random processes, independent of each other, and jointly
</p>
<p>WSS (see Chapter 19) with the same ACF, rU(T) = rv(T). Then, as before X(t) is
a zero mean Gaussian random process, which as we now show is also WSS. Clearly,
</p>
<p>since both U(t) and V(t) are zero mean, from (20.19) so is X(t), and the ACF is
</p>
<p>rX(T) E[X(t)X(t + T)] (20.20)
E[[U(t) cos (21rFot) - V(t) sin(21rFot)]
</p>
<p>&middot;[U(t + T) cos(21rFo(t + T)) - V(t + T) sin(21rFo(t + T))]
rU(T) cos(21rFot) cos(21rFo(t + T)) + rV(T) sin(21rFot) sin(21rFo(t + T))
</p>
<p>= rU(T) cos(21rFoT) (20.21)
</p>
<p>since E[U(tt}V(t2)] = 0 for all tl and t2 due to the independence assumption, and
rU(T) = rV(T) by assumption. Note that this extends the previous case in which
</p>
<p>U(t) = U and V(t) = V and rU(T) = (}2 (see (20.17)) . The PSD is found by taking
</p>
<p>the Fourier transform of the ACF to yield
</p>
<p>1 1
Px(F) = -Pu(F + Fo) + -Pu(F - Fo).
</p>
<p>2 2
(20.22)</p>
<p/>
</div>
<div class="page"><p/>
<p>20.7. SPECIAL CONTINUOUS-TIME GAUSSIAN RANDOM PROCESSES 693
</p>
<p>If U(t) and V(t) have the lowpass PSD shown in Figure 20.10, then in accordance
</p>
<p>with (20.22) Px(F) is given by the dashed curve . As desired, we now have a repre-
</p>
<p>Pu(F) = Pv(F)
</p>
<p>-, ,, ,, ,, ,, ,
</p>
<p>Px(F)
</p>
<p>/, ,, ,, ,, ,, ,
-Fo - ~ w""2 Fo
</p>
<p>Figure 20.10: PSD for lowpass random processes U(t) and V(t). The PSD for the
</p>
<p>bandpass random process X(t) is shown as the dashed curve.
</p>
<p>sentation for a bandpass random process. It is obtained by modulating two lowpass
</p>
<p>random processes U(t) and V(t) up to a center frequency of Fo Hz. Hence, (20.19)
</p>
<p>is called the bandpass random process representation and since the random process
</p>
<p>may either represent a signal or noise, it is also referred to as the bandpass signal
</p>
<p>representation or the bandpass noise representation. Note that because Pu(F) is
</p>
<p>symmetric about F = 0, Px(F) must be symmetric about F = Fo. To represent
bandpass PSDs that are not symmetric requires the assumption that U(t) and V(t)
</p>
<p>are correlated [Van Trees 1971].
</p>
<p>In summary, to model a WSS Gaussian random process X(t) that has a zero
</p>
<p>mean and a bandpass PSD given by
</p>
<p>where Pu(F) = 0 for IFI &gt; Wj2 as shown in Figure 20.10 by the dashed curve, we
use
</p>
<p>X(t) = U(t) cos(27fFot) - V(t) sin(27fFot).
</p>
<p>The assumptions are that U(t), V(t) are each Gaussian random processes with zero
</p>
<p>mean, independent of each other and each is WSS with PSD Pu(F). The random
</p>
<p>processes U(t), V(t) are lowpass random processes and are sometimes referred to
</p>
<p>as the in phase and quadrature components of X(t) . This is because the "carrier"
</p>
<p>sinusoid cos(27fFot) is in phase with the sinusoidal carrier in U(t) cos(27fFot) and 900
</p>
<p>out of phase with the sinusoidal carrier in V(t) sin(27fFot). See Problem 20.24 on how
</p>
<p>to extract the lowpass random processes from X(t). In addition, the amplitude of
</p>
<p>X(t) , which is JU2(t) + V2(t) is called the envelope of X(t). This is because if X(t)
is written as X(t) = JU2(t) + V2(t) cos(27fFot + arctan(V(t)jU(t))) (see Problem
12.42) the envelope consists of the maximums of the waveform. An example of a</p>
<p/>
</div>
<div class="page"><p/>
<p>694 CHAPTER 20. GAUSSIAN RANDOM PROCESSES
</p>
<p>deterministic bandpass signal, given for sake of illustration, is s(t) = 3t cos(27r20t) -
</p>
<p>4t sin(27r20t) for 0 ::; t ::; 1, and is shown in Figure 20.11. Note that the envelope
</p>
<p>is V(3t)2 + (4t)2 = 51tl. For a bandpass random process the envelope will also be a
</p>
<p>... .. . . . . . . ....
</p>
<p>" ...
</p>
<p>5,------.------,-----.--------,----..,
</p>
<p>4
</p>
<p>0.80.4 0.6
t (sec)
</p>
<p>0.2
</p>
<p>3 ~
</p>
<p>: "",
2 " ..
</p>
<p>. ""
</p>
<p>~+v\}tf&middot; &middot; &middot; ..
-1 .....:V V .
</p>
<p>-2
</p>
<p>-3 .
</p>
<p>-4
</p>
<p>-5 '--__----'-__-' --'--__----l.__--'-'
</p>
<p>o
</p>
<p>Figure 20.11: Plot of the deterministic bandpass signal s(t) = 3tcos(27r20t) -
4t sin(27r20t) for 0 ::; t ::; 1. The envelope is shown as the dashed line.
</p>
<p>random process. Since U(t) and V(t) both have the same ACF, the characteristics of
</p>
<p>the envelope depend directly on rU(T). An illustration is given in the next example.
</p>
<p>Example 20.8 - Bandpass random process envelope
</p>
<p>Consider the bandpass Gaussian random process whose PSD is shown in Figure
</p>
<p>20.12. This is often used as a model for bandpass "white" Gaussian noise. It results
</p>
<p>from having filtered WGN with a bandpass filter. Note that from (20.22) the PSD
</p>
<p>Px(F)
</p>
<p>!!.sl.
2
</p>
<p>-Fo Fo
</p>
<p>Fo - If Fo+ If
</p>
<p>Figure 20.12: PSD for bandpass "white" Gaussian noise.</p>
<p/>
</div>
<div class="page"><p/>
<p>20.7. SPECIAL CONTINUOUS-TIME GAUSSIAN RANDOM PROCESSES 695
</p>
<p>of U(t) and V(t) must be
</p>
<p>{
</p>
<p>No
Pu(F) = Pv(F) = 0
</p>
<p>and therefore by taking the inverse Fourier transform, the ACF becomes (see also
</p>
<p>(17.55) for a similar calculation)
</p>
<p>sin(7rWT)
rU(T) = rV(T) = NoW W . (20.23)
</p>
<p>7r T
</p>
<p>The correlation between two samples of the envelope will be approximately zero when
</p>
<p>T &gt; l/W since then rU(T) = rV(T) ~ O. Examples of some bandpass realizations
are shown for Fo = 20 Hz, W = 1 Hz in Figure 20.13a and for Fo = 20 Hz, W = 4
Hz in Figure 20.13b. The time for which two samples must be separated before they
</p>
<p>become uncorrelated is called the correlation time Te . It is defined by rX(T) ~ 0 for
</p>
<p>T &gt; Te. Here it is Tc ~ l/W, and is shown in Figure 20.13.
</p>
<p>20 20
</p>
<p>:. 1 .: -1JrI-W
10 . 10 . .
</p>
<p>,.-... ,.-........, .....,
"--" "--"
</p>
<p>t-1 0 t-1 0
</p>
<p>-10 -10 .
</p>
<p>-20 -20
0 0.5 1 1.5 2 0 0.5 1 1.5 2
</p>
<p>t (sec) t (sec)
</p>
<p>(a) Fo = 20 Hz, W = 1 Hz (b) Fo = 20 Hz, W = 4 Hz
</p>
<p>Figure 20.13: Typical realizations of bandpass "white" Gaussian noise. The PSD is
</p>
<p>given in Figure 20.12.
</p>
<p>A typical probability calculation might be to determine the probability that the
</p>
<p>envelope at t = to exceeds some threshold 'Y. Thus, we wish to find P[A(to) &gt; 'Y],
where A(to) = JU2(tO) + V2(tO). Since the U(t) and V(t) are independent Gaussian
random processes with U(t) '" N(O, a2) and V(t) '" N(O,a2), it follows that A(to)
is a Rayleigh random variable. Hence, we have that
</p>
<p>P[A(to) &gt; 'Y] = 100 ~ exp (_l~) da
a 2 217
</p>
<p>'Y
</p>
<p>exp (-~~) .</p>
<p/>
</div>
<div class="page"><p/>
<p>696 CHAPTER 20. GAUSSIAN RANDOM PROCESSES
</p>
<p>To complete the calculation we need to determine 0-2 &bull; But 0-2 = E[U 2(to)] = ru[O] =
NoW from (20.23). Therefore, we have that
</p>
<p>P[A(to) &gt; ,] = exp ( -~ N~~ ) .
</p>
<p>20.8 Computer Simulation
</p>
<p>We now discuss the generation of a realization of a discrete-time Gaussian ran-
</p>
<p>dom process. The generation of a continuous-time random process realization can
</p>
<p>be accomplished by approximating it by a discrete-time realization with a suffi-
</p>
<p>ciently small time interval between samples. We have done this to produce Figure
</p>
<p>20.6 (see also Problem 20.18). In particular, we wish to generate a realization
</p>
<p>of a WSS Gaussian random process with mean zero and ACS rx[k] or equiva-
</p>
<p>lently a PSD Px (f). For nonzero mean random processes we need only add the
mean to the realization. The method is based on Theorem 20.4.1, where we use a
</p>
<p>WGN random process Urn] as the input to an LSI filter with frequency response
H(f). Then, we know that the output random process will be WSS and Gaus-
</p>
<p>sian with a PSD Px(f) = IH (f)1 2Pu(f). Now assuming that Pu(f) = o-~ = 1,
so that Px(f) = IH(fW , we see that a filter whose frequency response magni-
tude is IH(f)1 = Jpx(f) and whose phase response is arbitrary (but must be
</p>
<p>an odd function) will be required. Finding the filter frequency response from the
</p>
<p>PSD is known as spectral factorization [Priestley 1981]. As special cases of this
</p>
<p>problem, if we wish to generate either the AR, MA, or ARMA Gaussian random
</p>
<p>processes described in Section 20.4, then the filters are already known and have
</p>
<p>been implemented as difference equations. For example, the MA random pro-
</p>
<p>cess is generated by filtering Urn] with the LSI filter whose frequency response
is H(f) = 1 - bexp(-j271-j) . This is equivalent to the implementation using the
</p>
<p>difference equation X[n] = Urn] - bUrn - 1]. For higher-order (more coefficients)
AR, MA, and ARMA random processes, the reader should consult [Kay 1988] for
</p>
<p>how the appropriate coefficients can be obtained from the PSD. Also, note that the
</p>
<p>problem of designing a filter whose frequency response magnitude approximates a
</p>
<p>given one is called digital filter design. Many techniques are available to do this
</p>
<p>[Jackson 1996]. We next give a simple example of how to generate a realization of
</p>
<p>a WSS Gaussian random process with a given PSD.
</p>
<p>Example 20.9 - Filter determination to produce Gaussian random pro-
</p>
<p>cess with given PSD
</p>
<p>Assume we wish to generate a realization of a WSS Gaussian random process with
</p>
<p>zero mean and PSD Px(f) = (1+cos(41rJ))/2. Then, for Pu(f) = 1 the magnitude
of the frequency response should be
</p>
<p>IH(f)1 = J~(1 + cos(41rJ)).</p>
<p/>
</div>
<div class="page"><p/>
<p>20.8. COMPUTER SIMULATION 697
</p>
<p>We will choose the phase response or L.H(J) = O(J) to be any convenient function.
</p>
<p>Thus, we wish to determine the impulse response h[k] of the filter whose frequency
</p>
<p>response is
</p>
<p>H(J) = J~(1 + cos(47fJ)) exp(jO(J))
</p>
<p>since then we can generate the random process using a convolution sum as
</p>
<p>00
</p>
<p>X[n] = L h[k]U[n - k].
k=-oo
</p>
<p>(20.24)
</p>
<p>The impulse response is found as the inverse Fourier transform of the frequency
</p>
<p>response
</p>
<p>1
</p>
<p>h[n] = i: H(J)exp(j27fjn)dj
2
</p>
<p>1i: J~(1 + cos(47fJ)) exp(jO(J)) exp(j27fjn)dj
2
</p>
<p>- 00 &lt; n &lt; 00.
</p>
<p>This can be evaluated by noting that cos(2a) = cos2(a) - sin2(a) and therefore
</p>
<p>Thus,
</p>
<p>J ~ ( 1 + cos(47fJ)) J~(1 + cos2(27fJ) - sin2(27fJ))
</p>
<p>J cos2 (27fJ)
1 cos(27fJ) I&middot;
</p>
<p>1
</p>
<p>h[n] = i: Icos(27fJ)I exp(jO(J)) exp(j27fjn)dj
2
</p>
<p>and we choose exp(jO(J)) = 1 if cos(27fJ) &gt; 0 and exp(jO(J)) = -1 if cos(27fJ) &lt; O.
This produces
</p>
<p>1
</p>
<p>h[n] = i: cos(27fJ) exp(j27f jn)dj
2
</p>
<p>which is easily shown to evaluate to
</p>
<p>h[n] = { ~
</p>
<p>Hence, from (20.24) we have that
</p>
<p>n= &plusmn;1
</p>
<p>otherwise.
</p>
<p>1 1
X[n] = "2U[n + 1] + "2U[n - 1].</p>
<p/>
</div>
<div class="page"><p/>
<p>698 CHAPTER 20. GAUSSIAN RANDOM PROCESSES
</p>
<p>Note that the filter is noncausal. We could also use X[n] = ~U[n] + ~U[n - 2]
if a causal filter is desired and still obtain the same PSD (see Problem 20.28).
</p>
<p>o
Finally, it should be pointed out that an alternative means of generating successive
</p>
<p>samples of a zero mean Gaussian WSS random process is by applying a matrix
</p>
<p>transformation to a vector of independent N(O, 1) samples. If a realization of X =
</p>
<p>[X [0] X[l] ... X[N - l]V, where X '" N(O, Rx) and Rx is the N x N Toeplitz
autocorrelation matrix given in (17.23) is desired, then the method described in
</p>
<p>Section 14.9 can be used. We need only replace C by Rx. For a nonzero mean WSS
</p>
<p>Gaussian random process, we add the mean J.L to each sample after this procedure
</p>
<p>is employed. The only drawback is that the realization is assumed to consist of
</p>
<p>a fixed number of samples N, and so for each value of N the procedure must be
</p>
<p>repeated. Filtering, as previously described, allows any number of samples to be
</p>
<p>easily generated.
</p>
<p>20.9 Real-World Example - Estimating Fish Popula-
</p>
<p>tions
</p>
<p>Of concern to biologists, and to us all, is the fish population. Traditionally, the
</p>
<p>population has been estimated using a count produced by a net catch. However,
</p>
<p>this is expensive, time consuming, and relatively inaccurate. A better approach
</p>
<p>is therefore needed. In the introduction we briefly indicated how an echo sonar
</p>
<p>would produce a Gaussian random process as the reflected waveform from a school
</p>
<p>of fish. We now examine this in more detail and explain how estimation of the fish
</p>
<p>population might be done. The discussion is oversimplified so that the interested
</p>
<p>reader may consult [Ehrenberg and Lytle 1972, Stanton 1983, Stanton and Chu
</p>
<p>1998] for more detail. Referring to Figure 20.14 a sound pulse, which is assumed
</p>
<p>to be sinusoidal, is transmitted from a ship. As it encounters a school of fish, it
</p>
<p>will be reflected from each fish and the entire waveform, which is the sum of all the
</p>
<p>reflections, will be received at the ship. The received waveform will be examined
</p>
<p>for the time interval from t = 2Rmin/c to t = 2Rmax/c, where Rmin and R max are
the minimum and maximum ranges of interest, respectively, and c is the speed of
</p>
<p>sound in the water. This corresponds to the time interval over which the reflections
</p>
<p>from the desired ranges will be present. Based on the received waveform we wish
</p>
<p>to estimate the number of fish in the vertical direction in the desired range window
</p>
<p>from Rmin to R max. Note that only the fish within the nearly dashed vertical lines,
</p>
<p>which indicate the width of the transmitted sound energy, will produce reflections.
</p>
<p>For different angular regions other pulses must be transmitted. As discussed in the
</p>
<p>introduction, if there are a large number of fish producing reflections, then by the
</p>
<p>central limit theorem, the received waveform can be modeled as a Gaussian random
</p>
<p>process. As shown in Figure 20.14 the sinusoidal pulse first encounters the fish
</p>
<p>nearest in range, producing a reflection, while the fish farthest in range produces</p>
<p/>
</div>
<div class="page"><p/>
<p>20.9. REAL-WORLD EXAMPLE - ESTIMATING FISH POPULATIONS 699
</p>
<p>the last reflection. As a result, the many reflected pulses will overlap in time, with
</p>
<p>two of the reflected pulses shown in the figure. Hence, each reflected pulse can be
</p>
<p>Figure 20.14: Fish counting by echo sonar.
</p>
<p>represented as
</p>
<p>(20.25)
</p>
<p>where Fo is the transmit frequency in Hz and Ti = 2I4./c is the time delay of the
</p>
<p>pulse reflected from the i th fish. As explained in the introduction, since Ai, ei will
depend upon the fish 's position, orientation, and motion, which are not known a
</p>
<p>priori, we assume that they are realizations of random variables. Futhermore, since
</p>
<p>the ranges of the individual fish are unknown, we also do not know Ti. Hence , we
</p>
<p>replace (20.25) by
</p>
<p>where e ~ = e i - 211"FoTi (which is reduced by multiples of 211" until it lies within the
interval (0,211")) , and model e ~ as a new random variable. Hence , for N reflections
</p>
<p>we have as our model
</p>
<p>N
</p>
<p>X(t) LXi(t)
i= l
</p>
<p>N
</p>
<p>L Ai cos(211"Fot + eD
i=l</p>
<p/>
</div>
<div class="page"><p/>
<p>700 CHAPTER 20. GAUSSIAN RANDOM PROCESSES
</p>
<p>and letting U; = Ai cos(8D and Vi = Ai sin(8~), we have
</p>
<p>N
</p>
<p>X(t) = 2)Ui cos(21fFot) - Vi sin(21fFot))
i=l
</p>
<p>(tU;) cos(2"Fot) ~ (tVi) sin(2"Fot)
U cos(21fFot) - V sin(21fFot)
</p>
<p>where U = I:I::1 U, and V = I:I::1 Vi. We assume that all the fish are about the
same size and hence the echo amplitudes are about the same. Then, since U and V
</p>
<p>are the sums of random variables that we assume are independent (reflection from
</p>
<p>one fish does not affect reflection from any of the others) and identically distributed
</p>
<p>(fish are same size), we use a central limit theorem argument to postulate a Gaussian
</p>
<p>PDF for U and V. We furthermore assume that U and V are independent so that if
</p>
<p>E[Ui] = E[Vi] = 0 and var(Ui) = var(Vi) = a 2 , then we have that U '" N(o, N a 2 ) ,
V", N(o, Na2 ) , and U and V are independent. This is the Rayleigh fading sinusoid
model discussed in Section 20.7. As a result , the envelope of the received waveform
</p>
<p>X(t), which is given by A = v'U2 + V2 has a Rayleigh PDF. Specifically, it is
</p>
<p>PA (a) = { N~2 exp ( - ~ ;;2) a ~ 0
o a &lt; O.
</p>
<p>Hence, if we have previously measured the reflection characteristics of a single fish,
</p>
<p>then we will know a2 &bull; To estimate N we recall that the mean of the Rayleigh
</p>
<p>random variable is
</p>
<p>E[A] = ViNa2
so that upon solving for N, we have
</p>
<p>To estimate the mean we can transmit a series of M pulses and measure the en-
</p>
<p>velope for each received waveform Xm(t) for m = 1,2 ... ,M. Calling the envelope
</p>
<p>measurement for the mth pulse Am, we can form the estimator for the number of
</p>
<p>fish as
</p>
<p>(20.26)
</p>
<p>See Problem 20.20 on how to obtain Am = JUJ." + V~ from Xm(t). It is shown
there that Um = [2Xm(t) cos(21fFot)]LPF and Vm = [-2Xm(t) sin(21fFot)]LPF, where
the designation "LPF" indicates that the time waveform has been lowpass filtered.</p>
<p/>
</div>
<div class="page"><p/>
<p>REFERENCES
</p>
<p>References
</p>
<p>701
</p>
<p>Bell Telephone Laboratories, Transmission Systems for Communications, Western
</p>
<p>Electric Co, Winston-Salem, NC, 1970.
</p>
<p>Ehrenberg, J.E. , D.W. Lytle, "Acoust ic Techniques for Estimating Fish Abun-
</p>
<p>dance," IEEE Trans. Geoscience Electronics , pp. 138-145, 1972.
</p>
<p>Jackson, L.B., Digital Filters and Signal Processing: with MATLAB Exercises, 3rd
</p>
<p>Ed. , Kluwer Academic Press, New York, 1996.
</p>
<p>Kay, S., Modern Spectral Estimation: Theory and Application, Prentice-Hall, En-
</p>
<p>glewood Cliffs, NJ , 1988.
</p>
<p>Priestley, M.B., Spectral Analysis and Time Series, Academic Press, New York,
</p>
<p>1981.
</p>
<p>Stanton, T.K., "Multiple Scattering with Application to Fish-Echo Processing,"
</p>
<p>Journal Acoustical Soc. of America, pp. 1164-1169, April1983.
</p>
<p>Stanton, T.K. , D. Chu, "Sound Scattering by Several Zooplankton Groups. II.
</p>
<p>Scattering Models, " Journal Acoustical Soc. of America, pp. 236-253, Jan.
</p>
<p>1998.
</p>
<p>Taylor, S., Modelling Financial Time Series, John Wiley &amp; Sons, New York, 1986.
</p>
<p>Urick , RJ., Principles of Underwater Sound, McGraw-Hill, New York, 1975.
</p>
<p>Van Trees, H.L. , Detection, Estimation, and Modulation Theory , Part III, John
</p>
<p>Wiley &amp; Sons , New York, 1971.
</p>
<p>Problems
</p>
<p>20.1. (w) Determine the probability that 5 successive samples {X[O], X[I], X[2], X [3],
X[4]} of discrete-time WGN with (Jb = 1 will all exceed zero. Then, repeat
the problem if the samples are {X[lO], X[l1], X[12], X[13],X[14]}.
</p>
<p>20.2 c.:.:J (w) If X[n] is the random process described in Example 20.2, find P[X[O] &gt;
0, X[3] &gt; 0] if (Jb = 1.
</p>
<p>20.3 (w) If X[n] is a discrete-time Wiener random process with var(X[n]) = 2(n+
1), determine P [-3 ::; X[5] ::; 3].
</p>
<p>20.4 (w) A discrete-time Wiener random process X[n] is input to a differencer to
generate the output random process Y[n] = X[n] - X[n - 1]. Describe the
characteristics of the output random process.</p>
<p/>
</div>
<div class="page"><p/>
<p>702 CHAPTER 20. GAUSSIAN RANDOM PROCESSES
</p>
<p>20.5 (..:....:..-) (w) If discrete-time WGN X[n] with 01- = 1 is input to a differencer to
generate the output random process Y[n] = X[n] - X[n - 1], find the PDF of
the samples Y[O] ,Y[I]. Are the samples independent?
</p>
<p>20.6 (w) If in Example 20.4 the input random process to the differencer is an
</p>
<p>AR random process with parameters a and o ~ = 1, determine the PDF of
</p>
<p>Y[O], Y[I]. What happens as a -+ 17 Explain your results.
</p>
<p>20.7 (t) In this problem we argue that if X[n] is a Gaussian random process
that is input to an LSI filter so that the output random process is Y[n] =
2:~- oo h[i]X[n - i], then Y[n] is also a Gaussian random process. To do
</p>
<p>so consider a finite impulse response filter so that Y[n] = 2:{::J h[i]X[n - i]
with I = 4 (the infinite impulse response filter argument is a bit more com-
plicated but is similar in nature) and choose to test the set of output samples
</p>
<p>nl = 0, n2 = 1, n3 = 2 so that K = 3 (again the more general case pro-
ceeds similarly). Now prove that the output samples have a 3-dimensional
</p>
<p>Gaussian PDF. Hint: Show that the samples of Y[n] are obtained as a linear
transformation of X[n].
</p>
<p>20.8 (w) A discrete-time WGN random process is input to an LSI filter with system
function 1i(z) = z - z- I . Determine the PDF of the output samples Y[n] for
n = 0,1 , . .. , N - 1. Are any of these samples independent of each other?
</p>
<p>20.9 (t) In this problem we prove that if X[n] and Y[n] are both Gaussian random
processes that are independent of each other, then Z[n] = X[n] +Y[n] is also a
Gaussian random process. To do so we prove that the characteristic function
</p>
<p>of Z = [Z[nl] Z[n2] ... Z[nK]V is that of a Gaussian random vector. First not e
that since X = [X[nl] X[n2]' " X[nK]V and Y = [Y[nl] Y[n2]'" Y[nK]]T are
both Gaussian random vectors (by definition of a Gaussian random process) ,
</p>
<p>then each one has the characteristic function
</p>
<p>&lt;/&gt;(w) = exp (jwTJ.L - ~wT Cw )
</p>
<p>where w = [WI W2... WKV. Next use the property that the characteristic func-
tion of a sum of independent random vectors is the product of the characteristic
</p>
<p>functions to show that Z has a K-dimensional Gaussian PDF.
</p>
<p>20.10 (..:....:..-) (w) Let X[n] and Y[n] be WSS Gaussian random processes with zero
mean and independent of each other. It is known that Z[n] = X[n]Y[n] is not
a Gaussian random process. However, can we say that Z[n] is a WSS random
process, and if so, what is its mean and PSD?
</p>
<p>20.11 (w) An AR random process is described by X[n] = ~X[n -1] +Urn], where
Urn] is WGN with o ~ = 1. This random process is input to an LSI filter with
system function 1i(z) = 1- ~ z - I to generate the output random process Y[n].
Find P[Y 2[0] + y 2 [1] &gt; 1]. Hint: Consider X[n] as the output of an LSI filter.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 703
</p>
<p>= -4&gt;x(w)Lk
</p>
<p>20.12 (t) We prove (20.11) in this problem by using the method of characteris-
tic functions. Recall that for a multivariate zero mean Gaussian PDF the
</p>
<p>characteristic function is
</p>
<p>4&gt;x(w) = exp (_~wTCw)
</p>
<p>and the fourth-order moment can be found using (see Section 14.6)
</p>
<p>f)44&gt;X(w) I
E[X1X2X3X4] = f) f) f) f) .
</p>
<p>WI W2 W3 W4 w=o
</p>
<p>Although straightforward, the algebra is tedious (see also Example 14.5 for the
</p>
<p>second-order moment calculations). To avoid frustration (with P[frustration] =
1) note that
</p>
<p>4 4
</p>
<p>wTCw = LLWiWjE[XiXj]
i=1 j=1
</p>
<p>and let L; = 2:J=1 WjE[XiXj]. Next show that
</p>
<p>f)4&gt;x(w)
</p>
<p>f)Wk
</p>
<p>ei;
</p>
<p>f)wk
</p>
<p>and finally note that Lilw=o = 0 to avoid some algebra in the last differenti-
ation.
</p>
<p>20.13 (w) It is desired to estimate rx[O] for X[n] being WGN. If we use the esti-
mator, rx[O] = (liN) 2:;;';;01X 2 [n], determine the mean and variance ofrx[O].
Hint: Use (20.13).
</p>
<p>20.14 C:..:..-) (f) If X[n] = Urn] + urn - 1], where Urn] is a WGN random process
with a ~ = 1, find E[X[0]X[1]X[2]X[3]].
</p>
<p>20.15 (f) Find the PSD of X 2[n] if X[n] is WGN with ai- = 2.
20.16 (t) To argue that the continuous-time Wiener random process is a Gaussian
</p>
<p>random process, we replace X(t) = J~ U(e)de, where U(O is continuous-time
WGN, by the approximation
</p>
<p>[ t / ~ t ]
</p>
<p>X(t) = L Z(nb.t)b.t
n=O
</p>
<p>where [x] indicates the largest integer less than or equal to x and Z(t) is a
</p>
<p>zero mean WSS Gaussian random process. The PSD of Z(t) is given by
</p>
<p>P (F) = {l!f IFI ~ w
z 0 IFI &gt; W</p>
<p/>
</div>
<div class="page"><p/>
<p>704 CHAPTER 20. GAUSSIAN RANDOM PROCESSES
</p>
<p>where W = 1/(2~t). Explain why X(t) is a Gaussian random process. Next
let ~t -+ 0 and explain why X(t) becomes a Wiener random process.
</p>
<p>20 .17 c.:.:,,) (w) To extract A from a realization of the random process X(t) =
A + U(t), where U(t) is WGN with PSD Pu(F) = 1 for all F, it is proposed
to use
</p>
<p>A. = ~ t' X(Ode.
T 10
</p>
<p>How large should T be chosen to ensure that P[IA. - AI :s; 0.01] = 0.99?
</p>
<p>20.18 (w) To generate a realization of a continuous-time Wiener random process on
a computer we must replace the continuous-time random process by a sampled
</p>
<p>approximation. To do so note that we can first describe the Wiener random
</p>
<p>process by breaking up the integral into integrals over smaller time intervals.
</p>
<p>This yields
</p>
<p>X(t) It U(Ode
~ l it~l U(e)de
- '-v-"
</p>
<p>Xi
</p>
<p>where ti = i~t with ~ t very small, and t&laquo; = n~t = t. It is assumed that t] ~t
is an integer. Thus, the samples of X(t) are conveniently found as
</p>
<p>n
</p>
<p>X(tn ) = X(n~t) = LXi
i=l
</p>
<p>and the approximation is completed by connecting the samples X(tn ) by
</p>
<p>straight lines. Find the PDF of the Xi 'S to determine how they should be
</p>
<p>generated. Hint: The Xi'S are increments of X(t).
</p>
<p>20.19 C..:,,) (f) For a continuous-time Wiener random process with var(X(t&raquo; = t,
determine P[IX(t)1 &gt; 1]. Explain what happens as t -+ 00 and why.
</p>
<p>20.20 (w) Show that if X(t) is a Rayleigh fading sinusoid, the "demodulation" and
lowpass filtering shown in Figure 20.15 will yield U and V, respectively. What
</p>
<p>should the bandwidth of the lowpass filter be?
</p>
<p>20.21 (c) Generate 10 realizations of a Rayleigh fading sinusoid for 0 :s; t :s; 1. Use
Fo = 10 Hz and (72 = 1 to do so. Overlay your realizations. Hint: Replace
X(t) = U cos(21l"Fot) - V sin(21l"Fot) by X[n] = X(n~t) = U cos(21l"Fon~t) -
V sin(21l" Fon~t) for n = 0,1, ... ,N ~ t , where ~ t = 1/Nand N is large.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 705
</p>
<p>X(t) Lowpass U
X(t) Lowpass V
</p>
<p>filter filter
</p>
<p>H(F) H(F)
</p>
<p>2COS(
21rF::m-. -2Sin(21r~
</p>
<p>w w F w w F
-2 2 -2 2
</p>
<p>(a) (b)
</p>
<p>Figure 20.15: Extraction of Rayleigh fading sinusoid lowpass components for Prob-
</p>
<p>lem 20.20.
</p>
<p>20.22 ( . ~ ) (w) Consider Xl (t) and X 2(t ), which are both Rayleigh fading sinusoids
</p>
<p>with frequency Fa = 1/2 and which are independent of each other. Each
</p>
<p>random process has the total average power (]"2 = 1. If Y (t) = X I (t) + X 2 (t) ,
find the joint PDF of Y(O) and Y(I/4).
</p>
<p>20.23 (f) A Rayleigh fading sinusoid has the PSD Px(F) = 8(F + 10) +8(F -10).
Find the PSDs of U(t) and V(t) and plot them.
</p>
<p>20.24 (w) Show that if X(t) is a bandpass random process, the "demodulat ion" and
lowpass filtering given in Figure 20.16 will yield U(t) and V(t), respectively.
</p>
<p>X(t)
Lowpass 1--_.
</p>
<p>filter
</p>
<p>H(F)
</p>
<p>2 cos(27fFat) r-fl,.
</p>
<p>--=&plusmn;-t-i- F-2 2
(a)
</p>
<p>U(t) X(t) Lowpass 1"--.. V (t)
filter
</p>
<p>H(F)
</p>
<p>- 2 sin(27fFat)r-fl,
</p>
<p>--=&plusmn;-t-i- F-2 2
(b)
</p>
<p>Figure 20.16: Extraction of bandpass random process lowpass components for Prob-
</p>
<p>lem 20.24.
</p>
<p>20.25 (..:..:..-) (f) If a bandpass random process has the PSD shown in Figure 20.17,
find the PSD of U(t) and V(t).</p>
<p/>
</div>
<div class="page"><p/>
<p>706 CHAPTER 20. GAUSSIAN RANDOM PROCESSES
</p>
<p>Px(F)
</p>
<p>4
</p>
<p>F
</p>
<p>90 110
</p>
<p>Figure 20.17: PSD for bandpass random process for Problem 20.25.
</p>
<p>20.26 (c) The random process whose realization is shown in Figure 20.2 appears
</p>
<p>to be similar in nature to the bandpass random processes shown in Figure
</p>
<p>20.13b. We have already seen that the marginal PDF appears to be Gaussian
</p>
<p>(see Figure 20.3). To see if it is reasonable to model it as a bandpass random
</p>
<p>process we estimate the PSD. First run the code given in Appendix 20A to
</p>
<p>produce the realization shown in Figure 20.2. Then, run the code given below
</p>
<p>to estimate the PSD using an averaged periodogram (see also Section 17.7
</p>
<p>for a description of this). Does the estimated PSD indicate that the random
</p>
<p>process is a bandpass random process? If so, explain how you can give a
</p>
<p>complete probabilistic model for this random process.
</p>
<p>Fs=100; %set sampling rate for later plotting
L=50;I=20; %L = length of block, I = number of blocks
n=[O:I*L-l], ; %set up time indices
Nfft=1024; %set FFT length for Fourier transform
Pav=zeros(Nfft,l);
</p>
<p>f=[O:Nfft-l] '/Nfft-O.5; %set discrete-time frequencies
for i=O:I-l
</p>
<p>nstart=l+i*L;nend=L+i*L; %set start and end time indices
%of block
</p>
<p>y=x(nstart:nend); %extract block of data
P a v = P a v + ( 1 / ( I * L ) ) * a b s ( f f t s h i f t ( f f t ( y , N f f t ) ) ) . ~ 2 ;
</p>
<p>%compute periodogram
%and add to average
%of periadograms
</p>
<p>end
</p>
<p>F=f*Fs; %convert to continuous-time (analog) frequency in Hz
Pest=Pav/Fs; %convert discrete-time PSD to continuous-time PSD
plot(F,Pest)</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS
</p>
<p>20.27 (f) For the Gaussian random process with mean zero and PSD
</p>
<p>P (F) = {4 90::; I ~ I ::; 110
x 0 otherwise
</p>
<p>707
</p>
<p>find the probability that its envelope will be less than or equal to 10 at t = 10
seconds. Repeat the calculation if t = 20 seconds.
</p>
<p>20.28 (w) Prove that XI[n] = ~U[n+l]+~U[n-l] andX2[n] = ~U[n]+~U[n-2],
where Urn] is WGN with ab = 1, both have the same PSD given by Px(F) =
~ (1 + cos(4'1I}&raquo;.
</p>
<p>20.29 (w) It is desired to generate a realization of a WSS Gaussian random process
</p>
<p>by filtering WGN with an LSI filter. If the desired PSD is Px(f) = 11 -
</p>
<p>~ exp( - j27rJ) 12, explain how to do this.
</p>
<p>20.30 C:.:-) (w) It is desired to generate a realization of a WSS Gaussian random
process by filtering WGN with an LSI filter. If the desired PSD is Px(f) =
</p>
<p>2 - 2 cos(27rJ), explain how to do this.
</p>
<p>20.31 c.:....:.,) (c) Using the results of Problem 20.30, generate a realization of X[n] .
To verify that your data generation appears correct, estimate the ACS for
</p>
<p>k = 0,1 , . . . , 9 and compare it to the theoretical ACS.</p>
<p/>
</div>
<div class="page"><p/>
<p>set up transmit pulse time interval
</p>
<p>for each 0.1 sec interval
</p>
<p>%time delay for each 0.1 sec interval
%is uniformly distributed - round
%time delay to integer
</p>
<p>Appendix 20A
</p>
<p>MATLAB Listing for Figure
</p>
<p>20.2
</p>
<p>clear all
rand ( , state' ,0)
</p>
<p>t=[0:0.01:0.99]'j %
FO=10j
</p>
<p>s=cos(2*pi*FO*t)j % transmit pulse
ss=[sjzeros(1000-length(s),1)]j %put transmit pulse in receive vindov
tt=[0:0.01:9.99]'j %set up receive vindov time interval
x=zeros(1000,1)j
</p>
<p>for i=1:100 %add up all echos, one
tau=round(10*i+l0*(rand(1,1)-O.5))j
</p>
<p>x=x+rand(l,l)*shift(ss,tau)j
</p>
<p>end
</p>
<p>shift .m subprogram
</p>
<p>This function SUbprogram shifts the given sequence by Ns points.
</p>
<p>Zeros are shifted in either from the left or right.
</p>
<p>Input parameters:
</p>
<p>x - array of dimension Lxl
</p>
<p>Ns - integer number of shifts
</p>
<p>% shift.m
</p>
<p>%
function y=shift(x,Ns)
</p>
<p>%
%
%
%
%
%
</p>
<p>% vhere Ns&gt;O means a shift to the</p>
<p/>
</div>
<div class="page"><p/>
<p>710 CHAPTER 20. GAUSSIAN RANDOM PROCESSES
</p>
<p>right and Ns&lt;O means a shift to the left and if Ns=O, then
</p>
<p>the sequence is not shifted
I.
I.
I.
% Output parameters:
% y - array of dimension Lx1 containing the
I. shifted sequence
L=length(x)j
</p>
<p>if abs(Ns&raquo;L
y=zeros(L,1)j
</p>
<p>else
if Ns&gt;O
</p>
<p>y (1 : Ns , 1) =0 j
</p>
<p>y(Ns+1:L,1)=x(1:L-Ns)j
</p>
<p>elseif Ns&lt;O
</p>
<p>y(L-abs(Ns)+1:L,1)=Oj
</p>
<p>y(1:L-abs(Ns),1)=x(abs(Ns)+1:L)j
</p>
<p>else
</p>
<p>y=Xj
</p>
<p>end
</p>
<p>end</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 21
</p>
<p>Poisson Random Processes
</p>
<p>21.1 Introduction
</p>
<p>A random process that is useful for modeling events occurring in time is the Poisson
</p>
<p>random process. A typical realization is shown in Figure 21.1 in which the events,
</p>
<p>indicated by the "x"s, occur randomly in time. The random process, whose real-
</p>
<p>N(t)
</p>
<p>5 ...-
I
</p>
<p>I
</p>
<p>4 &bull; ,I
I
</p>
<p>3 f--'
I
</p>
<p>2 , ,
1 &bull;I
</p>
<p>I
</p>
<p>t
</p>
<p>X X X X X ~
</p>
<p>Figure 21.1: Poisson process events and the Poisson counting random process N(t).
</p>
<p>ization is a set of times, is called the Poisson random process. The random process
</p>
<p>that counts the number of events in the time interval [0, t], and which is denoted
by N(t), is called the Poisson counting random process. It is clear from Figure
</p>
<p>21.1 that the two random processes are equivalent descriptions of the same random
</p>
<p>phenomenon. Note that N(t) is a continuous-time/discrete-valued (CTDV) random
</p>
<p>process. Also, because N(t) counts the number of events from the initial time t = 0
</p>
<p>up to and including the time t, the value of N(t) at a jump is N(t+). Thus, N(t) is
</p>
<p>right-continuous (the same property as for the CDF of a discrete random variable).
</p>
<p>The motivation for the widespread use of the Poisson random process is its ability</p>
<p/>
</div>
<div class="page"><p/>
<p>712 CHAPTER 21. POISSON RANDOM PROCESSES
</p>
<p>to model a wide range of physical and man-made random phenomena. Some of
</p>
<p>these are the distribution in time of radioactive counts, the arrivals of customers
</p>
<p>at a cashier, requests for service in computer networks, and calls made to a central
</p>
<p>location, to name just a few. In Chapter 5 we gave an example of the application
</p>
<p>of the Poisson PMF to the servicing of customers at a supermarket checkout. Here
</p>
<p>we examine the characteristics of a Poisson random process in more detail, paying
</p>
<p>particular attention not only to the probability of a given number of events in a time
</p>
<p>interval but also to the probability for the arrival times of those events. In order to
</p>
<p>avoid confusing the probabilistic notion of an event with the common usage, we will
</p>
<p>refer to the events shown in Figure 21.1 as arrivals.
</p>
<p>The Poisson random process is a natural extension of a sequence of independent
</p>
<p>and identically distributed Bernoulli trials (see Example 16.1). The Poisson counting
</p>
<p>random process N (t) then becomes the extension of the binomial counting random
</p>
<p>process discussed in Example 16.5. To make this identification, consider a Bernoulli
</p>
<p>random process, which is defined as a sequence of IID Bernoulli trials, with Urn] = 1
with probability p and Urn] = 0 with probability 1 - p. Now envision a Bernoulli
trial for each small time slot of width tlt in the interval [0, t] as shown in Figure
21.2. Thus, we will observe either a 1 with probability p or a 0 with probability
</p>
<p>1 -,..
</p>
<p>I I I
</p>
<p>I I
</p>
<p>t:.t 2t:.t
</p>
<p>I I I I
</p>
<p>I I I I
</p>
<p>t=Mt:.t
</p>
<p>k = 0,1, ... ,M.
</p>
<p>Figure 21.2: IID Bernoulli random process with one trial per time slot.
</p>
<p>1 - p for each of the M = tftlt time slots. Recall that on the average we will
observe Mp ones. Now if tlt -+ 0 and M -+ 00 with t = M tlt held constant, we
will obtain the Poisson random process as the limiting form of the Bernoulli random
</p>
<p>process. Also, recall that the number of ones in M IID Bernoulli trials is a binomial
</p>
<p>random variable. Hence, it seems reasonable that the number of arrivals in a Poisson
</p>
<p>random process should be a Poisson random variable in accordance with our results
</p>
<p>in Section 5.6. We next argue that this is indeed the case. For the binomial counting
</p>
<p>random process, thought of as one trial per time slot, we have that the number of
</p>
<p>ones in the interval [0,t] has the PMF
</p>
<p>P[N(t) = k] = (~) pk(l _ p)M-k
</p>
<p>But as M -+ 00 and p -+ 0 with E[N(t)] = Mp being fixed, the binomial PMF</p>
<p/>
</div>
<div class="page"><p/>
<p>21.2. SUMMARY 713
</p>
<p>becomes the Poisson PMF or N(t) '" Pois(A'), where A' = E[N(t)] = Mp . (Note
that as the number of time slots M increases, we need to let p -+ 0 in order to
</p>
<p>maintain an average number of arrivals in [0, t].) Thus, replacing A' by E[N(t)], we
write the Poisson PMF as
</p>
<p>P[N(t) = k] = exp( -E[N(t)]) E k [ ~ ( t ) ] k = 0,1, .... (21.1)
</p>
<p>To determine E[N(t)] for use in (21.1), where t may be arbitrary, we examine Mp
</p>
<p>in the limit. Thus,
</p>
<p>E[N(t)] = lim Mp
M-+ =
</p>
<p>P-+O
</p>
<p>1&middot; t 1&middot; pim -p=t 1m-
tl.t-+o bot tl.t-+O bot
P-+ o P-+o
</p>
<p>At
</p>
<p>where we define A as the limit of pibot. Since A = E[N(t)]lt, we can interpret A as
</p>
<p>the average number of arrivals per second or the rate of the Poisson random process.
</p>
<p>This is a parameter that is easily specified in practice. Using this definition we have
</p>
<p>that
(At)k
</p>
<p>P[N(t) = k] = exp(-At)~ k = 0,1, ... . (21.2)
</p>
<p>As mentioned previously, N (t) is the Poisson counting random process and the
</p>
<p>probability of k arrivals from t = 0 up to and including t is given by (21.2) . It is a
</p>
<p>semi-infinite random process with N(O) = 0 by definition.
It is possible to derive all the properties of a Poisson counting random process
</p>
<p>by employing the previous device of viewing it as the limiting form of a binomial
</p>
<p>counting random process as bot -+ O. However, it is cumbersome to do so and
</p>
<p>therefore, we present an alternative derivation that is consistent with the same basic
</p>
<p>assumptions. One advantage of viewing the Poisson random process as a limiting
</p>
<p>form is that many of its properties become more obvious by consideration of a
</p>
<p>sequence of lID Bernoulli trials. These properties are inherited from the binomial,
</p>
<p>such as, for example, the increments N(t2) - N(tt} must be independent. (Can you
explain why this must be true for the binomial counting random process?)
</p>
<p>21.2 Summary
</p>
<p>The Poisson counting random process is introduced in Section 21.1. The probability
</p>
<p>of k arrivals in the time interval [0, t] is given by (21.2). This probability is also
derived in Section 21.3 based on a set of axioms that the Poisson random process
</p>
<p>should adhere to. Some examples of typical problems for which this probability is
</p>
<p>useful are also described in that section. The times between arrivals or interarrival</p>
<p/>
</div>
<div class="page"><p/>
<p>714 CHAPTER 21. POISSON RANDOM PROCESSES
</p>
<p>times is shown in Section 21.4 to be independent and exponentially distributed as
</p>
<p>given by (21.6). The arrival times of a Poisson random process are described by an
</p>
<p>Erlang PDF given in (21.8). An extension of the Poisson random process that is
</p>
<p>useful is the compound Poisson random process described in Section 21.6. Moments
</p>
<p>of the random process can be found from the characteristic function of (21.12).
</p>
<p>In particular, the mean is given by (21.13). A Poisson random process is easily
</p>
<p>simulated on a computer using the MATLAB code listed in Section 21.7. Finally,
</p>
<p>an application of the compound Poisson random process to automobile traffic signal
</p>
<p>planning is the subject of Section 21.8.
</p>
<p>21.3 Derivation of Poisson Counting Random Process
</p>
<p>We next derive the Poisson counting random process by appealing to a set of axioms
</p>
<p>that are consistent with our previous assumptions. Clearly, since the random process
</p>
<p>starts at t = 0, we assume that N(O) = O. Next, since the binomial counting
random process has increments that are independent and stationary (Bernoulli trials
</p>
<p>are IID), we assume the same for the Poisson counting random process. Thus,
</p>
<p>for two increments we assume that the random variables h = N(t2) - N(tl) and
h = N(t4) - N(t3) are independent if t4 &gt; t3 &gt; t2 &gt; tl and also have the same
PDF if additionally t4- t3 = t2 - ti- Likewise, we assume this is true for all possible
</p>
<p>sets of increments. Note that t4 &gt; t3 &gt; t2 &gt; tl corresponds to nonoverlapping time
intervals. The increments will still be independent if t2 = t3 or the time intervals
</p>
<p>have a single point in common since the probability of N(t) changing at a point
</p>
<p>is zero as we will see shortly. As for the Bernoulli random process, there can be
</p>
<p>at most one arrival in each time slot. Similarly, for the Poisson counting random
</p>
<p>process we allow at most one arrival for each time slot so that
</p>
<p>P[N(t + ~t) - N(t) = k] = { pI - p k = 0
k=1
</p>
<p>and recall that
</p>
<p>lim L =,\
Ll.t-+o ~ t
p-tO
</p>
<p>so that for ~t small, p = '\~t and
</p>
<p>P[N(t + Ll&lt;t) - N(t) ~ k] = {
</p>
<p>Therefore, our axioms become
</p>
<p>Axiom 1 N(O) = O.
</p>
<p>1 - '\~t
</p>
<p>'\~t
</p>
<p>o
</p>
<p>k=O
k=1
k ? 2.
</p>
<p>Axiom 2 N(t) has independent and stationary increments.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.3. DERIVATION OF POISSON COUNTING RANDOM PROCESS 715
</p>
<p>. { 1 - Ab.t k = 0
AXIOm 3 P[N(t + b.t) - N(t) = k] = Ab.t k = 1
</p>
<p>for all t.
</p>
<p>With these axioms we wish to prove that (21.2) follows. The derivation is indica-
</p>
<p>tive of an approach commonly used for analyzing continuous-time Markov random
</p>
<p>processes [Cox and Miller 1965] and so is of interest in its own right.
</p>
<p>21.3.1 Derivation
</p>
<p>To begin, consider the determination of P[N(t) = 0] for an arbitrary t &gt; O. Then
referring to Figure 21.3a we see that for no arrivals in [0, t], there must be no arrivals
in [0, t - b.t] and also no arrivals in (t - b.t, t]. Therefore,
</p>
<p>oarrivals 1 arrival
oarrivals oarrivals 1 arrival oarrivals
</p>
<p>[ I ] [ I ]
0 t - b.t t 0 t - b.t t
</p>
<p>(a) N(t) = 0 (b) N(t) = 1
</p>
<p>Figure 21.3: Possible number of arrivals in indicated time intervals.
</p>
<p>P[N(t) = 0] P[N(t - b.t) = 0, N(t) - N(t - b.t) = 0]
P[N(t - b.t) = O]P[N(t) - N(t - b.t) = 0]
</p>
<p>P[N(t - b.t) = O]P[N(t+ b.t) - N(t) = 0]
P[N(t - b.t) = 0](1 - Ab.t)
</p>
<p>If we let Po(t) = P[N(t) = 0] , then
</p>
<p>Po(t) = Po(t - b.t)(l - Ab.t)
</p>
<p>or
</p>
<p>(Axiom 2 - independence)
</p>
<p>(Axiom 2 - stationarity)
</p>
<p>(Axiom 3).
</p>
<p>Po(t) - Po(t - b.t) = -APO(t _ b.t)
b.t .
</p>
<p>Now letting b.t -t 0, we arrive at the linear differential equation
</p>
<p>dPo(t) = -APo(t)
dt
</p>
<p>for which the solution is Po(t) = cexp(-At), where c is an arbitrary constant. To
</p>
<p>evaluate the constant we invoke the initial condition that Po(O) = P[N(O) = 0] = 1
by Axiom 1 to yield c = 1. Thus, we have finally that
</p>
<p>P[N(t) = 0] = Po(t) = exp( -At) .</p>
<p/>
</div>
<div class="page"><p/>
<p>716 CHAPTER 21. POISSON RANDOM PROCESSES
</p>
<p>Next we use the same argument to find a differential equation for PI (t) = P[N(t) =
1] by referring to Figure 21.3b. We can either have no arrivals in [0,t - .6.t] and one
arrival in (t - .6.t, t] or one arrival in [0,t - .6.t] and no arrivals in (t - .6.t, t]. These
are the only possibilities since there can be at most one arrival in a time interval of
</p>
<p>length .6.t. The two events are mutually exclusive so that
</p>
<p>P[N(t) = 1] = P[N(t - .6.t) = 0,N(t) - N(t - .6.t) = 1]
</p>
<p>+P[N(t - .6.t) = 1,N(t) - N(t - .6.t) = 0]
P[N(t - .6.t) = O]P[N(t) - N(t - .6.t) = 1]
</p>
<p>+P[N(t - .6.t) = l]P[N(t) - N(t - .6.t) = 0]
</p>
<p>P[N(t - .6.t) = O]P[N(t + .6.t) - N(t) = 1]
+P[N(t - .6.t) = l]P[N(t + .6.t) - N(t) = 0].
</p>
<p>Using the definition of PI(t) and Axiom 3,
</p>
<p>PI (t) = Po(t - .6.t)A.6.t + PI (t - .6.t)(l - A.6.t)
</p>
<p>or
</p>
<p>Pdt) - PI(t - .6.t) = -API(t _ .6.t) + APo(t - .6.t)
.6.t
</p>
<p>and as .6.t --+ 0, we have the differential equation
</p>
<p>(independence)
</p>
<p>(stationarity)
</p>
<p>dPI(t)
~ + API(t) = APo(t).
</p>
<p>In like fashion we can show (see Problem 21.1) that if Pk(t) = P[N(t) = k], then
</p>
<p>(21.3)
</p>
<p>where we know that Po(t) = exp(-At). This is a set of simultaneous linear differen-
</p>
<p>tial equations that fortunately can be solved recursively. Since Po(t) is known, we
</p>
<p>can solve for PI(t). Once PI(t) has been found, then P2(t) can be solved for, etc.
</p>
<p>It is shown in Problem 21.2 that by using Laplace transforms, we can easily solve
</p>
<p>these equations. The result is
</p>
<p>(At)k
Pk(t) = exp(-At)~ k = 0,1 , ...
</p>
<p>so that finally we have the desired result
</p>
<p>P[N(t) = k] = exp( -At) ( ~ t k = 0,1, ... (21.4)
</p>
<p>which is the usual Poisson PMF. The only difference from that described in Section
</p>
<p>5.5.4 is that here A represents an arrival rate. Since if X "" Pois(A'), then E[X] = A',
we have A' = At. Hence, A = A'it = E[N(t)]/t, which is seen to be the average
number of arrivals per second.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.3. DERIVATION OF POISSON COUNTING RANDOM PROCESS 717
</p>
<p>21.3.2 Some Examples
</p>
<p>Before proceeding with some examples it should be pointed out that the Poisson
</p>
<p>counting random process is not stationary or even WSS. This is evident from the
</p>
<p>PMF of N(t) since E[N(t2)] = )..t2 f- )..tl = E[N(tt}] for tz f- tl. As its properties
are inherited from the binomial counting random process, it exhibits the properties
</p>
<p>of a sum random process (see Section 16.4). Also, in determining probabilities of
</p>
<p>events, the fact that the increments are independent and stationary will greatly
</p>
<p>simplify our calculations.
</p>
<p>Example 21.1 - Customer arrivals
</p>
<p>Customers arrive at a checkout lane at the rate of 0.1 customers per second ac-
</p>
<p>cording to a Poisson random process. Determine the probability that 5 customers
</p>
<p>will arrive during the first minute the lane is open and also 5 customers will arrive
</p>
<p>the second minute it is open. During the time interval [0,60] the probability of 5
</p>
<p>arrivals is from (21.4)
</p>
<p>P[N(60) = 5] = exp[-0.1(60)] [ ( 0 . 1 ) 5 ~ 6 0 ) ] 5 = 0.1606.
</p>
<p>This will also be the probability of 5 customers arriving during the second minute
</p>
<p>interval or for anyone minute interval [t, t + 60] since
</p>
<p>P[N(t + 60) - N(t) = 5] P[N(60) - N(O) = 5]
P[N(60) = 5]
</p>
<p>(increment stationarity)
</p>
<p>(N(O) = 0)
</p>
<p>which is not dependent on t. Hence, the probability of 5 customers arriving in the
first minute and 5 more arriving in the second minute is
</p>
<p>P[N(60) - N(O) = 5, N(120) - N(60) = 5]
</p>
<p>P[N(60) - N(O) = 5]P[N(120) - N(60) = 5] (increment independence)
</p>
<p>P[N(60) - N(O) = 5]P[N(60) - N(O) = 5] (increment stationarity)
</p>
<p>p 2[N (60) = 5] = 0.0258 (N(O) = 0)
</p>
<p>Example 21.2 - Traffic bursts
</p>
<p>Consider the arrival of cars at an intersection. It is known that for any 5 minute
</p>
<p>interval 50 cars arrive on the average. For any 5 minute interval what is the prob-
</p>
<p>ability of 20 cars in the first minute and 30 cars in the next 4 minutes? Since the
</p>
<p>probabilities of the increments do not change with the time origin due to stationar-
</p>
<p>ity, we can assume that the 5 minute interval in question starts at t = 0 and ends
at t = 300 seconds. Thus, we wish to determine the probability of a traffic burst
PB, which is
</p>
<p>PB = P[N(60) = 20, N(300) - N(60) = 30].</p>
<p/>
</div>
<div class="page"><p/>
<p>718 CHAPTER 21. POISSON RANDOM PROCESSES
</p>
<p>Since the increments are independent, we have
</p>
<p>PB = P[N(60) = 20]P[N(300) - N(60) = 30]
</p>
<p>and because they are also stationary
</p>
<p>PB = P[N(60) = 20]P[N(240) - N(O) = 30]
</p>
<p>P[N(60) = 20]P[N(240) = 30]
</p>
<p>(60A)20 (240A)30
exp( -60A) 20! exp( -240A) 30!
</p>
<p>Finally, since the arrival rate is given by A = 50/300 = 1/6, the probability of a
traffic burst is
</p>
<p>(10)20 (40)30
PB = exp( -10) -20' exp( -40)--I = 3.4458 X 10-5 .
</p>
<p>. 30.
</p>
<p>o
In many applications it is important to assess not only the probability of a number
</p>
<p>of arrivals within a given time interval but also the distribution of these arrival
</p>
<p>times. Are they evenly spaced or can they bunch up as in the last example? In the
</p>
<p>next section we answer these questions.
</p>
<p>21.4 Interarrival Times
</p>
<p>Consider a typical realization of a Poisson random process as shown in Figure
</p>
<p>21.4. The times tl , tz , ts , ... are called the arrival times while the time intervals
</p>
<p>N(t)
</p>
<p>5
</p>
<p>4
</p>
<p>3
</p>
<p>2
</p>
<p>1
</p>
<p>.....-
I
</p>
<p>-Zs-'. '
I
</p>
<p>Z4 ....-
</p>
<p>,.-'
....-- Z3-----""
, I
</p>
<p>-Z2---... '
I
</p>
<p>Zl~
</p>
<p>t
</p>
<p>)( )( )( )(
</p>
<p>ts
</p>
<p>Figure 21.4: Definition of arrival times ti's and interarrival times Zi 'S.
</p>
<p>Zl , Z2, Z3 ,&middot; .. are called the interarrival times. The interarrival times shown in Fig-
</p>
<p>ure 21.4 are realizations of the random variables Zl, Z2, Z3,.... We wish to be</p>
<p/>
</div>
<div class="page"><p/>
<p>21.4. INTERARRIVAL TIMES 719
</p>
<p>able to compute probabilities for a finite set , say ZI, Z2, . . " ZK. Since N(t) is a
</p>
<p>continuous-time random process, the time between arrivals is also continuous and
</p>
<p>so a joint PDF is sought. To begin we first determine PZ
1
(zd. Note that ZI = TI ,
</p>
<p>where TI is the random variable denoting the first arrival. By the definition of the
</p>
<p>first arrival if ZI &gt; 6, then N(6) = 0 as shown in Figure 21.4. Conversely, if
N(6) = 0, then the first arrival has not occurred as of time 6 and so ZI &gt; 6&middot;
This argument shows that the events {ZI &gt; 6} and {N(6) = O} are equivalent
and therefore
</p>
<p>P[ZI &gt; 6] = P[N(6) = 0]
</p>
<p>exp(-&gt;'6) 6 ~ 0
</p>
<p>where we have used (21.4). As a result , the PDF is for ZI ~ 0
</p>
<p>(21.5)
</p>
<p>and finally the PDF of the first arrival is
</p>
<p>(21.6)
</p>
<p>or ZI rv exp(X]. An example follows.
</p>
<p>Example 21.3 - Waiting for an arrival
</p>
<p>Assume that at t = 0 we start to wait for an arrival. Then we know from (21.6)
that the time we will have to wait is a random variable with ZI rv exp(&gt;.). On the
</p>
<p>average we will have to wait E[ZI] = 1/&gt;. seconds. This is reasonable in that&gt;' is
average arrivals per second and therefore 1/&gt;. is seconds per arrival. However, say
</p>
<p>we have already waited 6 seconds-what is the probability that we will have to wait
</p>
<p>more than an additional 6 seconds? In probabilistic terms we wish to compute the
conditional probability P[ZI &gt; 6 +61ZI &gt; 6]. This is found as follows.
</p>
<p>P[ZI &gt; 6 +6,ZI &gt; 6]
P[ZI &gt; 6]
</p>
<p>P[ZI &gt; 6 +6]
P[ZI &gt; 6]
</p>
<p>since the arrival time will be greater than both 6 + 6 and 6 only if it is greater</p>
<p/>
</div>
<div class="page"><p/>
<p>720 CHAPTER 21. POISSON RANDOM PROCESSES
</p>
<p>than the former. Now using (21.5) we have that
</p>
<p>exp[-.\(6 +6)]
exp(-.\6)
</p>
<p>exp(-.\6)
</p>
<p>= P[ZI &gt; 6] &middot; (21.7)
</p>
<p>Hence, the conditional probability that we will have to wait more than an additional
</p>
<p>6 seconds given that we have already waited 6 seconds is just the probability that
</p>
<p>we will have to wait more than 6 seconds. The fact that we have already waited
</p>
<p>does not in any way affect the probability of the first arrival. Once we have waited
</p>
<p>and observed that no arrival has occured up to time 6, then the random process in
</p>
<p>essence starts over as if it were at time t = O. This property of the Poisson random
</p>
<p>process is referred to as the memoryless property. It is somewhat disconcerting to
</p>
<p>know that the chances your bus will arrive in the next 5 minutes, given that it is
</p>
<p>already 5 minutes late, is not any better than your chances it will be late by 5
</p>
<p>minutes. However, this conclusion is consistent with the Poisson random process
</p>
<p>model. It is also evident by examining the similar result of waiting for a fair coin
</p>
<p>to comes up heads given that it has already exhibited 10 tails in a row. In Problem
</p>
<p>21.12 an alternative derivation of the memoryless property is given which makes use
</p>
<p>of the geometric random variable.
</p>
<p>&lt;:;
We next give the joint PDF for two or more interarrival times. It is shown in
</p>
<p>Appendix 2IA that the interarrival times Zl, Z2,' . . , ZK are lID random variables
</p>
<p>with each one having Zl rv exp(X]. This result may also be reconciled in light of
</p>
<p>the Poisson random process being the limiting form of a Bernoulli random process.
</p>
<p>Consider a Bernoulli random process {X[O] = 0, X[I]' X [2]' .. .}, where X[O] = 0 by
</p>
<p>definition, and assume interarrival times of k: and k2, where k1 2:: 1, k2 2:: 1. For
example, if X[I] = 0, X[2] = 1, X[3] = 0, X [4] = 0, and X[5] = 1, then we would
have k1 = 2 and k2 = 3. In general,
</p>
<p>P[first interarrival time = k1 , second interarrival time = k2 ]
</p>
<p>P[X[n] = 0 for 1:::; n:::; k1 -I,X[k1] = I ,X[n] = 0
</p>
<p>for k1 + 1 :::; n :::; k1 + kz - 1, X[k1 + k2 ] = 1]
[(1 - p)kl- lp][(1 _ p)k2 - 1p].
</p>
<p>Hence , the joint PMF factors so that the interarrival times are independent and
</p>
<p>furthermore they are identically distributed (let k 1 = k2 ) . An example follows.
</p>
<p>Example 21.4 - Expected time for calls
</p>
<p>A customer call service center opens at 9 A.M. The calls received follow a Poisson
</p>
<p>random process at the average rate of 600 calls per hour. The 20th call comes in
</p>
<p>at 9:01 A.M. At what time can we expect the next call to come in? Let Z21 be</p>
<p/>
</div>
<div class="page"><p/>
<p>21.5. ARRIVAL TIMES 721
</p>
<p>(21.8)
</p>
<p>the elapsed time from 9:01 A.M. until the next call comes in. Since the interarrival
</p>
<p>times are independent, they do not depend upon the past history of arrivals. Hence,
</p>
<p>Z21 = T21 - T20 ,......, exp[A]. Since the mean of an exponential random variable Z
</p>
<p>is just 1/A and from the information given A = 600/3600 = 1/6 calls per second,
</p>
<p>we have that E[Z21] = 1/(1/6) = 6 seconds. Hence, we can expect the next call to
come in at 9:01:06 A.M.
</p>
<p>21.5 Arrival Times
</p>
<p>The kth arrival time Tk is defined as the time from t = 0 until the kth arrival occurs.
</p>
<p>The arrival times are illustrated in Figure 21.4, where Tk is also referred to as the
</p>
<p>waiting time until the kth arrival. In this section we will determine the PDF of Tk.
</p>
<p>It is seen from Figure 21.4 that tk = 2:7=1 Zi so that the random variable of interest
is
</p>
<p>But we saw in the last section that the Zi'S are IID with Zl ,......, exn(X]. Hence, the
</p>
<p>PDF of Tk is obtained by determining the PDF for a sum of IID random variables.
</p>
<p>This is a problem that has been studied in Section 14.6, and is solved most readily by
</p>
<p>the use of the characteristic function. Recall that if Xl, X 2 , .&bull;&bull; ,Xk are IID random
</p>
<p>variables, then the characteristic function for Y = 2:7=1 Xi is &cent;y(w) = &cent;'X(w).
Thus, the PDF for Y , assuming that Y is a continuous random variable, is found
</p>
<p>from the continuous-time inverse Fourier transform (defined to correspond to the
</p>
<p>Fourier transform used in the characteristic function definition, and uses a - j and
</p>
<p>radian frequency w) as
</p>
<p>/
</p>
<p>00 dw
py(y) = &cent;'X(w) exp(-jwY)-2 .
</p>
<p>- 00 1r
</p>
<p>From Table 11.1 we have that &cent;Zl (w) = A/(A - jw) and therefore
</p>
<p>&cent;Tk(W) = (A:jw)k = (l_~W/A)k
Again referring to Table 11.1, we see that this is the characteristic function of a
</p>
<p>Gamma random variable with a = k so that Tk ,......, r(k, A). Specifically, this is the
Erlang random variable described in Section 10.5.6 . Hence, we have that
</p>
<p>A
k
</p>
<p>k-l
PTk (t) = (k _ 1)! t exp( -At).
</p>
<p>(See also Problem 21.15 for the derivation for k = 2 using a convolution integral and
</p>
<p>Problem 21.16 for an alternative derivation for the general case.) Note that for a</p>
<p/>
</div>
<div class="page"><p/>
<p>722 CHAPTER 21. POISSON RANDOM PROCESSES
</p>
<p>I'(o, &gt;') random variable the mean is a/&gt;. so that with a = k, we have the expected
time for the kth arrival as
</p>
<p>(21.9)
</p>
<p>or equivalently
</p>
<p>(21.10)
</p>
<p>On the average the time to the kth arrival is just k times the time to the first arrival,
</p>
<p>a somewhat pleasing result. An example follows.
</p>
<p>Example 21.5 - Computer servers
</p>
<p>A computer server is designed to provide downloaded software when requested. It
</p>
<p>can honor a total of 80 requests in each hour before it becomes overloaded. If the
</p>
<p>requests are made in accordance with a Poisson random process at an average rate
</p>
<p>of 60 requests per hour, what is the probability that it will be overloaded in the first
</p>
<p>hour? We need to determine the probability that the 81st request will occur at a
</p>
<p>time t ::; 3600 seconds. Thus, from (21.8) with k = 81
</p>
<p>P[overloaded in first hour] P[TS1 ::; 3600]
</p>
<p>r3600 &gt;.SI
= 10 80! tSO exp( -&gt;.t)dt.
</p>
<p>Here the arrival rate of the requests is &gt;. = 60/3600 = 1/60 per second and therefore
</p>
<p>1 13600 1 (t) soP[overloaded in first hour] = - -, - exp(-t/60)dt
60 0 80. 60
</p>
<p>Using the result
</p>
<p>! (at)n exp( -at)dt = _ exp( -at) ~ (a.t)in! a L...J z!
i=O
</p>
<p>it follows that
</p>
<p>= ~ [_ exp(-t/60) so (t/60)i 1
3600
</p>
<p>]
P[overloaded in first hour] 60 1/60 t; i! 0
</p>
<p>[
</p>
<p>SO (60)i ]
= - exp(-60) t; i!-1
</p>
<p>so (60)i
1 - exp( -60) L -.-, = 0.0056.
</p>
<p>i=O z.</p>
<p/>
</div>
<div class="page"><p/>
<p>21.6. COMPOUND POISSON RANDOM PROCESS
</p>
<p>21.6 Compound Poisson Random Process
</p>
<p>723
</p>
<p>A Poisson counting random process increments its value by one for each new arrival.
</p>
<p>In some applications we may not know the increment in advance. An example would
</p>
<p>be to determine the average amount of all transactions within a bank for a given
</p>
<p>day. In this case the amount obtained is the sum of all deposits and withdrawals.
</p>
<p>To model these transactions we could assume that customers arrive at the bank
</p>
<p>according to a Poisson random process. If, for example, each customer deposited
</p>
<p>one dollar , then at the end of the day, say at time to, the total amount of the
</p>
<p>transactions X(to) could be written as
</p>
<p>N(to)
</p>
<p>X(to) = I: 1 = N(to).
i=l
</p>
<p>This is the standard Poisson counting random process. If, however, there are with-
</p>
<p>drawals, then this would no longer hold. Furthermore, if the deposits and with-
</p>
<p>drawals are unknown to us before they are made, then we would need to model
</p>
<p>each one by a random variable, say Ui. The random variable would take on positive
</p>
<p>values for deposits and negative values for withdrawals and probabilities could be
</p>
<p>assigned to the possible values of Ui. The total dollar amount of the transactions at
</p>
<p>the end of the day would be
N(to)
</p>
<p>I: u;
i=l
</p>
<p>With this motivation we will consider the more general case in which the Ui'S are
</p>
<p>either discrete or continuous random variables, and denote the total at time t by
</p>
<p>the random process X(t). This random process is therefore given by
</p>
<p>N(t)
</p>
<p>X(t) = I: o,
i=l
</p>
<p>t ~ O. (21.11)
</p>
<p>It is a continuous-time random process but can be either continuous-valued or
</p>
<p>discrete-valued depending upon whether the Ui'S are continuous or discrete random
</p>
<p>variables. We furthermore assume that the Ui'S are IID random variables. Hence,
</p>
<p>X(t) is similar to the usual sum of IID random variables except that the number
</p>
<p>of terms in the sum is random and the number of terms is distributed according
</p>
<p>to a Poisson random process. This random process is called a compound Poisson
</p>
<p>random process.
</p>
<p>In summary, we let X(t) = L:~(i) U, for t ~ 0, where the Ui'S are IID random
variables and N(t) is a Poisson counting random process with arrival rate A. Also,
</p>
<p>we define X(O) = 0, and furthermore assume that the Ui'S and N(t) are independent
</p>
<p>of each other for all t.</p>
<p/>
</div>
<div class="page"><p/>
<p>724 CHAPTER 21. POISSON RANDOM PROCESSES
</p>
<p>We next determine the marginal PMF or PDF of X(t). To do so we will use
</p>
<p>characteristic functions in conjunct ion with conditioning arguments. The key to
</p>
<p>success here is to turn the sum with a random number of terms into one with a fixed
</p>
<p>number by conditioning. Then , the usual characteristic function approach described
</p>
<p>in Section 14.6 will be applicable. Hence, consider for a fixed t = to the random
variable X(to) and write its characteristic function as
</p>
<p>cPX(to)(W) = E[exp(jwX(to))]
</p>
<p>E[exp (jW ~) U;)]
</p>
<p>(definition)
</p>
<p>~ EN(") [Eu" ....a, IN(t,) [exp (jWtU;) N(to) ~ k] ]
(see Problem 21.18)
</p>
<p>EN(,,) [EU,, u, [exp (jWt u;) ]] (Uis independent of N (to))
EN(,,) [EU" u, [g expuwu;)]]
EN (,,) [g Eu, (exPUWU;)I] (U;'s are independent]
</p>
<p>= EN(") [g q,u,(w)] (definition of char. function)
EN(to) [cPtl (w)] (Ui'S identically dist .)
</p>
<p>00
</p>
<p>L cPtl(w)PN(to)[k]
k=O
</p>
<p>00 k (Ato)k
[; cPUl (w) exp ( -AtO)~
</p>
<p>exp( -Ato) f (AtOcP~~ (w))k
k=O
</p>
<p>exp( -Ato) exp(AtOcPUl (w))
</p>
<p>so that finally we have the characteristic function
</p>
<p>cPX(to)(w) = exp[AtO(cPUl (w) - 1)]. (21.12)
</p>
<p>To determine the PMF or PDF of X(to) we would need to take the inverse Fourier
</p>
<p>transform of the characteristic fun ction. As a check , if we let U, = 1 for all i so that</p>
<p/>
</div>
<div class="page"><p/>
<p>21.6. COMPOUND POISSON RANDOM PROCESS
</p>
<p>from (21.11) X(to) = N(to), then since
</p>
<p>cPu! (w) = E[exp(jwU1)] = exp(jw)
</p>
<p>725
</p>
<p>we have the usual characteristic function of a Poisson random variable (see Table
</p>
<p>6.1)
</p>
<p>cPX(to)(w) = exp[Ato(exp(jw) - 1)].
</p>
<p>(The derivation of (21.12) can be shown to hold for this choice of the Ui'S, which
</p>
<p>are degenerate random variables.) An example follows.
</p>
<p>Example 21.6 - Poisson random process with dropped arrivals
</p>
<p>Consider a Poisson random process in which some of the arrivals are dropped. This
</p>
<p>means for example that a Geiger counter may not record radioactive particles if their
</p>
<p>intensity is too low. Assume that the probability of dropping an arrival is 1 - p,
</p>
<p>and that this event is independent of the Poisson arrival process. Then, we wish to
</p>
<p>determine the PMF of the number of arrivals within the time interval [0, to]. Thus,
</p>
<p>the number of arrivals can be represented as
</p>
<p>N(to)
</p>
<p>X(to) = L o.
i=l
</p>
<p>where U; = 1 if the ith arrival is counted and U, = 0 if it is dropped. Assuming that
</p>
<p>the Ui'S are IID, we have a compound Poisson random process. The characteristic
</p>
<p>function of X(to) is found using (21.12) where we note that
</p>
<p>E[exp(jwUd]
</p>
<p>pexp(jw) + (1 - p)
</p>
<p>so that from (21.12)
</p>
<p>exp[Ato(pexp(jw) + (1 - p) - 1)]
exp[pAto(exp(jw) - 1)].
</p>
<p>But this is just the characteristic of a Poisson counting random process with arrival
</p>
<p>rate of p); Hence, by dropping arrivals the arrival rate is reduced but X(t) is still
</p>
<p>a Poisson counting process, a very reasonable result.
</p>
<p>c
Since the characteristic function of a compound Poisson random process is available,
</p>
<p>we can use it to easily find the moments of X(to). In particular, we now determine
</p>
<p>the mean, leaving the variance as a problem (see Problem 21.22). Using (21.12) we</p>
<p/>
</div>
<div class="page"><p/>
<p>726
</p>
<p>have
</p>
<p>E[X(to) ]
</p>
<p>CHAPTER 21. POISSON RANDOM PROCESSES
</p>
<p>! d&lt;/&gt;x( to )(w) I (using (6.13))
j dw w= O
</p>
<p>~'\to d&lt;/&gt;Udwl(w) exp[,\to(&lt;/&gt;Ul (w) - 1)]1
J w=O
</p>
<p>\ 1 d&lt;/&gt;Ul(W) \/\to- --'--~:........:..
j dw w=O
</p>
<p>since &lt;/&gt;Ul (0) = 1. But
</p>
<p>so that the average value is
</p>
<p>(21.13)
</p>
<p>It is seen that the average value of X(to) is just the average value of U1 times
</p>
<p>- the expected number of arrivals. This result also holds even if the Ui 'S only have
</p>
<p>the same mean, without the IID assumption (see Problem 21.25 and the real-world
</p>
<p>problem). An example follows.
</p>
<p>Example 21. 7 - Expected number of points scored in basketball game
</p>
<p>A basketball player, dubbed the "Poisson pistol Pete" of college basketball, shoots
</p>
<p>the ball at an average rate of 1 shot per minute according to a Poisson random
</p>
<p>process. He shoots a 2 point shot with a probability of 0.6 and a 3 point shot with a
</p>
<p>probability of 0.4. If his 2 point field goal percentage is 50% and his 3 point field goal
</p>
<p>percentage is 30%, what is his expected total number of points scored in a 40 minute
</p>
<p>game? (We assume that the referees "let them play" so that no fouls are called and
</p>
<p>hence no free throw points.) The average number of points is E[N(to)]E[Ul], where
</p>
<p>to = 2400 seconds and Ul is a random variable that denotes his points made for the
</p>
<p>first shot (the distribution for each shot is identical). We first determine the PMF
</p>
<p>for Ul , where we have implicitly assumed that the Ui'S are IID random variables.
</p>
<p>From the problem description we have that
</p>
<p>if 2 point shot attempted and made
</p>
<p>if 3 point shot attempted and made
</p>
<p>otherwise.
</p>
<p>Hence,
</p>
<p>PUI [2] P [2 point shot attempted and made]
</p>
<p>- P[2 point shot made I 2 point shot attempted]P[2 point shot attempted]
= 0.5(0.6) = 0.3</p>
<p/>
</div>
<div class="page"><p/>
<p>21.7. COMPUTER SIMULATION 727
</p>
<p>and similarly PUI [3] = 0.3(0.4) = 0.12 and therefore, PUI [0] = 0.58. The expected
value becomes E[Ul] = 2(0.3) + 3(0.12) = 0.96 and therefore the expected number
of points scored is
</p>
<p>E[N(to)]E[Ul] ,\toE[Ul]
</p>
<p>6
10
</p>
<p>(2400)(0.96)
</p>
<p>38.4 points per game.
</p>
<p>21.7 Computer Simulation
</p>
<p>To generate a realization of a Poisson random process on a computer is relatively
</p>
<p>simple. It relies on the property that the interarrival times are lID exp('\) random
</p>
<p>variables. We observe from Figure 21.4 that the ith interarrival time is Z; = Ti -
</p>
<p>Ti-l, where T; is the ith arrival time. Hence,
</p>
<p>i = 1,2, ...
</p>
<p>where we define To = O. Each Z, has the PDF exp('\) and the Zi'S are lID. Hence,
</p>
<p>to generate a realization of each Zi we use the inverse probability integral transfor-
</p>
<p>mation technique (see Section 10.9) to yield
</p>
<p>1 1
Zi=,ln-
</p>
<p>UA 1- i
</p>
<p>where Ui '" U(O, 1) and the Ui'S are lID. A typical realization using the following
MATLAB code is shown in Figure 21.5a for ,\ = 2. The arrivals are indicated now
by + 's for easier viewing. If we were to increase the arrival rate to ,\ = 5, then a
typical realization is shown in Figure 21.5b.
</p>
<p>clear all
rand ( )state) ,0)
</p>
<p>lambda=2; %set arrival rate
T=5; %set time interval in seconds
for i=1:1000
</p>
<p>z(i,l)=(l/lambda)*log(l/(l-rand(l,l))); %generate interarrival times
if i==l %generate arrival time
</p>
<p>t (i , 1) =z (i) ;
</p>
<p>else
</p>
<p>t(i,l)=t(i-l)+z(i,l);
</p>
<p>end
</p>
<p>if t(i&raquo;T %test to see if desired time interval has elapsed</p>
<p/>
</div>
<div class="page"><p/>
<p>728 CHAPTER 21. POISSON RANDOM PROCESSES
</p>
<p>...++ .:+. + + :. +. + . .
</p>
<p>'"~ - ._ .
.s::
~
&sect; r++'+ . ':+' 4+'" ~ .* ..:.... ..... -++- ...
'"'"'0
p.,
</p>
<p>o 2 3
t (sec)
</p>
<p>(a) &gt;. = 2
</p>
<p>4 5 o 2 3
t (sec)
</p>
<p>(b) &gt;. = 5
</p>
<p>4 5
</p>
<p>Figure 21.5: Realizations of Poisson random process.
</p>
<p>break
</p>
<p>end
</p>
<p>end
</p>
<p>M=length(t)-l; I. number of arrivals in interval [O,T]
arrivals=t(l:M); I. arrival times in interval [O,T]
</p>
<p>21.8 Real-World Example - Automobile Traffic Signal
</p>
<p>Planning
</p>
<p>An important responsibility of traffic engineers is to decide which intersections re-
</p>
<p>quire traffic lights. Although general guidelines are available [Federal Highway Ad.
</p>
<p>1988], new situations constantly arise that warrant a reassessment of the situation-
</p>
<p>principally an unusually high accident rate [Imada 2001]. In this example, we sup-
</p>
<p>pose that a particular intersection, which has two stop signs, is prone to accidents.
</p>
<p>The situation is depicted in Figure 21.6, where it is seen that the two intersecting
</p>
<p>streets are one-way streets with a stop sign at the corner of each one. A traffic
</p>
<p>engineer believes that the high accident rate is due to motorists who ignore the stop
</p>
<p>signs and proceed at full speed through the intersection. If this is indeed the case,
</p>
<p>then the installation of a traffic light is warranted. To determine if the accident rate
</p>
<p>is consistent with his belief that motorists are "running" the stop signs , he wishes
</p>
<p>to det ermine the average number of accidents that would occur if this is true. As
</p>
<p>shown in Figure 21.6, if 2 vehicles arrive at the intersection within a given time
</p>
<p>interval, an accident will occur. It is assumed the two cars are identical and move
</p>
<p>with the same speed. The traffic engineer then models the arrivals as two indepen-</p>
<p/>
</div>
<div class="page"><p/>
<p>21.8. AUTOMOBILE TRAFFIC SIGNAL PLANNING 729
</p>
<p>Figure 21.6: Intersection with two automobiles approaching at constant speed.
</p>
<p>dent Poisson random processes, one for each direction of travel. A typical set of car
</p>
<p>arrivals based on this assumption is shown in Figure 21.7. Specifically, an accident
</p>
<p>&middot; &middot; -EW&middot; -: .
</p>
<p>*++...+..ff . ++:* ..++ .-t:+ .+
</p>
<p>NS
</p>
<p>++ *."
</p>
<p>o 500 1000 1500 2000 2500 3000 3500
t (sec)
</p>
<p>Figure 21.7: Automobile arrivals.
</p>
<p>will occur if any two arrivals satisfy ITEW - TNsl ~ T, where T EW and T NS refer
</p>
<p>to the arrival time at the center of the intersection from the east-west direction and
</p>
<p>the north-south direction, respectively, and T is some minimum time for which the
</p>
<p>cars can pass each other without colliding. The actual value of T can be estimated
</p>
<p>using T = d]c, where d is the length of a car and c is its speed. As an example,
if we assume that d = 22 ft and c = 44 ft/sec (about 30 mph), then T = 0.5 sec.
An accident will occur if two arrivals are within one-half second of each other. In
</p>
<p>Figure 21.7 this does not occur, but there is a near miss as can be seen in Figure
</p>
<p>21.8, which is an expanded version. The east-west car arrives at t = 2167.5 seconds
while the north-south car arrives at t = 2168.4 seconds.</p>
<p/>
</div>
<div class="page"><p/>
<p>730 CHAPTER 21. POISSON RANDOM PROCESSES
</p>
<p>... . . .. .... . ~ '.' . . . . . . . . . .. . .. .
</p>
<p>00
</p>
<p>Cd&gt; : 1 : &middot; &middot; EW&middot; &middot; ' &middot; &middot;
</p>
<p>.&sect; : ~ .+ : ~ , .
</p>
<p>..s; I ::0 : '1' : .
o 1 : NS :
&sect; '+"': +'+'+'" + ..
~ .I : .
&lt;r: I .
</p>
<p>2000 2100 2200 2300
t (sec)
</p>
<p>2400 2500
</p>
<p>(21.14)
</p>
<p>Figure 21.8: Automobile arrivals-expanded version of Figure 21.7. There is a near
</p>
<p>miss at t = 2168 seconds , shown by the dashed vertical line.
</p>
<p>We now describe how to determine the average number of accidents per day.
</p>
<p>This can be obtained by defining a set of indicator random variables (see Example
</p>
<p>11.4) as
</p>
<p>1- = {I if there is at least one NS arrival with ITi
EW
</p>
<p>- TNS I :::; T
z 0 otherwise.
</p>
<p>Here T NS can be any NS arrival time and T i
EW is the ith arrival time for the EW
</p>
<p>traffic. (More explicitly, the event for which the indicator random variable is 1
</p>
<p>occurs when minj=1,2,... ITi
EW
</p>
<p>- TfSI :::; T , where Tfs is the jth arrival for the NS
traffic.) Now the number of accidents in the time interval [0, t] is
</p>
<p>N(t)
</p>
<p>X(t) = L t,
i=l
</p>
<p>where N(t) is the Poisson counting random process for the EW traffic. To find
</p>
<p>the expected value of X(t) we note that the equation (21.13) , although originally
</p>
<p>derived under the assumption that the Ui 'S are lID, is also valid under the weaker
</p>
<p>assumption that the means of the Ui'S are the same as shown in Problem 21.25.
</p>
<p>Since the Ii'S will be seen shortly to have the same mean, the expected value of
</p>
<p>(21.14) is from (21.13) with Us = t,
</p>
<p>E[X(t)] = &gt;.tE[/r].
</p>
<p>Now to evaluate E[Ii], we note that
</p>
<p>E[Ii] = P[ITi
EW
</p>
<p>- T NSI :s; T]
</p>
<p>(21.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>21.8. AUTOMOBILE TRAFFIC SIGNAL PLANNING 731
</p>
<p>and the probability can be found using a conditioning approach (see (13.12)). This
</p>
<p>produces
</p>
<p>P[ITpw - TNSI :s; T] =100 P[ITiEW - TNSI :s; TITiEW = t]PT;(t)dt.
Proceeding we have that
</p>
<p>P[ITi
EW
</p>
<p>- TNSI :s; T] 100 P[lt - TNSI :s; TITiEW = t]PT;(t)dt
= 100 P[lt - TNSI :s; T]PTi(t)dt (TiEW , T N S are independent)
</p>
<p>1
00
</p>
<p>P[t - T :s; T NS :s; t +T]PTi (t)dt. (21.16)
</p>
<p>Note that t - T :s; T NS :s; t + T is the event that the NS traffic will have at least one
arrival (and hence an accident) in the interval [t - T, t + T]. Its probability is just
</p>
<p>P[t - T :s; T NS :s; t + T] Prone or more arrivals in [t - T, t + T]]
1 - P[no arrival in [t - T, t + T]]
</p>
<p>= 1 - P[no arrivals in [0,2T]] (increment stationarity)
</p>
<p>= 1 - P[N(2T) = 0]
</p>
<p>1 - exp( -2.XT) (from (21.2))
</p>
<p>and is not dependent on t. Thus,
</p>
<p>E[Ii] P[ITi
EW
</p>
<p>- TNSI :s; T]
</p>
<p>= 1&deg;O(1-exP(-2AT))PTi(t)dt
</p>
<p>= 1 - exp( -2AT)
</p>
<p>(from (21.16))
</p>
<p>for all i, and therefore all the Ii'S have the same mean. From (21.15)
</p>
<p>E[X(t)] = At(l - exp(-2AT)).
</p>
<p>For the same example as before with T = 0.5, the average number of accidents per
second is
</p>
<p>E[X(t)] = A(l - exp( -A)).
t
</p>
<p>For a more meaningful measure we convert this to the average number of accidents
</p>
<p>per hour, which is (E[X(t)]jt)3600. This is plotted versus X, where X is in arrivals
per hour, in Figure 21.9. Specifically, it is given by
</p>
<p>3600
E[X(t)]
</p>
<p>= 3600A(1 - exp( -A))
t</p>
<p/>
</div>
<div class="page"><p/>
<p>732 CHAPTER 21. POISSON RANDOM PROCESSES
</p>
<p>605020 30 40
&gt;..' (arrivals per hour)
</p>
<p>10
OL-_='--__'----_----J'----_----J'--_----J'--_---l
</p>
<p>o
</p>
<p>0.9
</p>
<p>~O .8 .
o
</p>
<p>..c:
.... 0.7
ClJ
0.0 6 .'" .
1::ClJ 0.5 .
'"C
</p>
<p>'80.4 .
ell
</p>
<p>&sect;h0.3 .
ell
6) 0.2 .
&gt;
</p>
<p>-&lt;0.1
</p>
<p>Figure 21.9: Average number of accidents per hour versus arrival rate (in per hour
</p>
<p>units) .
</p>
<p>where X' = arrivals per hour = 3600&gt;'. As seen in Figure 21.9 for about 1 arrival
every 3 minutes or 20 arrivals per hour, we will have an average of 0.1 accidents
</p>
<p>per hour or about an average of one accident every two days. This assumes a
</p>
<p>busy intersection for about 5 hours per day. Thus, if the traffic engineer notices an
</p>
<p>accident nearly every other day, he will request that a traffic light be put in.
</p>
<p>References
</p>
<p>Cox , D.R., H.D. Miller ,The Theory of Stochastic Processes, John Wiley &amp; Sons,
</p>
<p>New York, 1965.
</p>
<p>Federal Highway Administration, Manual on Uniform Traffic Control Devices, U.S.
</p>
<p>Govt. Printing Office, 1988.
</p>
<p>Imada, T ., "Traffic Control of Closely Located Intersections: the U.S. 1 Busway
</p>
<p>Experience," ITE Journal on the Web, pp. 81-84, May 2001.
</p>
<p>Problems
</p>
<p>21.1 (t) Prove that the differential equation describing Pk(t) = P[N(t) = k] for a
Poisson counting random process is given by (21.3) . To do so use Figure 21.3
</p>
<p>with either k arrivals in [0, t - .6.t] and no arrivals in (t - .6.t,t] or k - 1 arrivals
</p>
<p>in [0, t - .6.t] and one arrival in (t - .6.t,t]. Since there can be at most one
</p>
<p>arrival in a time interval of length .6.t, these are the only possibilities.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 733
</p>
<p>21.2 (t) Solve the differential equation of (21.3) by taking the (one-sided) Laplace
transform of both sides, noting that Pk(O+) = O. Explain why the latter
</p>
<p>condition is consistent with the assumptions of a Poisson random process.
</p>
<p>You should be able to show that the Laplace transform of Pk(t) is
</p>
<p>by finding PI (s) from Po(s ), and then P2(s) from PI (s), etc. The desired in-
verse Laplace transform is found by referring to a table of Laplace transforms.
</p>
<p>21.3 L.:..) (f) Find the probability of 6 arrivals of a Poisson random process in the
time interval [7,12] if A = 1. Next determine the average number of arrivals
for the same time interval.
</p>
<p>21.4 (w) For a Poisson random process with an arrival rate of 2 arrivals per second,
find the probability of exactly 2 arrivals in 5 successive time intervals of length
</p>
<p>1 second each.
</p>
<p>21.5 (f) What is the probability of a single arrival for a Poisson random process
with arrival rate A in the time interval [t, t + .6.t] if .6.t -+ O?
</p>
<p>21.6 (w) Telephone calls come into a service center at an average rate of one per 5
</p>
<p>seconds. What is the probability that there will be more than 12 calls in the
</p>
<p>first one minute?
</p>
<p>21. 7 C:.:..) (f,c) For a Poisson random process with an arrival rate of A use a com-
puter simulation to estimate the arrival rate if A = 2 and also if A = 5. To do
so relate A to the average number of arrivals in [0,t]. Hint: Use the MATLAB
code in Section 21.7.
</p>
<p>21.8 (w) Two independent Poisson random processes both have an arrival rate of
</p>
<p>A. What is the expected time of the first arrival observed from either of the
</p>
<p>two random processes? Explain your results. Hint: Let this time be denoted
</p>
<p>by T and note that T = min(Tp) ,Ti
2)),
</p>
<p>where Ti
i
) is the first arrival time of
</p>
<p>the ith random process. Then, note that P[T &gt; t] = P[TP) &gt; t, Ti 2) &gt; t].
</p>
<p>21.9 (t) In this problem we prove that the sum of two independent Poisson counting
random processes is another Poisson counting random process whose arrival
</p>
<p>rate is the sum of the arrival rates of the two random processes. Let the Poisson
</p>
<p>counting random processes be NI(t) and N2(t) and consider the increments
</p>
<p>N(t2) - N(tl) and N(t4) -N(t3) for nonoverlapping time intervals. Argue that
</p>
<p>the corresponding increments for the sum random process are independent
</p>
<p>and stationary, knowing that this is true for each individual random process.
</p>
<p>Then, use characteristic functions to prove that if NI(t) f'V POiS(AIt) and</p>
<p/>
</div>
<div class="page"><p/>
<p>734 CHAPTER 21. POISSON RANDOM PROCESSES
</p>
<p>N 2(t) f"V Pois(-\2t) and NI(t) and N 2(t) are independent, then NI(t) +N2(t) f"V
</p>
<p>POiS&laquo;-\1 + -\2)t).
</p>
<p>21.10 C:.:,,) (w) If N(t) is a Poisson counting random process, determine E[N(t2)-
N(tl)] and var(N(t2) - N(tl)).
</p>
<p>21.11 (w) Commuters arrive at a subway station that has 3 turnstyles with the
</p>
<p>arrivals at each turnstyle characterized by an independent Poisson random
</p>
<p>process with arrival rate of -\ commuters per second. Determine the probability
</p>
<p>of a total of k arrivals in the time interval [0,t]. Hint: See Problem 21.9.
</p>
<p>21.12 (t) In this problem we present an alternate proof that the Poisson random
process has no memory as described by (21.7). It is based on the observation
</p>
<p>that a Poisson random process is the limiting form of a Bernoulli random
</p>
<p>process as explained in Section 21.1. Consider first the geometric PMF of the
</p>
<p>first success or arrival which is P[X = k] = (1 - p)k-Ip for k = 1,2, .... Then
</p>
<p>show that
</p>
<p>P[X &gt; kl + k21X &gt; k l ] = (1- p)k 2 &bull;
</p>
<p>Next let p = -\tlt and k l = 6/tlt and k2 = 6/tlt and prove that as tlt --+ 0
</p>
<p>Hint: As x --+ 0, (1 - ax) l/x --+ exp( -a).
</p>
<p>21.13 (...:..:,) (w) Taxi cabs arrive at the rate of 1 per minute at a taxi stand. If a
</p>
<p>person has already waited 10 minutes for a cab, what is the probability that
</p>
<p>he will have to wait less than 1 additional minute?
</p>
<p>21.14 (w) A computer memory has the capacity to store 106 words. If requests
for word storage follow a Poisson random process with a request rate of 1
</p>
<p>per millisecond, how long on average will it be before the memory capacity is
</p>
<p>exceeded?
</p>
<p>21.15 (t) If Xl f"V exp'(X}, X 2 f"V exp[X}, and Xl and X2 are independent random
variables, derive the PDF of the sum by using a convolution integral.
</p>
<p>21.16 (t) We give an alternate derivation of the PDF for the kth arrival time of a
Poisson random process. This PDF can be expressed as
</p>
<p>Use the fact that the event {t - tlt ~ Tk ~ t} can only occur as tlt --+ 0 if
there are k - 1 arrivals in [0,t - tlt] and 1 arrival in (t - tlt, t].</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 735
</p>
<p>21.17 C:...:...) (w) People arrive at a football game at a rate of 100 per minute. If
the 1000th person is to receive a seat at the 50th yard line (which is highly
</p>
<p>desirable), how long should you wait before entering the stadium?
</p>
<p>21.18 (t) Prove that if X, Y, Z are jointly distributed continuous random variables,
</p>
<p>then EX,Y,z[g(X,Y,Z)] = Ez [Ex,YIZ[g(X ,Y,Z)lz]] by expressing the expec-
tations using integrals. You may wish to refer back to Section 13.6.
</p>
<p>21.19 (t) The Poisson random process exhibits the Markov property. This says
that the conditional probability of N(t) based on past samples of the random
</p>
<p>process only depends upon the most recent sample. Mathematically, if ts &gt;
t2 &gt; ts , then
</p>
<p>Prove that this is true by making use of the property that the increments are
</p>
<p>independent. Specifically, consider the equivalent probability
</p>
<p>and also explain why this probability is equivalent.
</p>
<p>21.20 C:...:,.,) (c) Use a computer simulation to generate multiple realizations of a
Poisson random process with &gt;.. = 1. Then, use the simulation to estimate
P[T2 ::; 1]. Compare your result to the true value. Hint: Use the MATLAB
</p>
<p>code in Section 21.7.
</p>
<p>21.21 (w) An airport has two security screening lines . An employee directs the
incoming travelers to one of the two lines at random. If the incoming travelers
</p>
<p>arrive at the airport with a rate of &gt;.. travelers per second, what is the arrival
</p>
<p>rate at each of the two security screening lines? What assumptions are implicit
</p>
<p>in arriving at your answer?
</p>
<p>21.22 (t) Prove that the variance of a compound Poisson random process is
</p>
<p>var(X(to)) = &gt;..toE[Uf]. If you guessed that the result would be &gt;..tovar(Ut},
then evaluate your guess for a Poisson random process (let U, = 1).
</p>
<p>21.23 C:...:...) (f) A compound Poisson random process X(t) is composed of random
variables U; that can take on the values &plusmn;1 with P[Ui = 1] = p. What is the
expected value of X(t)?
</p>
<p>21.24 (c) Perform a computer simulation to lend credibility to the expected number
</p>
<p>of points scored in the basketball game described in Example 21.7.</p>
<p/>
</div>
<div class="page"><p/>
<p>736 CHAPTER 21. POISSON RANDOM PROCESSES
</p>
<p>21.25 (t) Derive (21.13) for the case where the Ui 'S have the same mean and are
independent of N(to) . Start your derivation with the expression
</p>
<p>and then follow the same approach as given in Section 21.6. You do not need
</p>
<p>the characteristic function to do this.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 21A
</p>
<p>Joint PDF for Interarrival
</p>
<p>Times
</p>
<p>We prove in this appendix that the first two interarrival times Zl, Z2 are lID with
</p>
<p>Z; f"Vexp(),) . The general case of any number of interarrival times can similarly be
</p>
<p>proven to be lID with an exp(),) PDF. We now refer to Figure 21.4 and prove that
</p>
<p>the joint CDF factors and each marginal CDF is that corresponding to the exp(),)
</p>
<p>PDF. The joint CDF is given as
</p>
<p>(21A.1)
</p>
<p>which follows from (13.12) where A = {Z2 : Z2 ~ 6}. But if Zl = Zl, then Z2 ~ 6 if
and only if N (Zl +6) - N (Zl) ~ 1 since an arrival must have occurred in [Zl, Zl+6].
Hence,
</p>
<p>and because the event Zl = Zl is equivalent to the increment N(Zl) - N(O) = 1,
and the increments are independent and stationary, we have
</p>
<p>P[Z2 ~ 61Zl = Zl] P[N(Zl + 6) - N(Zl) ~ 11Zl = Zl]
P[N(Zl + 6) - N(zd ~ 1]
</p>
<p>= P[N(6) ~ 1]
</p>
<p>Using this in (21A.1) produces
</p>
<p>(independence)
</p>
<p>(stationarity) .</p>
<p/>
</div>
<div class="page"><p/>
<p>738 CHAPTER 21. POISSON RANDOM PROCESSES
</p>
<p>P[ZI ~ 6, Z2 ~ 6] 16 P[N(6) ~ l]pz1 (zl)dz1
1~1 [1 - P[N(6) &lt; 1]]pzl (zl)dz1
</p>
<p>16 (1 - P[N(6) = O])PZl (zl)dz1
16 (1 - exp( -)..6))PZl (zt}dz1
</p>
<p>= [1 - exp( -)..6)]16 PZl(zl)dz1
[1- exp(-)..6)]P[Zl ~ 6]
</p>
<p>[1 - exp( -)..6)]P[N(6) ~ 1]
</p>
<p>= [1- exp(-)..6)][1 - P[N(6) &lt; 1]]
</p>
<p>[1 - exp( -)..6)][1 - exp(-)..6)]
</p>
<p>= P[ZI ~ Zl]P[Z2 ~ Z2]:
</p>
<p>It is seen that the joint CDF factors into the product of the marginal CDFs, where
</p>
<p>each marginal is the CDF of an exp()..) random variable. Thus, the first two inter-
</p>
<p>arrival times are IID with PDF exp()..) .</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 22
</p>
<p>Markov Chains
</p>
<p>22.1 Introduction
</p>
<p>We have seen in Chapter 16 that an important random process is the lID random
</p>
<p>process. When applicable to a specific problem, it lends itself to a very simple
</p>
<p>analysis. A Bernoulli random process, which consists of independent Bernoulli trials,
</p>
<p>is the archetypical example of this. In practice, it is found , however, that there is
</p>
<p>usually some dependence between samples of a random process. In Chapters 17 and
</p>
<p>18 we modeled this dependence using wide sense stationary random process theory,
</p>
<p>but restricted the modeling to only the first two moments. In an effort to introduce a
</p>
<p>more general dependence into the modeling of a random process, we now reconsider
</p>
<p>the Bernoulli random process but assume dependent samples. We briefly introduced
</p>
<p>this extension in Example 4.10 as a sequence of dependent Bernoulli trials. The
</p>
<p>dependence of the PMF that we will be interested in is dependence on the previous
</p>
<p>trial only. This type of dependence leads to what is generically referred to as a
</p>
<p>Markov random process. A special case of this for a discrete-time/discrete-valued
</p>
<p>(DTDV) random process is called a Markov chain. Specifically, it has the property
</p>
<p>that the probability of the random process X[n] at time n = no only depends upon
the outcome or realization of the random process at the previous time n = no - 1. It
</p>
<p>can then be viewed as the next logical step in extending an lID random process to a
</p>
<p>random process with statistical dependence. Recall from Chapter 8 that for discrete
</p>
<p>random variables statistical dependence is quantified using conditional probabilities.
</p>
<p>The reader should review Example 4.10 and also Chapter 8 in preparation for our
</p>
<p>discussion of Markov chains.
</p>
<p>Although we will restrict our description to a DTDV Markov random process,
</p>
<p>i.e., the Markov chain, there are many generalizations that are important in practice.
</p>
<p>The interested reader can consult the excellent books by [Bharucha-Reid 1988],
</p>
<p>[Cox and Miller 1965], [Gallagher 1996] and [Parzen 1962] for these other random
</p>
<p>processes. Before proceeding with our discussion we present an example to illustrate
</p>
<p>typical concepts associated with a Markov chain.</p>
<p/>
</div>
<div class="page"><p/>
<p>740 CHAPTER 22. MARKOV CHAINS
</p>
<p>In the game of golf it is very desirable to be a good putter. The best golfers
</p>
<p>in the world are able to hit a golf ball lying on the green into the hole using only
</p>
<p>a few strokes, called putting the ball. At times they can even "one-putt" the ball,
</p>
<p>in which they require only a single stroke to hit the ball into the hole. Of course,
</p>
<p>their chances of doing so rely heavily on how far the ball is from the hole when
</p>
<p>they first reach the green. If the ball is say 3 feet from the hole, then they will
</p>
<p>almost always one-putt. If, however, it is near the edge of the green, possibly 20
</p>
<p>feet from the hole, then their chances are small. For our hypothetical golfer we will
</p>
<p>assume that her chance of a one-putt is 50% at the start of a round of golf, i.e., at
</p>
<p>hole one. If she one-putts on hole one, then her chances on hole two will remain at
</p>
<p>50%. If not, she becomes somewhat discouraged which reduces her chances at hole
</p>
<p>two to only 25%. Hence , at each hole her chances of a one-putt are 50% if she has
</p>
<p>one-putted the previous hole and 25% if she has not. To model this situation we
</p>
<p>let X[n] = 1 for a one-putt at hole nand X[n] = 0 otherwise. We label hole one
by n = O. A round of golf, which consists of 18 holes, produces a sequence of 18
</p>
<p>1's and O's with a 1 indicating a one-putt. For the probabilities assumed a typical
</p>
<p>set of outcomes is shown in Figure 22.1. Note that she has played three rounds of
</p>
<p>2,------.--------.----.------,--------.------.
</p>
<p>1.5 .
</p>
<p>~ o&middot;: ...ll..I..lff..Il.!..I..........I..Il..I...ffl.......
-0.5
</p>
<p>504020 30
Hole, n
</p>
<p>10
</p>
<p>-1 '--__--'-__---'- '--__--'-__---'------J
</p>
<p>o
</p>
<p>Figure 22.1: Outcomes of three rounds of golf. A 1 indicates a one-putt on hole n.
</p>
<p>golf or 54 holes , of which 18 were one-putts. It appears that her probability of a
</p>
<p>one-putt is closer to 1/3 than either 1/2 or 1/4. Also, it is of interest to determine
</p>
<p>the average number of holes played between one-putts. The actual number varies
</p>
<p>as seen in Figure 22.1 and is {4, 1,3,3,1,1,3,1 ,2,3,11,3,1,3,4,1,1} for an average
</p>
<p>of 46/17 = 2.70. It would seem that the expected number of holes played between
</p>
<p>one-putts, about 3, is the reciprocal of the probability of a one-putt, about 1/3.
</p>
<p>This suggests a geometric-type PMF, which we will confirm in Section 22.6.
</p>
<p>Probabilistically, we are observing a sequence of dependent Bernoulli trials. The</p>
<p/>
</div>
<div class="page"><p/>
<p>22.1. INTRODUCTION 741
</p>
<p>Prone-putt at hole nlone-putt at hole n - 1] =
</p>
<p>P[X[n] = 11X[n -1] = 1]
</p>
<p>P[X[n] = OIX[n - 1] = 0]
</p>
<p>P[X[n] = 11X[n - 1] = 1]
</p>
<p>P[X[n] = OIX[n - 1] = 1] =
</p>
<p>dependence arises (in contrast to the usual Bernoulli random process which had
</p>
<p>independent trials) due to the probability of a one-putt at hole n being dependent
</p>
<p>upon the outcome at hole n - 1. We can model this dependence using conditional
</p>
<p>probabilities to say that
</p>
<p>1
prone-putt at hole nino one-putt at hole n - 1] = 4
</p>
<p>1
</p>
<p>2
</p>
<p>or
</p>
<p>1
P[X[n] = 11X[n - 1] = 0] = 4
</p>
<p>1
</p>
<p>2
</p>
<p>Completing the conditional probability description, we have
</p>
<p>3
</p>
<p>4
1
</p>
<p>P[X[n] = 11X[n -1] = 0] = 4
</p>
<p>1
</p>
<p>2
1
</p>
<p>2
</p>
<p>Note that we have assumed that the conditional probabilities do not change with
</p>
<p>"time" (actually hole number) . Lastly, we require the initial probability of a one-
</p>
<p>putt for the first hole. We assign this to be P[X[O] = 1] = 1/2. In summary, we
have two sets of conditional probabilities and one set of initial probabilities which
</p>
<p>can be arranged conveniently using a matrix and vector to be
</p>
<p>and
</p>
<p>p = [
P[X[n] = OIX[n - 1] = 0] P[X[n] = 11X[n - 1] = 0] ]
P[X[n] = OIX[n -1] = 1] P[X[n] = 11X[n -1] = 1]
</p>
<p>[i n
(22.1)
</p>
<p>prO]
[
</p>
<p>P[X[O] = 0] ]
</p>
<p>P[X[O] = 1]
</p>
<p>[:J.
(22.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>742 CHAPTER 22. MARKOV CHAINS
</p>
<p>1
4
</p>
<p>3 1
4 2
</p>
<p>I 1
</p>
<p>: 2
initial probabili ty - - - - .J
</p>
<p>Figure 22.2: Markov state probability diagram for putting example.
</p>
<p>The probabilities can also be summarized using the diagram shown in Figure 22.2,
</p>
<p>where for example the conditional probability of a one-putt on hole n given that the
</p>
<p>golfer has not one-putted on hole n - 1 is 1/4. We may view this diagram as one
</p>
<p>in which we are in "state" 0, which corresponds to the previous outcome of no one-
</p>
<p>putt and will move to "state" 1, which corresponds to a one-putt, with a conditional
</p>
<p>probability of 1/4. If we do move to a new state, it means the outcome is a 1 and
</p>
<p>otherwise, the outcome is a O. In int erpreting the diagram one should visualize that
</p>
<p>a 0 or 1 is emit ted as we enter the 0 or 1 state, respectively. Then, the current state
</p>
<p>becomes the last value emitted. Also, our initial un conditional probabilities of 1/2
</p>
<p>and 1/2 of entering state 0 or state 1 are shown as dashed lines. The diagram is called
</p>
<p>the Markov state probability diagram. The use of the term "state" is derived from
</p>
<p>physics in that the future evolution (in terms of probabilities) of the process is only
</p>
<p>dependent upon the cur rent state and not upon how the process arrived in that state.
</p>
<p>The probabilistic structure summarized in Figure 22.2 is called a Markov chain.
</p>
<p>As mentioned previously, it is a DTDV random process. Although we have used a
</p>
<p>dependent Bernoulli random process as an example, it easily generalizes to any finite
</p>
<p>number of states. It is common in the discussion of Markov chains to term the matrix
</p>
<p>of conditional probabilities P in (22.1) as the state transition probability matrix or
</p>
<p>more succinctly the transition probability matrix. The initial probability vector prO]
in (22.2) is called the initial state probability vector or more succinctly the initial
</p>
<p>probability vector. Note that in using the state probability diagram to summarize
</p>
<p>the Markov chain we will henceforth omit the initial probability assignment in the
</p>
<p>diagram but it should be kept in mind that it is necessary in order to complete the
</p>
<p>description.
</p>
<p>As an example of a typical probability computation, consider the probability of
</p>
<p>X[O] = 0, X[I] = 1, X[2] = 1 versus X [O] = 1, X[I] = 1, X[2] = 1. Then, using the
chain rule (see (4.10)) we have
</p>
<p>P[X[O] = 0,X[1] = 1,X[2] = 1] = P[X[2] = 1IX[1] = 1,X[0] = 0]
&middot;P[X[1] = 1IX[0] = O]P[X[O] = 0].</p>
<p/>
</div>
<div class="page"><p/>
<p>22.1. INTRODUCTION 743
</p>
<p>But due to the assumption that the probability of X[n] only depends upon the
out come at time n - 1, which is called the M arkov property, we have
</p>
<p>P[X[2 ] = lIX[I] = I ,X[O] = 0] = P[X [2] = l IX[I] = 1]
</p>
<p>and therefore
</p>
<p>P[X[O] = 0, X [I ] = 1, X[2] = 1] P[X[2] = l IX[I] = I ]P[X[I] = lIX[O] = 0]
</p>
<p>. P[X[O] = 0] .
</p>
<p>But from Figure 22.2 this is
</p>
<p>P[X[O] = O,X[I] = I ,X[2] = 1] = (~) (~) (~) = 11
6
</p>
<p>,
</p>
<p>Similary,
</p>
<p>P[X[O] = I ,X[I] = I ,X[2] = 1] = P[X[2] = lIX[I] = I]P[X[I] = lIX[O] = 1]
</p>
<p>.P[X[O] = 1] = (~) (~) (~) = ~ .
</p>
<p>We see that joint probabilities are easily det ermined from the initial probabilities
</p>
<p>and the transit ion probabilities. If we are only interested in the mar ginal PMF at a
</p>
<p>given t ime say P[X [n] = k] for k = 0,1 , as , for example, P[X[2] = 1], we need only
sum over the other vari ables of the joint P MF . This produces
</p>
<p>1 1
</p>
<p>P[X [2 ] = 1] = LLP[X[O] = i ,X[I] = j ,X[2] = 1]
i=O j =O
</p>
<p>1 1
</p>
<p>LLP[X[2] = lIX[O] = i ,X[I] = j]P[X[I] =jIX[O] = i]
i=O j=O
</p>
<p>. P[X[O] = i]
1 1
</p>
<p>L L P[X[2] = lIX[I] = j]P[X[I] = jIX[O] = i]P[X[O] = i]
i = O j=O
</p>
<p>(Markov property)
</p>
<p>v
</p>
<p>P[X [l ]=j]
</p>
<p>1 1
</p>
<p>LP[X[2] = lIX[I ] = j ]LP[X[I] = j IX[O] = i]P[X[O] = i].
j=O i =O
</p>
<p>,"---------...,,---------'
</p>
<p>Note that P[X [I ] = j] can be found and then used to find P[X [2] = 1]. Of course,
t his is get ting somewhat messy algebra ically but as shown in the next sect ion the
</p>
<p>use of vectors and matrices will simplify the computation.
</p>
<p>Finally, some questions of interest to the golfer are:</p>
<p/>
</div>
<div class="page"><p/>
<p>744 CHAPTER 22. MARKOV CHAINS
</p>
<p>1. After playing many holes , will the probability of a one-putt settle down to some
</p>
<p>constant value? Mathematically, will P[X[n] = k] converge to some constant
PMF as n -+ oo?
</p>
<p>2. Given that the golfer has just one-putted, how many holes on the average will she
</p>
<p>have to wait until the next one-putt? Or given that she has not one-putted,
</p>
<p>how many holes on the average will she have to wait until she one-putts? In
</p>
<p>the first case, mathematically we wish to det ermine if given X[no] = 1 and
X[no + 1] = 0, . .. , X[no + N - 1] = 0, X[no + N] = 1, what is E[N]?
</p>
<p>We will answer both these questions shortly, but before doing so some definitions
</p>
<p>are necessary.
</p>
<p>22.2 Summary
</p>
<p>A motivating example of a Markov chain is given in Section 22.1. A Markov chain is
</p>
<p>defined by the property of (22.3). The state transition probabilities, which describe
</p>
<p>the probabilities of movements between states, is given by (22.4). When arranged
</p>
<p>in a matrix it is equivalent to (22.5) for a two-state Markov chain and is called
</p>
<p>the transition probability matrix. The probabilities of the states are defined in
</p>
<p>(22.6) and succinctly summarized by the vector of (22.7) for a two-state Markov
</p>
<p>chain. Table 22.1 summarizes the notational conventions. The state probability
</p>
<p>vector can be found for any time by using (22.9). To evaluate a power of the
</p>
<p>transition probability matrix (22.12) can be used if the eigenvalues of the matrix
</p>
<p>are distinct. For a two-state Markov chain the state probabilities are explicitly found
</p>
<p>in Section 22.4 with the general transition probability matrix given by (22.14). For
</p>
<p>an ergodic Markov chain the state probabilities approach a constant value as time
</p>
<p>increases and this value is found by solving (22.17). Also, the value of the n-step
</p>
<p>transition probability matrix approaches the steady-state value given by (22.19). In
</p>
<p>Section 22.6 the occupation time of a state for an ergodic Markov chain is shown
</p>
<p>to be given by the steady-state probabilities and also , the mean recurrence time
</p>
<p>is the inverse of the occupation time. An explicit solution for the steady-state
</p>
<p>or stationary probabilities can be found using (22.22). The MATLAB code for
</p>
<p>a computer simulation of a 3-state Markov chain is given in Section 22.8 while a
</p>
<p>concluding real-world example is given in Section 22.9.
</p>
<p>22.3 Definitions
</p>
<p>We restrict ourselves to a discrete-time random process X[n] with K possible values
or states. In the introduction K = 2 and the values were 0,1. This is a DTDV
</p>
<p>random process that starts at n = &deg;(semi-infinite). We define X[n] as a Markov
chain if given the entire past set of outcomes, the PMF of X[n] depends on only the</p>
<p/>
</div>
<div class="page"><p/>
<p>22.3. DEFINITIONS
</p>
<p>outcome of the previous sample X[n - 1] so that
</p>
<p>745
</p>
<p>P[X[n] = jlX[n - 1] = i , X[n - 2] = k, ... , X[O] = l] = P[X[n] = j lX[n - 1] = i].
</p>
<p>Using the concept of a PMF this is equivalent to
</p>
<p>PX[nllX[n-I] ,...,X [O] = PX[nllX[n-I ]' (22.3)
</p>
<p>This implies that the joint PMF only depends on the product of the first-order
</p>
<p>conditional PMFs and the initial probabilities, for example
</p>
<p>PX[0] ,X [I] ,X[2]
</p>
<p>=
</p>
<p>PX[211X[1 ],X[O]PX[l] IX[O]PX [0]
</p>
<p>PX[211X[I] PX[lllX[O]
'-..--" '-..--"
</p>
<p>conditional conditional
probability probability
</p>
<p>PX[O]
~
</p>
<p>initial
probability
</p>
<p>As mentioned previously, this is an extension of the idea of independence in that
</p>
<p>it asserts a type of conditional independence. Most importantly, the joint PMF is
</p>
<p>obtained as the product of first-order conditional PMFs. An example follows.
</p>
<p>Example 22.1 - A coin with memory
</p>
<p>Assume that a coin is tossed three times with the outcome of a head represented by
</p>
<p>a 1 and a tail by a O. If the coin has memory and is modeled by the state probability
diagram of Figure 22.2, determine the probability of the sequence HTH. Note that
</p>
<p>the conditional probabilities are equivalent to those in Example 4.10. Writing the
</p>
<p>joint probability in the more natural order of increasing time, we have
</p>
<p>P[X[O] = 1, X[l] = 0, X [2] = 1] = P[X[O] = l]P[X[l] = OIX[O] = 1]
</p>
<p>&middot;P[X[2] = lIX[l] = 0]
</p>
<p>( ~) (~) (~) = 116,
Hence, the sequence HTH is less probable than for a fair coin without memory for
</p>
<p>which 3 independent tosses would yield a probability of 1/8. Can you explain why
</p>
<p>this is less probable?
</p>
<p>o
We will now use the terminology of the introduction to refer to the conditional
</p>
<p>probabilities P[X[n] = jlX[n - 1] = i] as the state transition probabilities. Note
that they are assumed not to depend on n and therefore the Markov chain is said
</p>
<p>to be homogeneous. To simplify the notation further and to prepare for subsequent
</p>
<p>probability calculations we denote the state transition probabilities as
</p>
<p>Pij = P[X[n] = jlX[n - 1] = i] i = 0,1 , ... ,K - 1; j = 0,1 , ... ,K - 1. (22.4)
</p>
<p>This is the conditional probability of observing an outcome j given that the previous
</p>
<p>outcome was i. It is also said that Pij is the probability of the chain moving from</p>
<p/>
</div>
<div class="page"><p/>
<p>746 CHAPTER 22. MARKOV CHAINS
</p>
<p>state i to state j, but keep in mind that it is a conditional probability. In the case
</p>
<p>of a two-state Markov chain or K = 2, we have i = 0, 1; j = 0, 1 and the state
transition probabilities are most conveniently arranged in a matrix P. From (22.1)
</p>
<p>we have
</p>
<p>P = [POO POI] (22.5)
P lO Pn
</p>
<p>which as previously mentioned is the transition probability matrix. Note that the
</p>
<p>sum of the elements along each row must be one since they represent all the values
</p>
<p>of a conditional PMF. In accordance with the assumption of homogeneity P is a
</p>
<p>constant matrix. Finally, we define the state probabilities at time n as
</p>
<p>pi[n] = P[X[n] = i] i = 0, 1, ... ,K - 1. (22.6)
</p>
<p>(22.7)
</p>
<p>This is the probability of observing an outcome i at time ri or equivalently the PMF
</p>
<p>of X[n]. This notation is somewhat at odds with our previous notation, which would
be PX[n][i], but is a standard one. The PMF depends on n and it is this PMF that
</p>
<p>we will be most concerned. In particular, how the PMF changes with n will be of
</p>
<p>interest. Hence, a Markov chain is in general a nonstationary random process. For
</p>
<p>ease of notation and later computation we also define the state probability vector for
</p>
<p>K = 2 as
</p>
<p>p[n] = [ porn] ] .
PI[n]
</p>
<p>A summary of these definitions and notation is given in Table 22.1. An exam-
</p>
<p>ple is given next to illustrate the utility of definitions (22.4) and (22.6) and their
</p>
<p>vector/matrix representations of (22.5) and (22.7).
</p>
<p>Example 22.2 - Two-state Markov chain
</p>
<p>Consider the computation of P[X[n] = j] for a two-state Markov chain (K = 2).
Then,
</p>
<p>P[X[n] =j]
I
</p>
<p>L P[X[n -1] = i , X[n] = j]
i=O
</p>
<p>I
</p>
<p>L P[X[n] = jlX[n - 1] = i]P[X[n - 1] = i]
i=O
</p>
<p>which can now be written as
</p>
<p>I
</p>
<p>pj[n] = LPijPi[n -1]
i=O
</p>
<p>j = 0,1.
</p>
<p>In vector/matrix notation we have
</p>
<p>Jporn] ..PI[n] J, = Jporn - 1] v PI[n - 1] l [;~~ ;~~]
pT[n] pT[n-l] ~</p>
<p/>
</div>
<div class="page"><p/>
<p>22.3. DEFINITIONS 747
</p>
<p>Terminology Description Notation
</p>
<p>Random process DTDV X[n] n = 0,1, ...
</p>
<p>State Sample space k = 0,1, ... ,J( - 1
</p>
<p>State
probability PMF of X[n] p[n] = [porn] ... PK-dn]V
vector
</p>
<p>pdn] = P[X[n] = k]
</p>
<p>[ P
</p>
<p>oo POI ... PO,K-l ]
State transition PlO Pn ... PI K-I
probability matrix
</p>
<p>Conditional prob. p=
</p>
<p>P K ~ ; ' K - lP K ~ I , O PK-I,l ...
Pi j = P[X[n] = jlX[n - 1] = i]
</p>
<p>Initial state
PMF of X[O] prO]probability vector
</p>
<p>Table 22.1: Markov chain definitions and notation.
</p>
<p>or
</p>
<p>(22.8)
</p>
<p>The evolution of the state probability vector in time is easily found by post-multiplying
</p>
<p>the previous state probability vector (in row form) by the transition probability ma-
</p>
<p>trix.
</p>
<p>&lt;:;
Note that we have defined p[n] as a column vector in accordance with our usual
convention. Other textbooks may use row vectors. A numerical example follows.
</p>
<p>Example 22.3 - Golfer one-putting
</p>
<p>From Figure 22.2 we have the transition probability matrix and initial state prob-
</p>
<p>ability vector as</p>
<p/>
</div>
<div class="page"><p/>
<p>748
</p>
<p>To find p[l] we use (22.8) to yield
</p>
<p>pT[l]
</p>
<p>CHAPTER 22. MARKOV CHAINS
</p>
<p>pT[O]p
</p>
<p>[
3 n1 ~ ] 42 12
</p>
<p>5 i ].8"
As expected the elements of p[l] sum to one. Also, note that pill] = 3/8 &lt; 1/2,
which means that initially the probability of a one-putt is 1/2 but after the first
</p>
<p>hole, it is reduced to 3/8. Can you explain why? We can continue in this manner
</p>
<p>to compute the state probability vector for n = 2 as
</p>
<p>and so forth for all n.
</p>
<p>22.4 Computation of State Probabilities
</p>
<p>We are now in a position to determine p[n] for all n. The key of course is the
</p>
<p>recursion of (22.8). In a slightly more general form where we wish to go from p[nIJ
</p>
<p>to p[n2], the resulting equations are known as the Chapman-Kolmogorovequations.
</p>
<p>For example, if n2 = nl + 2, then
</p>
<p>pT[n2 -l]P
</p>
<p>(pT[n2 - 2]P)P
</p>
<p>pT[nl]p2.
</p>
<p>The matrix p 2 is known as the two-step transition probability matrix. It allows the
</p>
<p>state probabilities for two steps into the future to be found if we know the state
</p>
<p>probabilities at the current time. In general, then we see that
</p>
<p>as is easily verified, where P" is the n-step transition probability matrix. In partic-
</p>
<p>ular, if nl = 0, then
</p>
<p>n = 1,2, .. . (22.9)
</p>
<p>which can be used to find the state probabilities for all time. These probabilities can
</p>
<p>exhibit markedly different behaviors depending upon the entries in P. To illustrate</p>
<p/>
</div>
<div class="page"><p/>
<p>22.4. COMPUTATION OF STATE PROBABILITIES 749
</p>
<p>1 - a 1- f3
</p>
<p>f3
</p>
<p>Figure 22.3: General two-st ate probability diagram.
</p>
<p>this consider the two-state Markov chain with the state probability diagram shown
</p>
<p>in Figure 22.3. This corresponds to the transition probability matrix
</p>
<p>p=[l- a a ]
f3 1- f3
</p>
<p>(22.10)
</p>
<p>where 0 ~ a ~ 1 and 0 ~ f3 ~ 1. As always the rows sum to one. We give an
example and then genera lize the results.
</p>
<p>Example 22.4 - State probability vector computation for all n
</p>
<p>Let a = f3 = 1/2 and pT[O] = [10] so that we are intially in st ate 0 and the
transition to eit her of the st ates is equally probable. Then from (22.9) we have
</p>
<p>pT [n] = pT[O]pn
</p>
<p>[ 1 o ] [ : :r
pT[l] = [ 1 o ] [ : !J ~ [1 1]2
pT[2] [ 1 o] [:tr= [ 1 1]2
</p>
<p>Clearly, pT[n ] = [ 1 1] for all n ~ 1. The Markov cha in is said to be in steady-
sta te for n ~ 1. In addit ion, for n ~ 1, the PMF pT[n] = [1 1] is called the
steady-state PMF.
</p>
<p>o
More generally, t he st ate probabilities of a Markov chain mayor may not approach
</p>
<p>a steady-state value. It dep end s upon the form of P. To study the behavior more
</p>
<p>thoroughly we require a means of det ermining P ". To do so we next review the
</p>
<p>diagonalizat ion of a matrix using an eigenanalysis (see also Appendix D).</p>
<p/>
</div>
<div class="page"><p/>
<p>750
</p>
<p>Computing Powers of P
</p>
<p>CHAPTER 22. MARKOV CHAINS
</p>
<p>Assuming that the eigenvalues of P are distinct, it is possible to find eigenvectors
</p>
<p>Vi that are linearly independent. Arranging them as the columns of a matrix and
</p>
<p>assuming that K = 2, we have the modal matrix Y = [VI V2] which is a nonsingular
matrix since the eigenvectors are linearly independent. Then we can write that
</p>
<p>y-1py = A (22.11)
</p>
<p>where A = diag(Al ' A2) and Ai is the eigenvalue corresponding to the ith eigenvector
of P . Now from (22.11) we have that P = YAy-1 and therefore, the powers of P
can be found as follows.
</p>
<p>p 2 (YAy-1)(YAy-l ) = YA2y-1
</p>
<p>p 3 p 2p = (YA2y-I)YAy-1 = YA3y-1
</p>
<p>and in general we have that
</p>
<p>(22.12)
</p>
<p>But since A is a diagonal matrix its powers are easily found as
</p>
<p>and finally we have that
</p>
<p>(22.13)
</p>
<p>It should be observed that the eigenvectors need not be normalized to unity length
</p>
<p>for (22.13) to hold . As an example, if
</p>
<p>P=[~ t]
then the eigenvalues are found from the characteristic equation as the solutions of
</p>
<p>det(P - AI) = O. This yields the equation (1/2 - A)(1 - A) = 0 which produces
Al = 1/2 and A2 = 1. The eigenvectors are found from
</p>
<p>(P - AII)VI ~ [ ~ t] VI = 0 =} V, = [ ~ ]
(P - A2I)V2 = [-01 3] V2 ~ 0 =} V2 ~ [ : ]</p>
<p/>
</div>
<div class="page"><p/>
<p>22.4. COMPUTATION OF STATE PROBABILITIES
</p>
<p>and hence the modal matrix and its inverse are
</p>
<p>v [VI V2] = [~ ~]
</p>
<p>V-I [~~1 ].
Finally, for n ~ 1 we can easily find the powers of P from (22.13) as
</p>
<p>This can easily be verified by direct multiplication of P .
</p>
<p>751
</p>
<p>Now returning to the problem of determining the state probability vector for
</p>
<p>the general two-state Markov chain, we need to first find the eigenvalues of (22.10).
</p>
<p>The characteristic equation is
</p>
<p>[
l - O: - A 0: ]
</p>
<p>det (P - AI) = det [3 1 _ [3 _ A = 0
</p>
<p>which produces (1 - 0: - A)(l - [3 - A) - 0:[3 = 0 or
</p>
<p>A2 + (0:+ [3 - 2)A + (1 - 0: - (3) = O.
</p>
<p>Letting r = 0:+ [3 , which is nonnegative, we have that A2 + (r - 2)A + (1 - r) = 0
for which the solution is
</p>
<p>-(r - 2) &plusmn; J(r - 2)2 - 4(1 - r)
</p>
<p>2
-(r-2)&plusmn;r
</p>
<p>2
1 and 1 - r .
</p>
<p>Thus, the eigenvalues are Al = 1 and A2
</p>
<p>corresponding eigenvectors as
</p>
<p>1 - 0: - [3. Next we determine the
</p>
<p>{P - AjI)vj ~ [fi" ~f3] vi ~ 0 =&gt; vi = [ : ]
</p>
<p>{P - A2I)v2 ~ [~ :] V2 = 0 =&gt; V2 ~ [ _1~ ]</p>
<p/>
</div>
<div class="page"><p/>
<p>752 CHAPTER 22. MARKOV CHAINS
</p>
<p>and therefore the modal matrix and its inverse are
</p>
<p>v
</p>
<p>V -I -1 ]
1 .
</p>
<p>With the matrix
</p>
<p>we have
</p>
<p>(22.14)
</p>
<p>pn _ _ 1 [1 1] [ 1 0 ] [_fi -11]
- 1+f3/a 1 -~ 0 (l-a-f3)n -1
</p>
<p>and after some algebra
</p>
<p>pn = [ar a : ~] + (1 - a - f3t [:i -~~ ~ ].
a+ ~ a+ f3 a+ f3 a+f3
</p>
<p>We now examine three cases of interest. They are distinguished by the value that
</p>
<p>&gt;'2 = l-a- f3 takes on. Clearly, as seen from (22.14) this is the factor that influences
the behavior of P" with n. Since a and f3 are both conditional probabilities we must
have that 0::; a+ f3::; 2 and hence -1 ::; &gt;'2 = l-a- f3 ::; 1. The cases are delineated
by whether this eigenvalue is st rictly less than one in magnitude or not.
</p>
<p>Case 1. -1 &lt; 1 - a - f3 &lt; 1
Here 11 - a - f31&lt; 1 and therefore from (22.14) as n -+ 00
</p>
<p>pn -+ [01 a:f3].
a+f3 a+ f3
</p>
<p>As a result ,
</p>
<p>= [~ a~ {3]
</p>
<p>(22.15)
</p>
<p>for any p[O]. Hence, the Markov chain approaches a steady-state irregardless
</p>
<p>of the initial state probabilities. It is said to be an ergodic Markov chain,
</p>
<p>the reason for which we will discuss later. Also , the state probability vector
</p>
<p>approaches the steady-state probability vector pT[oo], which is denoted by
</p>
<p>T_[ J-[JL a]7r - 7fo 7fl - a+f3 a+f3 .
Finally, note that each row of P" becomes the same as n -+ 00.
</p>
<p>(22.16)</p>
<p/>
</div>
<div class="page"><p/>
<p>22.4. COMPUTATION OF STATE PROBABILITIES 753
</p>
<p>Case 2. 1 - a - (3 = 1 or a = (3 = 0
If we draw the state probability diagram in this case, it should become clear
</p>
<p>what will happen. This is shown in Figure 22.4a, where the zero transition
</p>
<p>probability branches are omitted from the diagram. It is seen that there is no
</p>
<p>chance of leaving the initial state so that we should have p[n] = p[O] for all n.
</p>
<p>To verify this , for a = (3 = 0, the eigenvalues are both 1 and therefore A = I.
</p>
<p>Hence, P = I and P" = I. Here the Markov chain also attains steady-state
</p>
<p>and 1T" = p[O] but the steady-state PMF depends upon the initial probability
</p>
<p>vector, unlike in Case 1. Note that the only possible realizations are 0000 ...
</p>
<p>and 1111 ....
</p>
<p>a=l
</p>
<p>cG
1-a=1
</p>
<p>(a) 0: = f3 = 0
</p>
<p>GD
1-(3=1 (3=1
</p>
<p>(b) 0: = f3 = 1
</p>
<p>Figure 22.4: State probability diagrams for anomalous behaviors of two-state
</p>
<p>Markov chain.
</p>
<p>Case 3. 1 - a - (3 = -lor a = (3 = 1
</p>
<p>It is also easy to see what will happen in this case by referring to the state
</p>
<p>probability diagram in Figure 22.4b. The outcomes must alternate and thus
</p>
<p>the only realizations are 0101 ... and 1010 ... , with the realization generated
</p>
<p>depending upon the initial state. Unlike the previous two cases, here there are
</p>
<p>no steady-state probabilities as we now show. From (22.14) we have
</p>
<p>pn = [t n+ (_1)n [!! -l]
[0
</p>
<p>1
01 ]= furne~n
</p>
<p>[~ ~] for n odd.
</p>
<p>Hence, the state probability vector is</p>
<p/>
</div>
<div class="page"><p/>
<p>754 CHAPTER 22. MARKOV CHAINS
</p>
<p>pT[n] pT[O]pn = [pO[O] PI[O]] pn
</p>
<p>{
</p>
<p>[ pO[0] PI[0]] for n even
</p>
<p>[PI[O] Po [0] ] for n odd.
</p>
<p>As an example, if pT[O] = [1/4 3/4], then
</p>
<p>T {[ i i] for n even
p [n] =
</p>
<p>[i i] for n odd
as shown in Figure 22.5. It is seen that the state probabilities cycle between
</p>
<p>two PMFs and hence there is no steady-state.
</p>
<p>.'0.8
</p>
<p>&gt;:: 0.6
o
~
</p>
<p>0.4
</p>
<p>0.2
</p>
<p>n
</p>
<p>0.8
</p>
<p>&gt;:: 0.6
</p>
<p>~
</p>
<p>0.4
</p>
<p>0.2
</p>
<p>o
o 2 4
</p>
<p>n
6 8 10
</p>
<p>(a) porn] = P[X[n] = 0] (b) pI[n] = P[X[n] = 1]
</p>
<p>Figure 22.5: Cycling of state probability vector for Case 3.
</p>
<p>The last two cases are of little practical importance for a two-state Markov chain
</p>
<p>since we usually have 0 &lt; Q &lt; 1 and 0 &lt; f3 &lt; 1. However, for a K-state Markov
chain it frequently occurs that some of the transition probabilities are zero (corre-
</p>
<p>sponding to missing branches of the state probability diagram and an inability of the
</p>
<p>Markov chain to transition between certain states). Then, the dependence upon the
</p>
<p>initial state and cycling or periodic PMFs become quite important. The interested
</p>
<p>reader should consult [Gallagher 1996] and [Cox and Miller 1965] for further details.
</p>
<p>We next return to our golfing friend.
</p>
<p>Example 22.5 - One-putting
</p>
<p>Recall that our golfer had a transition probability matrix given by
</p>
<p>h[i n</p>
<p/>
</div>
<div class="page"><p/>
<p>22.4. COMPUTATION OF STATE PROBABILITIES 755
</p>
<p>It is seen from (22.10) that a = 1/4 and {3 = 1/2 and so this corresponds to Case
</p>
<p>1 in which the same steady-state probability is reached regardless of the initial
</p>
<p>probability vector. Hence , as n ~ 00, P " will converge to a constant matrix and
</p>
<p>therefore so will p [n]. After many rounds of golf the probability of a one-putt or
</p>
<p>of going to state 1 is found from the second element of the stationary probability
</p>
<p>vector zr. This is from (22.16)
</p>
<p>7r
T = [ 1ro 1rl] [ (Jp o:~ p
</p>
<p>= [m m]
[~ ~]
</p>
<p>so that her probability of a one-putt is now only 1/3 as we surmised by examination
</p>
<p>of Figure 22.1. At the first hole it was pdO] = 1/2. To determine how many holes
she must play until this steady-state probability is attained we let this be n = n ss
and determine from (22.14) when (1 - a - (3 )nss = (1/4)n ss ~ O. This is about
n ss = 10 for which (1/4)10 = 10- 6 . The actual state probability vector is shown in
Figure 22.6 using an initial state probability of pT[O] = [1/2 1/2]. The steady-state
values of 7r = [2/3 1/3]T are also shown as dashed lines.
</p>
<p>f-- - - ..., - - - - &gt;-- - - -
</p>
<p>.'.
</p>
<p>- - ..J - - - - &gt;---&lt; - - -
</p>
<p>0.8
</p>
<p>~ 0.6
o
I::l..
</p>
<p>0.4
</p>
<p>0.2
</p>
<p>o
o 2 4
</p>
<p>n
6 8 10
</p>
<p>0.8
</p>
<p>~ 0.6
</p>
<p>~
</p>
<p>0.4
</p>
<p>0.2
</p>
<p>o
o 2 4
</p>
<p>n
6 8 10
</p>
<p>(a) porn] = P[X[n] = 0] (b) pI(n] = P[X[n] = 1]
</p>
<p>Figure 22.6: Convergence of state probability vector for Case 1 with a = 1/4 and
</p>
<p>{3 = 1/2.</p>
<p/>
</div>
<div class="page"><p/>
<p>756 CHAPTER 22. MARKOV CHAINS
</p>
<p>22.5 Ergodic Markov Chains
</p>
<p>We saw in the previous section that as n -+ 00, then for some P the state probability
</p>
<p>vector approaches a steady-state value irregardless of the initial state probabilities.
</p>
<p>This was Case 1 for which each element of P was nonzero or
</p>
<p>[
I - a a ]
</p>
<p>P= (3 1-(3 &gt;0
</p>
<p>where the "&gt; 0" is meant to indicate that every element of P is greater than zero.
Equivalently, all the branches of the state probability diagram were present. A
</p>
<p>Markov chain of this type is said to be ergodic in that a temporal average is equal
</p>
<p>to an ensemble average as we will later show. The key requirement for this to be
</p>
<p>true for any K-state Markov chain is that the K x K transition probability matrix
</p>
<p>satisfies P &gt; O. The matrix P then has some special properties. We already have
pointed out that the rows must sum to one; a matrix of this type is called a stochastic
</p>
<p>matrix, and for ergodicity, we must have P &gt; 0; a matrix satisfying this requirement
is called an irreducible stochastic matrix. The associated Markov chain is known
</p>
<p>as an ergodic or irreducible Markov chain. A theorem termed the Perron-Frobenius
</p>
<p>theorem [Gallagher 1996] states that ifP &gt; 0, then the transition probability matrix
will always have one eigenvalue equal to 1 and the remaining eigenvalues will have
</p>
<p>magnitudes strictly less than 1. Such was the case for the two-state probability
</p>
<p>transition matrix of Case 1 for which Al = 1 and IA21 = 11 - a - (31 &lt; 1. This
condition on P assures convergence of P" to a constant matrix. Convergence may
</p>
<p>also occur if some of the elements of P are zero but it is not guaranteed. A slightly
</p>
<p>more general condition for convergence is that P" &gt; 0 for some n (not necessarily
n = 1). An example is
</p>
<p>(see Problem 22.13).
</p>
<p>We now assume that P &gt; 0 and determine the steady-state probabilities for a
general K-state Markov chain. Since
</p>
<p>pT[n] = pT[n - I]P
</p>
<p>and in steady-state we have that pT[n - 1] = pT[n] = pT[oo] , it follows that
</p>
<p>pT[oo] = pT[oo]P.
</p>
<p>Letting the steady-state probability vector be 1T" = p[oo], we have
</p>
<p>1T"T = 1T"T p
</p>
<p>and we need only solve for 1T" . An example follows.
</p>
<p>(22.17)</p>
<p/>
</div>
<div class="page"><p/>
<p>22.5. ERGODIC MARKOV CHAINS 757
</p>
<p>E xample 22.6 - T wo-state Markov chain
</p>
<p>We solve for the steady-state probability vector for Case 1. From (22.17) we have
</p>
<p>[7fO 7f1] = [7fO 7f1] [ 1 ~ a 1 ~ (3 ]
</p>
<p>so that
</p>
<p>7fo (1 - a)7fo + (37f1
7f1 = a7fo + (1 - (3)7f1
</p>
<p>or
</p>
<p>o -a7fo + (37f1
o a7fo - (37f1.
</p>
<p>The yields 7f1 = (a j (3)7fo since the two linear equations are identical. Of course, we
also require that 7fo + 7f1 = 1 and so this forms the second linear equation. The
solution then is
</p>
<p>7fo
(3
</p>
<p>a+(3
a
</p>
<p>a+(3
(22.18)
</p>
<p>and agrees with our previous results of (22.16).
</p>
<p>c
It can further be shown that if a steady-state probability vector exists (which will be
</p>
<p>the case if P &gt; 0) , then the solution for 7r is unique [Gallagher 1996]. Finally, note
that if we intialize the Markov chain with prO] = zr, then since pT[l ] = pT[O]p =
7r
</p>
<p>T p = 7r T , the state probability vector will be 7r for n ~ O. T he Markov chain
</p>
<p>is then stationary since the state probability vector is the same for all nand 7r is
</p>
<p>therefore refer red to as the stationary probability vector. We will henceforth use this
</p>
<p>terminology for zr.
</p>
<p>Another observation of importance is that if P &gt; 0, then P " converges, and
it converges to P ?", whose rows are identical. This was borne out in (22.15) and
</p>
<p>is true in general (see Problem 22.17) . (Note that this is not true for Case 2 in
</p>
<p>which although P " converges , it converges to I , whose rows are not the same.) As
</p>
<p>a result of this property, the steady-state value of the state probability vector does
</p>
<p>not depend upon the initial probabilities since
</p>
<p>pT[n] pT[O]pn
</p>
<p>= [ polO] PliO] ] [ +.~ P ]+ pT[OJ(1 -" - fJ)n [ _.~ g -p~P ]
a+13 a+13 a+13 a+13,
</p>
<p>v
,
</p>
<p>-tOT as n-too
</p>
<p>--+ [ 13 ~ ] = 7rTa+13 a+13</p>
<p/>
</div>
<div class="page"><p/>
<p>758 CHAPTER 22. MARKOV CHAINS
</p>
<p>independent of pT[O] . Also, as previously mentioned, if P &gt; 0, then as n -+ 00
</p>
<p>p n -+ [ aip
</p>
<p>a+p
</p>
<p>a~ p ]
a+p
</p>
<p>whose rows are identical. As a result , we have that
</p>
<p>j = 0,1 , . .. , J( - 1. (22.19)
</p>
<p>Hence, the stationary probabilities may be obtained either by solving the set of
</p>
<p>linear equations as was done for Example 22.6 or by examining a row of P" as
</p>
<p>n -+ 00. In Section 22.7 we give the general solution for the stationary probabilities.
</p>
<p>We next give another example.
</p>
<p>Example 22.7 - Machine failures
</p>
<p>A machine is in operation at the beginning of day n = O. It may break during
</p>
<p>operation that day in which case repairs will begin at the beginning of the next
</p>
<p>day (n = 1). In this case , the machine will not be in operation at the beginning
of day n = 1. There is a probability of 1/2 that the technician will be able to
repair the machine that day. If it is repaired, then the machine will be in operation
</p>
<p>for day n = 2 and if not, the technician will again attempt to fix it the next day
</p>
<p>(n = 2). The probability that the machine will operate without a failure during the
day is 7/8. After many days of operation or failure what is the probability that the
</p>
<p>machine will be working at the beginning of a day? Here there are two states, either
</p>
<p>X[nJ= 0 if the machine is not in operation at the beginning of day n, or X[nJ = 1 if
the machine is in operation at the beginning of day n. The transition probabilities
</p>
<p>are given as
</p>
<p>POI
</p>
<p>Pn
</p>
<p>P[machine operational on day nlmachine nonoperational on day n - IJ = ~
</p>
<p>= P[machine operational on day nlmachine operational on day n - IJ = ~
8
</p>
<p>7rO
</p>
<p>and so the state transition probability matrix is
</p>
<p>p~ [: n
noting that POO = 1 - POI = 1/2 and PIO = 1 - Pn = 1/8. This Markov chain is
shown in Figure 22.7. Since P &gt; 0, a steady-state is reached and the stationary
probabilities are from (22.18)
</p>
<p>(3 1 1
__ =_8 _
</p>
<p>a+(3 ~ + l 5
</p>
<p>4
7r1 = 1 - 7ro = 5'</p>
<p/>
</div>
<div class="page"><p/>
<p>22.6. FURTHER STEADY-STATE CHARACTERISTICS 759
</p>
<p>l-a= ~ I - f 3 = ~
</p>
<p>f3 - 1-8
</p>
<p>o- machine nonoperational at beginning of day
1 - machine operational at beginning of day
</p>
<p>Figure 22.7: State probability diagram for Example 22.7.
</p>
<p>The machine will be in operation at the beginning of a day with a probability of
</p>
<p>0.8.
</p>
<p>o
Note that in the last example the states of 0 and 1 are arbitrary labels. They could
</p>
<p>just as well have been "nonoperat ional" and "operat ional" . In problems such as
</p>
<p>these the state description is chosen to represent meaningful attributes of inter-
</p>
<p>est . One last comment concerns our apparent preoccupation with the steady-state
</p>
<p>behavior of a Markov chain. Although not always true, we are many times only
</p>
<p>interested in this because the choice of a starting time, i.e. , at n = 0, is not easy
to specify. In the previous example, it is conceivable that the machine in question
</p>
<p>has been in operation for a long time and it is only recently that a plant manager
</p>
<p>has become interested in its failure rate. Therefore, its initial starting time was
</p>
<p>probably some time in the past and we are now observing the states for some large
</p>
<p>n. We continue our discussion of steady-state characteristics in the next section.
</p>
<p>22.6 Further Steady-State Characteristics
</p>
<p>22.6.1 State Occupation Time
</p>
<p>It is frequently of interest to be able to determine the percentage of time that a
</p>
<p>Markov chain is in a particular state, also called the state occupation time. Such
</p>
<p>was the case in Example 22.7, although a careful examination reveals that what we
</p>
<p>actually computed was the probability of being operational at the beginning of each
</p>
<p>day. In essence we are now asking for the relative frequency (or percentage of time)
</p>
<p>of the machine being operational. This is much the same as asking for the relative
</p>
<p>frequency of heads in a long sequence of independent fair coin tosses. We have
</p>
<p>proven by the law of large numbers (see Chapter 15) that this relative frequency</p>
<p/>
</div>
<div class="page"><p/>
<p>760 CHAPTER 22. MARKOV CHAINS
</p>
<p>must approach a probability of 1/2 as the number of coin tosses approaches infinity.
</p>
<p>For Markov chains the trials are not independent and so the law of large numbers
</p>
<p>does not apply directly. However , as we now show, if steady-state is attained, then
</p>
<p>the fraction of time the Markov chain spends in a particular state approaches the
</p>
<p>steady-state probability. This allows us to say that the fraction of time that the
</p>
<p>Markov chain spends in state j is just 'Trj.
</p>
<p>Again consider a two-state Markov chain with states 0 and 1 and assume that
</p>
<p>p &gt; O. We wish to determine the fraction of time spent in state 1. For some large
n this is given by
</p>
<p>1 n+N-I
</p>
<p>N L X[j]
j=n
</p>
<p>which is recognized as the sample mean of the N state outcomes for {X[n], X[n +
1], ... ,X[n + N - I]}. We first determine the expected value as
</p>
<p>But
</p>
<p>Ex!o] [E [ ~ n};l XiiI X[O] = i]]
</p>
<p>[
</p>
<p>1 n+N-I ]
Ex [0] N ~ E[X[j]IX[O] = i] . (22.20)
</p>
<p>E[X[j]IX[O] = i] P[X[j] = 1IX[0] = i]
[Pj]il -+ 'Trl
</p>
<p>as j ~ n -+ 00 which follows from (22.19). The expected value does not depend
upon the initial state i, Therefore, we have from (22.20) that
</p>
<p>[
1n+N-I] [1 n+N-I ]
</p>
<p>E N ~ X[j] -+ Ex[o] N ~ 'Trl = 'Trl&middot;
</p>
<p>Thus, as n -+ 00, the expected fraction of time in state 1 is 'Trl. Furthermore, although
</p>
<p>it is more difficult to show, the variance of the sample mean converges to zero as
</p>
<p>N -+ 00 so that the fraction of time (and not just the expected value) spent in state
</p>
<p>1 will converge to 'TrIor
1 n+N-I
</p>
<p>N L X[j] -+ 'Trl&middot;
j=n
</p>
<p>(22.21)
</p>
<p>This is the same result as for the repeated independent tossing of a fair coin. The re-
</p>
<p>sult stated in (22.21) is that the temporal mean is equal to the ensemble mean which
</p>
<p>says that for large n , i.e. , in steady-state, ~ 'Lj::/:-l X[j] -+ 'Trl as N -+ 00. This</p>
<p/>
</div>
<div class="page"><p/>
<p>22.6. FURTHER STEADY-STATE CHARACTERISTICS 761
</p>
<p>is the property of ergodicity as previously described in Chapter 17. Thus, a Markov
</p>
<p>chain that achieves a steady-state irregardless of the initial state probabilities is
</p>
<p>called an ergodic Markov chain.
</p>
<p>Returning to our golfing friend, we had previously questioned the fraction of
</p>
<p>the time she will achieve one-putts. We know that her stationary probability is
</p>
<p>7fl = 1/3. Thus, after playing many rounds of golf, she will be one-putting about
1/3 of the time.
</p>
<p>22.6.2 Mean Recurrence Time
</p>
<p>Another property of the ergodic Markov chain that is of interest is the average
</p>
<p>number of steps before a state is revisited. For example, the golfer may wish to
</p>
<p>know the average number of holes she will have to play before another one-putt
</p>
<p>occurs, given that she has just one-putted. This is equivalent to determining the
</p>
<p>average number of steps the Markov chain will undergo before it returns to state
</p>
<p>1. The time between visits to the same state is called the recurrence time and the
</p>
<p>average of this is called the mean recurrence time. We next determine this average.
</p>
<p>Let TR denote the recurrence time and note that it is an integer random variable
</p>
<p>that can take on values in the sample space {I , 2, ... }. For the two-state Markov
chain shown in Figure 22.3 we first assume that we are in state 1 at time n = no.
Then, the value of the recurrence time will be 1, or 2, or 3, etc. if X[no + 1] = 1,
or X[no + 1] = O,X[no + 2] = 1, or X[no + 1] = O,X[no + 2] = O,X[no + 3] = 1,
etc., respectively. The probabilities of these events are 1 - {3, {3a., and {3(1 - a)a,
</p>
<p>respectively as can be seen by referring to Figure 22.3. In general, the PMF is given
</p>
<p>as
</p>
<p>P[T kl' " 11 . 1] {I - {3 k = 1R = initia y III state = {3a(l _ a)k-2 k 2:: 2
</p>
<p>which is a geometric-type PMF (see Chapter 5). To find the mean recurrence time
</p>
<p>we need only determine the expected value of TR. This is
</p>
<p>00
</p>
<p>E[TRlinitially in state 1] = (1 - (3) + L k [{3a(l - a)k-2]
k=2
</p>
<p>00
</p>
<p>(1 - (3) + a{3 L(l + 1)(1 - a)l-l (let l = k - 1)
1=1
</p>
<p>(1 - (3) + [a{3 f(1 -a)l-l + {3f l ?(1 - ... a.)I_l~]
1=1 1=1 geom(a) PMF
</p>
<p>1 1
(1 - (3) + a{31 _ (1 _ a) + {3-;; (from Section 6.4.3)
</p>
<p>a+{3</p>
<p/>
</div>
<div class="page"><p/>
<p>762
</p>
<p>so that we have finally
</p>
<p>CHAPTER 22. MARKOV CHAINS
</p>
<p>E[TRlinitially in state 1] = ~.
11"1
</p>
<p>It is seen that mean recurrence time is the reciprocal of the stationary state prob-
</p>
<p>ability. This is much the same result as for a geometric PMF and is interpreted as
</p>
<p>the number of failures (not returning to state 1) before a success (returning to state
</p>
<p>1). For our golfer , since she has a stationary probability of one-putting of 1/3, she
</p>
<p>must wait on the average 1/(1/3)=3 holes between one-putts. This agrees with our
</p>
<p>simulation results shown in Figure 22.1.
</p>
<p>22.7 K-State Markov Chains
</p>
<p>Markov chains with more than two states are quite common and useful in practice
</p>
<p>but their analysis can be difficult . Most of the previous properties of a Markov
</p>
<p>chain apply to any finite number K of states. Computation of the n-step transition
</p>
<p>probability matrix is of course more difficult and requires computer evaluation. Most
</p>
<p>importantly, however, is that steady-state is still attained if P &gt; O. The solution
for the stationary probabilities is given next. It is derived in Appendix 22A.
</p>
<p>The stationary probability vector for a K-state Markov chain is 7rT = [11"011"1 . . .
</p>
<p>1I"K-1]. Its solution is given as
</p>
<p>(22.22)
</p>
<p>currently cloudy (state 1)
</p>
<p>currently raining (state 0)
</p>
<p>currently sunny (state 2) :
</p>
<p>where I is the K x K identity matrix and 1 = [11 .. . IV, which is a K x 1 vector
of ones. We next give an example of a 3-state Markov chain.
</p>
<p>Example 22.8 - Weather modeling
</p>
<p>Assume that the weather for each day can be classified as being either rainy (state
</p>
<p>0) , cloudy (state 1), or sunny (state 2). We wish to determine in the long run
</p>
<p>(steady-state) the percentage of sunny days. From the discussion in Section 22.6.1
</p>
<p>this is the state occupation time, and is equal to the stationary probability 11"2. To
</p>
<p>do so we assume the conditional probabilities
</p>
<p>431
Poo = 8'POI = 8'P02 = 8
</p>
<p>323
P lO = 8'Pn = 8'P12 = 8
</p>
<p>134
P20 = 8 ,P21 = 8 ,P22 = 8'
</p>
<p>This says that if it is currently raining, then it is most probable that the next day
</p>
<p>will also have rain (4/8). The next most probable weather condition will be cloudy
</p>
<p>for the next day (3/8), and the least probable weather condition is sunny for the</p>
<p/>
</div>
<div class="page"><p/>
<p>22.7. K -STATE MARKOV CHAINS 763
</p>
<p>next day (1/8) . See if you can rationalize the other entries in P . T he complete state
</p>
<p>transition probability matrix is
</p>
<p>p ~ [!!n
and the state probability diagram is shown in Figure 22.8. We can use this to
</p>
<p>1
'8
</p>
<p>4
'8
</p>
<p>4
'8
</p>
<p>0: rainy
</p>
<p>1 : cloudy
2: sunny
</p>
<p>1
'8
</p>
<p>Figure 22.8: Three-state probability diagram for weather example.
</p>
<p>determine the probability of the weather conditions on any day if we know the
</p>
<p>weather on day n = O. For example, to find the probability of the weather on
</p>
<p>Saturday knowing that it is raining on Monday, we use
</p>
<p>pT [n] = pT [O]pn
</p>
<p>with n = 5 and pT [O] = [1 0 0]. Using a computer to evalute this we have that
</p>
<p>[
</p>
<p>0.3370 ]
</p>
<p>p[5] = 0.3333
</p>
<p>0.3296
</p>
<p>and it appears that the possible weather conditions are nearly equiprobable. To find
</p>
<p>the stationary probabilities for the weather conditions we must solve 1rT = 1rT p .
</p>
<p>Using the solution of (22.22) , we find that</p>
<p/>
</div>
<div class="page"><p/>
<p>764 CHAPTER 22. MARKOV CHAINS
</p>
<p>As n -+ 00, it is equiprobable that the weather will be rainy, cloudy, or sunny.
</p>
<p>Furthermore, because of ergodicity the fraction of days that it will be rainy, or be
</p>
<p>cloudy, or be sunny will all be 1/3.
</p>
<p>&lt;&gt;
The previous result that the stationary probabilities are equal is true in general for
</p>
<p>the type of transition probability matrix given. Note that P not only has all its rows
summing to one but also its column entries sum to one for all the columns. This is
</p>
<p>called a doubly stochastic matrix and always results in equal stationary probabilities
</p>
<p>(see Problem 22.27).
</p>
<p>22.8 Computer Simulation
</p>
<p>The computer simulation of a Markov chain is very simple. Consider the weather
</p>
<p>example of the previous section. We first need to generate a realization of a random
</p>
<p>variable taking on the values 0,1,2 with the PMF po[0],pI[0],P2[0]. This can be
done using the approach of Section 5.9. Once the realization has been obtained,
</p>
<p>say x[O] = i, we continue the same procedure but must choose the next PMF, which
is actually a conditional PMF. If x[O] = i = 1 for example, then we use the PMF
</p>
<p>p[Oll] = PlO,p[111] = Pn,p[211] = P12, which are just the entries in the second row
of P. We continue this procedure for all n 2:: 1. Some MATLAB code to generate a
realization for the weather example is given below.
</p>
<p>clear all
rand ( , state' ,0)
</p>
<p>N=1000; %set number of samples desired
pO=[1/3 1/3 1/3]'; %set initial probability vector
P=[4/8 3/8 1/8;3/8 2/8 3/8;1/8 3/8 4/8]; % set transition prob. matrix
xi=[O 1 2]'; %set values of PMF
XO=PMFdata(1,xi,pO); %generate X[O] (see Appendix 6B for PMFdata.m
</p>
<p>%function subprogram)
i=XO+1; %choose appropriate row for PMF
X(1,1)=PMFdata(1,xi,P(i,:)); %generate X[1]
i=X(1,1)+1; %choose appropriate row for PMF
for n=2:N %generate X[n]
</p>
<p>i=X(n-1,1)+1; %choose appropriate row for PMF
X(n,1)=PMFdata(1,xi,P(i,:));
</p>
<p>end
</p>
<p>The reader may wish to modify and run this program to gain some insight into the
</p>
<p>effect of the conditional probabilities on the predicted weather patterns.</p>
<p/>
</div>
<div class="page"><p/>
<p>22.9. STRANGE MARKOV CHAIN DYNAMICS
</p>
<p>22.9 Real-World Example - Strange Markov
</p>
<p>Chain Dynamics
</p>
<p>765
</p>
<p>It is probably fitting that as the last real-world example, we choose one that ques-
</p>
<p>tions what the real-world actually is. Is it a place of determinism, however complex,
</p>
<p>or one that is subject to the whims of chance events? Random, as defined by Web-
</p>
<p>ster's dictionary, means "lacking a definite plan, purpose, or pattern". Is this a valid
</p>
<p>definition? We do not plan to answer this question, but only to present some "food
</p>
<p>for thought". The seemingly random Markov chain provides an interesting example.
</p>
<p>Consider a square arrangement of 101 x 101 points and define a set of states
</p>
<p>as the locations of the integer points within this square. The points are therefore
</p>
<p>denoted by the integer coordinates (i,j), where i = 0,1, ... , 100;j = 0,1, ... ,100.
</p>
<p>The number of states is K = 1012 . Next define a Markov chain for this set of states
such that the nth outcome is a realization of the random point X[n] = [I[n] J[n]V,
where I[n] and J[n] are random variables taking on integer values in the interval
[0,100]. The initial point is chosen to be X[O] = [1080V and succeeding points
</p>
<p>evolve according to the random process:
</p>
<p>1. Choose at random one of the reference points (0,0), (100,0), (50, 100).
</p>
<p>2. Find the midpoint between the initial point and the chosen reference point and
</p>
<p>round it to the nearest integer coordinates (so that it becomes a state output).
</p>
<p>3. Replace the initial point with the one found in step 2.
</p>
<p>4. Go to step 1 and repeat the process, always using the previous point and one of
</p>
<p>the reference points chosen at random.
</p>
<p>This procedure is equivalent to the formula
</p>
<p>X[n] = [~(x[n -1] + R[nD]
round
</p>
<p>(22.23)
</p>
<p>where R[n] = h[n]r2[n]V is the reference point chosen at random and [']round
denotes rounding of both elements of the vector to the nearest integer. Note that
</p>
<p>this is a Markov chain. The points generated must all lie within the square at integer
</p>
<p>coordinates due to the averaging and rounding that is ongoing. Also, the current
</p>
<p>output only depends upon the previous output X[n - 1], i.e., justifying the claim
of a Markov chain. The process is "random" due to our choice of R[n] from the
sample space {(O, 0), (100,0) , (50, 100)} with equal probabilities.
</p>
<p>The behavior of this Markov chain is shown in Figure 22.9, where the successive
</p>
<p>output points have been plotted with the first few shown with their values of n.
</p>
<p>It appears that the chain attains a steady-state and its steady-state PMF is zero
</p>
<p>over many triangular regions. It is interesting to note that the pattern consists of
</p>
<p>3 triangles-one with vertices (0,0), (50,0), (25,50), and the others with vertices</p>
<p/>
</div>
<div class="page"><p/>
<p>766 CHAPTER 22. MARKOV CHAINS
</p>
<p>(50,0), (100,0), (75,50), and (25,50), (75,50), (50, 100). Within each of these trian-
</p>
<p>gles resides an exact replica of the whole pattern and within each replica resides
</p>
<p>another replica, etc.! Such a figure is called a fractal with this particular one termed
</p>
<p>a Sierpinski triangle. The MATLAB code used to produce this figure is given below.
</p>
<p>Figure 22.9: Steady-state Markov chain.
</p>
<p>%sierpinski.m
%
</p>
<p>clear all
rand ( , state' ,0)
</p>
<p>r(:,l)=[O 0]'; %set up reference points
r ( : ,2) =[100 0]';
</p>
<p>r(:,3)=[50 100]';
</p>
<p>xO=[10 80]'; %set initial state
plot(xO(1),xO(2),'.') %plot state outcome as point
axis([O 100 0 100])
</p>
<p>hold on
xn_l=xO;</p>
<p/>
</div>
<div class="page"><p/>
<p>REFERENCES
</p>
<p>for n=1:10000 %generate states
j=floor(3*rand(1,1)+1); %choose at random one of three
</p>
<p>%reference points
xn=round(O.5*(r(:,j)+xn_l)); %generate new state
plot(xn(1),xn(2),'.') %plot state outcome as point
xn_l=xn; %make current state the previous one for
</p>
<p>%next transition
end
grid
</p>
<p>hold off
</p>
<p>767
</p>
<p>The question arises as to whether the Markov chain is deterministic or random.
</p>
<p>We choose not to answer this question (because we don't know the answerl). Instead
</p>
<p>we refer the interested reader to the excellent book [Peitgen, Jurgens, and Saupe
</p>
<p>1992] and also the popular layman's account [Gleick 1987] for further details. As
</p>
<p>a more practical application, it is observed that seemingly complex figures can be
</p>
<p>generated using a simple algorithm. This leads to the idea of data compression in
</p>
<p>which the only information needed to store a complex figure is the details of the
</p>
<p>algorithm. A field of sunflowers is such an example for which the reader should
</p>
<p>consult [Barnsley 1988] on how this is done.
</p>
<p>References
</p>
<p>Barnsley, M. , Fractals Ev erywh ere, Academic Press, New York, 1988.
</p>
<p>Bharucha-Reid, A.T., Elements of the Theory of Markov Processes and Their Ap-
</p>
<p>plications, Dover, Mineola, NY, 1988.
</p>
<p>Cox, D.R. , H.D. Miller, The Theory of Stochastic Processes, Methuen and Co,
</p>
<p>LTD , London, 1965.
</p>
<p>Gallagher, R.G., Discrete Stochastic Processes, Kluwer Academic Press, Boston,
</p>
<p>1996.
</p>
<p>Gleick , J. , Chaos, Making a New Science, Penguin Books, New York, 1987.
</p>
<p>Parzen, E., Stochastic Processes, Holden-Day, San Francisco, 1962.
</p>
<p>Peitgen, H., H. Jurgens, D. Saupe, Chaos and Fractals, Springer-Verlag, New York,
</p>
<p>1992.
</p>
<p>Problems
</p>
<p>22.1 (w) A Markov chain has the states "A" and "B" or equivalently 0 and 1. If
the conditional probabilities are P[AIB] = 0.1 and P[BIA] = 0.4, draw the</p>
<p/>
</div>
<div class="page"><p/>
<p>768 CHAPTER 22. MARKOV CHAINS
</p>
<p>state probability diagram. Also, find the transition probability matrix.
</p>
<p>22.2 C...:...) (f) For the state probability diagram shown in Figure 22.2 find the prob-
ability of obtaining the outcomes X[n] = 0,1 ,0,1 ,1 for n = 0,1 ,2,3,4, respec-
tively.
</p>
<p>22.3 (f) For the state probability diagram shown in Figure 22.3 find the probabil-
</p>
<p>ities of the outcomes X[n] = 0,1 ,0,1 ,1 ,1 for n = 0,1 ,2,3,4,5, respectively
and also for X[n] = 1,1 ,0,1 ,1 ,1 for n = 0,1 ,2,3,4,5, respectively. Compare
the two and explain the difference.
</p>
<p>22.4 (w) In some communication systems it is important to determine the percent-
age of time a person is talking. From measurements it is found that if a person
</p>
<p>is talking in a given time interval, then he will be talking in the next time in-
</p>
<p>terval with a probability of 0.75. If he is not talking in a time interval, then
</p>
<p>he will be talking in the next time interval with a probability of 0.5. Draw the
</p>
<p>state probability diagram using the states "talking" and "not talking" .
</p>
<p>22.5 C:.:....) (t) In this problem we give an example of a random process that does
not have the Markov property. The random process is defined as an exclusive
</p>
<p>OR logical function. This is Y[n] = X[n] ED X[n - 1] for n ~ 0, where X[n]
for n ~ 0 takes on values 0 and 1 with probabilities 1 - p and p, respectively.
</p>
<p>The X[n],s are lID. Also, for n = 0 we define Y[O] = X[O]. The definition
of this operation is that Y[n] = 0 only if X[n] and X [n - 1] are the same
(both equal to 0 or both equal to 1), and otherwise Y[n] = 1. Determine
P[Y[2] = 1IY[1] = 1, Y[O] = 0] and P[Y[2] = 1IY[1] = 1] to show that they
are not equal in general.
</p>
<p>22.6 (f) For the transition probability matrix given below draw the corresponding
state probability diagram.
</p>
<p>22.7 (w) A fair die is tossed many times in succession. The tosses are independent
of each other. Let X[n] denote the maximum of the first n + 1 tosses. De-
termine the transition probability matrix. Hint: The maximum value cannot
</p>
<p>decrease as n increases.
</p>
<p>22.8 (w) A particle moves along the circle shown in Figure 22.10 from one point
</p>
<p>to the other in a clockwise (CW) or counterclockwise (CCW) direction. At
</p>
<p>each step it can move either CW 1 unit or CCW 1 unit. The probabilities
</p>
<p>are P[CCW] = p and P[CW] = 1 - p and do not depend upon the current</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS
</p>
<p>1
</p>
<p>769
</p>
<p>2
</p>
<p>3
</p>
<p>o
</p>
<p>Figure 22.10: Movement of particle along a circle for Problem 22.8.
</p>
<p>location of the particle. For the states 0, 1,2,3 find the transition probability
</p>
<p>matrix.
</p>
<p>22.9 c.:..:..) (w,c) A digital communication system transmits a 0 or a 1. After 10
miles of cable a repeater decodes the bit and declares it either a 0 or a 1. The
</p>
<p>probability of a decoding error is 0.1 as shown schematically in Figure 22.11.
</p>
<p>It is then retransmitted to the next repeater located 10 miles away. If the
</p>
<p>repeaters are all located 10 miles apart and the communication system is 50
</p>
<p>miles in length, find the probability of an error if a 0 is initially transmitted.
</p>
<p>Hint: You will need a computer to work this problem.
</p>
<p>0.9
o ~--.--~
</p>
<p>1
0.9
</p>
<p>o
</p>
<p>1
</p>
<p>Figure 22.11: One section of a communication link.
</p>
<p>22.10 (w,c) If a = (3 = 1/4 for the state probability diagram shown in Figure 22.3,
determine n so that the Markov chain is in steady-state. Hint: You will need
</p>
<p>a computer to work this problem.
</p>
<p>22.11 (;..:..) (w) There are two urns filled with red and black balls. Urn 1 has 60%
red balls and 40% black balls while urn 2 has 20% red balls and 80% black
</p>
<p>balls. A ball is drawn from urn 1, its color noted, and then replaced. If it is
</p>
<p>red , the next ball is also drawn from urn 1, its color noted and then replaced.
</p>
<p>If the ball is black, then the next ball is drawn from urn 2, its color noted
</p>
<p>and then replaced. This procedure is continued indefinitely. Each time a ball
</p>
<p>is drawn the next ball is drawn from urn 1 if the ball is red and from urn 2
</p>
<p>if it is black. After many trials of this experiment what is the probability of</p>
<p/>
</div>
<div class="page"><p/>
<p>770 CHAPTER 22. MARKOV CHAINS
</p>
<p>drawing a red ball? Hint: Define the states 1 and 2 as urns 1 and 2 chosen.
</p>
<p>Also, note that P[red drawn] = P[red drawnlurn 1 chosen]P[urn 1 chosen] +
P[red drawnlurn 2 chosen]P[urn 2 chosen].
</p>
<p>22.12 C.':"') (w) A contestant answers questions posed to him from a game show
</p>
<p>host. If his answer is correct, the game show host gives him a harder question
</p>
<p>for which his probability of answering correctly is 0.01. If however, his answer
</p>
<p>is incorrect, the contestant is given an easy question for which his probability
</p>
<p>of answering correctly is 0.99. After answering many questions, what is the
</p>
<p>probability of answering a question correctly?
</p>
<p>22.13 (f) For the transition probability matrix
</p>
<p>p ~ [~ ; n
will P" converge as n -+ oo? You should be able to answer this question
without the use of a computer. Hint: Determine p2.
</p>
<p>22.14 C:.:J (w,c) For the transition probability matrix
</p>
<p>1 1 0 02 2
1 3 0 0
</p>
<p>p= 4: 4:
1 1 1 1
4: 4: 4: 4:
1 1 1 1
4: 4: 4: 4:
</p>
<p>does the Markov chain attain steady-state? If it does, what are the steady-
</p>
<p>state probabilities? Hint: You will'need a computer to evaluate the answer.
</p>
<p>22.15 (w,c) There are three lightbulbs that are always on in a room. At the begin-
ning of each day the custodian checks to see if at least one lightbulb is working.
</p>
<p>If all three lightbulbs have failed , then he will replace them all. During the day
</p>
<p>each lightbulb will fail with a probability of 1/2 and the failure is independent
</p>
<p>of the other lightbulbs failing. Letting the state be the number of working
</p>
<p>lightbulbs draw the state probability diagram and determine the transition
</p>
<p>probability matrix. Show that eventually all three bulbs must fail and the
</p>
<p>custodian will then have to replace them. Hint: You will need a computer to
</p>
<p>work this problem.
</p>
<p>22.16 (f) Find the stationary probabilities for the transition probability matrix
</p>
<p>p=[: n</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 771
</p>
<p>22.17 (t) In this problem we discuss the proof of the property that if P &gt; 0, the
rows of P" will all converge to the same values and that these values are the
</p>
<p>stationary probabilities. We consider the case of K = 3 for simplicity and
</p>
<p>assume distinct eigenvalues. Then, it is known from the Perron-Frobenius
</p>
<p>theorem that we will have the eigenvalues Al = 1, IA21 &lt; 1, and IA31 &lt; 1.
From (22.12) we have that P" = V Any-I which for K = 3 is
</p>
<p>[ 1 ,00n2 ,OO~] [W::~I ]pn = [VI V2 V3] ~ A A
--...--
</p>
<p>w
</p>
<p>where W = V-I and W[ is the ith row of W. Next argue that as n -t 00,
P" -t VI wf. Use the relation POOl = 1 (why?) to show that VI = el, where
c is a constant. Next use 7t"Tpoo = 7t"T (why?) to show that WI = dst , where
d is a constant. Finally, use the fact that w[VI = 1 since WY = I to show
that cd = 1 and therefore, poo = 17t"T. The latter is the desired result which
</p>
<p>can be verified by direct multiplication of 1 by 7t"T.
</p>
<p>22 .18 (f,c) For the transition probability matrix
</p>
<p>[
</p>
<p>0.1 0.4 0.5]
P = 0.2 0.5 0.3
</p>
<p>0.3 0.3 0.4
</p>
<p>find plOD using a computer evaluation. Does the form of plOD agree with the
</p>
<p>theory?
</p>
<p>22.19 t:...:.,,) (f,c) Using the explicit solution for the stationary probability vector
given by (22.22), determine its value for the transition probability matrix given
</p>
<p>in Problem 22.18. Hint: You will need a computer to evaluate the solution.
</p>
<p>22.20 (w) The result of multiplying two identical matrices together produces the
same matrix as shown below.
</p>
<p>[
</p>
<p>0.2 0.1 0.7] [0.2 0.1 0.7] [0.2 0.1 0.7]
0.2 0.1 0.7 0.2 0.1 0.7 = 0.2 0.1 0.7 .
</p>
<p>0.2 0.1 0.7 0.2 0.1 0.7 0.2 0.1 0.7
</p>
<p>Explain what this means for Markov chains.
</p>
<p>22.21 (f) For the transition probability matrix
</p>
<p>p = [0.99 0.01]
0.01 0.99</p>
<p/>
</div>
<div class="page"><p/>
<p>772 CHAPTER 22. MARKOV CHAINS
</p>
<p>solve for the stationary probabilities. Compare your probabilities to those ob-
</p>
<p>tained if a fair headed coin is tossed repeatedly and the tosses are independent.
</p>
<p>Do you expect the realization for this Markov chain to be similar to that of
</p>
<p>the fair coin tossing?
</p>
<p>22.22 (c) Simulate on the computer the Markov chain described in Problem 22.21.
</p>
<p>Use pT[O] = [1/2 1/2] for the initial probability vector. Generate a realization
for n = 0, 1, ... ,99 and plot the results. What do you notice about the real-
ization? Next generate a realization for n = 0,1, .. . ,9999 and estimate the
</p>
<p>stationary probability of observing 1 by taking the sample mean of the real-
</p>
<p>ization. Do you obtain the theoretical result found in Problem 22.21 (recall
</p>
<p>that this type of Markov chain is ergodic and so a temporal average is equal
</p>
<p>to an ensemble average).
</p>
<p>22.23 (w) A person is late for work on his first day with a probability of 0.1. On
succeeding days he is late for work with a probability of 0.2 if he was late the
</p>
<p>previous day and with a probability of 0.4 if he was on time the previous day.
</p>
<p>In the long run what percentage of time is he late to work?
</p>
<p>22.24 (...:....:..-) (f,c) Assume for the weather example of Example 22.8 that the transi-
tion probability matrix is
</p>
<p>What is the steady-state probability of rain? Compare your answer to that
</p>
<p>obtained in Example 22.8 and explain the difference. Hint: You will need a
</p>
<p>computer to find the solution.
</p>
<p>22.25 (w,c) Three machines operate together on a manufacturing floor, and each
</p>
<p>day there is a possibility that any of the machines may fail. The probability of
</p>
<p>their failure depends upon how many other machines are still in operation. The
</p>
<p>number of machines in operation at the beginning of each day is represented by
</p>
<p>the state values of 0, 1,2,3 and the corresponding state transition probability
</p>
<p>matrix is
</p>
<p>[
</p>
<p>1 0 0 0]
p = 0.5 0.5 0 0
</p>
<p>0.1 0.3 0.6 0
</p>
<p>0.4 0.3 0.2 0.1
</p>
<p>First explain why P has zero entries. Next determine how many days will pass
</p>
<p>before the probability of all 3 machines failing is greater than 0.8. Assume
</p>
<p>that intially all 3 machines are working. Hint: You will need a computer to
</p>
<p>find the solution.</p>
<p/>
</div>
<div class="page"><p/>
<p>PROBLEMS 773
</p>
<p>22.26 L...:..) (w,c) A pond holds 4 fish. Each day a fisherman goes fishing and his
probability of catching k = 0,1,2,3,4 fish that day follows a binomial PDF
</p>
<p>with p = 1/2. How many days should he plan on fishing so that the probability
</p>
<p>of his catching all 4 fish exceeds 0.9? Note that initially, i.e., at n = 0, all 4
</p>
<p>fish are present. Hint : You will need a computer to find the solution.
</p>
<p>22.27 (t) In this problem we prove that a doubly stochastic transition probability
matrix with P &gt; 0 produces equal stationary probabilities. First recall that
since the columns of P sum to one, we have that pTl = 1 and therefore argue
</p>
<p>that pooT 1 = 1. Next use the results of Problem 22.17 that poo = I1t"T to
show that 1t" = 1/K .
</p>
<p>22.28 C:..:,,) (c) Use a computer simulation to generate a realization of the golf ex-
ample for a large number of holes (very much greater than 18). Estimate the
</p>
<p>percentage of one-putts from your realization and compare it to the theoretical
</p>
<p>results.
</p>
<p>22.29 (c) Repeat Problem 22.28 but now estimate the average time between one-
</p>
<p>putts. Compare your results to the theoretical value.
</p>
<p>22.30 (c) Run the program sierpinski.m given in Section 22.9 but use instead the
</p>
<p>initial position X[O] = [5030V. Do you obtain similar results to those shown
in Figure 22.9? What is the difference, if any?</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 22A
</p>
<p>Solving for the Stationary PMF
</p>
<p>We derive the formula of (22.22). The set of equations to be solved (after transpo-
</p>
<p>sition) is pT7r = rr or equivalently
</p>
<p>(I - pT)7r = O. (22A.l)
</p>
<p>Since we have assumed a unique solution, it is clear that the matrix 1 - pT cannot
</p>
<p>be invertible or else we would have 7r = O. This is to say that the linear equations
</p>
<p>are not all independent. To make them independent we must add the constraint
</p>
<p>equation 2 : ~ ( / 1ri = 1 or in vector form this is IT7r = 1. Equivalently, the constraint
equation is lITtt = 1. Adding this to (22A.l) produces
</p>
<p>or
</p>
<p>(I - pT + 11T )7r = 1.
</p>
<p>It can be shown that the matrix 1 - pT + 11T is now invertible and so the solution
is</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix A
</p>
<p>Glossary of Symbols and
</p>
<p>Abbrevations
</p>
<p>Symbols
</p>
<p>Boldface characters denote vectors or matrices. All others are scalars. All vec-
</p>
<p>tors are column vectors. Random variables are denoted by capital letters such as
</p>
<p>U, V, w,X ,Y, Z and random vectors by U , V , W , X, Y , Z and their values by corre-
sponding lowercase letters.
</p>
<p>*
*
</p>
<p>[x]
x +
</p>
<p>x
</p>
<p>AxB
</p>
<p>[A]ij
</p>
<p>A(z)
[b]i
Ber(p)
</p>
<p>bin(M,p)
</p>
<p>xIv
(~)
c
</p>
<p>cov(X,Y)
</p>
<p>C
</p>
<p>angle of
</p>
<p>complex conjugate
</p>
<p>convolut ion operator, either convolution sum or integral
</p>
<p>denotes est imator
</p>
<p>denotes is distributed according to
</p>
<p>denotes the largest integer ::; x
</p>
<p>denotes a number slightly larger than x
</p>
<p>denotes a number slightly smaller than x
</p>
<p>cartesian product of sets A and B
</p>
<p>(i ,j)th element of A
</p>
<p>z-transform of a[n] sequence
ith element of b
</p>
<p>Bernoulli random variable
</p>
<p>binomial random variable
</p>
<p>chi-squared distribution with N degrees of freedom
</p>
<p>number of combinations of N things taken k at a time
</p>
<p>complement of set
</p>
<p>covariance of X and Y
</p>
<p>covariance matrix</p>
<p/>
</div>
<div class="page"><p/>
<p>778 APPENDIX A. GLOSSARY OF SYMBOLS AND ABBREVATIONS
</p>
<p>ex
CX,y
</p>
<p>cx [nl , n2]
</p>
<p>CX(tl, t2)
</p>
<p>o(t)
</p>
<p>o[n]
s.,
b.1
b.t
</p>
<p>b.x
</p>
<p>b.t
det(A)
</p>
<p>diag(al1, ... ,aNN)
</p>
<p>e i
</p>
<p>'fJ
E[&middot;]
E[Xn ]
E[(X - E[x])n]
</p>
<p>Ex[&middot;]
</p>
<p>Ex,Y[']
</p>
<p>Ex[&middot;]
</p>
<p>Ey1x[YIX]
Ey1x[Yl xi]
</p>
<p>Ey1x[Ylx]
E[X]
</p>
<p>E
</p>
<p>exp('x')
</p>
<p>1
F
</p>
<p>Fx(x)
</p>
<p>FX1(x)
</p>
<p>Fx,y(x,y)
</p>
<p>FX1 ,...,XN(Xl,'" , X N )
</p>
<p>FYlx(ylx)
</p>
<p>covariance matrix of X
</p>
<p>covariance matrix of X and Y
</p>
<p>covariance sequence of discrete-time random process X[n]
</p>
<p>covariance function of continuous-time random process X (t)
</p>
<p>Dirac delta function or impulse function
</p>
<p>discrete-time unit impulse sequence
</p>
<p>Kronecker delta
</p>
<p>small interval in frequency 1
small interval in t
small interval in x
</p>
<p>time interval between samples
</p>
<p>determinant of matrix A
</p>
<p>diagonal matrix with elements aii on main diagonal
</p>
<p>natural unit vector in ith direction
</p>
<p>signal-to-noise ratio
</p>
<p>expected value
</p>
<p>nth moment
</p>
<p>nth central moment
</p>
<p>expected value with respect to PMF or PDF of X
</p>
<p>expected value with respect to joint PMF or
</p>
<p>joint PDF of (X, Y)
</p>
<p>expected value with respect to
</p>
<p>N-dimensional joint PMF or PDF
</p>
<p>shortened notation for EX1,X2,...,XN [.]
conditional expected value considered as random variable
</p>
<p>expected value of PMF PYlx[Yjlxi]
</p>
<p>expected value of PDF PYIX (ylx)
</p>
<p>expected value of random vector X
</p>
<p>element of set
</p>
<p>exponential random variable
</p>
<p>discrete-time frequency
</p>
<p>continuous-time frequency
</p>
<p>cumulative distribution function of X
</p>
<p>inverse cumulative distribution function of X
</p>
<p>cumulative distribution function of X and Y
</p>
<p>cumulative distribution function of Xl, ... , X N
</p>
<p>cumulative distribution function of Y conditioned
</p>
<p>onX=x
</p>
<p>Fourier transform
</p>
<p>inverse Fourier transform
</p>
<p>general notation for function of real variable
</p>
<p>general notation for inverse function of g(.)</p>
<p/>
</div>
<div class="page"><p/>
<p>r(X)
r(a, A)
</p>
<p>rX,y(J)
</p>
<p>geom(p)
</p>
<p>h[n]
h(t)
</p>
<p>H(J)
</p>
<p>H(F)
</p>
<p>1-l(z)
</p>
<p>IA(x)
I
</p>
<p>n
J
8(w,z)
</p>
<p>8(x,y)
8(Xl ,...,XN )
</p>
<p>8(Yl ,&middot;..,YN )
</p>
<p>A
</p>
<p>mse
</p>
<p>p
</p>
<p>px[n]
</p>
<p>px(t)
</p>
<p>J,I,
</p>
<p>i..:.. ,kN )
n
</p>
<p>N!
</p>
<p>(N)r
</p>
<p>NA
N(p, (J2)
</p>
<p>N(J,I" C)
</p>
<p>Ilxll
(/)
</p>
<p>opt
</p>
<p>1
</p>
<p>Pois(A)
</p>
<p>PX[Xi]
</p>
<p>px[k]
</p>
<p>PX,Y[Xi, Yj]
</p>
<p>PX1 " ",XN[XI, ... , XN]
</p>
<p>px[x]
</p>
<p>PX1 ,,,,,XN[kl, ... , kN]
</p>
<p>779
</p>
<p>Gamma function
</p>
<p>Gamma random variable
</p>
<p>coherence function for discrete-time random processes
</p>
<p>X[n] and Y[n]
geometric random variable
</p>
<p>impulse response of LSI system
</p>
<p>impulse response of LTI system
</p>
<p>frequency response of LSI system
</p>
<p>frequency response of LTI system
</p>
<p>system function of LSI system
</p>
<p>indicator function for the set A
</p>
<p>identity matrix
</p>
<p>intersection of sets
</p>
<p>A
Jacobian matrix of transformation of w = g(x, y), z = h(x, y)
</p>
<p>Jacobian matrix of transformation from y to x
</p>
<p>diagonal matrix with eigenvalues on main diagonal
</p>
<p>mean square error
</p>
<p>mean
</p>
<p>mean sequence of discrete-time random process X[n]
mean function of continuous-time random process X (t)
mean vector
</p>
<p>multinomial coefficient
</p>
<p>discrete-time index
</p>
<p>N factorial
</p>
<p>equal to N(N - 1)&middot;&middot;&middot; (N - r + 1)
number of elements in set A
</p>
<p>normal or Gaussian random variable with mean p and variance (J2
</p>
<p>multivariate normal or Gaussian random vector with mean J,I,
</p>
<p>and covariance C
</p>
<p>Euclidean norm or length of vector x
</p>
<p>null or empty set
</p>
<p>optimal value
</p>
<p>vector of all ones
</p>
<p>Poisson random variable
</p>
<p>PMF of X
</p>
<p>PMF of integer-valued random variable X (or px[i], px[j])
joint PMF of X and Y
</p>
<p>joint PMF of Xl,&middot; .. , XN
</p>
<p>shortened notation for PX1,,,,,XN[XI, ... ,XN]
</p>
<p>joint PMF of integer-valued random variables Xl, ... , XN</p>
<p/>
</div>
<div class="page"><p/>
<p>780 APPENDIX A. GLOSSARY OF SYMBOLS AND ABBREVATIONS
</p>
<p>PYIX[Yj!Xi]
</p>
<p>PXNlxl, ...,X N - l [xNI
Xl,&middot;&middot;&middot;, XN-l]
</p>
<p>PX,y[i ,j]
</p>
<p>PYIXUli]
</p>
<p>PX(X)
</p>
<p>PX,y(X, y)
</p>
<p>PXl, oo .,XN(Xl, ... ,XN)
PX(X)
</p>
<p>PYlx(ylx)
</p>
<p>prE]
r;
Px(f)
</p>
<p>PX(z)
</p>
<p>Px(F)
</p>
<p>Px,y(F)
</p>
<p>&lt;PX(W)
</p>
<p>&lt;PX,Y(WX,wy)
</p>
<p>&lt;PXl ,oo. ,XN(Wl,'" ,WN)
&lt;I&gt; (x)
</p>
<p>Q(x)
Q-l(u)
</p>
<p>PX,Y
rx[k]
</p>
<p>conditional PMF of Y given X = Xi
</p>
<p>conditional PMF of XN given Xl, . . &middot;, XN-l
joint PMF of integer-valued random variables X and Y
</p>
<p>conditional PMF of integer-valued random variable Y
</p>
<p>given X = i
</p>
<p>PDF of X
</p>
<p>joint PDF of X and Y
</p>
<p>joint PDF of Xl , ... ,XN
</p>
<p>shortened notation for PXl,oo.,XN (Xl , .. . ,XN)
conditional PDF of Y given X = X
</p>
<p>probability of the event E
</p>
<p>probability of error
</p>
<p>power spectral density of discrete-time
</p>
<p>random process X[n]
z-transform of autocorrelation sequence rx[k]
power spectral density of continuous-time
</p>
<p>random process X(t)
</p>
<p>cross-power spectral density of discrete-time
</p>
<p>random processes X[n] and Y[n]
cross-power spectral density of continuous-time
</p>
<p>random processes X (t) and Y (t)
</p>
<p>characteristic function of X
</p>
<p>joint characteristic function of X and Y
</p>
<p>joint characteristic function of Xl, .. ' ,XN
</p>
<p>cumulative distribution function of N(o ,1) random variable
probability that a N(o,1) random variable exceeds X
value of N(o,1) random variable that is exceeded
</p>
<p>with probability of u
</p>
<p>correlation coefficient of X and Y
</p>
<p>autocorrelation sequence of discrete-time
</p>
<p>random process X[n]
autocorrelation function of continuous-time
</p>
<p>random process X(t)
</p>
<p>cross-correlation sequence of discrete-time
</p>
<p>random processes X[n] and Y[n]
cross-correlation function of continuous-time
</p>
<p>random processes X(t) and Y(t)
</p>
<p>denotes real line
</p>
<p>denotes N-dimensional Euclidean space
</p>
<p>autocorrelation matrix
</p>
<p>sample space</p>
<p/>
</div>
<div class="page"><p/>
<p>Sx
</p>
<p>SX,Y
</p>
<p>SX1,X2" " ,XN
</p>
<p>Si
</p>
<p>S
</p>
<p>0'2
</p>
<p>O ' ~
O ' ~ [ n ]
</p>
<p>O ' ~ ( t )
</p>
<p>s[n]
s
</p>
<p>s(t)
</p>
<p>t
</p>
<p>T
</p>
<p>U(a ,b)
</p>
<p>U
</p>
<p>urn]
u(x)
U(z)
</p>
<p>V
</p>
<p>var(X)
</p>
<p>var(YIXi)
</p>
<p>X s
</p>
<p>X[n]
x[n]
X(t)
</p>
<p>X(t )
</p>
<p>X( z)
</p>
<p>X
x
X
</p>
<p>x
</p>
<p>YI(X = Xi)
z
Z-l
</p>
<p>o
</p>
<p>781
</p>
<p>sample space of random variable X
</p>
<p>sample space of random variables X and Y
</p>
<p>sample space of random variables Xl, X 2 , ..&bull; , XN
</p>
<p>element of discrete sample space
</p>
<p>element of continuous sample space
</p>
<p>variance
</p>
<p>variance of random variable X
</p>
<p>variance sequence of discrete-time random process X[n]
variance function of continuous-time random process X(t)
</p>
<p>discrete-time signal
</p>
<p>vector of signal samples
</p>
<p>continuous-time signal
</p>
<p>continuous time
</p>
<p>transpose of matrix
</p>
<p>uniform random variable over the interval (a, b)
</p>
<p>union of sets
</p>
<p>discrete unit step function
</p>
<p>unit step function
</p>
<p>z-transform of urn] sequence
modal matrix
</p>
<p>variance of X
</p>
<p>variance of conditional PMF or of PY\X [Yj IXi]
</p>
<p>value of discrete random variable
</p>
<p>value of continuous random variable
</p>
<p>standardized version of random variable X
</p>
<p>value for X;
</p>
<p>discrete-time random process
</p>
<p>realization of discrete-time random process
</p>
<p>continuous-time random process
</p>
<p>realization of continuous-time random process
</p>
<p>z-t ransform of x[n] sequence
sample mean random variable
</p>
<p>value of X
random vector (XI,X2 , &bull; &bull;&bull; ,XN)
</p>
<p>value (Xl, X2, . &middot; &middot; , XN) of random vector X
</p>
<p>random variable Y conditioned on X = Xi
z-transform
</p>
<p>inverse z-transform
</p>
<p>vector or matrix of all zeros</p>
<p/>
</div>
<div class="page"><p/>
<p>782 APPENDIX A. GLOSSARY OF SYMBOLS AND ABBREVATIONS
</p>
<p>Abbreviations
</p>
<p>ACF
ACS
AR
</p>
<p>AR(p)
</p>
<p>ARMA
</p>
<p>CCF
CCS
CDF
CPSD
CTCV
CTDV
D/A
dB
</p>
<p>DC
DFT
DTCV
DTDV
FFT
FIR
</p>
<p>GHz
</p>
<p>Hz
</p>
<p>IID
</p>
<p>IIR
</p>
<p>KHz
</p>
<p>LSI
</p>
<p>LTI
</p>
<p>MA
</p>
<p>MHz
</p>
<p>MSE
</p>
<p>PDF
PMF
</p>
<p>PSD
SNR
</p>
<p>WGN
</p>
<p>WSS
</p>
<p>autocorrelation function
</p>
<p>autocorrelation sequence
</p>
<p>autoregressive
</p>
<p>autoregressive process of order p
</p>
<p>autoregressive moving average
</p>
<p>cross-correlation function
</p>
<p>cross-correlation sequence
</p>
<p>cumulative distribution function
</p>
<p>cross-power spectral density
</p>
<p>continuous-time/ continuous-valued
</p>
<p>continuous-time/ discrete-valued
</p>
<p>digital-to-analog
</p>
<p>decibel
</p>
<p>constant level (direct current)
</p>
<p>discrete Fourier transform
</p>
<p>discrete-time/ continuous-valued
</p>
<p>discrete-time/ discrete-valued
</p>
<p>fast Fourier transform
</p>
<p>finite impulse response
</p>
<p>giga-hertz
</p>
<p>hertz
</p>
<p>independent and identically distributed
</p>
<p>infinite impulse response
</p>
<p>kilo-hertz
</p>
<p>linear shift invariant
</p>
<p>linear time invariant
</p>
<p>moving average
</p>
<p>mega-hertz
</p>
<p>mean square error
</p>
<p>probability density function
</p>
<p>probability mass function
</p>
<p>power spectral density
</p>
<p>signal-to-noise ratio
</p>
<p>white Gaussian noise
</p>
<p>wide sense stationary</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix B
</p>
<p>Assorted Math Facts and
</p>
<p>Formulas
</p>
<p>An extensive summary of math facts and formulas can be found in [Gradshteyn and
</p>
<p>Ryzhik 1994].
</p>
<p>B.l Proof by Induction
</p>
<p>To prove that a statement is true, for example,
</p>
<p>N N
l:i = 2(N + 1)
i=1
</p>
<p>by mathematical induction we proceed as follows:
</p>
<p>1. Prove the statement is true for N = 1.
</p>
<p>(B.1)
</p>
<p>2. Assume the statement is true N = n and prove that it therefore must be true
</p>
<p>for N = n + 1.
</p>
<p>Obviously, (B.1) is true for N = 1 since 2:;=1 i = 1 and (N/2)(N+1) = (1/2)(2) = 1.
Now assume it is true for N = n. Then for N = n + 1 we have
</p>
<p>n+1 n
</p>
<p>l:i l:i+(n+1)
i=1 i=1
</p>
<p>n
"2(n+ 1) + (n + 1) (since it is true for N = n)
</p>
<p>n+1
-2-(n+2)
</p>
<p>(n; 1) [(n + 1) + 1]</p>
<p/>
</div>
<div class="page"><p/>
<p>784 APPENDIX B. ASSORTED MATH FACTS AND FORMULAS
</p>
<p>which proves that it is also true for N = n + 1. By induction, since it is true for
N = n = 1 from step 1, it must also be true for N = (n + 1) = 2 from step 2. And
since it is true for N = n = 2, it must also be true for N = n + 1 = 3, etc.
</p>
<p>B.2 Trigonometry
</p>
<p>Some useful trigonometric identities are:
</p>
<p>Fundamental
</p>
<p>Sum of angles
</p>
<p>(B.2)
</p>
<p>sin(a + (3)
cos(a + (3)
</p>
<p>Double angle
</p>
<p>sin(2a)
</p>
<p>cos(2a)
</p>
<p>Squared sine and cosine
</p>
<p>Euler identities For j = yCT
</p>
<p>sin a cos (3 + cos a sin (3
cos a cos (3 - sin a sin (3
</p>
<p>2sinacosa
</p>
<p>cos2 a - sin2 a = 2 cos2 a - 1
</p>
<p>1 1
- - - cos(2a)
2 2
1 1
2+ 2cos(2a)
</p>
<p>(B.3)
</p>
<p>(B.4)
</p>
<p>(B.5)
</p>
<p>(B.6)
</p>
<p>(B.7)
</p>
<p>(B.8)
</p>
<p>exp(ja) = cos a + j sin a (B.9)
</p>
<p>B.3 Limits
</p>
<p>cos a
</p>
<p>sin o
</p>
<p>exp(ja) + exp(-ja)
2
</p>
<p>exp(ja) - exp(-ja)
=
</p>
<p>2j
</p>
<p>(B.lO)
</p>
<p>(B.ll)
</p>
<p>Alternative definition of exponential function
</p>
<p>lim (1 + ~)M = exp(x)
M-too M
</p>
<p>(B.12)</p>
<p/>
</div>
<div class="page"><p/>
<p>BA. SUMS
</p>
<p>Taylor series expansion about the point x = xo
</p>
<p>00 g(i) (xo) .
g(x) = L ., (x - XO)2
</p>
<p>z.
i= O
</p>
<p>785
</p>
<p>(B.13)
</p>
<p>where g(i)(xo) is the ith derivative of g(x) evaluated at x = Xo and g(O) (xo) =
g(xo). As an example, consider g(x) = exp(x) , which when expanded about
</p>
<p>x = Xo = 0 yields
00 .
</p>
<p>x 2
exp( x) = L 1
</p>
<p>z.
i= O
</p>
<p>B.4 Sums
</p>
<p>Integers
</p>
<p>Real geometric series
</p>
<p>i=O
</p>
<p>N(N -1)
</p>
<p>2
</p>
<p>N(N - 1)(2N - 1)
</p>
<p>6
</p>
<p>(B.14)
</p>
<p>(8.15)
</p>
<p>If Ixl &lt; 1, then
00 k
</p>
<p>L
&middot;&middot; Xx 2 _
</p>
<p>I-x
i = k
</p>
<p>(x is real) (B.16)
</p>
<p>(8.17)
</p>
<p>Complex geometric series
</p>
<p>(z is complex) (8.18)
</p>
<p>A special case is when z = exp(jO) . Then
</p>
<p>N-l
</p>
<p>L exp(jO)
i=O
</p>
<p>1 - exp(jNO)
</p>
<p>1 - exp(jO)
</p>
<p>[.(N -1) oJ sin (&yen;)exp J 2 . (8)
sm 2
</p>
<p>(B.19)</p>
<p/>
</div>
<div class="page"><p/>
<p>786 APPENDIX B. ASSORTED MATH FACTS AND FORMULAS
</p>
<p>If Izi = Ix+ j yl = Jx2+ y2 &lt; 1, then as N ---+ 00 (B.I8) becomes
</p>
<p>Double sums
</p>
<p>00 k
</p>
<p>L
&middot; ZZl _
</p>
<p>I- z
i= k
</p>
<p>(B.20)
</p>
<p>(B.2I)
</p>
<p>B.5 Calculus
</p>
<p>Convergence of sum to integral
</p>
<p>If g(x) is a continuous function over [a, b], then
</p>
<p>M b
</p>
<p>lim Lg(xd.6.x = r g(x)dx
~ x ~O J a
</p>
<p>i=O
</p>
<p>(B.22)
</p>
<p>where Xi = a + i.6.x and X M = b. Also, this shows how to approximate an
integral by a sum.
</p>
<p>Approximation of integral over small interval
</p>
<p>i
X O + ~ X / 2
</p>
<p>g(x)dx ~g( xo).6. x
xo - ~x /2
</p>
<p>Differentiation of composite function
</p>
<p>dg(h(x)) I = dg(u) I dh(x) I
dx x=xo du u=h(xo) dx x=xo
</p>
<p>Change of integration variable
</p>
<p>If u = h(x), then
</p>
<p>(chain rule)
</p>
<p>(B.23)
</p>
<p>(B.24)
</p>
<p>b h-1(b)
</p>
<p>r g(u)du = r g(h(x))h'(x)dx
l; Jh-1 (a)
</p>
<p>(B.25)
</p>
<p>where h'(x) is the derivative of h(x) and h-1( .) denotes the inverse function.
</p>
<p>This assumes that there is one solution to the equation u = h(x) over the
</p>
<p>interval a ~ u ~ b.
</p>
<p>Fundamental theorem of calculus
</p>
<p>d r
dx J-co g(t)dt = g(x) (B.26)</p>
<p/>
</div>
<div class="page"><p/>
<p>REFERENCES 787
</p>
<p>Integration of even and odd functions An even function is defined as having
</p>
<p>the property 9(- x) = 9(x), while an odd function has the property 9(- x) =
-g(x). As a result,
</p>
<p>2 ~M g(x)dx
</p>
<p>o
</p>
<p>for g(x) an even function
</p>
<p>for g(x) an odd function
</p>
<p>Integration by parts
</p>
<p>If U and V are both functions of x , then
</p>
<p>! UdV = UV - ! V dU (B.28)
Dirac delta "function" or impulse
</p>
<p>Denoted by 8(x) it is not really a function but a symbol that has the definition
</p>
<p>{
O x # 0
</p>
<p>8(x) = oo X = 0
</p>
<p>and
</p>
<p>l
b
</p>
<p>{ 1 0 E [a-,b+]
a 8(x)dx = 0 otherwise
</p>
<p>Some properties are for u(x) the unit step function
</p>
<p>Double integrals
</p>
<p>du(x)
dx
</p>
<p>= 8(x)
</p>
<p>u(x)
</p>
<p>l d l b g(x)h(y)dxdy = (l b 9(X)dX) (l d h(y)dY)
References
</p>
<p>(B.29)
</p>
<p>Gradshteyn, 1.S., 1.M. Ryzhik, Tables of Integrals, Series, and Products, Fifth Ed.,
</p>
<p>Academic Press, New York, 1994.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix C
</p>
<p>Linear and Matrix Algebra
</p>
<p>Important results from linear and matrix algebra theory are reviewed in this ap-
</p>
<p>pendix. It is assumed that the reader has had some exposure to matrices. For
</p>
<p>a more comprehensive treatment the books [Noble and Daniel 1977] and [Graybill
</p>
<p>1969] are recommended.
</p>
<p>C.l Definitions
</p>
<p>Consider an M x N matrix A with elements aij, i = 1,2, ... , M; j = 1,2, ... , N. A
shorthand notation for describing A is
</p>
<p>[A]&middot;&middot; -a"X) - X),
</p>
<p>Likewise a shorthand notation for describing an N x 1 vector b is
</p>
<p>An M x N matrix A may multiply an N x 1 vector b to yield a new M x 1 vector
</p>
<p>c whose ith element is
</p>
<p>N
</p>
<p>Ci = Laijbj
j=l
</p>
<p>i = 1,2, ... ,M.
</p>
<p>Similarly, an M x N matrix A can multiply an N x L matrix B to yield an M x L
</p>
<p>matrix C = AB whose (i,j) element is
</p>
<p>N
</p>
<p>Cij = L aikbkj
k=l
</p>
<p>i = 1,2, ... ,M;j = 1,2, ... ,L.
</p>
<p>Vectors and matrices that can be multiplied together are said to be conformable.</p>
<p/>
</div>
<div class="page"><p/>
<p>790 APPENDIX C. LINEAR AND MATRIX ALGEBRA
</p>
<p>[A T].. -a "X) - )X'
</p>
<p>The transpose of A, which is denoted by AT, is defined as the N x M matrix
</p>
<p>with elements aji or
</p>
<p>A square matrix is one for which M = N. A square matrix is symmetric if
</p>
<p>AT = A or aji = aij'
The inverse of a square N x N matrix is the square N x N matrix A -1 for which
</p>
<p>A-1A = AA-1 = I
</p>
<p>where I is the N x N identity matrix. If the inverse does not exist, then A is
</p>
<p>singular. Assuming the existence of the inverse of a matrix, the unique solution to
</p>
<p>a set of N simultaneous linear equations given in matrix form by Ax = b, where A
is N x N, x is N x 1, and b is N x 1, is x = A -lb.
</p>
<p>The determinant of a square N x N matrix is denoted by det(A). It is computed
</p>
<p>as
N
</p>
<p>det(A) = L aijCij
j=l
</p>
<p>where
</p>
<p>Cij = (_l)i+j D ij.
</p>
<p>Dij is the determinant of the submatrix of A obtained by deleting the ith row and
</p>
<p>jth column and is termed the minor of aij' Cij is the cofactor of aij' Note that any
</p>
<p>choice of i for i = 1,2, . .. , N will yield the same value for det(A). A square N x N
</p>
<p>matrix is nonsingular if and only if det(A) i= O.
A quadratic form Q, which is a scalar, is defined as
</p>
<p>N N
</p>
<p>Q = L L aijXiXj.
i=l j=l
</p>
<p>In defining the quadratic form it is assumed that aji = aij. This entails no loss in
generality since any quadratic function may be expressed in this manner. Q may
also be expressed as
</p>
<p>Q = xTAx
</p>
<p>where x = [Xl X2 ... XNV and A is a square N x N matrix with aji = aij or A is a
</p>
<p>symmetric matrix.
</p>
<p>A square N x N matrix A is positive semidefinite if A is symmetric and
</p>
<p>for all x. If the quadratic form is strictly positive for x i= 0, then A is positive
definite. When referring to a matrix as positive definite or positive semidefinite, it
</p>
<p>is always assumed that the matrix is symmetric.</p>
<p/>
</div>
<div class="page"><p/>
<p>0.2. SPECIAL MATRICES 791
</p>
<p>A partitioned M x N matrix A is one that is expressed in terms of its submatrices.
</p>
<p>An example is the 2 x 2 partitioning
</p>
<p>Each "element" A i j is a submatrix of A. The dimensions of the partitions are given
</p>
<p>as
</p>
<p>[
K x L K x (N - L) ]
</p>
<p>(M - K) xL (M - K) x (N - L) .
</p>
<p>C.2 Special Matrices
</p>
<p>A diagonal matrix is a square N x N matrix with aij = 0 for i =1= j or all elements
not on the principal diagonal (the diagonal containing the elements aii) are zero.
</p>
<p>The elements aij for which i =1= j are termed the off-diagonal elements. A diagonal
</p>
<p>matrix appears as
</p>
<p>A = [a~l a~ ~] .
o 0 aNN
</p>
<p>A diagonal matrix will sometimes be denoted by diag(all, a22, . . . , aNN)' The in-
</p>
<p>verse of a diagonal matrix is found by simply inverting each element on the principal
</p>
<p>diagonal, assuming that aii =1= 0 for i = 1,2, ... ,N (which is necessary for invertibil-
ity) .
</p>
<p>A square N x N matrix is orthogonal if
</p>
<p>For a matrix to be orthogonal the columns (and rows) must be orthonormal or if
</p>
<p>where a, denotes the ith column, the conditions
</p>
<p>T . _ {O for i =1= j
ai aJ - 1 f . .or Z = J
</p>
<p>must be satisfied. Other "matrices" that can be constructed from vector operations
</p>
<p>on the N x 1 vectors x and yare the inner product, which is defined as the scalar
</p>
<p>N
</p>
<p>xTy = 2: Xi Yi
i= l</p>
<p/>
</div>
<div class="page"><p/>
<p>792 APPENDIX C. LINEAR AND MATRIX ALGEBRA
</p>
<p>and the outer product, which is defined as the N x N matrix
</p>
<p>[
</p>
<p>XIYl
</p>
<p>X2Yl
</p>
<p>X ~ Y l XNY2
</p>
<p>XIYN ]
X2YN
</p>
<p>XN:YN .
</p>
<p>C.3 Matrix Manipulation and Formulas
</p>
<p>Some useful formulas for the algebraic manipulation of matrices are summarized in
</p>
<p>this section. For N x N matrices A and B the following relationships are useful.
</p>
<p>(AT )- 1
</p>
<p>(AB)-1 =
</p>
<p>det(AT )
</p>
<p>det(eA) =
</p>
<p>det(AB)
</p>
<p>det(A-1)
</p>
<p>(A-If
</p>
<p>B-1A-I
</p>
<p>det(A)
</p>
<p>eN det(A) (e a scalar)
</p>
<p>det(A) det(B)
</p>
<p>1
</p>
<p>det(A) .
</p>
<p>Also, for any conformable matrices (or vectors) we have
</p>
<p>It is frequently necessary to determine the inverse of a matrix analytically. To do so
</p>
<p>one can make use of the following formula. The inverse of a square N x N matrix is
</p>
<p>A-I = aT
det(A)
</p>
<p>where C is the square N x N matrix of cofactors of A. The cofactor matrix is
</p>
<p>defined by
</p>
<p>[C]ij = (-l)i+jDij
</p>
<p>where Dij is the minor of aij obtained by deleting the ith row and jth column of
</p>
<p>A .
</p>
<p>Partitioned matrices may be manipulated according to the usual rules of matrix
</p>
<p>algebra by considering each submatrix as an element . For multiplication of parti-
</p>
<p>tioned matrices the submatrices that are multiplied together must be conformable.
</p>
<p>As an illustration, for 2 x 2 partitioned matrices
</p>
<p>AB = [~~~ ~~~] [:~~ :~~]
= [A l1B l1 + A 12B21 A l1B 12 + A 12B22 ]
</p>
<p>A 21Bl1 + A 22B21 A 21B12 + A 22B22 &bull;</p>
<p/>
</div>
<div class="page"><p/>
<p>C.4. SOME PROPERTIES OF PD (PSD) MATRICES 793
</p>
<p>Other useful relationships for partitioned matrices for an M x N matrix A and N x 1
</p>
<p>vectors Xi are
</p>
<p>(C.l)
</p>
<p>which is a M x N matrix and
</p>
<p>[
</p>
<p>an
</p>
<p>[auxl a22X2 ... aNNxN] = [Xl X2 ... XN] ~
</p>
<p>which is an N x N matrix.
</p>
<p>C.4 Some Properties of Positive Definite
</p>
<p>(Semidefinite) Matrices
</p>
<p>o
</p>
<p>o JJ
(C.2)
</p>
<p>Some useful properties of positive definite (semidefinite) matrices are:
</p>
<p>1. A square N x N matrix A is positive definite if and only if the principal minors
</p>
<p>are all positive. (The ith principal minor is the determinant of the submatrix
</p>
<p>formed by deleting all rows and columns with an index greater than i.) If the
principal minors are only nonnegative, then A is positive semidefinite.
</p>
<p>2. If A is positive definite (positive semidefinite) , then
</p>
<p>a. A is invertible (singular).
</p>
<p>b. the diagonal elements are positive (nonnegative).
</p>
<p>c. the determinant of A, which is a principal minor, is positive (nonnegative).
</p>
<p>C.5 Eigendecomposition of Matrices
</p>
<p>An eigenvector of a square N x N matrix A is an N x 1 vector v satisfying
</p>
<p>Av = &gt;'v (C.3)
</p>
<p>for some scalar &gt;., which may be complex. &gt;. is the eigenvalue of A corresponding
to the eigenvector v. To determine the eigenvalues we must solve for the N &gt;.'s in
</p>
<p>det(A - &gt;'1) = 0, which is an Nth order polynomial in &gt;.. Once the eigenvalues are
found, the corr esponding eigenvectors are determined from the equation (A-&gt;'I)v =
</p>
<p>o. It is assumed that the eigenvector is normalized to have unit length or v T v = 1.
If A is symmetric, then one can always find N linearly independent eigenvectors,
</p>
<p>although they will not in general be unique. An example is the identity matrix for</p>
<p/>
</div>
<div class="page"><p/>
<p>794 APPENDIX C. LINEAR AND MATRIX ALGEBRA
</p>
<p>which any vector is an eigenvector with eigenvalue 1. If A is symmetric, then the
</p>
<p>eigenvectors corresponding to distinct eigenvalues ar e orthonormal or v[vj = 0 for
i =1= j and v[vj = 1 for i = i , and the eigenvalues are real. If, furthermore, the
matrix is positive definite (positive semidefinite) , then the eigenvalues are positive
</p>
<p>(nonnegative).
</p>
<p>The defining relation of (C.3) can also be written as (using (C.1) and (C.2))
</p>
<p>[AV1 AV2 ... AVN] = [A1V1 A2V2 ... ANVn ]
</p>
<p>or
</p>
<p>where
</p>
<p>AY=YA
</p>
<p>Y [V1 V2 v n ]
</p>
<p>A diag(A1' A2 , , An).
</p>
<p>(C.4)
</p>
<p>If A is symmetric so that the eigenvectors corresponding to distinct eigenvalues
</p>
<p>are orthonormal and the remaining eigenvectors are chosen to yield an orthonormal
</p>
<p>eigenvector set, then Y is an orthogonal matrix. As such, its inverse is vr, so that
(C.4) becomes
</p>
<p>A=YAyT
</p>
<p>Also, the inverse is easily determined as
</p>
<p>A -1 y T- 1 A -ly-1
</p>
<p>YA-1yT .
</p>
<p>References
</p>
<p>Graybill, F.A ., Introduction to Matrices with Applications in Statistics, Wadsworth,
</p>
<p>Belmont, CA, 1969.
</p>
<p>Noble, B., Daniel, J.W., Applied Linear Algebra, Prentice-Hall, Englewood Cliffs,
</p>
<p>NJ , 1977.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix D
</p>
<p>Summary of Signals, Linear
</p>
<p>Transforms, and Linear Systems
</p>
<p>In this appendix we summarize the important concepts and formulas for discrete-
</p>
<p>time signal and system analysis. This material is used in Chapters 18-20. Some
</p>
<p>examples are given so that the reader unfamiliar with this material should try to
</p>
<p>verify the example results. For a more comprehensive treatment the books [Jackson
</p>
<p>1991], [Oppenheim, Willsky, and Nawab 1997], [Poularikis and Seeley 1985] are
</p>
<p>recommended.
</p>
<p>D.1 Discrete-Time Signals
</p>
<p>A discrete-time signal is a sequence x[n] for n = ... , -1, 0,1 , . . .. It is defined only
for the integers. Some important signals are:
</p>
<p>a. Unit impulse - x[n] = 1 for n = 0 and x[n] = 0 for n i- o. It is also denoted by
8[n] .
</p>
<p>b. Unit step - x[n] = 1 for n 2: 0 and x[n] = 0 for n &lt; O. It is also denoted by urn].
</p>
<p>c. Real sinusoid - x [n] = A cos(21ffon + ()) for -00 &lt; n &lt; 00, where A is the
amplitude (must be nonnegative), fo is the frequency in cycles per sample and
</p>
<p>must be in the interval 0 &lt; fo &lt; 1/2, and () is the phase in radians.
</p>
<p>d. Complex sinusoid - x[n] = A exp(j21f fon + ()) for -00 &lt; n &lt; 00, where A is the
amplitude (must be nonnegative), fo is the frequency in cycles per sample and
</p>
<p>must be in the interval -1/2 &lt; fo &lt; 1/2, and () is the phase in radians.
</p>
<p>e. Exponential - x[n] = anu[n]</p>
<p/>
</div>
<div class="page"><p/>
<p>796 APPENDIX D. REVIEW OF SIGNALS AND SYSTEMS
</p>
<p>Note that any sequence can be written as a linear combination of unit impulses that
</p>
<p>are weighted by x[k] and shifted in time as o[n - k] to form
</p>
<p>00
</p>
<p>x[n] = L x[kJo[n - k].
k=-oo
</p>
<p>(D.1)
</p>
<p>For example, anu[n] = o[n] + ao[n - 1J + a2o[n - 2] + ....
Some special signals are defined next.
</p>
<p>a. A signal is causal if x[n] = &deg;for n &lt; 0, for example, x[n] = urn].
b. A signal is anticausal if x[n] =&deg;for n &gt; 0, for example, x[n] = u[-nJ .
c. A signal is even if x[-n] = x[n] or it is symmetric about n = 0, for example,
</p>
<p>x[n] = cos(27rfon).
</p>
<p>d. A signal is odd if x[-n] = -x[n] or it is antisymmetric about n = 0, for example,
x[n] = sin(27rfon).
</p>
<p>e. A signal is stable if 2:~=-00 Ix[n]1 &lt; 00 (also called absolutely summable) , for
example, x[n] = (1/2)n u[n].
</p>
<p>D.2 Linear Transforms
</p>
<p>D.2.1 Discrete-Time Fourier Transforms
</p>
<p>The discrete-time Fourier transform XU) of a discrete-time signal x[n] is defined
</p>
<p>as 00
</p>
<p>XU) = L x[n]exp(-j27rfn)
n=-oo
</p>
<p>- 1/2 '.5: f '.5: 1/2. (D.2)
</p>
<p>An example is x[n] = (1/2)n u[n] for which XU) = 1/(1 - (1/2)exp(-j27rJ)).
It converts a discrete-time signal into a complex function of f, where f is called
the frequency and is measured in cycles per sample. The operation of taking the
</p>
<p>Fourier transform of a signal is denoted by F {x[n]} and the signal and its Fourier
</p>
<p>transform are referred to as a Fourier transform pair. The latter relationship is
</p>
<p>usually denoted by x[nJ {:} XU). The discrete-time Fourier transform is periodic in
</p>
<p>frequency with period one and for this reason we need only consider the frequency
</p>
<p>interval [-1/2,1/2]. Since the Fourier transform is a complex function offrequency,
</p>
<p>it can be represented by the two real functions
</p>
<p>IXU)I = C~OO x[n]COS(21rjnl) 2 + C~OO X[n]Sin(21r jn)) 2
</p>
<p>"'(f) - 2:~=-00 x[n] sin(27rfn)
'+' = arctan - : : : : : : : = . : o o O : : = - - - . . : : : . = - - : - - = - : - - = - - - - - : ~ - , . . . : . . .
</p>
<p>2:n=-oo x[nJ cos(27r fn)</p>
<p/>
</div>
<div class="page"><p/>
<p>D.2. LINEAR TRANSFORMS 797
</p>
<p>Signal name x[n] X(J) ( - ~ : S f:S ~ )
</p>
<p>Unit impulse o[n] = { ~ n=O I
n#O
</p>
<p>Real sinusoid cos(21rfon) ~0(J + fa) + ~0(J - fa)
</p>
<p>Complex sinusoid exp(j21r fan) 0(J - fa)
</p>
<p>Exponential anu[n] 1 lal &lt; Il-aexp(-j27rj)
</p>
<p>Double-sided exponential a1nl l-a
2
</p>
<p>lal &lt; I1+a2-2acos(27rJ)
</p>
<p>Table D.I: Discrete-time Fourier transform pairs.
</p>
<p>which are called the magnitude and phase, respectively. For example, if x[n] =
</p>
<p>(1/2)n u[n], then
</p>
<p>I
</p>
<p>1&gt;(J) =
</p>
<p>IX(J)I
")5/4 - cos(21rf)
</p>
<p>~ sin(21rf)
- arctan .
</p>
<p>I - ~ cos(21rf)
</p>
<p>Note that the magnitude is an even function or IX( - f)1 = IX(J)I and the phase
</p>
<p>is an odd function or 1&gt;( - f) = -1&gt;(J). Some Fourier transform pairs are given in
Table D.l. Some important properties of the discrete-time Fourier transform are:
</p>
<p>a. Linearity - F{ax[n] + by[n]} = aX(J) + bY(J)
</p>
<p>b. Time shift - F{x[n - no]} = exp( -j21rfno)X(J)
</p>
<p>c. Modulation - F{cos(21rfon)x[n]} = ~X(J + fa) + ~X(J - fa)
</p>
<p>d. Time reversal- F{x[-n]} = X*(J)
</p>
<p>e. Symmetry - if x[n] is even, then X(J) is even and real, and if x[n] is odd, then
</p>
<p>X (J) is odd and purely imaginary.
</p>
<p>f. Energy - the energy defined as L:~=-oo x2[n] can be found from the Fourier
transform using Parseval's theorem
</p>
<p>00 1
</p>
<p>L x2[n] = i: IX(J)12dj.
n=-oo 2</p>
<p/>
</div>
<div class="page"><p/>
<p>798 APPENDIX D. REVIEW OF SIGNALS AND SYSTEMS
</p>
<p>g. Inner product - as an extension of Parseval's theorem we have
</p>
<p>00 1
</p>
<p>L x[n]y[n] = i: X*(J)Y(J)dj.
n=-oo 2
</p>
<p>Two signals x[n] and y[n] are said to be convolved together to yield a new signal
</p>
<p>z[n] if
00
</p>
<p>z[n] = L x[k]y[n - k]
k=-oo
</p>
<p>- 00 &lt; n &lt; 00.
</p>
<p>As an example, if x[n] = urn] and y[n] = urn], then z[n] = (n+1)u[n]. The operation
of convolving two signals together is called convolution and is implemented using a
</p>
<p>convolution sum. It is denoted by x[n] *y[n]. The operation is commutative in that
</p>
<p>x[n] * y[n] = y[n] * x[n] so that an equivalent form is
00
</p>
<p>z[n] = L y[k]x[n - k]
k=-oo
</p>
<p>- 00 &lt; n &lt; 00.
</p>
<p>As an example, if y[n] = o[n - no], then it is easily shown that x[n] * o[n - no] =
o[n- no] *x[n] = x[n - no]. The most important property of convolution is that two
signals that are convolved together produce a signal whose Fourier transform is the
</p>
<p>product of the signals' Fourier transforms or
</p>
<p>F{x[n] *y[n]} = X(J)Y(J).
</p>
<p>Two signals x[n] and y[n] are said to be correlated together to yield a new signal
</p>
<p>z[n] if
00
</p>
<p>z[n] = L x[k]y[k+ n]
k=-oo
</p>
<p>- 00 &lt; n &lt; 00.
</p>
<p>The Fourier transform of z[n] is X*(J)Y(J). The sequence z[n] is also called the
</p>
<p>deterministic cross-correlation. If x[n] = y[n], then z[n] is called the deterministic
autocorrelation and its Fourier transform is IX(JW .
</p>
<p>The discrete-time signal may be recovered from its Fourier transform by using
</p>
<p>the discrete-time inverse Fourier transform
</p>
<p>1
</p>
<p>x[n] = i: X(J) exp(j21rfn)df
2
</p>
<p>- 00 &lt; n &lt; 00. (D.3)
</p>
<p>As an example, if X(J) = ~o(J + fa) + ~o(J - fo), then the integral yields x[n] =
cos(21rfon). It also has the interpretation that a discrete-time signal x[n] may be
</p>
<p>thought of as a sum of complex sinusoids X(J) exp(j21rfn)b.f for -1/2 :s: f :s: 1/2
with amplitude IX(J)Ib.f and phase LX(J). There is a separate sinusoid for each
</p>
<p>frequency i , and the total number of sinusoids is uncountable.</p>
<p/>
</div>
<div class="page"><p/>
<p>D.2. LINEAR TRANSFORMS 799
</p>
<p>D.2.2 Numerical Evaluation of Discrete-Time Fourier Transforms
</p>
<p>(D.4)- 1/2 ::; f ::; 1/2.
</p>
<p>The discrete-time Fourier transform of a signal x[n], which is nonzero only for n =
</p>
<p>0, 1, ... ,N - 1, is given by
</p>
<p>N-l
</p>
<p>X(f) = L x[n] exp( -j271-jn)
n=O
</p>
<p>Such a signal is said to be time-limited. Since the Fourier transform is periodic
</p>
<p>with period one, we can equivalently evaluate it over the interval &deg;::; f ::; 1. Then,
if we desire the Fourier transform for -1/2 ::; f' &lt; 0, we use the previously eval-
uated X(f) with f = f' + 1. To numerically evaluate the Fourier transform we
therefore can use the frequency interval [0, 1] and compute samples of X (f) for
</p>
<p>f = 0, l/N, 2/N, ... ,(N - l)/N. This yields the discrete Fourier transform (DFT)
which is defined as
</p>
<p>N-l
</p>
<p>X[k] = X(f)lf=k jN = L x [n]exp (-j21r(k/N)n)
n=O
</p>
<p>k = 0,1, ... , N - 1.
</p>
<p>Since there are only N time samples, we may wish to compute more frequency
</p>
<p>samples since X (f) is a continuous function of frequency. To do so we can zero
</p>
<p>pad the time samples with zeros to yield a new signal x' [n] of length M &gt; N with
samples {x[O] , x[l], ... , x [N -1],0,0, . .. ,O}. This new signal x' [n ] will consist of N
</p>
<p>time samples and M - N zeros so that the DFT will compute more finely spaced
</p>
<p>frequency samples as
</p>
<p>M-l
</p>
<p>X[k] = X(f)lf=kjM = L x' [n]exp (-j21r(k/M)n)
n=O
</p>
<p>k = 0, 1, ... ,M-1
</p>
<p>N-l
</p>
<p>L x [n]exp (-j21r(k/M)n)
n=O
</p>
<p>k = 0, 1, ... ,M - 1.
</p>
<p>The actual DFT is computed using the fast Fourier transform (FFT) , which is an
</p>
<p>algorithm used to reduce the computation.
</p>
<p>The inverse Fourier transform of an infinite length causal sequence can be ap-
</p>
<p>proximated using an inverse DFT as
</p>
<p>1 1
</p>
<p>x [n] = i: X(f) exp(j21rfn)df = faX(f) exp(j21rfn)df
2
</p>
<p>M -l
</p>
<p>~ ~ L X[k] exp (j21r(k/M)n) n = 0,1, ... ,M - 1. (D.5)
k=O
</p>
<p>One should choose M large. The actual inverse DFT is computed using the inverse
</p>
<p>FFT.</p>
<p/>
</div>
<div class="page"><p/>
<p>800 APPENDIX D. REVIEW OF SIGNALS AND SYSTEMS
</p>
<p>D.2.3 z-Tra n sfo r m s
</p>
<p>The z-transform of a discrete-time signal x[n] is defined as
</p>
<p>00
</p>
<p>X( z) = L x[n]z-n
n=-oo
</p>
<p>(D.6)
</p>
<p>where z is a complex variable that takes on values for which IX(z)1 &lt; 00. As an
example, if x[n] = (1/2)n u[n], then
</p>
<p>1
X(z) = 1 -1
</p>
<p>1- 2"z
</p>
<p>1
Izi &gt; 2' (D.7)
</p>
<p>The operation of taking the z-transform is indicated by Z{x[n]} . Some important
</p>
<p>properties of the z-transform are:
</p>
<p>a . Linearity - Z{ax[n] + by[n]} = aX(z) + bY(z)
</p>
<p>b. Time shift - Z{x[n - no]} = z-nox(z)
</p>
<p>c. Convolution - Z{x[n] *y[n]} = X(z)Y(z) .
</p>
<p>Assuming that the z-transform converges on the unit circle, the discrete-time Fourier
</p>
<p>transform is given by
</p>
<p>XU) = X(z)lz=exp(j21rf) (D.8)
</p>
<p>as is seen by comparing (D.6) to (D.2). As an example, if x[n] = (1/2)n u[n], then
</p>
<p>from (D.7)
</p>
<p>XU) = 1 1 .
1 - 2" exp( -J2nJ)
</p>
<p>since X(z) converges for Izl = Iexp(j2nJ) I = 1 &gt; 1/2.
</p>
<p>D.3 Discrete-Time Linear Systems
</p>
<p>A discrete-time system takes an input signal x[n] and produces an output signal y[n].
</p>
<p>The transformation is symbolically represented as y[n] = &pound;{x[n]}. The system is
</p>
<p>linear if &pound;{ax[n] + by[n]} = a&pound;{x[n]} + b&pound;{y[n]}. A system is defined to be shift
invariant if &pound;{x[n - no]} = y[n - no]. If the system is linear and shift invariant
(LSI) , then the output is easily found if we know the output to a unit impulse. To</p>
<p/>
</div>
<div class="page"><p/>
<p>D.3. DISCRETE-TIME LINEAR SYSTEMS
</p>
<p>see this we compute the output of the system as
</p>
<p>y[n] &pound;{ x[n]}
</p>
<p>= c {J==x[k]o[n - kJ}
00
</p>
<p>L x[k]&pound;{8[n - k]}
k=-oo
</p>
<p>00
</p>
<p>L x[k] &pound;{8[n]}ln--+n_k
k=-oo
</p>
<p>00
</p>
<p>L x[k]h[n - k]
k=-oo
</p>
<p>(using (D.l))
</p>
<p>(linearity)
</p>
<p>(shift invariance)
</p>
<p>801
</p>
<p>where h[n] = &pound;{ 8[n]} is called the impulse response of the system. Note that
y[n] = x[n]* h[n] = h[n]*x[n] and so the output of the LSI system is also given by
the convolution sum
</p>
<p>00
</p>
<p>y[n] = L h[k]x[n - k].
k=-oo
</p>
<p>(D.9)
</p>
<p>(D.10)
</p>
<p>A causal system is defined as one for which h[k] = 0 for k &lt; 0 since then the output
depends only on the present input x[n] and the past inputs x[n - k] for k ~ 1. The
system is said to be stable if
</p>
<p>00
</p>
<p>L Ih[k]1 &lt; 00.
k=-oo
</p>
<p>If this condition is satisfied, then a bounded input signal or Ix[n]l &lt; 00 for -00 &lt;
n &lt; 00 will always produce a bounded output signal or Iy[n]l &lt; 00 for -00 &lt; n &lt; 00.
As an example, the LSI system with impulse response h[k] = (l/2)ku[k] is stable
but not the one with impulse response h[k] = u[k]. The latter system will produce
the unbounded output y[n] = (n + l)u[n] for the bounded input x[n] = urn] since
urn] * urn] = (n + 1)u[n].
</p>
<p>Since for an LSI system y[n] = h[n]* x[n], it follows from the properties of z-
transforms that Y( z) = ll(z)X(z), where ll(z) is the z-transform of the impulse
</p>
<p>response. As a result, we have that
</p>
<p>ll(z) = Y(z) = Output z-transform
X(z) Input z-transform
</p>
<p>and ll(z) is called the system function. Note that since it is the z-transform of the
</p>
<p>impulse response h[n] we have
</p>
<p>00
</p>
<p>ll(z) = L h[n]z-n.
k=-oo
</p>
<p>(D.ll)</p>
<p/>
</div>
<div class="page"><p/>
<p>802 APPENDIX D. REVIEW OF SIGNALS AND SYSTEMS
</p>
<p>If the input to an LSI system is a complex sinusoid, x[n] = exp(j2nIon), then
</p>
<p>the output is from (D.9)
</p>
<p>00
</p>
<p>y[n] = 2:= h[k]exp[j27rfo(n - k)]
k=-oo
</p>
<p>00
</p>
<p>v
</p>
<p>H(fo)
</p>
<p>2:= h[k]exp(-j27rfok) exp(j27rfon).
k=-oo,
</p>
<p>(D.12)
</p>
<p>It is seen that the output is also a complex sinusoid with the same frequency but
</p>
<p>multiplied by the Fourier transform of the impulse response evaluated at the sinu-
</p>
<p>soidal frequency. Hence, H(f) is called the frequency response. Also, from (D.12)
</p>
<p>the frequency response is obtained from the system function (see (D.ll)) by let-
</p>
<p>ting z = exp(j27rf) . Finally, note that the frequency response is the discrete-time
Fourier transform of the impulse response. As an example, if h[n] = (1/2)nu[n],
then
</p>
<p>1
1i(z) = 1 -1
</p>
<p>1- 2z
</p>
<p>and
</p>
<p>H(f) = 1i(exp(j27rf)) = 1 1 . .
1- 2exp(-J27rf)
</p>
<p>The magnitude response of the LSI system is defined as IH(f)1 and the phase re-
</p>
<p>sponse as L.H(f).
</p>
<p>As we have seen, LSI systems can be characterized by the equivalent descriptions:
</p>
<p>impulse response, system function, or frequency response. This means that given
one of these descriptions the output can be determined for any input. LSI systems
</p>
<p>can also be characterized by linear difference equations with constant coefficients.
</p>
<p>Some examples are
</p>
<p>Y1[n] x[n] - bx[n - 1]
</p>
<p>Y2[n] aY2[n - 1] + x[n]
Y3[n] aY3[n - 1] + x[n] - bx[n -1]
</p>
<p>and more generally
</p>
<p>p q
</p>
<p>y[n] = 2:= a[k]y[n - k] + x[n] - ~ b[k]x[n - k].
k=l k=l
</p>
<p>(D.13)
</p>
<p>The system function is found by taking the z-transform of both sides of the difference</p>
<p/>
</div>
<div class="page"><p/>
<p>D.3. DISCRETE-TIME LINEAR SYSTEMS
</p>
<p>equations and using (D.lO) to yield
</p>
<p>803
</p>
<p>Y3(Z)
</p>
<p>X(z) - bz-IX(z) "* 1-l1(Z) = 1- bz- I
1
</p>
<p>az-
I
Y2(Z) + X(z) "* 1-l2(Z) = 1 _ az- I
</p>
<p>1 -bz- I
az-
</p>
<p>I
Y3(Z) + X(z) - bz- IX(z) "* 1-l3(Z) = 1 _ az- I
</p>
<p>and the frequency response is obtained using H(f) = 1-l(exp(j2nJ)) . More generally,
for the LSI system whose difference equation description is given by (D.13) we have
</p>
<p>1 - "q b[k]z-k
1-l( ) L..Jk=1
</p>
<p>Z = 1 - L~=I a[k]z-k'
(D.14)
</p>
<p>The impulse response is obtained by taking the inverse z-transform of the system
</p>
<p>function to yield for the previous examples
</p>
<p>{
</p>
<p>I n = 0
</p>
<p>-b n = 1
</p>
<p>o otherwise
anu[n]
</p>
<p>anu[n] - ban-Iu[n - 1]
</p>
<p>(assuming system is causal)
</p>
<p>(assuming system is causal).
</p>
<p>The impulse response could also be obtained by letting x[n] = 8[n] in the difference
equations and setting y[-l] = 0, due to causality, and recursing the difference
</p>
<p>equation. For example, if the difference equation is y[n] = (1/2)y[n -1] + x[n], then
by definition the impulse response satisfies the equation h[n] = (1/2)h[n - 1]+ 8[n].
By recursing this we obtain
</p>
<p>h[O] ~h[-l] + 8[0] = 1
h[l] = ~h[O] + 8[1] = ~
h[2] ~h[l] = i
</p>
<p>etc.
</p>
<p>(since h[-l] = 0 due to causality)
</p>
<p>(since 8[n] = 0 for n 2: 1)
</p>
<p>and so in general we have the impulse response h[n] = (1/2)n u[n]. The system with
</p>
<p>impulse response hI [n] is called a finite impulse response (FIR) system while those of
</p>
<p>h2[n] and h3[n] are called infinite impulse response (IIR) systems. The terminology
</p>
<p>refers to the number of nonzero samples of the impulse response.
</p>
<p>For the system function H3(Z) = (1- bz- I)/(1 - az- I) , the value of z for which
</p>
<p>the numerator is zero is called a zero and the value of z for which the denominator is
</p>
<p>zero is called a pole. In this case the system function has one zero at z = b and one
</p>
<p>pole at z = a. For the system to be stable, assuming it is causal, all the poles of the
</p>
<p>system function must be within the unit circle of the z-plane. Hence, for stability</p>
<p/>
</div>
<div class="page"><p/>
<p>804 APPENDIX D. REVIEW OF SIGNALS AND SYSTEMS
</p>
<p>we require lal &lt; 1. The zeros may lie anywhere in the z-plane. For a second-order
system function (let p = 2 and q = 0 in (D.14)) given as
</p>
<p>1&pound; (z) __----=--=----:-l_ ___=_=_~
- 1 - a[l] z-l - a[2]z-2
</p>
<p>the poles, assuming they are complex, are located at z = r exp(&plusmn;jO). Hence, for
stability we require r &lt; 1 and we note that since the poles are the z values for which
the denominator polynomial is zero, we have
</p>
<p>1 - a[l]z-l - a[2]z-2 = z-2(z - rexp(jO))(z - r exp( -jO)).
</p>
<p>Therefore, the coefficients are related to the complex poles as
</p>
<p>all] 2r cos(O)
</p>
<p>a[2] _r2
</p>
<p>which puts restrictions on the possible values of all] and a[2]. As an example, the
</p>
<p>coefficients all] = 0, a[2] = -1/4 produce a stable filter but not all] = 0, a[2] = -2.
An LSI system whose frequency response is
</p>
<p>H(f) = {I IfI '.5: B
o IfI&gt; B
</p>
<p>is said to be an ideal lowpass filter. It passes complex sinusoids undistorted if their
</p>
<p>frequency is IfI '.5: B but nullifies ones with a higher frquency. The band of positive
frequencies from f = 0 to f = B is called the passband and the band of positive
</p>
<p>frequencies for which f &gt; B is called the stopband.
</p>
<p>D.4 Continuous-Time Signals
</p>
<p>A continuous-time signal is a function of time x(t) for -00 &lt; t &lt; 00. Some impor-
tant signals are:
</p>
<p>a. Unit impulse - It is denoted by 8(t). An impulse 8(t), also called the Dirac delta
</p>
<p>function, is defined as the limit of a very narrow pulse as the pulsewidth goes
</p>
<p>to zero and the pulse amplitude goes to infinity, such that the overall area
</p>
<p>remains at one. Therefore, if we define a very narrow pulse as
</p>
<p>XT(t) = {o~ ItI '.5: T/2
ItI &gt; T/2
</p>
<p>then the unit impulse is defined as
</p>
<p>8(t) = lim XT(t).
T--+O</p>
<p/>
</div>
<div class="page"><p/>
<p>D.5. LINEAR TRANSFORMS 805
</p>
<p>The impulse has the important sifting property that if x{t) is continuous at
</p>
<p>t = to , then
</p>
<p>i : x {t )8{t - to)dt = x{to).
</p>
<p>b. Unit step - x {t ) = 1 for t ~ 0 and x{t) = 0 for t &lt; O. It is also denoted by u{t).
</p>
<p>c. Real sinusoid - x {t ) = A cos{27fFot + 0) for -00 &lt; t &lt; 00, where A is the
amplitude (must be nonnegative) , Fo is the frequency in Hz (cycles per second) ,
</p>
<p>and 0 is the phase in radians.
</p>
<p>d. Complex sinusoid - x{t) = Aexp{j27fFot + 0) for -00 &lt; t &lt; 00, with the
amplitude, frequency, and phase taking on same values as for real sinusoid.
</p>
<p>e. Exponential - x{t) = exp{at)u{t)
</p>
<p>f. Puls e - x{t) = 1 for ItI :::; T /2 and x{t) = 0 for ItI&gt; T /2.
</p>
<p>Some special signals are defined next.
</p>
<p>a. A signal is causal if x{t) = 0 for t &lt; 0, for example, x {t ) = u{t) .
</p>
<p>b. A signal is anticausal if x {t ) = 0 for t &gt; 0, for example, x{t) = u{ -t).
</p>
<p>c. A signal is even if x {-t) = x {t ) or it is symmetric about t = 0, for example,
x {t ) = cos{27fFot).
</p>
<p>d. A signal is odd if x {-t) = -x{t) or it is antisymmetric about t = 0, for example,
</p>
<p>x {t ) = sin{27fFot).
</p>
<p>e. A signal is stabl e if J~ oo Ix{t )ldt &lt; 00 (also called absolutely integrable), for ex-
ample, x{t) = exp{-t)u{t).
</p>
<p>D.5 Linear Transforms
</p>
<p>D.5.! Continuous-Time Fourier Transforms
</p>
<p>The continuous-time Fouri er transform X(F) of a continuous-time signal x{t) is
</p>
<p>defined as
</p>
<p>X{F) = i: x(t)exp(-j27fFt)dt -oo&lt;F&lt;oo. (D.15)
</p>
<p>An example is x(t) = exp( -t)u(t) for which X(F) = 1/(1 + j27fF). It converts a
continuous-t ime signal into a complex function of F , where F is called the frequency
</p>
<p>and is measured in Hz (cycles per second). The operation of taking the Fourier
</p>
<p>transform of a signal is denoted by F{x{t)} and the signal and its Fourier trans-
</p>
<p>form are referred to as a Fourier transform pair. The latter relationship is usually</p>
<p/>
</div>
<div class="page"><p/>
<p>806 APPENDIX D. REVIEW OF SIGNALS AND SYSTEMS
</p>
<p>Signal name x(t) X(F)
</p>
<p>Unit impulse 8(t) ={~ t=O 1
t#O
</p>
<p>Real sinusoid cos(211"Fot) ~8(F + Fo) + ~8(F - Fo)
</p>
<p>Complex sinusoid exp(j211"Fot) 8(F - Fo)
</p>
<p>Exponential exp( -at)u(t) 1 a&gt;Oa+j27rF
</p>
<p>Pulse = { ~ ItI 5:. T /2 T sin(1TF T )ItI&gt; T/2 1TFT
</p>
<p>Table D.2: Continuous-time Fourier transform pairs.
</p>
<p>denoted by x(t) {:} X(F). Note that the magnitude of X(F) is an even function
</p>
<p>or IX( -F)I = IX(F)I and the phase is an odd function or &cent;( -F) = -&cent;(F). Some
Fourier transform pairs are given in Table D.2.
</p>
<p>Some important properties of the continuous-time Fourier transform are:
</p>
<p>a. Linearity - F{ax(t) + by(t)} = aX(F) + bY(F)
</p>
<p>b. Time shift - F{x(t - to)} = exp(-j211"Fto)X(F)
</p>
<p>c. Modulation - F{cos(211"Fot) x(t)} = ~X(F + Fo) + ~X(F - Fo)
</p>
<p>d. Time reversal- F{x( -t)} = X*(F)
</p>
<p>e. Symmetry - if x(t) is even, then X(F) is even and real, and if x(t) is odd, then
</p>
<p>X (F) is odd and purely imaginary.
</p>
<p>f. Energy - the energy defined as J~oo x2 (t )dt can be found from the Fourier trans-
</p>
<p>form using Parseval's theorem
</p>
<p>g. Inner product - as an ext ension of Parseval's theorem we have
</p>
<p>i: x(t)y(t)dt = i: X *(F)Y(F)dF.
Two signals x(t) and y(t) are said to be convolved together to yield a new signal
z(t) if
</p>
<p>z(t) = i: x(r)y(t - r)dr - 00 &lt; t &lt; 00.</p>
<p/>
</div>
<div class="page"><p/>
<p>D.6. CONTINUOUS-TIME LINEAR SYSTEMS 807
</p>
<p>- 00 &lt; t &lt; 00.
</p>
<p>As an example, if x(t) = u(t) and y(t) = u(t), then z(t) = tu(t). The operation
of convolving two signals together is called convolution and is implemented using a
</p>
<p>convolution integral. It is denoted by x(t) * y(t). The operation is commutative in
that x(t) * y(t) = y(t) * x(t) so that an equivalent form is
</p>
<p>z(t) = i: y(r)x(t - r)dr
As an example, if y(t) = o(t - to), then it is easily shown that x(t) * o(t - to) =
o(t - to)* x(t) = x(t - to). The most important property of convolution is that two
signals that are convolved together produce a signal whose Fourier transform is the
</p>
<p>product of the signals' Fourier transforms or
</p>
<p>F{x(t) *y(t)} = X(F)Y(F).
</p>
<p>The continuous-time signal may be recovered from its Fourier transform by using
</p>
<p>the continuous-time inverse Fourier transform
</p>
<p>x(t) = i: X(F) exp(j21rFt)dF - 00 &lt; t &lt; 00. (D.16)
As an example, if X(F) = ~o(F + Fo)+ ~o(F - Fo), then the integral yields x(t) =
cos(21rFot). It also has the interpretation that a continuous-time signal x(t) may be
</p>
<p>thought of as a sum of complex sinusoids X(F) exp(j21rFt)b.F for -00 &lt; F &lt; 00
with amplitude IX(F)Ib.F and phase L.X(F). There is a separate sinusoid for each
</p>
<p>frequency F, and the total number of sinusoids is uncountable.
</p>
<p>D.6 Continuous-Time Linear Systems
</p>
<p>A continuous-time system takes an input signal x(t) and produces an output signal
</p>
<p>y(t). The transformation is symbolically represented as y(t) = L:{x(t)}. The system
is linear if L:{ax(t) + by(t)} = aL:{x(t)} + bL:{y(t)}. A system is defined to be time
invariant if L:{x(t-to)} = y(t-to). If the system is linear and time invariant (LTI),
</p>
<p>then the output is easily found if we know the output to a unit impulse. It is given
</p>
<p>by the convolution integral
</p>
<p>y(t) = i: h(r)x(t - r)dr (D.17)
where h(t) = L:{o(t)} is called the impulse response of the system. A causal system
</p>
<p>is defined as one for which h(r) = 0 for r &lt; 0 since then the output depends only
on the present input x(t) and the past inputs x(t - r) for r &gt; O. The system is said
to be stable if</p>
<p/>
</div>
<div class="page"><p/>
<p>808 APPENDIX D. REVIEW OF SIGNALS AND SYSTEMS
</p>
<p>(D.18)
</p>
<p>If this condition is satisfied, then a bounded input signal or Ix(t)1 &lt; 00 for -00 &lt;
t &lt; 00 will always produce a bounded output signal or ly(t)1 &lt; 00 for -00 &lt; t &lt;
00. As an example, the LTI system with impulse response h(r) = exp( rrT' )u( r) is
</p>
<p>stable but not the one with impulse response h(r) = u(r). The latter system will
produce the unbounded output y(t) = tu(t) for the bounded input x(t) = u(t) since
u(t) * u(t) = tu(t).
</p>
<p>If the input to an LTI system is a complex sinusoid, x(t) = exp(j211"Fot) , then
</p>
<p>the output is from (D.17)
</p>
<p>y(t) = i: h(r) exp[j211"Fo(t - r)]dr
i: h(r) exp (-j211"For)drexp(j211"Fot).
, .,...
</p>
<p>H(Fo)
</p>
<p>It is seen that the output is also a complex sinusoid with the same frequency but
</p>
<p>multiplied by the Fourier transform of the impulse response evaluated at the si-
</p>
<p>nusoidal frequency. Hence, H (F) is called the frequency response. Finally, note
</p>
<p>that the frequency response is the continuous-time Fourier transform of the impulse
</p>
<p>response. As an example, if h(t) = exp( -at)u(t), then for a &gt; 0
</p>
<p>H(F) _ 1
a + j211"F
</p>
<p>The magnitude response of the LSI system is defined as IH(F)j and the phase re-
</p>
<p>sponse as LH(F).
</p>
<p>An LTI system whose frequency response is
</p>
<p>H(F) = {1 IFI ~ W
o IFI &gt; W
</p>
<p>is said to be an ideal lowpass filter. It passes complex sinusoids undistorted if their
</p>
<p>frequency is IFI ~ W Hz but nullifies ones with a higher frquency. The band of
positive frequencies from F = 0 to F = W is called the passband and the band of
</p>
<p>positive frequencies for which F &gt; W is called the stopband.
</p>
<p>References
</p>
<p>Jackson, L.B., Signals, Systems, and Transforms, Addison-Wesley, Reading, MA,
</p>
<p>1991.
</p>
<p>Oppenheim, A.V., A.S. Willsky, S.H. Nawab, Signal and Systems, Prentice-Hall,
</p>
<p>Upper Saddle River, NJ , 1997.
</p>
<p>Poularikas, A.D., S. Seely, Signals and Systems, PWS, Boston, 1985.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix E
</p>
<p>Answers to Selected Problems
</p>
<p>Note: For problems based on computer simulations the number of realizations used
</p>
<p>in the computer simulation will affect the numerical results. In the results listed
</p>
<p>below the number of realizations is denoted by N rea1. Also, each result assumes
</p>
<p>that rand('state' ,0) and/or randn('state' ,0) have been used to initialize the
</p>
<p>random number generator (see Appendix 2A for further details).
</p>
<p>Chapter 1
</p>
<p>1. exp eriment: toss a coin; outcomes: {head, tail}; probabilities: 1/2,1/2
</p>
<p>5. a. continuous; b. discrete; c. discrete; d. continuous; e. discrete
</p>
<p>7. yes, yes
</p>
<p>10. P[k = 9] = 0.0537, probably not
</p>
<p>13. 1/2
</p>
<p>14. 0.9973 for .6. = 0.001
</p>
<p>Chapter 2
</p>
<p>1. P[Y = 0] = 0.7490, P[Y = 1] = 0.2510 (Nreal = 1000)
</p>
<p>3. via simulation: P[-1 ~ X ~ 1] = 0.6863; via numerical integration with .6. =
0.01, P[-1 ~ X ~ 1] = 0.6851 (Nreal = 10,000)
</p>
<p>6. values near zero
</p>
<p>8. estimated mean = 0.5021; true mean = 1/2 (Nreal = 1000)
</p>
<p>11. estimated mean = 1.0042; true mean = 1 (Nreal = 1000)
</p>
<p>13. 1.2381 (Nrea1 = 1000)</p>
<p/>
</div>
<div class="page"><p/>
<p>810 APPENDIX E. ANSWERS TO SELECTED PROBLEMS
</p>
<p>14. no; via simulation: mean of ..;u = 0.6589; via simulation: Jmean of U =
0.7125 (Nreal = 1000)
</p>
<p>Chapter 3
</p>
<p>1. a. AC = {x : x ~ I} , B C = {x : x &gt; 2}
b. AU B = {x: -00 &lt; x &lt; oo} = 5, An B = {x: 1 &lt; x &lt; 2}
c. A - B = {x: x &gt; 2}, B - A = {x : x ~ I}
</p>
<p>7. A = {1,2,3}, B = {4,5}, C = {1,2,3}, D = {4,5,6}
</p>
<p>12. a. 107 , discrete b. 1, discrete c. 00 (uncountable), continuous d. 00 (uncount-
</p>
<p>able) , continuous e. 2, discrete f. 00 (countable), discrete
</p>
<p>14. a. 5 = {t : 30 ~ t ~ 100} b. outcomes are all t in interval [30,100] c. set of
outcomes having no elements, i.e., {negative temperatures} d. A = {t : 40 ~
</p>
<p>t ~ 60}, B = {t : 40 ~ t ~ 50 or 60 ~ t ~ 70}, C = {100} (simple event) e.
A = {t : 40 ~ t ~ 60}, B = {t : 60 ~ t ~ 70}
</p>
<p>18. a. 1/2 b. 1/2 c. 6/36 d. 24/36
</p>
<p>19. Peven = 1/2, Peven = 0.5080 (Nrea1 = 1000)
</p>
<p>21. a. even, 2/3 b. odd, 1/3 c. even or odd, 1 d. even and odd, 0
</p>
<p>23. 1/56
</p>
<p>25. 10/36
</p>
<p>27. no
</p>
<p>33. 90/216
</p>
<p>35. 676,000
</p>
<p>38. 0.00183
</p>
<p>40. total number = 16, two-toppings = 6
</p>
<p>44. a. 4 of a kind
</p>
<p>b. flush
</p>
<p>13&middot;48
</p>
<p>( 55
2)
</p>
<p>4&middot; C;)
(5n</p>
<p/>
</div>
<div class="page"><p/>
<p>811
</p>
<p>49. P[k 2: 95] = 0.4407, P[k 2: 95] = 0.4430 (Nreal = 1000)
</p>
<p>Chapter 4
</p>
<p>2. 1/4
</p>
<p>5. 1/4
</p>
<p>7. a. 0.53 b. 0.34
</p>
<p>11. 0.5
</p>
<p>14. yes
</p>
<p>19. 0.03
</p>
<p>21. a. no b. no
</p>
<p>22.4
</p>
<p>26. 0.0439
</p>
<p>28. 5/16
</p>
<p>33. P[k] = (k - 1)(1 - p)k-2p2, k = 2,3, ... ,
</p>
<p>38. 2 red , 2 black, 2 white
</p>
<p>40. 3/64
</p>
<p>43. 165/512
</p>
<p>Chapter 5
</p>
<p>4. Sx = {O, 1,4, 9}
~ Xi = 0
</p>
<p>~ Xi = 1
</p>
<p>~ Xi = 4
</p>
<p>~ Xi = 9
</p>
<p>6. 0 &lt; P &lt; 1, a = (1 _ p) / p2
</p>
<p>8. 0.99 19
</p>
<p>13. Average value = 5.0310, true value shown in Chapter 6 to be ).. = 5 (Nreal =
1000)
</p>
<p>14. px[5] = 0.0029, px[5] = 0.0031 (from Poisson approximation)</p>
<p/>
</div>
<div class="page"><p/>
<p>812 APPENDIX E. ANSWERS TO SELECTED PROBLEMS
</p>
<p>18. P[X = 3] = 0.0613, P[X = 3] = 0.0607 (Nreal = 10,000)
</p>
<p>20. py[k] = exp( _&gt;..&raquo;,k/2 /k! for k = 0,2,4, ...
</p>
<p>26. px[k] = 1/5 for k = 1,2,3,4,5
</p>
<p>28. 0.4375
</p>
<p>31. 8.68 x 10-7
</p>
<p>Chapter 6
</p>
<p>2. 9/2
</p>
<p>4. 2/3
</p>
<p>8. geometric PMF
</p>
<p>13. yes, if X = constant
</p>
<p>14. predictor = E[X] = 21/8, msemin = 47/64 = 0.7343
</p>
<p>15. estimated msemin = 0.7371 (Nreal = 10,000)
</p>
<p>20. &gt;..2 + &gt;..
</p>
<p>26. L ~ = o ( _ 1 ) n - k ( ~ ) En-k[X]E[Xk]
</p>
<p>27. &lt;/Jy(w) = exp (jwb)&lt;/Jx (aw)
</p>
<p>28. (1+ 2cos(w) + 2cos(2w))/5
</p>
<p>32. true mean = 1/2, true variance = 3/4 ; estimated mean = 0.5000, estimated
variance = 0.7500 (Nrea1= 1000)
</p>
<p>Chapter 7
</p>
<p>3. S = {(p,n), (p.d), (n,p), (n,d), (d.p), (d,n)}
</p>
<p>SX,y = {(I, 5), (1, 10), (5, 1), (5, 10), (10, 1), (10, 5)}
</p>
<p>8.
</p>
<p>{
</p>
<p>1/4 (i,j) = (0,0)
</p>
<p>. . _ 1/4 (i,j) = (1, -1)
PX,y[z ,)] - 1/4 ( ~ , ~ ) = (1,1)
</p>
<p>1/4 (z,)) = (2,0)
</p>
<p>10. 1/5</p>
<p/>
</div>
<div class="page"><p/>
<p>813
</p>
<p>13. pX [i] = (1&minus; p)i&minus;1p for i = 1, 2, . . . and same for pY [j]
</p>
<p>16. pX,Y [0, 0] = 1/4, pX,Y [0, 1] = 0, pX,Y [1, 0] = 1/8, pX,Y [1, 1] = 5/8
</p>
<p>19. no
</p>
<p>23. yes, X &sim; bin(10, 1/2), Y &sim; bin(11, 1/2)
</p>
<p>27. pZ [0] = 1/4, pZ [1] = 1/2, pZ [2] = 1/4, variance always increases when uncorre-
lated random variables are added
</p>
<p>33. 1/8
</p>
<p>37. 0
</p>
<p>38. 3/22
</p>
<p>40. minimum MSE prediction = EY [Y ] = 5/8 and minimum MSE = var(Y ) =
15/64 for no knowledge
minimum MSE prediction = Y = &minus;(1/15)x + 2/3 and minimum MSE =
var(Y )(1 &minus; 2X,Y ) = 7/30 based on observing outcome of X
</p>
<p>41. W = 5.4109h &minus; 205.0344
</p>
<p>43. W,Z =
&radic;
</p>
<p>/( + 1), where  = EX [X
2]/EN [N
</p>
<p>2]
</p>
<p>46. see solution for Problem 7.27
</p>
<p>48. pX,Y [0, 0] = 0.1190, pX,Y [0, 1] = 0.1310, pX,Y [1, 0] = 0.2410, pX,Y [1, 1] = 0.5090
(Nreal = 1000)
</p>
<p>49. X,Y =
&radic;
5/15 = 0.1490, X,Y = 0.1497 (Nreal = 100, 000)
</p>
<p>Chapter 8
</p>
<p>2. pY |X [j|0] = 1 for j = 0
pY |X [j|1] = 1/6 for j = 1, 2, 3, 4, 5, 6
P [Y = 1] = 1/12
</p>
<p>5. no, no, no
</p>
<p>6. pY |X [j|0] = 1/3 for j = 0 and = 2/3 for j = 1
pY |X [j|1] = 2/3 for j = 0 and = 1/3 for j = 1
pX|Y [i|0] = 1/3 for i = 0 and = 2/3 for i = 1
pX|Y [i|1] = 2/3 for i = 0 and = 1/3 for i = 1
</p>
<p>8. pY |X [j|i] = 1/5 for j = 0, 1, 2, 3, 4; i = 1, 2
pX|Y [i|j] = 1/2 for i = 1, 2; j = 0, 1, 2, 3, 4</p>
<p/>
</div>
<div class="page"><p/>
<p>814 APPENDIX E. ANSWERS TO SELECTED PROBLEMS
</p>
<p>11. 0.4535
</p>
<p>13. a. PYlx[YjIO] = 0,1,&deg;for Yj = -1/V2, 0, 1/V2, respectively
PYlx[Yjll/V2] = 1/2,0 ,1/2 for Yj = -1/V2, 0, 1/V2, respectively
PYlx[YjlV2] = 0, 1,&deg;for Yj = -1/V2, 0, 1/V2, respectively
not independent (conditional PMF depends on Xi)
</p>
<p>b. PYlx[YjIO] = 1/2,1/2 for Yj = 0,1 , respectively
PYlx[Yjll] = 1/2,1/2 for Yj = 0,1, respectively
independent
</p>
<p>17. pz[k] = px[k] L : ~ k P y [ j ] +Py[k] L:~k+lPX[j]
</p>
<p>21. EYlx[YIO] = 0, Ey 1x[Y!I] = 1/2, EYlx [Y12] = 1
</p>
<p>22. var(YIO) = 0, var(Yll) = 1/4, var(YI2) = 2/3
</p>
<p>28. optimal predictor: Y = &deg;for X = -1, Y = 1/2 for x = 0, and Y = &deg;for x = 1
optimal linear predictor: Y = 1/4 for x = -1,0,1
</p>
<p>-- --30. Ey1x[YIO] = 0.5204, EYlx [Yll ] = 0.6677 (Nrea1 = 10,000)
Chapter 9
</p>
<p>1. 0.0567
</p>
<p>4. yes
</p>
<p>6. (Xl , X 2 ) independent of X3
</p>
<p>10. E[X] = Ex[X], var(X) = var(X)/N
</p>
<p>13. Cx = [~ ~], det(Cx) = 0, no
</p>
<p>17. a. no, b. no , c. yes, d. no
</p>
<p>20. Cx = [~ ~]
</p>
<p>[
0.9056 0.4242]
</p>
<p>26. A = -0.4242 0.9056 for MATLAB 5.2
</p>
<p>[
-0.9056 0.4242]
</p>
<p>A = 0.4242 0.9056 for MATLAB 6.5, R13
</p>
<p>var(Yl ) = 7.1898, var(Y2 ) = 22.8102
</p>
<p>35. B = [ V3fi J5/2]
- V3fi /5f2</p>
<p/>
</div>
<div class="page"><p/>
<p>A [4.0693 0.9996]
36. ex = 0.9996 3.9300 (Nreal = 1000)
</p>
<p>Chapter 10
</p>
<p>2. 1/80
</p>
<p>4. a. no b. yes c. no
</p>
<p>12. 0.0252
</p>
<p>14. Gaussian: 0.0013 Laplacian: 0.0072
</p>
<p>17. first person probability = 0.393, first two persons probability = 0.090
</p>
<p>19. Fx(x) = 1/2 + (llrr) arctan(x)
</p>
<p>22 . Fx(x) = cI&gt; (~)
</p>
<p>28. 2.28%
</p>
<p>30. eastern U.S.
</p>
<p>33. yes
</p>
<p>36. c ~ 14
</p>
<p>40.
</p>
<p>py(y) = { ~(Y_~)3/4 exp[-.\(y - 1)1/4] Y 2: 1
y&lt;l
</p>
<p>43. py(y) = px(Y) + px(-Y)
</p>
<p>46.
</p>
<p>{
</p>
<p>_1_ O&lt;y&lt;l
py(y) = 02.,fY
</p>
<p>otherwise
</p>
<p>51.
</p>
<p>815
</p>
<p>P[-2:::; X:::; 2]
</p>
<p>P[-l :::; X:::; 1]
</p>
<p>P[-l &lt; X:::; 1]
</p>
<p>P[-l &lt; X &lt; 1]
</p>
<p>P[-l:::; X &lt; 1]
</p>
<p>1- kexp(-2)
</p>
<p>1- kexp(-l)
</p>
<p>i - kexp(-l)
k- kexp (- l )
~ - kexp (- l )</p>
<p/>
</div>
<div class="page"><p/>
<p>816 APPENDIX E. ANSWERS TO SELECTED PROBLEMS
</p>
<p>54. g(U) = J21n(1/(1 - U))
</p>
<p>Chapter 11
</p>
<p>1. 7/6
</p>
<p>10. &plusmn;9.12
</p>
<p>11. 0.1353
</p>
<p>14. N
</p>
<p>19. 0.0078
</p>
<p>21. JE[U] = ..fl72,E[VU] = 2/3
</p>
<p>22. E[s(to)] = 0, E[s2(tO)] = 1/2
</p>
<p>26. CJ2/2
</p>
<p>27. CJ2/2
</p>
<p>30. Tmin = 5.04, T max = 8.96
</p>
<p>38. E[Xn ] = 0 for n odd, E[Xn ] = n! for n even
</p>
<p>42. 8(x - J.l)
</p>
<p>44. J2var(X)
</p>
<p>--- --46. E[X] = 1.2533, E[X] = 1.2538; var(X) = 0.4292, var(X) = 0.4269 (Nrea1 =
1000)
</p>
<p>Chapter 12
</p>
<p>1. 7/16
</p>
<p>3. no, probability is 1/4
</p>
<p>5. 1r = 4P[X2 + y2 ~ 1], -n- = 3.1140 (Nrea1 = 10,000)
</p>
<p>7. 1/4
</p>
<p>10. P = 0.19, P = 0.1872 (Nrea1 = 10,000)
</p>
<p>11. 0</p>
<p/>
</div>
<div class="page"><p/>
<p>817
</p>
<p>15. px(x) = 2x for 0 &lt; x &lt; 1 and zero otherwise, py(y) = 2(1 - y) for 0 &lt; y &lt; 1
and zero otherwise
</p>
<p>18.
o x &lt; 0 or y &lt; 0
</p>
<p>kxy 0::; x &lt; 2,0 ::; y &lt; 4
</p>
<p>Fx,Y(x,y) = ~y x ~ 2,0::; Y &lt; 4
</p>
<p>~x 0 ::; x &lt; 2, Y ~ 4
</p>
<p>1 x ~ 2,y ~ 4
</p>
<p>23. (1- exp(-2)f
</p>
<p>25. no
</p>
<p>26. Q(2)
</p>
<p>30. P[bullseye] = 1 - exp(-2) = 0.8646, P[bullseye] = 0.8730 (Nreal = 1000)
</p>
<p>36. W rv N(J-lw, a~), Z rv N(J-lZ, a~)
</p>
<p>38. [:] - N(/L, C), where
</p>
<p>43. vr;:;r
</p>
<p>45. uncorrelated but not necessarily independent
</p>
<p>47.
</p>
<p>52. Q(l)
</p>
<p>Chapter 13
</p>
<p>2. yes, c = l/x
</p>
<p>4. PYlx(yl x) = exp(-y)/(l - exp( - x)) for 0 ::; y ::; x , x ~ 0</p>
<p/>
</div>
<div class="page"><p/>
<p>818 APPENDIX E. ANSWERS TO SELECTED PROBLEMS
</p>
<p>8. Px, Y(x, y) = 1/x for 0 &lt; Y &lt; x, 0 &lt; x &lt; 1; PY(y) = - In y for 0 &lt; Y &lt; 1
</p>
<p>10. PYlx(ylx) = l/x for 0 &lt; y &lt; x, 0 &lt; x &lt; 1; PXly(xly) = 1/(1- y) for y &lt; x &lt; 1,
O&lt;y&lt;1
</p>
<p>1
</p>
<p>14. Use P = J!l P[IXzl-IXII &lt; 0IXI = XI]PXl (XI)dxI and note independence of
2
</p>
<p>1
</p>
<p>Xl and Xz so that P = J!l P[lXzl :s XI]dxI
2
</p>
<p>16. Q(-I), assume Rand E are independent
</p>
<p>21. 1/2
</p>
<p>24. Use E(x+Y)lx[X + Ylx] = EYlx[Ylx] + x to yield E(x+Y)lx[X + YIX = 50] =
77.45 and E(x+Y)lx[X + YIX = 75] = 84.57
</p>
<p>Chapter 14
</p>
<p>1. Ey[Y] = 6, var(Y) = 11/2
</p>
<p>6. 1/16
</p>
<p>9. Y '" N(o, (J"I + (J"~ + (J"~)
</p>
<p>12. no since var(X) -+ (J"z /2 as N -+ 00
</p>
<p>19. Ey[Y] = 0, var(Y) = 1
</p>
<p>21. X3 = 7/5
</p>
<p>24. msemin = 8/15 = 0.5333
</p>
<p>25. IDSemin = 0.5407 (Nreal = 5000)
</p>
<p>Chapter 15
</p>
<p>7. no since the variance does not converge to zero
</p>
<p>13. Y '" N(2000 , 1000/3)
</p>
<p>19. N = 5529
</p>
<p>20. 1 - Q(-77.78) ~ 0
</p>
<p>22. Gaussian, "converges" for all N ;::: 1
</p>
<p>23. no since approximate 95% confidence interval is [0.723,0.777]</p>
<p/>
</div>
<div class="page"><p/>
<p>819
</p>
<p>26. drug group has approximate 95% confidence interval of [0.69,0.91] and placebo
</p>
<p>group has [0.47,0.73]. Can't say if drug is effective since true value of p could
</p>
<p>be 0.7 for either group.
</p>
<p>Chapter 16
</p>
<p>1. a. temperature at noon b. expense in dollars and cents c. time left in hours and
</p>
<p>minutes
</p>
<p>4. p50(1 - p) 50 , 0
</p>
<p>7. E[X[n]] = (n + 1)/2, var(X[n]) = (3/4)(n + 1)
</p>
<p>9. exp( -3)
</p>
<p>13. independent but not stationary
</p>
<p>16. p,x[n] = 0, cx[nl, n2] = 6[n2 - nIl, exactly the same as for WGN with ( 5 ~ = 1
</p>
<p>18. P[X[n] &gt; 3] = 0.000011, P[U[n] &gt; 3] = 0.0013
</p>
<p>22. E[X(t)] = 0, CX(tI, t2) = cos(21rtI) cOS(21rt2)
</p>
<p>24. E[Y[n]] = 0, cov(Y[Ol,Y[l]) = -1, not lID since samples are not independent
</p>
<p>26. E[X[n]] = 0, cx[nl,n2] = (5~min(nl,n2)
</p>
<p>27. cx[l ,l] = 1/2, cx[l ,2] = 1/4, cx[l, 3] = 0, ex[l, 1] = 0.5057, ex[l, 2] = 0.2595,
ex [1,3] = -0.0016 (Nreal = 10,000)
</p>
<p>31. p,x[n] = 0, cx[nl ,n2] = (5~(5~6[n2 - nIl, white noise
</p>
<p>34. N = 209
</p>
<p>Chapter 17
</p>
<p>1. yes, p,x[n] = P, = 2p - 1, rx[k] = 1 for k = 0 and rx[k] = p,2 for k =/:- 0
</p>
<p>5. WSS but not stationary since PX[O] =/:- PX[I]
</p>
<p>9. a &gt; 0, Ibl ::; 1
</p>
<p>12. b,d,e
</p>
<p>17. E[X[n]] = 0, var(X[n]) = (5~(1 - a2(n+l ))/ (1 - a2); as n -+ 00 , var(X[n]) -+
(5;/(1- a2 )
</p>
<p>19. Principal minors are 1, 15/64, and -17/32 for 1 X 1, 2 X 2 and 3 X 3, respectively.
</p>
<p>Fourier transform is 1 - (7/4) cos(21r1) which can be negative.</p>
<p/>
</div>
<div class="page"><p/>
<p>820 APPENDIX E. ANSWERS TO SELECTED PROBLEMS
</p>
<p>20. J.Lx[n] = J.L , rx[k] = (1/2)ru[k] + (1/4)ru[k - 1] + (1/4)ru[k + 1]
</p>
<p>28. Px(J) = 2o-b(1- cos(2n}))
</p>
<p>30. Px(J) = o-~o-b
</p>
<p>34. rx[k] ~ o-b8[k] + J.L2 , Px(J) = o-b + J.L28(J)
</p>
<p>38. rx[k] = 9/4,3/2,1/2 for k = 0,1,2, resp ectively, and zero otherwise
</p>
<p>40. a:2: Ibl
</p>
<p>42. E1X[n]] = 0, E[X[10]] = -Q.0105, E[X[12]] = 0.0177; rx[2] = 0.1545,
E[X[10]X[12]] = 0.1501, E[X[12]X[14]] = 0.1533 (Nreal = 1000)
</p>
<p>44. Px(J) = 1, increasing N does not improve estimate - must average over en-
semble of periodograms
</p>
<p>47. 2(exp(-10) - exp( -100))
</p>
<p>50. J.Lx(t) = 0, var(X(t)) = No/(2T) , no
</p>
<p>51. var(fJ,N) = (I/N) L:f';!(N-i)(1 - Ikl/N)NoW sin(7rk/2)/(7rk/2) for N = 20.
It is 0.9841 times that of the variance of the sample mean for Nyquist rate
</p>
<p>sampled data.
</p>
<p>Chapter 18
</p>
<p>1. rx[k] = 3 for k = 0, rx[k] = -1 for k = &plusmn;2, and equals zero otherwise; Px(J) =
3 - 2 cos(47rJ)
</p>
<p>4. bi = 0, bz = -1
</p>
<p>7. rx[k] = 3 for k = 0, rx[k] = -2 for k = &plusmn;1, rx[k] = 1/2 for k = &plusmn;2, and equals
zero otherwise; Px(J) = 3 - 4 cos(27rJ) + cos(47rJ)
</p>
<p>13. H opt = (2 - 2cos(27rJ))/(3 - 2cos(27rJ)); msemin = 0.5552
</p>
<p>18. msemin = rx[O](1 - Pi-[no],x[no+l])
</p>
<p>22. X[no + 1] = -[b(1 + b2 )/ (1 + b2 + b4 )]X [no] - [b2 / (1 + b2 + b4 )]X [no - 1];
msemin = 1 + b6/(1 + b2 + b4 )
</p>
<p>24. msemin = 1 + [b6/(1 + b2 + b4 ) ] = 85/84 = 1.0119, mASemin = 1.0117 (Nreal =
10,000)
</p>
<p>27. X[no] = [a/(1 + a2)](X[no + 1] + X[no - 1])
</p>
<p>29. Px(F) = (NoT2/2)[(sin(7rFT))/(7rFT)j2</p>
<p/>
</div>
<div class="page"><p/>
<p>821
</p>
<p>32. rx(O) = No/(4RC), no
</p>
<p>Chapter 19
</p>
<p>1. E[X[n]Y[n + k]] = (-1)n+k(1~8[k], no
</p>
<p>5. rx,y[k] = 0
</p>
<p>6. rX,u[k] = 0 for k &gt; 0 and rx,y[k] = (1/2)(-k) for k &lt; 0
</p>
<p>10. IPx,y(J)! = )5 + 4 cos(27rJ), LPx,y(J) = arctan 1 ~ ; S ~ ~ ; 2 ; ! f
</p>
<p>12. rz[k] = rx[k] -rx,y[k] -rY,x[k] +ry[k], Pz(J) = Px(J) -PX,y(J) -Py,x(J) +
Py(J)
</p>
<p>15. rX,y(J) = -1, perfectly predictable using Y[no] = -X[no]
</p>
<p>18. Hopt(J) = PX,y(J)/Px(J)
</p>
<p>23. rX,y(T) = N o/ (2T ) for 0 ~ T ~ T and zero otherwise
</p>
<p>26. rx,y[k] = 8[k] - M[k - 1], for b = -1
</p>
<p>k rx,y[k] rx,y[k] k rx,y[k] rx,y[k]
</p>
<p>-5 0 -0.0077 0 1 0.9034
</p>
<p>-4 0 -0.0242 1 1 0.9031
</p>
<p>-3 0 0.0259 2 0 -0.0064
</p>
<p>-2 0 0.0004 3 0 -0.0007
</p>
<p>-1 0 -0.0062 4 0 0.0267
</p>
<p>5 0 -0.0238
</p>
<p>Chapter 20
</p>
<p>2. 1/4
</p>
<p>5. Y = [Y[O] Y[l]]T '" N(o, Cy), where
</p>
<p>Cy = [~1 -;1]
not independent
</p>
<p>10. WSS with t-z = tixuv , Pz(J) = Px(J) *Py(J)
</p>
<p>14. 1</p>
<p/>
</div>
<div class="page"><p/>
<p>822 APPENDIX E. ANSWERS TO SELECTED PROBLEMS
</p>
<p>17. T = 66,347
</p>
<p>19. 2Q(1/vIT)
</p>
<p>22. Y = [Y(O) Y(1/4)jT '" N(O, Cy), where
</p>
<p>Cy = [~ ~]
</p>
<p>25. Pu(F) = Pv(F) = 8(1 - IFI/1O) for IFI ::; 10 and zero otherwise
</p>
<p>30. X[n] = Urn] - Urn - 1], where Urn] is WGN with ( Y ~ = 1
</p>
<p>31. rx[O] = 2, rx[O] = 1.9591; rx[l] = -1, rx[l] = -0.9614; rx[2] = 0, rx[2] =
-0.0195; rx[3] = 0, rx[3] = -0.0154
</p>
<p>Chapter 21
</p>
<p>3. probability = 0.1462, average = 5
</p>
<p>7. A = 2,5. = 1.9629; A = 5,5. = 4.9072 (based on 10,000 arrivals with 5. =
E[N(t)]/t)
</p>
<p>10. E[N(t2) - N(td] = var(N(t2) - N(tl)) = A(t2 - tl),
</p>
<p>13. 0.6321
</p>
<p>17. 10 minutes
</p>
<p>20. P[T2::; 1] = 1- 2exp(-1) = 0.2642, P[T2::; 1] = 0.2622 (Nreal = 10,000)
</p>
<p>23. Ato(2p - 1)
</p>
<p>Chapter 22
</p>
<p>2. 1/128
</p>
<p>5. P[Y[2] = 1IY[1] = 1, Y[O] = 0] = 1 - p , P[Y[2] = 1IY[1] = 1] = 1/2 for all p
</p>
<p>9. r; = 0.3362
</p>
<p>11. P[red drawn] = 1/3
</p>
<p>12. 1/2
</p>
<p>14. yes, 7("T = [i ~ 0 0]
19. 7("T = [0.2165 0.4021 0.3814]
</p>
<p>24. P[rain] = 0.6964
</p>
<p>26. n = 6
</p>
<p>28. 1rl = 1/3, ih = 0.3240 (based on playing 1000 holes)</p>
<p/>
</div>
<div class="page"><p/>
<p>Index
</p>
<p>Abbreviations , 782
ACF (see Autocorrelation function)
</p>
<p>ACS (see Autocorrelation sequence)
</p>
<p>Affine transformation, 313, 400
AR (see Autoregressive)
ARMA (see Autoregressive moving average)
</p>
<p>Arrival angle measurement , 671
</p>
<p>Arrival rate of Poisson pro cess, 713
Arri val times of Poi sson process, 721, 734
</p>
<p>Aut ocorrelation functi on:
definition, 581
</p>
<p>propert ies, 581, 624
Autocorrelation matrix , 561
Autocorrelation method of LP C, 628
</p>
<p>Autocorrelat ion sequence:
definition, 552
</p>
<p>properties, 553
est imator , 577, 627
</p>
<p>LSI system output , 602, 604
</p>
<p>MATLAB code for est imat ion, 578
for deterministic signal , 798
</p>
<p>Autoregressive:
</p>
<p>definition, 558, 633, 681
autocorre lat ion sequence, 560
power spectral density, 572
</p>
<p>linear prediction, 618
generat ion of realization, 592, 595
</p>
<p>for mod eling of PSD , 588,628
</p>
<p>cross-corre lat ion of filter
</p>
<p>input/output , 668
Autoregressive moving average , 681
Auxiliary random variab le, 183, 251, 403
</p>
<p>Average d periodogram:
definition, 579
MATLAB code for , 580
</p>
<p>Averages:
</p>
<p>ensemble, 563
</p>
<p>temporal, 563
</p>
<p>Average power , 553, 575, 582
</p>
<p>Average value (see Expected value)
</p>
<p>Bandpass random process:
definition, 691
representat ion, 692
"white" Gaussian noise , 694
</p>
<p>Bayes' theorem, 86
</p>
<p>Bayes' theorem for condit ional PDF, 223
</p>
<p>Bernoulli law of large numbers, 490
Bernoulli tri al:
</p>
<p>independent , 90
dependent , 94
Markov chain, 739
</p>
<p>Bernoulli probability law (mass function), 111
Bernoulli random pro cess, 517
Bernoulli sequence, 90
</p>
<p>Binomial coefficient (see Combinati ons)
</p>
<p>Binomial counting random pro cess, 526
</p>
<p>Binomial prob ability law (mass function):
</p>
<p>definition , 4, 63, 91, 112
</p>
<p>maximum value, 129
approximation by Poisson, 113, 152, 712
approximation by Gaussian, 501
</p>
<p>Binomial theorem, 11,68, 71
Bins , 21
</p>
<p>Birthday problem, 57
Bivariate Gaussian PDF :
</p>
<p>definition, 401, 408
</p>
<p>standardized version , 385
</p>
<p>Bivariate PMF (see Joint PMF)
</p>
<p>Body mass index, 202
Boole's inequality, 52
</p>
<p>Box-Mueller t ransform, 431
Brownian motion (see Wien er
</p>
<p>random process)
</p>
<p>Cartesian product of set s, 89
</p>
<p>Cauchy PDF, 299,402
</p>
<p>Cauchy-Schwarz inequality, 197, 213, 645</p>
<p/>
</div>
<div class="page"><p/>
<p>824
</p>
<p>Causal signal, 796
</p>
<p>Causal syst em, 605, 801, 807
CCF (see Cross-correlation function)
</p>
<p>CCS (see Cross-correlation sequence)
CDF (see Cumulative distribution function)
</p>
<p>Central limit theorem:
description, 495, 497, 501
</p>
<p>proof,513
using to compute binomial
</p>
<p>probabilities, 501
</p>
<p>using to compute chi-squared
</p>
<p>probabilities, 497
used as justification for modeling, 674
</p>
<p>Central moments , 146, 190
</p>
<p>Certain set , 44
Chain rule (probability) , 85, 266
Chain rule for Markov chains, 745
Channel delay measurement , 658
Chapman-Kolmogorov equations, 748
</p>
<p>Characteristic function:
</p>
<p>scalar definition, 147, 359
joint definition, 198, 265, 414, 467
</p>
<p>properties, 151
finding PMF from, 185
finding PDF from, 361
finding moments from, 149, 199, 266, 468
convergence of sequence, 152, 361
</p>
<p>table of, 145, 355
for multivariate Gaussian , 481
</p>
<p>Chebyshev inequali ty, 362
</p>
<p>Chi-squared PDF, 302, 429, 480, 498, 509
</p>
<p>Cholesky decomposition, 417, 475
</p>
<p>Clut ter , 674
Coherence function , 649
</p>
<p>Combinations, 60
</p>
<p>Combinatorics, 54
Communications:
</p>
<p>phase shift keying, 24
</p>
<p>channel modeling, 104
</p>
<p>model of link, 81
</p>
<p>error probability, 308
</p>
<p>Compl ement set , 39
</p>
<p>Compound Poisson random process:
</p>
<p>definition, 723
</p>
<p>characterist ic function , 724
mean function, 726
variance function , 735
</p>
<p>Computer data generat ion:
</p>
<p>INDEX
</p>
<p>discret e random variables, 122
</p>
<p>discrete random vectors, 200
conditioning approach, 235, 447
</p>
<p>given covariance matrix, 283
</p>
<p>AR random process, 592, 595, 633
</p>
<p>Cauchy, 368
</p>
<p>exponential, 366
Gaussian , 18, 27, 33, 431
bivariate Gaussian , 416, 448
</p>
<p>multivariate Gaussian , 475
</p>
<p>Gaussian from sum of uniforms , 481
</p>
<p>Uniform , 26, 33
</p>
<p>Laplacian, 326
WSS with given PSD , 696
</p>
<p>cont inuous-t ime Wiener
random process, 704
</p>
<p>Poisson random process, 727
</p>
<p>Markov chain, 764
Conditional expe ctation:
</p>
<p>definition, 229, 446
</p>
<p>properties, 233, 447
</p>
<p>Conditional mean
</p>
<p>(see Condi tional expectation)
Condi tional probabili ty, 75
Conditional PDF:
</p>
<p>definition, 437
properties, 440
of bivariate Gaussian , 439
</p>
<p>Conditional PMF:
</p>
<p>definition , 218, 266
</p>
<p>properti es, 222
</p>
<p>and independence, 224
and Markov property, 745
</p>
<p>Confidence interval , 504
Continuity theorem for characteristic
</p>
<p>functions, 152, 361
</p>
<p>Contours of constant PDF, 382, 386
Convergence in probability, 491
</p>
<p>Convergence of random variables, 507
</p>
<p>Convolution:
</p>
<p>integral, 396, 493, 624, 685, 806
</p>
<p>sum , 200, 599, 798
</p>
<p>to compute PMF, 184
to compute PDF, 493
</p>
<p>MATLAB program to compute, 511
</p>
<p>Correlation:
</p>
<p>definition, 196
and causality, 197</p>
<p/>
</div>
<div class="page"><p/>
<p>INDEX
</p>
<p>Correlation coefficient:
</p>
<p>definition, 196
for prediction, 196
</p>
<p>estimation of, 210
</p>
<p>for standard bivariate Gaussian, 407
for random processes, 555, 649
</p>
<p>Correlation of signals, 798
Correlation time, 695
Countable set , 43
</p>
<p>Countably infinite set, 43
Counting methods, 37, 54
</p>
<p>Covariance:
</p>
<p>definition, 188
</p>
<p>properties, 208
</p>
<p>of independent random variables, 191
</p>
<p>est imat ion of, 211, 545
for bivariate Gaussian PDF, 406
</p>
<p>Covariance function , 533
Covariance matrix:
</p>
<p>definition, 258, 281
</p>
<p>properties, 258, 561
</p>
<p>for uncorrelated random
variables, 259, 465
</p>
<p>diagonalization, 260
eigenanalysis, 261, 431
</p>
<p>est imat ion of, 270
for Gaussian PDF, 408, 459
</p>
<p>Covariance sequence, 533
CPSD (see Cross-power spectral density)
Cross-correlation function:
</p>
<p>definition, 657
</p>
<p>properties, 657
</p>
<p>Cross-correlation sequence:
</p>
<p>definition, 643
</p>
<p>properties, 645
</p>
<p>estimation of, 662
</p>
<p>MATLAB program for
estimation, 662
</p>
<p>for deterministic signals, 798
Cross-power spectral density:
</p>
<p>discrete-time definition, 647, 648
</p>
<p>continuous-time definition, 657
</p>
<p>properties, 650
</p>
<p>at input/output of filter , 654
</p>
<p>Cross-spectral matrix, 669
</p>
<p>CTCV (see Continuous-time/continuous-
valued under random process)
</p>
<p>CTDV (see Continous-time/ discrete-
</p>
<p>825
</p>
<p>valued under random process)
</p>
<p>Cumulative distribution function :
</p>
<p>discrete random variable, 118, 250
</p>
<p>continuous random variable, 303, 389, 463
</p>
<p>condit ional, 441, 449
mixed random variable, 319
</p>
<p>D/ A (see Digital-to-analog)
Data generation (see Computer
</p>
<p>data generation)
</p>
<p>Data compression via Markov chain , 767
Data compression via source encoding, 155
</p>
<p>dB (see Decibel)
</p>
<p>DC (see Direct current)
Decibel , 629
</p>
<p>Decorrelation of
</p>
<p>random variables, 208, 260, 410
Demodulation, 705
DeMoivre-Laplace theorem, 501
De Morgan 's laws, 42
Dependent sub experiments, 94
</p>
<p>DFT (see Discrete Fourier transform)
Differencer, 552, 681
</p>
<p>Digital-to-analog convertor, 629
</p>
<p>Dirac delta function, 319, 336, 787, 804
</p>
<p>Direct current (DC level) signal, 477, 566
Discrete Fourier transform:
</p>
<p>definition, 799
</p>
<p>use in spectral analysis , 580
to approximate inverse
</p>
<p>Fourier transform, 616, 799
Disjoint sets , 41, 44, 67
</p>
<p>Dow-Jones industrial average, 517
</p>
<p>DTCV(see Discrete-time/continuous-
</p>
<p>valued under random process)
</p>
<p>DTDV(see Discrete-time/discrete-
</p>
<p>valued under random process)
</p>
<p>Dyad (see Outer product)
</p>
<p>Eigenanalysis (eigendecomposition):
</p>
<p>definition, 261, 793
</p>
<p>used to find powers of matrix, 750
</p>
<p>Eigenvalue/eigenvector, 793
</p>
<p>Empty set , 38
</p>
<p>Ensemble average for Markov chain, 563
</p>
<p>Entropy, 157
</p>
<p>(see also Real-world example-
</p>
<p>data compression)
</p>
<p>Envelope of random process, 693, 705</p>
<p/>
</div>
<div class="page"><p/>
<p>826
</p>
<p>Ergodic:
</p>
<p>definition , 564
mean , 564
</p>
<p>autocorrelat ion, 577
</p>
<p>Markov chain, 752, 761
Erlang PDF, 302, 721
</p>
<p>Estimation:
autocorre lat ion sequence, 577
autoregressive par ameters, 623
</p>
<p>CDF ,123
correlat ion coefficient, 210
</p>
<p>covariance matrix, 270
cross-correlat ion sequence, 662
</p>
<p>interval , 20
</p>
<p>least squares, 538, 546
</p>
<p>mean , 21, 154,364
</p>
<p>moments, 154, 202
</p>
<p>PDF, 14, 19
PMF, 122, 202
power spectral density, 568, 579
</p>
<p>probabili ty of heads, 7, 19
probability of error , 27
variance , 154, 364
</p>
<p>Even function , 334, 368, 787,805
</p>
<p>Even sequence, 554, 796
</p>
<p>Events :
definition, 44
</p>
<p>imp ossible, 44
joint , 76, 458
simple, 44
zero probability, 48, 53, 384
</p>
<p>Expected value:
scalar , 136, 345
</p>
<p>vector , 255, 459
</p>
<p>matrix, 260, 281
</p>
<p>center of mass analogy, 158, 346
</p>
<p>est imation of, 154, 364
nonexist ence of, 139, 159, 348
</p>
<p>properties, 159, 160
</p>
<p>of sum, 187,405,465
of product , 187,405
</p>
<p>for function of, 140, 187, 351, 405
</p>
<p>table of, 145, 355
of conditional PMF, 229
</p>
<p>of condit ional PDF, 446
</p>
<p>and condit ional expectation, 230
</p>
<p>using conditioning, 234, 447
</p>
<p>bounds on , 368
</p>
<p>INDEX
</p>
<p>from CDF , 370
Experiments:
</p>
<p>description, 1
</p>
<p>subexperiments, 89
</p>
<p>independence of subexperiments , 89
</p>
<p>dependence of subexperiments, 94
</p>
<p>Exponential function:
</p>
<p>as limit , 784
as infinite series, 785
</p>
<p>Exp onential PDF, 296, 302
Extrap olation (see Prediction)
</p>
<p>Factorial:
</p>
<p>definition, 4, 57
</p>
<p>computing, 72
</p>
<p>and Gamma function, 301
</p>
<p>Fast Fourier transform , 33, 579, 615
</p>
<p>FFT (see Fast Fouri er transform)
Filtering:
</p>
<p>all-pole, 627
</p>
<p>bandpass, 694
digital filter design , 696
</p>
<p>interference rejection , 624
</p>
<p>of jointly WSS random pro cesses, 653
</p>
<p>lowpass, 25, 659, 804, 808
</p>
<p>passband , 656, 804, 808
stopband, 804, 808
</p>
<p>Wiener , 613
Fini te dimensional distribution , 523
Finite impul se response filter , 600, 803
Finite set , 43
FIR (see Fini te impulse response filter)
Fish population measurement , 698
</p>
<p>Forecasting (see Prediction)
</p>
<p>Fourier transform:
</p>
<p>discrete-time Fourier transform:
</p>
<p>definition , 796, 798
table of, 797
</p>
<p>continuous-t ime Fourier transform:
</p>
<p>definition , 805
table of, 806
</p>
<p>as narrowband filter , 670
</p>
<p>Frequency response, 600, 802, 808
Functions:
</p>
<p>even, 334, 368, 787
</p>
<p>hermi tian, 650
</p>
<p>monotone, 337
</p>
<p>odd, 368, 787</p>
<p/>
</div>
<div class="page"><p/>
<p>INDEX
</p>
<p>Fundamental theorem of calculus, 310, 786
</p>
<p>Gamma function, 300
</p>
<p>Gamma PDF, 300
Gaussian PDF, 5, 20, 296, 459
Gaussian mixture PDF, 332, 371
</p>
<p>Gaussian random variable, 296
Gaussian random process:
</p>
<p>definition, 677
</p>
<p>linear filtering of, 682
(see also Bandpass random process)
</p>
<p>Geometric probability law
</p>
<p>(mass function) , 91, 113, 720
</p>
<p>Geometric series, 47, 785
</p>
<p>Histogram, 14
Homogeneous transition probabilities, 745
</p>
<p>Huffman code , 156
Hypergeometric probability law
</p>
<p>(mass function) , 63
</p>
<p>IID (see Independent and
</p>
<p>identically distributed)
IIR (see Infinite impulse response filter)
Image coding, 272
</p>
<p>Image signal pro cessing, 419
Importance sampling, 364
Impossible event , 44
</p>
<p>Impulse (discrete-time), 795
Impulses (for continuous-time
</p>
<p>see Dirac delta function)
</p>
<p>Impulse response, 599, 801, 807
Increment of random process:
</p>
<p>definition, 526
independent, 526
</p>
<p>stationary, 526
</p>
<p>for Poisson random process, 714
for Wiener random process, 679, 687
</p>
<p>Independent events, 78, 83
</p>
<p>Ind ependent increments, 526
</p>
<p>Independent and identically
</p>
<p>distributed, 466, 678
</p>
<p>Independent random variables:
</p>
<p>definition, 24, 179
</p>
<p>factorization of CDF, 392
</p>
<p>factorization of PDF, 392, 462
factorization of PMF, 179, 224
factorization of characteristic
</p>
<p>827
</p>
<p>function, 282, 468
transforming Gaussian random
</p>
<p>variables, 410
functions of, 199
</p>
<p>Independent sub experiments, 90
Indicator random variable, 353
</p>
<p>Induction, proof by, 783
Infinite impulse response filter , 803
Infinite set , 43
</p>
<p>Inn er product, 791, 798
</p>
<p>In-phase component, 693
</p>
<p>Integration by parts, 368, 787
</p>
<p>Integration using approximating sum, 12, 786
</p>
<p>Interarrival times of Poisson process, 718
Interference suppression, 624
</p>
<p>Interpolation, 611
Intersection of sets, 39
Inverse probability integral
</p>
<p>transformation, 324
</p>
<p>Inverse Q function , 308
</p>
<p>Irreducible Markov chain, 756
</p>
<p>Joint CDF:
</p>
<p>discrete-time random variables, 177
continuous-time random variables, 389
computing probability from, 391
</p>
<p>Joint moments, 189, 192, 266, 412
Joint probability, 76
</p>
<p>Jointly distributed random variables, 170,379
Jointly WSS, 642
</p>
<p>Joint PDF:
</p>
<p>definition, 381, 458
bivariate Gaussian, 385
</p>
<p>from joint CDF, 391
Joint PMF:
</p>
<p>definition, 171
</p>
<p>estimation of, 202
Joint sample space , 170
</p>
<p>Karhunen-Loeve transform, 273
</p>
<p>Laplacian PDF, 298
</p>
<p>Law of averages (see Law of large numbers)
</p>
<p>Law of large numbers, 491
</p>
<p>Least squares, 538, 546
</p>
<p>Leibnitz rule, 335, 787
Linear predictive coding, 628
</p>
<p>Linear prediction equat ions, 618</p>
<p/>
</div>
<div class="page"><p/>
<p>828
</p>
<p>(see also Prediction )
</p>
<p>Linear shift invari ant system:
</p>
<p>definition, 599, 800
effect on WSS random process, 602, 654
</p>
<p>describ ed by difference equation, 605, 802
</p>
<p>Linear systems (see Appendix D)
Linear t ime invariant system:
</p>
<p>definition , 623, 807
effect on WSS random process, 624
</p>
<p>Linear transformation:
of Gaussian random variable, 314
</p>
<p>joint PDF, 399
of Gaussian random vector , 399, 408, 464
</p>
<p>Line fitting, 538
Lowpass filter (see Filtering)
</p>
<p>Lowpass random process, 693
LPC (see Linear predicti ve coding)
</p>
<p>LSI (see Linear shift invari ant system)
LTI (see Linear time invar iant system)
</p>
<p>MA (see Moving average)
Magnitude response of linear
</p>
<p>system, 802, 808
Mappings:
</p>
<p>one-to-one, 108
</p>
<p>many-to-one, 108
</p>
<p>Marginal CDF, 392
Marginal characteristic function, 265
Marginal PDF, 387, 441, 461
Marginal P MF , 174,224,249
Marginal probability, 76
</p>
<p>Markov chain , 96, 742
</p>
<p>Markov property, 267, 735
</p>
<p>Markov random process, 715, 739
</p>
<p>Markov sequence , 95
</p>
<p>Markov state probability, 742
</p>
<p>MATLAB overview, 31
MATLAB programs:
</p>
<p>simulat ion of heads for four coin tosses, 7
</p>
<p>simulation of three random outcomes, 18
genera t ion of Gaussian noise, 18
</p>
<p>est imated PDF, 21
</p>
<p>est imated probability, 22
</p>
<p>est imated mean, 22
</p>
<p>est imated mean of squared Gaussian
</p>
<p>random variable, 23
</p>
<p>est imated probability of error in digital
</p>
<p>communication system, 27
</p>
<p>INDEX
</p>
<p>tutorial MATLAB program, 35
</p>
<p>simulat ion of birthday problem, 59
</p>
<p>generation of discrete random variable, 122
</p>
<p>general program for discrete
</p>
<p>random variable generation , 165
generation of multiple discrete
</p>
<p>random variabl es, 201
simulat ion of die experiment
</p>
<p>(dependent Bernoulli trials) , 232
</p>
<p>generation of discrete random vector
using condit ioning, 236
</p>
<p>decorrelation of random vector, 271
</p>
<p>simulation of repeated coin tossing
</p>
<p>experiment , 290
</p>
<p>est imat ion of PDF using histogram, 328
</p>
<p>calculation of Q function , 341
calculat ion of inverse Q function, 341
calculat ion of tail probability, 366
generati on of Gaussian random vector and
</p>
<p>estimation of mean
and covari ance, 418
</p>
<p>generation of Gaussian random vector
</p>
<p>using condit ioning, 448
</p>
<p>generation of Gaussian random vector
</p>
<p>using Cholesky decomp osition, 476
</p>
<p>demonstration of cent ral limit theorem, 511
</p>
<p>simulat ion of nonstationary random
processes, 524
</p>
<p>generat ion of MA random process, 529
scatter diagram for randomly
</p>
<p>phased sinusoid, 537
</p>
<p>line fitting of summer rainfall, 541
</p>
<p>generation of AR random process, 559
</p>
<p>estimation of ACS of AR
</p>
<p>random process, 578
</p>
<p>averaged periodogram
</p>
<p>spectral est imator, 580
Wiener smoother, 615
</p>
<p>AR and periodogram
spectra l est imators, 629
</p>
<p>cross-corre lation est imator, 662
</p>
<p>reverb eration spectral estimation, 706
</p>
<p>reverb eration simulation, 709
</p>
<p>Poisson random pro cess simulation, 727
</p>
<p>three-state Markov chain simulation, 764
</p>
<p>Sierpinski triangle, 766
</p>
<p>Matrix:
</p>
<p>autocorrelation, 561</p>
<p/>
</div>
<div class="page"><p/>
<p>INDEX
</p>
<p>cofactor, 790
conformable, 789
</p>
<p>determinant, 790, 792
diagonal, 791
</p>
<p>doubly stochastic, 764, 773
</p>
<p>eigenanalysis, 793
</p>
<p>inversion:
</p>
<p>definition, 790
formula, 792, 794
</p>
<p>irreducible, 756
minor , 790
modal,261
orthogonal, 261, 282, 791
</p>
<p>partitioned, 791, 792
</p>
<p>positive definite (semidefinite), 259, 280,
</p>
<p>790, 793
rotation, 264, 282, 430
</p>
<p>square, 790
stochastic, 756
symmetric, 790
Toeplitz, 623
transpose, 790, 792
</p>
<p>Mean of random variable (see
</p>
<p>Expected value)
</p>
<p>Mean function, 533, 581
</p>
<p>Mean recurrence time, 761
Mean sequence, 533
</p>
<p>Mean square error:
definition, 142, 193, 208, 471
estimator, 194
</p>
<p>(see also Prediction)
</p>
<p>Median, 333, 450
Memoryless property of Poisson process, 720
</p>
<p>Mixed random variables, 317, 354
</p>
<p>Mode, 369
Modulation property of Fourier
</p>
<p>transform, 797, 806
Moments:
</p>
<p>definition, 146, 355, 467
</p>
<p>exponential PDF, 357, 359
geometrical PMF, 149
multivariate Gaussian, 468, 684
</p>
<p>table of, 145, 355
central vs. noncentral , 146, 358
</p>
<p>from characteristic function, 149, 199
</p>
<p>existence of, 161
</p>
<p>Monotonically increasing function , 337
</p>
<p>Monte Carlo method, 13, 365
</p>
<p>829
</p>
<p>Monty Hall problem, 81
</p>
<p>Moving average :
definition, 528, 681
</p>
<p>general form, 544
</p>
<p>miscellaneous, 534, 557, 564, 606, 621,
</p>
<p>651, 678, 697
MSE (see Mean square error)
</p>
<p>Multinomial coefficient, 93, 103
Multinomial probability law
</p>
<p>(mass function), 93, 249
Multinomial theorem, 249, 278
</p>
<p>Multipath fading (see Rayleigh
fading sinusoid)
</p>
<p>Mutually exclusive events, 44, 84
</p>
<p>Narrowband representation (see Bandpass
</p>
<p>random process)
</p>
<p>n-step transition probability matrix, 748
Nonstationarity, 526, 689, 711, 746
Normal (see Gaussian)
</p>
<p>Notational conventions, 777
Null set (see Empty set)
Nyquist rate sampling, 584
</p>
<p>Odd function, 368, 787, 805
</p>
<p>Odd sequence, 796
</p>
<p>Odds ratio, 87, 97
Opinion polling, 503
Optical character recognition, 419
</p>
<p>Outcome, 38, 44, 518 (see also Realization)
Outer product, 792
Orthogonality, 791
Orthogonality principle:
</p>
<p>definition, 474
</p>
<p>for linear prediction, 474
</p>
<p>geometric interpretation, 474, 482
</p>
<p>Parseval's theorem, 797, 806
</p>
<p>Partitioning of set, 41
</p>
<p>Pascal triangle, 12
</p>
<p>Pattern recognition, 421
PDF (see Probability density function)
</p>
<p>Periodic random process, 591
Periodogram, 568
</p>
<p>(see also Averaged periodogram)
</p>
<p>Permutations, 57
</p>
<p>Phase of random process, 569
</p>
<p>Phase response of linear system, 802, 808</p>
<p/>
</div>
<div class="page"><p/>
<p>830
</p>
<p>Phase shift keying, 24
P MF (see Probability mass function)
Poisson count ing random process, 126, 711
Poisson probability law (mass function), 113
</p>
<p>Poisson random pro cess, 711
</p>
<p>Poles of linear system, 803
</p>
<p>Positive semidefinite sequence, 562
</p>
<p>Posterior P MF , 239
Posterior probability, 87
</p>
<p>Power (average):
of random variable, 553
</p>
<p>from PSD , 575
Power spect ral density :
</p>
<p>cont inuous-t ime definition , 582
</p>
<p>discrete-time definition, 569, 571
</p>
<p>properties, 573
AR random process, 572
</p>
<p>used for physical modeling, 587, 628
</p>
<p>est imation of, 568, 579
MATLAB code for est imation, 580
</p>
<p>at out put of LSI system, 602
at out put of nonlinear system, 685
one-sided version, 576
physical interpret at ion , 608
of t hermal noise, 583
</p>
<p>P rediction:
definition , 142, 471
</p>
<p>linear , 192, 413, 471
</p>
<p>mean square error, 142, 208, 276
nonlinear , 195, 245, 447, 454
for bivari ate Gaussian , 413, 447
</p>
<p>of sinusoid, 544, 634
of WSS random process, 618, 622
L-step for WSS random process, 611
</p>
<p>for AR random process, 618, 634
</p>
<p>for MA random process, 621
</p>
<p>(see also Line fitting)
</p>
<p>Prior PMF, 238
Prior probabili ty, 80, 87
</p>
<p>Probability:
</p>
<p>definition , 44
</p>
<p>axioms, 45
</p>
<p>of poin t , 53, 294
of un ion , 49
</p>
<p>monotonicity, 50
</p>
<p>of interval , 121, 309
</p>
<p>zero probability events, 384
</p>
<p>Probabili ty calculations via
</p>
<p>INDEX
</p>
<p>condit ioning, 444
Probabili ty density function:
</p>
<p>definit ion, 6, 20, 289, 381, 458
properties, 293, 458
</p>
<p>Cauchy:
</p>
<p>definition, 299
</p>
<p>from ratio of Gaussians, 402
</p>
<p>chi-squared, 302
</p>
<p>Erl ang ,302
exponent ial, 293, 296, 302
</p>
<p>gamma, 300
Gaussian :
</p>
<p>scalar, 296
bivariate, 401
</p>
<p>multivariate, 459
</p>
<p>Gaussian mixture, 332, 371
Lapl acian, 298
</p>
<p>normal (see Gaus sian)
</p>
<p>Rayleigh:
</p>
<p>definition , 302
from square root of Gaussians, 403, 690
</p>
<p>table of, 355
uniform:
</p>
<p>definition, 290, 295
from angle of Gaussians, 403, 690
</p>
<p>condit ional, 437
est imation of, 14, 19
</p>
<p>from CDF, 310
</p>
<p>mass analogy, 292
of mixed random variables, 317, 354
</p>
<p>approximating by PMF, 288
Probabili ty of error, 26, 80, 308, 446
Probability function, 44
Probability integral t ransformation, 324
Probability mass function:
</p>
<p>definition, 109, 171
</p>
<p>Bernoulli, 111
Binomi al , 112
</p>
<p>Geometric, 113
Poisson , 113
</p>
<p>table of, 145
</p>
<p>mass analogy, 130
</p>
<p>est imation of, 122, 202
using Dirac delta function , 324
</p>
<p>Probabili ty of point , 53
</p>
<p>Problem designations, 8
</p>
<p>Projection theorem, 474
</p>
<p>PSD (see Power spectral density)</p>
<p/>
</div>
<div class="page"><p/>
<p>INDEX
</p>
<p>Pseudorandom noise, 629
PSK (see Phase shift keying)
</p>
<p>Q function:
definition, 306
approx imation of, 308
evaluation of, 341
</p>
<p>Quadrati c form, 280, 790
</p>
<p>Quadrature component, 693
</p>
<p>Random numb er generator, 7
</p>
<p>Random process:
</p>
<p>definition , 518
Bernoulli , 519
</p>
<p>binomial counting, 526
Gaussian, 677
Markov chain, 744
</p>
<p>Poisson , 711
sum, 525, 702
</p>
<p>infinite, 520
</p>
<p>semi-infinite , 520
stationary, 524, 551
</p>
<p>nonst at ionary, 526
discrete-t ime/ discrete-valued, 520
discrete-time/ conti nuous-valued , 520
continuous-time/ discrete-valued, 520
continuous-time/ cont inuous-valued , 520
</p>
<p>Random variable:
</p>
<p>discrete, 17, 107, 169
cont inuous , 287, 377
</p>
<p>mixed, 317
</p>
<p>Random vectors:
definit ion, 23, 170, 248, 458
Gaussian , 459
</p>
<p>Random walk, 267, 522
Rayleigh fading sinusoid, 403, 689
</p>
<p>Rayleigh PDF, 302,403
</p>
<p>Realization, 18, 518
</p>
<p>Real-world examples:
</p>
<p>digital communications, 24
</p>
<p>quality cont rol, 64
</p>
<p>cluster recognition , 97
</p>
<p>servicing customers, 124
data compression , 155
</p>
<p>assessing health risks, 202
model ing human learning, 237
</p>
<p>image coding, 272
</p>
<p>setting clipping levels, 328
</p>
<p>831
</p>
<p>crit ical software tes t ing, 364
</p>
<p>optical character recognition, 419
ret irement planning, 449
</p>
<p>signal detection , 476
</p>
<p>opinion polling, 503
statistical data analysis, 538
</p>
<p>random vibration test ing, 586
</p>
<p>speech synt hesis, 626
</p>
<p>brain physiology research, 663
</p>
<p>estimating fish populations, 698
</p>
<p>automobile traffic signal planning, 728
st range Markov chain dynamics, 765
</p>
<p>Relative frequency, 1, 7, 19, 759
(see also Law of large numbers)
</p>
<p>Replica corre lator, 477
Reproducting property, 184, 279, 414
</p>
<p>Reverberation, 674
</p>
<p>Sampl e mean:
</p>
<p>definit ion, 21, 154, 279, 466
</p>
<p>in detection, 478
law of large numbers, 490
(see also Ergodic)
</p>
<p>Sample space :
general, 44, 518
discrete, 47, 54
cont inuous, 52
</p>
<p>reduced, 75
</p>
<p>numerical vs. nonnumerical , 106
multiple random variables, 248, 458
</p>
<p>Sampling with (without replacement) , 56
</p>
<p>Scat ter diagram , 24, 537
Sets, 38
Sierp inski t riangle, 766
Signals (see Appendix D)
</p>
<p>Signal detection, 476
</p>
<p>Signal-t o-noise ratio, 369
Simple set, 39
</p>
<p>Sinusoidal signal:
</p>
<p>cont inuous-t ime definition, 805
</p>
<p>discrete-time definition, 795
</p>
<p>as random process:
</p>
<p>ran dom phase, 370, 530, 536, 591, 596
random amplitude, 544
</p>
<p>random amplitude/ phase, 403
(see also Rayleigh fading)
</p>
<p>PDF of random phase, 530
</p>
<p>recursive generation of, 544</p>
<p/>
</div>
<div class="page"><p/>
<p>832
</p>
<p>prediction of, 544
</p>
<p>miscellaneous, 557
</p>
<p>Size of set, 43
Smoothing, Wiener:
</p>
<p>definition, 611
MATLAB code for, 615
</p>
<p>SNR (see Signal-to-noise ratio)
</p>
<p>Source encoding, 155
Spectral estimation, 568, 579
</p>
<p>Spectral factorization, 696
Spectral representation theorem, 569
</p>
<p>Speech synthesis, 626
</p>
<p>Stable linear system, 801, 807
Stable sequence, 796
</p>
<p>Standard bivariate Gaussian, 385, 394
</p>
<p>Standard deviation, 356, 371
Standardized random variable, 195
Standard normal random variable, 296, 305
Standardized sum, 495
</p>
<p>State, 742
</p>
<p>State occupation time, 759
</p>
<p>State probabilities, 742
</p>
<p>State probability diagram, 95, 742
State transition matrix, 742
</p>
<p>Stationarity:
definition, 524
relation to lID, 524
</p>
<p>for WSS random process, 680
Stationary increments, 526
Stationary probabilities for Markov chain,
</p>
<p>definition, 749, 757
</p>
<p>computing, 750, 762, 771
Steady-state probabilities (see Stationary
</p>
<p>probabilities for Markov chain)
</p>
<p>Step function, 805
Step sequence, 795
</p>
<p>Stirling's formula, 71
Sub experiments, 89
</p>
<p>Symbols, 777
</p>
<p>System function, 600, 604, 681, 801
</p>
<p>Sum of Poisson random processes, 733
</p>
<p>Sum of random number of
</p>
<p>random variables , 234, 723
</p>
<p>Sum of random processes, 653
</p>
<p>Sum of random variables:
</p>
<p>finding PMF, 254
</p>
<p>finding PDF, 397, 470
</p>
<p>binomial from Bernoulli, 254
</p>
<p>INDEX
</p>
<p>finding variance, 257
</p>
<p>of uniform random variables, 395
</p>
<p>of Gaussian random variables, 414
</p>
<p>of Poisson random variables, 279
</p>
<p>Tail probability, 305
(see also Importance sampling)
</p>
<p>Taylor expansion, 130, 785
</p>
<p>Temporal average, 563
Temporal average for Markov chain, 760
</p>
<p>Total probability:
</p>
<p>for probability of event, 79
</p>
<p>for PMFs, 229
for PDFs, 445
</p>
<p>Transforms (see Appendix D)
</p>
<p>Transform coding, 273
</p>
<p>Transformations:
of discrete random variable, 115
of continuous random
</p>
<p>variable, 22, 313, 316
</p>
<p>of multiple random
</p>
<p>variables, 181, 251, 400, 464
</p>
<p>sum of random variables, 184, 397
</p>
<p>general funcion of, 464
using CDF approach, 317
PDF of product, 429
PDF of quotient, 402, 429, 445
using conditioning approach, 225
nonlinear-Gaussian
</p>
<p>random processes, 684
</p>
<p>Trellis, 96
Tuples, 55
</p>
<p>Uncorrelated random variables:
definition, 196
</p>
<p>and independence, 462
</p>
<p>Uncorrelated WSS random processes, 647
Uncountable set, 43, 285
Union, 39
</p>
<p>Union bound (see Boole's inequality)
</p>
<p>Uniform PDF, 290, 295
</p>
<p>Uniform PMF, 145
</p>
<p>Unit step function, 320
</p>
<p>Unit step sequence, 183
</p>
<p>Universal set, 38
</p>
<p>Variance:
</p>
<p>definition, 143, 355</p>
<p/>
</div>
<div class="page"><p/>
<p>INDEX
</p>
<p>properties, 145
</p>
<p>table of, 145
</p>
<p>est imat ion of, 154, 364
</p>
<p>sequence, 533
</p>
<p>function , 533
</p>
<p>of sum, 188,257,465
</p>
<p>conditional, 230
</p>
<p>Venn diagram, 40
</p>
<p>Vibration analysis, 586
</p>
<p>Waiting times (see Arrival times
</p>
<p>of Poisson process)
</p>
<p>WGN (see White Gaussian noise)
</p>
<p>White Gaussian noise:
</p>
<p>discrete-time definition, 528
</p>
<p>continuous-time definition, 686
</p>
<p>obtaining discrete-time from
</p>
<p>continuous-time, 583
</p>
<p>bandpass version, 584, 694
</p>
<p>miscellaneous, 534, 677
</p>
<p>Whitening (preemphasis) , 661
</p>
<p>White noise , 556, 569, 571
</p>
<p>Wide sense stationary:
</p>
<p>definition, 550
</p>
<p>jointly distributed, 642
</p>
<p>generating realization of, 681
</p>
<p>Wiener filtering:
</p>
<p>definition, 609
</p>
<p>filtering, 609
</p>
<p>smoothing, 611
</p>
<p>prediction, 611
</p>
<p>interpolation, 611
</p>
<p>Wiener-Hopf equations, 623
</p>
<p>Wiener-Khinchine theorem, 571
</p>
<p>Wiener random process:
</p>
<p>discrete-time, 679
</p>
<p>continuous-time, 687, 703
</p>
<p>computer generation of realization, 704
</p>
<p>Wolfer sunspot data, 548
</p>
<p>WSS (see Wide sense stationary)
</p>
<p>Yule-Walker equations, 623
</p>
<p>z-t ransform, 800
</p>
<p>833</p>
<p/>
</div>
<ul>	<li>INTUITIVE PROBABILITY AND RANDOM PROCESSES USING MATLAB&reg;&#13;</li>
	<li>NOTE TO INSTRUCTORS</li>
	<li>Preface</li>
	<li>Contents</li>
	<li>Chapter 1: Introduction&#13;</li>
<ul>	<li>1.1 What Is Probability?</li>
	<li>1.2 Types of Probability Problems</li>
	<li>1.3 Probabilistic Modeling</li>
	<li>1.4 Analysis versus Computer Simulation</li>
	<li>1.5 Some Notes to the Reader</li>
	<li>References</li>
	<li>Problems</li>
</ul>
	<li>Chapter 2: Computer Simulation&#13;</li>
<ul>	<li>2.1 Introduction&#13;</li>
	<li>2.2 Summary&#13;</li>
	<li>2.3 Computer Simulation?&#13;</li>
	<li>References</li>
	<li>Problems</li>
</ul>
	<li>Chapter 3: Basic Probability&#13;</li>
<ul>	<li>3.1 Introduction</li>
	<li>3.2 Summary</li>
	<li>3.3 Review of Set Theory</li>
	<li>3.4 Assigning and Determining Probabilities</li>
	<li>3.5 Properties of the Probability Function</li>
	<li>3.6 Probabilities for Continuous Sample Spaces</li>
	<li>3.7 Probabilities for Finite Sample Spaces - Equally Likely Outcomes&#13;</li>
	<li>3.8 Combinatorics</li>
	<li>3.9 Binomial Probability Law</li>
	<li>3.10 Real-World Example Quality Control</li>
	<li>References</li>
	<li>Problems</li>
</ul>
	<li>Chapter 4: Conditional Probability&#13;</li>
<ul>	<li>4.1 Introduction</li>
	<li>4.2 Summary</li>
	<li>4.5 Bayes' Theorem</li>
	<li>4 .6 Multiple Experiments</li>
	<li>4.7 Real-World Example Cluster Recognition</li>
	<li>References</li>
	<li>Problems</li>
</ul>
	<li>Chapter 5: Discrete Random Variables&#13;</li>
<ul>	<li>5.1 Introduction</li>
	<li>5.2 Summary</li>
	<li>5.3 Definition of Discrete Random Variable</li>
	<li>5.4 Probability of Discrete Random Variables</li>
	<li>5.5 Important Probability Mass Functions</li>
<ul>	<li>5.5.1 Bernoulli</li>
	<li>5.5.2 Binomial</li>
	<li>5.5.3 Geometric</li>
	<li>5.5.4 Poisson</li>
</ul>
	<li>5.6 Approximation of Binomial PMF by Poisson PMF</li>
	<li>5.7 Transformation of Discrete Random Variables</li>
	<li>5.8 Cumulative Distribution Function</li>
	<li>5.9 Computer Simulation</li>
	<li>5.10 Real-World Example Servicing Customers</li>
	<li>References</li>
	<li>Problems</li>
</ul>
	<li>Chapter 6: Expected Values for Discrete Random Variables&#13;</li>
<ul>	<li>6.1 Introduction</li>
	<li>6.2 Summary</li>
	<li>6.4 Expected Values of Some Important Random Variables</li>
	<li>6.6 Variance and Moments of a Random Variable</li>
	<li>6.7 Characteristic Functions</li>
	<li>6.8 Estimating Means and Variances</li>
	<li>6.9 Real-World Example Data Compression</li>
	<li>References</li>
	<li>Problems</li>
</ul>
	<li>Appendix 6A: Derivation of E[g(X)] Formula&#13;</li>
	<li>Chapter 7: Multiple Discrete Random Variables&#13;</li>
<ul>	<li>7.1 Introduction</li>
	<li>7.2 Summary</li>
	<li>7.3 Jointly Distributed Random Variables</li>
	<li>7.4 Marginal PMFs and CDFs</li>
	<li>7.5 Independence of Multiple Random Variables</li>
	<li>7.6 Transformations of Multiple Random Variables</li>
	<li>7.7 Expected Values</li>
	<li>7.8 Joint Moments</li>
	<li>7.9 Prediction of a Random Variable Outcome</li>
	<li>7.10 Joint Characteristic Functions</li>
	<li>7.11 Computer Simulation of Random Vectors</li>
	<li>7.12 Real-World Example - Assessing Health Risks</li>
	<li>References</li>
	<li>Problems</li>
</ul>
	<li>Appendix 7A: Derivation of the Cauchy-Schwarz Inequality&#13;</li>
	<li>Chapter 8: Conditional Probability Mass Functions&#13;</li>
<ul>	<li>8.1 Introduction</li>
	<li>8.2 Summary</li>
	<li>References</li>
	<li>Problems</li>
</ul>
	<li>Chapter 9: Discrete N-Dimensional Random Variables&#13;</li>
<ul>	<li>9.1 Introduction</li>
	<li>9.2 Summary</li>
	<li>9.3 Random Vectors and Probability Mass Functions</li>
	<li>9.4 Transformations&#13;</li>
	<li>9.5 Expected Values</li>
	<li>9.6 Joint Moments and the Characteristic Function</li>
	<li>9.7 Conditional Probability Mass Functions</li>
	<li>9.8 Computer Simulat ion of Random Vectors</li>
	<li>9.9 Real-World Example Image Coding</li>
</ul>
	<li>Chapter 10: Continuous Random Variables&#13;</li>
<ul>	<li>10.1 Introduction</li>
	<li>10.2 Summary</li>
	<li>10.5 Important PDFs</li>
	<li>10.6 Cumulative Distribution Functions&#13;</li>
	<li>10.7 Transformat ions&#13;</li>
	<li>10. 8 Mixed R andom Variables</li>
	<li>10.9 Computer Simulation</li>
</ul>
	<li>Chapter 11: Expected Values for Continuous Random Variables&#13;</li>
<ul>	<li>11.1 Introduction</li>
	<li>11.2 Summary</li>
	<li>11.3 Determining the Expected Value</li>
	<li>11.4 Expected Values for Important PDFs</li>
	<li>11.5 Expected Value for a Function of a Random Variable</li>
	<li>11.6 Variance and Moments of a Continuous Random Variable</li>
	<li>11.7 Characteristic Functions</li>
	<li>11.9 Estimating the Mean and Variance</li>
	<li>11.10 Real-World Example Critical Software Testing Using Importance Sampling</li>
	<li>References</li>
	<li>Problems</li>
</ul>
	<li>Appendix 11A: Partial Proof of Expected Value of Function of Continuous Random Variable&#13;</li>
	<li>Chapter 12: Multiple Continuous RandomVariables&#13;</li>
<ul>	<li>12.1 Introduction</li>
	<li>12.2 Summary</li>
	<li>12.3 Jointly Distributed Random Variables</li>
	<li>12.4 Marginal PDFs and the Joint CDF</li>
	<li>12.5 Independence of Multiple Random Variables</li>
	<li>12.6 Transformations</li>
	<li>12.7 Expected Values</li>
</ul>
	<li>Chapter 13: Conditional Probability Density Functions &#13;</li>
<ul>	<li>13.1 Introduction</li>
	<li>13.2 Summary</li>
	<li>13.3 Conditional PDF</li>
	<li>13.4 J oint, Conditional, and Marginal PDFs</li>
	<li>13.5 Simplifying Probability Calculations Using Conditioning&#13;</li>
	<li>13.6 Mean of Conditional PDF</li>
	<li>13.7 Computer Simulation of Jointly Continuous Random Variables&#13;</li>
	<li>13.8 Real-World Example - Retirement Planning</li>
</ul>
	<li>Chapter 14: Continuous N-Dimensional Random Variables &#13;</li>
<ul>	<li>14.1 Introduction</li>
	<li>14.2 Summary</li>
	<li>14.3 Random Vectors and PDFs</li>
	<li>14.4 Transformations&#13;</li>
	<li>14.5 Expected Values</li>
	<li>14.6 Joint Moments and the Characteristic Function&#13;</li>
	<li>14.7 Conditional PDFs</li>
	<li>14.8 Prediction of a Random Variable Outcome</li>
	<li>14.9 Computer Simulation of Gaussian Random Vectors&#13;</li>
	<li>14.10 Real-World Example - Signal Detection</li>
	<li>References</li>
	<li>Problems</li>
</ul>
	<li>Chapter 15: Probability and Moment Approximations Using Limit Theorems&#13;</li>
<ul>	<li>15.1 Introduction</li>
	<li>15.2 Summary&#13;</li>
	<li>15.3 Convergence and Approximation of a Sum</li>
	<li>15 .4 Law of Large Numbers</li>
	<li>15.5 Central Limit Theorem</li>
	<li>15.6 Real-World Example - Opinion Polling</li>
	<li>References</li>
	<li>Problems</li>
</ul>
	<li>Appendix 15A: MATLAB Program to Compute Repeated Convolution of PDFs&#13;</li>
	<li>Appendix 15B: Proof of Central Limit Theorem&#13;</li>
	<li>Chapter 16: Basic Random Processes&#13;</li>
<ul>	<li>16.1 Introduction</li>
	<li>16.2 Summary</li>
	<li>16.3 What Is a Random Process?</li>
	<li>16.4 Types of Random Processes</li>
	<li>16.5 The Important Property of Stationarity</li>
	<li>16.6 Some More Examples</li>
	<li>16.7 Joint Moments</li>
	<li>16.8 Real-World Example - Statistical Data Analysis</li>
	<li>References</li>
	<li>Problems</li>
</ul>
	<li>Chapter 17: Wide Sense Stationary Random Processes &#13;</li>
<ul>	<li>17.1 Introduction</li>
	<li>17.2 Summary&#13;</li>
	<li>17.3 Definition of WSS Random Process</li>
	<li>17.4 Autocorrelation Sequence</li>
	<li>17.5 Ergodicity and Temporal Averages</li>
	<li>17.6 The Power Spectral Density</li>
	<li>17.7 Estimation of the ACS and PSD</li>
	<li>17.8 Continuous-Time WSS Random Processes</li>
	<li>17.9 Real-World Example - Random Vibration Testing</li>
	<li>References</li>
	<li>Problems</li>
</ul>
	<li>Chapter 18: Linear Systems and Wide Sense Stationary Random Processes &#13;</li>
<ul>	<li>18.1 Introduction</li>
	<li>18.2 Summary</li>
	<li>18.3 Random Process at Output of Linear System</li>
	<li>18.4 Interpretation of the PSD</li>
	<li>18.5 Wiener Filtering</li>
<ul>	<li>18.5.1 Wiener Smoothing</li>
	<li>18.5.2 Prediction</li>
</ul>
	<li>18.6 Continuous-Time Definitions and Formulas</li>
	<li>18.7 Real-World Example Speech Synthesis</li>
	<li>References</li>
	<li>Problems</li>
</ul>
	<li>Chapter 19: Multiple Wide Sense Stationary Random Processes&#13;</li>
<ul>	<li>19 .1 Introduction</li>
	<li>19.2 Summary</li>
	<li>19.3 Jointly Distributed WSS Random Processes</li>
	<li>19.4 The Cross-Power Spectral Density</li>
	<li>19.5 Transformations of Multiple Random Processes</li>
	<li>19.6 Continuous-Time Definitions and Formulas</li>
	<li>19.7 Cross-correlation Sequence Estimation</li>
	<li>19.8 Real-World Example Brain Physiology Research</li>
	<li>References</li>
	<li>Problems</li>
</ul>
	<li>Chapter 20: Gaussian Random Processes&#13;</li>
<ul>	<li>20.1 Introduction</li>
	<li>20.2 Summary</li>
	<li>20.3 Definition of the Gaussian Random Process</li>
	<li>20.4 Linear Transformations</li>
	<li>20.5 Nonlinear Transformations</li>
	<li>20.6 Continuous-Time Definitions and Formulas</li>
	<li>20.7 Special Continuous-Time Gaussian Random Processes</li>
<ul>	<li>20.7.1 Rayleigh Fading Sinusoid</li>
	<li>20.7.2 Bandpass Random Process</li>
</ul>
	<li>20.8 Computer Simulation</li>
	<li>20.9 Real-World Example - Estimating Fish Populations&#13;</li>
	<li>References</li>
	<li>Problems</li>
</ul>
	<li>Chapter 21: Poisson Random Processes&#13;</li>
<ul>	<li>21.1 Introduction</li>
	<li>21.2 Summary</li>
	<li>21.3 Derivation of Poisson Counting Random Process</li>
	<li>21.4 Interarrival Times</li>
	<li>21.5 Arrival Times</li>
	<li>21.6 Compound Poisson Random Process</li>
	<li>21.7 Computer Simulation</li>
	<li>21.8 Real-World Example - Automobile Traffic Signal Planning&#13;</li>
	<li>References</li>
	<li>Problems</li>
</ul>
	<li>Chapter 22: Markov Chains&#13;</li>
<ul>	<li>22.1 Introduction</li>
	<li>22.2 Summary</li>
	<li>22.3 Definitions</li>
	<li>22.4 Computation of State Probabilities</li>
	<li>22.5 Ergodic Markov Chains</li>
	<li>22.6 Further Steady-State Characteristics</li>
<ul>	<li>22.6.1 State Occupation Time</li>
	<li>22.6.2 Mean Recurrence Time</li>
</ul>
	<li>22.7 K-State Markov Chains</li>
	<li>22.8 Computer Simulation&#13;</li>
	<li>22.9 Real-World Example Strange Markov Chain Dynamics</li>
	<li>References</li>
	<li>Problems</li>
</ul>
	<li>Appendix 22A: Solving for the Stationary PMF&#13;</li>
	<li>Appendix A: Glossary of Symbols and Abbrevations&#13;</li>
<ul>	<li>Symbols</li>
	<li>Abbreviations</li>
</ul>
	<li>Appendix B: Assorted Math Facts and Formulas&#13;</li>
<ul>	<li>B.l Proof by Induction</li>
	<li>B.2 Trigonometry</li>
	<li>B.3 Limits</li>
	<li>B.4 Sums</li>
	<li>B.5 Calculus</li>
	<li>References</li>
</ul>
	<li>Appendix C: Linear and Matrix Algebra&#13;</li>
<ul>	<li>C.l Definitions</li>
	<li>C.2 Special Matrices</li>
	<li>C.3 Matrix Manipulation and Formulas</li>
	<li>C.4 Some Properties of Positive Definite (Semidefinite) Matrices&#13;</li>
	<li>C.5 Eigendecomposition of Matrices</li>
	<li>References</li>
</ul>
	<li>Appendix D: Summary of Signals, Linear Transforms, and Linear Systems&#13;</li>
<ul>	<li>D.1 Discrete-Time Signals</li>
	<li>D.2 Linear Transforms</li>
<ul>	<li>D.2.1 Discrete-Time Fourier Transforms</li>
	<li>D.2.2 Numerical Evaluation of Discrete-Time Fourier Transforms</li>
	<li>D.2.3 z-Tra n s fo rm s</li>
</ul>
	<li>D.3 Discrete-Time Linear Systems</li>
	<li>D.4 Continuous-Time Signals</li>
	<li>D.5 Linear Transforms</li>
<ul>	<li>D.5.1 Continuous-Time Fourier Transforms&#13;</li>
</ul>
	<li>D.6 Continuous-Time Linear Systems</li>
	<li>References</li>
</ul>
	<li>Appendix E: Answers to Selected Problems&#13;</li>
	<li>Index</li>
</ul>
</body></html>