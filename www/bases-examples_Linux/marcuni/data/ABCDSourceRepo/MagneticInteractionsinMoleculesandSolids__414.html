<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Untitled</title>
</head>
<body><div class="page"><p/>
</div>
<div class="page"><p/>
<p>Principles of Quantum 
Mechanics 
</p>
<p>SECOND EDITION </p>
<p/>
</div>
<div class="page"><p/>
</div>
<div class="page"><p/>
<p>Principles of Quantum 
Mechanics 
</p>
<p>SECOND EDITION 
</p>
<p>R. Shankar 
Yale University 
</p>
<p>New Haven, Connecticut 
</p>
<p>~Springer </p>
<p/>
</div>
<div class="page"><p/>
<p>Library of Congress Cataloging&ndash;in&ndash;Publication Data 
</p>
<p>Shankar, Ramamurti. 
Principles of quantum mechanics / R. Shankar. 2nd ed. 
</p>
<p>p. cm. 
Includes bibliographical references and index. 
ISBN 0-306-44790-8 
1. Quantum theory. I. Title. 
</p>
<p>QC174. 12.S52 1994 
530. 1&rsquo;2&ndash;dc20 94&ndash;26837 
 CIP 
</p>
<p>&copy; 1994, 1980 Springer Science+Business Media, LLC 
All rights reserved. This work may not be translated or copied in whole or in part without the written 
permission of the publisher (Springer Science+Business Media, LLC, 233 Spring Street, New York, 
NY 10013, USA), except for brief excerpts in connection with reviews or scholarly analysis. Use in 
connection with any form of information storage and retrieval, electronic adaptation, computer 
software, or by similar or dissimilar methodology now known or hereafter developed is forbidden. 
The use in this publication of trade names, trademarks, service marks, and similar terms, even if they 
are not identified as such, is not to be taken as an expression of opinion as to whether or not they are 
subject to proprietary rights. 
</p>
<p>Printed in the United States of America. 
</p>
<p>19 18                (corrected printing, 2008) 
</p>
<p>springer.com 
</p>
<p>DOI: 10.1007/978-1-4757-0576-8
</p>
<p>ISBN 978-1-4757-0576-8 (eBook)  ISBN 978-1-4757-0578-2 </p>
<p/>
</div>
<div class="page"><p/>
<p>To 
</p>
<p>My Parent~ 
</p>
<p>and to 
</p>
<p>Uma, Umesh, Ajeet, Meera, and Maya </p>
<p/>
</div>
<div class="page"><p/>
</div>
<div class="page"><p/>
<p>Preface to the Second Edition 
</p>
<p>Over the decade and a half since I wrote the first edition, nothing has altered my 
</p>
<p>belief in the soundness of the overall approach taken here. This is based on the 
</p>
<p>response of teachers, students, and my own occasional rereading of the book. I was 
</p>
<p>generally quite happy with the book, although there were portions where I felt I 
</p>
<p>could have done better and portions which bothered me by their absence. I welcome 
this opportunity to rectify all that. 
</p>
<p>Apart from small improvements scattered over the text, there are three major 
</p>
<p>changes. First, I have rewritten a big chunk of the mathematical introduction in 
</p>
<p>Chapter 1. Next, I have added a discussion of time-reversal in variance. I don't know 
</p>
<p>how it got left out the first time-1 wish I could go back and change it. The most 
</p>
<p>important change concerns the inclusion of Chaper 21, "Path Integrals: Part II." 
</p>
<p>The first edition already revealed my partiality for this subject by having a chapter 
</p>
<p>devoted to it, which was quite unusual in those days. In this one, I have cast off all 
</p>
<p>restraint and gone all out to discuss many kinds of path integrals and their uses. 
Whereas in Chapter 8 the path integral recipe was simply given, here I start by 
</p>
<p>deriving it. I derive the configuration space integral (the usual Feynman integral), 
phase space integral, and (oscillator) coherent state integral. I discuss two applica-
tions: the derivation and application of the Berry phase and a study of the lowest 
Landau level with an eye on the quantum H.all effect. The relevance of these topics 
</p>
<p>is unquestionable. This is followed by a section of imaginary time path integrals~ 
its description of tunneling, instantons, and symmetry breaking, and its relation to 
classical and quantum statistical mechanics. An introduction is given to the transfer 
matrix. Then I discuss spin coherent state path integrals and path integrals for 
</p>
<p>fermions. These were thought to be topics too advanced for a book like this, but I 
believe this is no longer true. These concepts are extensively used and it seemed a 
good idea to provide the students who had the wisdom to buy this book with a head 
start. 
</p>
<p>How are instructors to deal with this extra chapter given the time constraints? 
I suggest omitting some material from the earlier chapters. (No one I know, myself 
included, covers the whole book while teaching any fixed group of students.) A 
realistic option is for the instructor to teach part of Chapter 21 and assign the rest 
as reading material, as topics for take-home exams, term papers, etc. To ignore it, vii </p>
<p/>
</div>
<div class="page"><p/>
<p>I think, would be to lose a wonderful opportunity to expose the student to ideas that are 
</p>
<p>central to many current research topics and to deny them the attendant excitement. Since 
</p>
<p>the aim of this chapter is to guide students toward more frontline topics, it is more 
</p>
<p>concise than the rest of the book. Students are also expected to consult the references 
</p>
<p>given at the end of the chapter.  
</p>
<p>Over the years, I have received some very useful feedback and I thank all those 
</p>
<p>students and teachers who took the time to do so. I thank Howard Haber for a 
</p>
<p>discussion of the Born approximation; Harsh Mathur and Ady Stern for discussions 
</p>
<p>of the Berry phase; Alan Chodos, Steve Girvin, Ilya Gruzberg, Martin Gutzwiller, 
</p>
<p>Ganpathy Murthy, Charlie Sommerfeld, and Senthil Todari for many useful comments 
</p>
<p>on Chapter 21. I am most grateful to Captain Richard F. Malm, U.S.C.G. (Retired), 
</p>
<p>Professor Dr. D. Schl&uuml;ter of the University of Kiel, and Professor V. Yakovenko of the 
</p>
<p>University of Maryland for detecting numerous errors in the first printing and taking the 
</p>
<p>trouble to bring them to my attention. I thank Amelia McNamara of Plenum for urging 
</p>
<p>me to write this edition and Plenum for its years of friendly and warm cooperation.  
</p>
<p>I thank Ron Johnson, Editor at Springer for his tireless efforts on behalf of this book, 
</p>
<p>and Chris Bostock, Daniel Keren and Jimmy Snyder for their generous help in 
</p>
<p>correcting errors in the 14
th
 printing. Finally, I thank my wife Uma for shielding me as 
</p>
<p>usual from real life so I could work on this edition, and my battery of kids (revised and 
</p>
<p>expanded since the previous edition) for continually charging me up. 
</p>
<p> R. Shankar 
</p>
<p>New Haven, Connecticut 
</p>
<p>viii 
</p>
<p>PREFACE TO THE 
</p>
<p>SECOND EDITION </p>
<p/>
</div>
<div class="page"><p/>
<p>Preface to the First Edition 
Publish and perish-Giordano Bruno 
</p>
<p>Given the number of books that already exist on the subject of quantum mechanics, 
one would think that the public needs one more as much as it does, say, the latest 
version of the Table oflntegers. But this does not deter me (as it didn't my predeces-
sors) from trying to circulate my own version of how it ought to be taught. The 
approach to be presented here (to be described in a moment) was first tried on a 
group of Harvard undergraduates in the summer of '76, once again in the summer 
of '77, and more recently at Yale on undergraduates ('77-'78) and graduates ('78-
'79) taking a year-long course on the subject. In all cases the results were very 
satisfactory in the sense that the students seemed to have learned the subject well 
and to have enjoyed the presentation. It is, in fact, their enthusiastic response and 
encouragement that convinced me of the soundness of my approach and impelled 
me to write this book. 
</p>
<p>The basic idea is to develop the subject from its postulates, after addressing 
some indispensable preliminaries. Now, most people would agree that the best way 
to teach any subject that has reached the point of development where it can be 
reduced to a few postulates is to start with the latter, for it is this approach that 
gives students the fullest understanding of the foundations of the theory and how it 
is to be used. But they would also argue that whereas this is all right in the case of 
special relativity or mechanics, a typical student about to learn quantum mechanics 
seldom has any familiarity with the mathematical language in which the postulates 
are stated. I agree with these people that this problem is real, but I differ in my belief 
that it should and can be overcome. This book is an attempt at doing just this. 
</p>
<p>It begins with a rather lengthy chapter in which the relevant mathematics of 
vector spaces developed from simple ideas on vectors and matrices the student is 
assumed to know. The level of rigor is what I think is needed to make a practicing 
quantum mechanic out of the student. This chapter, which typically takes six to 
eight lecture hours, is filled with examples from physics to keep students from getting 
too fidgety while they wait for the "real physics." Since the math introduced has to 
be taught sooner or later, I prefer sooner to later, for this way the students, when 
they get to it, can give quantum theory their fullest attention without having to ix </p>
<p/>
</div>
<div class="page"><p/>
<p>X 
</p>
<p>PREFACE TO THE 
</p>
<p>FIRST EDITION 
</p>
<p>battle with the mathematical theorems at the same time. Also, by segregating the 
</p>
<p>mathematical theorems from the physical postulates, any possible confusion as to 
</p>
<p>which is which is nipped in the bud. 
</p>
<p>This chapter is followed by one on classical mechanics, where the Lagrangian 
</p>
<p>and Hamiltonian formalisms are developed in some depth. It is for the instructor to 
</p>
<p>decide how much of this to cover; the more students know of these matters, the 
</p>
<p>better they will understand the connection between classical and quantum mechanics. 
</p>
<p>Chapter 3 is devoted to a brief study of idealized experiments that betray the 
</p>
<p>inadequacy of classical mechanics and give a glimpse of quantum mechanics. 
</p>
<p>Having trained and motivated the students I now give them the postulates of 
</p>
<p>quantum mechanics of a single particle in one dimension. I use the word "postulate" 
</p>
<p>here to mean "that which cannot be deduced from pure mathematical or logical 
</p>
<p>reasoning, and given which one can formulate and solve quantum mechanical prob-
</p>
<p>lems and interpret the results." This is not the sense in which the true axiomatist 
</p>
<p>would use the word. For instance, where the true axiomatist would just postulate 
</p>
<p>that the dynamical variables are given by Hilbert space operators, I would add the 
</p>
<p>operator identifications, i.e., specify the operators that represent coordinate and 
</p>
<p>momentum (from which others can be built). Likewise, I would not stop with the 
</p>
<p>statement that there is a Hamiltonian operator that governs the time evolution 
</p>
<p>through the equation i1101lf/) ;at= HI 'If); I would say the His obtained from the 
classical Hamiltonian by substituting for x and p the corresponding operators. While 
</p>
<p>the more general axioms have the virtue of surviving as we progress to systems of 
</p>
<p>more degrees of freedom, with or without classical counterparts, students given just 
</p>
<p>these will not know how to calculate anything such as the spectrum of the oscillator. 
</p>
<p>Now one can, of course, try to "derive" these operator assignments, but to do so 
</p>
<p>one would have to appeal to ideas of a postulatory nature themselves. (The same 
</p>
<p>goes for "deriving'' the Schrodinger equation.) As we go along, these postulates are 
</p>
<p>generalized to more degrees of freedom and it is for pedagogical reasons that these 
</p>
<p>generalizations are postponed. Perhaps when students are finished with this book, 
</p>
<p>they can free themselves from the specific operator assignments and think of quantum 
</p>
<p>mechanics as a general mathematical formalism obeying certain postulates (in the 
</p>
<p>strict sense of the term). 
</p>
<p>The postulates in Chapter 4 are followed by a lengthy discussion of the same, 
</p>
<p>with many examples from fictitious Hilbert spaces of three dimensions. Nonetheless, 
</p>
<p>students will find it hard. It is only as they go along and see these postulates used 
</p>
<p>over and over again in the rest of the book, in the setting up of problems and the 
</p>
<p>interpretation of the results, that they will catch on to how the game is played. It is 
</p>
<p>hoped they will be able to do it on their own when they graduate. I think that any 
</p>
<p>attempt to soften this initial blow will be counterproductive in the long run. 
</p>
<p>Chapter 5 deals with standard problems in one dimension. It is worth mentioning 
</p>
<p>that the scattering off a step potential is treated using a wave packet approach. If 
</p>
<p>the subject seems too hard at this stage, the instructor may decide to return to it 
</p>
<p>after Chapter 7 (oscillator), when students have gained more experience. But I think 
</p>
<p>that sooner or later students must get acquainted with this treatment of scattering. 
</p>
<p>The classical limit is the subject of the next chapter. The harmonic oscillator is 
</p>
<p>discussed in detail in the next. It is the first realistic problem and the instructor may 
</p>
<p>be eager to get to it as soon as possible. If the instructor wants, he or she can discuss 
</p>
<p>the classical limit after discussing the oscillator. </p>
<p/>
</div>
<div class="page"><p/>
<p>We next discuss the path integral formulation due to Feynman. Given the intui-
</p>
<p>tive understanding it provides, and its elegance (not to mention its ability to give 
</p>
<p>the full propagator in just a few minutes in a class of problems), its omission from 
</p>
<p>so many books is hard to understand. While it is admittedly hard to actually evaluate 
</p>
<p>a path integral (one example is provided here), the notion of expressing the propag-
</p>
<p>ator as a sum over amplitudes from various paths is rather simple. The importance 
</p>
<p>of this point of view is becoming clearer day by day to workers in statistical mechanics 
</p>
<p>and field theory. I think every effort should be made to include at least the first three 
</p>
<p>(and possibly five) sections of this chapter in the course. 
</p>
<p>The content of the remaining chapters is standard, in the first approximation. 
</p>
<p>The style is of course peculiar to this author, as are the specific topics. For instance, 
</p>
<p>an entire chapter ( 11) is devoted to symmetries and their consequences. The chapter 
</p>
<p>on the hydrogen atom also contains a section on how to make numerical estimates 
</p>
<p>starting with a few mnemonics. Chapter 15, on addition of angular momenta, also 
</p>
<p>contains a section on how to understand the "accidental" degeneracies in the spectra 
</p>
<p>of hydrogen and the isotropic oscillator. The quantization of the radiation field is 
</p>
<p>discussed in Chapter 18, on time-dependent perturbation theory. Finally the treat-
</p>
<p>ment of the Dirac equation in the last chapter (20) is intended to show that several 
</p>
<p>things such as electron spin, its magnetic moment, the spin-orbit interaction, etc. 
</p>
<p>which were introduced in an ad hoc fashion in earlier chapters, emerge as a coherent 
</p>
<p>whole from the Dirac equation, and also to give students a glimpse of what lies 
</p>
<p>ahead. This chapter also explains how Feynman resolves the problem of negative-
</p>
<p>energy solutions (in a way that applies to bosons and fermions). 
</p>
<p>For Whom Is this Book Intended? 
</p>
<p>In writing it, I addressed students who are trying to learn the subject by them-
</p>
<p>selves; that is to say, I made it as self-contained as possible, included a lot of exercises 
</p>
<p>and answers to most of them, and discussed several tricky points that trouble students 
</p>
<p>when they learn the subject. But I am aware that in practice it is most likely to be 
</p>
<p>used as a class text. There is enough material here for a full year graduate course. 
</p>
<p>It is, however, quite easy so adapt it to a year-long undergraduate course. Several 
</p>
<p>sections that may be omitted without loss of continuity are indicated. The sequence 
</p>
<p>of topics may also be changed, as stated earlier in this preface. I thought it best to 
</p>
<p>let the instructor skim through the book and chart the course for his or her class, 
</p>
<p>given their level of preparation and objectives. Of course the book will not be particu-
</p>
<p>larly useful if the instructor is not sympathetic to the broad philosophy espoused 
</p>
<p>here, namely, that first comes the mathematical training and then the development 
</p>
<p>of the subject from the postulates. To instructors who feel that this approach is all 
</p>
<p>right in principle but will not work in practice, I reiterate that it has been found to 
</p>
<p>work in practice, not just by me but also by teachers elsewhere. 
</p>
<p>The book may be used by nonphysicists as well. (I have found that it goes well 
</p>
<p>with chemistry majors in my classes.) Although I wrote it for students with no familiar-
</p>
<p>ity with the subject, any previous exposure can only be advantageous. 
</p>
<p>Finally, I invite instructors and students alike to communicate to me any sugges-
</p>
<p>tions for improvement, whether they be pedagogical or in reference to errors or 
</p>
<p>misprints. 
</p>
<p>xi 
</p>
<p>PREP ACE TO THE 
</p>
<p>FIRST EDITION </p>
<p/>
</div>
<div class="page"><p/>
<p>xii 
</p>
<p>PREFACE TO THE 
</p>
<p>FIRST EDITION 
</p>
<p>Acknowledgments 
</p>
<p>As I look back to see who all made this book possible, my thoughts first turn 
to my brother R. Rajaraman and friend Rajaram Nityananda, who, around the 
same time, introduced me to physics in general and quantum mechanics in particular. 
Next come my students, particularly Doug Stone, but for whose encouragement and 
enthusiastic response I would not have undertaken this project. I am grateful to 
Professor Julius Kovacs of Michigan State, whose kind words of encouragement 
assured me that the book would be as well received by my peers as it was by 
my students. More recently, I have profited from numerous conversations with my 
colleagues at Yale, in particular Alan Chodos and Peter Mohr. My special thanks 
go to Charles Sommerfield, who managed to make time to read the manuscript and 
made many useful comments and recommendations. The detailed proofreading was 
done by Tom Moore. I thank you, the reader, in advance, for drawing to my notice 
any errors that may have slipped past us. 
</p>
<p>The bulk of the manuscript production cost were borne by the J. W. Gibbs 
fellowship from Yale, which also supported me during the time the book was being 
written. Ms. Laurie Liptak did a fantastic job of typing the first 18 chapters and 
Ms. Linda Ford did the same with Chapters 19 and 20. The figures are by Mr. J. 
Brosious. Mr. R. Badrinath kindly helped with the index.t 
</p>
<p>On the domestic front, encouragement came from my parents, my in-laws, and 
most important of all from my wife, Uma, who cheerfully donated me to science for 
a year or so and stood by me throughout. Little Umesh did his bit by tearing up all 
my books on the subject, both as a show of support and to create a need for this 
one. 
</p>
<p>R. Shankar 
New Haven, Connecticut 
</p>
<p>tIt is a pleasure to acknowledge the help of Mr. Richard Hatch, who drew my attention to a number 
of errors in the first printing. </p>
<p/>
</div>
<div class="page"><p/>
<p>Prelude 
</p>
<p>Our description of the physical world is dynamic in nature and undergoes frequent 
change. At any given time, we summarize our knowledge of natural phenomena by 
means of certain laws. These laws adequately describe the phenomenon studied up 
to that time, to an accuracy then attainable. As time passes, we enlarge the domain 
of observation and improve the accuracy of measurement. As we do so, we constantly 
check to see :r &bull;he laws continue to be valid. Those laws that do remain valid gain 
in stature, and those that do not must be abandoned in favor of new ones that do. 
</p>
<p>In this changing picture, the laws of classical mechanics formulated by Galileo, 
Newton, and later by Euler, Lagrange, Hamilton, Jacobi, and others, remained 
unaltered for almost three centuries. The expanding domain of classical physics met 
its first obstacles around the beginning of this century. The obstruction came on two 
fronts: at large velocities and small (atomic) scales. The problem of large velocities 
was successfully solved by Einstein, who gave us his relativistic mechanics, while the 
founders of quantum mechanics-Bohr, Heisenberg, Schrodinger, Dirac, Born, and 
others-solved the problem of small-scale physics. The union of relativity and quan-
tum mechanics, needed for the description of phenomena involving simultaneously 
large velocities and small scales, turns out to be very difficult. Although much pro-
gress has been made in this subject, called quantum field theory, there remain many 
open questions to this date. We shall concentrate here on just the small-scale problem, 
that is to say, on non-relativistic quantum mechanics. 
</p>
<p>The passage from classical to quantum mechanics has several features that are 
common to all such transitions in which an old theory gives way to a new one: 
</p>
<p>( 1) There is a domain Dn of phenomena described by the new theory and a sub-
domain Do wherein the old theory is reliable (to a given accuracy). 
</p>
<p>(2) Within the subdomain Do either theory may be used to make quantitative pre-
dictions. It might often be more expedient to employ the old theory. 
</p>
<p>(3) In addition to numerical accuracy, the new theory often brings about radical 
conceptual changes. Being of a qualitative nature, these will have a bearing on 
all of Dn. 
</p>
<p>For example, in the case of relativity, Do and Dn represent (macroscopic) 
phenomena involving small and arbitrary velocities, respectively, the latter, of course, xiii </p>
<p/>
</div>
<div class="page"><p/>
<p>xiv 
</p>
<p>PRELUDE 
</p>
<p>being bounded by the velocity of light. In addition to giving better numerical pre-
</p>
<p>dictions for high-velocity phenomena, relativity theory also outlaws several cherished 
</p>
<p>notions of the Newtonian scheme, such as absolute time, absolute length, unlimited 
</p>
<p>velocities for particles, etc. 
</p>
<p>In a similar manner. quantum mechanics brings with it not only improved 
</p>
<p>numerical predictions for the microscopic world, but also conceptual changes that 
</p>
<p>rock the very foundations of classical thought. 
</p>
<p>This book introduces you to this subject, starting from its postulates. Between 
</p>
<p>you and the postulates there stand three chapters wherein you will find a summary 
</p>
<p>of the mathematical ideas appearing in the statement of the postulates, a review of 
</p>
<p>classical mechanics, and a brief description of the empirical basis for the quantum 
</p>
<p>theory. In the rest of the book, the postulates are invoked to formulate and solve a 
</p>
<p>variety of quantum mechanical problems. rt is hoped thaL by the time you get to 
</p>
<p>the end of the book, you will be able to do the same yourself. 
</p>
<p>Note to the Student 
</p>
<p>Do as many exercises as you can, especially the ones marked * or whose results 
carry equation numbers. The answer to each exercise is given &lt;~ither with the exercise 
</p>
<p>or at the end of the book. 
The first chapter is very important. Do not rush through it. Even if you know 
</p>
<p>the math, read it to get acquainted with the notation. 
</p>
<p>I am not saying it is an easy subject. But I hope this book makes it seem 
</p>
<p>reasonable. 
</p>
<p>Good luck. </p>
<p/>
</div>
<div class="page"><p/>
<p>Contents 
</p>
<p>l. Mathematical Introduction 1 
</p>
<p>1.1. Linear Vector Spaces: Basics . 1 
1.2. Inner Product Spaces . 7 
1.3. Dual Spaces and the Dirac Notation 11 
1.4. Subspaces . 17 
1.5. Linear Operators . 18 
1.6. Matrix Elements of Linear Operators 20 
1.7. Active and Passive Transformations. 29 
1.8. The Eigenvalue Problem. 30 
1.9. Functions of Operators and Related Concepts 54 
1.10. Generalization to Infinite Dimensions 57 
</p>
<p>2. Review of Classical Mechanics . 75 
</p>
<p>2.1. The Principle of Least Action and Lagrangian Mechanics 78 
2.2. The Electromagnetic Lagrangian 83 
2.3. The Two-Body Problem . 85 
2.4. How Smart Is a Particle? 86 
2.5. The Hamiltonian Formalism . 86 
2.6. The Electromagnetic Force in the Hamiltonian Scheme 90 
2.7. Cyclic Coordinates, Poisson Brackets, and Canonical 
</p>
<p>Transformations 91 
2.8. Symmetries and Their Consequences 98 
</p>
<p>3. Allis Not Well with Classical Mechanics 107 
</p>
<p>3.1. Particles and Waves in Classical Physics . 107 
3.2. An Experiment with Waves and Particles (Classical) 108 
3.3. The Double-Slit Experiment with Light 110 
3.4. Matter Waves (de Broglie Waves) 112 
3.5. Conclusions 112 XV </p>
<p/>
</div>
<div class="page"><p/>
<p>xvi 
</p>
<p>CONTENTS 
</p>
<p>4. The Postulates-a General Discussion 
</p>
<p>4.1. The Postulates . . . . . . . . 
4.2. Discussion of Postulates 1-111 . 
4.3. The Schrodinger Equation (Dotting Your i's and 
</p>
<p>Crossing your fz's) . . . . . . . . . . . . . . 
</p>
<p>5. Simple Problems in One Dimension . 
</p>
<p>5.1. The Free Particle . . . . . . 
5.2. The Particle in a Box . . . . 
5.3. The Continuity Equation for Probability. 
5.4. The Single-Step Potential: a Problem in Scattering 
5.5. The Double-Slit Experiment 
5.6. Some Theorems . . . . . . . . . . . . . . . 
</p>
<p>6. The Classical Limit . 
</p>
<p>7. The Harmonic Oscillator 
</p>
<p>7 .1. Why Study the Harmonic Oscillator? 
7.2. Review of the Classical Oscillator. . 
7.3. Quantization of the Oscillator (Coordinate Basis). 
7.4. The Oscillator in the Energy Basis . . . . . 
7.5. Passage from the Energy Basis to the X Basis 
</p>
<p>8. The Path Integral Formulation of Quantum Theory 
</p>
<p>8.1. The Path Integral Recipe . . . . . . . . 
8.2. Analysis of the Recipe . . . . . . . . . 
8.3. An Approximation to U(t) for the Free Particle 
8.4. Path Integral Evaluation of the Free-Particle Propagator. 
8.5. Equivalence to the Schrodinger Equation . . . . 
8.6. Potentials of the Form V=a+hx+cx2 +dx+exx. 
</p>
<p>9. The Heisenberg Uncertainty Relations. . . . . 
</p>
<p>9 .I. Introduction . . . . . . . . . . . . . 
9.2. Derivation of the Uncertainty Relations . 
9.3. The Minimum Uncertainty Packet . . . 
9.4. Applications of the Uncertainty Principle 
9.5. The Energy-Time Uncertainty Relation 
</p>
<p>10. Systems with N Degrees of Freedom . . . 
</p>
<p>1 0.1. N Particles in One Dimension . . . 
10.2. More Particles in More Dimensions 
10.3. Identical Particles . . . . . . . . 
</p>
<p>115 
</p>
<p>115 
116 
</p>
<p>143 
</p>
<p>151 
</p>
<p>151 
157 
</p>
<p>164 
</p>
<p>167 
175 
</p>
<p>176 
</p>
<p>179 
</p>
<p>185 
</p>
<p>185 
188 
</p>
<p>189 
</p>
<p>202 
216 
</p>
<p>223 
</p>
<p>223 
224 
225 
226 
229 
</p>
<p>231 
</p>
<p>237 
</p>
<p>237 
237 
</p>
<p>239 
241 
</p>
<p>245 
</p>
<p>247 
</p>
<p>247 
259 
260 </p>
<p/>
</div>
<div class="page"><p/>
<p>11. Symmetries and Their Consequences 279 xv:ii 
</p>
<p>11.1. Overview. 279 CONTENTS 
</p>
<p>11.2. Translational Invariance in Quantum Theory 279 
</p>
<p>11.3. Time Translational In variance. 294 
</p>
<p>11.4. Parity Invariance 297 
</p>
<p>11.5. Time-Reversal Symmetry . 301 
</p>
<p>12. Rotational Invariance and Angular Momentum 305 
</p>
<p>12.1. Translations in Two Dimensions. 305 
</p>
<p>12.2. Rotations in Two Dimensions . 306 
</p>
<p>12.3. The Eigenvalue Problem of Lc. 313 
12.4. Angular Momentum in Three Dimensions 318 
</p>
<p>12.5. The Eigenvalue Problem of L 2 and Lc 321 
12.6. Solution of Rotationally Invariant Problems 339 
</p>
<p>13. The Hydrogen Atom 353 
</p>
<p>13.1. The Eigenvalue Problem 353 
</p>
<p>13.2. The Degeneracy of the Hydrogen Spectrum . 359 
</p>
<p>13.3. Numerical Estimates and Comparison with Experiment . 361 
</p>
<p>13.4. Multielectron Atoms and the Periodic Table 369 
</p>
<p>14. Spin . 373 
</p>
<p>14.1. Introduction 373 
14.2. What is the Nature of Spin? 373 
14.3. Kinematics of Spin 374 
</p>
<p>14.4. Spin Dynamics 385 
14.5. Return of Orbital Degrees of Freedom 397 
</p>
<p>15. Addition of Angular Momenta 403 
</p>
<p>15.1. A Simple Example . 403 
</p>
<p>15.2. The General Problem 408 
15.3. Irreducible Tensor Operators 416 
15.4. Explanation of Some "Accidental" Degeneracies. 421 
</p>
<p>16. Variational and WKB Methods 429 
</p>
<p>16.1. The Variational Method 429 
16.2. The Wentzel-Kramers-Brillouin Method 435 
</p>
<p>17. Time-Independent Perturbation Theory 451 
</p>
<p>17.1. The Formalism 451 
17.2. Some Examples . 454 
17.3. Degenerate Perturbation Theory . 464 </p>
<p/>
</div>
<div class="page"><p/>
<p>xviii 
</p>
<p>CONTENTS 
</p>
<p>18. Time-Dependent Perturbation Theory . . 
</p>
<p>18.1. The Problem . . 
</p>
<p>18.2. First-Order Perturbation Theory. 
18.3. Higher Orders in Perturbation Theory 
</p>
<p>18.4. A General Discussion of Electromagnetic Interactions 
</p>
<p>18.5. Interaction of Atoms with Electromagnetic Radiation 
</p>
<p>19. Scattering Theory . . . . . . . . . . . . . . . . . . . 
</p>
<p>473 
</p>
<p>473 
</p>
<p>474 
484 
492 
</p>
<p>499 
</p>
<p>523 
</p>
<p>19 .1. Introduction . . . . . . . . . . . . . . . . . . 523 
</p>
<p>19.2. Recapitulation of One-Dimensional Scattering and Overview 524 
</p>
<p>19.3. The Born Approximation (Time-Dependent Description) 529 
</p>
<p>19.4. Born Again (The Time-Independent Approximation). 534 
</p>
<p>19.5. The Partial Wave Expansion 545 
</p>
<p>19.6. Two-Particle Scattering. 555 
</p>
<p>20. The Dirac Equation . . . . . 
</p>
<p>20.1. 
</p>
<p>20.2. 
</p>
<p>20.3. 
</p>
<p>The Free-Particle Dirac Equation 
</p>
<p>Electromagnetic Interaction of the Dirac Particle 
</p>
<p>More on Relativistic Quantum Mechanics 
</p>
<p>21. Path Integrals-II . . . . . . . . . 
</p>
<p>21. 1. Derivation of the Path Integral 
</p>
<p>21.2. Imaginary Time Formalism . . 
</p>
<p>21.3. Spin and Fermion Path Integrals 
</p>
<p>21.4. Summary. . . . . . . 
</p>
<p>Appendix 
</p>
<p>A. I. Matrix Inversion. 
</p>
<p>A.2. Gaussian Integrals 
</p>
<p>A.3. Complex Numbers . 
A.4. The i8 Prescription . 
</p>
<p>ANSWERS TO SELECTED ExERCISES 
</p>
<p>TABLE oF CoNsTANTs 
</p>
<p>bJDEX ...... . 
</p>
<p>563 
</p>
<p>563 
566 
</p>
<p>574 
</p>
<p>581 
</p>
<p>582 
</p>
<p>613 
636 
</p>
<p>652 
</p>
<p>655 
</p>
<p>655 
659 
</p>
<p>660 
661 
</p>
<p>665 
</p>
<p>669 
</p>
<p>671 </p>
<p/>
</div>
<div class="page"><p/>
<p>1 
</p>
<p>Mathematical Introduction 
</p>
<p>The aim of this book is to provide you with an introduction to quantum mechanics, 
</p>
<p>starting from its axioms. It is the aim of this chapter to equip you with the necessary 
</p>
<p>mathematical machinery. All the math you will need is developed here, starting from 
</p>
<p>some basic ideas on vectors and matrices that you are assumed to know. Numerous 
</p>
<p>examples and exercises related to classical mechanics are given, both to provide some 
</p>
<p>relief from the math and to demonstrate the wide applicability of the ideas developed 
</p>
<p>here. The effort you put into this chapter will be well worth your while: not only 
</p>
<p>will it prepare you for this course, but it will also unify many ideas you may have 
</p>
<p>learned piecemeal. To really learn this chapter, you must, as with any other chapter, 
</p>
<p>work out the problems. 
</p>
<p>1.1. Linear Vector Spaces: Basics 
</p>
<p>In this section you will be introduced to linear vector spaces. You are surely 
</p>
<p>familiar with the arrows from elementary physics encoding the magnitude and 
</p>
<p>direction of velocity, force, displacement, torque, etc. You know how to add them 
</p>
<p>and multiply them by scalars and the rules obeyed by these operations. For example, 
</p>
<p>you know that scalar multiplication is distributive: the multiple of a sum of two 
</p>
<p>vectors is the sum of the multiples. What we want to do is abstract from this simple 
</p>
<p>case a set of basic features or axioms, and say that any set of objects obeying the same 
</p>
<p>forms a linear vector space. The cleverness lies in deciding which of the properties to 
</p>
<p>keep in the generalization. If you keep too many, there will be no other examples; 
</p>
<p>if you keep too few, there will be no interesting results to develop from the axioms. 
</p>
<p>The following is the list of properties the mathematicians have wisely chosen as 
</p>
<p>requisite for a vector space. As you read them, please compare them to the world 
</p>
<p>of arrows and make sure that these are indeed properties possessed by these familiar 
</p>
<p>vectors. But note also that conspicuously missing are the requirements that every 
</p>
<p>vector have a magnitude and direction, which was the first and most salient feature 
</p>
<p>drilled into our heads when we first heard about them. So you might think that in 
</p>
<p>dropping this requirement, the baby has been thrown out with the bath water. 
</p>
<p>However, you will have ample time to appreciate the wisdom behind this choice as 1 </p>
<p/>
</div>
<div class="page"><p/>
<p>2 
</p>
<p>CHAPTER I 
</p>
<p>you go along and see a great unification and synthesis of diverse ideas under the 
heading of vector spaces. You will see examples of vector spaces that involve entities 
that you cannot intuitively perceive as having either a magnitude or a direction. 
While you should be duly impressed with all this, remember that it does not hurt at 
all to think of these generalizations in terms of arrows and to use the intuition to 
prove theorems or at the very least anticipate them. 
</p>
<p>Definition 1. A linear vector space W is a collection of objects 11 ), 
12), ... , IV), ... , I W), ... , called vectors, for which there exists 
</p>
<p>1. A definite rule for forming the vector sum, denoted I V) + I W) 
2. A definite rule for multiplication by scalars a, b, ... , denoted al V) with the 
following features: 
</p>
<p>&bull; The result of these operations is another element of the space, a feature called 
closure: IV)+ I W)e'V. 
</p>
<p>&bull; Scalar multiplication is distributive in the vectors: a( IV)+ I W)) = 
al V)+al W). 
</p>
<p>&bull; Scalar multiplication is distributive in the scalars: (a+b)l V)=al V)+bl V). 
&bull; Scalar multiplication is associative: a(bl V)) = abl V). 
&bull; Addition is commutative: I V) + I W) = I W) + I V). 
&bull; Addition is associative: IV)+ (I W) + IZ)) =(IV)+ I W)) + IZ). 
&bull; There exists a null vector 10) obeying IV)+ 10) =IV). 
&bull; For every vector IV) there exists an inverse under addition, 1-V), such that 
</p>
<p>IV&gt;+ I-V)= IO). 
</p>
<p>There is a good way to remember all of these; do what comes naturally. 
</p>
<p>Definition 2. The numbers a, b, ... are called the field over which the vector 
space is defined. 
</p>
<p>If the field consists of all real numbers, we have a real vector space, if they are 
complex, we have a complex vector space. The vectors themselves are neither real 
nor complex; the adjective applies only to the scalars. 
</p>
<p>Let us note that the above axioms imply 
</p>
<p>&bull; 10) is unique, i.e., if IO') has all the properties of 10), then 10) = 10'). 
&bull; OJV)=IO). 
</p>
<p>&bull; 1-V)=-JV). 
&bull; 1- V) is the unique additive inverse of IV). 
</p>
<p>The proofs are left as to the following exercise. You don't have to know the proofs, 
but you do have to know the statements. 
</p>
<p>Exercise 1.1.1. Verify these claims. For the first consider 10) + 10') and use the advertised 
properties of the two null vectors in turn. For the second start with 10) = (0+ 1)1 V) + 1- V). 
For the third, begin with jV)+(-jV))=OjV)=IO). For the last, let I W) also satisfy 
IV)+ I W) = 10). Since 10) is unique, this means IV)+ I W) =IV)+ 1- V). Take it from here. </p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 1. 1. The rule for vector addition. Note that it obeys axioms 
</p>
<p>(i)-(iii). 
</p>
<p>Exercise 1.1.2. Consider the set of all entities of the form (a, b, c) where the entries are 
</p>
<p>real numbers. Addition and scalar multiplication are defined as follows: 
</p>
<p>(a, b, c)+(d, e, f)=(a+d, b+e, c+f) 
</p>
<p>a(a, b, c)= (a a, ab, a c). 
</p>
<p>Write down the null vector and inverse of (a, b, c). Show that vectors of the form (a, b, 1) do 
not forr.1 ::: vector space. 
</p>
<p>Observe that we are using a new symbol I V) to denote a generic vector. This 
object is called ket V and this nomenclature is due to Dirac whose notation will be 
</p>
<p>discussed at some length later. We do not purposely use the symbol V to denote the 
</p>
<p>vectors as the first step in weaning you away from the limited concept of the vector as 
</p>
<p>an arrow. You are however not discouraged from associating with IV) the arrowlike 
</p>
<p>object till you have seen enough vectors that are not arrows and are ready to drop 
</p>
<p>the crutch. 
You were asked to verify that the set of arrows qualified as a vector space as 
</p>
<p>you read the axioms. Here are some of the key ideas you should have gone over. 
</p>
<p>The vector space consists of arrows, typical ones being Vand V'. The rule for 
</p>
<p>addition is familiar: take the tail of the second arrow, put it on the tip of the first, 
</p>
<p>and so on as in Fig. 1.1. 
</p>
<p>Scalar multiplication by a corresponds to stretching the vector by a factor a. 
</p>
<p>This is a real vector space since stretching by a complex number makes no sense. (If 
</p>
<p>a is negative, we interpret it as changing the direction of the arrow as well as rescaling 
</p>
<p>it by lal.) Since these operations acting on arrows give more arrows, we have closure. 
Addition and scalar multiplication clearly have all the desired associative and distri-
</p>
<p>butive features. The null vector is the arrow of zero length, while the inverse of a 
</p>
<p>vector is the vector reversed in direction. 
</p>
<p>So the set of all arrows qualifies as a vector space. But we cannot tamper with 
</p>
<p>it. For example, the set of all arrows with positive z-componeat~ do not form a 
</p>
<p>vector space: there is no inverse. 
</p>
<p>Note that so far, no reference has been made to magnitude or direction. The 
</p>
<p>point is that while the arrows have these qualities, members of a vector space need 
</p>
<p>not. This statement is pointless unless I can give you examples, so here are two. 
</p>
<p>Consider the set of all 2 x 2 matrices. We know how to add them and multiply 
</p>
<p>them by scalars (multiply all four matrix elements by that scalar). The corresponding 
</p>
<p>rules obey closure, associativity, and distributive requirements. The null matrix has 
</p>
<p>all zeros in it and the inverse under addition of a matrix is the matrix with all elements 
</p>
<p>negated. You must agree that here we have a genuine vector space consisting of 
</p>
<p>things which don't have an obvious length or direction associated with them. When 
</p>
<p>we want to highlight the fact that the matrix M is an element of a vector space, we 
</p>
<p>may want to refer to it as, say, ket number 4 or: 14). 
</p>
<p>3 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>4 
</p>
<p>CHAPTER I 
</p>
<p>As a second example, consider all functionsf(x) defined in an interval 0 sx:::; L. 
</p>
<p>We define scalar multiplication by a simply as af(x) and addition as pointwise 
</p>
<p>addition: the sum of two functionsfand g has the valuef(x)+g(x) at the point x. 
</p>
<p>The null function is zero everywhere and the additive inverse off is - f 
</p>
<p>Exercise I. 1.3. Do functions that vanish at the end points x = 0 and x '= L form a vector 
space? How about periodic fimctions obeying .f(O) =.f(L)? How about functions that obey 
</p>
<p>.f(O) = 4? If the functions do not qualify, list the things that go wrong. 
</p>
<p>The next concept is that of linear independence of a set of vectors i l), i 2) ... In). 
First consider a linear relation of the form 
</p>
<p>L: adi)=IO) (1.1.1) 
i-" 1 
</p>
<p>We may assume without loss of generality that the left-hand side does not 
</p>
<p>contain any multiple of I 0), for if it did, it could be shifted to the right, and combined 
with the 10) there to give 10) once more. (We are using the fact that any multiple 
</p>
<p>of 10) equals 10).) 
</p>
<p>Definition 3. The set of vectors is said to be finear(v independent if the only such 
</p>
<p>linear relation as Eq. ( 1.1.1) is the trivial one with all ai = 0. If the set of vectors 
</p>
<p>is not linearly independent, we say they are linearly dependent. 
</p>
<p>Equation ( 1.1.1) tells us that it is not possible to vvrite any member of the 
</p>
<p>linearly independent set in terms of the others. On the other hand, if the set of 
</p>
<p>vectors is linearly dependent, such a relation will exist, and it must contain at least 
</p>
<p>two nonzero coefficients. Let us say rl} &yen;0. Then we could write 
</p>
<p>13) = L: 
&middot;&middot;&middot;-a 
--' I i) (1.1.2) 
</p>
<p>i= !,;&lt;S3 a3 
</p>
<p>thereby expressing 13) in terms of the others. 
</p>
<p>As a concrete example, consider two nonparallel vectors II) and 12) in a plane. 
These form a linearly independent set. There is no way to write one as a multiple of 
</p>
<p>the other, or equivalently, no way to combine them to get the null vector. On the 
</p>
<p>other hand, if the vectors are parallel, we can clearly write one as a multiple of the 
</p>
<p>other or equivalently play them against each other to get 0. 
</p>
<p>Notice I said 0 and not 10). This is, strictly speaking, incorrect since a set of 
</p>
<p>vectors can only add up to a vector and not a number. It is, however, common to 
</p>
<p>represent the null vector by 0. 
Suppose we bring in a third vector 13 &gt; also in the plane. If it is parallel to either 
</p>
<p>of the first two, we already have a linearly dependent set. So let us suppose it is not. 
</p>
<p>But even now the three of them are linearly dependent. This is because we can write 
</p>
<p>one of them, say 13), as a linear combination of the other two. To find the combina-
tion, draw a line from the tail of 13) in the direction of 11 ). Next draw a line 
antiparallel to from the tip of 13). These lines will intersect since I) and 12) are </p>
<p/>
</div>
<div class="page"><p/>
<p>not parallel by assumption. The intersection point P will determine how much of 
</p>
<p>11) and 12) we want: we go from the tail of 13) to P using the appropriate multiple 
</p>
<p>of 11) and go from P to the tip of 13) using the appropriate multiple of 12). 
</p>
<p>Exercise 1.1. 4. Consider three elements from the vector space of real 2 x 2 matrices: 
</p>
<p>11&gt;=[~ ~] 12&gt;=[~ ~] 13)= [-2 -1] 0 -2 
Are they linearly independent? Support your answer with details. (Notice we are calling 
</p>
<p>these matrices vectors and using kets to represent them to emphasize their role as elements 
</p>
<p>of a vector space) 
</p>
<p>Exercise 1.1.5. Show that the following row vectors are linearly dependent: (1, I, 0), 
</p>
<p>(1, 0, 1), and (3, 2, 1). Show the opposite for (1, 1, 0), (1, 0, 1), and (0, 1, 1). 
</p>
<p>Definition 4. A vector space has dimension n if it can accommodate a maximum 
</p>
<p>of n linearly independent vectors. It will be denoted by 'l.lr(R) if the field is real 
</p>
<p>and by 'l.lr( C) if the field is complex. 
</p>
<p>In view of the earlier discussions, the plane is two-dimensional and the set of 
</p>
<p>all arrows not limited to the plane define a three-dimensional vector space. How 
</p>
<p>about 2 x 2 matrices? They form a four-dimensional vector space. Here is a proof. 
</p>
<p>The following vectors are linearly independent: 
</p>
<p>II&gt;=[~ ~] 12&gt;=[~ ~] 13&gt;=[~ ~] 14&gt;=[~ ~] 
</p>
<p>since it is impossible to form linear combinations of any three of them to give the 
</p>
<p>fourth any three of them will have a zero in the one place where the fourth does 
</p>
<p>not. So the space is at least four-dimensional. Could it be bigger? No, since any 
</p>
<p>arbitrary 2 x 2 matrix can be written in terms of them: 
</p>
<p>[: !]=all)+bl2)+cl3)+dl4) 
</p>
<p>If the scalars a, b, c, dare real, we have a real four-dimensional space, if they 
</p>
<p>are complex we have a complex four-dimensional space. 
</p>
<p>Theorem 1. Any vector IV) in ann-dimensional space can be written as a linear 
</p>
<p>combination of n linearly independent vectors II) ... In). 
</p>
<p>The proof is as follows: if there were a vector I V) for which this were not 
</p>
<p>possible, it would join the given set of vectors and form a set of n + I linearly 
</p>
<p>independent vectors, which is not possible in an n-dimensional space by definition. 
</p>
<p>5 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>6 
</p>
<p>CHAPTER I 
</p>
<p>Definition 5. A set of n linearly independent vectors in an n-dimensional space 
is called a basis. 
</p>
<p>Thus we can write, on the strength of the above 
</p>
<p>IV)= L V;li) (1.1.3) 
i=l 
</p>
<p>where the vectors I i) form a basis. 
</p>
<p>Definition 6. The coefficients of expansion V; of a vector in terms of a linearly 
independent basis (I i)) are called the components of the vector in that basis. 
</p>
<p>Theorem 2. The expansion in Eq. (1.1.3) is unique. 
</p>
<p>Suppose the expansion is not unique. We must then have a second expansion: 
</p>
<p>n 
</p>
<p>IV)= L v;li) (1.1.4) 
i=l 
</p>
<p>Subtracting Eq. (1.1.4) from Eq. (1.1.3) (i.e., multiplying the second by the 
scalar -1 and adding the two equations) we get' 
</p>
<p>IO)=I (v;-v;)li) (1.1.5) 
</p>
<p>which implies that 
</p>
<p>v,=v; ( 1.1.6) 
</p>
<p>since the basis vectors are linearly independent and only a trivial linear reiation 
between them can exist. Note that given a basis the components are unique, but if 
we change the basis, the components will change. We refer to I V) as the vector in 
the abstract, having an existence of its own and satisfying various relations involving 
other vectors. When we choose a basis the vectors assume concrete forms in terms 
of their components and the relation between vectors is satisfied by the components. 
Imagine for example three arrows in the plane, A, il, C satisfying A+ B= C according 
to the laws for adding arrows. So far no basis has been chosen and we do not need 
a basis to make the statement that the vectors from a closed triangle. Now we choose 
a basis and write each vector in terms of the components. The components will 
satisfy C; =A;+ B;, i = 1, 2. If we choose a different basis, the components will change 
in numerical value, but the relation between them expressing the equality of C to 
the sum of the other two will still hold between the new set of components. </p>
<p/>
</div>
<div class="page"><p/>
<p>In the case of no narrow vectors, adding them in terms of components proceeds 
</p>
<p>as in the elementary case thanks to the axioms. If 
</p>
<p>( 1.1.7) 
</p>
<p>I W)=L: wdi) then ( 1.1.8) 
</p>
<p>(1.1.9) 
</p>
<p>where we have used the axioms to carry out the regrouping of terms. Here is the 
</p>
<p>conclusion: 
</p>
<p>To add two vectors, add their components. 
</p>
<p>There is no reference to taking the tail of one and putting it on the tip of the 
</p>
<p>other, etc., since in general the vectors have no head or tail. Of course, if we are 
</p>
<p>dealing with arrows, we can add them either using the tail and tip routine or by 
</p>
<p>simply adding their components in a basis. 
</p>
<p>In the same way, we have: 
</p>
<p>(l.l.lO) 
</p>
<p>In other words, 
</p>
<p>To multiply a vector by a scalar, multiply all its components by the scalar. 
</p>
<p>1.2. Inner Product Spaces 
</p>
<p>The matrix and function examples must have convinced you that we can have 
</p>
<p>a vector space with no preassigned definition of length or direction for the elements. 
</p>
<p>However, we can make up quantities that have the same properties that the lengths 
</p>
<p>and angles do in the case of arrows. The first step is to define a sensible analog of 
</p>
<p>the dot product, for in the case of arrows, from the dot product 
</p>
<p>A &middot; B = I A II Bl cos e (1.2.1) 
</p>
<p>we can read off the length of say A as JfAI&middot;IAI and the cosine of the angle between 
two vectors as A&middot; B/IAIIBI. Now you might rightfully object: how can you use the dot 
product to define the length and angles, if the dot product itself requires knowledge of 
the lengths and angles? The answer is this. Recall that the dot product has a second 
</p>
<p>7 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>8 
</p>
<p>CHAPTER l 
</p>
<p>I 
</p>
<p>I 
I 
I 
I 
I 
</p>
<p>~--Pk-: 
I 
I 
</p>
<p>I V; 
</p>
<p>Pj&middot;-&middot;~ 
</p>
<p>1-------- p jk -- .. - -~ 
</p>
<p>Figure 1.2. Geometrical proof that the dot product obeys axiom (3) 
</p>
<p>for an inner product. The axiom requires that the projections obey 
</p>
<p>P,+P1 =P1k. 
</p>
<p>equivalent expression in terms of the components: 
</p>
<p>( !.2.2) 
</p>
<p>Our goal is to define a similar formula for the general case where we do have the 
</p>
<p>notion of components in a basis. To this end we recall the main features of the above 
</p>
<p>dot product: 
</p>
<p>I. A-B=B&middot;A (symmetry) 
2. A- A :2: 0 0 iff A = 0 (positive semidefiniteness) 
3. X (bE+ cC) = b.A- B+ ot&middot; C (linearity) 
</p>
<p>The linearity of the dot product is illustrated in Fig. 1.2. 
</p>
<p>We want to invent a generalization called the inner product or scalar product 
</p>
<p>between any two vectors IV) and I W). We denote it by the symbol (VI W). It is 
</p>
<p>once again a number (generally complex) dependent on the two vectors. We demand 
</p>
<p>that it obey the following axioms: 
</p>
<p>&bull; (VI W) = ( W! V) * (skew-symmetry) 
&bull; (VI V) :2:0 0 iffl V) =I 0) (positive semidefiniteness) 
</p>
<p>&bull; (VI (al W) +biZ))== ( VlaW+ hZ) =a( VI W) +h( VIZ) (linearity in ket) 
</p>
<p>Definition 7. A vector space with an inner product is called an inner product 
</p>
<p>space. 
</p>
<p>Notice that we have not yet given an explicit rule for actually evaluating the 
</p>
<p>scalar product, we are merely demanding that any rule we come up with must have 
</p>
<p>these properties. With a view to finding such a rule, let us familiarize ourselves with 
</p>
<p>the axioms. The first differs from the corresponding one for the dot product and 
makes the inner product sensitive to the order of the two factors, with the two 
choices leading to complex conjugates. In a real vector space this axioms states the 
</p>
<p>symmetry of the dot product under exchange of the two vectors. For the present, 
</p>
<p>let us note that this axiom ensures that (VI V) is real. 
The second axiom says that (VI V) is not just real bnt also positive semidefinite, 
</p>
<p>vanishing only if the vector itself does. If we are going to define the length of the 
</p>
<p>vector as the square root of its inner product with itself (as .in the dot product) this 
</p>
<p>quantity had better be real and positive for all nonzero vectors. </p>
<p/>
</div>
<div class="page"><p/>
<p>The last axiom expresses the linearity of the inner product when a linear super-
</p>
<p>position al W) + bl Z) =I a W + bZ) appears as the second vector in the scalar prod-
uct. We have discussed its validity for the arrows case (Fig. 1.2). 
</p>
<p>What if the first factor in the product is a linear superposition, i.e., what is 
</p>
<p>(aW+bZI V)? This is determined by the first axiom: 
</p>
<p>(aW+bZI V) = (VIaW+bZ) * 
</p>
<p>=(a( VI W) + b( VIZ))* 
</p>
<p>=a*( VI W)*+b*(VIZ)* 
</p>
<p>=a*(WI V)+b*(ZI V) (1.2.3) 
</p>
<p>which expresses the antilinearity of the inner product with respect to the first factor 
</p>
<p>in the inner product. In other words, the inner product of a linear superposition 
</p>
<p>with another vector is the corresponding superposition of inner products if the super-
</p>
<p>position occurs in the second factor, while it is the superposition with all coefficients 
</p>
<p>conjugated if the superposition occurs in the first factor. This asymmetry, unfamiliar 
</p>
<p>in real vector spaces, is here to stay and you will get used to it as you go along. 
</p>
<p>Let us continue with inner products. Even though we are trying to shed the 
</p>
<p>restricted notion of a vector as an arrow and seeking a corresponding generalization 
</p>
<p>of the dot product, we still use some of the same terminology. 
</p>
<p>Definition 8. We say that two vectors are orthogonal or perpendicular if their 
</p>
<p>inner product vanishes. 
</p>
<p>Definition 9. We will refer to .j( VI V) =I VI as the norm or length of the vector. 
A normalized vector has unit norm. 
</p>
<p>Definition 10. A set of basis vectors all of unit norm, which are pairwise ortho-
</p>
<p>gonal will be called an orthonormal basis. 
</p>
<p>We will also frequently refer to the inner or scalar product as the dot product. 
</p>
<p>We are now ready to obtain a concrete formula for the inner product in terms 
</p>
<p>of the components. Given IV) and I W) 
</p>
<p>IV)=:[ V;li) 
</p>
<p>I W&gt;=I W;IJ&gt; 
</p>
<p>we follow the axioms obeyed by the inner product to obtain: 
</p>
<p>(VI W)=:[ I v?wj(i ( 1.2.4) 
i j 
</p>
<p>To go any further we have to know (iiJ), the inner product between basis vectors. 
That depends on the details of the basis vectors and all we know for sure is that 
</p>
<p>9 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>10 
</p>
<p>CHAPTER l 
</p>
<p>they are linearly independent This situation exists for arrows as well. Consider a 
</p>
<p>two-dimensional problem where the basis vectors are two linearly independent but 
</p>
<p>nonperpendicular vectors. If we write all vectors in terms of this basis, the dot 
</p>
<p>product of any two of them will likewise be a double sum with four terms ( detem1ined 
</p>
<p>by the four possible dot products between the basis vectors) as well as the vector 
</p>
<p>components. However, if we use an orthonormal basis such as z', j, only diagonal 
terms like (il i) will survive and we will get the familiar result A&middot; B=A,Bx+ A,B,. 
depending only on the components. 
</p>
<p>For the more general nonarrow case .. we invoke Theorem 3. 
</p>
<p>Theorem 3 (Gram-Schmidt). Given a linearly independent basis we can form 
</p>
<p>linear combinations of the basis vectors to obtain an orthonormal basis. 
</p>
<p>Postponing the proof for a moment. let us assume that the procedure has been 
</p>
<p>implemented and that the current basis is orthonormal: 
</p>
<p>(i 
for i=j ~ 
</p>
<p>=(\ 
forirj 1 
</p>
<p>where bu is called the Kronecker delta syrnhol. Feeding this into Eq. (1.2.4) we find 
</p>
<p>the double sum collapses to a single one due to the Kronecker delta, to give 
</p>
<p>(V! W)=L: v(w1 ( 1.2.5) 
</p>
<p>This is the form of the inner product we will use from now on. 
</p>
<p>You can now appreciate the first axiom; but for the complex conjugation of 
</p>
<p>the components of the first vector. &lt;VI V) would not even be real, not to mention 
</p>
<p>positive. But now it is given by 
</p>
<p>( 1.2.6) 
</p>
<p>and vanishes only for the null vector. This makes it sensible to refer to &lt;VI V) as 
the length or norm squared of a vector. 
</p>
<p>Consider Eq. (1.2.5). Since the vector IV) is uniquely specified by its compo-
nents in a given basis, we may, in this basis, write it as a column vector: 
</p>
<p>[
VI] v., 
</p>
<p>IV)-&middot;+ : 
</p>
<p>Vn 
</p>
<p>in this basis ( 1.2. 7) </p>
<p/>
</div>
<div class="page"><p/>
<p>Likewise 
</p>
<p>IW)--+ in this basis (1.2.8) 
</p>
<p>The inner product (VI W) is given by the matrix product of the transpose conjugate 
</p>
<p>of the column vector representing I V) with the column vector representing I W): 
</p>
<p>(VI W) = [vf, vf, ... , v!] ( 1.2.9) 
</p>
<p>1.3. Dual Spaces and the Dirac Notation 
</p>
<p>There is a technical point here. The inner product is a number we are trying to 
</p>
<p>generate from two kets IV) and I W), which are both represented by column vectors 
</p>
<p>in some basis. Now there is no way to make a number out of two columns by direct 
</p>
<p>matrix multiplication, but there is a way to make a number by matrix multiplication 
</p>
<p>of a row times a column. Our trick for producing a number out of two columns has 
</p>
<p>been to associate a unique row vector with one column (its transpose conjugate) 
</p>
<p>and form its matrix product with the column representing the other. This has the 
</p>
<p>feature that the answer depends on which of the two vectors we are going to convert 
</p>
<p>to the row, the two choices (&lt;VI W) and (WI V)) leading to answers related by 
</p>
<p>complex conjugation. 
</p>
<p>But one can also take the following alternate view. Column vectors are concrete 
</p>
<p>manifestations of an abstract vector IV) or ket in a basis. We can also work back-
</p>
<p>ward and go from the column vectors to the abstract kets. But then it is similarly 
</p>
<p>possible to work backward and associate with each row vector an abstract object 
</p>
<p>(WI, called bra- W. Now we can name the bras as we want but let us do the following. 
</p>
<p>Associated with every ket I V) is a column vector. Let us take its adjoint, or transpose 
conjugate, and form a row vector. The abstract bra associated with this will bear 
</p>
<p>the same label, i.e., it will be called (VI. Thus there are two vector spaces, the space 
</p>
<p>of kets and a dual space of bras, with a ket for every bra and vice versa (the 
</p>
<p>components being related by the adjoint operation). Inner products are really defined 
</p>
<p>only between bras and kets and hence from elements of two distinct but related 
</p>
<p>vector spaces. There is a basis of vectors I i) for expanding kets and a similar basis 
</p>
<p>(il for expanding bras. The basis ket li) is represented in the basis we are using by 
</p>
<p>a column vector with all zeros except for a I in the ith row, while the basis bra (il 
</p>
<p>is a row vector with all zeros except for a I in the ith column. 
</p>
<p>11 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>12 
</p>
<p>CHAPTER I 
</p>
<p>All this may be summarized as follows: 
</p>
<p>IV) .._. &lt;--&gt; [vi, vi, ... v~] &lt;--&gt; &lt;VI (1.3.1) 
</p>
<p>where&lt;--&gt; means "within a basis." 
There is, however, nothing wrong with the first viewpoint of associating a scalar 
</p>
<p>product with a pair of columns or kets (making no reference to another dual space) 
and living with the asymmetry between the first and second vector in the inner 
product (which one to transpose conjugate?). If you found the above discussion 
heavy going, you can temporarily ignore it. The only thing you must remember is 
that in the case of a general nonarrow vector space: 
</p>
<p>&bull; Vectors can still be assigned components in some orthonormal basis, just as with 
arrows, but these may be complex. 
</p>
<p>&bull; The inner product of any two vectors is given in terms of these components by 
Eq. (1.2.5). This product obeys all the axioms. 
</p>
<p>1.3.1. Expansion of Vectors in an Orthonormal Basis 
</p>
<p>Suppose we wish to expand a vector IV) in an orthonormal basis. To find the 
components that go into the expansion we proceed as follows. We take the dot 
product of both sides of the assumed expansion with If): (or &lt;JI if you are a purist) 
</p>
<p>I V)=I V;li) ( 1.3.2) 
</p>
<p>(JI V)=I v;(Jii) (1.3.3) 
</p>
<p>( 1.3.4) 
</p>
<p>i.e., to find thejth component of a vector we take the dot product with thejth unit 
vector, exactly as with arrows. Using this result we may write 
</p>
<p>I V&gt;=I li)(il V&gt; ( 1.3.5) 
</p>
<p>Let us make sure the basis vectors look as they should. If we set IV)= IJ) in Eq. 
( 1.3.5), we find the correct answer: the ith component of the jth basis vector is 8 iJ. 
Thus for example the column representing basis vector number 4 will have a 1 in 
the 4th row and zero everywhere else. The abstract relation 
</p>
<p>( 1.3.6) </p>
<p/>
</div>
<div class="page"><p/>
<p>becomes in this basis 
</p>
<p>VI 1 0 () 
</p>
<p>v2 0 1 () 
</p>
<p>=vi + v2 0 +&middot; .. v, (1.3.7) 
</p>
<p>Vn 0 0 
</p>
<p>1.3.2. Adjoint Operation 
</p>
<p>We have seen that we may pass from the column representing a ket to the 
</p>
<p>row representing the corresponding bra by the adjoint operation, i.e., transpose 
</p>
<p>conjugation. Let us now ask: if &lt;VI is the bra corresponding to the ket I V) what 
bra corresponds to a! V) where a is some scalar? By going to any basis it is readily 
found that 
</p>
<p>a! V)----&gt; ----&gt; [a*vf, a*vi &bull; ... , a*v~]--&gt; (VI a* ( 1.3.8) 
</p>
<p>It is customary to write a! V) as I a V) and the corresponding bra as (a VI. What 
</p>
<p>we have found is that 
</p>
<p>(aV! =(VIa* ( 1.3.9) 
</p>
<p>Since the relation between bras and kets is linear we can say that if we have an 
</p>
<p>equation among kets such as 
</p>
<p>a!V) =b! ~V) +c!Z)+ &middot; &middot; &middot; (1.3.10) 
</p>
<p>this implies another one among the corresponding bras: 
</p>
<p>(VIa*= ( W!b* + (Zic* + &middot; &middot; &middot; (1.3.11) 
</p>
<p>The two equations above are said to be a(ljoints of each other . .Just as any equation 
</p>
<p>involving complex numbers implies another obtained by taking the complex conju-
</p>
<p>gates of both sides, an equation between (bras) kets implies another one between 
(kets) bras. If you think in a basis, you will see that this follows simply from the 
</p>
<p>fact that if two columns are equal, so are their transpose conjugates. 
</p>
<p>Here is the rule for taking the adjoint: 
</p>
<p>13 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>14 
</p>
<p>CHAPTER I 
</p>
<p>To take the adjoint of a linear equation relating kets (bras), replace every ket 
(bra) by its bra (ket) and complex conjugate all coefficients. 
</p>
<p>We can extend this rule as follows. Suppose we have an expansion for a vector: 
</p>
<p>IV)= I V;li) ( 1.3.12) 
i=1 
</p>
<p>in terms of basis vectors. The adjoint is 
</p>
<p>(VI= I (ilv7 
i=J 
</p>
<p>Recalling that v;= (il V) and v7 =&lt;VIi), it follows that the adjoint of 
</p>
<p>IV)= I li)(iiV) (1.3.13) 
i=! 
</p>
<p>is 
</p>
<p>(VI= I (VIi)(il ( 1.3.14) 
i=] 
</p>
<p>from which comes the rule: 
</p>
<p>To take the adjoint of an equation involving bras and kets and coefficients, 
reverse the order of all factors, exchanging bras and kets and complex conjugating 
all coefficients. 
</p>
<p>Gram-Schmidt Theorem 
</p>
<p>Let us now take up the Gram-Schmidt procedure for converting a linearly 
independent basis into an orthonormal one. The basic idea can be seen by a simple 
example. Imagine the two-dimensional space of arrows in a plane. Let us take two 
nonparallel vectors, which qualify as a basis. To get an orthonormal basis out of 
these, we do the following: 
</p>
<p>&bull; Rescale the first by its own length, so it becomes a unit vector. This will be the 
first basis vector. 
</p>
<p>&bull; Subtract from the second vector its projection along the first, leaving behind only 
the part perpendicular to the first. (Such a part will remain since by assumption 
the vectors are nonparallel.) 
</p>
<p>&bull; Rescale the left over piece by its own length. We now have the second basis vector: 
it is orthogonal to the first and of unit length. 
</p>
<p>This simple example tells the whole story behind this procedure, which will now 
be discussed in general terms in the Dirac notation. </p>
<p/>
</div>
<div class="page"><p/>
<p>Let II), Ill),... be a linearly independent basis. The first vector of the 
</p>
<p>orthonormal basis will be 
</p>
<p>Clearly 
</p>
<p>II)=~ where Ill =J&lt;IIl&gt; 
III 
</p>
<p>As for the second vector in the basis, consider 
</p>
<p>12') =III&gt; -II)(lill) 
</p>
<p>which is Ill) minus the part pointing along the first unit vector. (Think of the arrow 
</p>
<p>example as you read on.) Not surprisingly it is orthogonal to the latter: 
</p>
<p>(112') =&lt;I ill)- (lll)(llll) =0 
</p>
<p>We now divide 12') by its norm to get 12) which will be orthogonal to the first and 
</p>
<p>normalized to unity. Finally, consider 
</p>
<p>13') =ill!) -ll)(lllll) -12)(21111) 
</p>
<p>which is orthogonal to both 11) and 12). Dividing by its norm we get 13), the third 
</p>
<p>member of the orthogonal basis. There is nothing new with the generation of the 
</p>
<p>rest of the basis. 
Where did we use the linear independence of the original basis? What if we had 
</p>
<p>started with a linearly dependent basis? Then at some point a vector like 12') or 13') 
</p>
<p>would have vanished, putting a stop to the whole procedure. On the other hand, 
</p>
<p>linear independence will assure us that such a thing will never happen since it amounts 
</p>
<p>to having a nontrivial linear combination of linearly independent vectors that adds 
up the null vector. (Go back to the equations for 12') or 13') and satisfy yourself 
</p>
<p>that these are linear combinations of the old basis vectors.) 
</p>
<p>Exercise 1.3.1. Form an orthonormal basis in two dimensions starting with A=3t+4] 
</p>
<p>and iJ = 2i- 6]. Can you generate another orthonormal basis starting with these two vectors? 
If so, produce another. 
</p>
<p>15 
</p>
<p>MATHEMATICAL 
INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>16 
</p>
<p>CHAPTER I 
</p>
<p>Exercise 1.3.2. Show how to go from the basis 
</p>
<p>to the orthonormal basis 
</p>
<p>12)=[1/~l 
2/JS 
</p>
<p>13)=[-2~{5] 
1/)5 
</p>
<p>When we first learn about dimensionality, we associate it with the number of 
perpendicular directions. In this chapter we defined it in terms of the maximum 
number of linearly independent vectors. The following theorem connects the two 
definitions. 
</p>
<p>Theorem 4. The dimensionality of a space equals n l_, the maximum number of 
mutually orthogonal vectors in it. 
</p>
<p>To show this, first note that any mutually orthogonal set is also linearly indepen-
dent. Suppose we had a linear combination of orthogonal vectors adding up to 
zero. By taking the dot product of both sides with any one member and using the 
orthogonality we can show that the coefficient multiplying that vector had to vanish. 
This can clearly be done for all the coefficients, showing the linear combination is 
trivial. 
</p>
<p>Now n1_ can only be equal to, greater than or lesser than n, the dimensionality 
of the space. The Gram-Schmidt procedure eliminates the last case by explicit con-
struction, while the linear independence of the perpendicular vectors rules out the 
penultimate option. 
</p>
<p>Schwarz and Triangle Inequalities 
</p>
<p>Two powerful theorems apply to any inner product space obeying our axioms: 
</p>
<p>Theorem 5. The Schwarz Inequality 
</p>
<p>I&lt;VI W)l ::;I VII WI (1.3.15) 
</p>
<p>Theorem 6. The Triangle Inequality 
</p>
<p>I V + WI ::; I VI +I WI (1.3.16) 
</p>
<p>The proof of the first will be provided so you can get used to working with bras 
and kets. The second will be left as an exercise. </p>
<p/>
</div>
<div class="page"><p/>
<p>Before proving anything, note that the results are obviously true for arrows: 
</p>
<p>the Schwarz inequality says that the dot product of two vectors cannot exceed the 
product of their lengths and the triangle inequality says that the length of a sum 
cannot exceed the sum of the lengths. This is an example which illustrates the merits 
of thinking of abstract vectors as arrows and guessing what properties they might 
share with arrows. The proof will of course have to rely on just the axioms. 
</p>
<p>To prove the Schwarz inequality, consider axiom (ZIZ)~O applied to 
</p>
<p>We get 
</p>
<p>IZ)=I V)- (WI V) I W) 
IWI 2 
</p>
<p>(ZIZ)=(V- (W/V) WIV- (W/V) W) 
IWI 2 IWI 2 
</p>
<p>=(VI V)- (WI V)(VI W) (~I V)*(WI V? 
IWI 2 IWI 2 
</p>
<p>(WI V)*(WI V)(WI W) 
+~~~~~~~~~ 
</p>
<p>IWI 4 
</p>
<p>( 1.3.17) 
</p>
<p>(1.3.18) 
</p>
<p>where we have used the antilinearity of the inner product with respect to the bra. 
Using 
</p>
<p>(WI V)*=(VI W) 
</p>
<p>we find 
</p>
<p>( 1.3.19) 
</p>
<p>Cross-multiplying by I Wl 2 and taking square roots, the result follows. 
</p>
<p>Exercise 1.3.3. When will this equality be satisfied? Does this agree with your experience 
</p>
<p>with arrows? 
</p>
<p>Exercise 1.3.4. Prove the triangle inequality starting with IV+ Wl 2 . You must use 
</p>
<p>Re( VI W)::;; I (VI W)l and the Schwarz inequality. Show that the final inequality becomes an 
</p>
<p>equality only if IV)= al W) where a is a real positive scalar. 
</p>
<p>1.4. Subspaces 
</p>
<p>Definition 11. Given a vector space V, a subset of its elements that form a 
vector space among themselvest is called a subspace. We will denote a particular 
subspace i of dimensionality n; by V7'. 
</p>
<p>t Vector addition and scalar multiplication are defined the same way in the subspace as in V. 
</p>
<p>17 
</p>
<p>MATHEMATICAL 
INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>18 
</p>
<p>CHAPTER I 
</p>
<p>Example 1.4.1. In the space V3(R), the following are some examples of sub-
spaces: (a) all vectors along the x axis, the space V 1 ; (b) all vectors along the y 
axis, the space V}; (c) all vectors in the x- y plane, the space V ;Y. Notice that all 
subspaces contain the null vector and that each vector is accompanied by its inverse 
to fulfill axioms for a vector space. Thus the set of all vectors along the positive x 
axis alone do not form a vector space. 0 
</p>
<p>Definition 12. Given two subspaces V7' and \lji, we define their sum 
V7'Ee Vji = Vf:k as the set containing ( 1) all elements of Vi', (2) all elements of 
Vji, (3) all possible linear combinations of the above. But for the elements (3), 
closure would be lost. 
</p>
<p>Example 1.4.2. If, for example, V~EBV; contained only vectors along the x 
and y axes, we could, by adding two elements, one from each direction, generate 
one along neither. On the other hand, if we also included all linear combinations, 
we would get the correct answer, V! EB V; = V~y. 0 
</p>
<p>Exercise 1.4.1. * In a space \r, prove that the set of all vectors {I Vi), I Vi), ... }, 
orthogonal to any IV) '# 0), form a subspace vn-l. 
</p>
<p>Exercise 1.4.2. Suppose v;'' and V;' are two subspaces such that any element of V1 is 
orthogonal to any element of V2 . Show that the dimensionality of V1Ef)V2 is n, +nz. (Hint: 
Theorem 4.) 
</p>
<p>l.S. Linear Operators 
</p>
<p>An operator n is an instruction for transforming any given vector IV) into 
another, IV'). The action of the operator is represented as follows: 
</p>
<p>OIV)=IV'&gt; (1.5.1) 
</p>
<p>One says that the operator n has transformed the ket IV) into the ket IV'). We 
will restrict our attention throughout to operators n that do not take us out of the 
vector space, i.e., if IV) is an element of a space V, so is IV')= 01 V). 
</p>
<p>Operators can also act on bras : 
</p>
<p>(V'IO=(V"I (1.5.2) 
</p>
<p>We will only be concerned with linear operators, i.e., ones that obey the following 
rules: 
</p>
<p>Qal V,) = aOI V,) 
</p>
<p>n{al V,)+ /31 Jj)} =aOI V,)+ /301 Jj) 
</p>
<p>(V1IaO=(V;jQa 
</p>
<p>((V,Ia + &lt;l'flf3)0=a(V;j0+ P&lt;l'JIO 
</p>
<p>(l.5.3a) 
</p>
<p>(1.5.3b) 
</p>
<p>(l.5.4a) 
</p>
<p>(l.5.4b) </p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 1.3. Action of t:~e operator R(hi). Note that 
</p>
<p>R[I2)+13)]=Ri2)+RI3) as expected of a linear operator. (We 
</p>
<p>will often refer to R( hi) as R if no confusion is likely.) 
</p>
<p>R 
</p>
<p>13) 12)+13) 
</p>
<p>Example 1.5.1. The simplest operator is the identity operator, I, which carries 
</p>
<p>the instruction: 
</p>
<p>I--&gt; Leave the vector alone! 
</p>
<p>Thus, 
</p>
<p>II V) =IV) for all kets IV) ( 1.5.5) 
</p>
<p>and 
</p>
<p>(VII=(VI for all bras (VI ( 1.5.6) 
</p>
<p>We next pass on to a more interesting operator on w'(R): 
</p>
<p>R(~ ni)-&gt; Rotate vector by~ n about the unit vector i 
</p>
<p>[More generally, R(O) stands for a rotation by an angle e = 101 about the axis parallel 
to the unit vector e = e I e.] Let us consider the action of this operator on the three 
unit vectors i, ~.and k, which in our notation will be denoted by 11 ), 12), and 13) 
</p>
<p>(see Fig. 1.3). From the figure it is clear that 
</p>
<p>R(~ni)ll)=ll) (1.5. 7a) 
</p>
<p>R(h012)=13) ( 1.5.7b) 
</p>
<p>R(~ ni) 13) = -12) (l.5.7c) 
</p>
<p>Clearly R(~ni) is linear. For instance, it is clear from the same figure that 
</p>
<p>R[I2)+13)]=Ri2)+RI3). D 
</p>
<p>The nice feature of linear operators is that once their action on the basis vectors 
</p>
<p>is known, their action on any vector in the space is determined. If 
</p>
<p>Oli) =It') 
</p>
<p>for a basis II), 12), ... , In) in 1,r, then for any I V)=I v;li) 
</p>
<p>(1.5.8) 
</p>
<p>19 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>20 
</p>
<p>CHAPTER I 
</p>
<p>This is the case in the example n = R(h'i ). If 
</p>
<p>is any vector, then 
</p>
<p>The product ol two operators stands for the instruction that the instruction~ 
</p>
<p>corresponding to the two operators be carried out in sequence 
</p>
<p>AOI =A(QI V) )=AIOV) (1.5.9) 
</p>
<p>where i Q V) is the ket obtained by the action of Q on I v"). The order of the operators 
in a product is very important: in general, 
</p>
<p>QA-AO=:[O, A] 
</p>
<p>called the commutator of Q and A isn't zero. For example R(~ rri) and R(~ rrj) do 
</p>
<p>not commute, i.e., their commutator is nonzero. 
</p>
<p>Two useful identities involving commutators are 
</p>
<p>[Q, A8] = J\[0, 8] + [Q, A]O (1.5.10) 
</p>
<p>[AQ, 8] = J\[0, 8] +[A, 8]0 (1.5.11) 
</p>
<p>Notice that apart from the emphasis on ordering, these rules resemble the chain rule 
</p>
<p>in calculus for the derivative of a product. 
The inverse of Q, denoted by !:T 1, satisfies:!: 
</p>
<p>(1.5.12) 
</p>
<p>Not every operator has an inverse. The condition for the existence of the inverse is 
given in Appendix A. L The operator Rd Jri) has an inverse: it is R( -~ Jri ). The 
inverse of a product of operators is the product of the inverses in reverse: 
</p>
<p>(1.5.13) 
</p>
<p>for only then do we have 
</p>
<p>(QA)(OA)- 1 = (QA)(A &middot;In&middot;&middot;&middot; I) =OAA&middot;&middot;&middot;Io&middot;&middot;&middot;&middot;l =no&middot;&middot;&middot; I =1 
</p>
<p>1.6. Matrix Elements of Linear Operators 
</p>
<p>\Ve are now accustomed to the idea of an abstract vector being represented in 
a basis by an n-tuple of numbers, called its components, in terms of which all vector 
</p>
<p>~In 'V"(C) with n finite, D 1 D~f.,,,.[2f2 1 =/. Prove this using the ideas introduced toward the end of 
</p>
<p>Theorem A.l.l., Appendix A.l. </p>
<p/>
</div>
<div class="page"><p/>
<p>operations can be carried out. We shall now see that in the same manner a linear 
</p>
<p>operator can be represented in a basis by a set of n2 numbers, written as an n x n 
</p>
<p>matrix, and called its matrix elements in that basis. Although the matrix elements, 
</p>
<p>just like the vector components, are basis dependent, they facilitate the computation 
</p>
<p>of all basis-independent quantities, by rendering the abstract operator more tangible. 
</p>
<p>Our starting point is the observation made earlier, that the action of a linear 
</p>
<p>operator is fully specified by its action on the basis vectors. If the basis vectors suffer 
</p>
<p>a change 
</p>
<p>!lli)= li') 
</p>
<p>(where I i') is known), then any vector in this space undergoes a change that is readily 
calculable: 
</p>
<p>!ll V)=Q L v;li)=L v;!lli)=L vdi') 
; i i 
</p>
<p>When we say I i') is known, we mean that its components in the original basis 
</p>
<p>(1.6.1) 
</p>
<p>are known. The n2 numbers, Q!'i, are the matrix elements of Q in this basis. If 
</p>
<p>!liV)=IV'&gt; 
</p>
<p>then the components of the transformed ket I V') are expressable in terms of the nij 
and the components of I V): 
</p>
<p>v; = (il V') = (il!ll V) = (iln(~ vjlj)) 
</p>
<p>=L Vj(il!llj) 
j 
</p>
<p>Equation ( 1.6.2) can be cast in matrix form: 
</p>
<p>[
</p>
<p>v;] [(11!lll) (ll!ll2) 
v2 = (21!ll1) . . . . . . 
v~ (nl!lll) 
</p>
<p>&middot; &middot; &middot; (llfiln)][~] 
(nl!lln) Vn 
</p>
<p>(1.6.2) 
</p>
<p>(1.6.3) 
</p>
<p>A mnemonic: the elements of the first column are simply the components of the first 
</p>
<p>transformed basis vector 11 ') = !lll) in the given basis. Likewise, the elements of the 
</p>
<p>jth column represent the image of the jth basis vector after n acts on it. 
</p>
<p>21 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>22 
</p>
<p>CHAPTER I 
</p>
<p>Convince yourself that the same matrix flu acting to the left on the row vector 
</p>
<p>corresponding to any ( v'l gives the row vector corresponding to ( v"l = ( v'l Q. 
</p>
<p>Example 1.6.1. Combining our mnemonic with the fact that the operator R(~ ni) 
has the following effect on the basis vectors: 
</p>
<p>R(jni)ll) =II) 
</p>
<p>R(jni)l2)=1 
</p>
<p>R(~ni)i3) = -12) 
</p>
<p>we can write down the matrix that represents it in the 11), 12), 13) basis: 
</p>
<p>R(&plusmn;ni)&bull;---&middot;[~ ~ -~] 
_0 1 0 
</p>
<p>( L6.4) 
</p>
<p>For instance, the ----1 in the third column tells us that R rotates 13) into -12). One 
</p>
<p>may also ignore the mnemonic altogether and simply use the definition Ru= (il RIJ) 
</p>
<p>to compute the matrix. D 
</p>
<p>Exercise 1.6. 1. An operator n is given by the matrix 
</p>
<p>What is its action'1 
</p>
<p>Let us now consider certain specific operators and see how they appear in matrix 
</p>
<p>form. 
</p>
<p>(1) The Identity Operator L 
</p>
<p>fu= (illiJ) = (iiJ) = ou ( 1.6.5) 
</p>
<p>Thus I is represented by a diagonal matrix with I 's along the diagonal. You should 
verify that our mnemonic gives the same result. 
</p>
<p>(2) The Projection Operators. Let us first get acquainted with projection opera-
</p>
<p>tors. Consider the expansion of an arbitrary kct IV) in a basis: 
</p>
<p>IV)= I li)(iJV) 
i=]_ </p>
<p/>
</div>
<div class="page"><p/>
<p>In terms of the objects I i)(il, which are linear operators, and which, by definition, 
</p>
<p>act on IV) to give li)(il V), we may write the above as 
</p>
<p>( 1.6.6) 
</p>
<p>Since Eq. ( 1.6.6) is true for all IV), the object in the brackets must be identified 
</p>
<p>with the identity (operator) 
</p>
<p>I= I li)(il =I IP'; ( 1.6. 7) 
i=J jooo ( 
</p>
<p>The object IP',= I i)(i I is called the projection operator for the ket I i). Equation ( 1.6.7), 
which is called the completeness relation, expresses the identity as a sum over projec-
</p>
<p>tion operators and will be invaluable to us. (If you think that any time spent on the 
identity, which seems to do nothing, is a waste of time, just wait and see.) 
</p>
<p>Consider 
</p>
<p>( 1.6.8) 
</p>
<p>Clearly IP', is linear. Notice that whatever IV) is, IP';I V) is a multiple of if) with 
</p>
<p>a coefficient (v;) which is the component of IV) along li). Since P; projects out the 
</p>
<p>component of any ket I V) along the direction I it is called a projection operator. 
The completeness relation, Eq. ( 1.6. 7), says that the sum of the projections of a 
</p>
<p>vector along all the n directions equals the vector itself. Projection operators can 
</p>
<p>also act on bras in the same way: 
</p>
<p>( 1.6.9) 
</p>
<p>Projection operators corresponding to the basis vectors obey 
</p>
<p>(1.6.10) 
</p>
<p>This equation tells us that (1) once IP' 1 projects out the part of IV) along !i), further 
applications of IP'; make no difference; and (2) the subsequent application of IP';(j of i) 
will result in zero, since a vector entirely along I i) cannot have a projection along a 
perpendicular direction IJ). 
</p>
<p>23 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>24 
</p>
<p>CHAPTER l 
</p>
<p>Figure 1.4. P, and P, are polarizers placed in the way of a beam traveling along the z axis. The action 
of the polarizers on the electric field E obeys the law of combination of projection operators: 
</p>
<p>P,P1=i5uP1. 
</p>
<p>The following example from optics may throw some light on the discussion. 
</p>
<p>Consider a beam of light traveling along the z axis and polarized in the x ---- y plane 
at an angle e with respect to the y axis (see Fig. 1.4). If a polarizer P,, that only 
admits light polarized along they axis, is placed in the way, the projection E cos 8 
</p>
<p>along the y axis is transmitted. An additional polarizer P, placed in the way has no 
</p>
<p>further effect on the beam. We may equate the action of the polarizer to that of a 
</p>
<p>projection operator ~DY that acts on the electric field vector E. If P,. is followed by a 
</p>
<p>polarizer P, the beam is completely blocked. Thus the polarizers obey the equation 
</p>
<p>P1P;= 8uP; expected of projection operators. 
</p>
<p>Let us next turn to the matrix elements of !P'i. There are two approaches. The 
</p>
<p>first one, somewhat indirect, gives us a feeling for what kind of an object I i) &lt; i is. 
We know 
</p>
<p>and 
</p>
<p>i) &lt;---+ 
</p>
<p>0 
</p>
<p>0 
</p>
<p>0 
</p>
<p>0 
</p>
<p>(i i ._ [0, 0, ...&bull; I, 0, O &bull;... , OJ </p>
<p/>
</div>
<div class="page"><p/>
<p>so that 
</p>
<p>0 0 0 
</p>
<p>0 
</p>
<p>0 
</p>
<p>li)(il &lt;-t 1 [0,0, ... , 1,0, ... ,0]= (1.6.11) 
</p>
<p>0 0 
</p>
<p>0 0 0 
</p>
<p>by the rules of matrix multiplication. Whereas &lt;VI V') = (1 x n matrix) x 
</p>
<p>(n x 1 matrix)= (1 x 1 matrix) is a scalar, I V)( V'l = (n x 1 matrix) x (1 x n matrix)= 
</p>
<p>(n x n matrix) is an operator. The inner product &lt;VI V') represents a bra and ket 
</p>
<p>which have found each other, while I V)(V'I, sometimes called the outer product, 
</p>
<p>has the two factors looking the other way for a bra or a ket to dot with. 
</p>
<p>The more direct approach to the matrix elements gives 
</p>
<p>( 1.6.12) 
</p>
<p>which is of course identical to Eq. (1.6.11 ). The same result also follows from mne-
</p>
<p>monic. Each projection operator has only one nonvanishing matrix element, a 1 at 
</p>
<p>the ith element on the diagonal. The completeness relation, Eq. (1.6.7), says that 
</p>
<p>when all the IP; are added, the diagonal fills out to give the identity. If we form the 
</p>
<p>sum over just some of the projection operators, we get the operator which projects 
</p>
<p>a given vector into the subspace spanned by just the corresponding basis vectors. 
</p>
<p>Matrices Corresponding to Products of Operators 
</p>
<p>Consider next the matrices representing a product of operators. These are related 
</p>
<p>to the matrices representing the individual operators by the application ofEq. (1.6.7): 
</p>
<p>(QA)u= (iiQAij) = (iiQIAij) 
</p>
<p>=L: (iiOik)(kiAIJ)=L; O;kAkj ( 1.6.13) 
k k 
</p>
<p>Thus the matrix representing the product of operators is the product of the matrices 
</p>
<p>representing the factors. 
</p>
<p>The Adjoint of an Operator 
</p>
<p>Recall that given a ket al V) =I a V) the corresponding bra is 
</p>
<p>(a VI= (Via* (and not (Via) 
</p>
<p>25 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>26 
</p>
<p>CHAPTER I 
</p>
<p>In the same way, given a ket 
</p>
<p>Ql V)= IQV) 
</p>
<p>the corresponding bra is 
</p>
<p>(1.6.14) 
</p>
<p>which defines the operator (l. One may state this equation in words: if Q turns a 
ket IV) to IV'), then n+ turns the bra (VI into ( V'l. Just as a and a*, IV) and 
(VI are related but distinct objects, so are Q and Qt. The relation between Q, and 
nt, called the adjoint of Q or "omega dagger," is best seen in a basis: 
</p>
<p>so 
</p>
<p>(Qt);j= (iiQtjj) = (Qilj) 
</p>
<p>= (jjQi) * = (jiOii) * 
</p>
<p>(1.6.15) 
</p>
<p>In other words, the matrix representing nt is the transpose conjugate of the matrix 
representing Q. (Recall that the row vector representing (VI is the transpose conju-
gate of the column vector representing IV). In a given basis, the adjoint operation is 
the same as taking the transpose conjugate.) 
</p>
<p>The adjoint of a product is the product of the adjoints in reverse: 
</p>
<p>(1.6.16) 
</p>
<p>To prove this we consider (QA VI. First we treat QA as one operator and get 
</p>
<p>(QAVI = ((QA)VI = (Vj(QA)t 
</p>
<p>Next we treat (AV) as just another vector, and write 
</p>
<p>(QAVI =(Q(AV)I =(AVIQ+ 
</p>
<p>We next pull out A, pushing nt further out: 
</p>
<p>Comparing this result with the one obtained a few lines above, we get the desired 
result. 
</p>
<p>Consider now an equation consisting of kets, scalars, and operators, such as 
</p>
<p>( 1.6.17a) </p>
<p/>
</div>
<div class="page"><p/>
<p>What is its adjoint? Our old rule tells us that it is 
</p>
<p>In the last term we can replace (QA V61 by 
</p>
<p>so that finally we have the adjoint of Eq. (1.6.17a): 
</p>
<p>(1.6.17b) 
</p>
<p>The final rule for taking the adjoint of the most general equation we will ever 
</p>
<p>encounter is this: 
</p>
<p>When a product of operators, bras, kets, and explicit numerical coefficients is 
</p>
<p>encountered, reverse the order of all factors and make the substitutions Q +--&gt; Q +, 
I)+--&gt; (I, a+--&gt; a*. 
</p>
<p>(Of course, there is no real need to reverse the location of the scalars a except in 
</p>
<p>the interest of unifcrmity.) 
</p>
<p>Hermitian, Anti-Hermitian, and Unitary Operators 
</p>
<p>We now turn our attention to certain special classes of operators that will play 
a major role in quantum mechanics. 
</p>
<p>Definition 13. An operator Q is Hermitian if nt = n. 
</p>
<p>Definition 14. An operator Q is anti-Hermitian if nt = -Q. 
</p>
<p>The adjoint is to an operator what the complex conjugate is to numbers. Hermitian 
and anti-Hermitian operators are like pure real and pure imaginary numbers. Just 
</p>
<p>as every number may be decomposed into a sum of pure real and pure imaginary 
parts, 
</p>
<p>a+a* a-a* 
a=---+---
</p>
<p>2 2 
</p>
<p>we can decompose every operator into its Hermitian and anti-Hermitian parts: 
</p>
<p>(1.6.18) 
</p>
<p>Exercise 1.6.2. * Given Q and A are Hermitian what can you say about (I) QA; (2) 
QA+AQ; (3) [Q, A]; and (4) i[Q, A]? 
</p>
<p>27 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>28 
</p>
<p>CHAPTER I 
</p>
<p>and 
</p>
<p>Definition 15. An operator U is unitary if 
</p>
<p>uut=I (1.6.19) 
</p>
<p>This equation tells us that U and ut are inverses of each other. Consequently, 
from Eq. (1.5.12), 
</p>
<p>( 1.6.20) 
</p>
<p>Following the analogy between operators and numbers, unitary operators are 
</p>
<p>like complex numbers of unit modulus, u = e;6 . Just as u* u = 1, so is ut U =I. 
</p>
<p>Exercise 1.6.3. * Show that a product of unitary operators is unitary. 
</p>
<p>Theorem 7. Unitary operators preserve the inner product between the vectors 
</p>
<p>they act on. 
</p>
<p>Proof Let 
</p>
<p>I V2)= Ul Vz) 
</p>
<p>Then 
</p>
<p>( V21 Vi)= ( UVzl UV,) 
</p>
<p>= (Vzl UtUI V,) = (Vzl V,) (1.6.21) 
</p>
<p>(Q.E.D.) 
</p>
<p>Unitary operators are the generalizations of rotation operators from W3(R) to 
</p>
<p>Wn( C), for just like rotation operators in three dimensions, they preserve the lengths 
</p>
<p>of vectors and their dot products. In fact, on a real vector space, the unitarity 
</p>
<p>condition becomes u-' = ur (T means transpose), which defines an orthogonal or 
rotation matrix. [R(hi) is an example.] 
</p>
<p>Theorem 8. If one treats the columns of an n x n unitary matrix as components 
</p>
<p>of n vectors, these vectors are orthonormal. In the same way, the rows may be 
</p>
<p>interpreted as components of n orthonormal vectors. 
</p>
<p>Proof 1. According to our mnemonic, thejth column of the matrix representing 
</p>
<p>U is the image of the jth basis vector after U acts on it. Since U preserves inner 
</p>
<p>products, the rotated set of vectors is also orthonormal. Consider next the rows. We 
</p>
<p>now use the fact that ut is also a rotation. (How else can it neutralize U to give 
ut U =I?) Since the rows of U are the columns of ut (but for an overall complex </p>
<p/>
</div>
<div class="page"><p/>
<p>conjugation which does not affect the question of orthonormality), the result we 
</p>
<p>already have for the columns of a unitary matrix tells us the rows of U are 
</p>
<p>orthonormal. 
</p>
<p>Proof 2. Since ut U =I, 
</p>
<p>oij= (illiJ) = (il utUIJ) 
</p>
<p>= L (il Utlk)(kl UIJ) 
k 
</p>
<p>= L: u)~ukj= L: ut;ukj 
k k 
</p>
<p>( 1.6.22) 
</p>
<p>which proves the theorem for the columns. A similar result for the rows follows if 
we start with the equation uut =I. Q.E.D. 
</p>
<p>Note that ut U =I and uut =I are not independent conditions. 
</p>
<p>Exercise 1.6.4. * It is assumed that you know (I) what a determinant is. (2) that det n 7 = 
det n ( T denotes transpose), (3) that the determinant of a product of matrices is the product 
of the determinants. [If you do not, verify these properties for a two-dimensional case 
</p>
<p>with det n = (a 8 - f1 y ).] Prove that the determinant of a unitary matrix is a complex number 
of unit modulus. 
</p>
<p>Exercise 1.6.5. * Verify that Rd rri) is unitary (orthogonal) by examining its matrix. 
</p>
<p>Exercise 1.6.6. Verify that the following matrices are unitary: 
</p>
<p>1Jl+i 1-iJ 
2[)-i Hi 
</p>
<p>Verify that the determinant is of the form e'" in each case. Are any of the above matrices 
Hermitian? 
</p>
<p>1. 7. Active and Passive Transformations 
</p>
<p>Suppose we subject all the vectors I V) in a space to a unitary transformation 
</p>
<p>I V)_,.UI V) (1.7.1) 
</p>
<p>Under this transformation, the matrix elements of any operator Q are modified as 
follows: 
</p>
<p>( V'IOI V)-&gt;( UV'iOI UV) = ( V'l UtQUI V) ( 1.7.2) 
</p>
<p>29 
</p>
<p>MATHEMATICAL 
INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>30 
</p>
<p>CHAPTER 1 
</p>
<p>It is dear that the same change would be effected if we left the vectors alone and 
subjected all operators to the change 
</p>
<p>n-.u'nu (1.7.3) 
</p>
<p>The first case is called an active transformation and the second a passive tram:fiJrma-
</p>
<p>tion. The present nomenclature is in reference to the vectors: they are affected in an 
active transfonnation and left alone in the passive case. The situation is exactly the 
</p>
<p>opposite from the point of view of the operators. 
</p>
<p>Later we will see that the physics in quantum theory lies in the matrix elements 
</p>
<p>of operators, and that active and passive transformations provide us with two equiva-
</p>
<p>lent ways of describing the same physical transfom1ation. 
</p>
<p>ExercL1&middot;e I. 7.1. * The trace of a matrix is defined to be the sum of its diagonal matrix 
elements 
</p>
<p>TrH=:Ln,, 
</p>
<p>Show that 
</p>
<p>( 1) Tr(!1A) = Tr(Arl) 
</p>
<p>(2) Tr(!1A8) = Tr(MKl) o Tr( I}HA) (The permutations arc cyclir). 
</p>
<p>(3) The trace of an operator is unaffected by a unitary change of basis li)-&gt;Uii). [Equiva-
</p>
<p>lently, show Tr !1 = Tr( U'Hl!).] 
</p>
<p>Exercise 1. 7.2. Show that the determinant of a matrix is unaffected by a unitary change 
</p>
<p>of basis. [Equivalently show det D = det( U+WJ).] 
</p>
<p>1.8. The Eigenvalue Problem 
</p>
<p>Consider some linear operator Q acting on an arbitrary nonzero ket I V): 
</p>
<p>01 V)=i V') (1.8.1) 
</p>
<p>Unless the operator happens to be a trivial one, such as the identity or its multiple, 
</p>
<p>the ket will suffer a nontrivial change, i.e., IV') will not be simply related to IV). 
So much for an arbitrary ket. Each operator, however, has certain kets of its own, 
</p>
<p>called its eigenkets, 0!1 which its action is simply that of rescaling: 
</p>
<p>fll V)=wl V) (1.8.2) 
</p>
<p>Equation ( 1.8.2) is an eigenvalue equation: I V) is an eigenket of Q with eigenvalue 
w. In this chapter we will see how, given an operator Q, one can systematically 
determine all its eigenvalues and eigenvectors. How such an equation enters physics 
will be illustrated by a few examples from mechanics at the end of this section, and 
once we get to quantum mtxhanics proper, it will be eigen, eigen, eigen all the way. </p>
<p/>
</div>
<div class="page"><p/>
<p>Example 1.8.1. To illustrate how easy the eigenvalue problem really is, we will 
begin with a case that will be completely solved: the case 0 =I. Since 
</p>
<p>IIV)=IV) 
</p>
<p>for alii V), we conclude that 
</p>
<p>( 1) the only eigenvalue of I is 1 ; 
(2) all vectors are its eigenvectors with this eigenvalue. 0 
</p>
<p>Example 1.8.2. After this unqualified success, we are encouraged to take on a 
slightly more difficult case: Q = IP v, the projection operator associated with a normal-
ized ket IV). Clearly 
</p>
<p>(1) any ket a 1 V) = 1 a V), parallel to I V) is an eigenket with eigenvalue 1 : 
</p>
<p>IP&gt;vla V) =I V)&lt;VIa V) = al V)l Vl 2 = l&middot;la V) 
</p>
<p>(2) any ket I V.1), perpendicular to IV), is an eigenket with eigenvalue 0: 
</p>
<p>(3) kets that are neither, i.e., kets of the form al V) + {31 V .L), are simply not 
eigenkets: 
</p>
<p>IP&gt;v(al V) + ,81 V1)) =I a V) =I y(al V) + ,81 V.1)) 
</p>
<p>Since every ket in the space falls into one of the above classes, we have found 
all the eigenvalues and eigenvectors. 0 
</p>
<p>Example 1.8.3. Consider now the operator R(!JTi}. We already know that it 
has one eigenket, the basis vector II ) along the x axis: 
</p>
<p>R(~lii)ll)=jl) 
</p>
<p>Are there others? Of course, any vector all) along the x axis is also unaffected by 
the x rotation. This is a general feature of the eigenvalue equation and reflects the 
linearity of the operator: 
</p>
<p>if 
</p>
<p>01 V) =(I) IV) 
</p>
<p>then 
</p>
<p>Oal V) = aOI V) = a(l)j V) = r.oal V) 
</p>
<p>31 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCfiON </p>
<p/>
</div>
<div class="page"><p/>
<p>32 
</p>
<p>CHAPTER I 
</p>
<p>for any multiple a. Since the eigenvalue equation fixes the eigenvector only up to 
an overall scale factor, we will not treat the multiples of an eigenvector as distinct 
eigenvectors. With this understanding in mind, let us ask if R(~ni) has any eigenvec-
tors besides II). Our intuition says no, for any vector not along the x axis necessarily 
gets rotated by R(~ ni) and cannot possibly transform into a multiple of itself. Since 
every vector is either parallel to II) or isn't, we have fully solved the eigenvalue 
problem. 
</p>
<p>The trouble with this conclusion is that it is wrong! R(~ ni) has two other 
eigenvectors besides 11 ). But our intuition is not to be blamed, for these vectors are 
in V\ C) and not W3(R). It is clear from this example that we need a reliable and 
systematic method for solving the eigenvalue problem in Wn( C). We now tum our 
attention to this very question. D 
</p>
<p>The Characteristic Equation and the Solution to the Eigenvalue Problem 
</p>
<p>We begin by rewriting Eq. ( 1.8.2) as 
</p>
<p>(Q- col)l V) = 10) 
</p>
<p>Operating both sides with (Q-col)- 1, assuming it exists, we get 
</p>
<p>(1.8.3) 
</p>
<p>( 1.8.4) 
</p>
<p>Now, any finite operator (an operator with finite matrix elements) acting on the null 
vector can only give us a null vector. It therefore seems that in asking for a nonzero 
eigenvector IV), we are trying to get something for nothing out of Eq. (1.8.4). This 
is impossible. It follows that our assumption that the operator (Q- col)- 1 exists (as 
a finite operator) is false. So we ask when this situation will obtain. Basic matrix 
theory tells us (see Appendix A.l) that the inverse of any matrix Misgiven by 
</p>
<p>M_ 1 =cofactor MT 
</p>
<p>det M 
( 1.8.5) 
</p>
<p>Now the cofactor of M is finite if M is. Thus what we need is the vanishing of the 
determinant. The condition for nonzero eigenvectors is therefore 
</p>
<p>det(Q- col)= 0 ( 1.8.6) 
</p>
<p>This equation will determine the eigenvalues co. To find them, we project Eq. (1.8.3) 
onto a basis. Dotting both sides with a basis bra &lt;il, we get 
</p>
<p>&lt;iiQ-co/1 V)=O </p>
<p/>
</div>
<div class="page"><p/>
<p>and upon introducing the representation of the identity [Eq. (1.6.7)], to the left of 
</p>
<p>IV), we get the following image of Eq. (1.8.3): 
</p>
<p>L (Qii-ro8ii)v1=0 
j 
</p>
<p>Setting the determinant to zero will give us an expression of the form 
</p>
<p>Equation ( 1.8.8) is called the characteristic equation and 
</p>
<p>n 
</p>
<p>r(ro)= L Cm(Om 
m=O 
</p>
<p>( 1.8. 7) 
</p>
<p>( 1.8.8) 
</p>
<p>(1.8.9) 
</p>
<p>is called the characteristic polynomial. Although the polynomial is being determined 
</p>
<p>in a particular basis, the eigenvalues, which are its roots, are basis independent, for 
</p>
<p>they are defined by the abstract Eq. (1.8.3), which makes no reference to any basis. 
</p>
<p>Now, a fundamental result in analysis is that every nth-order polynomial has n 
</p>
<p>roots, not necessarily distinct and not necessarily real. Thus every operator in \r(C) 
</p>
<p>has n eigenvalues. Once the eigenvalues are known, the eigenvectors may be found, 
</p>
<p>at least for Hermitian and unitary operators, using a procedure illustrated by the 
</p>
<p>following example. [Operators on \In( C) that are not of the above variety may not 
</p>
<p>haven eigenvectors-see Exercise 1.8.4. Theorems I 0 and 12 establish that Hermitian 
</p>
<p>and unitary operators on vnc c) will have n eigenvectors.] 
</p>
<p>Example 1.8.4. Let us use the general techniques developed above to find all 
</p>
<p>the eigenvectors and eigenvalues of R(! rri). Recall that the matrix representing it is 
</p>
<p>Therefore the characteristic equation is 
</p>
<p>1-ro 0 0 
</p>
<p>det(R-ro/)= 0 -ro -1 =0 
</p>
<p>0 -ro 
</p>
<p>i.e., 
</p>
<p>( 1.8.10) 
</p>
<p>33 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>34 
</p>
<p>CHAPTER I 
</p>
<p>with roots ro = I, &plusmn; i. We know that ro = I corresponds to II). Let us see this come 
out of the formalism. Feeding ro = I into Eq. ( I.8. 7) we find that the components 
Xi , x2 , and x 3 of the corresponding eigenvector must obey the equations 
</p>
<p>0 
</p>
<p>0-1 
</p>
<p>Thus any vector of the form 
</p>
<p>is acceptable, as expected. It is conventional to use the freedom in scale to normalize 
the eigenvectors. Thus in this case a choice is 
</p>
<p>I say a choice, and not the choice, since the vector may be multiplied by a number 
of modulus unity without changing the norm. There is no universally accepted con-
vention for eliminating this freedom, except perhaps to choose the vector with real 
components when possible. 
</p>
<p>Note that of the three simultaneous equations above, the first is not a real 
equation. In general, there will be only (n- I) LI equations. This is the reason the 
norm of the vector is not fixed and, as slwwn in Appendix A.l, the reason the 
determinant vanishes. 
</p>
<p>Consider next the equations corresponding to ro = i. The components of the 
eigenvector obey the equations 
</p>
<p>(1-i)Xi =0 (i.e., xi =0) 
</p>
<p>(i.e., x2 = ix3) 
</p>
<p>Notice once again that we have only n- I useful equations. A properly normalized 
solution to the above is </p>
<p/>
</div>
<div class="page"><p/>
<p>A similar procedure yields the third eigenvector: 
</p>
<p>I co= -i) +-+ - 1. [~i] 
i/2 
</p>
<p>1 
</p>
<p>0 
</p>
<p>In the above example we have introduced a popular convention: labeling the 
</p>
<p>eigenvectors by the eigenvalue. For instance, the ket corresponding to co= co; is 
labeled 1 co= ro 1) or simply I ro1). This notation presumes that to each co1 there is just 
one vector labeled by it. Though this is not always the case, only a slight change in 
this notation will be needed to cover the general case. 
</p>
<p>The phenomenon of a single eigenvalue representing more than one eigenvector 
</p>
<p>is called degeneracy and corresponds to repeated roots for the characteristic poly-
nomial. In the face of degeneracy, we need to modify not just the labeling, but also 
the procedure used in the example above for finding the eigenvectors. Imagine that 
instead of R(~ni) we were dealing with another operator non 'V3(R) with roots co 1 
and ro 2 = co 3 &bull; It appears as if we can get two eigenvectors, by the method described 
above, one for each distinct co. How do we get a third? Or is there no third? These 
questions will be answered in all generality shortly when we examine the question 
of degeneracy in detail. We now turn our attention to two central theorems on 
Hermitian operators. These play a vital role in quantum mechanics. 
</p>
<p>Theorem 9. The eigenvalues of a Hermitian operator are real. 
</p>
<p>Proof Let 
</p>
<p>Dot both sides with &lt;co I : 
</p>
<p>( 1.8.11) 
</p>
<p>Take the adjoint to get 
</p>
<p>Since n = n t, this becomes 
</p>
<p>Subtracting from Eq. (1.8.11) 
</p>
<p>O=(ro-co*)(rolco) 
</p>
<p>ro = ro* Q.E.D. 
</p>
<p>35 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>36 
</p>
<p>CHAPTER I 
</p>
<p>Theorem 10. To every Hermitian operator n, there exists (at least) a basis 
consisting of its orthonormal eigenvectors. It is diagonal in this eigenbasis and 
</p>
<p>has its eigenvalues as its diagonal entries. 
</p>
<p>Proof Let us start with the characteristic equation. It must have at least one 
</p>
<p>root, call it w 1 &bull; Corresponding to (u 1 there must exist at least one nonzero eigenvector 
lm1&gt;- [If not, Theorem (A.l.l) would imply that (Q-w1/) is invertible.] Consider 
</p>
<p>the subspace 'W~ 11 of all vectors orthogonal to I m 1). Let us choose as our basis the 
vector I m1) (normalized to unity) and any n ----&middot;1 orthonormal vectors 
{vi,' V~t' ... ' V";; 1 } in I I. In this basis Q has the following fom1: 
</p>
<p>W1 0 0 0 0 '.. 0 
</p>
<p>0 
</p>
<p>n.-. o 
</p>
<p>0 
</p>
<p>(1.8.12) 
</p>
<p>The first column is just the image of lw,) after Q has acted on it. Given the 
first column, the first row follows from the Hermiticity of n. 
</p>
<p>The characteristic equation now takes the form 
</p>
<p>i m1- OJ)&middot; (determinant of boxed submatrix) = 0 
</p>
<p>1/-! 
</p>
<p>(w~-----m) I cmw"'=(w~&middot;----w)P"- 1 (w)=O 
0 
</p>
<p>Now the polynomial P'' &middot;&middot; 1 must also generate one root, m2 , and a normalized 
</p>
<p>eigenvector lm2). Define the subspace 'W'li.~ of vectors in 1 1 orthogonal to fw 2) 
(and automatically to I Wt)) and repeat the same procedure as before. Finally, the 
matrix n becomes, in the basis fro,), lm,), ... , 1 
</p>
<p>OJ] 0 0 
</p>
<p>,:J 
0 OJ? 0 
</p>
<p>n ...... 0 0 Oh 
</p>
<p>0 0 0 
</p>
<p>Since every I w,) was chosen from a space that was orthogonal to the previous 
ones, I m 1 ), I w 2 ), .&bull;. , I w,_ 1); the basis of eigenvectors is orthonormaL (Notice that 
nowhere did we have to assume that the eigenvalues were all distinct.) Q.E.D. 
</p>
<p>[The analogy between real numbers and Hermitian operators is further strength--
</p>
<p>ened by the fact that in a certain basis (of eigenvectors) the Hermitian operator can 
be represented by a matrix with all real. elements.] 
</p>
<p>In stating Theorem 10, it was indicated that there might exist more than one 
basis of eigenvectors that diagonalized n. This happens if there is any degeneracy. 
Suppose m1 = m2 = (J). Then we have two orthonormal vectors obeying </p>
<p/>
</div>
<div class="page"><p/>
<p>It follows that 
</p>
<p>for any a and [3. Since the vectors lw 1) and loh) are orthogonal (and hence LI), 
we find that there is a whole two-dimensional subspace spanned by m,) and 1m2), 
the elements of which are eigenvectors of Q with eigenvalue m. One refers to this 
space as an eigenspace of Q with eigenvalue w. Besides the vectors I w 1) and I m2), 
there exists an infinity of orthonormal pairs lm!), lm;), obtained by a rigid rotation 
of I w 1 ), I m2 ), from which we may select any pair in forming the eigenbasis of n. 
In general, if an eigenvalue occurs m 1 times, that is, if the characteristic equation has 
</p>
<p>m; of its roots equal to some w 1, there will be an eigenspace w;~: from which we may 
choose any m; orthonormal vectors to form the basis referred to in Theorem 10. 
</p>
<p>In the absence of degeneracy, we can prove Theorem 9 and 10 very easily. Let 
</p>
<p>us begin with two eigenvectors: 
</p>
<p>(1.8.13a) 
</p>
<p>( 1.8.13b) 
</p>
<p>Dotting the first with &lt; m; I and the second with &lt; w;!, we get 
</p>
<p>( 1.8.14a) 
</p>
<p>( 1.8.14b) 
</p>
<p>Taking the adjoint of the last equation and using the Hermitian nature of Q, we get 
</p>
<p>Subtracting this equation from Eq. (1.8.14a), we get 
</p>
<p>(1.8.15) 
</p>
<p>If i=j, we get, since (m1 lm;)#O, 
</p>
<p>(1.8.16) 
</p>
<p>37 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>38 
</p>
<p>CHAPTER I 
</p>
<p>If i#-j, we get 
</p>
<p>(1.8.17) 
</p>
<p>since mi- mf = mi- mj#-0 by assumption. That the proof of orthogonality breaks 
</p>
<p>down form;= mj is not surprising, for two vectors labeled by a degenerated eigenvalue 
</p>
<p>could be any two members of the degenerate space which need not necessarily be 
</p>
<p>orthogonal. The modification of this proof in this case of degeneracy calls for argu-
</p>
<p>ments that are essentially the ones used in proving Theorem 10. The advantage in 
</p>
<p>the way Theorem 10 was proved first is that it suffers no modification in the degener-
</p>
<p>ate case. 
</p>
<p>Degeneracy 
</p>
<p>We now address the question of degeneracy as promised earlier. Now, our 
</p>
<p>general analysis of Theorem 10 showed us that in the face of degeneracy, we have 
</p>
<p>not one, but an infinity of orthonormal eigenbases. Let us see through an example 
</p>
<p>how this variety manifests itself when we look for eigenvectors and how it is to be 
</p>
<p>handled. 
</p>
<p>Example 1.8.5. Consider an operator Q with matrix elements 
</p>
<p>in some basis. The characteristic equation is 
</p>
<p>i.e., 
</p>
<p>m=O, 2, 2 
</p>
<p>The vector corresponding to m = 0 is found by the usual means to be 
</p>
<p>The case m = 2 leads to the following equations for the components of the 
</p>
<p>eigenvector: 
</p>
<p>0=0 </p>
<p/>
</div>
<div class="page"><p/>
<p>Now we have just one equation, instead of the two (n- 1) we have grown accustomed 
</p>
<p>to! This is a reflection of the degeneracy. For every extra appearance (besides the 
</p>
<p>first) a root makes, it takes away one equation. Thus degeneracy permits us extra 
</p>
<p>degrees of freedom besides the usual one (of normalization). The conditions 
</p>
<p>x 2 arbitrary 
</p>
<p>define an ensemble of vectors that are perpendicular to the first, I w = 0), i.e., lie in 
</p>
<p>a plane perpendicular to I w = 0). This is in agreement with our expectation that a 
twofold degeneracy should lead to a two-dimensional eigenspace. The freedom in X2 
</p>
<p>(or more precisely, the ratio x2/x3) corresponds to the freedom of orientation in this 
</p>
<p>plane. Let us arbitrarily choose x 2 = 1, to get a normalized eigenvector corresponding 
</p>
<p>to w = 2: 
</p>
<p>The third vector is now chosen to lie in this plane and to be orthogonal to the second 
</p>
<p>(being in this plane automatically makes it perpendicular to the first I w = 0)): 
</p>
<p>Clearly each distinct choice of the ratio, x2/x3 , gives us a distinct doublet of orthonor-
</p>
<p>mal eigenvectors with eigenvalue 2. D 
</p>
<p>Notice that in the face of degeneracy, I W;) no longer refers to a single ket but 
to a generic element of the eigenspace w::;;. To refer to a particular element, we must 
use the symbol I W;, a), where a labels the ket within the eigenspace. A natural 
choice of the label a will be discussed shortly. 
</p>
<p>We now consider the analogs of Theorems 9 and 10 for unitary operators. 
</p>
<p>Theorem 11. The eigenvalues of a unitary operator are complex numbers of 
</p>
<p>unit modulus. 
</p>
<p>Theorem 12. The eigenvectors of a unitary operator are mutually orthogonal. 
</p>
<p>(We assume there is no degeneracy.) 
</p>
<p>39 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>40 
</p>
<p>CHAPTER I 
</p>
<p>Proof of Both Theorems (assuming no degeneracy). Let 
</p>
<p>(1.8.18a) 
</p>
<p>and 
</p>
<p>( 1.8.18b) 
</p>
<p>If we take the adjoint of the second equation and dot each side with the corresponding 
side of the first equation, we get 
</p>
<p>so that 
</p>
<p>( 1.8.19) 
</p>
<p>If i=j, we get, since (udui)#O, 
</p>
<p>( 1.8.20a) 
</p>
<p>while if i#j, 
</p>
<p>( 1.8.20b) 
</p>
<p>since lui) #lu 1 )~ui#Uf*Uiut #uiuf~uiuf # 1. (Q.E.D.) 
If U is degenerate, we can carry out an analysis parallel to that for the Hermitian 
</p>
<p>operator !1, with just one difference. Whereas in Eq. (1.8.12), the zeros of the first 
row followed from the zeros of the first column and nt = n, here they follow from 
the requirement that the sum of the modulus squared of the elements in each row 
adds up to I. Since I u11 = I, all the other elements in the first row must vanish. 
</p>
<p>Diagonalization of Hermitian Matrices 
</p>
<p>Consider a Hermitian operator Q on 'Vn( C) represented as a matrix in some 
orthonormal basis 11), ... , li), ... , In). If we trade this basis for the eigenbasis 
lroi), ... , lroi), ... , Icon), the matrix representing Q will become diagonal. Now the 
operator U inducing the change of basis 
</p>
<p>leo)= VIi) (1.8.21) 
</p>
<p>is clearly unitary, for it "rotates" one orthonormal basis into another. (If you wish 
you may apply our mnemonic to U and verify its unitary nature: its columns contain 
the components of the eigenvectors I co) that are orthonormal.) This result is often 
summarized by the statement: 
</p>
<p>Every Hermitian matrix on 'Vn( C) may be diagonalized by a unitary change of 
basis. </p>
<p/>
</div>
<div class="page"><p/>
<p>We may restate this result in terms of passive transformations as follows: 
</p>
<p>If n is a Hermitian matrix, there exists a unitary matrix U (built out of the 
eigenvectors of Q) such that utnu is diagonal. 
</p>
<p>Thus the problem of finding a basis that diagonalizes Q is equivalent to solving 
</p>
<p>its eigenvalue problem. 
</p>
<p>Exercise 1.8.1. (1) Find the eigenvalues and normalized eigenvectors of the matrix 
</p>
<p>n=[~ ~ ~l 
0 I 4J 
</p>
<p>(2) Is the matrix Hermitian? Are the eigenvectors orthogonal? 
</p>
<p>Exercise 1.8.2. * Consider the matrix 
</p>
<p>(I) Is it Hermitian? 
</p>
<p>(2) Find its eigenvalues and eigenvectors. 
</p>
<p>(3) Verify that u'nu is diagonal, U being the matrix of eigenvectors of n. 
</p>
<p>Exercise 1.8.3. * Consider the Hermitian matrix 
</p>
<p>n=i [~ ~ -~: 
.0 -1 3 
</p>
<p>(I) Showthatw 1 =w2=1;w 3 =2. 
</p>
<p>(2) Show that lw = 2) is any vector of the form 
</p>
<p>(3) Show that the w =I eigenspace contains all vectors of the form 
</p>
<p>either by feeding w = 1 into the equations or by requiring thilt the w = 1 eigenspace be ortho-
gonal to I w = 2). 
</p>
<p>41 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODliCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>42 
</p>
<p>CHAPTER I 
</p>
<p>Exercise 1.8.4. An arbitrary n x n matrix need not have n eigenvectors. Consider as an 
</p>
<p>example 
</p>
<p>'l'' 4 1] 
Q= -1 2i 
</p>
<p>(l) Showthat(u 1 =wc=3. 
</p>
<p>(2) By feeding in this value show we gel only one eigenvector of the form 
</p>
<p>We cannot find another one that is LL 
</p>
<p>Exercise 1. 8. 5. * Consider the matrix 
</p>
<p>(I) Show that it is unitary. 
</p>
<p>[ 
cos () 
</p>
<p>Q= -sinO 
</p>
<p>(2) Show that its eigenvalues are ed' and e '" 
</p>
<p>sin 8 J 
cos I) 
</p>
<p>(3) Find the corresponding eigenvectors; show that they are orthogonal. 
</p>
<p>(4) Verify that [/QU=(diagonal matrix), where Uis the matrix of eigenvectors ofQ. 
</p>
<p>Exercise 1.8.6. * (I) We have seen that the determinant of a matrix is unchanged under 
a unitary change of basis. Argue now that 
</p>
<p>det f! =prodUCt Of eigenvalUeS Of Q = n (L); 
i=! 
</p>
<p>for a Hermitian or unitary Q. 
</p>
<p>(2) Using the invariance of the trace under the same transformation, show that 
</p>
<p>Td2 = 2.: w, 
i'-''1 
</p>
<p>Exercise 1.81 By using the results on the trace and determinant from the last problem, 
</p>
<p>show that the eigenvalues of the matrix 
</p>
<p>are 3 and -I. Verify this by explicit computation. Note that the Hermitian nature of the 
</p>
<p>matrix is an essential ingredient. </p>
<p/>
</div>
<div class="page"><p/>
<p>Exercise 1.8.8. * Consider Hermitian matrices M\ M 2 , M', M 4 that obey 
</p>
<p>i,j= I, ... , 4 
</p>
<p>(1) Show that the eigenvalues of M' are &plusmn; 1. (Hint: go to the eigenbasis of M', and use 
the equation for i=j.) 
</p>
<p>(2) By considering the relation 
</p>
<p>M'M'=-MiM' fori~j 
</p>
<p>show that M' are traceless. [Hint: Tr(ACB)=Tr(CBA).] 
</p>
<p>(3) Show that they cannot be odd-dimensional matrices. 
</p>
<p>Exercise 1.8.9. A collection of masses ma, located at ra and rotating with angular velocity 
</p>
<p>oo around a common axis has an angular momentum 
</p>
<p>where Va = oo x r a is the velocity of ma. By using the identity 
</p>
<p>A X (B X c) = B( A &bull; c)- C( A &bull; B) 
</p>
<p>show that each Cartesian component !, of I is given by 
</p>
<p>where 
</p>
<p>or in Dirac notation 
</p>
<p>ll)=Miw) 
</p>
<p>(I) Will the angular momentum and angular velocity always be parallel? 
</p>
<p>(2) Show that the moment of inertia matrix Mu is Hermitian. 
</p>
<p>(3) Argue now that there exist three directions tor oo such that I and oo will be parallel. 
</p>
<p>How are these directions to be found? 
</p>
<p>( 4) Consider the moment of inertia matrix of a sphere. Due to the complete symmetry 
</p>
<p>of the sphere, it is clear that every direction is its eigendirection for rotation. What does this 
</p>
<p>say about the three eigenvalues of the matrix M? 
</p>
<p>Simultaneous Diagonalization of Two Hermitian Operators 
</p>
<p>Let us consider next the question of simultaneously diagonalizing two Hermitian 
operators. 
</p>
<p>Theorem 13. If Q and A are two commuting Hermitian operators, there exists 
(at least) a basis of common eigenvectors that diagonalizes them both. 
</p>
<p>43 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>44 
</p>
<p>CHAPTER I 
</p>
<p>Proof. Consider first the case where at least one of the operators is nondegener-
ate, i.e., to a given eigenvalue, there is just one eigenvector, up to a scale. Let us 
assume n is nondegenerate. Consider any one of its eigenvectors: 
</p>
<p>Since [A, Q] =0, 
</p>
<p>ilAI OJ;)= OJ;AI OJ;) ( 1.8.22) 
</p>
<p>i.e., AI OJ;) is an eigenvector of n with eigenvalue OJ;. Since this vector is unique up 
to a scale, 
</p>
<p>( 1.8.23) 
</p>
<p>Thus I OJ;) is also an eigenvector of A with eigenvalue A;. Since every eigenvector of 
n is an eigenvector of A, it is evident that the basis I OJ;) will diagonalize both 
operators. Since n is nondegenerate, there is only one basis with this property. 
</p>
<p>What if both operators are degenerate? By ordering the basis vectors such that 
the elements of each eigenspace are adjacent, we can get one of them, say n, into 
the form (Theorem 10) 
</p>
<p>OJ, 
</p>
<p>Now this basis is not unique: in every eigenspace w::;; = W7'' corresponding to the 
eigenvalue OJ;, there exists an infinity of bases. Let us arbitrarily pick in w::;; a set 
!OJ;, a) where the additional label a runs from 1 tom;. 
</p>
<p>How does A appear in the basis? Although we made no special efforts to get A 
into a simple form, it already has a simple form by virtue of the fact that it commutes 
with n. Let us start by mimicking the proof in the nondegenerate case: 
</p>
<p>ilAIOJ;, a)=Ail!OJ;, a)=OJ;AIOJ;, a) </p>
<p/>
</div>
<div class="page"><p/>
<p>However, due to the degeneracy of n, we can only conclude that 
</p>
<p>Alm 1, a) lies in V'('&bull; 
</p>
<p>Now, since vectors from different eigenspaces are orthogonal [Eq. (1.8.15)], 
</p>
<p>iflm1, a) and lmj, f3&gt; are basis vectors such that m1#mj. Consequently, in this basis, 
</p>
<p>which is called a block diagonal matrix for obvious reasons. The block diagonal form 
</p>
<p>of A reflects the fact that when A acts on some element I m 1, a&gt; of the eigenspace 
V'('', it turns it into another element of V'(''. Within each subspace i, A is given by 
</p>
<p>a matrix A1, which appears as a block in the equation above. Consider a matrix A1 
</p>
<p>in V'(''. It is Hermitian since A is. It can obviously be diagonalized by trading the 
</p>
<p>basis lm1 , 1), lm1, 2), ... , lm 1, m1) in W'('' that we started with, for the eigenbasis of 
</p>
<p>A1&bull; Let us make such a change of basis in each eigenspace, thereby rendering A 
</p>
<p>diagonal. Meanwhile what of n? It remains diagonal of course, since it is indifferent 
to the choice of orthonormal basis in each degenerate eigenspace. If the eigenvalues 
of A; are ;.p&gt; ;.p&gt;, ... , Mm,) then we end up with 
</p>
<p>Q.E.D. 
</p>
<p>45 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>46 
</p>
<p>CHAPTER I 
</p>
<p>If A is not degenerate within any given subspace, i= , for any k, !, and i, the 
</p>
<p>basis we end up with is unique: the freedom Q gave us in each eigenspace is fully 
</p>
<p>eliminated by A. The elements of this basis may be named uniquely by the pair of 
</p>
<p>indices w and it as jw, it). with A playing the role of the extra label u. If A is 
</p>
<p>degenerate within an eigenspace of Q, if say A\))= , there is a two-dimensional 
eigenspace from which we can choose any two orthonormal vectors for the common 
</p>
<p>basis. lt is then necessary to bring in a third operator r, that commutes with both 
Q and A, and which will be nondegenerate in this subspace. In general, one can 
</p>
<p>always find, for finite n, a set of operators { Q, A, 1, ... } that commute with each 
</p>
<p>other and that nail down a unique, common, eigenbasis, the elements of which may 
</p>
<p>be labeled unambiguously as ! w, it, y, . .. ). ln our study of quantum mechanics it 
</p>
<p>will be assumed that such a complete set of commuting operators exists if n is infinite. 
</p>
<p>Exercise 1.8. 10. * By considering the commutator, :&gt;how that the following Hermitian 
matrices may be simultaneously diagonalized. Find the eigenvectors common to both and 
</p>
<p>verify that under a unitary transformation to this basis, both matrices are diagonalized. 
</p>
<p>0 
</p>
<p>0 
</p>
<p>0 
</p>
<p>1 
</p>
<p>0 -i] 
</p>
<p>Since Q is degenerate and!\ is not you must be prudent in deciding which matrix dictates 
</p>
<p>the choice of basis. 
</p>
<p>Example 1.8.6. We will nov&middot; discuss, in some detail, the complete solution to a 
</p>
<p>problem in mechanics. It i~ in&middot;.portant that you understand this example thoroughly, 
</p>
<p>for it not only illustrates tl1e use of the mathematical techniques developed in this 
</p>
<p>chapter but also contains the main features of the central problem in quantum 
</p>
<p>mechanics. 
</p>
<p>The mechanical syste'n in question is depicted in Fig. 1.5. The two masses m 
</p>
<p>are coupled to each other and the walls by springs of force constant k. If x1 and x2 
</p>
<p>measure the displacements of the masses from their equilibrium points, these coordi-
</p>
<p>nates obey the following equations, derived through an elementary application of 
</p>
<p>Newton's laws: 
</p>
<p>2k k 
.YJ = -- X1 + X2 
</p>
<p>m m 
</p>
<p>k 2k 
~'i'2=-x1--x2 
</p>
<p>m m 
</p>
<p>( 1.8.24a) 
</p>
<p>( 1.8.24b) 
</p>
<p>Figure 1.5. The coupled mass problem. All masses arc 
</p>
<p>m. all spring constants are k. and the displacements of 
</p>
<p>the masses from equilibrium are x 1 and x .&bull;. </p>
<p/>
</div>
<div class="page"><p/>
<p>The problem is to find x 1(t) and x2(t) given the initial-value data, which in this 
</p>
<p>case consist of the initial positions and velocities. If we restrict ourselves to the case 
</p>
<p>of zero initial velocities, our problem is to find x 1(t) and x 2(t), given x 1 (0) and x 2(0). 
</p>
<p>In what follows, we will formulate the problem in the language of linear vector 
</p>
<p>spaces and solve it using the machinery developed in this chapter. As a first step, we 
</p>
<p>rewrite Eq. (1.8.24) in matrix form: 
</p>
<p>(1.8.25a) 
</p>
<p>where the elements of the Hermitian matrix Qii are 
</p>
<p>QII=Qn=-2k/m, (1.8.25b) 
</p>
<p>We now view x1 and x2 as components of an abstract vector lx), and Qii as the matrix 
elements of a Hermitian operator n. Since the vector jx) has two real components, it 
is an element of w2(R), and Q is a Hennitian operator on w\R). The abstract form 
of Eq. (1.8.25a) is 
</p>
<p>I.X(t))=Qjx(t)) ( 1.8.26) 
</p>
<p>Equation ( 1.8.25a) is obtained by projecting Eq. (1.8.26) on the basis vectors II), 
</p>
<p>12), which have the following physical significance: 
</p>
<p>11 ) , ___ ) 1 J &lt;---+ I first mass displace~ by unity] 
Lo L second mass und1splaced _ 
</p>
<p>) I OJ I first mass undisplaced l 
12 
</p>
<p>&lt;---+ l1 &lt;---+ Lsecond mass displaced by unity_ 
</p>
<p>(1.8.27a) 
</p>
<p>(1.8.27b) 
</p>
<p>An arbitrary state, in which the masses are displaced by x 1 and x2 , is given in this 
</p>
<p>basis by 
</p>
<p>( 1.8.28) 
</p>
<p>The abstract counterpart of the above equation is 
</p>
<p>( 1.8.29) 
</p>
<p>It is in this 11), 12) basis that Q is represented by the matrix appearing in Eq. 
(1.8.25), with elements -2k/m, k/m, etc. 
</p>
<p>The basis II), 12) is very desirable physically, for the components of jx) in this 
basis (x1 and x2) have the simple interpretation as displacements of the masses. 
</p>
<p>However, from the standpoint of finding a mathematical solution to the initial-value 
</p>
<p>problem, it is not so desirable, for the components x1 and x 2 obey the coupled 
</p>
<p>47 
MATHEMATICAL 
</p>
<p>JNTRODUCT!ON </p>
<p/>
</div>
<div class="page"><p/>
<p>48 
</p>
<p>CHAPTER I 
</p>
<p>differential equations (1.8.24a) and (1.8.24b). The coupling is mediated by the off-
diagonal matrix elements Cl12 =Cl21 =kjm. 
</p>
<p>Having identified the problem with the 11), 12) basis, we can now see how to 
get around it: we must switch to a basis in which Cl is diagonal. The components of 
lx) in this basis will then obey another uncoupled differential equations which may 
be readily solved. Having found the solution, we can return to the physically prefer-
able 11), 12) basis. This, then, is our broad strategy and we now turn to the details. 
</p>
<p>From our study of Hermitian operators we know that the basis that diagonalizes 
Cl is the basis of its normalized eigenvectors. Let II) and III) be its eigenvectors 
defined by 
</p>
<p>Clll)=-wfll) (1.8.30a) 
</p>
<p>(1.8.30b) 
</p>
<p>We are departing here from our usual notation: the eigenvalue of n is written as 
- w2 rather than as w in anticipation of the fact that n has eigenvalues of the form 
-oi, with w real. We are also using the symbols II) and III) to denote what should 
be called I - w~) and I - coi1) in our convention. 
</p>
<p>It is a simple exercise (which you should perform) to solve the eigenvalue prob-
lem of Cl in the 11), 12) basis (in which the matrix elements of Clare known) and 
to obtain 
</p>
<p>-(k)l/2 
WI- - , 
</p>
<p>m 
</p>
<p>-(3k)l/2 
Wu- - ' 
</p>
<p>m 
III)+-. -k [ 1] 
</p>
<p>2 -1 
</p>
<p>If we now expand the vector lx(t)) in this new basis as 
</p>
<p>lx(t)) =I I)xi(t) +I Il)xu(t) 
</p>
<p>(1.8.31a) 
</p>
<p>(1.8.31b) 
</p>
<p>(1.8.32) 
</p>
<p>[in analogy with Eq. (1.8.29)], the components X1 and xu will evolve as follows: 
</p>
<p>(1.8.33) 
</p>
<p>We obtain this equation by rewriting Eq. (1.8.26) in the II), III) basis in which n 
has its eigenvalues as the diagonal entries, and in which lx) has components x1 and </p>
<p/>
</div>
<div class="page"><p/>
<p>xn. Alternately we can apply the operator 
</p>
<p>to both sides of the expansion of Eq. ( 1.8.32), and get 
</p>
<p>(1.8.34) 
</p>
<p>Since I I) and I II) are orthogonal, each coefficient is zero. 
The solution to the decoupled equations 
</p>
<p>i=I, II ( 1.8.35) 
</p>
<p>subject to the condition of vanishing initial velocities, is 
</p>
<p>i= I, II ( 1.8.36) 
</p>
<p>As anticipated, the components of lx) in the I J), I II) basis obey decoupled equations 
that can be readily solved. Feeding Eq. ( 1.8.36) into Eq. ( 1.8.32) we get 
</p>
<p>(1.8.37a) 
</p>
<p>= II)(IIx(O)) cos w1t+ III)(IIIx(O)) cos m11 t ( 1.8.37b) 
</p>
<p>Equation (1.8.37) provides the explicit solution to the initial-value problem. It corre-
</p>
<p>sponds to the following algorithm for finding lx(t)) given lx(O)). 
</p>
<p>Step (l). Solve the eigenvalue problem of n. 
</p>
<p>Step (2). Find the coefficients x1(0) =(I: x(O)) and x 11(0) = (Hjx(O)) m the 
</p>
<p>expansion 
</p>
<p>lx(O)) =I I)xi(O) +I II)xn(O) 
</p>
<p>Step (3). Append to each coefficient xdO) (i= I, II) a time dependence cos m;t 
</p>
<p>to get the coefficients in the expansion of I x(l) ). 
Let me now illustrate this algorithm by solving the following (general) initial-
</p>
<p>value problem: Find the future state of the system given that at t = 0 the masses are 
displaced by x 1(0) and x2(0). 
</p>
<p>Step (1). We can ignore this step since the eigenvalue problem has been solved 
[Eq. (1.8.31)]. 
</p>
<p>49 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>50 
</p>
<p>CHAPTER I 
</p>
<p>Step (2). 
</p>
<p>x1(0) = (IIx(O)) = 
</p>
<p>1 [X1 (Q)J X1 (0)- Xo(O) 
x (O)=IIIIx(O))= (1 -1) . &middot; = " 
</p>
<p>ll ' . , .xdO) i/2 
</p>
<p>Step (3 ). 
</p>
<p>, x1 (0) + x2(0) . . x 1 (0)- x 2(0) 
I x( t)) = I I; &middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;:&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot; cos OJ r t + I II) &middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot; &middot; cos OJ t 
</p>
<p>&middot; 2L2 2112 IT 
</p>
<p>The explicit solution above can be made even more explicit by projecting lx(t)) onto 
</p>
<p>the 11 ), 12) basis to find x 1 ( t) and x2( t), the displacements of the masses. We get 
</p>
<p>(feeding in the explicit formulas for w 1 and wu) 
</p>
<p>x1(t) = &lt;llx(t)) 
</p>
<p>= &lt;J II) XJ(O) ~:2(0) cos[(~ r2 t]+ &lt;liii&gt; Xt(O) ~;'"(0) cos [ek')l.'21J 
2 ,m, 2 ,m 
</p>
<p>[(k' "
2 J . [(" k' 1/2 J =~(x 1 (0)+x 2 (0)]cos )1 t. + 1 [xi(O)-x2(0)]cos ~-&middot;) t 
</p>
<p>2 rn .. 2 .. m, .. 
( 1.8.38a) 
</p>
<p>using the fact that 
</p>
<p>&lt;LII)=&lt;JIII&gt;= 1 
</p>
<p>It can likewise be shown that 
</p>
<p>(1.8.38b) 
</p>
<p>We can rewrite Eq. ( 1.8.38) in matrix form as 
</p>
<p>X1 (t) 2 [
</p>
<p>cos [(k/m) 112 t] +cos [(3k/m) 112 tJ 
</p>
<p>L,(J ~ oo'[(kjm)' ''tl-
2 
m; [(3kjm)' "t] 
</p>
<p>cos [(k/m) 1 12t]- cos [(3k/m) 112t]l 
2 
</p>
<p>1 .-'2 l /') 
cos [(kjm) t] +;cos [(3~/'1l)"t] 
</p>
<p>( 1.8.39) </p>
<p/>
</div>
<div class="page"><p/>
<p>This completes our determination of the future state of the system given the initial 
</p>
<p>state. 
</p>
<p>The Propagator 
</p>
<p>There are two remarkable features in Eq. ( 1.8.39): 
</p>
<p>(1) The final-state vector is obtained from the initial-state vector upon multiplication 
</p>
<p>by a matrix. 
</p>
<p>(2) This matrix is independent of the initial state. We call this matrix the propagator. 
</p>
<p>Finding the propagator is tantamount to finding the complete solution to the 
</p>
<p>problem, for given any other initial state with displacements i 1 (0) and i 2(0), we 
</p>
<p>get .x1(t) and x2(t) by applying the same matrix to the initial-state vector. 
</p>
<p>We may view Eq. (1.8.39) as the image in the II), 12) basis of the abstract 
</p>
<p>relation 
</p>
<p>lx(t))= U(t)lx(O)) ( 1.8.40) 
</p>
<p>By comparing this equation with Eq. (l.8.37b), we find the abstract representation 
</p>
<p>of U: 
</p>
<p>U(t) = ii)(II cos m, t + III)(III cos m11 t (l.8.4la) 
</p>
<p>II 
</p>
<p>=I li)(il cos m,l (1.8.4lb) 
i=I 
</p>
<p>You may easily convince yourself that if we take the matrix elements of this operator 
</p>
<p>in the ll ), 12) basis, we regain the matrix appearing in Eq. ( 1.8.39). For example 
</p>
<p>u,l = 01 ull &gt; 
</p>
<p>=(II {II)(II cos[ (~} 12 tJ + IH)(HI cos[ (~) 1 12t]}11) 
</p>
<p>= ( 11 1)(111) cos [ (;;)
112 
</p>
<p>t l + (II II)(Hil) cos [ (~f"rJ 
l { [(k) 112 J [/3k' 112 ]} =2. cos ; t +cos ~;) t 
</p>
<p>Notice that U(t) [Eq. (1.8.41)] is determined completely by the eigenvectors 
and eigenvalues of Q. We may then restate our earlier algorithm as follows. To solve 
the equation 
</p>
<p>51 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>52 
</p>
<p>CHAPTER I 
</p>
<p>(1) Solve the eigenvalue problem of Q. 
</p>
<p>(2) Construct the propagator U in terms of the eigenvalues and eigenvectors. 
(3) lx(t))= U(t)lx(O)). 
</p>
<p>The Normal Modes 
</p>
<p>There are two initial states lx(O)) for which the time evolution is particularly 
</p>
<p>simple. Not surprisingly, these are the eigenkets II) and I II). Suppose we have 
lx(O))=II). Then the state at timet is 
</p>
<p>II(t)) = U(t)ll) 
= (i I) (II cos w, t +I H)(HI cos Wn t)l I) 
</p>
<p>=IT)cosm1 t ( 1.8.42) 
</p>
<p>Thus the system starting off in I I) is only modified by an overall factor cos w1 t. A 
similar remark holds with I-+II. These two modes of vibration, in which all (two) 
</p>
<p>components of a vector oscillate in step are called normal modes. 
</p>
<p>The physics of the normal modes is dear in the 11), 12) basis. In this basis 
</p>
<p>II)~ I [~J 
</p>
<p>and corresponds to a state in which both masses are displaced by equal amounts. 
</p>
<p>The middle spring is then a mere spectator and each mass oscillates with a frequency 
</p>
<p>w1 = (k/m) 112 in response to the end spring nearest to it. Consequently 
</p>
<p>( I [cos [(k/m)
11
</p>
<p>.
2t]. J II l) ~ l 
</p>
<p>cos [(k/m) "2 t] 
</p>
<p>On the other hand, if we start with 
</p>
<p>IH)~ 1 [_:J 
the masses are displaced by equal and opposite amounts. In this case the middle 
</p>
<p>spring is distorted by twice the displacement of each mass. If the masses are adjusted 
by ~ and -~, respectively, each mass feels a restoring force of 3k~ (2k~ from the 
middle spring and k~ from the end spring nearest to it). Since the effective force 
constant is kerr=3k~/~=3k, the vibrational frequency is (3k/m) 112 and 
</p>
<p>, . , l [. cos [(3k/rn.) 1' 2t(l 
IH;(t);+-&gt;1"' . ,, 
</p>
<p>2 '" &middot;&middot;&middot;&middot;cos [(3kjm) '~tL 
</p>
<p>If the system starts off in a linear combination of I I) and I II) it evolves into 
the corresponding linear combination of the normal modes ll(t)) and III(t)). This </p>
<p/>
</div>
<div class="page"><p/>
<p>is the content of the propagator equation 
</p>
<p>lx(t))= U(t)lx(O)) 
</p>
<p>=ll)(llx(O)) cos WJt+III)(III x(O)) cos Wnt 
</p>
<p>= l/(t) )(llx(O)) + III(t) )(IIIx(O)) 
</p>
<p>Another way to see the simple evolution of the initial states I I) and I II) is to 
determine the matrix representing U in the II), III) basis: 
</p>
<p>U &lt;---+ [cos co1 t 0 J 
I,n 0 cos COn t 
</p>
<p>( 1.8.43) 
</p>
<p>basis 
</p>
<p>You should verify this result by taking the appropriate matrix elements of U(t) in 
Eq. (1.8.4lb). Since each column above is the image of the corresponding basis 
vectors (II) or III)) after the action of U(t), (which is to say, after time evolution), 
we see that the initial states II) and III) evolve simply in time. 
</p>
<p>The central problem in quantum mechanics is very similar to the simple example 
that we have just discussed. The state of the system is described in quantum theory 
by a ket I Vt) which obeys the Schrodinger equation 
</p>
<p>iii I ift) =HI VI) 
</p>
<p>where 1i is a constant related to Planck's constant h by 1i =hj27r, and His a Hermitian 
operator called the Hamiltonian. The problem is to find I Vt( t)) given I Vt(O)). [Since 
the equation is first order in t, no assumptions need be made about I if/(0)), which 
is determined by the Schrodinger equation to be (- i/1i)HI Vt(O) ).] 
</p>
<p>In most cases, His a time-independent operator and the algorithm one follows 
in solving this initial-value problem is completely analogous to the one we have just 
seen: 
</p>
<p>Step (1). Solve the eigenvalue problem of H. 
</p>
<p>Step (2). Find the propagator U(t) in terms of the eigenvectors and eigenvalues 
of H. 
</p>
<p>Step (3). I Vt(t)) = U(t)l Vt(O)). 
You must of course wait till Chapter 4 to find out the physical interpretation 
</p>
<p>of I Vf), the actual form of the operator H, and the precise relation between U(t) 
and the eigenvalues and eigenvectors of H. D 
</p>
<p>Exercise 1.8.11. Consider the coupled mass problem discussed above. 
(l) Given that the initial state is 11), in which the first mass is displaced by unity and 
</p>
<p>the second is left alone, calculate ll(t)) by following the algorithm. 
(2) Compare your result with that following from Eq. (1.8.39). 
</p>
<p>53 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>54 
</p>
<p>CHAPTER I 
</p>
<p>Exercise 1.8.12. Consider once again the problem discussed in the previous example. 
</p>
<p>(I) Assuming that 
</p>
<p>jx)=fljx) 
</p>
<p>has a solution 
</p>
<p>jx(t)) = U(t)jx(O)) 
</p>
<p>find the differential equation satisfied by U(t). Use the fact that jx(O)) is arbitrary. 
</p>
<p>(2) Assuming (as is the case) that n and U can be simultaneously diagonalized, solve 
for the elements of the matrix U in this common basis and regain Eq. (1.8.43). Assume 
</p>
<p>jx(O))=O. 
</p>
<p>1.9. Functions of Operators and Related Concepts 
</p>
<p>We have encountered two types of objects that act on vectors: scalars, which 
</p>
<p>commute with each other and with all operators; and operators, which do not 
</p>
<p>generally commute with each other. It is customary to refer to the former as c 
</p>
<p>numbers and the latter as q numbers. Now, we are accustomed to functions of c 
numbers such as sin(x), log(x), etc. We wish to examine the question whether 
</p>
<p>functions of q numbers can be given a sensible meaning. We will restrict ourselves 
</p>
<p>to those functions that can be written as a power series. Consider a series 
</p>
<p>(1.9.1) 
</p>
<p>where x is a c number. We define the same function of an operator or q number to 
</p>
<p>be 
</p>
<p>(1.9.2) 
</p>
<p>This definition makes sense only if the sum converges to a definite limit. To see what 
</p>
<p>this means, consider a common example: 
</p>
<p>(1.9.3) 
</p>
<p>Let us restrict ourselves to Hermitian n. By going to the eigenbasis of n we can 
readily perform the sum of Eq. (1.9.3). Since 
</p>
<p>n= [
ll)l 
</p>
<p>J ( 1.9.4) </p>
<p/>
</div>
<div class="page"><p/>
<p>and 
</p>
<p>gm= [
</p>
<p>(J)j 
</p>
<p>(1.9.5) 
</p>
<p>(1.9.6) 
</p>
<p>Since each sum converges to the familiar limit e0", the operator e0 is indeed well 
defined by the power series in this basis (and therefore in any other). 
</p>
<p>Exercise 1.9.1. * We know that the series 
</p>
<p>f(x)= I x" 
n=O 
</p>
<p>may be equated to the functionf(x) = (1- x)- 1 if lxl &lt; 1. By going to the eigenbasis, examine 
when the q number power series 
</p>
<p>00 
</p>
<p>/CO.)= I n" 
n=O 
</p>
<p>of a Hermitian operator 0. may be identified with (1-0.)- 1&bull; 
</p>
<p>Exercise 1.9.2. * If His a Hermitian operator, show that U=em is unitary. (Notice the 
analogy with c numbers: if() is real, u=e18 is a number of unit modulus.) 
</p>
<p>Exercise 1.9.3. For the case above, show that det U=enrH. 
</p>
<p>Derivatives of Operators with Respect to Parameters 
</p>
<p>Consider next an operator O(A.) that depends on a parameter A. Its derivative 
with respect to A. is defined to be 
</p>
<p>dO( A.)= lim [O(A.+ AA.)- O(A.)J 
dA. ,u~o AA. 
</p>
<p>If O(A.) is written as a matrix in some basis, then the matrix representing dO(A.)jdA. 
is obtained by differentiating the matrix elements of O(A.). A special case of O(A.) we 
</p>
<p>55 
</p>
<p>MATHEMATICAL 
INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>56 
</p>
<p>CHAPTER I 
</p>
<p>are interested in is 
</p>
<p>where Q is Hermitian. We can show, by going to the eigenbasis of Q, that 
</p>
<p>dB(A.) An Ml 
~---=Qe =e Q=O(A.)Q 
</p>
<p>d). 
(1.9.7) 
</p>
<p>The same result may be obtained, even if Q is not Hermitian, by working with the 
</p>
<p>power series, provided it exists: 
</p>
<p>d cc )."Q" 0() nA."-lgn 00 ;.n~lgn-1 00 ;.mgm 
- I &middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;= I &middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;=n L: ~ ~ =n L: -~--=neA.Q 
dA.n~o n! n=l n! n-1 (n-l)! m-o m! 
</p>
<p>Conversely, we can say that if we are confronted with the differential Eq. ( 1.9. 7), 
</p>
<p>its solution is given by 
</p>
<p>O(A.)=c exp(f Q dX )=c exp(QA.) 
</p>
<p>(It is assumed here that the exponential exists.) In the above, cis a constant (opera-
</p>
<p>tor) of integration. The solution e =em corresponds to the choice c =I. 
In all the above operations, we see that Q behaves as if it were just a c number. 
</p>
<p>Now, the real difference between c numbers and q numbers is that the latter do not 
generally commute. However, if only one q number (or powers of it) enter the 
</p>
<p>picture, everything commutes and we can treat them as c numbers. If one remembers 
this mnemonic, one can save a lot of time. 
</p>
<p>If, on the other hand, more than one q number is involved, the order of the 
</p>
<p>factors is all important. For example, it is true that 
</p>
<p>as may be verified by a power-series expansion, while it is not true that 
</p>
<p>or that 
</p>
<p>unless [Q, 0] = 0. Likewise, in differentiating a product, the chain rule is 
</p>
<p>(1.9.8) </p>
<p/>
</div>
<div class="page"><p/>
<p>We are free to move n through e'.n and write the first term as 
</p>
<p>but not as 
</p>
<p>unless [0, 0]=0. 
</p>
<p>1.10. Generalization to Infinite Dimensions 
</p>
<p>In all of the preceding discussions, the dimensionality (n) of the space was 
unspecified but assumed to be some finite number. We now consider the generaliza-
tion of the preceding concepts to infinite dimensions. 
</p>
<p>Let us begin by getting acquainted with an infinite-dimensional vector. Consider 
a function defined in some interval, say, a~x~b. A concrete example is provided 
by the displacementf(x, t) of a string clamped at x=O and x=L (Fig. 1.6). 
</p>
<p>Suppose we want to communicate to a person on the moon the string's displace-
mentf(x), at some timet. One simple way is to divide the interval 0- L into 20 equal 
parts, measure the displacementf(x;) at the 19 points x=L/20, 2Lj20, ... , 19Lj20, 
and transmit the 19 values on the wireless. Given thesef(x;), our friend on the moon 
will be able to reconstruct the approximate picture of the string shown in Fig. 1.7. 
</p>
<p>If we wish to be more accurate, we can specify the values of f(x) at a larger 
number of points. Let us denote by f,(x) the discrete approximation to f(x) that 
coincides with it at n points and vanishes in between. Let us now interpret the ordered 
n-tuple Un(xi), f,(x2), ... , f,(xn)} as components of a ket I fn&gt; in a vector space 
'lor(R): 
</p>
<p>Figure 1.6. The string is clamped at x=O 
and x = L. It is free to oscillate in the plane 
of the paper. 
</p>
<p>Figure 1.7. The string as reconstructed by the 
person on the moon. 
</p>
<p>(1.10.1) 
</p>
<p>57 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>58 
</p>
<p>CHAPTER I 
</p>
<p>The basis vectors in this space are 
</p>
<p>0 
</p>
<p>0 
</p>
<p>I x;) &lt;--&gt; 1 +-- ith place (1.10.2) 
0 
</p>
<p>0 
</p>
<p>corresponding to the discrete function which is unity at x = x; and zero elsewhere. 
</p>
<p>The basis vectors satisfy 
</p>
<p>(x;!xj) = Ou (orthogonality) (1.10.3) 
</p>
<p>L lx;)(x;l =I (completeness) (l.l0.4) 
i=l 
</p>
<p>Try to imagine a space containing n mutually perpendicular axes, one for each 
</p>
<p>point X;. Along each axis is a unit vector lx;). The functionf,(x) is represented by 
</p>
<p>a vector whose projection along the ith direction is fn(x;): 
</p>
<p>If,)= L fn(X;)Ix;) ( 1.10.5) 
i=1 
</p>
<p>To every possible discrete approximation gn(x), hn(x), etc., there is a corresponding 
</p>
<p>ket lg11 ), lh11), etc., and vice versa. You should convince yourself that if we define 
</p>
<p>vector addition as the addition of the components, and scalar multiplication as the 
</p>
<p>multiplication of each component by the scalar, then the set of all kets representing 
</p>
<p>discrete functions that vanish at x = 0, L and that are specified at n points in between, 
</p>
<p>forms a vector space. 
</p>
<p>We next define the inner product in this space: 
</p>
<p>11 
</p>
<p>( 1.10.6) 
i=l 
</p>
<p>Two functionsf,(x) and gn(x) will be said to be orthogonal if &lt;J~Ign)=O. 
</p>
<p>Let us now forget the man on the moon and consider the; maximal specification 
</p>
<p>of the string's displacement, by giving its value at every point in the interval 0- L. 
</p>
<p>In this casefoo(x)=f(x) is specified by an ordered infinity of numbers: anf(x) for 
</p>
<p>each point x. Each function is now represented by a ket I/"") in an infinite-dimen-
sional vector space and vice versa. Vector addition and scalar multiplication are 
</p>
<p>defined just as before. Consider, however, the inner product. For finite n it was </p>
<p/>
</div>
<div class="page"><p/>
<p>defined as 
</p>
<p>n 
</p>
<p>(Jnlgn) = I fn(X;)gn(X;) 
i=l 
</p>
<p>in particular 
</p>
<p>n 
</p>
<p>(j,,l fn) = I [fn(X;)] 2 
i=l 
</p>
<p>If we now let n go to infinity, so does the sum, for practically any function. What 
we need is the redefinition of the inner product for finite n in such a way that as n 
tends to infinity, a smooth limit obtains. The natural choice is of course 
</p>
<p>(fnlgn) = I fn(X;)gn(x;)ll, !l=L/(n+ I) (1.10.6') 
i=l 
</p>
<p>If we now Jet n go to infinity, we get, by the usual definition of the integral, 
</p>
<p>(fig)= rf(x)g(x) dx 
0 
</p>
<p>(1.10.7) 
</p>
<p>~I 
</p>
<p>(fl f&gt; = j .. / 2(x) dx 
&middot;o 
</p>
<p>(1.10.8) 
</p>
<p>If we wish to go beyond the instance of the string and consider complex functions 
of x as well, in some interval as, x-&lt;;;;, b, the only modification we need is in the inner 
product: 
</p>
<p>&lt;Jig)= r f*(x)g(x) dx 
a 
</p>
<p>(1.10.9) 
</p>
<p>What are the basis vectors in this space and how are they normalized? We know 
that each point x gets a basis vector lx). The orthogonality of two different axes 
requires that 
</p>
<p>(xlx')=O, xi=x' (1.10.10) 
</p>
<p>What if x=x'? Should we require, as in the finite-dimensional case, (xlx)= 1? The 
answer is no, and the best way to see it is to deduce the correct normalization. We 
start with the natural generalization of the completeness relation Eq. (1.10.4) to the 
case where the kets are labeled by a continuous index x': 
</p>
<p>f lx')(x'l dx'=I (1.10.11) 
</p>
<p>59 
</p>
<p>MATHEMATICAL 
INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>CHAPTER I 
</p>
<p>where, as always, the identity is required to leave each ket unchanged. Dotting both 
</p>
<p>sides of Eq. ( 1.10.11) with some arbitrary ket I f) from the right and the basis bra 
</p>
<p>(xl from the left, 
</p>
<p>r (xlx')(x'lf&gt; dx'= (xiiif)= &lt;xlf&gt; 
a 
</p>
<p>(1.10.12) 
</p>
<p>Now, (xl f), the projection of If) along the &middot;basis ket lx), is just f(x). Likewise 
</p>
<p>(x'l f)= f(x'). Let the inner product (xlx') be some unknown function o(x, x'). 
</p>
<p>Since o (x, x') vanishes if xi' x' we can restrict the integral to an infinitesimal region 
near x'=x in Eq. (1.10.12): 
</p>
<p>fx+&lt; o(x, x') f(x') dx' = f(x) 
x---&middot;e 
</p>
<p>( 1.10.13) 
</p>
<p>In this infinitesimal region,f(x') (for any reasonably smooth f) can be approximated 
</p>
<p>by its value at x' = x, and pulled out of the integral: 
</p>
<p>f(x) r+&bull; o(x, x') dx'=f(x) 
x-&amp; 
</p>
<p>(1.10.14) 
</p>
<p>so that 
</p>
<p>IX+&lt; 
x-e 8(x, x') dx' = 1 (1.10.15) 
</p>
<p>Clearly o(x, x') cannot be finite at x' = x, for then its integral over an infinitesimal 
</p>
<p>region would also be infinitesimal. In fact o(x, x') should be infinite in such a way 
</p>
<p>that its integral is unity. Since o(x, x') depends only on the difference X- x'' let us 
</p>
<p>write it as o(x- x'). The "function," 8(x- x'), with the properties 
</p>
<p>o(x-x')=O, r o(x-x') dx'= 1, 
a 
</p>
<p>x#x' 
(1.10.16) 
</p>
<p>a&lt;x&lt;b 
</p>
<p>is called the Dirac delta function and fixes the normalization of the basis vectors: 
</p>
<p>(xlx') = 8(x- x') (l.l0.17) 
</p>
<p>It will be needed any time the basis kets are labeled by a continuous index such as 
</p>
<p>x. Note that it is defined only in the context of an integration: the integral of the 
</p>
<p>delta function o(x- x') with any smooth functionf(x') isf(x). One sometimes calls </p>
<p/>
</div>
<div class="page"><p/>
<p>(a) (b) 
</p>
<p>ci1J1 4 (x-a'~ 
dx x-&laquo; ~ 
</p>
<p>It +tr 
</p>
<p>Figure 1.8. (a) The Gaussian g11 approaches the delta function as .&amp;-+0. (b) Its derivative (dg/dx)(x- x') 
</p>
<p>approaches o'(x-x') as .&amp; .... o. 
</p>
<p>the delta function the sampling function, since it samples the value of the function 
</p>
<p>f(x') at one pointt 
</p>
<p>f S(x-x')f(x') dx'=f(x) (1.10.18) 
The delta function does not look like any function we have seen before, its 
</p>
<p>values being either infinite or zero. It is therefore useful to view it as the limit of a 
more conventional function. Consider a Gaussian 
</p>
<p>(1.10.19) 
</p>
<p>as shown in Fig. 1.8a. The Gaussian is centered at x' = x, has width A, maximum 
height (nA2) 112, and unit area, independent of A. As A approaches zero, g" becomes 
a better and better approximation to the delta function.&sect; 
</p>
<p>It is obvious from the Gaussian model that the delta function is even. This may 
be verified as follows: 
</p>
<p>S(x-x') = (xlx')= (x'lx)"' = o(x'- x)"' = o(x' -x) 
</p>
<p>since the delta function is real. 
Consider next an object that is even more peculiar than the delta function: its 
</p>
<p>derivative with respect to the first argument x: 
</p>
<p>o'(x-x') =j_ o(x- x') = _!!__ o(x-x') 
dx dx' 
</p>
<p>(1.10.20) 
</p>
<p>What is the action of this function under the integral? The clue comes from the 
Gaussian model. Consider dg{j,(x-x')/dx=-dg&amp;(x-x')jdx' as a function of x'. As 
g&amp; shrinks, each bump at &plusmn; e will become, up to a scale factor, the o function. The 
</p>
<p>:t We will often omit the limits of integration if they are unimportant. 
&sect;A fine point that will not concern you till Chapter 8: This formula for the delta function is valid even 
</p>
<p>if .&amp;2 is pure imaginary, say, equal to ifJ2 &bull; First we see from Eq. (A.2.5) that g has unit area. Consider 
next the integral of g times f(x') over a region in x' that includes x. For the most part, we get zero 
because f is smooth and g is wildly oscillating as fJ -+0. However, at x = x', the derivative of the phase 
of g vanishes and the oscillations are suspended. Pullingf(x' = x) out of the integral, we get the desired 
result. 
</p>
<p>61 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>62 
</p>
<p>CHAPTER 1 
</p>
<p>first one will sample -f(x-s) and the second one +f(x+t:), again up to a scale, 
so that 
</p>
<p>Io '(x- x')f(x') dx' ocf(x+ e)-f(x- t:) = 2t: d~ 1 dx x'~x 
The constant of proportionality happens to be 1/2t: so that 
</p>
<p>Io'(x-x')f(x')dx'= d~l =df(x) dx x'~x dx (1.10.21) 
</p>
<p>This result may be verified as follows: 
</p>
<p>I o'(x-x')f(x') dx'= I do(;;;x') f(x') dx&middot;= ~ f o(x-x')f(x') dx' 
df(x) 
</p>
<p>dx 
</p>
<p>Note that o'(x-x') is an odd function. This should be clear from Fig. 1.8b or Eq. 
</p>
<p>(1.10.20). An equivalent way to describe the action of the o' function is by the 
equation 
</p>
<p>d 
o'(x-x') = o(x-x')-
</p>
<p>dx' 
(1.10.22) 
</p>
<p>where it is understood that both sides appear in an integral over x' and that the 
</p>
<p>differential operator acts on any function that accompanies the o' function in the 
integrand. In this notation we can describe the action of higher derivatives of the 
</p>
<p>delta function: 
</p>
<p>(1.10.23) 
</p>
<p>We will now develop an alternate representation of the delta function. We know 
</p>
<p>from basic Fourier analysis that, given a functionf(x), we may define its transform 
</p>
<p>1 f 00 f(k)=--1-12 e-ikx f(x) dx 
(2n) --n 
</p>
<p>(1.10.24) </p>
<p/>
</div>
<div class="page"><p/>
<p>and its inverse 
</p>
<p>J(x')=~J-1_.'2 fx' eikxj(k) dk 
(2n) ,. &middot;&middot;&middot;cc 
</p>
<p>Feeding Eq. (1.10.24) into Eq. (1.10.25), we get 
</p>
<p>Comparing this result with Eq. (1.10.18), we see that 
</p>
<p>1 fx d'k ik(x' .. x)_ "( ' ) - e -u X ...... x 
2n -x 
</p>
<p>( 1.1 0.25) 
</p>
<p>( 1.1 0.26) 
</p>
<p>Exercise I.JO. I.* Show that D(ax) = 8(x)/lal. [Consider J 8(ax) d(ax). Remember that 
8(x) = 8( -x).] 
</p>
<p>Exercise 1.10.2. * Show that 
</p>
<p>8(x,-x) 
8({(x))=L:&middot; ..................... .. 
</p>
<p>&middot; &middot; , ldf/dx,j 
</p>
<p>where x, are the zeros off(x). Hint: Where does 15(/(x)) blow up? Expand.f(.C~:) near such 
</p>
<p>points in a Taylor series, keeping the first nonzero term. 
</p>
<p>Exercise 1.10.3. * Consider the thetafimction B(x- x') which vanishes if x- x' is negative 
and equals l if x-x' is positive. Show that 8(x-x')=d/dx IJ(x-x'). 
</p>
<p>Operators in Infinite Dimensions 
</p>
<p>Having acquainted ourselves with the elements of this function space, namely, 
</p>
<p>the kets If) and the basis vectors lx), let us turn to the (linear) operators that act 
on them. Consider the equation 
</p>
<p>O.f) =I]&gt; 
</p>
<p>Since the kets are in correspondence with the functions, 0 takes the function f(x) 
</p>
<p>into another,](x). Now, one operator that does such a thing is the familiar differen-
</p>
<p>tial operator, which, acting onf(x), gives](x) ={{((x)jdx. In the function space we 
can describe the action of this operator as 
</p>
<p>Dl f&gt; = dfldx) 
</p>
<p>where ldf/dx) is the ket corresponding to the function djjdx. What are the matrix 
elements of D in the I x) basis? To find out, we dot both sides of the above equation 
</p>
<p>63 
</p>
<p>MATHEMATICAL 
INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>64 
</p>
<p>CHAPTER I 
</p>
<p>with (xi, 
</p>
<p>and insert the resolution of identity at the right place 
</p>
<p>Jl' (xI D I x') (x' If) dx' =~I{ dx 
</p>
<p>Comparing this to Eq. (l.I0.21), we deduce that 
</p>
<p>ID 
d 
</p>
<p>=D .. &middot;= 8'(x-x') = o(x-x')-
n &middot; &middot; dx' 
</p>
<p>(1.10.27) 
</p>
<p>(1.10.28) 
</p>
<p>It is worth remembering that Du = o '(x- .x') is to be integrated over the second index 
(x') and pulls aut the derivative off at the first index (x). Some people prefer to 
</p>
<p>integrate o'(x-x') over the first index, in which case it pulls out -df/dx'. Our 
</p>
<p>convention is more natural if one views Dxx' as a matrix acting to the right on the 
</p>
<p>components f~ =f(x') of a vector If). Thus the familiar differential operator is an 
infinite-dimensional matrix with the elements given above. Normally one doesn't 
</p>
<p>think of D as a matrix for the following reason. Usually when a matrix acts on a 
</p>
<p>vector, there is a sum over a common index. In fact, Eq. ( 1.1 0.27) contains such a 
</p>
<p>sum over the index X 1&bull; If, however, we feed into this equation the value of Dx-:&middot;, the 
delta function renders the integration trivial: 
</p>
<p>J. ') d /' ' d ' ({f 8(x .... x- (x) x=-d.x'&middot; dxl 
df 
</p>
<p>dx 
</p>
<p>Thus the action of Dis simply to apply d/d.x tof(x) with no sum over a common 
</p>
<p>index in sight. Although we too will drop the integral over the common index 
</p>
<p>ultimately, we will continue to use it for a while to remind us that D, like all linear 
operators, is a matrix. 
</p>
<p>Let us now ask if Dis Hermitian and examine its eigenvalue problem. If D were 
</p>
<p>Hermitian, we would have 
</p>
<p>D,,.=D~, 
</p>
<p>But this is not the case: 
</p>
<p>D"=o'(x-x') 
</p>
<p>while 
</p>
<p>D~&middot;x = 0 '(x'- = 8'(x'- x) = -o'(x- x') </p>
<p/>
</div>
<div class="page"><p/>
<p>But we can easily convert D to a Hermitian matrix by multiplying it with a pure 
imaginary number. Consider 
</p>
<p>K=-iD 
</p>
<p>which satisfies 
</p>
<p>x:x= [-io'(x'-x)]*= +io'(x'-x)= -io'(x- x') =K,:.: 
</p>
<p>It turns out that despite the above, the operator K is not guaranteed to be Hermitian, 
as the following analysis will indicate. Let I/) and lg) be two kets in the function 
space, whose images in the X basis are two functions f(x) and g(x) in the interval 
a- b. If K is Hermitian, it must also satisfy 
</p>
<p>(giK]/)= (gl Kf)=(Kfig)*=&lt;JIKtig)*= (/IK]g)* 
</p>
<p>So we ask 
</p>
<p>r r (gix)(xiK]x')(x'lf)dxdx' 
a a 
</p>
<p>:!: (f r &lt;II X&gt; &lt;xl K] x') (x' I g &gt; dx dxJ" 
a a 
</p>
<p>Integrating the left-hand side by parts gives 
</p>
<p>-ig*(x)f(x) +i dg (~}f(x)dx 1
/; fb * 
a a dx 
</p>
<p>So K is Hermitian only if the surface term vanishes: 
</p>
<p>-ig*(x)f(x{ =0 (1.10.29) 
</p>
<p>In contrast to the finite-dimensional case, Kxx' = x:x is not a sufficient condition for 
K to be Hermitian. One also needs to look lit the behavior of the functions at the 
end points a and b. Thus K is Hermitian if the space consists of functions that 
obey Eq. (1.10.29). One set of functions that obey this condition are the possible 
configurationsf(x) of the string clamped at x=O, L, sincef(x) vanishes at the end 
points. But condition (1.10.29) can also be fulfilled in another way. Consider 
functions,Jn our own three-dimensional space, parametrized by r, (}, and 4&gt; ( 4&gt; is the 
angle measured around the z axis). Let us require that these functions be single 
</p>
<p>65 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>66 
</p>
<p>CHAPTER I 
</p>
<p>valued. In particular, if we start at a certain point and go once around the z axis, 
returning to the original point, the function must take on its original value, i.e., 
</p>
<p>f(cp)=f(&cent;+2n:) 
</p>
<p>In the space of such periodic functions, K= -i d/dcp is a Hermitian operator. The 
surface term vanishes because the contribution from one extremity cancels that from 
</p>
<p>the other: 
</p>
<p>1
</p>
<p>2rr 
</p>
<p>-ig*( &cent;) /(&cent;) 
0 
</p>
<p>= -i[g*(2n:) f (2n:)- g*(O)f(O)] = 0 
</p>
<p>In the study of quantum mechanics, we will be interested in functions defined 
over the full interval - oo ::;; x::;; + oo. They fall into two classes, those that vanish as 
lxl -+ oo, and those that do not, the latter behaving as eikX, k being a real parameter 
that labels these functions. It is dear that K= -i d/dx is Hermitian when sandwiched 
</p>
<p>between two functions of the first class or a function from each, since in either case 
</p>
<p>the surface term vanishes. When sandwiched between two functions of the second 
</p>
<p>class, the Hermiticity hinges on whether 
</p>
<p>eikx e-ik'xloo ~0 
-oe 
</p>
<p>If k=k', the contribution from one end cancels that from the other. If kiok', the 
answer is unclear since ei&lt;k-k')x oscillates, rather than approaching a limit as lxl --+ oo. 
Now, there exists a way of defining a limit for such functions that cannot make up 
</p>
<p>their minds: the limit as lxl -+ oo is defined to be the average over a large interval. 
According to this prescription, we have, say as x-+ oo, 
</p>
<p>dx=O ifki:-k' 
</p>
<p>and so K is Hermitian in this space. 
We now tum to the eigenvalue problem of K. The task seems very formidable 
</p>
<p>indeed, for we have now to find the roots of an infinite-order characteristic poly-
nomial and get the corresponding eigenvectors. It turns out to be quite simple and 
you might have done it a few times in the past without giving yourself due credit. 
</p>
<p>Let us begin with 
</p>
<p>K]k)=klk&gt; (1.10.30) </p>
<p/>
</div>
<div class="page"><p/>
<p>Following the standard procedure, 
</p>
<p>(xi.K]k) ==k(xl k) 
</p>
<p>f (xj.K]x') (x'l k) dx' =k'l'k(x) 
.d 
</p>
<p>-z- IP'k(x) =k'lfk(x) 
dx 
</p>
<p>(1.10.31) 
</p>
<p>where by definition 'l'k(x) =(xi k). This equation could have been written directly 
</p>
<p>had we made the immediate substitution K= -i d/dx in the X basis. From now on 
we shall resort to this shortcut unless there are good reasons for not doing so. 
</p>
<p>The solution to the above equation is simply 
</p>
<p>(l.l 0.32) 
</p>
<p>where A, the overall scale, is a free parameter, unspecified by the eigenvalue problem. 
</p>
<p>So the eigenvalue problem of K is fully solved: any real number k is an eigenvalue, 
and the corresponding eigenfunction is given by A e1kx. As usual, the freedom in 
</p>
<p>scale will be used to normalize the solution. We choose A to be (l/2nr 112 so that 
</p>
<p>jk).,.... _1 _____ eikx 
(2n)l/2 
</p>
<p>and 
</p>
<p>(klk')= fro (k!x)(xlk')dx=2~ Joo e-i(k-k')xdx=.S(k-k') (1.10.33) 
-oo -oo 
</p>
<p>(Since (k I k) is infinite, no choice of A can normalize lk) to unity. The delta function 
</p>
<p>normalization is the natural one when the eigenvalue spectrum is continuous.) 
</p>
<p>The attentive reader may have a question at this point. 
</p>
<p>"Why was it assumed that the eigenvalue k was real? It is clear that the function 
A eikx with k=k 1 +ik2 also satisfies Eq. (1.10.31)." 
</p>
<p>The answer is, yes, there are eigenfunctions of K with complex eigenvalues. If, 
however, our space includes such functions, K must be classified a non-Hermitian 
</p>
<p>operator. (The surface term no longer vanishes since eikx blows up exponentially as 
</p>
<p>x tends to either +oo or -oo, depending on the sign of the imaginary part k2-) In 
restricting ourselves to real k we have restricted ourselves to what we will call the 
physical Hilbert space, which is of interest in quantum mechanics. This space is 
defined as the space of functions that can be either normalized to unity or to the 
</p>
<p>Dirac delta function and plays a central role in quantum mechanics. (We use the 
</p>
<p>qualifier "physical" to distinguish it from the Hilbert space as defined by mathemat-
icians, which contains only proper vectors, i.e., vectors normalizable to unity. The 
role of the improper vectors in quantum theory will be clear later.) 
</p>
<p>67 
</p>
<p>MATHEMATICAL 
INTRODUC"'TION </p>
<p/>
</div>
<div class="page"><p/>
<p>68 
</p>
<p>CHAPTER I 
</p>
<p>We will assume that the theorem proved for finite dimensions, namely, that the 
</p>
<p>eigenfunctions of a Hermitian operator form a complete basis, holds in the Hilbertt 
</p>
<p>space. (The trouble with infinite-dimensional spaces is that even if you have an 
</p>
<p>infinite number of orthonormal eigenvectors, you can never be sure you have them 
</p>
<p>all, since adding or subtracting a few still leaves you with an infinite number of 
</p>
<p>them.) 
</p>
<p>Since K is a Hermitian operator, functions that were expanded in the X basis 
</p>
<p>with componentsf(x)=(xlf&gt; must also have an expansion in the K basis. To find 
</p>
<p>the components, we start with a ket If), and do the following: 
</p>
<p>f(k)=(klf&gt;= fx" (kjx)(xlf)dx=(2:)112 f .f(x) dx (1.10.34) 
The passage back to the X basis is done as follows: 
</p>
<p>.f(x)=(xl.f&gt;= fcc (klx)(klf)dk= (2:)1/2 rxy_ e1k'f(k)dk (1.10.35) 
</p>
<p>Thus the familiar Fourier transform is just the passage from one complete basis 
</p>
<p>to another, lk). Either basis may be used to expand functions that belong to the 
</p>
<p>Hilbert space. 
</p>
<p>The matrix elements of K are trivial in the K basis: 
</p>
<p>(kl Klk') =k'(kl k') =k'8(k -k') (1.10.36) 
</p>
<p>Now, we know where the K basis came from: it was generated by the Hermitian 
</p>
<p>operator K. Which operator is responsible for the orthonormal X basis? Let us call 
it the operator X. The kets lx) are its eigenvectors with eigenvalue x: 
</p>
<p>Xlx) =xlx) 
</p>
<p>Its matrix elements in the X basis are 
</p>
<p>(x'IXIx) = x1S(x'- x) 
</p>
<p>To find its action on functions, let us begin with 
</p>
<p>XI/)=!]) 
</p>
<p>and follow the routine: 
</p>
<p>(xiXI /) = J (xiX!x') (x' I/) dx' = xf(x) = (x ll&gt; =l(x) 
.'. _1(x) = x.f(x) 
</p>
<p>:j: Hereafter we will omit the qualifier "physical." 
</p>
<p>( 1.10.37) 
</p>
<p>( 1.10.38) </p>
<p/>
</div>
<div class="page"><p/>
<p>Thus the effect of X is to multiply f(x) by x. As in the case of the K operator, one 
</p>
<p>generally suppresses the integral over the common index since it is rendered trivial 
</p>
<p>by the delta function. We can summarize the action of X in Hilbert space as 
</p>
<p>Xlf(x))=ixf(x)) ( 1.1 0.39) 
</p>
<p>where as usual lxf(x)) is the ket corresponding to the function xf(x). 
</p>
<p>There is a nice reciprocity between the X and K operators which manifests itself 
if we compute the matrix elements of X in the K basis: 
</p>
<p>d ( 1 J'"' . . . ') = +i dk 2; e'&lt;k -k)' dx = io '(k --- k')t 
---.x 
</p>
<p>Thus if jg(k)) is a ket whose image in the k basis is g(k), then 
</p>
<p>Xlg(k)) =I i dg(k)) 
dk 
</p>
<p>(1.1 0.40) 
</p>
<p>In summary then, in the X basis, X acts as x and K as -i djdx [on the functions 
</p>
<p>f(x)], while in the K basis, K acts like k and X like i djdk [onf(k)]. Operators with 
such an interrelationship are said to be conjugate to each other. 
</p>
<p>The conjugate operators X and K do not commute. Their commutator may be 
</p>
<p>calculated as follows. Let us operate X and K in both possible orders on some ket 
If) and follow the action in the X basis: 
</p>
<p>So 
</p>
<p>Therefore 
</p>
<p>XI .f)-&gt; 4(x) 
</p>
<p>Jq f)__. -i df(x) 
dx 
</p>
<p>. df(x) 
X K] .f) -&gt; -zx ---------------
</p>
<p>dx 
</p>
<p>d 
KXI.f) _,. -i ------ :x.f(x) 
</p>
<p>dx 
</p>
<p>[X, K]l.f)-&gt; -ix df_+ ix d.f +if= if&middot;-'&gt; il] /) 
dx dx 
</p>
<p>tIn the last step we have used the fact that o(k' --- k) = o(k --- k'). 
</p>
<p>69 
MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>70 
</p>
<p>CHAPTER I 
</p>
<p>Since If) is an arbitrary ket, we now have the desired result: 
</p>
<p>[X, K]=il (1.10.41) 
</p>
<p>This brings us to the end of our discussion on Hilbert space, except for a final 
</p>
<p>example. Although there are many other operators one can study in this space, we 
</p>
<p>restricted ourselves to X and K since almost all the operators we will need for 
</p>
<p>quantum mechanics are functions of X and P= nK, where n is a constant to be 
</p>
<p>defined later. 
</p>
<p>Example 1.10.1: A Normal Mode Problem in Hilbert Space. Consider a string 
</p>
<p>of length L clamped at its two ends x = 0 and L. The displacement 1Jf(x, t) obeys the 
</p>
<p>differential equation 
</p>
<p>( 1.10.42) 
</p>
<p>Given that at t = 0 the displacement is 1Jf(x, 0) and the velocity ift(x, 0) = 0, we wish 
</p>
<p>to determine the time evolution of the string. 
</p>
<p>But for the change in dimensionality, the problem is identical to that of the 
</p>
<p>two coupled masses encountered at the end of Section 1.8 [see Eq. (1.8.26)]. It is 
</p>
<p>recommended that you go over that example once to refresh your memory before 
</p>
<p>proceeding further. 
</p>
<p>We first identify 1Jf(x, t) as components of a vector llJI( t)) in a Hilbert space, 
</p>
<p>the elements of which are in correspondence with possible displacements 1Jf, i.e., 
</p>
<p>functions that are continuous in the interval 0 ~ x ~ L and vanish at the end points. 
</p>
<p>You may verify that these functions do form a vector space. 
</p>
<p>The analog of the operator n in Eq. ( 1.8.26) is the operator iP I 8x2 &bull; We recognize 
this to be minus the square of the operator K ~ -i8j8x. Since K acts on a space in 
</p>
<p>which 1J1(0)= 1Jf(L)=O, it is Hermitian, and so is K 2 . Equation (1.10.42) has the 
</p>
<p>abstract counterpart 
</p>
<p>( 1.10.43) 
</p>
<p>We solve the initial-value problem by following the algorithm developed in Example 
</p>
<p>1.8.6: 
</p>
<p>Step (1). Solve the eigenvalue problem of -K2&bull; 
</p>
<p>Step (2). Construct the propagator U(t) in terms of the eigenvectors and 
</p>
<p>eigenvalues. 
</p>
<p>Step (3). 
</p>
<p>11J!(t)) = U(t)I1J!(O)) (1.10.44) </p>
<p/>
</div>
<div class="page"><p/>
<p>The equation to solve is 
</p>
<p>(1.10.45) 
</p>
<p>In the X basis, this becomes 
</p>
<p>(1.10.46) 
</p>
<p>the general solution to which is 
</p>
<p>'1-'k(x) =A cos kx + B sin kx (1.10.47) 
</p>
<p>where A and Bare arbitrary. However, not all these solutions lie in the Hilbert space 
we are considering. We want only those that vanish at x=O and x=L. At x=O we 
find 
</p>
<p>lf/k(O)=O=A (1.10.48a) 
</p>
<p>while at x = L we find 
</p>
<p>O=BsinkL (1.10.48b) 
</p>
<p>If we do not want a trivial solution (A= B= 0) we must demand 
</p>
<p>sin kL=O, kL=mn, m= 1, 2, 3, ... (1.10.49) 
</p>
<p>We do not consider negative m since it doesn't lead to any further LI solutions 
[sin( -x) =-sin x]. The allowed eigenveQ'tors thus form a discrete set labeled by an 
integer m: 
</p>
<p>( 2 )
112 
</p>
<p>&bull; (mnx) lf/m(x) = L sm L (1.10.50) 
</p>
<p>where we have chosen B=(2/L} 112 so that 
</p>
<p>JL lf/m(X)lflrn'(x) dx=Omm' 
0 
</p>
<p>(1.10.51) 
</p>
<p>Let us associate with each solution labeled by the integer man abstract ket lm): 
</p>
<p>lm&gt;---&gt; (2/ L} 112 sin (mnx) 
Xbasis L 
</p>
<p>(1.10.52) 
</p>
<p>71 
</p>
<p>MATHEMATICAL 
INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>72 
</p>
<p>CHAPTER 1 
</p>
<p>If we project llfl(t)) on the lm) basis, in which K is diagonal with eigenvalues 
</p>
<p>(mn I L)2, the components (m llf/(1)) will obey the decoupled equations 
</p>
<p>m= 1, 2, ... (1.10.53) 
</p>
<p>in analogy with Eq. (1.8.33). These equations may be readily solved (subject to the 
</p>
<p>condition of vanishing initial velocities) as 
</p>
<p>Consequently 
</p>
<p>or 
</p>
<p>(mn1) (mllf!(1))=(mllf!(O))cos L 
</p>
<p>X' 
</p>
<p>llf/(1))= L lm)(mllfl(t)) 
m=l 
</p>
<p>X 
</p>
<p>= L lm) (m llf/(0)) COS COm1, 
m=I 
</p>
<p>'lj 
</p>
<p>U(t) = L lm) (m I cos COm1, 
m=l 
</p>
<p>m7r 
co =-
</p>
<p>m L 
</p>
<p>mn 
co =-
</p>
<p>m L 
</p>
<p>The propagator equation 
</p>
<p>ilfl(l)) = U(1)ilfi(O)) 
</p>
<p>becomes in the I x) basis 
</p>
<p>(x llf/(1)) = lf!(X, 1) 
</p>
<p>=(xi U(1)llf/(O)) 
</p>
<p>= foL (xl U(1)lx') (x' llf/(0)) dx' 
</p>
<p>It follows from Eq. (1.10.56) that 
</p>
<p>(xl U(1)lx')=I (xlm) (ml x') cos COm1 
</p>
<p>" ( 2) . (mnx) . (mnx') 
=~ L sm L sm L coscom1 
</p>
<p>(1.10.54) 
</p>
<p>( 1.10.55) 
</p>
<p>(1.10.56) 
</p>
<p>( 1.10.57) 
</p>
<p>(1.10.58) </p>
<p/>
</div>
<div class="page"><p/>
<p>Thus, given any lJI(x', 0), we can get lJI(X, t) by performing the integral in Eq. 
</p>
<p>(1.10.57), using (x\ U(t)\x') from Eq. (1.10.58). If the propagator language seems 
</p>
<p>too abstract, we can begin with Eq. (1.10.55). Dotting both sides with (x\, we get 
</p>
<p>00 
</p>
<p>lJI(x,t)= I (x\m)(m\lJI(O))cosmmt 
m=l 
</p>
<p>oo ( 2 )
112 
</p>
<p>. (mtcx) I - sm -- cos Wmt(m\ lJI(O)) 
m~I L L 
</p>
<p>(1.10.59) 
</p>
<p>Given \IJI(O)), one must then compute 
</p>
<p>Usually we will find that the coefficients (m \IJI(O)) fall rapidly with m so that a few 
</p>
<p>leading terms may suffice to get a good approximation. D 
</p>
<p>Exercise I. 10.4. A string is displaced as follows at t = 0: 
</p>
<p>Show that 
</p>
<p>2xh 
IJI(x,O)=z:&middot; 
</p>
<p>2h 
=z;(L-x), 
</p>
<p>L 
O&lt;x&lt;-- -2 
</p>
<p>L 
-&lt;x&lt;L 2- -
</p>
<p>( ) ~ . (mtrx) ( 8h ) . ( trm) lJI x, t = L.... sm - cos romt &middot; ~ sm -
m~l L tr m 2 
</p>
<p>73 
</p>
<p>MATHEMATICAL 
</p>
<p>INTRODUCTION </p>
<p/>
</div>
<div class="page"><p/>
<p>2 
</p>
<p>Review of Classical Mechanics 
</p>
<p>In this chapter we will develop the Lagrangian and Hamiltonian formulations of 
mechanics starting from Newton's laws. These subsequent reformulations of mechan-
ics bring with them a great deal of elegance and computational ease. But our principal 
interest in them stems from the fact that they are the ideal springboards from which 
to make the leap to quantum mechanics. The passage from the Lagrangian formula-
tion to quantum mechanics was carried out by Feynman in his path integral formal-
ism. A more common route to quantum mechanics, which we will follow for the 
most part, has as its starting point the Hamiltonian formulation, and it was dis-
covered mainly by Schrodinger, Heisenberg, Dirac, and Born. 
</p>
<p>It should be emphasized, and it will soon become apparent, that all three formu-
lations of mechanics are essentially the same theory, in that their domains of validity 
and predictions are identical. Nonetheless, in a given context, one or the other may 
be more inviting for conceptual, computational, or simply aesthetic reasons. 
</p>
<p>2.1. &middot;The Principle of Least Action and Lagrangian Mechanics 
</p>
<p>Let us take as our prototype of the Newtonian scheme a point particle of mass 
m moving along the x axis under a potential V(x). According to Newton's Second 
Law, 
</p>
<p>(2.1.1) 
</p>
<p>If we are given the initial state variables, the position x(tt) and velocity x(tt), we 
can calculate the classical trajectory Xcr (t) as follows. Using the initial velocity and 
acceleration [obtained from Eq. (2.1.1)] we compute the position and velocity at a 
time t1+ At. For example, 
</p>
<p>Having updated the state variables to the time t1+At, we can repeat the process 
again to inch forward to t1+2At and so on. 75 </p>
<p/>
</div>
<div class="page"><p/>
<p>76 
</p>
<p>CHAPTER 2 
</p>
<p>X 
</p>
<p>Figure 2.1. The Lagrangian formalism asks what dis-
</p>
<p>tinguishes the actual path xc1 ( 1) taken by the particle from 
</p>
<p>all possible paths connecting the end points (x,, t;) and 
</p>
<p>(x1, t1 ). 
</p>
<p>The equation of motion being second order in time, two pieces of data, x(t;) 
</p>
<p>and x(t;), are needed to specify a unique xc1 (t). An equivalent way to do the same, 
</p>
<p>and one that we will have occasion to employ, is to specify two space-time points 
</p>
<p>(x;, t;) and (xf, t1 ) on the trajectory. 
</p>
<p>The above scheme readily generalizes to more than one particle and more than 
</p>
<p>one dimension. If we use n Cartesian coordinates (x1 , X2, ... , Xn) to specify the 
</p>
<p>positions of the particles, the spatial configuration of the system may be visualized 
</p>
<p>as a point in an n-dimensional configuration space. (The term "configuration space" 
</p>
<p>is used even if then coordinates are not Cartesian.) The motion of the representative 
</p>
<p>point is given by 
</p>
<p>(2.1.2) 
</p>
<p>where mj stands for the mass of the particle whose coordinate is xj. These equations 
</p>
<p>can be integrated step by step, just as before, to determine the trajectory. 
</p>
<p>In the Lagrangian formalism, the problem of a single particle in a potential 
</p>
<p>V(x) is posed in a different way: given that the particle is at X; and x1 at times t; and 
</p>
<p>t1 , respectively, what is it that distinguishes the actual trajectory xc1 (t) from all other 
</p>
<p>trajectories or paths that connect these points? (See Fig. 2.1.) 
</p>
<p>The Lagrangian approach is thus global, in that it tries to determine at one 
</p>
<p>stroke the entire trajectory Xc1 (t), in contrast to the local approach of the Newtonian 
</p>
<p>scheme, which concerns itself with what the particle is going to do in the next 
</p>
<p>infinitesimal time interval. 
</p>
<p>The answer to the question posed above comes in three parts: 
</p>
<p>(l) Define a function !&pound;',called the Lagrangian, given by!&pound;= T-V, T and V 
</p>
<p>being the kinetic and potential energies of the particle. Thus !&pound; = !f(x, x, t). The 
explicit t dependence may arise if the particle is in an external time-dependent field. 
</p>
<p>We will, however, assume the absence of this t dependence. 
</p>
<p>(2) For each path x(t) connecting (xio t;) and (x1 , t1 ), calculate the action 
</p>
<p>S[x(t)] defined by 
</p>
<p>fit S[x{t)] = !f'(x, x) dt 
,, 
</p>
<p>(2.1.3) </p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 2.2. If xc1 (t) minimizes S, then 8S 11 &gt; =0 if we 
go to any nearby path xc1 (t) + TJ(I). 
</p>
<p>lC 
</p>
<p>We use square brackets to enclose the argument of S to remind us that the function 
S depends on an entire path or function x(t), and not just the value of x at some 
time t. One calls S afunctional to signify that it is a function of a function. 
</p>
<p>(3) The classical path is one on which S is a minimum. (Actually we will only 
require that it be an extremum. It is, however, customary to refer to this condition 
as the principle of least action.) 
</p>
<p>We will now verify that this principle reproduces Newton's Second Law. 
The first step is to realize that a functional S[x(t)] is just a function ofn variables 
</p>
<p>as n-+oo. In other words, the function x(t) simply specifies an infinite number of 
values x(t;), ... , x(t), ... , x(t1 ), one for each instant in time t in the interval 
t;$,t$,tf&gt; and Sis a function of these variables. To find its minimum we simply 
generalize the procedure for the finite n case. Let us recall that iff= f(x 1 , &bull;&bull; &bull; , Xn) = 
f(x); the minimum x0 is characterized by the fact that if we move away from it by 
a small amount TJ in any direction, the first-order change lif0 ) in f vanishes. That 
is, if we make a Taylor expansion: 
</p>
<p>then 
</p>
<p>f(x0 +TJ)=f(x0)+ I. ofi1J;+higher-ordertermsin 11 
;-I OX; x0 
</p>
<p>(2.1.4) 
</p>
<p>(2.1.5) 
</p>
<p>From this condition we can deduce an equivalent and perhaps more familiar 
expression of the minimum condition: every first-order partial derivative vanishes at 
x0 &bull; To prove this, for say, of/ox;, we simply choose TJ to be along the ith direction. 
Thus 
</p>
<p>of! =o 
OX; x0 ' 
</p>
<p>i=l, ... ,n (2.1.6) 
</p>
<p>Let us now mimic this procedure for the action S. Let xc1 (t) be the path of least 
action and xc1 (t) + 17(1) a "nearby" path (see Fig. 2.2). The requirement that all 
paths coincide at t; and t1 means 
</p>
<p>(2.1.7) 
</p>
<p>77 
</p>
<p>REVIEW OF 
CLASSICAL 
</p>
<p>MECHANICS </p>
<p/>
</div>
<div class="page"><p/>
<p>78 
CHAPTER 2 
</p>
<p>Now 
</p>
<p>I,, S[xc1 (t) + 17(t)] = !t'(xc~{t) + 17(t); ic~(t) + l)(t)) dt 
t, 
</p>
<p>I
('[ a!t' I = . !t'(xc~(t), ic1(t)) + -- &middot; 17(t) 
</p>
<p>t, ax(f) Xd 
</p>
<p>+ a!t' I . l)(t)+&middot; &middot; &middot;Jdt 
a.x(t) x" 
</p>
<p>= S[ xc1 ( t)] + 8 S 0 1 + higher -order terms 
</p>
<p>We set os&lt; 11 =0 in analogy with the finite variable case: 
</p>
<p>I
''[ a!t' I . a!t' I J 0=8S&lt; 11 = -- &middot; 17(t)+-.- &middot;l)(t) dt 
</p>
<p>I; ax(f) Xd ax(f) X&lt;] 
</p>
<p>If we integrate the second term by parts, it turns into 
</p>
<p>a!t' I l'f I''[d a!t' J - &middot; 11U) - - - &middot; 77{t) dt 
ax(t) X" ,, '; dt ax(t) x" 
</p>
<p>The first of these terms vanishes due to Eq. (2.1.7). So that 
</p>
<p>o= 8s&lt; 11 = I'~[ a!t' _!:__ a!t' J &middot; 11(t) dt 
I; ax(t) dt ax(t) x" 
</p>
<p>(2.1.8) 
</p>
<p>Note that the condition 8s&lt;IJ = 0 implies that S is extremized and not necessarily 
</p>
<p>minimized. We shall, however, continue the tradition of referring to this extremum 
</p>
<p>as the minimum. This equation is the analog of Eq. (2.1.5): the discrete variable 1]; 
</p>
<p>is replaced by 17(t); the sum over i is replaced by an integral over t, and ajj ax; is 
</p>
<p>replaced by 
</p>
<p>a2 d a2 
</p>
<p>ax(t) dt ax(t) 
</p>
<p>There are two terms here playing the role of ajjax; since !t' (or equivalently S) has 
</p>
<p>both explicit and implicit (through the .X terms) dependence on x(t). Since 77(t) is 
</p>
<p>arbitrary, we may extract the analog of Eq. (2.1.6): 
</p>
<p>{ a!t' _!__[a2 ]} =0 fort;5,t5,tf 
ax(t) df ax(t) Xd{l) 
</p>
<p>(2.1.9) 
</p>
<p>To deduce this result for some specific time t0 , we simply choose an 1J{t) that vanishes 
</p>
<p>everywhere except in an infinitesimal region around t0 &bull; </p>
<p/>
</div>
<div class="page"><p/>
<p>Equation (2.1.9) is the celebrated Euler-Lagrange equation. If we feed into it 
:&pound;=T-V, T= 1mx2, V= V(x), we get 
</p>
<p>off ar . 
-=-=mx 
ox ox 
</p>
<p>and 
</p>
<p>a:&pound; av 
ox ox 
</p>
<p>so that the Euler-Lagrange equation becomes just 
</p>
<p>d av 
-(mx)=--
dt ox 
</p>
<p>which is just Newton's Second Law, Eq. (2.1.1). 
If we consider a system described by n Cartesian coordinates, the same procedure 
</p>
<p>yields 
</p>
<p>~ (a~)= off (i= 1, ... 'n) 
dt OX; OX; 
</p>
<p>Now 
</p>
<p>n 
</p>
<p>T=~ I m;(x;f 
i=l 
</p>
<p>and 
</p>
<p>V= V(Xt, ... , Xn) 
</p>
<p>so that Eq. (2.1.10) becomes 
</p>
<p>d av 
-(m;x;)=--
dt OX; 
</p>
<p>(2.1.10) 
</p>
<p>which is identical to Eq. (2.1.2). Thus the minimum (action) principle indeed repro-
duces Newtonian mechanics if we choose :&pound; = T- V. 
</p>
<p>Notice that we have assumed that Vis velocity-independent in the above proof. 
An important force, that of a magnetic field B on a moving charge is excluded by 
this restriction, since F B = qv x B, q being the charge of the particle and v = t its 
velocity. We will show shortly that this force too may be accommodated in the 
Lagrangian formalism, in the sense that we can find an :&pound; that yields the correct 
force law when Eq. (2.1.10) is employed. But this:&pound; no longer has the form T-V. 
One therefore frees oneself from the notion that:&pound;= T-V; and views:&pound; as some 
</p>
<p>79 
</p>
<p>REVIEW OF 
CLASSICAL 
</p>
<p>MECHANICS </p>
<p/>
</div>
<div class="page"><p/>
<p>80 
</p>
<p>CHAPTER 2 
</p>
<p>function 2'(x;, i;) which yields the correct Newtonian dynamics when fed into the 
</p>
<p>Euler-Lagrange equations. To the reader who wonders why one bothers to even 
</p>
<p>deal with a Lagrangian when all it does is yield Newtonian force laws in the end, I 
</p>
<p>present a few of its main attractions besides its closeness to quantum mechanics. 
</p>
<p>These will then be illustrated by means of an example. 
</p>
<p>( 1) In the Lagrangian scheme one has merely to construct a single scalar !&pound;' 
</p>
<p>and all the equations of motion follow by simple differentiation. This must be con-
</p>
<p>trasted with the Newtonian scheme, which deals with vectors and is thus more 
</p>
<p>complicated. 
</p>
<p>(2) The Euler-Lagrange equations (2.1.10) have the same form if we use, instead 
</p>
<p>of then Cartesian coordinates x &bull;&bull;... , Xn, any general set of n independent coordi-
</p>
<p>nates q1 , q2, ... , q". To remind us of this fact we will rewrite Eq. (2.1.10) as 
</p>
<p>(2.1.11) 
</p>
<p>One can either verify this by brute force, making a change of variables in Eq. (2.1.10) 
</p>
<p>and seeing that an identical equation with x; replaced by q; follows, or one can simply 
</p>
<p>go through our derivation of the minimum action condition and see that nowhere 
</p>
<p>were the coordinates assumed to be Cartesian. Of course, at the next stage, in showing 
</p>
<p>that the Euler-Lagrange equations were equivalent to Newton's, Cartesian coordi-
</p>
<p>nates were used, for in these coordinates the kinetic energy T and the Newtonian 
</p>
<p>equations have simple forms. But once the principle ofleast action is seen to generate 
</p>
<p>the correct dynamics, we can forget all about Newton's laws and use Eq. (2.1.11) 
</p>
<p>as the equations of motion. What is being emphasized is that these equations, which 
</p>
<p>express the condition for least action, are form invariant under an arbitrary change 
</p>
<p>of coordinates. This form in variance must be contrasted with the Newtonian equation 
</p>
<p>(2.1.2), which presumes that the X; are Cartesian. If one trades the X; for another 
</p>
<p>non-Cartesian set of q;, Eq. (2.1.2) will have a different form (see Example 2.1.1 at 
</p>
<p>the end of this section). 
</p>
<p>Equation (2.1.11) can be made to resemble Newton's Second Law if one defines 
</p>
<p>a quantity 
</p>
<p>called the canonical momentum conjugate to q; and the quantity 
</p>
<p>8!&pound;' 
F;=-
</p>
<p>oq; 
</p>
<p>(2.1.12) 
</p>
<p>(2.1.13) 
</p>
<p>called the generalized force conjugate to q;. Although the rate of change of the 
</p>
<p>canonical momentum equals the generalized force, one must remember that neither 
</p>
<p>is p; always a linear momentum (mass times velocity or "mv" momentum), nor is F; 
</p>
<p>always a force (with dimensions of mass times acceleration). For example, if q; is an 
</p>
<p>angle (), p; will be an angular momentum and F; a torque. </p>
<p/>
</div>
<div class="page"><p/>
<p>(3) Conservation laws are easily obtained in this formalism. Suppose the Lag-
</p>
<p>rangian depends on a certain velocity q; but not on the corresponding coordinate q;. 
The latter is then called a cyclic coordinate. [t follows that the corresponding p; is 
</p>
<p>conserved: 
</p>
<p>(2.1.14) 
</p>
<p>Although Newton's Second Law, Eq. (2.1.2), also tells us that if a Cartesian coordi-
</p>
<p>nate X; is cyclic, the corresponding momentum m;x; is conserved, Eq. (2.1.14) is more 
</p>
<p>general. Consider, for example, a potential V(x, y) in two dimensions that depends 
</p>
<p>only upon p=(x2 +y) 112 , and not on the polar angle o/, so that V(p, &cent;)= V(p). It 
follows that &cent; is a cyclic coordinate, as T depends only on &cent; (see Example 2.1.1 
below). Consequently of!? I a&cent;= pq, is conserved. In contrast, no obvious conservation 
law arises from the Cartesian Eqs. (2.1.2) since neither x nor y is cyclic. If one 
</p>
<p>rewrites Newton's laws in polar coordinates to exploit o V/ ocfo = 0, the corresponding 
equations get complicated due to centrifugal and Coriolis terms. It is the Lagrangian 
</p>
<p>formalism that allows us to choose coordinates that best reflect the symmetry of the 
</p>
<p>potential, without altering the simple form of the equations. 
</p>
<p>Example 2.1.1. We now illustrate the above points through an example. Con-
</p>
<p>sider a particle moving in a plane. The Lagrangian, in Cartesian coordinates, is 
</p>
<p>!I?= ~m(.i 2 + Jj2)- V(x, y) 
</p>
<p>= hm"v- V(x, y) (2.1.15) 
</p>
<p>where v is the velocity of the particle, with v = r, r being its position vector. The 
corresponding equations of motion are 
</p>
<p>av 
mi=-
</p>
<p>ax 
</p>
<p>av 
my= .... -;;-
</p>
<p>oy 
</p>
<p>(2.1.16) 
</p>
<p>(2.1.17) 
</p>
<p>which are identical to Newton's laws. If one wants to get the same Newton's laws 
</p>
<p>in terms of polar coordinates p and &cent;, some careful vector analysis is needed to 
unearth the centrifugal and Corio lis terms: 
</p>
<p>av . , 
mp= -~:_ +mp(&cent;Y 
</p>
<p>op 
</p>
<p>.. 1 av 2mpcfo 
m&cent;=-;; ---~-
</p>
<p>p a&cent; P 
</p>
<p>(2.1.18) 
</p>
<p>(2.1.19) 
</p>
<p>81 
</p>
<p>REVIEW OF 
</p>
<p>CLASSICAL 
</p>
<p>MECHANTCS </p>
<p/>
</div>
<div class="page"><p/>
<p>82 
</p>
<p>CHAPTER 2 
y 
</p>
<p>Figure 2.3. Points (I) and (2) are positions of the 
</p>
<p>particle at times differing by At. 
</p>
<p>Notice the difference in form between Eqs. (2.1.16) and (2.1.17) on the one hand 
</p>
<p>and Eqs. (2.1.18) and (2.1.19) on the other. 
</p>
<p>In the Lagrangian scheme one has only to recompute !!:' in polar coordinates. 
</p>
<p>From Fig. 2.3 it is clear that the distance traveled by the particle in time !l.t is 
</p>
<p>so that the magnitude of velocity is 
</p>
<p>and 
</p>
<p>(2.1.20) 
</p>
<p>(Notice that in these coordinates T involves not just the velocities p and cfi but also 
the coordinate p. This does not happen in Cartesian coordinates.) The equations of 
</p>
<p>motion generated by this !!:' are 
</p>
<p>d av &middot;z 
-(mp)=--+mpcp 
dt op 
</p>
<p>d 2 &bull; av 
-(mp cp)=--
dt oc/J 
</p>
<p>(2.1.21) 
</p>
<p>(2.1.22) 
</p>
<p>which are the same as Eqs. (2.1.18) and (2.1.19). In Eq. (2.1.22) the canonical 
</p>
<p>momentump.p=mp2cfi is the angular momentum and the generalized force -oV/oc/J 
is the torque, both along the z axis. Notice how easily the centrifugal and Coriolis 
</p>
<p>forces came out. 
Finally, if V(p, cp)= V(p), the conservation of P&lt;t&gt; is obvious in Eq. (2.1.22). 
</p>
<p>The conservation of P&lt;t&gt; follows from Eq. (2.1.19) only after some manipulations and 
</p>
<p>is practically invisible in Eqs. (2.1.16) and (2.1.17). Both the conserved quantity and 
</p>
<p>its conservation law arise naturally in the Lagrangian scheme. 0 </p>
<p/>
</div>
<div class="page"><p/>
<p>Exercise 2.1.1. * Consider the following system, called a harmonic oscillator. The block 
has a mass m and lies on a frictionless surface. The spring has a force constant k. 
</p>
<p>m 
</p>
<p>Write the Lagrangian and get the equations of motion. 
</p>
<p>Exercise 2.1.2. * Do the same for the coupled-mass problem discussed at the end of 
Section 1.8. Compare the equations of motion with Eqs. (1.8.24) and (1.8.25). 
</p>
<p>Exercise 2.1.3."' A particle of mass m moves in three dimensions under a potential 
</p>
<p>V(r, (), 1/J) = V(r). Write its ft' and find the equations of motion. 
</p>
<p>2.2. The Electromagnetic Lagrangian+ 
</p>
<p>Recall that the force on a charge q due to an electric field E and magnetic field 
</p>
<p>B is given by 
</p>
<p>(2.2.1) 
</p>
<p>where v = t is the velocity of the particle. Since the force is velocity-dependent, we 
</p>
<p>must analyze the problem afresh, not relying on the preceding discussion, which was 
</p>
<p>restricted to velocity-i.ndependent forces. 
</p>
<p>Now it turns out that if we use 
</p>
<p>(2.2.2) 
</p>
<p>we get the correct electromagnetic force laws. In Eq. (2.2.2) cis the velocity of light, 
</p>
<p>while q, and A are the scalar and vector potentials related to E and B via 
</p>
<p>and 
</p>
<p>1 oA 
E=-V&cent;---
</p>
<p>c at 
</p>
<p>B=VxA 
</p>
<p>t See Section 18.4 for a review of classical electromagnetism. 
</p>
<p>(2.2.3) 
</p>
<p>(2.2.4) 
</p>
<p>83 
</p>
<p>REVIEW OF 
CLASSICAL 
</p>
<p>MECHANICS </p>
<p/>
</div>
<div class="page"><p/>
<p>84 
</p>
<p>CHAPTER 2 
</p>
<p>The Euler-Lagrange equations corresponding to :fe&middot;m are 
</p>
<p>i=l,2,3 
</p>
<p>Combining the three equations above into a single vector equation we get 
</p>
<p>d (' qA ') q - mv+- = -qV&cent;+ V(v&middot;A) 
dt . c ' c 
</p>
<p>The canonical momentum is 
</p>
<p>Rewriting Eq. (2.2.6), we get 
</p>
<p>qA 
p=mv+-
</p>
<p>c 
</p>
<p>!!_ (mv) = -qV &cent; + (_1 r&middot; _1~ + V(v &middot; A)J. 
dr c L dr 
</p>
<p>(2.2.5) 
</p>
<p>(2.2.6) 
</p>
<p>(2.2. 7) 
</p>
<p>(2.2.8) 
</p>
<p>Now, the total derivative dA/dt has two parts: an explicit time dependence cA/ct, 
plus an implicit one (v-V)A which represents the fact that a spatial variation in A 
</p>
<p>will appear as a temporal variation to the moving particle.Now Eq. (2.2.8) becomes 
</p>
<p>d q r'A q 
- (mv) = -qV&cent;-- -;:-+- [V(v&middot; A)- (v&middot;V)A] 
dt c (f c 
</p>
<p>(2.2.9) 
</p>
<p>which is identical to Eq. (2.2.1) by virtue of the identity 
</p>
<p>vx(VxA)=V(v-A)--(v&middot;V)A 
</p>
<p>Notice that :fe&middot;m is not of the form T-V, for the quantity U=q&cent;-(q/c)v-A 
</p>
<p>(sometimes called the generalized potential) cannot be interpreted as the potential 
</p>
<p>energy of the charged particle. First of all, the force due to a time-dependent electro-
</p>
<p>magnetic field is not generally conservative and does not admit a path-independent 
work function to play the role of a potential. Even in the special cases when the 
force is conservative, only q&cent; can be interpreted as the electrical potential energy. 
</p>
<p>The [-q(v&middot;A)/c] term is not a magnetic potential energy, since the magnetic force 
</p>
<p>F8 =q(v x B)/c never does any work, being always perpendicular to the velocity. To 
accommodate forces such as the electro-magnetic, we must, therefore, redefine .Y' to 
</p>
<p>be that function :f(q, q, t) which, when fed into the Euler Lagrange equations, 
reproduces the correct dynamics. The rule :f = T- V becomes just a useful mnemonic 
</p>
<p>for the case of conservative forces. </p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 2.4. The relation between r,, r2 and reM, r. 
</p>
<p>2.3. The Two-Body Problem 
</p>
<p>We discuss here a class of problems that plays a central role in classical physics: 
</p>
<p>that of two masses m1 and. m2 exerting equal and opposite forces on each other. 
</p>
<p>Since the particles are responding to each other and nothing external, it follows that 
</p>
<p>the potential between them depends only on the relative coordinate r = rt - r2 and 
</p>
<p>not the individual positions r 1 and r2 . But V(r1 , r2) = V(r1 - r2) means in tum that 
</p>
<p>there are three cyclic coordinates, for V depends on only three variables rather than 
</p>
<p>the possible six. (In Cartesian coordinates, since Tis a function only of velocities, a 
</p>
<p>coordinate missing in Vis also cyclic.) The corresponding conserved momenta will 
</p>
<p>of course be the three components of the total momentum, which are conserved in 
</p>
<p>the absence of external forces. To bring out these features, it is better to trade r 1 
and r2 in favor of 
</p>
<p>(2.3.1) 
</p>
<p>and 
</p>
<p>(2.3.2) 
</p>
<p>where reM is called the center-of-mass (CM) coordinate. One can invert Eqs. (2.3.1) 
</p>
<p>and (2.3.2) to get (see Fig. 2.4) 
</p>
<p>If one rewrites the Lagrangian 
</p>
<p>m2r 
rt=rcM+---
</p>
<p>mt+m2 
</p>
<p>mtr r2=rcM ___ _ 
m1+m2 
</p>
<p>!&pound;= ~mtlttl 2 + ~m2lt2l 2 - V(rt-r2) 
</p>
<p>in terms of reM and r, one gets 
</p>
<p>(2.3.3) 
</p>
<p>(2.3.4) 
</p>
<p>(2.3.5) 
</p>
<p>(2.3.6) 
</p>
<p>85 
</p>
<p>REVIEW OF 
</p>
<p>CLASSICAL 
</p>
<p>MECHANICS </p>
<p/>
</div>
<div class="page"><p/>
<p>86 
</p>
<p>CHAPTER 2 
</p>
<p>The main features of Eq. (2.3.6) are the following. 
</p>
<p>( l) The p.roblem of two mutually interacting particles has been transformed to 
that of two fictitious particles that do not interact with each other. In other words, 
the equations of motion for r do not involve reM and vice versa, because 2(r, t; 
reM, i"cM) = 2(r, t) + 2(rcM, i"cM ). 
</p>
<p>(2) The first fictitious particle is the CM, of mass M = m1 + m2. Since reM is a 
cyclic variable, the momentum PeM = MtcM (which is just the total momentum) is 
conserved as expected. Since the motion of the CM is uninteresting one usually 
ignores it. One clear way to do this is to go to the CM frame in which i"cM = 0, so 
that the CM is completely eliminated in the Lagrangian. 
</p>
<p>(3) The second fictitious particle has mass 11 =m 1m1/(m 1 +m1 ) (called the 
reduced mass), momentum p = pt and moves under a potential V(r). One has just to 
solve this one-body problem. If one chooses, one may easily return to the coordinates 
r1 and r2 at the end, using Eqs. (2.3.1) and (2.3.2). 
</p>
<p>Exercise 2.3.1. * Derive Eq. (2.3.6) from (2.3.5) by changing variables. 
</p>
<p>2.4. How Smart Is a Particle? 
</p>
<p>The Lagrangian formalism seems to ascribe to a particle a tremendous amount 
of foresight: a particle at (x;, t;) destined for (x1 , t1 ) manages to calculate ahead of 
time the action for every possible path linking these points, and takes the one with 
the least action. But this, of course, is an illusion. The particle need not know its 
entire trajectory ahead of time, it needs only to obey the Euler-Lagrange equations 
at each instant in time to minimize the action. This in turn means just following 
Newton's law, which is to say, the particle has to sample the potential in its immediate 
vicinity and accelerate in the direction of greatest change. 
</p>
<p>Our esteem for the particle will sink further when we learn quantum mechanics. 
We will discover that far from following any kind of strategy, the particle, in a sense, 
goes from (x;, t;) to (xr, tr) along all possible paths, giving equal weight to each! 
How it is that despite this, classical particles do seem to follow xc1 (t) is an interesting 
question that will be answered when we come to the path integral formalism of 
quantum mechanics. 
</p>
<p>2.5. The Hamiltonian Formalism 
</p>
<p>In the Lagrangian formalism, the independent variables are the coordinates q; 
and velocities q;. The momenta are derived quantities defined by 
</p>
<p>a2 
p;=--;,::-
</p>
<p>uq; 
(2.5.1) </p>
<p/>
</div>
<div class="page"><p/>
<p>In the Hamiltonian formalism one exchanges the roles of q and p: one replaces the 
Lagrangian .!l'(q, q)t by a Hamiltonian Jf(q,p) which generates the equations of 
</p>
<p>motion, and q becomes a derived quantity, 
</p>
<p>. o.ll' 
q;=-
</p>
<p>op; 
</p>
<p>thereby completing the role reversal of the q's and the p's. 
</p>
<p>(2.5.2) 
</p>
<p>There exists a standard procedure for effecting such a change, called a Legendre 
</p>
<p>transformation, which is illustrated by the following simple example. Suppose we 
</p>
<p>have a functionf(x) with 
</p>
<p>df 
u(x)=-
</p>
<p>dx 
(2.5.3) 
</p>
<p>Let it be possible to invert u(x) to get x(u). [For example if u(x) = x3, x(u) = u113, 
</p>
<p>etc.] If we define a function 
</p>
<p>g(u) =x(u)u-f(x(u)) (2.5.4) 
</p>
<p>then 
</p>
<p>dg dx df dx 
-=- &middot; u+x(u)-- &middot; -=x(u) 
du du dx du 
</p>
<p>(2.5.5) 
</p>
<p>That is to say, in going fromfto g (or vice versa) we exchange the roles of x and 
</p>
<p>u. One calls Eq. (2.5.4) a Legendre transformation andfand g Legendre transforms 
</p>
<p>of each other. 
More generally, iff= f(x 1 , x2 , &bull;&bull;&bull; , Xn), one can eliminate a subset {x;, i= I to 
</p>
<p>j} in favor of the partial derivatives U;= of/ OX; by the transformation 
</p>
<p>j 
</p>
<p>g(ui, ... , Uj, Xj+l, &bull;.. , Xn) = L U;X;-f(xi, ... , Xn) 
i=l 
</p>
<p>(2.5.6) 
</p>
<p>It is understood in the right-hand side of Eq. (2.5.6) that all the x;'s to be eliminated 
</p>
<p>have been rewritten as functions of the allowed variables in g. It can be easily verified 
</p>
<p>that 
</p>
<p>og 
-=xi 
OU; 
</p>
<p>(2.5.7) 
</p>
<p>where in taking the above partial derivative, one keeps all the other variables in g 
</p>
<p>constant. 
</p>
<p>t We will often refer to q, , ... , q" as q and p 1 , &bull;&bull;&bull; , Pn asp. 
</p>
<p>87 
</p>
<p>REVIEW OF 
CLASSICAL 
</p>
<p>MECHANICS </p>
<p/>
</div>
<div class="page"><p/>
<p>88 
</p>
<p>CHAPTER 2 
</p>
<p>Table 2.1. Comparison of the Lagrangian and Hamiltonian Formalisms 
</p>
<p>Lagrangian formalism 
</p>
<p>(I) The state of a system with n degrees of 
freedom is described by n coordinates 
(q1 &bull;.&bull;&bull; , q.,) and n velocities (q1 , &bull;&bull;&bull;&bull; q.,), or 
in a more compact notation by (q, q). 
</p>
<p>(2) The state of the system may be represented 
by a point moving with a definite velocity in 
an n-dirnensional configuration space. 
</p>
<p>(3) The n coordinates evolve according to n 
second-order equations. 
</p>
<p>(4) For a given !!', several trajectories may pass 
through a given point in configuration space 
depending on q. 
</p>
<p>Hamiltonian formalism 
</p>
<p>(I) The state of a system with n degrees of free-
dom is described by n coordinates and n 
momenta (q1 , .&bull;. , q.,; p 1 , .&bull;.&bull; p.,) or. more 
succinctly, by (q,p). 
</p>
<p>(2) The state of the system may be represented 
by a point in a 2n-dirnensional phase space, 
with coordinates (q,, ... , q.,; p,, ... , p., ). 
</p>
<p>(3) The 2n coordinates and momenta obey 2n 
first-order equations. 
</p>
<p>( 4) For a given .Yf only one trajectory passes 
through a given point in phase space. 
</p>
<p>Applying these methods to the problem in question, we define 
</p>
<p>n 
</p>
<p>.Yt(q,p)= I p/;;-!l'(q, 4&gt; (2.5.8) 
i=l 
</p>
<p>where the q's are to be written as functions of q's and p's. This inversion is generally 
easy since 2' is a polynomial of rank 2 in q, and p;= off' joq; is a polynomial of rank 
1 in the q's, e.g., Eq. (2.2.7). Consider now 
</p>
<p>o&pound; =_i_ (Ip,iJ,-2') 
op; op; , 
</p>
<p>=q; ( . off') smce Pi=-. 
oq, 
</p>
<p>(2.5.9) 
</p>
<p>(2.5.10) 
</p>
<p>[There are no (o!l'joq,)(oqjop;) terms since q is held constant in o.Ytjop;; that is, 
q and pare independent variables.] Similarly, 
</p>
<p>o&pound; =I Paq_1 _off'_ I off' oq, =_off' 
oq; j 1 oq; oq; j oq, oq; oq; 
</p>
<p>(2.5.11) 
</p>
<p>We now feed in the dynamics by replacing (off' joq;) by p;, and obtain Hamilton's 
canonical equations: 
</p>
<p>(}&pound;' . 
--;--=q;, 
Op; 
</p>
<p>o.Yt' . 
---=p; 
</p>
<p>oq; 
(2.5.12) 
</p>
<p>Note that we have altogether 2n first-order equations (in time) for a system with n 
degrees of freedom. Given the initial-value data, (q;(O),p;(O)), i= 1, .. , n, we can 
integrate the equations to get (q;(t),p;(t)). 
</p>
<p>Table 2.1 provides a comparison of the Lagrangian and Hamiltonian 
formalisms. </p>
<p/>
</div>
<div class="page"><p/>
<p>Now, just as!&pound;' may be interpreted as T- V if the force is conservative, so there 
</p>
<p>exists a simple interpretation for &pound;' in this case. Consider the sum l,,p,q,. Let us 
use Cartesian coordinates, in terms of which 
</p>
<p>n 
</p>
<p>T= L 
i=l 
</p>
<p>ay oT . 
p,=-. =-. =m,x, 
</p>
<p>OX; OX; 
</p>
<p>and 
</p>
<p>n n 
</p>
<p>I p,x,= I m,x~=2T (2.5.13) 
i=I i=I 
</p>
<p>so that 
</p>
<p>(2.5.14) 
</p>
<p>the total energy. Notice that although we used Cartesian coordinates along the 
</p>
<p>way, the resulting equation (2.5.14) is a relation among scalars and thus coordinate 
</p>
<p>independent. 
</p>
<p>Exercise 2.5.1. Show that if T= I:J::j Tif(q)q;i/j, where tj's are generalized velocities, 
l,,p,tj,=2T. 
</p>
<p>The Hamiltonian method is illustrated by the simple example of a harmonic 
</p>
<p>oscillator, for which 
</p>
<p>The canonical momentum is 
</p>
<p>oY . 
p= ox =mx 
</p>
<p>It is easy to invert this relation to obtain i as a function of p: 
</p>
<p>i=p/m 
</p>
<p>89 
</p>
<p>REVIEW OF 
</p>
<p>CLASSICAL 
</p>
<p>MECHANICS </p>
<p/>
</div>
<div class="page"><p/>
<p>90 
</p>
<p>CHAPTER 2 
</p>
<p>and obtain 
</p>
<p>/It (x, p) = T+ V= lm[.i(p)f+ ~kx 2 
</p>
<p>p +--~ kx:' 
2m 2 
</p>
<p>The equations of motion are 
</p>
<p>DcYl . p 
--=q-&gt; ..... = .\" 
i:p m 
</p>
<p>a.ff . . 
&middot;&middot;&middot;~&middot;&middot;&middot;&middot;&middot;&middot;&middot;=p-&gt; -kx=p 
cq 
</p>
<p>(2.5.15) 
</p>
<p>(2.5.16) 
</p>
<p>(2.5.17) 
</p>
<p>These equations can be integrated in time, given the initial q and p. If, however, we 
</p>
<p>want the familiar second-order equation, we differentiate Eq. (2.5.16) with respect 
</p>
<p>to time, and feed it into Eq. (2.5. J 7) to get 
</p>
<p>m.i;+kx=O 
</p>
<p>Exercise 2.5] Using the conservation of energy, show that the trajectories in phase 
</p>
<p>space for the oscillator are ell!pses of the form (x/a) 2 +(p/b) 2 = L where a2 =2E/k and b2 = 
</p>
<p>2m&pound;. 
</p>
<p>Exercise 2.5.3. Solve Exercise 2.1.2 using the Hamiltonian formalism. 
</p>
<p>Exercise 2.5.4. * Show that .fi corresponding lo !.!'in Eq. (2.3.6) is ./l = IPcMI'i2M +!pi' 
211 + V(r), where /i,;f is the total mass, p is the reduced mass, llcM and p are the momenta 
conjugate to reM and r. respectively. 
</p>
<p>2.6. The Electromagnetic Force in the Hamiltonian Scheme 
</p>
<p>The passage from .f[e&middot;m to its Legendre transform .Xt'em is not sensitive in any 
</p>
<p>way to the velocity-dependent nature of the force. If generated the correct force 
</p>
<p>laws, so will ,/{,.,, the dynamical content of the schemes being identical. In contrast, 
</p>
<p>the velocity independence of the force was assumed in showing that the numerical 
</p>
<p>value of ,Y{ is T + V, the total energy. Let us therefore repeat the analysis for the 
electromagnetic case. As 
</p>
<p>andt 
</p>
<p>!!',, = ~nn&middot;&middot;v-q&cent;+q v-A 
c 
</p>
<p>; Note that in this discussion. q is the charge and not the coordinate. fhe (Cartesian) coordinate r is 
hidden in the functions A(r, I) and &lt;j&gt;(r.l). </p>
<p/>
</div>
<div class="page"><p/>
<p>we have 
</p>
<p>qA 
p=mv+-
</p>
<p>c 
</p>
<p>.Yfe&middot;m = p&bull; V- !f'e&middot;m 
</p>
<p>v-A 1 qv&middot;A 
= mv &middot; v + q ---- mv &middot; v + qr/J---
</p>
<p>c 2 c 
</p>
<p>= ~mv&middot;v+q&cent;= T+q&cent; (2.6.1) 
</p>
<p>Now, there is something very disturbing about Eq. (2.6.1): the vector potential A 
</p>
<p>seems to have dropped out along the way. How is .Yfe&middot;m to generate the correct 
</p>
<p>dynamics without knowing what A is? The answer is, of course, the .Yf is more than 
</p>
<p>just T + q&cent;; it is T + qr/J written in terms of the correct variables, in particular, in 
terms of p and not v. Making the change of variables, we get 
</p>
<p>_ =l(p-qA/c)l 2 + A. 
.:n e&middot;m q'f' 
</p>
<p>2m 
</p>
<p>with the vector potential very much in the picture. 
</p>
<p>(2.6.2) 
</p>
<p>2.7. Cyclic Coordinates, Poisson Brackets, and Canonical Transformations 
</p>
<p>Cyclic coordinates are defined here just as in the Lagrangian case and have the 
</p>
<p>same significance: if a coordinate q; is missing in .Yf, then 
</p>
<p>. o.Yf 
p;=--=0 
</p>
<p>oq; 
(2.7.1) 
</p>
<p>Now, there will be other quantities, such as the energy, that may be conserved in 
</p>
<p>addition to the canonical momenta.&sect; There exists a nice method of characterizing 
</p>
<p>these in the Hamiltonian formalism. Let w(p, q) be some function of the state vari-
</p>
<p>ables, with no explicit dependence on t. Its time variation is given by 
</p>
<p>dw (ow ow ) -=I -q;+-p; 
dt i oq; op; 
</p>
<p>=I (ow o.Yf _ ~w o.Yf) 
i oq; op; op; oq; 
</p>
<p>= {w, .Yf} (2.7.2) 
</p>
<p>&sect;Another example is the conservation of l,=xp,.-ypx when V(x,y)= V(x2 +y2 ). There are no cyclic 
</p>
<p>coordinates here. Of course, if we work in polar coordinates, V(p, cjJ)= V(p), and p~=mp 2 tfo=i, is 
</p>
<p>conserved because it is the momentum conjugate to the cyclic coordinate cjJ. 
</p>
<p>91 
</p>
<p>REVIEW OF 
</p>
<p>CLASSICAL 
</p>
<p>MECHANICS </p>
<p/>
</div>
<div class="page"><p/>
<p>92 
</p>
<p>CHAPTER 2 
</p>
<p>where we have defined the Poisson bracket (PB) between two variables w(p, q) and 
A(p, q) to be 
</p>
<p>(2.7.3) 
</p>
<p>It follows from Eq. (2.7.2) that any variable whose PB with Yf vanishes is constant in 
time, i.e., conserved. In particular Yf itself is a constant of motion (identified as the 
total energy) if it has no explicit t dependen&middot;~e. 
</p>
<p>Exercise 2. 7.1. * Show that 
</p>
<p>{m,..l}=-{A.,m} 
</p>
<p>{m, ..l+cr} = {m, A.}+ {m, cr} 
</p>
<p>{m, A.cr} = {m, A.}cr+..l{m, cr} 
</p>
<p>Note the similarity between the above and Eqs. (1.5.10) and (1.5.11) for commutators. 
</p>
<p>Of fundamental importance are the PB between the q's and the p's. Observe 
that 
</p>
<p>{q;, qJ} = {p;,pj} =0 
</p>
<p>{qi&gt;pj}=oij 
</p>
<p>(2.7.4a) 
</p>
<p>(2.7.4b) 
</p>
<p>since (q;, ... ,pn) are independent variables (8q;/8qj=oij, 8q;j8pk=O, etc.). Hamil-
ton's equations may be written in terms of PB as 
</p>
<p>q;= {q;, Yf} 
</p>
<p>p;= {p;, Yf} 
</p>
<p>by setting w = q; or p; in Eq. (2. 7.2). 
</p>
<p>(2. 7.5a) 
</p>
<p>(2.7.5b) 
</p>
<p>Exercise 2. 7.2. * (i) Verify Eqs. (2.7.4) and (2.7.5). (ii) Consider a problem in two dimen-
sions given by Yf=p;+p;.+ax2 +b/. Argue that if a=b, {!., Yf} must vanish. Verify by 
explicit computation. 
</p>
<p>Canonical Transformations 
</p>
<p>We have seen that the Euler-Lagrange equations are form invariant under an 
arbitraryt change of coordinates in configuration space 
</p>
<p>i= I, ... , n (2. 7.6a) 
</p>
<p>t We assume the transformation is invertible, so we may write q in terms of ij: q= q(ij). The transformation 
may also depend on time explicitly [ij=q(q, t)], but we do not consider such cases. </p>
<p/>
</div>
<div class="page"><p/>
<p>or more succinctly 
</p>
<p>q-+ij(q) (2.7.6b) 
</p>
<p>The response of the velocities to this transformation follows from Eq. (2.7.6a): 
</p>
<p>(2.7.7) 
</p>
<p>The response of the canonical momenta may be found by rewriting !l' in terms of 
</p>
<p>(ij, q) and taking the derivative with respect to q: 
</p>
<p>- 8!l'(ij, q) 
p;= 
</p>
<p>aq; 
(2. 7.8) 
</p>
<p>The result is (Exercise 2.7.8): 
</p>
<p>(2.7.9) 
</p>
<p>Notice that although !l' enters Eq. (2.7.8), it drops out in Eq. (2.7.9), which connects 
</p>
<p>p to the old variables. This is as it should be, for we expect that the response of the 
</p>
<p>momenta to a coordinate transformation (say, a rotation) is a purely kinematical 
</p>
<p>question. 
</p>
<p>A word of explanation about !l'(ij, lj). By !l'(ij, lj) we mean the Lagrangian (say 
</p>
<p>T- V, for definiteness) written in terms of ij and ij. Thus the numerical value of the 
</p>
<p>Lagrangian is unchanged under (q, q)-+ (ij, lj); for (q, q) and (ij, lj) refer to the same 
</p>
<p>physical state. The functional form of the Lagrangian, however, does change and so 
</p>
<p>we should really be using two different symbols !l'(q, q) and .!i(ij, lj). Nonetheless 
</p>
<p>we follow the convention of denoting a given dynamical variable, such as the Lag-
</p>
<p>rangian, by a fixed symbol in all coordinate systems. 
</p>
<p>The invariance of the Euler-Lagrange equations under (q, q)-+ (ij, fj) implies 
the in variance of Hamilton's equation under (q, p)-+ (ij, p), i.e., (ij, p) obey 
</p>
<p>(2.7.10) 
</p>
<p>where Yf=Yf(ij,p) is the Hamiltonian written in terms of ij and ft. The proof is 
</p>
<p>simple: we start with !l'(ij, lj), perform a Legendre transform, and use the fact that 
</p>
<p>ij obeys Euler-Lagrange equations. 
</p>
<p>The transformation 
</p>
<p>(2.7.11) 
</p>
<p>93 
</p>
<p>REVIEW OF 
</p>
<p>CLASSICAL 
</p>
<p>MECHANICS </p>
<p/>
</div>
<div class="page"><p/>
<p>94 
CHAPTER 2 
</p>
<p>is called a point transformation. If we view the Hamiltonian formalism as something 
derived from the Lagrangian scheme, which is formulated in n-dimensional config-
uration space, this is the most general (time-independent) transformation which 
preserves the form of Hamilton's equations (that we can think of). On the other 
hand, if we view the Hamiltonian formalism in its own right, the backdrop is the 
2n-dimensional phase space. In this space, the point transformation is unnecessarily 
restrictive. One can contemplate a more general transformation of phase space 
coordinates: 
</p>
<p>q .... ij(q, p) 
</p>
<p>p--&gt;p(q,p) 
(2.7.12) 
</p>
<p>Although all sets of 2n independent coordinates (ij,p} are formally adequate for 
describing the state of the system, not all of them will preserve the canonical form 
of Hamilton's equations. (This is like saying that although Newton's laws may be 
written in terms of any complete set of coordinates, the simple form mij;= -8Vj8q; 
is valid only if the q; are Cartesian). If, however, (ij, p) obey the canonical equations 
(2.7.10), we say that they are canonical coordinates and that Eq. (2.7.12) defines a 
canonical transformation. Any set of coordinates ( q1, ... , qn), and the corresponding 
momenta generated in the Lagrangian formalism (p;= 82 joq;), are canonical coordi-
nates. Given one set, (q,p), we can get another, (ij,p), by the point transformation, 
which is a special case of the canonical transformation. This does not, however, 
exhaust the possibilities. Let us now ask the .following question. Given a new set of 
coordinates (ij(q, p), p(q, p)), how can we tell if they are canonical [assuming (q, p) 
are]? Now it is true for any w(q, p) that 
</p>
<p>w={ro &pound;}=I (aw 8&pound; _8ro 8&pound;) 
' i 8q; 8p; op; oq; 
</p>
<p>Applying this to ih( q, p) we find 
</p>
<p>q&middot;= I (oiL a&pound;_ oih a&pound;) 
1 
</p>
<p>i aq; ap; op; oq; 
</p>
<p>If we view &pound;' as a function of (ij, p) and use the chain rule, we get 
</p>
<p>8.Yt'(q,p) i)Jf(ij,p) I (a&pound; aqk +a&pound; oftk) 
8p; 8p; k aqk 8p; 8ftk 8p; 
</p>
<p>and 
</p>
<p>o&pound;(q,p) iJ.n"(ij,p) I ( 8&pound; aqk +a&pound; 8ftk) 
8q; oq; k oiik 8q; ofik 8q; 
</p>
<p>(2.7.13) 
</p>
<p>(2. 7.14) 
</p>
<p>(2.7.15a) 
</p>
<p>(2.7.15b) </p>
<p/>
</div>
<div class="page"><p/>
<p>Feeding all this into Eq. (2.7.14) we find, upon regrouping-terms, 
</p>
<p>(2.7.16) 
</p>
<p>It can similarly be established that 
</p>
<p>(2.7.17) 
</p>
<p>If Eqs. (2.7.16) and (2.7.17) are to reduce to the canonical equations (2.7.10) for 
</p>
<p>any .Yf(q,p), we must have 
</p>
<p>{ih, qk} =0= {pj,fik} 
</p>
<p>{qj,fik}=8jk 
(2.7.18) 
</p>
<p>These then are the conditions to be satisfied by the new variables if they are to be 
</p>
<p>canonical. Notice that these constraints make no reference to the specific functional 
</p>
<p>form of .Yf: the equations defining canonical variables are purely kinematical and 
</p>
<p>true for any .Yf(q,p). 
</p>
<p>Exercise 2.7.3. Fill in the missing steps leading to Eq. (2.7.18) starting from Eq. (2.7.14). 
</p>
<p>Exercise 2. 7.4. Verify that the change to a rotated frame 
</p>
<p>x = x cos ll- y sin ll 
</p>
<p>y=xsin ll+ycos ll 
</p>
<p>jjy = Px sin ll + py cos (} 
</p>
<p>is a canonical transformation. 
</p>
<p>Exercise 2. 7.5. Show that the polar variables p= (r+ /) 112, l/J= tan-1(y/x), 
</p>
<p>xpx+ypy 
</p>
<p>(xz+l)l/2' 
</p>
<p>are canonical. (ep is. the unit vector in the radial direction.) 
</p>
<p>95 
</p>
<p>REVIEW OF 
</p>
<p>CLASSICAL 
</p>
<p>MECHANICS </p>
<p/>
</div>
<div class="page"><p/>
<p>96 
</p>
<p>CHAPTER 2 
</p>
<p>Exercise 2. 7.6. * Verify that the change from the variables r,, r2, p,, p2 to reM, PcM, r, 
and p is a canonical transformation. (See Exercise 2.5.4). 
</p>
<p>Exercise 2.7.7. Verify that 
</p>
<p>q=ln(q 1 sinp) 
</p>
<p>p=qcotp 
</p>
<p>is a canonical transformation. 
</p>
<p>Exercise 2. 7.8. We would like to derive here Eq. (2.7.9), which gives the transformation 
of the momenta under a coordinate transformation in configuration space: 
</p>
<p>(I) Argue that if we invert the above equation to get q = q(q), we can derive the following 
counterpart ofEq. (2.7.7): 
</p>
<p>(2) Show from the above that 
</p>
<p>(3) Now calculate 
</p>
<p>-.= [ilff'(q, {J)J = [aSf'(q, 4JJ 
p, a&middot; a&middot; 
</p>
<p>q; 4 q; ij 
</p>
<p>Use the chain rule and the fact that q=q(q) and not q(q, q) to derive Eq. (2.7.9). 
(4) Verify, by calculating the PB in Eq. (2.7.18), that the point transformation 1s 
</p>
<p>canonical. 
</p>
<p>If ( q, p) and ( ij, p) are both canonical, we must give them both the same status, 
for Hamilton's equations have the same appearance when expressed in terms of 
either set. Now, we have defined the PB of two variables m and a in terms of (q, p) 
as 
</p>
<p>(om oa om oa) r {m, a} =I - ----;;--- = 'tm, a}q,p 
i oq, op, op, oq; 
</p>
<p>Should we not also define a PB, { m, a }q,ft for every canonical pair (ij, p)? Fortunately 
it turns out that the PB are invariant under canonical transformations: 
</p>
<p>(2.7.19) 
</p>
<p>(It is understood that m and a are written as functions of ij and p on the right-hand 
side.) </p>
<p/>
</div>
<div class="page"><p/>
<p>Exercise 2.7.9. Verify Eq. (2.7.19) by direct computation. Use the chain rule to go from 
q, p derivatives to ij, p derivatives. Collect terms that represent PB of the latter. 
</p>
<p>Besides the proof by direct computation (as per Exercise 2.7.9 above) there is 
an alternate way to establish Eq. (2.7.19). 
</p>
<p>Consider first u=:Yf. We know that since (q,p) obey canonical equations, 
</p>
<p>But then (ij, p) also obey canonical equations, so 
</p>
<p>dJ ={co :Yf}--' q,p 
</p>
<p>Now co is some physical quantity such as the kinetic energy or the component 
of angular momentum in some fixed direction, so its rate of change is independent 
of the phase space coordinates used, i.e., dJ is dJ, whether co=co(q,p) or co(ij,p). So 
</p>
<p>(2.7.20) 
</p>
<p>Having proved the result for what seems to be the special case u = :Yf, we now pull 
the following trick. Note that nowhere in the derivation did we have to assume that 
:Yf was any particular function of q and p. In fact, Hamiltonian dynamics, as a 
consistent mathematical scheme, places no restriction on :Yf.lt is the physical require-
ment that the time evolution generated by :Yf coincide with what is actually observed, 
that restricts :Yf to be T + V. Thus :Yf could have been any function at all in the 
preceding argument and in the result Eq. (2.7.20) (which is just a relation among 
partial derivatives.) If we understand that :Yf is not T+ V in this argument but an 
arbitrary function, call it u, we get the desired result. 
</p>
<p>Active Transformations 
</p>
<p>So far, we have viewed the transformation 
</p>
<p>ij=ij(q,p) 
</p>
<p>p=p(q,p) 
</p>
<p>as passive: both (q,p) and (ij,p) refer to the same point in phase space described 
in two different coordinate systems. Under the transformation (q,p) ~ (ij,p), the 
numerical values of all dynamical variables are unchanged (for we are talking about 
the same physical state), but their functional form is changed. For instance, 
under a change from Cartesian to spherical coordinates, co(x, y, z) = 
x 2 +l+z 2 ~co(r, (}, I/J)=r2&bull; As mentioned earlier, we use the same symbol for a 
given variable even if its functional dependence on the coordinates changes when we 
change coordinates. 
</p>
<p>Consider now a restricted class of transformations, called regular trans-
formations, which preserve the range of the variables: (q,p) and (ij,p) have the same 
range. A change from one Cartesian coordinate to a translated or rotated one is 
</p>
<p>97 
</p>
<p>REVIEW OF 
CLASSICAL 
</p>
<p>MECHANICS </p>
<p/>
</div>
<div class="page"><p/>
<p>98 
</p>
<p>CHAPTER 2 
</p>
<p>regular (each variable goes from -oo to +oo before and after), whereas a change to 
</p>
<p>spherical coordinates (where some coordinates are nonnegative, some are bounded 
</p>
<p>by 2rr, etc.) is not. 
</p>
<p>A regular transformation ( q, p) -&gt; ( ij, p) permits an alternate interpretation: 
</p>
<p>instead of viewing ( ij, p) as the same phase space point in a new coordinate system, 
</p>
<p>we may view it as a new point in the same coordinate system. This corresponds to 
</p>
<p>an active transformation which changes the state of the system. Under this change, 
</p>
<p>the numerical value of any dynamical variable m(q,p) will generally change: 
</p>
<p>m (q,p)=f:.m(ij,p), though its functional dependence will not: m(ij,p) is the same 
</p>
<p>function m(q,p) evaluated at the new point (q=ij,p=p). 
</p>
<p>We say that m is invariant under the regular transformation (q,p)-&gt; (ij,p) if 
</p>
<p>m(q, p) = m(ij, p) (2.7.21) 
</p>
<p>(This equation has content only if we are talking about the active transformations, 
</p>
<p>for it is true for any m under a passive transformation.) 
Whether we view the transformation (q, p)-&gt; (ij, p) as active or passive, it is 
</p>
<p>called canonical if (ij,p) obey Eq. (2.7.18). As we shall see, only regular canonical 
</p>
<p>transformations are physically interesting. 
</p>
<p>2.8. Symmetries and Their Consequences 
</p>
<p>Let us begin our discussion by examining what the word "symmetry" means in 
</p>
<p>daily usage. We say that a sphere is a very symmetric object because it looks the 
</p>
<p>same when seen from many directions. Or, equivalently, a sphere looks the same 
</p>
<p>before and after it is subjected to a rotation around any axis passing through its 
</p>
<p>center. A cylinder has symmetry too, but not as much: the rotation must be per-
</p>
<p>formed around its axis. Generally then, the symmetry of an object implies its in vari-
</p>
<p>ance under some transformations, which in our examples are rotations. 
</p>
<p>A symmetry can be discrete or continuous, as illustrated by the example of a 
</p>
<p>hexagon and a circle. While the rotation angles that leave a hexagon unchanged 
</p>
<p>form a discrete set, namely, multiples of 60&deg;, the corresponding set for a circle is a 
</p>
<p>continuum. We may characterize the continuous symmetry of the circle in another 
</p>
<p>way. Consider the identity transformation, which does nothing, i.e., rotates by oo in 
our example. This leaves both the circle and the hexagon invariant. Consider next 
</p>
<p>an infinitesimal transformation, which is infinitesimally "close" to the identity; in our 
</p>
<p>example this is a rotation by an infinitesimal angle c. The infinitesimal rotation 
leaves the circle invariant but not the hexagon. The circle is thus characterized by 
its invariance under infinitesimal rotations. Given this property, its invariance under 
</p>
<p>finite rotations follows, for any finite rotation may be viewed as a sequence of 
</p>
<p>infinitesimal rotations (each of which leaves it invariant). 
It is also possible to think of functions of some variables as being symmetric in 
</p>
<p>the sense that if one changes the values of the variables in a certain way, the value 
</p>
<p>of the function is invariant. Consider for example </p>
<p/>
</div>
<div class="page"><p/>
<p>If we make the following change 
</p>
<p>X --+ X= X COS 8 - y sin 8 
</p>
<p>y --+ ji = X sin 8 + y COS 8 
(2.8.1) 
</p>
<p>in the arguments, we find thatfis invariant. We say thatfis symmetric under the 
above transformation. In the terminology introduced earlier, the transformation in 
question is continuous: its infinitesimal version is 
</p>
<p>X --+ X= X COS E - y sin E = X- y E 
</p>
<p>y--+ ji=x sins+ y cos s=xs+ y (to orders) 
(2.8.2) 
</p>
<p>Consider now the function Jf(q, p). There are two important dynamical conse-
quences that follow from its invariance under regular canonical transformations. 
</p>
<p>I. If Jf is invariant under the following infinitesimal transformation (which you 
may verify is canonical, Exercise 2.8.2), 
</p>
<p>(2.8.3) 
- og 
</p>
<p>p;--+p;=p;- E -;;-=:p;+ Dp; 
uq; 
</p>
<p>where g(q, p) is any dynamical variable, then g is conserved, i.e., a constant of motion. 
One calls g the generator of the transformation. 
</p>
<p>II. If Jf is invariant under the regular, canonical, but not necessarily infinitesi-
mal, transformation (q,p)--+ (ij,p), and if (q(t),p(t)) is a solution to the equations 
of motion, so is the transformed (translated, rotated, etc.) trajectory, (ij(t),p(t)). 
</p>
<p>Let us now analyze these two consequences. 
Consequence I. Let us first verify that g is indeed conserved if Jf is invariant 
</p>
<p>under the transformation it generates. Working to first order in s, if we equate the 
change in Jf under the change of its arguments to zero, we get 
</p>
<p>a.Yt ( og) a.Yt ( og) 8Jt=2:- s- +- -s- =s{.Yf,g}=O 
i oq; op; op; oq; 
</p>
<p>(2.8.4) 
</p>
<p>But according to Eq. (2.7.2), 
</p>
<p>{g, .Yf} = 0--+ g is conserved (2.8.5) 
</p>
<p>(More generally, the response of any variable w to the transformation is 
</p>
<p>8w=s{w,g} (2.8.6) 
</p>
<p>99 
</p>
<p>REVIEW OF 
</p>
<p>CLASSICAL 
</p>
<p>MECHANICS </p>
<p/>
</div>
<div class="page"><p/>
<p>100 
</p>
<p>CHAPTER 2 
</p>
<p>Note that lip and 8q in Eq. (2.8.3) may also be written as PBs.) Consider as an 
</p>
<p>example, a particle in one dimension and the case g=p. From Eq. (2.8.3), 
</p>
<p>lix= sap= s 
ap 
</p>
<p>ap 
lip=-s-=0 
</p>
<p>ox 
</p>
<p>(2.8.7) 
</p>
<p>which we recognize to be an infinitesimal translation. Thus the linear momentum p 
</p>
<p>is the generator of spatial translations and is conserved in a translationally invariant 
</p>
<p>problem. The physics behind this result is clear. Since p is unchanged in a translation, 
</p>
<p>so is T= p2 /2m. Consequently V(x + s) = V(x). But if the potential doesn't vary from 
point to point, there is no force and p is conserved. 
</p>
<p>Next consider an example from two dimensions with g = l= = xpy-YPx. Here, 
</p>
<p>( at-) 8x= -ys =sap-~ 
</p>
<p>(2.8.8) 
</p>
<p>which we recognize to be an infinitesimal rotation around the z axis, [Eq. (2.8.2)]. 
</p>
<p>Thus the angular momentum around the z axis is the generator of rotations around 
</p>
<p>that axis, and is conserved if .Yf is invariant under rotations of the state around that 
</p>
<p>axis. The relation between the symmetry and the conservation law may be understood 
</p>
<p>in the following familiar terms. Under the rotation of the coordinates and the 
</p>
<p>momenta, IPI doesn't change and so neither does T=lpl 2/2m. Consequently, Vis a 
constant as we go along any circle centered at the origin. This in turn means that 
</p>
<p>there is no force in the tangential direction and so no torque around the z axis. The 
</p>
<p>conservation of l= then follows. 
</p>
<p>Exercise 2.8.1. Show that p =p 1 + p2 , the total momentum, is the generator ofinfintesimal 
translations for a two-particle system. 
</p>
<p>Exercise 2.8.2. * Verify that the infinitesimal transformation generated by any dynamical 
variable g is a canonical transformation. (Hint: Work, as usual, to first order in s.) 
</p>
<p>Exercise 2.8.3. Consider 
</p>
<p>p;+p~ I 2 2 2 
ff=--'+-mw (x +y) 
</p>
<p>2m 2 </p>
<p/>
</div>
<div class="page"><p/>
<p>whose in variance under the rotation of the coordinates and momenta leads to the conservation 
</p>
<p>of lz. But J'f is also invariant under the rotation ofjust the coordinates. Verify that this is a 
</p>
<p>noncanonical transformation. Convince yourself that in this case it is not possible to write 
8&pound;' as t:{ J'f, g} for any g, i.e., that no conservation law follows. 
</p>
<p>Exercise 2.8.4. * Consider J'f = 1p2 + , which is invariant under infinitesimal rotations 
in phase space (the xp plane). Find the generator of this transformation (after verifying that 
</p>
<p>it is canonical). (You could have guessed the answer based on Exercise 2.5.2.). 
</p>
<p>The preceding analysis yields, as a by-product, a way to generate infinitesimal 
</p>
<p>canonical transformations. We take any function g(q,p) and obtain the transforma-
</p>
<p>tion given by Eq. (2.8.6). (Recall that although we defined a canonical transformation 
</p>
<p>earlier, until now we had no means of generating one.) Given an infinitesimal canon-
</p>
<p>ical transformation, we can get a finite one by "integrating" it. The following 
</p>
<p>examples should convince you that this is possible. Consider the transformation 
</p>
<p>generated by g=.tt. We have 
</p>
<p>bq;= c.{q;, .#} 
</p>
<p>bp;= c.{p;, Jf} 
</p>
<p>But we know from the equations of motion that rj1= {q1 , ff} etc. So 
</p>
<p>8q;= &amp;rj; 
</p>
<p>bpi= qi; 
</p>
<p>(2.8.9) 
</p>
<p>(2.8.10) 
</p>
<p>Thus the new point in phase space ( ij, jj) = ( q + 8q, p + 8 p) obtained by this canonical 
transformation of (q, p) is just the point to which (q, p) would move in an infinitesi-
</p>
<p>mal time interval &amp;. In other words, the motion of points in phase space under the 
</p>
<p>time evolution generated by ff is an active canonical transformation. Now, you 
</p>
<p>know that by integrating the equations of motion, we can find (ij, p) at any future 
</p>
<p>time, i.e., get the finite canonical transformation. Consider now a general case of 
</p>
<p>gi=ff. We still have 
</p>
<p>8qt=t:{q,,g} 
(2.8.11) 
</p>
<p>8p,=t:{p;,g} 
</p>
<p>Mathematically, these equations are identical to Eq. (2.8.9), with g playing the role 
</p>
<p>of the Hamiltonian. Clearly there should be no problem integrating these equations 
</p>
<p>for the evolution of the phase space points under the "fake" Hamiltonian g, and 
fake "time" t:. Let us consider for instance the case g=/, which has units erg sec 
</p>
<p>and the corresponding fake time &amp; = 8 e, an angle. The transformation of the coordi-
nates is 
</p>
<p>8x= t:{x, '=}=-&amp;yo= ( -88)y 
</p>
<p>8y= (8G)x 
(2.8.12) 
</p>
<p>REVIEW OF 
</p>
<p>CLASSICAL 
</p>
<p>MECHANICS </p>
<p/>
</div>
<div class="page"><p/>
<p>102 
</p>
<p>CHAPTER 2 
</p>
<p>The fake equations of motion are 
</p>
<p>dx 
de= -y, 
</p>
<p>dy 
-=x 
d(} 
</p>
<p>(2.8.13) 
</p>
<p>Differentiating first with respect to (}, and using the second, we get 
</p>
<p>and likewise, 
</p>
<p>So 
</p>
<p>X = A cos (} + B sin e 
</p>
<p>y = C si11 e + D cos (} 
</p>
<p>We find the constants from the "initial" ( (} = 0) coordinates and "velocities": A= 
Xo, D= yo, B= (oxjiJe)o =-yo, C= (iJyjiJB)o= Xo. Reverting to the standard nota-
tion in which (x, y), rather than (x0 , y0), labels the initial point and (x, y), rather 
than (x, y), denotes the transformed one, we may write the finite canonical trans-
formation (a finite rotation) as 
</p>
<p>.\' = X COS (} - y sin (} 
</p>
<p>j/ = X sin (} + y COS (} 
</p>
<p>Similar equations may be derived for fix and jjy in terms of Px a:1d py. 
</p>
<p>(2.8.14) 
</p>
<p>Although a wide class of canonical transformations is now open to us, there 
are many that aren't. For instance, (q,p)-&gt;(-q, -p) is a discrete canonical trans-
formation that has no infinitesimal version. There are also the transformations that 
are not regular, such as the change from Cartesian to spherical coordinates, which 
have neither infintesimal forms, nor an active interpretation. We do not consider 
ways of generating these.t 
</p>
<p>Consequence II. Let us understand the content of this result through an example 
before turning to the proof. Consider a two-particle system whose Hamiltonian is 
invariant under the translation of the entire system, i.e., both particles. Let an 
observer SA prepare, at t=O, a state (x?, xLp? ,pg) which evolves as (x 1(t), x1(t); 
p 1 (t), p 2( t)) for some time and ends up in the state (x{, xf; p{, pf&gt; at time T. Let 
</p>
<p>t For an excellent and lucid treatment of this question and many other topics in advanced classical 
mechanics, see H. Goldstein. Classical Mechanics, Addison-Wesley, Reading, Massachusetts (1950); E. 
C. G. Sudharshan and N. Mukunda, Classical Dynamics: A Modern Perspective, Wiley, New York 
(1974). </p>
<p/>
</div>
<div class="page"><p/>
<p>us call the final state the outcome of the experiment conducted by SA. We are told 
</p>
<p>that as a result of the translational invariance of .!If, any other trajectory that is 
</p>
<p>related to this by an arbitrary translation a is also a solution to the equations of 
</p>
<p>motion. In this case, the initial state, for example, is (x? +a, xg +a; p?, pg ). The final 
state and all intermediate states are likewise displaced by the same amount. To an 
</p>
<p>observer Sa, displaced relative to SA by an amount a, the evolution of the second 
</p>
<p>system will appear to be identical to what SA saw in the first. Assuming for the sake 
</p>
<p>of this argument that Sa had in fact prepared the second system, we may say that 
</p>
<p>a given experiment and its translated version will give the same result (as seen by 
</p>
<p>the observers who conducted them) if .!If is translationally invariant. 
</p>
<p>The physical idea is the following. For the usual reasons, translational invariance 
</p>
<p>of .!If implies the invariance of V(x1 , x2). This in turn means that V(x1 , x2) = 
</p>
<p>V(x1 - x2). Thus each particle cares only about where the other is relative to it, and 
</p>
<p>not about where the system as a whole is in space. Consequently the outcome of the 
</p>
<p>experiment is not affected by an overall translation. 
</p>
<p>Consequence II is just a generalization of this result to other canonical trans-
</p>
<p>formations that leave .!If invariant. For instance, if .!If is rotationally invariant, a 
</p>
<p>given experiment and its rotated version will give the same result (according to the 
</p>
<p>observers who conducted them). 
Let us now turn to the proof of the general result. 
</p>
<p>Proof Imagine a trajectory (q(t), p(t)) in phase space that satisfies the equations 
</p>
<p>of motion. Let us associate with it an image trajectory, (ij(t), p(t)), which is obtained 
</p>
<p>by transforming each point (q,p) to the image point (ij,p) by means of a regular 
</p>
<p>canonical transformation. We ask if the image point moves according to Hamilton's 
</p>
<p>equation of motion, i.e., if 
</p>
<p>8.Jif(ij, p) 
ih (2.8.15) 
</p>
<p>if .!If is invariant under the transformation (q, p)-+ (ij, p). Now ijj(q, p), like any 
</p>
<p>dynamical variable w(q,p), obeys 
</p>
<p>(2.8.16) 
</p>
<p>If (q,p)-+ (ij,p) were a passive canonical transformation, we could write, since the 
</p>
<p>PB are invariant under such a transformation, 
</p>
<p>But it is an active transformation. However, because of the symmetry of .!If, i.e., 
.Jif(q,p)=.Jif(ij,p), we can go through the very same steps that led to Eq. (2.7.16) 
</p>
<p>from Eq. (2.7.14) and prove the result. If you do not believe this, you may verify it 
</p>
<p>103 
</p>
<p>REVIEW OF 
</p>
<p>CLASSICAL 
MECHANICS </p>
<p/>
</div>
<div class="page"><p/>
<p>104 
</p>
<p>CHAPTER 2 
</p>
<p>by explicit computation using Yl'(q,p)=Yl'(ij,p). A similar argument shows that 
</p>
<p>(2.8.17) 
</p>
<p>So the image point moves according to Hamilton's equations. Q.E.D. 
</p>
<p>Exercise 2.8.5. Why is it that a noncanonical transformation that leaves .Yf invariant 
does not map a solution into another? Or, in view of the discussions on consequence II, why 
is it that an experiment and its transformed version do not give the same result when the 
transformation that leaves .Yf invariant is not canonical? It is best to consider an example. 
Consider the potential given in Exercise 2.8.3. Suppose I release a particle at (x =a, y = 0) 
with (px=b,py=O) and you release one in the transformed state in which (x=O, y=a) and 
(Px = b, py = 0), i.e., you rotate the coordinates but not the momenta. This is a noncanonical 
transformation that leaves .Yf invariant. Convince yourself that at later times the states of the 
two particles are not related by the same transformation. Try to understand what goes wrong 
in the general case. 
</p>
<p>As you go on and learn quantum mechanics, you will see that the symmetries 
of the Hamiltonian have similar consequences for the dynamics of the system. 
</p>
<p>A Useful Relation Between SandE 
</p>
<p>We now prove a result that will be invoked in Chapter 16: 
</p>
<p>asci (x1 , tf; x;, t;) 
atr 
</p>
<p>where Sc1 (Xj, lf; X;, l;) is the action of the classical path from X;, l; to Xf, lt and&pound;' 
is the Hamiltonian at the upper end point. Since we shall be working with problems 
where energy is conserved we may write 
</p>
<p>asci (xi, t1 ; x;, t;) 
at, 
</p>
<p>-E 
</p>
<p>where E is the conserved energy, constant on the whole trajectory. 
At first sight you may think that since 
</p>
<p>fit sc1 = 2? dt 
t, 
</p>
<p>(2.8.18) </p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 2.5. The upper trajectory takes time t while the lower 
</p>
<p>takes t+ !J.t. 
</p>
<p>the right side must equal ff and not -E. The explanation requires Fig. 2.5 wherein 
</p>
<p>we have set x;=t;=O for convenience. 
</p>
<p>The derivative we are computing is governed by the change in action of the 
</p>
<p>classical path due to a change in travel by At holding the end points X; and x1 fixed. 
</p>
<p>From the figure it is clear that now the particle takes a different classical trajectory 
</p>
<p>x(t)=xc~(t)+1](t) with 7](0)=0. 
</p>
<p>so that the total change in action comes from the difference in paths between t = 0 
</p>
<p>and t = t1 as well as the entire action due to the extra travel between t1 and t1+ !J.t1 . 
</p>
<p>Only the latter is given ff At. The correct answer is then 
</p>
<p>f '~ [off off J 8Sc1= -71(t)+-. lj(t) dt+ff(t1 )At 
0 ax ax 
</p>
<p>f'f ( d off off) f'~ d [off J = ---+-- 1](t)dt+ - -. 1](t) dt+ff(tj)At dt ox ax 0 dt ax 
0 Xc1 
</p>
<p>off I =0+-. 1](t) +ff(tJ) At. 
ax 'I 
</p>
<p>It is clear from the figure that 1](t1 ) = -x(t1 ) At so that 
</p>
<p>os=[-~~ x+ff] At=-Yf(tl)At 
'J 
</p>
<p>from which the result follows. 
</p>
<p>Exercise 2.8.6. Show that oSc~/ox 1 = p(t1 ). 
</p>
<p>Exercise 2.8. 7. Consider the harmonic oscillator, for which the general solution is 
</p>
<p>x(t) =A cos wt+ B sin wt. 
</p>
<p>105 
</p>
<p>REVIEW OF 
</p>
<p>CLASSICAL 
</p>
<p>MECHANICS </p>
<p/>
</div>
<div class="page"><p/>
<p>106 
</p>
<p>CHAPTER 2 
</p>
<p>Express the energy in terms of A and Band note that it does not depend on time. Now choose 
A and B such that x(O) = x1 and x( T) = Xz. Write down the energy in terms of x1, Xz, and T. 
Show that the action for the trajectory connecting x 1 and x2 is 
</p>
<p>Verify that 8Sc~/8T= -E. </p>
<p/>
</div>
<div class="page"><p/>
<p>All Is Not Well with 
Classical Mechanics 
</p>
<p>3 
</p>
<p>It was mentioned in the Prelude that as we keep expanding our domain of observa-
</p>
<p>tions we must constantly check to see if the existing laws of physics continue to 
</p>
<p>explain the new phenomena, and that, if they do not, we must try to find new laws 
</p>
<p>that do. In this chapter you will get acquainted with experiments that betray the 
</p>
<p>inadequacy of the classical scheme. The experiments to be described were never 
</p>
<p>performed exactly as described here, but they contain the essential features of the 
</p>
<p>actual experiments that were performed (in the first quarter of this century) with 
</p>
<p>none of their inessential complications. 
</p>
<p>3.1. Particles and Waves in Classical Physics 
</p>
<p>There exist in classical physics two distinct entities: particles and waves. We 
</p>
<p>have studied the particles in some detail in the last chapter and may summarize their 
</p>
<p>essential features as follows. Particles are localized bundles of energy and momentum. 
</p>
<p>They are described at any instant by the state parameters q and q (or q and p ). These 
parameters evolve in time according to some equations of motion. Given the initial 
</p>
<p>values q(ti) and q(td at time ti, the trajectory q(t) may be deduced for all future 
times from the equations of motion. A wave, in contrast. is a disturbance spread over 
</p>
<p>space. Jt is described by a wave function tp(r, t) which characterizes the disturbance at 
the point r at time t. 
</p>
<p>In the case of sound waves, tp is the excess air pressure above the normal, while 
in the case of electromagnetic waves, tp can be any component of the electric field 
</p>
<p>vector E. The analogs of q and q for a wave are tp and 1jr at each point r, assuming 
tp obeys a second-order wave equation in time, such as 
</p>
<p>107 </p>
<p/>
</div>
<div class="page"><p/>
<p>108 
</p>
<p>CHAPTER 3 
</p>
<p>(a) (b) 
</p>
<p>Figure 3.1. (a) When a wave w=e''c.-wn 
</p>
<p>is incident on the screen with either slit S1 
or S, open, the intensity patterns ! 1 and / 2 &bull; 
</p>
<p>respectively, are measured by the row of 
</p>
<p>detectors on A B. (b) With both slits open, 
</p>
<p>the pattern I, , 2 is observed. Note that 
</p>
<p>/1 ~ 2 /o/1 +I,. This is called interference. 
</p>
<p>which describes waves propagating at the speed of light, c. Given 1/'(r, 0) and lfr(r, 0) 
one can get the wave function 1/'(r, t) for all future times by solving the wave 
equation. 
</p>
<p>Of special interest to us are waves that are periodic in space and time, called 
</p>
<p>plane waves. In one dimension, the plane wave may be written as 
</p>
<p>ljl(x,t)=Aexpl&lt; 2; x- 2; r)}=Aexp[i&lt;;b] (3.1.1) 
</p>
<p>At some given time t, the wave is periodic in space with a period A, called its 
</p>
<p>wavelength, and likewise at a given point x, it is periodic in time, repeating itself 
</p>
<p>every T seconds, T being called the time period. We will often use, instead of A and 
</p>
<p>T, the related quantities k = 2n: I A. called the wave number and w = 2n: IT called the 
(angular) frequency. In terms of the phase .P in Eq. (3.1.1 ), k measures the phase 
change per unit length at any fixed time t, while w measures the phase change per 
unit time at any fixed point x. This wave travels at a speed v = w lk. To check this 
claim, note that if we start out at a point where .P = 0 and move along x at a rate 
x = ( m /k)t, .P remains zero. The overall scale A up front is called the amplitude. For 
any wave, the intensity is defined to be I= 11!'1 2 . For a plane wave this is a constant 
equal to IAI 2&bull; If II' describes an electromagnetic wave, the intensity is a measure of 
the energy and momentum carried by the wave. [Since the electromagnetic field is 
</p>
<p>real, only the real part of II' describes it. However, time averages of the energy and 
</p>
<p>momentum flow are still proportional to the intensity (as defined above) in the case 
</p>
<p>of plane waves.] 
</p>
<p>Plane waves in three dimension are written as 
</p>
<p>w=lk!v (3. 1.2) 
</p>
<p>where each component k 1 gives the phase changes per unit length along the ith axis. 
One calls k the H.:ave vector.t 
</p>
<p>3.2. An Experiment with Waves and Particles (Classical) 
</p>
<p>\Vaves exhibit a phenomenon called interference, which is peculiar to them and 
is not exhibited by particles described by classical mechanics. This phenomenon is 
illustrated by the following experiment (Fig. 3.la). Let a wave II'= A e1(kr wtl be 
</p>
<p>t Unfortunately we also us&lt;" k to denote the unit wctor along the ~ axis. It should be clear from the 
context what it stands for. </p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 3.2. (a) Intensity pattern 
when S1 or S2 is open, due to a 
beam of incident particles. (b) The 
pattern with both slits open accord-
ing to classical mechanics (/, + 2 = 
I,+ lz). 
</p>
<p>(o) 
</p>
<p>8 
</p>
<p>(b) A 
</p>
<p>incident normally on a screen with slits S1 and S2 , which are a distance a apart. At 
a distance d parallel to it is a row of detectors that measures the intensity as a 
function of the position x measured along AB. 
</p>
<p>If we first keep only S1 open, the incident wave will come out of S1 and propagate 
radially outward. One may think of S1 as the virtual source of this wave lflt, which 
has the same frequency and wavelength as the incident wave. The intensity pattern 
11 =I lf/112 is registered by the detectors. Similarly if S2 is open instead of S1 , the wave 
lfl 2 produces the pattern 12 = I lfl 212&bull; In both cases the arrival of energy at the detectors 
is a smooth function of x and t. 
</p>
<p>Now if both St and S2 are opened, both waves lf/t and lf/2 are present and 
produce an intensity pattern It +2 =I lf/t + lf/2l 2. 
</p>
<p>The interesting thing is that 11 + 2 =f. 11 + 12 , but rather the interference pattern 
shown in Fig. 3.1 b. The ups and downs are due to the fact that the waves lfl 1 and 
lf/2 have to travel different distances d1 and d2 to arrive at some given x (see Fig. 
3.la) and thus are not always in step. In particular, the maxima correspond to the 
case d2 - d1 = nA. (n is an integer), when the waves arrive exactly in step, and the 
minima correspond to the case d2 - d1 = (2n + I )A./2, when the waves are exactly out 
of step. In terms of the phases l/&gt; 1 and l/&gt;2 , l/&gt;2(x)- l/&gt;1(x) =2mr at a maximum and 
l/&gt;2(x)-l/&gt;1(x)=(2n+ l)1r at a minimum. One can easily show that the spacing Ax 
between two adjacent maxima is Ax=Mja. 
</p>
<p>The feature to take special note of is that if Xmin is an interference minimum, 
there is more energy flowing into Xmin with just one slit open than with both. In 
other words, the opening of an extra slit can actually reduce the energy flow into 
Xmin&middot; 
</p>
<p>Consider next the experiment with particles (Fig. 3.2a). The source of the inci-
dent plane waves is replaced by a source of particles that shoots them toward the 
screen with varying directions but fixed energy. Let the line AB be filled with an 
array of particle detectors. Let us define the intensity l(x) to be the number of 
particles arriving per second at any given x. The patterns with S1 or S2 open are 
shown in (Fig. 3.2a). These look very much like the corresponding patterns for the 
wave. The only difference will be that the particles arrive not continuously, but in a 
staccato fashion, each particle triggering a counter at some single point x at the time 
of arrival. Although this fact may be obscured if the beam is dense, it can be easily 
detected as the incident flux is reduced. 
</p>
<p>What if both St and S2 are opened? Classical mechanics has an unambiguous 
prediction: It+ 2 = 11 + 12 &bull; The reasoning is as follows: each particle travels along a 
definite trajectory that passes via St or s2 to the destination X. To a particle headed 
</p>
<p>109 
</p>
<p>ALL IS NOT WELL 
WITH CLASSICAL 
</p>
<p>MECHANICS </p>
<p/>
</div>
<div class="page"><p/>
<p>110 
</p>
<p>CHAPTER 3 
</p>
<p>for S,, it is immaterial whether S2 is open or closed. Being localized in space it has 
</p>
<p>no way of even knowing if S2 is open or closed, and thus cannot respond to it in 
</p>
<p>any way. Thus the number coming viaS, to xis independent of whether S2 is open 
</p>
<p>or not and vice versa. It follows that I 1 + 2 = I 1 + I 2 (Fig. 3.2b). 
The following objection may be raised: although particles heading for S 1 are 
</p>
<p>not aware that S2 is open, they certainly can be deflected by those coming out of 
</p>
<p>S2, if, for instance, the former are heading for x 1 and the latter for x2 (see Fig. 3.la). 
</p>
<p>This objection can be silenced by sending in one particle at a time. A given 
</p>
<p>particle will of course not produce a pattern like I 1 or I 2 by itself, it will go to some 
</p>
<p>point x. If, however, we make a histogram, the envelope of this histogram, after 
</p>
<p>many counts, will define the smooth functions I 1 , I 2 , and I 1 + 2 &bull; Now the conclusion 
</p>
<p>I, + 2 = I1 + I2 is inevitable. 
This is what classical physics predicts particles and waves will do in the double-
</p>
<p>slit experiment. 
</p>
<p>3.3. The Double-Slit Experiment with Light 
</p>
<p>Consider now what happens when we perform the following experiment to check 
</p>
<p>the classical physics notion that light is an electromagnetic wave phenomenon. 
</p>
<p>We set up the double slit as in Fig. 3.la, with a row of light-sensitive meters 
</p>
<p>along AB and send a beam IJI =A ei&lt;ky-wtl in a direction perpendicular to the screen. 
</p>
<p>(Strictly speaking, the electromagnetic wave must be characterized by giving the 
</p>
<p>orientation of the E and B vectors in addition to w and k. However, for a plane 
</p>
<p>wave, B is uniquely fixed by E. If we further assume E is polarized perpendicular to 
</p>
<p>the page, this polarization is unaffected by the double slit. We can therefore suppress 
</p>
<p>the explicit reference to thir C011stant vector and represent the field as a scalar function 
</p>
<p>IJI.) We find that with the slits open one at a time we get patterns I 1 and I 2 , and 
</p>
<p>with both slits open we get the interference pattern I 1 + 2 as in Figs. 3.la and 3.1 b. 
</p>
<p>(The interference pattern is of course what convinced classical physicists that light 
</p>
<p>was a wave phenomenon.) The energy arrives at the dete~tors smoothly and continu-
</p>
<p>ously as befitting a wave. 
</p>
<p>Say we repeat the experiment with a change that is expected (in classical physics) 
</p>
<p>to produce no qualitative effects. We start with S 1 open and cut down the intensity. 
</p>
<p>A very strange thing happens. We find that the energy is not arriving continuously, 
</p>
<p>but in sudden bursts, a burst here, a burst there, etc. We now cut down the intensity 
</p>
<p>further so that only one detector gets activated at a given time and there is enough 
</p>
<p>of a gap, say a millisecond, between counts. As each burst occurs at some x, we 
</p>
<p>record it and plot a histogram. With enough data, the envelope of the histogram 
</p>
<p>becomes, of course, the pattern I 1 &bull; We have made an important discovery: light 
</p>
<p>energy is not continuous-it comes in bundles. This discrete nature is obscured in 
</p>
<p>intense beams, for the bundles come in so fast and all over the line AB, that the 
</p>
<p>energy flow seems continuous in space and time. 
</p>
<p>We pursue our study of these bundles, called photons, in some detail and find 
</p>
<p>the following properties: 
</p>
<p>1. Each bundle carries the same energy E. 
</p>
<p>2. Each bundle carries the same momentum p. </p>
<p/>
</div>
<div class="page"><p/>
<p>3. E=pc. From the famous equation E 2 =lc2 +m2c4 , we deduce that these bundles 
</p>
<p>are particles of zero mass. 
</p>
<p>4. If we vary the frequency of the light source we discover that 
</p>
<p>E=fiw (3.3.1) 
</p>
<p>p= fik (3.3.2) 
</p>
<p>where fi = hj2n: is a constant. The constant h is called Planck's constant, and has the 
dimensions of erg sec, which is the same as that of action and angular momentum. 
</p>
<p>Its value is 
</p>
<p>h &middot;1:. J() &middot;27 
~= .,, ~ erg sec 
2n: 
</p>
<p>(3.3.3) 
</p>
<p>For those interested in history, the actual experiment that revealed the granular 
</p>
<p>nature of light is called the photoelectric effect. The correct explanation of this experi-
</p>
<p>ment, in terms of photons, was given by Einstein in 1905. 
</p>
<p>That light is made of particles will, of course, surprise classical physicists but 
</p>
<p>will not imply the end of classical physics, for physicists are used to the idea that 
</p>
<p>phenomena that seem continuous at first sight may in reality be discrete. They will 
</p>
<p>cheerfully plunge into the study of the dynamics of the photons, trying to find the 
</p>
<p>equations of motion for its trajectory and so on. What really undermines classical 
</p>
<p>physics is the fact that if we now open both slits, still keeping the intensity so low 
</p>
<p>that only one photon is in the experimental region at a given time, and watch the 
</p>
<p>histogram take shape, we won't find that ! 1 + 2 equals / 1 + ! 2 as would be expected of 
particles, but is instead an interference pattern characteristic of wave number k. 
This result completely rules out the possibility that photons move in well-defined 
</p>
<p>trajectories like the particles of classical mechanics-for if this were true, a photon 
</p>
<p>going in via S 1 should be insensitive to whether S2 is open or not (and vice versa), 
</p>
<p>and the result ft + 2 = / 1 + ! 2 is inescapable! To say this another way, consider a point 
Xmin which is an interference minimum. More photons arrive here with either S 1 or 
</p>
<p>S2 open than with both open. If photons followed definite trajectories, it is incompre-
</p>
<p>hensible how opening an extra pathway can reduce the number coming to Xrnin. Since 
</p>
<p>we are doing the experiment with one photon at a time, one cannot even raise the 
</p>
<p>improbable hypothesis that photons coming out of S 1 collide with those coming out 
</p>
<p>of S2 to modify (miraculously) the smooth pattern / 1 + /2 into the wiggly interference 
pattern. 
</p>
<p>From these facts Born drew the following conclusion: with each photon is 
</p>
<p>associated a wave IJI, called the probability amplitude or simply amplitude, whose 
</p>
<p>modulus squared IIJI(x)l 2 gives the probability of finding the particle at x. [Strictly 
speaking, we must not refer to IIJI(x)l 2 as the probability for a given x, but rather 
</p>
<p>as the probability density at x since x is a continuous variable. These subtleties can, 
</p>
<p>however, wait.] The entire experiment may be understood in terms of this hypothesis 
as follows. Every incoming photon of energy E and momentum p has a wave function 
</p>
<p>lJI associated with it, which is a plane wave with (0 = E/fi and k = pjfi. This wave 
</p>
<p>interferes with itself and forms the oscillating pattern I1J!(x)l 2 along AB, which gives 
</p>
<p>111 
</p>
<p>ALL IS NOT WELL 
</p>
<p>WITH CLASSICAL 
</p>
<p>MECHANICS </p>
<p/>
</div>
<div class="page"><p/>
<p>112 
</p>
<p>CHAPTER 3 
</p>
<p>the probability that the given photon will arive at x. A given photon of course arrives 
at some definite x and does not reveal the probability distribution. If, however, we 
wait till several photons, all described by the same If!, have arrived, the number at 
</p>
<p>any x will become proportional to the probability function 11f!(x)l 2 Likewise, if an 
intense (macroscopic) monochromatic beam is incident, many photons, all described 
</p>
<p>by the same wave and hence the same probability distribution, arrive at the same 
</p>
<p>time and all along the line AB. The intensity distribution then assumes the shape of 
</p>
<p>the probability distribution right away and the energy flow seems continuous and in 
</p>
<p>agreement with the predictions of classical electromagnetic theory. 
</p>
<p>The main point to note, besides the probability interpretation, is that a wave 
</p>
<p>is associated not with a beam of photons, but with each photon. If the beam is 
</p>
<p>monochromatic, every photon is given by the same If! and the same probability 
distribution. A large ensemble of such photons will reproduce the phenomena 
</p>
<p>expected of a classical electromagnetic wave If! and the probabilistic aspect will be 
</p>
<p>hidden. 
</p>
<p>3.4. Matter Waves (de Broglie Waves) 
</p>
<p>That light, which one thought was a pure wave phenomenon, should consist of 
</p>
<p>photons, prompted de Broglie to conjecture that entities like the electron, generally 
</p>
<p>believed to be particles, should exhibit wavelike behavior. More specifically, he con-
</p>
<p>jectured, in analogy with photons, that particles of momentum p will produce an 
interference pattern corresponding to a wave number k = p/h in the double-slit experi-
</p>
<p>ment. This prediction was verified for electrons by Davisson and Germer, shortly 
</p>
<p>thereafter. It is now widely accepted that all particles are described by probability 
</p>
<p>amplitudes lJI(x), and that the assumption that they move in defi.nite trajectories is 
</p>
<p>ruled out by experiment. 
</p>
<p>But what about common sense, which says that billiard balls and baseballs 
</p>
<p>travel along definite trajectories? How did classical mechanics survive for three cen-
</p>
<p>turies? The answer is that the wave nature of matter is not apparent for macroscopic 
</p>
<p>phenomena since h is so small. The precise meaning of this explanation will become 
</p>
<p>clear only after we fully master quantum mechanics. Nonetheless, the following 
</p>
<p>example should be instructive. Suppose we do the double-slit experiment with pellets 
</p>
<p>of mass I g, moving at 1 em/sec. The wavelength associated with these particles is 
</p>
<p>, 2rr h _26 
~o=&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;= &middot;::ociO em 
</p>
<p>k p 
</p>
<p>which is 10- 13 times smaller than the radius of the proton! For any reasonable values 
of the parameters a and d (see Fig. 3.1 b), the interference pattern would be so dense 
in x that our instruments will only measure the smooth average, which will obey 
</p>
<p>/ 1 + 2 = /1 + h as predicted classically. 
</p>
<p>3.5. Conclusions 
</p>
<p>The main objective of this chapter was to expose the inadequacy of classical 
physics in explaining certain phenomena and, incidentally, to get a glimpse of what </p>
<p/>
</div>
<div class="page"><p/>
<p>the new (quantum) physics ought to look like. We found that entities such as the 
</p>
<p>electron are particles in the classical sense in that when detected they seem to carry 
</p>
<p>all their energy, momentum, charge, etc. in localized form; and at the same time 
they are not particlelike in that assuming they move along definite trajectories leads 
to conflict with experiment. It appears that each particle has associated with it a 
wave function '!f(X, t), such that I '!f(X, t)l 2 gives the probability of finding it at a 
point x at time t. This is called wave-particle duality. 
</p>
<p>The dynamics of the particle is then the dynamics of this function '!f(X, t) or, if 
we think of functions as vectors in an infinite-dimensional space, of the ket I lfl{t) ). 
In the next chapter the postulates of quantum theory win define the dynamics in 
terms of I '!'(t)). The postulates, which specify what sort of information is contained 
</p>
<p>in I '!'( t)) and how I'!'( t}) evolves with time, summarize the results of the double-
slit experiment and many others not mentioned here. The double-slit experiment was 
described here to expose the inadequacy of classical physics and not to summarize 
the entire body of experimental results from which all the postulates could be inferred. 
Fortunately, the double-slit experiment contains most of the central features of the 
theory, so that when the postulates are encountered in the next chapter, they will 
appear highly plausible. 
</p>
<p>113 
</p>
<p>ALL IS NOT WELL 
WITH CLASSICAL 
</p>
<p>MECHANICS </p>
<p/>
</div>
<div class="page"><p/>
<p>4 
</p>
<p>The Postulates-a 
General Discussion 
</p>
<p>Having acquired the necessary mathematical training and physical motivation, you 
</p>
<p>are now ready to get acquainted with the postulates of quantum mechanics. In this 
</p>
<p>chapter the postulates will be stated and discussed in broad tem1s to bring out 
</p>
<p>the essential features of quantum theory. The subsequent chapters will simply be 
</p>
<p>applications of these postulates to the solution of a variety of physically interesting 
</p>
<p>problems. Despite your preparation you may still find the postulates somewhat 
</p>
<p>abstract and mystifying on this first encounter. These feelings will, however, dis-
</p>
<p>appear after you have worked with the subject for some time. 
</p>
<p>4.1. The Postulatest 
</p>
<p>The following are the postulates of nonrelativistic quantum mechanics. We 
consider first a system with one degree of freedom, namely, a single particle in one 
</p>
<p>space dimension. The straightforward generalization to more particles and higher 
</p>
<p>dimensions will be discussed towards the end of the chapter. In what follows, the 
</p>
<p>quantum postulates are accompanied by their classical counterparts (in the Hamil-
tonian formalism) to provide some perspective. 
</p>
<p>Classical Mechanics 
</p>
<p>I. The state of a particle at any given 
</p>
<p>time is specified by the two variables 
x(t) and p(t), i.e., as a point in a two-
dimensional phase space. 
</p>
<p>II. Every dynamical variable OJ is a 
function of x and p: OJ= OJ(x, p). 
</p>
<p>Quantum Mechanics 
I. The state of the particle is represen-
</p>
<p>ted by a vector 11/f(t)) in a Hi! bert 
space. 
</p>
<p>II. The independent variables x and p of 
classical mechanics are represt~nted 
</p>
<p>t Recall the discussion in the Preface regarding the sense in which the word is used here. 115 </p>
<p/>
</div>
<div class="page"><p/>
<p>116 
</p>
<p>CHAPTER4 
</p>
<p>III. If the particle is in a state given by 
x and p, the measurement II of the 
variable OJ will yield a value OJ (x, p). 
The state will remain unaffected. 
</p>
<p>IV. The state variables change with time 
</p>
<p>according to Hamilton's equations: 
</p>
<p>. j)yt' 
x=-
</p>
<p>iJp 
</p>
<p>. j)yt' 
p=--
</p>
<p>iJx 
</p>
<p>4.2. Discussion of Postulates 1-111 
</p>
<p>by Hermitian operators X and P 
with the following matrix elements 
in the eigenbasis of xt 
</p>
<p>(xiXIx') =x8(x-x') 
</p>
<p>(xiPix') = -i1i8 '(x- x') 
</p>
<p>The operators corresponding to 
dependent variables OJ(x,p) are 
given Hermitian operators 
</p>
<p>Q(X, P) = OJ(x-+X, p-+P)&sect; 
</p>
<p>III. If the particle is in a state I yt), meas-
urement11 of the variable (corre-
</p>
<p>sponding to) Q will yield one of the 
eigenvalues OJ with probability 
P(OJ)oci(OJI yt)l 2&bull; The state of the 
system will change from I yt) to I OJ) 
as a result of the measurement. 
</p>
<p>IV. The state vector I yt(t)) obeys the 
Schrodinger equation 
</p>
<p>. d 
zli -I yt(t))=HI yt(t)) 
</p>
<p>dt 
</p>
<p>where H(X, P)=Yf'(x-+X, p-+P) is 
the quantum Hamiltonian operator 
and Yf' is the Hamiltonian for the 
corresponding classical problem. 
</p>
<p>The postulates (of classical and quantum mechanics) fall naturally into two 
</p>
<p>sets: the first three, which tell us how the system is depicted at a given time, and the 
</p>
<p>last, which specifies how this picture changes with time. We will confine our attention 
</p>
<p>to the first three postulates in this section, leaving the fourth for the next. 
The first postulate states that a particle is described by a ket I yt) in a Hilbert 
</p>
<p>space which, you will recall, contains proper vectors normalizable to unity as well as 
</p>
<p>t Note that the X operator is the same one discussed at length in Section 1.10. Likewise P = 11K, where 
K was also discussed therein. You may wish to go over that section now to refresh your memory. 
</p>
<p>&sect; By this we mean that 0 is the same function of X and P as m is of x and p. 
</p>
<p>11 That is, in an ideal experiment consistent with the theory. It is assumed you are familiar with the ideal 
</p>
<p>classical measurement which can determine the state of the system without disturbing it in any way. A 
</p>
<p>discussion of ideal quantum measurements follows. </p>
<p/>
</div>
<div class="page"><p/>
<p>improper vectors, normalizable only to the Dirac delta functions.t Now, a ket in 
such a space has in general an infinite number of components in a given basis. One 
wonders why a particle, which had only two independent degrees of freedom, x and 
p, in classical mechanics, now needs to be specified by an infinite number of variables. 
What do these variables tell us about the particle? To understand this we must go 
on to the next two postulates, which answer exactly this question. For the present 
let us note that the double-slit experiment has already hinted to us that a particle 
such as the electron needs to be described by a wave function IJI(x). We have seen 
in Section 1.10 that a function f(x) may be viewed as a ket I f) in a Hilbert space. 
The ket I 'I') of quantum mechanics is none other than the vector representing the 
probability amplitude IJI(x) introduced in the double-slit experiment. 
</p>
<p>When we say that I 'I') is an element of a vector space we mean that if I 'I') and 
I lJI') represent possible states of a particle so does al 'I')+ f3l '1''). This is called the 
principle of superposition. The principle by itself is not so new: we know in classical 
physics, for example, that if f(x) and g(x) [with f(O) = f(L) = g(O) = g(L) =0] are 
two possible displacements of a string, so is the superposition af(x) + f3g(x). What 
is new is the interpretation of the superposed state al 'I')+ /31 lJI'). In the case of the 
string, the state af + f3g has very different attributes from the states f and g: it will 
look different, have a different amount of stored elastic energy, and so on. In quantum 
theory, on the other hand, the state al 'I')+ /31 'I'') will, loosely speaking, have attri-
butes that sometimes resemble that of I 'I') and at other times those of I lJI'). There 
is, however, no need to speak loosely, since we have postulates II and III to tell us 
exactly how the state vector I 'I') is to be interpreted in quantum theory. Let us find 
out. 
</p>
<p>In classical mechanics when a state (x, p) is given, one can say that any dynam-
ical variable m has a value m(x, p ), in the sense that if the variable is measured the 
result m(x,p) will obtain. What is the analogous statement one can make in quantum 
mechanics given that the particle is in a state I lJI)? The answer is provided by 
Postulates II and III, in terms of the following steps: 
</p>
<p>Step 1. Construct the corresponding quantum operator n = m(x-&gt;X, p-&gt;P), 
where X and P are the operators defined in postulate II. 
</p>
<p>Step 2. Find the orthonormal eigenvectors I W;) and eigenvalues W; of n. 
</p>
<p>Step 3. Expand I 'I') in this basis: 
</p>
<p>Step 4. The probability P( m) that the result m will obtain is proportional to 
the modulus squared of the projection of I 'I') along the eigenvector I m ), that is 
P(m)ocl&lt;ml '1')1 2 &bull; In terms of the projection operator P.,=lm)&lt;ml, 
P(m) ex: I &lt;m I '1')1 2 =&lt;'I' I m) &lt;m I 'I')=&lt; 'I' I P, I 'I')=&lt; 'I' I P "'P rul 'I')= &lt;P .,lJf I P ru'I')-
</p>
<p>There is a tremendous amount of information contained in these steps. Let us 
note, for the present, the following salient points. 
</p>
<p>t The status of the two classes will be clarified later in this chapter. 
</p>
<p>117 
</p>
<p>THE POSTULATES 
</p>
<p>-A GENERAL 
</p>
<p>DISCUSSION </p>
<p/>
</div>
<div class="page"><p/>
<p>118 
</p>
<p>CHAPTER 4 
</p>
<p>(I) The theory makes only probabilistic predictions for the result of a measure-
</p>
<p>ment of n. Further, it assigns (relative) probabilities only for obtaining some eigen-
value OJ of n. Thus the only possible values of n are its eigenvalues. Since postulate 
II demands that n be Hermitian, these eigenvalues are all real. 
</p>
<p>(2) Since we are told that P(OJ;)cx:I(OJ;I '1')1 2, the quantity I(OJ;I '1')1 2 is only 
the relative probability. To get the absolute probability, we divide I(OJ;I '1')1 2 by the 
sum of all relative probabilities: 
</p>
<p>( 4.2.1) 
</p>
<p>It is clear that if we had started with a normalized state 
</p>
<p>we would have had 
</p>
<p>(4.2.2) 
</p>
<p>If I 'I') is a proper vector, such a rescaling is possible and will be assumed 
</p>
<p>hereafter. The probability interpretation breaks down if I 'I') happens to be one of 
</p>
<p>the improper vectors in the space, for in this case ('I' I 'I')= o(O) is the only sensible 
</p>
<p>normalization. The status of such vectors will be explained in Example 4.2.2 below. 
</p>
<p>Note that the condition ('I' I 'I')= 1 is a matter of convenience and not a physical 
</p>
<p>restriction on the proper vectors. (In fact the set of all normalized vectors does not 
</p>
<p>even form a vector space. If I 'I') and I 'I'') are normalized, then an arbitrary linear 
</p>
<p>combination, al 'I')+ /31 '1''), is not.) 
Note that the relative probability distributions corresponding to the states I 'I') 
</p>
<p>and al 'If), when they are renormalized to unity, reduce to the same absolute probabil-
ity distribution. Thus, corresponding to each physical state, there exists not one 
</p>
<p>vector, but a ray or "'direction" in Hilbert space. When we speak of the state of the 
</p>
<p>particle, we usually mean the ket I 'I') with unit norm. Even with the condition 
</p>
<p>('I' I 'I')= I, we have the freedom to multiply the ket by a number of the form e;o 
without changing the physical state. This freedom will be exploited at times to make 
</p>
<p>the components of I 'I') in some basis come out real. 
</p>
<p>(3) If I 'I') is an eigenstate I OJ;), the measurement of n is guaranteed to yield 
the result OJ;. A particle in such a state may be said to have a value OJ; for n in the 
classical sense. 
</p>
<p>(4) When two states IOJ 1) and IOJ2 ) are superposed to form a (normalized) 
state, such as 
</p>
<p>one gets the state, which upon measurement of n, can yield either OJ 1 or OJ 2 with 
probabilities I al 2 /(1 al 2 +I /31 2) and I /31 2 /(1 al 2 + 1/31 2), respectively. This is the peculiar </p>
<p/>
</div>
<div class="page"><p/>
<p>o) b) c) 
</p>
<p>; lo/) 
lwi) 
</p>
<p>Figure 4.1. (a) The normalized ket in V3(R) representing the state of the particle. (b) The 0 basis, lro 1), 
1 w 2), and 1 w 3 ). (c) The 0 and the A bases. To get the statistical information on a variable, we find the 
</p>
<p>eigenvectors of the corresponding operator and project 11/f) on that basis. 
</p>
<p>consequence of the superposition principle in quantum theory, referred to earlier. It 
</p>
<p>has no analog in classical mechanics. For example, if a dynamical variable of the 
</p>
<p>string in the state af + f3g is measured, one does not expect to get the value corre-
sponding to f some of the time and that corresponding to g the rest of the time; 
instead, one expects a unique value generally distinct from both. Likewise, the 
</p>
<p>functions f and af (a real) describe two distinct configurations of the string and are 
</p>
<p>not physically equivalent. 
</p>
<p>(5) When one wants information about another variable A, one repeats the 
</p>
<p>whole process, finding the eigenvectors I A;) and the eigenvalues A;. Then 
</p>
<p>P( .?c) = I I l!' &gt; 12 
</p>
<p>The bases of Q and A will of course be different in general. In summary, we have a 
</p>
<p>single ket Ill') representing the state ofthe particle in Hilbert space, and it contains 
</p>
<p>the statistical prediction for all observables. To extract this information for any 
</p>
<p>observable, we must determine the eigenbasis of the corresponding operator and find 
</p>
<p>the projection of Ill') along all its eigenkets. 
</p>
<p>(6) As our interest switches from one variable Q, to another, A, so does our 
</p>
<p>interest go from the kets lm), to the kets Ill). l'here is, however, no need to change 
</p>
<p>the basis each time. Suppose for example we are working in the Q basis in which 
</p>
<p>and P(m;) =I (w;ll!')l 2. If we want P(AJ we take the operator A (which is some 
given matrix with elements Au=(w;IA1w1)); find its eigenvectors lA;) (which are 
</p>
<p>column vectors with components (w1 1 ), and take the inner product (ll;llf/) in 
this basis: 
</p>
<p>Example 4.2.1. Consider the following example from a fictitious Hilbert space 
w3(R) (Fig. 4.1). In Fig. 4.la we have the normalized state llf!), with no reference 
</p>
<p>ll9 
</p>
<p>THE POSTULATES 
</p>
<p>A. GENERAL 
</p>
<p>DISCUSSION </p>
<p/>
</div>
<div class="page"><p/>
<p>120 
</p>
<p>CHAPTER 4 
</p>
<p>to any basis. To get predictions on n, we find its eigenbasis and express the state 
</p>
<p>vector I 'I') in terms of the orthonormal eigenvectors !rot), lw2), and lw3) (Fig. 
</p>
<p>4.lb). Let us suppose 
</p>
<p>This means that the values w1, OJ2, and w3 are expected with probabilities ~, k, 
and 4, respectively, and other values of OJ are impossible. If instead I '1/) were some 
eigenvector, say I Wt), then the result w1 would obtain with unit probability. Only a 
</p>
<p>particle in a state I 'If)= lwi) has a well-defined value of n in the classical sense. If 
</p>
<p>we want P(A.i) we construct the basis IA.t), IA.2), and IA.3), which can in general be 
</p>
<p>distinct from the n basis. In our example (Fig. 4.lc) there is just one common 
</p>
<p>eigenvector lro3)=IA.3). 0 
</p>
<p>Returning to our main discussion, there are a few complications that could arise 
</p>
<p>as one tries to carry out the steps 1-4. We discuss below the major ones and how 
</p>
<p>they are to be surmounted. 
</p>
<p>Complication 1: The Recipe Q=wl_x~x.p~P) Is Ambiguous. If, for example, 
</p>
<p>OJ =xp, we don't know ifQ=XP or PX since xp=px classically. There is no universal 
</p>
<p>recipe for resolving such ambiguities. In the present case, the rule is to use the 
</p>
<p>symmetric sum: n= (XP+ PX)/2. Notice incidentally that symmetrization also 
</p>
<p>renders n Hermitian. Symmetrization is the answer as long as n does not involve 
</p>
<p>products of two or more powers of X with two or more powers of P. If it does, only 
</p>
<p>experiment can decide the correct prescription. We will not encounter such cases in 
</p>
<p>this book. 
</p>
<p>Complication 2: The Operator Q Is Degenerate. Let us say w 1 = OJ2 =OJ. What 
</p>
<p>is P( OJ) in this case? We select some orthonormal basis I OJ, I ) and I OJ, 2) in the 
</p>
<p>eigenspace 'W"' with eigenvalue OJ. Then 
</p>
<p>which is the modulus squared of the projection of I 'If) in the degenerate eigenspace. 
</p>
<p>This is the result we will get if we assume that Wt and OJz are infintesimally distinct 
</p>
<p>and ask for P(w 1 or ro 2). In terms of the projection operator for the eigenspace, 
</p>
<p>IP'ro=lw. l)(w, ll+lw,2)(w,21 (4.2.3a) 
</p>
<p>we have 
</p>
<p>(4.2.3b) 
</p>
<p>In general, one can replace in Postulate III 
</p>
<p>P(w)cx:('lfiiP'"' I '1/) </p>
<p/>
</div>
<div class="page"><p/>
<p>where IP'"' is the projection operator for the eigenspace with eigenvalue ro. Then 
postulate III as stated originally would become a special case in which there is no 
degeneracy and each eigenspace is simply an eigenvector. 
</p>
<p>In our example from W3(R), if ro 1 = ro 2 = ro (Fig. 4.1 b) then P( ro) is the square 
of the component of llfl) in the "x y" plane. 
</p>
<p>Complication 3: The Eigenvalue Spectrum of Q Is Continuous. In this case one 
expands llfl) as 
</p>
<p>One expects that as ro varies continuously, so will (ro llf/), that is to say, one expects 
(ro llfl) to be a smooth function lf/(ro). To visualize this function one introduces an 
auxiliary one-dimensional space, called the ro space, the points in which are labeled 
by the coordinate ro. In this space lf/( ro) will be a smooth function of ro and is called 
the wave function in the ro space. We are merely doing the converse of what we did 
in Section 1.10 wherein we started with a functionf(x) and tried to interpret it as 
the components of an infinite-dimensional ket llfl) in the lx) basis. As far as the 
state vector llfl) is concerned, there is just one space, the Hilbert space, in which it 
resides. The ro space, the ll space, etc. are auxiliary manifolds introduced for the 
purpose of visualizing the components of the infinite-dimensional vector llf/) in the 
n basis, the A basis, and so on. The wave function lf/( ro) is also called the probability 
amplitude for finding the particle with n = ro. 
</p>
<p>Can we interpret I (ro llfl)l 2 as the probability for finding the particle with a 
value ro for 0? No. Since the number of possible values for ro is infinite and the 
total probability is unity, each single value of ro can be assigned only an infinitesimal 
probability. One interprets P( ro) =I ( ro llf/ )1 2 to be the probability density at ro, by 
which one means that P(ro) dro is the probability of obtaining a result between ro 
and ro + dro. This definition meets the requirement that the total probability be unity, 
since 
</p>
<p>f P( ro) dro = f I&lt; ro I lfl )1 2 dro = f &lt; lf/1 ro) &lt; ro llfl) dro 
= (lf/1/llfl&gt; = (lf/llf/) = 1 (4.2.4) 
</p>
<p>If (lf/llf/)=8(0) is the only sensible normalization possible, the state cannot be 
normalized to unity and P( ro) must be interpreted as the relative probability density. 
We will discuss such improper states later. 
</p>
<p>An important example of a continuous spectrum is that of X, the operator 
corresponding to the position x. The wave function in the X basis (or the x space), 
lfl(x), is usually referred to as just the wave function, since the X basis is almost 
always what one uses. In our discussions in the last chapter, llfl(x)l 2 was referred to 
as the probability for finding the particle at a given x, rather than as the probability 
density, in order to avoid getting into details. Now the time has come to become 
precise! 
</p>
<p>121 
THE POSTULATES 
</p>
<p>-A GENERAL 
DISCUSSION </p>
<p/>
</div>
<div class="page"><p/>
<p>122 
</p>
<p>CHAPTER 4 
</p>
<p>Earlier on we were wondering why it was that a classical particle defined by 
</p>
<p>just two numbers x and p now needs to be described by a ket which has an infinite 
</p>
<p>number of components. The answer is now clear. A classical particle has, at any 
</p>
<p>given time, a definite position. One simply has to give this value of x in specifying 
</p>
<p>the state. A quantum particle, on the other hand, can take on any value of x upon 
</p>
<p>measurement and one must give the relative probabilities for all possible outcomes. 
</p>
<p>This is part of the information contained in lf!(x) = (x llf!), the components of llf!) 
</p>
<p>in the X basis. Of course, in the case of the classical particle, one needs also to specify 
</p>
<p>the momentum p as well. In quantum theory one again gives the odds for getting 
</p>
<p>different values of momenta, but one doesn't need a new vector for specifying this; 
</p>
<p>the same ket Ill') when expanded in terms of the eigenkets I p) of the momentum 
operator P gives the odds through the wave function in p space, lf!(p) = (p llf!). 
</p>
<p>Complication 4: The Quantum Variable Q Has No Classical Counterpart. Even 
</p>
<p>"point" particles such as the electron are now known to carry "spin," which is an 
</p>
<p>internal angular momentum, that is to say, angular momentum unrelated to their 
</p>
<p>motion through space. Since such a degree of freedom is absent in classical mechanics, 
</p>
<p>our postulates do not tell us which operator is to describe this variable in quantum 
</p>
<p>theory. As we will see in Chapter 14, the solution is provided by a combination of 
</p>
<p>intuition and semi-classical reasoning. It is worth bearing in mind that no matter 
</p>
<p>how diligently the postulates are constructed, they must often be supplemented by 
</p>
<p>intuition and classical ideas. 
Having discussed the four-step program for extracting statistical information 
</p>
<p>from the state vector, we continue with our study of what else the postulates of 
</p>
<p>quantum theory tell us. 
</p>
<p>CoUapse of the State Vector 
</p>
<p>We now examine another aspect of postulate III, namely, that the measurement 
</p>
<p>of the variable n changes the state vector, which is in general some superposition 
of the form 
</p>
<p>into the eigenstate I co) corresponding to the eigenvalue co obtained in the measure-
ment. This phenomenon is called the collapse or reduction of the state vector. 
</p>
<p>Let us first note that any definitive statement about the impact of the measure-
</p>
<p>ment process presupposes that the measurement process is of a definite kind. For 
example, the classical mechanics maxim that any dynamical variable can be measured 
</p>
<p>without changing the state of the particle, assumes that the measurement is an ideal 
</p>
<p>measurement (consistent with the classical scheme). But one can think up nonideal 
</p>
<p>measurements which do change the state; imagine trying to locate a chandelier in a 
</p>
<p>dark room by waving a broom till one makes contact. What makes Postulate III 
</p>
<p>profound is that the measurement process referred to there is an ideal quantum 
</p>
<p>measurement, which in a sense is the best one can do. We now illustrate the notion 
</p>
<p>of an ideal quantum measurement and the content of this postulate by an example. </p>
<p/>
</div>
<div class="page"><p/>
<p>Consider a particle in a momentum eigenstate lp). The postulate tells us that 
</p>
<p>if the momentum in this state is measured we are assured a result p, and that the 
</p>
<p>state will be the same after the measurement (since llJI) = IP) is already an eigenstate 
of the operator Pin question). One way to measure the momentum of the particle 
</p>
<p>is by Compton scattering, in which a photon of definite momentum bounces off the 
</p>
<p>particle. 
Let us assume the particle is forced to move along the x-axis and that we send 
</p>
<p>in a right-moving photon of energy ncJ) that bounces off the particle and returns as 
</p>
<p>a left-moving photon of energy tim'. (How do we know what the photon energies 
</p>
<p>are? We assume we have atoms that are known to emit and absorb photons of any 
</p>
<p>given energy.) Using momentum and energy conservation: 
</p>
<p>cp' = cp + 1i( OJ+ w') 
</p>
<p>E' = E+ fi( m &middot;&middot;&middot;&middot;&middot; w') 
</p>
<p>it is now possible from this data to reconstruct the initial and final momenta of the 
</p>
<p>particle: 
</p>
<p>(11w +11m') .J n?c4 nw- tim' 
cp'= &middot;+ 1+-----
</p>
<p>2 n2w w' 2 
</p>
<p>Solving for w' and p' in terms of wand p, one readily sees that for any choice of p, if w &gt; 0, 
</p>
<p>then so does w'. Thus one can always make the change in momentum p'- p arbi-
trarily small. Hereafter, when we speak of a momentum measurement, this is 
</p>
<p>what we will mean. We will also assume that to each dynamical variable there exists 
</p>
<p>a corresponding ideal measurement. We will discuss, for example, the ideal position 
</p>
<p>measurement, which, when conducted on a particle in state lx), will give the result 
x with unit probability and leave the state vector unchanged. 
</p>
<p>Suppose now that we measure the position of a particle in a momentum eigenstate 
</p>
<p>lp). Since IP) is a sum of position eigenkets lx), 
</p>
<p>lp)= Jlx)(xlp) dx 
</p>
<p>the measurement will force the system into some state lx). Thus even the ideal 
</p>
<p>position measurement will change the state which is not a position eigenstate. Why 
does a position measurement alter the state I p ), while momentum measurement does 
not? The answer is that an ideal position measurement uses photons of infinitely 
high momentum (as we will see) while an ideal momentum measurement uses photons 
of infinitesimally low momentum (as we have seen). 
</p>
<p>This then is the big difference between classical and quantum mechanics: an 
ideal measurement of any variable w in classical mechanics leaves any state invariant, 
</p>
<p>123 
</p>
<p>THE POSTULATES 
</p>
<p>&middot;&middot;&middot;&middot;&middot;A GENERAL 
DISCUSSION </p>
<p/>
</div>
<div class="page"><p/>
<p>124 
</p>
<p>CHAPTER 4 
</p>
<p>whereas the ideal measurement off! in quantum mechanics leaves only the eigenstates 
</p>
<p>of n invariant. 
The effect of measurement may be represented schematically as follows: 
</p>
<p>Ill'&gt;-----~ 
Q measured, w obtained 
</p>
<p>where IP"' is the projection operator associated with I OJ), and the state after measure-
</p>
<p>ment has been normalized. If OJ is degenerate, 
</p>
<p>lli')....,.(IP liP )I/2 
mll' mll' 
</p>
<p>where IP"' is the projection operator for the eigenspace W"'. Special note should be 
</p>
<p>taken of the following point: if the initial state llft) were unknown, and the measure-
</p>
<p>ment yielded a degenerate eigenvalue OJ, we could not say what the state was after 
</p>
<p>the measurement, except that it was some state in the eigenspace with eigenvalue OJ. 
</p>
<p>On the other hand, if the initial state Ill') were known, and the measurement yielded 
</p>
<p>a degenerate value OJ, the state after measurement is known to be IP'rulll') (up to 
</p>
<p>normalization). Consider our example from W3(R) (Fig. 4.lb). Say we had OJ 1 = 
</p>
<p>OJ2 =OJ. Let us use an orthonormal basis I OJ, I), I OJ, 2), I OJ 3), where, as usual, the 
</p>
<p>extra labels I and 2 are needed to distinguish the basis vectors in the degenerate 
</p>
<p>eigenspace. If in this basis we know, for example, that 
</p>
<p>and the measurement gives a value OJ, the normalized state after measurement is 
</p>
<p>known to us to be 
</p>
<p>If, on the other hand, the initial state were unknown and a measurement gave a 
</p>
<p>result OJ, we could only say 
</p>
<p>where a and f3 are arbitrary real numbers. 
Note that although we do not know what a and f3 are from the measurement, 
</p>
<p>they are not arbitrary. In other words, the system had a well-defined state vector 
</p>
<p>Ill') before the measurement, though we did not know ll/f), and has a well-defined 
</p>
<p>state vector IP'wll!') after the measurement, although all we know is that it lies within 
</p>
<p>a subspace V"' . </p>
<p/>
</div>
<div class="page"><p/>
<p>How to Test Quantum Theory 
</p>
<p>One of the outstanding features of classical mechanics is that it makes fully 
deterministic predictions. It may predict for example that a particle leaving x =X; 
with momentump; in some potential V(x) will arrive 2 seconds later at x=xfwith 
momentum p =Pi&middot; To test the prediction we release the particle at x =X; with p = p; 
at t = 0 and wait at x = Xf and see if the particle arrives there with p = PJ at t = 2 
seconds. 
</p>
<p>Quantum theory, on the other hand, makes statistical predictions about a 
particle in a state I 'I') and claims that this state evolves in time according to 
Schrodinger's equation. To test these predictions we must be able to 
</p>
<p>(1) Create particles in a well-defined state I ljl). 
(2) Check the probabilistic predictions at any time. 
</p>
<p>The collapse of the state vector provides us with a good way of preparing definite 
states: we begin with a particle in an arbitrary state I 'I') and meaure a variable n. 
If we get a nondegenerate eigenvalue ro, we have in our hands the state I ro ). (If ro 
is degenerate, further measurement is needed. We are not ready to discuss this 
problem.) Notice how in quantum theory, measurement, instead of telling us what 
the system was doing before the measurement, tells us what it is doing just after the 
measurement. (Of course it does tell us that the original state had some projection 
on the state I ro) obtained after measurement. But this information is nothing com-
pared to the complete specifications of the state just after measurement.) 
</p>
<p>Anyway, assume we have prepared a state lro). If we measure some variable A, 
immediately thereafter, so that the state could not have changed from I ro ), and if 
say, 
</p>
<p>the theory predicts that A1 and A2 will obtain with probabilities 1/3 and 2/3, respec-
tively. If our measurement gives a A;, i#1, 2 (or worse still a A#any eigenvalue!) 
that is the end of the theory. So let us assume we get one of the allowed values, say 
A1 &bull; This is consistent with the theory but does not fully corroborate it, since the 
odds for A1 could have been 1/30 instead of 1/3 and we could still get A1 &bull; Therefore, 
we must repeat the experiment many times. But we cannot repeat the experiment 
with this particle, since after the measurement the state of the particle is I A1). We 
must start afresh with another particle in I ro ). For this purpose we require a quantum 
ensemble, which consists of a large number N of particles all in the same state I ro ). 
If a measurement of A is made on every one of these particles, approximately N /3 
will yield a value A1 and end up in the state I A1) while approximately 2N /3 will yield 
a value A2 and end up in a state jA,2 ). For sufficiently large N, the deviations from 
the fractions 1/3 and 2/3 will be negligible. The chief difference between a classical 
ensemble, of the type one encounters in, say, classical statistical mechanics, and the 
quantum ensemble referred to above, is the following. If in a classical ensemble of 
N particles N /3 gave a result A1 and 2N /3 a result il2 , one can think of the ensemble 
as having contained N /3 particles with A= il1 and the others with il = il2 before the 
</p>
<p>125 
</p>
<p>THE POSTULATES 
-A GENERAL 
</p>
<p>DISCUSSION </p>
<p/>
</div>
<div class="page"><p/>
<p>126 
</p>
<p>CHAPTER 4 
</p>
<p>measurement. In a quantum ensemble, on the other hand, every particle is assumed 
</p>
<p>to be in the same state I m) prior to measurement (i.e., every particle is potentially 
capable of yielding either result A. 1 or A.2). Only after that measurement are a third 
</p>
<p>of them forced into the state I A1) and the rest into I .Av2). 
Once we have an ensemble, we can measure any other variable and test the 
</p>
<p>expectations of quantum theory. We can also prepare an ensemble, let it evolve in 
</p>
<p>time, and study it at a future time to see if the final state is what the Schrodinger 
</p>
<p>equation tells us it should be. 
</p>
<p>Example 4.2.2. An example of an ensemble being used to test quantum theory 
</p>
<p>was encountered in the double-slit experiment, say with photons. A given photon of 
</p>
<p>momentum p and energy E was expected to hit the detectors with a probability 
</p>
<p>density given by the oscillating function I 'lf(x)l 2&bull; One could repeat the experiment 
</p>
<p>N times, sending one such photon at a time to see if the final number distribution 
</p>
<p>indeed was given by I 'l'(x)l 2. One could equally well send in a macroscopic, mono-
</p>
<p>chromatic beam of light of frequency m=E/'Ii and wave number k=p/'li, which 
</p>
<p>consists of a large number of photons of energy E and momentum p. If one makes the 
</p>
<p>assumption (correct to a high degree) that the photons are noninteracting, sending in 
</p>
<p>the beam is equivalent to experimenting with the ensemble. In this case the intensity 
</p>
<p>pattern will take the shape of the probability density I ~r{x)l 2 , the instant the beam 
is turned on. 0 
</p>
<p>Example 4.2.3. The following example is provided to illustrate the distinction 
</p>
<p>between the probabilistic descriptions of systems in classical mechanics and in quan-
</p>
<p>tum mechanics. 
</p>
<p>We choose as our classical system a six-faced die for which the probabilities 
</p>
<p>P(n) of obtaining a number n have been empirically determined. As our quantum 
</p>
<p>system we take a particle in a state 
</p>
<p>6 
</p>
<p>1'1')= 2: C;lm;) 
i=l 
</p>
<p>Suppose we close our eyes, toss the die, and cover it with a mug. Its statistical 
</p>
<p>description has many analogies with the quantum description of the state I 'I'): 
</p>
<p>(1) The state of the die is described by a probability function P(n) before the mug 
</p>
<p>is lifted. 
(2) The only possible values of n are 1, 2, 3, 4, 5, and 6. 
(3) If the mug is lifted, and some value-say n = 3-is obtained, the function P(n) 
</p>
<p>collapses to On3. 
(4) If an ensemble of N such dice are thrown, NP(n) of them will give the result n 
</p>
<p>(as N-+oo). 
</p>
<p>The corresponding statements for the particle in the state I 'I') are no doubt 
known to you. Let us now examine some of the key differences between the statistical 
</p>
<p>descriptions in the two cases. </p>
<p/>
</div>
<div class="page"><p/>
<p>(1) It is possible, at least in principle, to predict exactly which face of the die 
</p>
<p>will be on top, given the mass of the die, its position, orientation, velocity, and 
</p>
<p>angular velocity at the time of release, the viscosity of air, the elasticity of the table 
</p>
<p>top, and so on. The statistical description is, however, the only possibility in the 
</p>
<p>quantum case, even in principle. 
</p>
<p>(2) If the result n = 3 was obtained upon lifting the mug, it is consistent to 
</p>
<p>assume that the die was in such a state even prior to measurement. In the quantum 
</p>
<p>case, however, the state after measurement, say I m3), is not the state before measure-
</p>
<p>ment, namely I 'I'&gt;&middot; 
(3) If N such dice are tossed and covered with N mugs, there will be NP(l) 
</p>
<p>dice with n = l, NP(2) dice with n = 2, etc. in the ensemble before and after the 
</p>
<p>measurement. In contrast, the quantum ensemble corresponding to I 'I') will contain 
</p>
<p>N particles all of which are in the same state I 'I') (that is, each can yield any of the 
</p>
<p>values m1 , &bull;&bull;&bull; , m6) before the measurement, and NP(m;) particles in lm;) after the 
</p>
<p>measurement. Only the ensemble before the measurement represents the state I '1'). 
</p>
<p>The ensemble after measurement is a mixture of six ensembles representing the states 
</p>
<p>lmi), ... ,Im6).t D 
</p>
<p>Having seen the utility of the ensemble concept in quantum theory, we now 
</p>
<p>define and discuss the two statistical variables that characterize an ensemble. 
</p>
<p>Expectation Value 
</p>
<p>Given a large ensemble of N particles in a state I'!'), quantum theory allows us 
</p>
<p>to predict what fraction will yield a value m if the variable Q is measured. This 
prediction, however, involves solving the eigenvalue problem of the operator n. If 
one is not interested in such detailed information on the state (or the corresponding 
</p>
<p>ensemble) one can calculate instead an average over the ensemble, called the expecta-
</p>
<p>tion value, (Q). The expectation value is just the mean value defined in statistics: 
</p>
<p>(4.2.5) 
</p>
<p>But for the factors m; multiplying each projection operator I m;) ( m;j, we could have 
</p>
<p>used L; lm;)(m;j =/.To get around this, note that m;lm;)=Qim;). Feeding this in 
and continuing, we get 
</p>
<p>Now we can use Li lm;) (m;l =I to get 
</p>
<p>(4.2.6) 
</p>
<p>t This is an example of a mixed ensemble. These will be discussed in the digression on density matrices, 
which follows in a while. 
</p>
<p>127 
</p>
<p>THE POSTULATES 
</p>
<p>-A GENERAL 
</p>
<p>DISCUSSION </p>
<p/>
</div>
<div class="page"><p/>
<p>128 
</p>
<p>CHAPTER 4 
</p>
<p>There are a few points to note in connection with this formula. 
</p>
<p>(1) To calculate (Q), one need only be given the state vector and the operator Q 
(say as a column vector and a matrix, respectively, in some basis). There is no 
need to find the eigenvectors or eigenvalues of Q. 
</p>
<p>(2) If the particle is in an eigenstate of n, that is Ollfl&gt; = wilf!), then (Q) = w. 
(3) By the average value of Q we mean the average over the ensemble. A given 
</p>
<p>particle will of course yield only one of the eigenvalues upon measurement. The 
mean value will generally be an inaccessible value for a single measurement unless 
it accidentally equals an eigenvalue. [A familiar example of this phenomenon is 
that of the mean number of children per couple, which may be 2.12, although 
the number in a given family is restricted to be an integer.] 
</p>
<p>The Uncertainty 
</p>
<p>In any situation described probabilistically, another useful quantity to specify 
besides the mean is the standard deviation, which measures the average fluctuation 
around the mean. It is defined as 
</p>
<p>(4.2.7) 
</p>
<p>and often called the root-mean-squared deviation. In quantum mechanics, it is 
referred to as the uncertainty in Q. If Q has a discrete spectrum 
</p>
<p>(4.2.8) 
</p>
<p>and if it has a continuous spectrum, 
</p>
<p>(4.2.9) 
</p>
<p>Notice that L\Q, just like (Q), is also calculable given just the state and the operator, 
for Eq. (4.2.7) means just 
</p>
<p>(4.2.10) 
</p>
<p>Usually the expectation value and the uncertainty provide us with a fairly good 
description of the state. For example, if we are given that a particle has (X) =a and 
L\X = L\, we know that the particle is likely to be spotted near x =a, with deviations 
of order L\. 
</p>
<p>So far, we have concentrated on the measurement of a single variable at a time. 
We now turn our attention to the measurement of more than one variable at a time. 
(Since no two independent measurements can really be performed at the same time, 
we really mean the measurement of two or more dynamical variables in rapid 
succession.) </p>
<p/>
</div>
<div class="page"><p/>
<p>Exercise 4.2.1 (Very Important). Consider the following operators on a Hilbert space 
</p>
<p>'\./3(C): 
</p>
<p>Ly=-k i 0 [
0 -j 
</p>
<p>2 0 -n 
(l) What are the possible values one can obtain if Lz is measured? 
</p>
<p>(2) Take the state in which Lz= 1. In this state what are (Lx), (L;), and 6.Lx? 
</p>
<p>(3) Find the normalized eigenstates and the eigenvalues of Lx in the Lz basis. 
</p>
<p>(4) If the particle is in the state with Lz =-I, and Lx is measured, what are the possible 
</p>
<p>outcomes and their probabilities? 
(5) Consider the state 
</p>
<p>[ 
l/2] 
</p>
<p>IV')= l/2 
l /21/2 
</p>
<p>in the Lz basis. If Li is measured in this state and a result +I is obtained. what is the state 
</p>
<p>after the measurement? How probable was this result? If Lz is measured immediately 
</p>
<p>afterwards, what are the outcomes and respective probabilities? 
</p>
<p>(6) A particle is in a state for which the probabilities are P(Lz=l)=l/4, P(Lz=O)= 
</p>
<p>1/2, and P(Lz=-1)=1/4. Convince yourself that the most general, normalized state with 
</p>
<p>this property is 
</p>
<p>It was stated earlier on that if I yt) is a normalized state then the state e'0 1 yt) is a physically 
</p>
<p>equivalent normalized state. Does this mean that the factors e16' multiplying the Lz eigenstates 
</p>
<p>are irrelevant? [Calculate for example P(Lx=O).] 
</p>
<p>Compatible and Incompatible Variables 
</p>
<p>A striking feature of quantum theory is that given a particle in a state I 1{1), one 
cannot say in general that the particle has a definite value for a given dynamical 
</p>
<p>variable Q: a measurement can yield any eigenvalue ro for which (ro I 1{1) is not zero. 
The exceptions are the states I ro). A particle in one of these states can be said, as 
in classical mechanics, to have a value ro for Q, since a measurement is assured to 
</p>
<p>give this result. To produce such states we need only take an arbitrary state I yt) and 
measure !1. The measurement process acts as a filter that lets through just one 
</p>
<p>component of ltJI), along some lw). The probability that this will happen is P(w) = 
l&lt;wltJ1)12. 
</p>
<p>We now wish to extend these ideas to more than one variable. We consider 
</p>
<p>first the question of two operators. The extension to more than two will be 
</p>
<p>129 
</p>
<p>THE POSTULATES 
</p>
<p>-A GENERAL 
</p>
<p>DISCUSSION </p>
<p/>
</div>
<div class="page"><p/>
<p>130 
</p>
<p>CHAPTER 4 
</p>
<p>straightforward. We ask: 
</p>
<p>Question 1. Is there some multiple filtering process by which we can take an 
ensemble of particles in some state llj/) and produce a state with well-defined values 
OJ and A for two variables 0 and A? 
</p>
<p>Question 2. What is the probability that the filtering will give such a state if we 
start with the state lljl)? 
</p>
<p>To answer these questions, let us try to devise a multiple filtering scheme. Let 
us first measure 0 on the ensemble described by llj/) and take the particles that yield 
a result OJ. These are in a state that has a well-defined value for 0. We immediately 
measure A and pick those particles that give a result A. Do we have now an ensemble 
that is in a state with 0 =OJ and A= A? Not generally. The reason is clear. After the 
first measurement, we had the system in the state I OJ), which assured a result OJ for 
0, but nothing definite for A (since I OJ) need not be an eigenstate of A). Upon 
performing the second measurement, the state was converted to 
</p>
<p>llji')=IA) 
</p>
<p>and we are now assured a result A for A, but nothing definite for 0 (since lA.) need 
not be an eigenstate of 0). 
</p>
<p>In other words, the second filtering generally alters the state produced by the 
first. This change is just the collapse of the state vector I OJ)= I IA.)&lt;A.I OJ) into the 
eigenstate I A.). 
</p>
<p>An exception occurs when the state produced after the first measurement is 
unaffected by the second. This in tum requires that I OJ) also be an eigenstate of A. 
The answer to the first question above is then in the affirmative only for the simulta-
neous eigenstates I OJA). The means for producing them are just as described above. 
These kets satisfy the equations 
</p>
<p>01 OJA) = OJI OJA) (4.2.11) 
</p>
<p>AI OJA) =AI OJ A.) (4.2.12) 
</p>
<p>The question that arises naturally is: When will two operators admit simulta-
neous eigenkets? A necessary (but not sufficient) condition is obtained by operating 
Eq. ( 4.2.12) with 0, Eq. ( 4.2.11) with A, and taking the difference: 
</p>
<p>(QA- An )I OJA) = 0 (4.2.13) 
</p>
<p>Thus [0, A] must have eigenkets with zero eigenvalue if simultaneous eigenkets are 
to exist. A pair of operators 0 and A will fall into one of the three classes: 
</p>
<p>A. Compatible: [0, A] =0 
B. Incompatible: [0, A]= something that obviously has no zero eigenvalue 
C. Others </p>
<p/>
</div>
<div class="page"><p/>
<p>Class A. If two operators commute, we know a complete basis of simultaneous 
eigenkets can be found. Each element lmA) of this basis has well-defined values for 
0 and A. 
</p>
<p>Class B. The most famous example of this class is provided by the position and 
momentum operators X and P, which obey the canonical commutation rule 
</p>
<p>[X, P] = i1i (4.2.14) 
</p>
<p>Evidently we cannot ever have i1illf/) = Ollf/) for any nontrivial I If/). This means there 
doesn't exist even a single ket for which both X and Pare well defined. Any attempt 
to filter X is ruined by a subsequent filtering for P and vice vesa. This is the origin 
of the famous Heisenberg uncertainty principle, which will be developed as we go 
along. 
</p>
<p>Class C. In this case there are some states that are simultaneous eigenkets. There 
is nothing very interesting we can say about this case except to emphasize that even 
if two operators don't commute, one can still find a few common eigenkets, though 
not a full basis. (Why?) 
</p>
<p>Let us now turn to the second question of the probability of obtaining a state 
lmA.) upon measurement of 0 and A in a state llfl). We will consider just case A; 
the question doesn't arise for case B, and case C is not very interesting. (You should 
be able to tackle case C yourself after seeing the other two cases.) 
</p>
<p>Case A. Let us first assume there is no degeneracy. Thus, to a given eigenvalue 
A, there is just one ket and this must be a simultaneous eigenket lmA). Suppose 
we measured 0 first. We get m with a probability P(m) =I (mA.IIf/)1 2 . After the 
measurement, the particle is in a state lmA). The measurement of A is certain to 
yield the result A. The probability for obtaining m for 0 and A for A is just the 
product of the two probabilities 
</p>
<p>Notice that if A were measured first and 0 next, the probability is the same for 
getting the results A. and m. Thus if we expand I If/) in the complete common eigenbasis 
as 
</p>
<p>(4.2.15a) 
</p>
<p>then 
</p>
<p>(4.2.15b) 
</p>
<p>The reason for calling 0 and A compatible if [0, A]= 0 is that the measurement 
of one variable followed by the other doesn't alter the eigenvalue obtained in the 
first measurement and we have in the end a state with a well-defined value for both 
observables. Note the emphasis on the invariance of the eigenvalue under the second 
measurement. In the non-degenerate case, this implies the invariance of the state 
vector as well. In the degenerate case, the state vector can change due to the second 
</p>
<p>131 
</p>
<p>THE POSTULATES 
</p>
<p>-A GENERAL 
</p>
<p>DISCUSSION </p>
<p/>
</div>
<div class="page"><p/>
<p>132 
</p>
<p>CHAPTER 4 
</p>
<p>measurement, though the eigenvalue will not, as the following example will show. 
Consider two operators A and n on w3(R). Let I W:v~. 3 ) be one common eigenvector. 
Let }q = Az =A. Let OJ I # COz be the eigenvalues of n in this degenerate space. Let us 
use as a basis lm"lc), lm2.A), and jw3A.3). Consider a normalized state 
</p>
<p>(4.2.16) 
</p>
<p>Let us say we measure n first and get w3 &bull; The state becomes i w3},3 ) and the subse-
quent measurement of i\ is assured to give a value ),3 and to leave the state alone. 
</p>
<p>Thus P( W3, A.3) =I (ro3A.3I '1')1 2 = a 2 . Evidently P(o&gt;3, A3) = P(A.3, w3). 
Suppose that the measurement of n gave a value m 1 &bull; The resulting state is I m 1},) 
</p>
<p>and the probability for this outcome is I (m 1A.I '1')1 2 The subsequent measurement of 
</p>
<p>i\ will leave the state alone and yield the result A with unit probability. Thus P(m 1 , A.) 
is the product of the probabilities: 
</p>
<p>(4.2.17) 
</p>
<p>Let us now imagine the measurements carried out in reverse order. Let the result 
</p>
<p>of the measurement be il. The state I'!'') after measurement is the projection of I 'If) 
in the degenerate A eigenspace: 
</p>
<p>p I VI\ Ill WJA) + rl Olzt,) 
I'!'')= I(IP"l!f~IP"~)Il/2 (/32+ y2)1;2 (4.2.18) 
</p>
<p>where, in the expression above, the projected state has been normalized. The prob-
</p>
<p>ability for this outcome is P(il) = [3 2 + y2 , the square of the projection of llfl) in the 
eigenspace. Iff.! is measured now, both results w 1 and w2 are possible. The probability 
for obtaining m1 is I (m 1A.I '1'')1 2 = f3 2/(f3 2 + y2). Thus, the probability for the result 
i\ =A., Q = m 1 , is the product of the probabilities: 
</p>
<p>."\ . 2 2 /3 2 2 
P(A, u&gt;J = (jJ + y ) &middot; 13--, 2= f3 = P(m1, A.) -+ y 
</p>
<p>(4.2.19) 
</p>
<p>Thus P(w 1 , .lc) = P(},, wJ) independent of the degeneracy. But this time the slate 
</p>
<p>suffered a change due to the second measurement (unless by accident llf!') has no 
component along I m2A.) ). Thus compatibility generally implies the invariance under 
the second measurement of the eigenvalue measured in the first. Therefore, the state 
</p>
<p>can only be said to remain in the same eigenspace after the second measurement. If 
</p>
<p>the first eigenvalue is non-degenerate, the eigenspace is one dimensional and the state 
vector itself remains invariant. 
</p>
<p>In our earlier discussion on how to produce well-defined states llf!) for testing 
quantum theory, it was observed that the measurement process could itself be used 
as a preparation mechanism: if the measurement of Q on an arbitrary, unknown 
initial state given a result w, we are sure we have the state I 'If)= I m ). But this 
presumes m is not a degenerate eigenvalue. If it is degenerate, we cannot nail down 
the state, except to within an eigenspace. It was therefore suggested that we stick to 
variables with a nondegenerate spectrum. We can now lift that restriction. Let us </p>
<p/>
</div>
<div class="page"><p/>
<p>say a degenerate eigenvalue m for the variable Q was obtained. We have then some 
vector in the m eigenspace. We now measure another compatible variable A. If we 
get a result A., we have a definite state lmA.), unless the value (m, A.) itself is degenerate. 
We must then measure a third variable r compatible with n and A and so on. 
Ultimately we will get a state that is unique, given all the simultaneous eigenvalues: 
lm, A., y, .. . ). It is presumed that such a set of compatible observables, called a 
complete set of commuting observables, exists. To prepare a state for studying quan-
tum theory then, we take an arbitrary initial state and filter it by a sequence of 
compatible measurements till it is down to a unique, known vector. Any nondegener-
ate operator, all by itself, is a "complete set." 
</p>
<p>Incidentally, even if the operators Q and A are incompatible, we can specify 
the probability P(m, A.) that the measurement of Q followed by that of A on a state 
I f/1) will give the results m and A., respectively. However, the following should be 
noted: 
</p>
<p>(1) P(m, A.)#P(A., m) in general. 
</p>
<p>(2) The probability P(m, A.) is not the probability for producing a final state 
that has well-defined values m and A. for Q and A. (Such a state doesn't exist by the 
definition of incompatibility.) The state produced by the two measurements is just 
the eigenstate of the second operator with the measured eigenvalue. 
</p>
<p>The Density Matrix-a Digressiont 
</p>
<p>So far we have considered ensembles of N systems all in the same state I f/1). 
They are hard to come by in practice. More common are ensembles of N systems, 
n; ( i = 1, 2, ... , k) of which are in the state I i). (We restrict ourselves to the case 
where I i) is an element of an orthonormal basis.) Thus the ensemble is described by 
k kets II), 12), ... , lk), and k occupancy numbers n1 , &bull;&bull;&bull; , nk. A convenient way to 
assemble all this information is in the form of the density matrix (which is really an 
operator that becomes a matrix in some basis) : 
</p>
<p>(4.2.20) 
</p>
<p>where p; = n;/ N is the probability that a system picked randomly out of the ensemble 
is in the state li). The ensembles we have dealt with so far are said to be pure; they 
correspond to all p; = 0 except one. A general ensemble is mixed. 
</p>
<p>Consider now the ensemble average of n. It is 
</p>
<p>(4.2.21) 
</p>
<p>The bar on (0) reminds us that two kinds of averaging have been carried out: a 
quantum average (ililli) for each system in li) and a classical average over the 
</p>
<p>t This digression may be omitted or postponed without loss of continuity. 
</p>
<p>133 
</p>
<p>THE POSTULATES 
-A GENERAL 
</p>
<p>DISCUSSION </p>
<p/>
</div>
<div class="page"><p/>
<p>134 
</p>
<p>CHAPTER 4 
</p>
<p>systems in different states I i). Observe that 
</p>
<p>Tr(Qp)=I &lt;JIQplj) 
j 
</p>
<p>i i i j 
</p>
<p>(4.2.22) 
</p>
<p>The density matrix contains all the statistical information about the ensemble. Sup-
</p>
<p>pose we want, not (Q), but instead P(ro ), the probability of obtaining a particular 
</p>
<p>value ro. We first note that, for a pure ensemble, 
</p>
<p>P( ro) = I &lt; ro I 'I'&gt; 12 = &lt; 'I' I ro &gt; &lt; ro I 'I'&gt; = &lt; 'I' liP' OJ I 'I'&gt; = &lt; 1P' OJ&gt; 
</p>
<p>which combined with Eq. (4.2.22) tells us that 
</p>
<p>The following results may be easily established: 
</p>
<p>(l).o&middot;=p 
</p>
<p>(2) Tr p= 1 
(3) p2= p 
</p>
<p>(4) p= (1/k)I 
</p>
<p>(5) Tr p2 ::; I 
</p>
<p>for a pure ensemble 
</p>
<p>for an ensemble uniformly distributed over k states 
</p>
<p>(equality holds for a pure ensemble) 
</p>
<p>You are urged to convince yourself of these relations. 
</p>
<p>( 4.2.23) 
</p>
<p>Example 4.2.4. To gain more familiarity with quantum theory let us con-
</p>
<p>sider an infinite-dimensional ket I lf/) expanded in the basis lx) of the position 
operator X: 
</p>
<p>1'1')= Joc lx)(xllfl)dx= f" lx)lfl(x)dx 
-x:.&middot; -rx: 
</p>
<p>We call lfl(x) the wave function (in the X basis). Let us assume lfl(x) is a Gaussian, 
</p>
<p>that is, lfl(x)=A exp[-(x-af/2.12 ] (Fig. 4.2a). We now try to extract information 
</p>
<p>about this state by using the postulates. Let us begin by normalizing the state: 
</p>
<p>l=(lflllfl)= I"' (lf!lx)(xllfl)dx= Joc llfl(x)l 2 dx 
-~ -~ 
</p>
<p>= f x A 2 e -(x-a)';A' dx = A2(n.12) 112 (see Appendix A.2) 
-c&lt;) </p>
<p/>
</div>
<div class="page"><p/>
<p>(a l (b) 
</p>
<p>D 
</p>
<p>Figure 4.2. (a) The modulus of the wave function, I (xlo/)1 '" 1 vr(x)l. (b) The modulus of the wave 
</p>
<p>function, I (plljf )I = IIJI(p )I. 
</p>
<p>So the normalized state is 
</p>
<p>The probability for finding the particle between x and x + dx is 
</p>
<p>P( )d -~ ( )12 f&middot;- j -(x-a)'iA'd X X - lJI X (,X ...... . 2 I !l e X 
(7rA ) . 
</p>
<p>which looks very much like Fig. 4.2a. Thus the particle is most likely to be found 
</p>
<p>around x =a, and chances of finding it away from this point drop rapidly beyond a 
</p>
<p>distance A. We can quantify these statements by calculating the expectation value 
</p>
<p>and uncertainty for X. Let us do so. 
</p>
<p>Now, the operator X defined in postulate H is the same one we discussed at 
length in Section 1.10. Its action in the X basis is simply to multiply by x, i.e., if 
</p>
<p>then, 
</p>
<p>x8(x ... x')!p(x') dx' 
</p>
<p>= XIJI(X) 
</p>
<p>Using this result, the mean or expectation value of X is 
</p>
<p>(X)=(IJIIXIIJI)= f (lJiix)(xiXIIJI)dx 
---x 
</p>
<p>= f cu lJI*(x)xlJI(x) dx 
-cv 
</p>
<p>dx 
</p>
<p>135 
</p>
<p>THE POSTULATES 
</p>
<p>A GENERAL 
</p>
<p>D!SCUSSJON </p>
<p/>
</div>
<div class="page"><p/>
<p>136 
</p>
<p>CHAPTER 4 
</p>
<p>If we define y=x-a, 
</p>
<p>=a 
</p>
<p>We should have anticipated this result of course, since the probability density is 
</p>
<p>symmetrically distributed around x =a. 
</p>
<p>Next, we calculate the fluctuations around (X)= a, i.e., the uncertainty 
</p>
<p>Now 
</p>
<p>So 
</p>
<p>AX=[(!J!I()i' (X))21!JI)]t2 
</p>
<p>= [(V!I.:\:"2 - 2.r(Y) + (Y/1 vr) ]1&middot; 2 
</p>
<p>=[(v;IX2-(X)'!lf!)]12 
</p>
<p>= [ (X2) _ (X/]t 2 
</p>
<p>(since (1/fiXI !Jf) =(X)) 
</p>
<p>A 
AX= ... i 2 
</p>
<p>So much for the information on the variable X. Suppose we next want to know 
</p>
<p>the probability distribution for different values of another dynamical variable, say 
</p>
<p>the momentum P. 
</p>
<p>(I) First we must construct the operator P in this basis. 
</p>
<p>(2) Then we must find its eigenvalues p, and eigenvectors lp). 
</p>
<p>(3) Finally, we must take the inner product (pllft). 
</p>
<p>(4) If pis discrete, l(p11 !JI)I 2 = P(p,), and if pis continuous, I(PIIJI)i 2 =P(p), the 
probability density. 
</p>
<p>Now, the P operator is just the K operator discussed in Section 1.10 multiplied by 
</p>
<p>1i and has the action of -in d/ dx in the X basis, for if 
</p>
<p>!Jf) = !Jf(X) </p>
<p/>
</div>
<div class="page"><p/>
<p>then 
</p>
<p>r"X 
</p>
<p>(xiPI1!')'= I (xiPix') 
1 &bull;-x 
</p>
<p>l/f) dx' 
</p>
<p>= Jx [-ino'(x-x')]1j!(x') dx' 
-Cfj 
</p>
<p>=-in~'!! 
dx 
</p>
<p>Thus, if we project the eigenvalue equation 
</p>
<p>onto the X basis, we get 
</p>
<p>or 
</p>
<p>Plp)=pip) 
</p>
<p>(x!Fp)=p(xlp) 
</p>
<p>. dljlp(x) -zn ~~=plflrlx) 
dx 
</p>
<p>(Postulate H) 
</p>
<p>where 1jlp(x) =(xi p ). The solutions, normalized to the Dirac delta functiont are 
(from Sectio~. 1.10) 
</p>
<p>Now we can compute 
</p>
<p>The modulus of 1j!(p) is a Gaussian (Fig. 4.2b) of width n/i 12b.. It follows that 
&lt;P)=O, and 11P=1i/2 112tl.. Since ti.X=A/2 112 ; we get the relation 
</p>
<p>t~.X &middot; t~.P = n/2 
</p>
<p>tHere we want &lt;PIp')= i5(p- p') = o(k- k')/'h. where p = hk. This explains the (2Jrhf. 112 normalization 
factor. 
</p>
<p>B7 
</p>
<p>THE POSTULATES 
</p>
<p>-A GENERAL 
</p>
<p>DISCUSSION </p>
<p/>
</div>
<div class="page"><p/>
<p>138 
</p>
<p>CHAPTER 4 
</p>
<p>The Gaussian happens to saturate the lower bound of the uncertainty relation (to 
be formally derived in chapter 9): 
</p>
<p>The uncertainty relation is a consequence of the general fact that anything 
narrow in one space is wide in the transform space and vice versa. So if you are a 
110-lb weakling and are taunted by a 600-lb bully, just ask him to step into momen-
~~~! D 
</p>
<p>This is a good place to point out that the plane waves eipx!R (and all improper 
vectors, i.e., vectors that can't be normalized to unity but only to the Dirac delta 
function) are introduced into the formalism as purely mathematical entities. Our 
inability to normalize them to unity translates into our inability to associate with 
them a sensible absolute probability distribution, so essential to the physical interpre-
tation of the wave function. In the present case we have a particle whose relative 
probability density is uniform in all of space. Thus the absolute probability of finding 
it in any finite volume, even as big as our solar system, is zero. Since any particle 
that we are likely to be interested in will definitely be known to exist in some finite 
volume of such large dimensions, it is clear that no physically interesting state will 
be given by a plane wave. But, since the plane waves are eigenfunctions of P, does 
it mean that states of well-defined momentum do not exist? Yes, in the strict sense. 
However, there do exist states that are both normalizable to unity (i.e., correspond 
to proper vectors) and come arbitrarily close to having a precise momentum. For 
example, a wave function that behaves as eipox/R over a large region of space and 
tapers off to zero beyond, will be normalizable to unity and will have a Fourier 
transform so sharply peaked at p = p 0 that momentum measurements will only give 
results practically indistinguishable from p0 &bull; Thus there is no conflict between the 
fact that plane waves are unphysical, while states of well-defined momentum exist, 
for "well defined" never means "mathematically exact," but only "exact to any 
measurable accuracy." Thus a particle coming out of some accelerator with some 
advertised momentum, say 500 GeV jc, is in a proper normalizable state (since it is 
known to be located in our laboratory) and not in a plane wave state corresponding 
to jp= 500 GeV /c). 
</p>
<p>But despite all this, we will continue to use the eigenkets I p &gt; as basis vectors 
and to speak of a particle being in the state jp), because these vectors are so much 
more convenient to handle mathematically than the proper vectors. It should, how-
ever, be borne in mind that when we say a particle is (coming out of the accelerator) 
in a state jp0 ), it is really in a proper state with a momentum space wave function 
so sharply peaked at p = p0 that it may be replaced by a delta function o(p-Po). 
</p>
<p>The other set of improper kets we will use in the same spirit are the position 
eigenkets lx), which also form a convenient basis. Again, when we speak of a particle 
being in a state jx0 ) we shall mean that its wave function is so sharply peaked at 
x=x0 that it may be treated as a delta function to a good accuracyJ 
</p>
<p>t Thus, by the physical Hilbert space, we mean the space of interest to physicists, not one whose elements 
all correspond to physically realizable states. </p>
<p/>
</div>
<div class="page"><p/>
<p>Occasionally, the replacement of a proper wave function by its improper coun-
terpart turns out to be a poor approximation. Here is an example from Chapter 19: 
Consider the probability that a particle coming out of an accelerator with a nearly 
exact momentum scatters off a target and enters a detector placed far away, and not 
in the initial direction. Intuition says that the answer must be zero if the target is 
absent. This reasonable condition is violated if we approximate the initial state of 
the particle by a plane wave (which is nonzero everywhere). So we proceed as follows. 
In the vicinity of the target, we use the plane wave to approximate the initial wave 
function, for the two are indistinguishable over the (finite and small) range of influ-
ence of the target. At the detector, however, we go back to the proper wave (which 
has tapered off) to represent the initial state. 
</p>
<p>Exercise 4.2.2. * Show that for a real wave function 'I'( X), the expectation value of 
momentum (P) =0. (Hint: Show that the probabilities for the momenta &plusmn;p are equal.) 
Generalize this result to the case 'I'= C'lf,, where 'If, is real and can arbitrary (real or complex) 
constant. (Recall that I 'I') and al 'I') are physically equivalent.) 
</p>
<p>Exercise 4.2.3. * Show that if ljl(x) has mean momentum (P), e'Pox!~ ljl(x) has mean 
momentum (P) +Po. 
</p>
<p>Example 4.2.5. The collapse of the state vector and the uncertainty principle 
play a vital role in explaining the following extension of the double slit experiment. 
Suppose I say, "I don't believe that a given particle (let us say an electron) doesn't 
really go through one slit or the other. So I will set up a light source in between the 
slits to the right of the screen. Each passing electron will be exposed by the beam 
and I note which slit it comes out of. Then I note where it arrives on the screen. I 
make a table of how many electrons arrive at each x and which slit they came from. 
Now there is no escape from the conclusion that the number arriving at a given x 
is the sum of the numbers arriving via S1 and S2 &bull; So much for quantum theory and 
its interference pattern!" 
</p>
<p>But the point of course is that quantum theory no longer predicts an interference 
pattern! The theory says that if an electron of definite momentum p is involved, the 
corresponding wave function is a wave with a well-defined wave number k = p jn, 
which interferes with itself and produces a nice interference pattern. This prediction 
is valid only as long as the state of the electron is what we say it is. But this state is 
necessarily altered by the light source, which upon measuring the position of the 
electron (as being next to S1, say) changes its wave function from something that 
was extended in space to something localized near S1 . Once the state is changed, the 
old prediction of interference is no longer valid. 
</p>
<p>Now, once in a while some electrons will get to the detectors without being 
detected by the light source. We note where these arrive, but cannot classify them 
as coming via S1 or S2. When the distribution of just these electrons is plotted; sure 
enough we get the interference pattern. We had better, for quantum theory predicts 
it, the state not having been tampered with in these cases. 
</p>
<p>The above experiment can also be used to demystify to some extent the collapse 
of the wave function under measurement. Why is it that even the ideal measurement 
produces unavoidable changes in the state? The answer, as we shall see, has to do 
with the fact that 1i is not zero. 
</p>
<p>139 
</p>
<p>THE POSTULATES 
-A GENERAL 
</p>
<p>DISCUSSION </p>
<p/>
</div>
<div class="page"><p/>
<p>140 
</p>
<p>CHAPTER 4 
</p>
<p>88~/-
,' e 
</p>
<p>t t 
Figure 4,3. Light of frequency ), bounces off the electron. enters 
</p>
<p>the objective 0 of the microscope, and enters the eye E of the 
</p>
<p>observer. 
</p>
<p>Consider the schematic set up in Fig. 4.3. Light of wavelength ?c illuminates an 
electron (e-), enters the objective ( 0) of a microscope ( M) and reaches our eye (E). 
</p>
<p>If o () is the opening angle of the cone of light entering the objective after interacting 
with the electron, classical optics limits the accuracy of the position measurement 
by an uncertainty 
</p>
<p>AX~?c/sin 80 
</p>
<p>Both classically and quantum mechanically, we can reduce A\' to 0 by reducing A. 
to zero.t In the latter description however, the improved accuracy in the position 
</p>
<p>measurement is at the expense of producing an increased uncertainty in the x compo-
</p>
<p>nent (Px) of the electron momentum. The reason is that light of wavelength A. is not 
</p>
<p>a continuous wave whose impact on the electron momentum may be arbitrarily 
reduced by a reduction of its amplitude, but rather a flux of photons of momentum 
</p>
<p>p = 2nfz/ A. As A. decreases, the collisions between the electron and the photons 
</p>
<p>become increasingly violent. This in itself would not lead to an uncertainty in the 
electron momentum, were it not for the fact that the x component of the photons 
entering the objective can range from 0 top sin o0=2n:fz sin 50/A.. Since at least 
</p>
<p>one photon must reach our eyes after bouncing off the electron for us to see it, there 
is a minimum uncertainty in the recoil momentum of the electron given by 
</p>
<p>2n:ft . '11 
!J.P, ::::: ---;-- sm o u 
</p>
<p>A 
</p>
<p>Consequently, we have at the end of our measurement an electron whose position 
and momenta are uncertain by AX and APx such that 
</p>
<p>[The symbols AX and AP, are not precisely the quantities defined in Eq_ (4.2.7) but 
are of the same order of magnitude.] This is the famous uncertainty principle. There is 
no way around it. If we soften the blow of each photon by increasing il or narrowing 
the objective to better constrain the final photon momentum, we lose in resolution. 
</p>
<p>t This would be the ideal position measurement. </p>
<p/>
</div>
<div class="page"><p/>
<p>More elaborate schemes, which determine the recoil of the microscope, are equally 
futile. Note that if 1i were 0, we could have AX and llPx simultaneously 0. Physically, 
it means that we can increase our position resolution without increasing the punch 
carried by the photons. Of course 1i is not zero and we can't make it zero in any 
experiment. But what we can do is to use bigger and bigger objects for our experiment 
so that in the scale of these objects 1i appears to be negligible. We then regain 
classical mechanics. The position of a billiard ball can be determined very well 
by shining light on it, but this light hardly affects its momentum. This is why one 
imagines in classical mechanics that momentum and position can be well defined 
simultaneously. 0 
</p>
<p>Generalization to More Degrees of Freedom 
</p>
<p>Our discussion so far has been restricted to a system with one degree of free-
dom-namely, a single particle in one dimension. We now extend our domain to a 
system with N degrees of freedom. The only modification is in postulate II, which 
now reads as follows. 
</p>
<p>Postulate II. Corresponding to the N Cartesian coordinates xi, ... , xN describ-
ing the classical system, there exist in quantum theory N mutually commuting 
operators XI, ... , XN. In the simultaneous eigenbasis I xi, X2, .&bull;&bull; , xN) of these 
operators, called the coordinate basis and normalized as 
</p>
<p>(the product of delta functions vanishes unless all the arguments vanish) we 
have the following correspondence: 
</p>
<p>P; being the momentum operator corresponding to the classical momentum 
p;. Dependent dynamical variables w(x1,p1) are represented by operators n= 
w(x1--+X;, Pr-+P1 ). 
</p>
<p>The other postulates remain the same. For example 
llf/(XI, ... , xN W x dxi ... dxN is the probability that the particle coordinates lie 
between XI, X2, &bull;&bull;&bull; , XN and XI +dXI, X2 +dx2 , &bull;.. , XN+dxN. 
</p>
<p>This postulate is stated in terms of Cartesian coordinates since only in terms 
of these can one express the operator assignments in the simple form X 1--+x1 , 
P;--+-i1i ajax;. Once the substitutions have been made and the desired equations 
obtained in the coordinate basis, one can perform any desired change of variable 
before solving them. Suppose, for example, that we want to find the eigenvalues and 
</p>
<p>141 
</p>
<p>THE POSTULATES 
-A GENERAL 
</p>
<p>DISCUSSION </p>
<p/>
</div>
<div class="page"><p/>
<p>142 
</p>
<p>CHAPTER 4 
</p>
<p>eigenvectors of the operator Q, corresponding to the classical variable 
</p>
<p>) 2 ') 
PI '~'P2+p3 
</p>
<p>w= + 
2m 
</p>
<p>, ) 
+x;;+ x; (4.2.24) 
</p>
<p>where x,, x 2 , and x 3 are the three Cartesian coordinates and p; the corresponding 
</p>
<p>momenta of a particle of mass m in three dimensions. Since the coordinates are 
</p>
<p>usually called x, y, and::, let us follow this popular notation and rewrite Eq. (4.2.24) 
</p>
<p>as 
</p>
<p>To solve the equation 
</p>
<p>with 
</p>
<p>we make the substitution 
</p>
<p>etc. and get 
</p>
<p>',"'''''"""."'''"""".""''+ + }'2 + :::2 
2m 
</p>
<p>Qjm)=wjw) 
</p>
<p>"i" ,2 ,., ~2] ' ( . ' ~)- ( ' . '') y , ~ lf'w X,y,~ -WVIw x,y,,. 
</p>
<p>( 4.2.25) 
</p>
<p>( 4.2.26) 
</p>
<p>Once we have obtained this differential equation, we can switch to any other set of 
</p>
<p>coordinates. In the present case the spherical coordinates r, 0, and &cent; recommend 
</p>
<p>themselves. Since </p>
<p/>
</div>
<div class="page"><p/>
<p>Eq. (4.2.26) becomes 
</p>
<p>-1:;2 [_!_ ~(r 2 Ol.flw)+-1- j_(sin () Ol.flw)+ 1 02 l.f/ro] 
2m r2 or or r2 sin () o() o() r2 sin2 () oqi 
</p>
<p>+r21.f!ro=llJl.flro (4.2.27) 
</p>
<p>What if we wanted to go directly from m in spherical coordinates 
</p>
<p>1 ( 2 p~ p~ ) m=- p,+2+ 2 &bull; 2 +? 
2m r rsm() 
</p>
<p>to Eq. (4.2.27)? It is clear upon inspection that there exists no simple rule [such as 
p,-+(-i1i ojor)] for replacing the classical momenta by differential operators in r, 0, 
and cp which generates Eq. (4.2.27) starting from the m above. There does exist a 
complicated procedure for quantizing in non-Cartesian coordinates, but we will not 
discuss it, since the recipe eventually reproduces what the Cartesian recipe (which 
seems to workt) yields so readily. 
</p>
<p>There are further generalizations, namely, to relativistic quantum mechanics 
and to quantum mechanics of systems in which particles are created and destroyed 
(so that the number of degrees of freedom changes!). Except for a brief discussion 
of these toward the end of the program, we will not address these matters. 
</p>
<p>4.3. The Schrodinger Equation (Dotting Your i 's and Crossing Your li's) 
</p>
<p>Having discussed in some detail the state at a given time, we now turn our 
attention to postulate IV, which specifies the change of this state with time. According 
to this postulate, the state obeys the Schrodinger equation 
</p>
<p>. d 
z1i -/l.fl(t))=H/ l.fl(t)) 
</p>
<p>dt 
</p>
<p>Our discussion of this equation is divided into three sections: 
</p>
<p>(1) Setting up the equation 
(2) General approach to its solution 
(3) Choosing a basis for solving the equation 
</p>
<p>Setting Up the Schrodinger Equation 
</p>
<p>(4.3.1) 
</p>
<p>To set up the Schrodinger equation one must simply make the substitution 
.Yf(x-+X,p--+P), where .Yf is the classical Hamiltonian for the same problem. Thus, 
</p>
<p>t In the sense that in cases where comparison with experiment is possible, as in say the hydrogen spectrum, 
there is agreement. 
</p>
<p>143 
</p>
<p>THE POSTULATES 
-A GENERAL 
</p>
<p>DISCUSSION </p>
<p/>
</div>
<div class="page"><p/>
<p>144 
</p>
<p>CHAPTER 4 
</p>
<p>if we are describing a harmonic oscillator. which ts classically described by the 
</p>
<p>Hamiltonian 
</p>
<p>the Hamiltonian operator in quantum mechanics is 
</p>
<p>p2 I ' 2 
II=&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;+ morX 
</p>
<p>2m 2 
</p>
<p>( 4.3.2) 
</p>
<p>(4.3.3) 
</p>
<p>In three dimensions, the Hamiltonian operator for the quantum oscillator is likewise 
</p>
<p>2 ~ ") 
</p>
<p>P~ + P; + P; l 2 2 2 &bull; 2 
H=&middot;&middot;&middot;&middot;&middot;-&middot;&middot;&middot;&middot;---+-mm (X + Y +Z) 
</p>
<p>2m 2 
</p>
<p>assuming the force constant is the same in all directions. 
</p>
<p>and 
</p>
<p>If the particle in one dimension is subject to a constant force f; then 
</p>
<p>&gt; 
</p>
<p>.tt=L-fx 
2m &middot; 
</p>
<p>pl 
H=--fX 
</p>
<p>2m &middot;&middot; 
</p>
<p>(4.3.4) 
</p>
<p>(4.3.5) 
</p>
<p>For a particle of charge q in an electromagnetic field in three dimensions, 
</p>
<p>lp-(q/c)A(r, t)l 2 
Yf =-&middot;&middot;&middot;&middot;-- +qcjJ(r, t) 
</p>
<p>2m 
( 4.3.6) 
</p>
<p>In constructing the corresponding quantum Hamiltonian operator, we must use the 
</p>
<p>symmetrized form 
</p>
<p>I , &middot;. 
</p>
<p>l ( q q q- \) 
ll=- P&middot;P--P&middot;A&middot;-&middot;-A&middot;P+2A&middot;A +q&cent; 
</p>
<p>2m, c c c . 
(4.3.7) 
</p>
<p>since P does not commute with A, which is a function of X, Y, and Z. 
</p>
<p>In this manner one can construct the Hamiltonian H for any problem with a 
</p>
<p>classical counterpart. Problems involving spin have no classical counterparts and 
</p>
<p>some improvisation is called for. We will discuss this question when we study spin 
</p>
<p>in some detail in Chapter 14. </p>
<p/>
</div>
<div class="page"><p/>
<p>General Approach to the Solution 
</p>
<p>Let us first assume that H has no explicit t dependence. In this case the equation 
</p>
<p>ifil ift)=H] yt) 
</p>
<p>is analogous to equations discussed in Chapter 1 
</p>
<p>lx)=fllx&gt; 
</p>
<p>and 
</p>
<p>describing the coupled masses and the vibrating string, respectively. Our approach 
will once again be to find the eigenvectors and eigenvalues of H and to construct 
the propagator U(t) in terms of these. Once we have U(t), we can write 
</p>
<p>llfl(t)) = U(t)l yt(O)) 
</p>
<p>There is no need to make assumptions about I ift(O) &gt; here, since it is determined by 
Eq. (4.3.1): 
</p>
<p>-i 
I ift(O) &gt; =-H] yt(O) &gt; 
</p>
<p>1i 
</p>
<p>In other words, Schrodinger's equation is first order in time, and the specification 
of I lfl &gt; at t = 0 is a sufficient initial-value datum. 
</p>
<p>Let us now construct an explicit expression for U(t) in terms of IE), the normal-
ized eigenkets of H with eigenvalues E which obey 
</p>
<p>H]E)=EIE) (4.3.8) 
</p>
<p>This is called the time-independent Schrodinger equation. Assume that we have solved 
it and found the kets IE). If we expand I yt) as 
</p>
<p>(4.3.9) 
</p>
<p>the equation for aE(t) follows if we act on both sides with (ifi fJjfJt- H): 
</p>
<p>(4.3.10) 
</p>
<p>where we have used the linear independence of the kets IE). The solution to Eq. 
(4.3.10) is 
</p>
<p>( 4.3.1la) 
</p>
<p>145 
</p>
<p>THE POSTULATES 
-A GENERAL 
</p>
<p>DISCUSSION </p>
<p/>
</div>
<div class="page"><p/>
<p>146 
</p>
<p>CHAPTER4 
</p>
<p>or 
</p>
<p>(&pound;11./f(t)) = (&pound;11./f(O)) e-iEtl~ (4.3.llb) 
</p>
<p>so that 
</p>
<p>11./f(t)) =I I E) (&pound;11./f(O)) e -iEt/~ (4.3.12) 
E 
</p>
<p>We can now extract U{t): 
</p>
<p>U(t) =I I E) (&pound;1 e -iEt!~ (4.3.13) 
E 
</p>
<p>We have been assuming that the energy spectrum is discrete and nondegenerate. If 
</p>
<p>E is degenerate, one must first introduce an extra label a (usually the eigenvalue of 
</p>
<p>a compatible observable) to specify the states. In this case 
</p>
<p>U(t)=I I IE, a)(&pound;, al e-iEt;~ 
a E 
</p>
<p>If E is continuous, the sum must be replaced by an integral. The normal modes 
</p>
<p>IE(t)) =IE) e-iEt/~ 
</p>
<p>are also called stationary states for the following reason: the probability distribution 
</p>
<p>P( w) for any variable n is time-independent in such a state: 
</p>
<p>P(w, t) = l(wll!'(t))l 2 
</p>
<p>=I (wl&pound;{t))l 2 
</p>
<p>= l&lt;wl&pound;) e-iEt/~12 
</p>
<p>=I (wl&pound;)1 2 
</p>
<p>=P(w,O) 
</p>
<p>There exists another expression for U(t) besides the sum, Eq. (4.3.13), and 
</p>
<p>that is 
</p>
<p>U(t) = e-iHtl~ (4.3.14) 
</p>
<p>If this exponential series converges (and it sometimes does not), this form of 
</p>
<p>U(t) can be very useful. (Convince yourself that 11./f(t)) = e -iHt/1i IIJI(O)) satisfies 
</p>
<p>Schrodinger's equation.) 
Since H (the energy operator) is Hermitian, it follows that U(t) is unitary. We 
</p>
<p>may therefore think of the time evolution of a ket 11./f(t)) as a "rotation" in Hilbert </p>
<p/>
</div>
<div class="page"><p/>
<p>space. One immediate consequence is that the norm (lf/(t)llfl(t)) is invariant: 
</p>
<p>&lt;'I'( t)i ;y&middot;i t)) = &lt; lf/(0) I U\ t) U(t)ilf/(0)) = &lt; lf/(O)ilf/(0)) (4.3.15) 
</p>
<p>so that a state, once normalized, stays normalized. There are other consequences of 
the fact that the time evolution may be viewed as a rotation. For example, one can 
abandon the fixed basis we have been using, and adopt one that also rotates at the 
same rate as the state vectors. In such a basis the vectors would appear frozen, but 
the operators, which were constant matrices in the fixed basis, would now appear to 
be time dependent. Any physical entity, such as a matrix element, would, however, 
come out the same as before since &lt;4&gt;1!11lf/), which is the dot product of &lt;4&gt;1 and 
l!llfl), is invariant under rotations. This view of quantum mechanics is called the 
Heisenberg picture, while the one we have been using is called the Schrodinger picture. 
Infinitely many pictures are possible, each labeled by how the basis is rotating. So 
if you think you were born too late to make a contribution to quantum theory fear 
not, for you can invent your own picture. We will take up the study of various 
pictures in Chapter 18. 
</p>
<p>Let us now consider the case H = H( t). We no longer look for normal modes, 
since the operator in question is changing with time. There exists no fixed strategy 
for solving such problems. In the course of our study we will encounter a time-
dependent problem involving spin which can be solved exactly. We will also study 
a systematic approximation scheme for solving problems with 
</p>
<p>where H 0 is a large time-independent piece and H 1{t) is a small time-dependent 
piece. 
</p>
<p>What is the propagator U(t) in the time-dependent case? In other words, how 
is U( t) in ilfl(t)) = U(t) 1'1'(0)) related to H(t)? To find out, we divide the interval 
(0- t) into N pieces of width 11 =tIN, where N is very large and 11 is very small. By 
integrating the Schrodinger equation over the first interval, we can write to first order 
in 11 
</p>
<p>which, to this order 
</p>
<p>llfl(/1))=ilfi(0))+/1 dllf/)1 
dt 0 
</p>
<p>i/1 
= llf/(0))- f; H(O)Ilf/(0)) 
</p>
<p>= [I -i: H(O)} lf/(0)) 
</p>
<p>[ -i/1 J = exp ----,;-- H(O) llf/(0)) 
</p>
<p>147 
</p>
<p>THE POSTULATES 
-A GENERAL 
</p>
<p>DISCUSSION </p>
<p/>
</div>
<div class="page"><p/>
<p>148 
</p>
<p>CHAPTER 4 
</p>
<p>[One may wonder whether in the interval 0- A, one must use l-1(0) or !J(A) or 
</p>
<p>H(A/2) and so on. The difference between these possibilities is of order A and hence 
</p>
<p>irrelevant, since there is already one power of A in front of H.] Inching forth in steps 
of A, we get 
</p>
<p>N&middot;&middot;&middot;l 
</p>
<p>IW(l))= [l e-i'.Hiw\J" W(O)) 
!I c~o (} 
</p>
<p>We cannot simply add the exponents to get in the N -&gt;XJ limit, 
</p>
<p>since 
</p>
<p>in general. For example, if 
</p>
<p>H(t)=X 2 cos2 wt+ P 2 sin2 mt 
</p>
<p>then 
</p>
<p>H(O)=X 2 
</p>
<p>and 
</p>
<p>H(rr/2w)=P 2 
</p>
<p>and 
</p>
<p>[H(O), H(rrj2w)] #0 
</p>
<p>It is common to usc the symbol, called the time-ordered integral 
</p>
<p>in such problems. We will not make much use of this form of U(t). But notice that 
</p>
<p>being a product of unitary operators, U(t) is unitary, and time evolution continues 
</p>
<p>to be a "rotation" whether or not H is time independent. </p>
<p/>
</div>
<div class="page"><p/>
<p>Whether or not H depends on time, the propagator satisfies the following 
conditions : 
</p>
<p>U(t3, tz)U(tz, t,)= U(t3, t,) 
</p>
<p>ut(t2 , t,) = u-'(tz, t,) = U(t,, tz) 
(4.3.16) 
</p>
<p>It is intuitively clear that these equations are correct. You can easily prove them by 
applying the U's to some arbitrary state and using the fact that U is unitary and 
U(t, t)=I. 
</p>
<p>Choosing a Basis for Solving Schrodinger's Equation 
</p>
<p>Barring a few exceptions, the Schrodinger equation is always solved in a particu-
lar basis. Although all bases are equal mathematically, some are more equal than 
others. First of all, since H = H(X, P) the X and P bases recommend themselves, for 
in going to one of them the corresponding operator is rendered diagonal. Thus one 
can go to the X basis in which X -+X and P-+-i1i djdx or to the P basis in which 
P-+p and X -+i1i djdp. The choice between the two depends on the Hamiltonian. 
Assuming it is of the form (in one dimension) 
</p>
<p>p2 
H= T+ V=-+ V(X) 
</p>
<p>2m &middot; 
(4.3.17) 
</p>
<p>the choice is dictated by V(X). Since V(X) is usually a more complicated function 
of X than T is of P, one prefers the X basis. Thus if 
</p>
<p>the equation 
</p>
<p>p2 1 
H=-+--
</p>
<p>2m cosh2 X 
</p>
<p>H]E)=EiE) 
</p>
<p>becomes in the X basis the second-order equation 
</p>
<p>( 
1i2 d2 1 ) 
</p>
<p>-- - 2+--2- lf/E(x)=EtJ!E(X) 
2m dx cosh x 
</p>
<p>( 4.3.18) 
</p>
<p>(4.3.19) 
</p>
<p>which can be solved. Had one gone to the P basis, one would have ended up with 
the equation 
</p>
<p>(4.3.20) 
</p>
<p>which is quite frightening. 
</p>
<p>149 
</p>
<p>THE POSTULATES 
-A GENERAL 
</p>
<p>DISCUSSION </p>
<p/>
</div>
<div class="page"><p/>
<p>150 
</p>
<p>CHAPTER 4 
</p>
<p>A problem where the P basis is preferred is that of a particle in a constant force 
</p>
<p>field f, for which 
</p>
<p>p2 
H=--JX 
</p>
<p>2m 
</p>
<p>In the P basis one gets a first-order differential equation 
</p>
<p>L-i"Flf- 'P&pound;(p)=Ev~E(P) ( 
2 d) 
</p>
<p>2m dp 
</p>
<p>whereas in the X basis one gets the second-order equation 
</p>
<p>(4.3.21) 
</p>
<p>(4.3.22) 
</p>
<p>(4.3.23) 
</p>
<p>The harmonic oscillator can be solved with equal ease in either basis since His 
</p>
<p>quadratic in X and P. It turns out to be preferable to solve it in a third basis in 
</p>
<p>which neither X nor P is diagonal! You must wait till Chapter 7 before you see how 
</p>
<p>this happens. 
</p>
<p>There exists a built-in bias in favor of the X basis. This has to do with the fact 
</p>
<p>that the x space is the space we live in. In other words, when we speak of the 
</p>
<p>probability of obtaining a value between x and x + dx if the variable X is measured, 
we mean simply the probability of finding the particle between x and x + dx in our 
space. One may thus visualize lf/(X) as a function in our space, whose modulus 
</p>
<p>squared gives the probability density for finding a particle near x. Such a picture is 
</p>
<p>useful in thinking about the double-slit experiment or the electronic states in a 
</p>
<p>hydrogen atom. 
</p>
<p>But like all pictures, it has its limits. First of all it must be borne in mind that 
</p>
<p>even though ljl(x) can be visualized as a wave in our space, it is not a real wave, 
</p>
<p>like the electromagnetic wave, which carries energy, momentum, etc. To understand 
</p>
<p>this point, consider a particle in three dimensions. The function lf/(X, y, z) can be 
</p>
<p>visualized as a wave in our space. But, if we consider next a two-particle system, 
</p>
<p>ljl(x 1 , y 1 , z1 , x 2 , y 2 , z2 ) is a function in a six-dimensional configuration space and 
</p>
<p>cannot be visualized in our space. 
</p>
<p>Thus the case of the single particle is really an exception: there is only one 
</p>
<p>position operator and the space of its eigenvalues happens to coincide with the space 
</p>
<p>in which we live and in which the drama of physics takes place. 
</p>
<p>This brings us to the end of our general discussion of the postulates. We now turn 
</p>
<p>to the application of quantum theory to various physical problems. For pedagogical 
</p>
<p>reasons, we will restrict ourselves to problems of a single particle in one dimension 
</p>
<p>in the next few chapters. </p>
<p/>
</div>
<div class="page"><p/>
<p>Simple Problems in 
One Dimension 
</p>
<p>5 
</p>
<p>Now that the postulates have been stated and explained, it is all over but for the 
applications. We begin with the simplest class of problems--&lt;:oncerning a single 
particle in one dimension. Although these one-dimensional problems are somewhat 
artificial, they contain most of the features of three-dimensional quantum mechanics 
but little of its complexity. One problem we will not discuss in this chapter is that 
of the harmonic oscillator. This problem is so important that a separate chapter has 
been devoted to its study. 
</p>
<p>5.1. The Free Particle 
</p>
<p>The simplest problem in this family is of course that of the free particle. The 
Schrodinger equation is 
</p>
<p>p2 
ilillfi)=Hilf/)=-1'1') 
</p>
<p>2m 
</p>
<p>The normal modes or stationary states are solutions of the form 
</p>
<p>(5.1.1) 
</p>
<p>(5.1.2) 
</p>
<p>Feeding this into Eq. (5.1.1), we get the time-independent Schrodinger equation 
for IE): 
</p>
<p>p2 
HIE)=-IE)=EIE) 
</p>
<p>2m 
(5.1.3) 
</p>
<p>This problem can be solved without going to any basis. First note that any eigenstate 151 </p>
<p/>
</div>
<div class="page"><p/>
<p>152 
</p>
<p>CHAPTER 5 
</p>
<p>of Pis also an eigenstate of P 2 &bull; So we feed the trial solution jp) into Eq. (5.1.3) 
and find 
</p>
<p>or 
</p>
<p>p2 
-jp)=Eip) 
2m 
</p>
<p>I J \ 
</p>
<p>(P- E}lp)=O 
\2Jn I 
</p>
<p>Since IP) is not a null vector, we find that the allowed values of p are 
</p>
<p>p= &plusmn;(2mE( 2 
</p>
<p>In other words, there are two orthogonal eigenstates for each eigenvalue E: 
</p>
<p>IE+)= jp= (2mE) 12) 
</p>
<p>IE,--- ) = IP = ----(2mE) 
</p>
<p>{5.1.4) 
</p>
<p>(5.1.5) 
</p>
<p>(5.1.6) 
</p>
<p>(5.1.7) 
</p>
<p>Thus, we find that to the eigenvalue E there corresponds a degenerate two-dimen-
</p>
<p>sional eigenspace, spanned by the above vectors. Physically this means that a particle 
of energy E can be moving to the right or to the left with momentum IPI = (2mE) 112 . 
Now, you might say, "This is exactly what happens in classical mechanics. So what's 
</p>
<p>new?" What is new is the fact that the state 
</p>
<p>IE)= f31p= (2rnE) 1 + yjp= -(2mE (5.1.8) 
</p>
<p>is also an eigenstate of energy E and represents a single particle of energy E that can 
be caught moving either to the right or to the left with momentum (2m E) 1/ 2 1 
</p>
<p>To construct the complete orthonormal eigenbasis of H, we must pick from 
</p>
<p>each degenerate eigenspace any two orthonormal vectors. The obvious choice is 
</p>
<p>given by the kets I E. +) and I E. -) themselves. In terms of the ideas discussed in 
the past, we are using the eigenvalue of a compatible variable P as an extra label 
</p>
<p>within the space degenerate with respect to energy. Since P is a nondegenerate 
</p>
<p>operator, the label p by itself is adequate. In other words, there is no need to call 
</p>
<p>the state lp, E= P 2/2m), since the value of E= E(p) follows, given p. We shall 
therefore drop this redundant label. 
</p>
<p>The propagator is then 
</p>
<p>U(t)= f' lr&gt;&lt;rle iEfp)rl'dp 
.., -- :t 
</p>
<p>( 5.1.9) </p>
<p/>
</div>
<div class="page"><p/>
<p>Exercise 5.1.1. Show that Eq. (5.1.9) may be rewritten as an integral over E and a sum 
</p>
<p>over the &plusmn; index as 
</p>
<p>U(t)= I f"'l~:;JIE,a)(E,ale'E' 1 ~dE 
a~&plusmn; 0 (2m&pound;) 
</p>
<p>Exercise 5.1.2. * By solving the eigenvalue equation (5.1.3) in the X basis, regain Eq. 
(5.1.8), i.e., show that the general solution of energy E is 
</p>
<p>'I' (x) = f3 exp[i(2mE) 112 ~(El + y exp[- i(2mE) 112xjli] 
E (2nll) 112 (2n1i) 112 
</p>
<p>[The factor (2nn) 112 is arbitrary and may be absorbed into f3 andy.] Though lfiE{X) 
will satisfy the equation even if E &lt; 0, are these functions in the Hilbert space'/ 
</p>
<p>The propagator U(t) can be evaluated explicitly in the X basis. We start with 
</p>
<p>the matrix element 
</p>
<p>U(x, t; x') =(xi U(t)ix') = rc (xip)(pix') e 'P'' 12 m~ dp 
-oc. 
</p>
<p>= _1_ foc eip(x-x')/~. e-ip't!2m~ dp 
2nfi 
</p>
<p>-ex:&middot; 
</p>
<p>(5.1.10) 
</p>
<p>using the result from Appendix A.2 on Gaussian integrals. In terms of this propa-
</p>
<p>gator, any initial-value problem can be solved, since 
</p>
<p>l/f(X, t) = f U(x, t; x') 1/f(x', 0) dx' (5.1.11) 
</p>
<p>Had we chosen the initial time to be t' rather than zero, we would have gotten 
</p>
<p>1/f(X, t)= f U(x, t; x', t')lj.~x', t')dx' (5.1.12) 
where U(x, t; x', t') =(xi U(t- t')ix'), since U depends only on the time interval t- t' 
and not the absolute values oft and t'. [Had there been a time-dependent potential 
such as V(t) = V0 e-at' in H, we could have told what absolute time it was by looking 
</p>
<p>at V(t). In the absence of anything defining an absolute time in the problem, only 
</p>
<p>time differences have physical significance.] Whenever we set t' = 0, we will resort to 
</p>
<p>our old convention and write U(x, t; x', 0) as simply U(x, t; x'). 
A nice physical interpretation may be given to U(x, t; x', t') by considering a 
</p>
<p>special case of Eq. (5.l.l2). Suppose we started off with a particle localized at 
</p>
<p>153 
</p>
<p>SIMPLE 
</p>
<p>PROBLEMS [N 
</p>
<p>ONE DIMENSION </p>
<p/>
</div>
<div class="page"><p/>
<p>154 
</p>
<p>CHAPTER 5 
</p>
<p>x'=xb, that is, with lf!(x', t')=o(x'- Then 
</p>
<p>lfi(X, t) = U(x, t; x(J, t') (5.1.13) 
</p>
<p>In other words. the propagator (in the X basis) is the amplitude that a particle 
</p>
<p>starting out at the space-time point (xb' n ends with at the space time point (x, l). 
[It can obviously be given such an interpretation in any basis: ( wj U( t, t')j w') is the 
</p>
<p>amplitude that a particle in the state jcv') at t' ends up with in the state jw) at t.] 
</p>
<p>Equation (5.1.12) then tells us that the total amplitude for the particle's arrival at 
</p>
<p>(x, r) is the sum of the contributions from all points x' with a weight proportional 
</p>
<p>to the initial amplitude lfl(x', t') that the particle was at .x' at time t'. One also refers 
to U(x, t; xb, t') as the "fate" of the delta function lf!(x', t') = 8(x' ---- xiJ). 
</p>
<p>Time Evolution of the Gaussian Packet 
</p>
<p>There is an unwritten law which says that the derivation of the free-particle 
propagator be followed by its application to the Gaussian packet. Let us follow this 
</p>
<p>tradition. 
</p>
<p>Consider as the initial wave function the wave packet 
</p>
<p>ljl(x', 0) = (5.1.14) 
</p>
<p>This packet has mean position (X)= 0, with an uncertainty AX= 2, and mean 
</p>
<p>momentum p 0 with uncertainty ti/2 1 2/\. By combining Eqs. ( 5.1.1 0) and (5.1.12) we 
</p>
<p>get 
</p>
<p>lipo (/ Pot.'), J xexp- x-----
ti , 2m 
</p>
<p>(5.1.15) 
</p>
<p>The corresponding probability density is 
</p>
<p>(5.1.16) 
</p>
<p>The main features of this result are as follows: 
</p>
<p>(I) The mean position of the particles is 
</p>
<p>p0 1 (P)t 
</p>
<p>m m </p>
<p/>
</div>
<div class="page"><p/>
<p>In other words, the classical relation x = (p jm)t now holds between average quanti-
</p>
<p>ties. Thi~ is just one of the consequences of the Ehrenfest theorem which states 
</p>
<p>that the classical equations obeyed by dynamical variables will have counterparts in 
quantum mechanics as relations among expectation values. The theorem will be 
</p>
<p>proved in the next chapter. 
(2) The width of the packet grows as follows: 
</p>
<p>(5.1.17) 
</p>
<p>The increasing uncertainty in position is a reflection of the fact that any uncertainty 
</p>
<p>in the initial velocity (that is to say, the momentum) will be reflected with passing 
</p>
<p>time as a growing uncertainty in position. In the present case, since Ll. V(O) = LI.P(O)! 
m = fi/i 12mtl., the uncertainty in X grows approximately as LI.X ~ fit/2 1i2mtl. which 
agrees with Eq. (5.1.17) for large times. Although we are able to understand the 
</p>
<p>spreading of the wave packet in classical tenns, the fact that the initial spread !\ V(O) 
is unavoidable (given that we wish to specify the position to an accuracy A) is a 
</p>
<p>purely quantum mechanical feature. 
</p>
<p>If the particle in question were macroscopic, say of mass 1 g, and we wished to 
fix its initial position to within a proton width, which is approximately I 0 13 em, the 
</p>
<p>uncertainty in velocity would be 
</p>
<p>11 -14 
L\V(O) ~-&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;~ 10 em/sec 
</p>
<p>2I2mtl. , 
</p>
<p>It would be over 300,000 years before the uncertainty L\(t) grew to 1 millimeter' We 
</p>
<p>may therefore treat a macroscopic particle classically for any reasonable length of 
</p>
<p>time. This and similar questions will be taken up in greater detail in the next chapter. 
</p>
<p>Exercise 5.1.3 (Another Way to Do rhe Gaussian Problem). We have seen that there exists 
</p>
<p>another formula for U(t), namely, U(t) = e iHt;~_ For a free particle this becomes 
</p>
<p>[ 
i (fi2/ d 2 \ J x. J I ifit)" d 2" 
</p>
<p>U(t)=exp &middot;&middot;&middot;&middot; &middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot; ::,) = 2.: J 
11 2m dx I n d) 11 . \2m 
</p>
<p>Consider the initial state in Eq. ( 5.1.14) with p0 = 0, and set A= 1, t' = 0: 
</p>
<p>1/f(X,0)=--1-,4 
(ir) . 
</p>
<p>Find 1/f(x, t) using Eq. (5.1.18) above and compare with Eq. (5.1.15). 
</p>
<p>Hints: ( 1) Write lf!(X, 0) as a power series: 
</p>
<p>Y ( ])" .'n 
</p>
<p>!f!(x,O)=(n)- 114 ~ - x 
n~o n 1(2)" 
</p>
<p>(5.1.18) 
</p>
<p>155 
</p>
<p>SIMPLE 
</p>
<p>PROBLEMS IN 
</p>
<p>ONE DIMENSION </p>
<p/>
</div>
<div class="page"><p/>
<p>156 
</p>
<p>CHAPTER 5 
</p>
<p>(2) Find the action of a few terms 
</p>
<p>etc., on this power series. 
</p>
<p>(3) Collect terms with the same power of x. 
</p>
<p>( 4) Look for the following series expansion in the coetiicient of x 2": 
</p>
<p>=1-(n+ 1.-2) +&middot; 
m 
</p>
<p>(5) Juggle around till you get the answer. 
</p>
<p>Exercise 5.1.4: A Famous Counrerexarnple. Consider the wave function 
</p>
<p>vr(x. 0) =sin 
L 
</p>
<p>xi :SL/2 
</p>
<p>=0. I xi&gt; L. 2 
</p>
<p>It is clear that when this function is differentiated any number of times we get another function 
</p>
<p>confined to the intervallxl sL2. Consequently the action of 
</p>
<p>on this function is to give a function confined to lxl-:s L &middot; 2. What about the spreading of the 
</p>
<p>wave packetry 
</p>
<p>[Answer: Consider the derivatives at the boundary. We have here an example where the 
</p>
<p>(exponential) operator power series doesn't converge. Notice that the convergence of an 
</p>
<p>operator power series depends not just on the operator but also on the operand. So there is 
</p>
<p>no paradox: if the function dies abruptly as above. so that there seems to be a paradox, the 
</p>
<p>derivatives are singular at the boundary, while if it falls off continuously. the function will 
</p>
<p>definitely leak out given enough time. no matter how rapid the falloff.] 
</p>
<p>Some General Features of Energy Eigenfunctions 
</p>
<p>Consider now the energy eigenfunctions in some potential V(x). These obey 
</p>
<p>... 2m(E- V) 
If! =- lfl 
</p>
<p>where each prime denotes a spatial derivative. Let us ask what the continuity of 
</p>
<p>V(x) implies. Let us start at some point x0 where lfl and lfl' have the values lfi(Xo) 
</p>
<p>and lJI'(x0). If we pretend that xis a time variable and that lJI is a particle coordinate, 
</p>
<p>the problem of finding lJI everywhere else is like finding the trajectory of a particle 
</p>
<p>(for all times past and future) given its position and velocity at some time and its 
</p>
<p>acceleration as a function of its position and time. It is clear that if we integrate </p>
<p/>
</div>
<div class="page"><p/>
<p>CD v CD 
a) b) 
</p>
<p>rr m 
</p>
<p>'------'---_J- X 
</p>
<p>-L/2 0 L/2 -L/2 0 
</p>
<p>Figure 5.1. (a) The box potential. (b) The first two levels and wave functions in the box. 
</p>
<p>these equations we will get continuous J.i!'(x) and J.i!(x). This is the typical situation. 
</p>
<p>There are, however, some problems where, for mathematical simplicity, we consider 
</p>
<p>potentials that change abruptly at some point. This means that J.il" jumps abruptly 
</p>
<p>there. However, J.il' will still be continuous, for the area under a function is continuous 
</p>
<p>even if the function jumps a bit. What if the change in Vis infinitely large? It means 
</p>
<p>that J.i!" is also infinitely large. This in turn means that J.il' can change abruptly as 
</p>
<p>we cross this point, for the area under J.il" can be finite over an infinitesimal region 
</p>
<p>that surrounds this point. But whether or not J.il' is continuous, J.il, which is the area 
</p>
<p>under it, will be continuous.t 
</p>
<p>Let us turn our attention to some specific cases. 
</p>
<p>5.2. The Particle in a Box 
</p>
<p>We now consider our first problem with a potential, albeit a rather artificial 
</p>
<p>one: 
</p>
<p>V(x)=O, 
</p>
<p>=oo, 
</p>
<p>lxl &lt;L/2 
</p>
<p>lxl ~L/2 (5.2.1) 
</p>
<p>This potential (Fig. 5.la) is called the box since there is an infinite potential barrier 
</p>
<p>in the way of a particle that tries to leave the region lxl &lt; L/2. The eigenvalue 
equation in the X basis (which is the only viable choice) is 
</p>
<p>(5.2.2) 
</p>
<p>We begin by partitioning space into three regions I, II, and III (Fig. 5.1a). The 
</p>
<p>solution J.i1 is called "'', "'", and 'I'm in regions I, II, and III, respectively. 
Consider first region III, in which V= oo. It is convenient to first consider the 
</p>
<p>case where Vis not infinite but equal to some V0 which is greater than E. Now 
</p>
<p>t We are assuming that the jump in 1/f' is finite. This will be true even in the artificial potentials we will 
encounter. But can you think of a potential for which this is not true? (Think delta.) 
</p>
<p>157 
</p>
<p>SIMPLE 
</p>
<p>PROBLEMS IN 
</p>
<p>ONE DIMENSION </p>
<p/>
</div>
<div class="page"><p/>
<p>158 
</p>
<p>CHAPTER 5 
</p>
<p>Eq. (5.2.2) becomes 
</p>
<p>(5.2.3) 
</p>
<p>which is solved by 
</p>
<p>{5.2.4) 
</p>
<p>where /(=[2m( V0 - E)jti2f 12&bull; 
Although A and B are arbitrary coefficients from a mathematical standpoint, 
</p>
<p>we must set B = 0 on physical grounds since B e"'x blows up exponentially as x-t oo 
and such functions are not members of our Hilbert space. If we now let V -too, we 
see that 
</p>
<p>It can similarly be shown that lj/1=0. In region II, since V=O, the solutions are 
exactly those of a free particle: 
</p>
<p>(5.2.5) 
</p>
<p>=A eikx+Be-ikx, (5.2.6) 
</p>
<p>It therefore appears that the energy eigenvalues are once again continuous as in the 
free-particle case. This is not so, for lj/11(x) = 1f1 only in region II and not in all of 
space. We must require that lf/u goes continuously into its counterparts lf/r and lf/m 
as we cross over to regions I and Ill, respectively. In other words we require that 
</p>
<p>lf/1(-L/2) = lf/n(- L/2) = 0 (5.2.7) 
</p>
<p>'I'm(+ L/2) = lf/u( + L/2) = 0 (5.2.8) 
</p>
<p>(We make no such continuity demands on lf/1 at the walls of the box since V 
jumps to infinity there.) These constraints applied to Eq. (5.2.6) take the form 
</p>
<p>A e -ikL/2 + B /kLil = 0 (5.2.9a) 
</p>
<p>A eikL/2 + B e -ikL/2 = 0 (5.2.9b) 
</p>
<p>or in matrix form 
</p>
<p>ikL/
2 J[AJ=[OJ 
</p>
<p>e-,kL/2 B 0 
(5.2.10) </p>
<p/>
</div>
<div class="page"><p/>
<p>Such an equation has nontrivial solutions only if the determinant vanishes: 
</p>
<p>that is, only if 
</p>
<p>k=mr 
L' 
</p>
<p>(5.2.11) 
</p>
<p>n=O, &plusmn; 1, &plusmn;2, ... (5.2.12) 
</p>
<p>To find the corresponding eigenfunctions, we go to Eqs. (5.2.9a) and (5.2.9b). Since 
only one of them is independent, we study just Eq. (5.2.9a), which says 
</p>
<p>A e-imt;2+ B eimtf2=0 (5.2.13) 
</p>
<p>Multiplying by ei"n12 , we get 
</p>
<p>A=-ein"B (5.2.14) 
</p>
<p>Since einrr = ( -1r, Eq. (5.2.6) generates two families of solutions (normalized to 
unity): 
</p>
<p>( )
1/2 ( ) 2 . mrx 
</p>
<p>'l'n(x)= L sm L, neven (5.2.15) 
</p>
<p>n odd (5.2.16) 
</p>
<p>Notice that the case n=O is uninteresting since 'l'o=O. Further, since 'l'n= '1'-n 
for n odd and 'I' n = -'I' -n for n even, and since eigenfunctions differing by an overall 
factor are not considered distinct, we may restrict ourselves to positive nonzero n. 
In summary, we have 
</p>
<p>n=1,3,5,7, ... (5.2.17a) 
</p>
<p>n=2, 4, 6, ... (5.2.17b) 
</p>
<p>and from Eqs. (5.2.6) and (5.2.12), 
</p>
<p>(5.2.17c) 
</p>
<p>[It is tacitly understood in Eqs. (5.2.17a) and (5.2.17b) that lxl &lt;L/2.] 
</p>
<p>159 
</p>
<p>SIMPLE 
</p>
<p>PROBLEMS IN 
</p>
<p>ONE DIMENSION </p>
<p/>
</div>
<div class="page"><p/>
<p>160 
</p>
<p>CHAPTER 5 
</p>
<p>We have here our first encounter with the quantization of a dynamical variable. 
</p>
<p>Both the variables considered so far, X and P, had a continuous spectrum of eigenval-
</p>
<p>ues from -oo to +oo, which coincided with the allowed values in classical mechanics. 
</p>
<p>In fact, so did the spectrum of the Hamiltonian in the free-particle case. The particle 
</p>
<p>in the box is the simplest example of a situation that will be encountered again 
</p>
<p>and again, wherein Schrodinger's equation, combined with appropriate boundary 
</p>
<p>conditions, leads to the quantization of energy. These solutions are also examples 
</p>
<p>of bound states, namely, states in which a potential prevents a particle from escaping 
</p>
<p>to infinity. Bound states are thus characterized by 
</p>
<p>lf!(X)-----&gt; 0 
lxl~x 
</p>
<p>Bound states appear in quantum mechanics exactly where we expect them classically, 
</p>
<p>namely, in situations where V( &plusmn; oo) is greater than E. 
</p>
<p>The energy levels of bound states are always quantized. Let us gain some insight 
</p>
<p>into how this happens. In the problem of the particle in a box, quantization resulted 
</p>
<p>from the requirement that lf/11 completed an integral number of half-cycles within 
</p>
<p>the box so that it smoothly joined its counterpa;-ts ljl1 and lf/111 which vanished 
</p>
<p>identically. Consider next a particle bound by a finite well, i.e., by a potential that 
</p>
<p>jumps from 0 to V0 at lxl =L/2. We have already seen [Eq. (5.2.4)] that in the 
</p>
<p>classically forbidden region(&pound;&lt; V0 , lxl "?.L/2) lfl is a sum of rising and falling expo-
</p>
<p>nentials (as I xl-&gt; oo) and that we must choose the coefficient of the rising exponential 
</p>
<p>to be zero to get an admissible solution. In the classically allowed region (lxl ~ 
</p>
<p>L/2) lfl is a sum 6f a sine and cosine. Since Vis everywhere finite, we demand that 
</p>
<p>lfl and lf!' be continuous at x = &plusmn;L/2. Thus we impose four conditions on lf!, which 
</p>
<p>has only three free parameters. (It may seem that there are four-the coefficients of 
</p>
<p>the two falling exponentials, the sine, and the cosine. However, the overall scale of 
</p>
<p>lfl is irrelevant both in the eigenvalue equation and the continuity conditions, these 
</p>
<p>being linear in lfl and lf!'. Thus if say, lf!' does not satisfy the continuity condition 
</p>
<p>at x=L/2, an overall rescaling of lfl and lf!' will not help.) Clearly, the continuity 
</p>
<p>conditions cannot be fulfilled except possibly at certain special energies. (See Exercise 
</p>
<p>5.2.6 for details). This is the origin of energy quantization here. 
</p>
<p>Consider now a general potential V(x) which tends to limits V&plusmn; as x-&gt;&plusmn;oo and 
</p>
<p>which binds a particle of energy E (less than both V&plusmn;)&middot; We argue once again that 
</p>
<p>we have one more constraint than we have parameters, as follows. Let us divide 
</p>
<p>space into tiny intervals such that in each interval V(x) is essentially constant. As 
</p>
<p>x-&gt;&plusmn;oo, these intervals can be made longer and longer since Vis stabilizing at its 
</p>
<p>asymptotic values V&plusmn;. The right- and leftmost intervals can be made infinitely wide, 
</p>
<p>since by assumption Vhas a definite limit as x-&gt;&plusmn;oo. Now in all the finite intervals, 
</p>
<p>lfl has two parameters: these will be the coefficients of the sine/cosine if&pound;&gt; V or 
</p>
<p>growing/falling exponential if&pound;&lt; V. (The rising exponential is not disallowed, since 
</p>
<p>it doesn't blow up within the finite intervals.) Only in the left- and rightmost intervals 
</p>
<p>does lfl have just one parameter, for in these infinite intervals, the growing exponential 
</p>
<p>can blow up. All these parameters are constrained by the continuity of lfl and lf!' at 
</p>
<p>each interface between adjacent regions. To see that we have one more constraint 
</p>
<p>than we have parameters, observe that every extra interval brings with it two free 
</p>
<p>parameters and one new interface, i.e., two new constraints. Thus as we go from </p>
<p/>
</div>
<div class="page"><p/>
<p>three intervals in the finite well to the infinite number of intervals in the arbitrary 
</p>
<p>potential, the constraints are always one more than the free parameters. Thus only 
</p>
<p>at special energies can we expect an allowed solution. 
</p>
<p>[Later we will study the oscillator potential, V=!mro 2x2, which grows without 
</p>
<p>limit as lxl-+oo. How do we understand energy quantization here? Clearly, any 
</p>
<p>allowed lfl will vanish even more rapidly than before as lxl-+ oo, since V- E, instead 
</p>
<p>of being a constant, grows quadratically, so that the particle is "even more forbidden 
</p>
<p>than before" from escaping to infinity. If E is an allowed energy,t we expect lfl to 
</p>
<p>fall off rapidly as we cross the classical turning points x0 = &plusmn; (2Efmro2) 112&bull; To a 
</p>
<p>particle in such a state, it shouldn't matter if we flatten out the potential to some 
</p>
<p>constant at distances much greater than lxol, i.e., the allowed levels and eigen-
</p>
<p>functions must be the same in the two potentials which differ only in a region that 
</p>
<p>the particle is&middot; so strongly inhibited from going to. Since the flattened-out potential 
</p>
<p>has the asymptotic behavior we discussed earlier, we can understand energy quantiza-
</p>
<p>tion as we did before.] 
Let us restate the origin of energy quantization in another way. Consider the 
</p>
<p>search for acceptable energy eigenfunctions, taking the finite well as an example. If 
</p>
<p>we start with some arbitrary values l{l(x0 ) and lfl'(x0 ), at some point x 0 to the right 
</p>
<p>of the well, we can integrate Schrodinger's equation numerically. (Recall the analogy 
</p>
<p>with the problem of finding the trajectory of a particle given its initial position and 
</p>
<p>velocity and the force on it.) As we integrate out to x-+oo, lfl will surely blow up 
</p>
<p>since lf/m contains a growing exponential. Since l{l(x0 ) merely fixes the overall scale, 
</p>
<p>-&middot;we vary lfl' (xo) until the growing exponential is killed. [Since we can solve the problem 
</p>
<p>analytically in region III, we can even say what the desired value of lfl'(x0 ) is: it is 
</p>
<p>given by lfl'(xo}=-Klfi(Xo). Verify, starting with Eq. (5.2.4), that this implies B= 
</p>
<p>0.] We are now out of the fix as x-+oo, but we are committed to whatever comes 
</p>
<p>out as we integrate to the left of x0 &bull; We wiil find that lfl grows exponentially till we 
</p>
<p>reach the well, whereupon it will oscillate. When we cross the well, lfl will again start 
</p>
<p>to grow exponentially, for lf/r also contains a growing exponential in general. Thus 
</p>
<p>there will be no acceptable solution at some randomly chosen energy. It can, however, 
</p>
<p>happen that for certain values of energy, lfl will be exponentially damped in both 
</p>
<p>regions I and III. [At any point x(, in region I, there is a ratio lfl'(xb)/l{l(xb) for which 
</p>
<p>only the damped exponential survives. The lfl we get integrating from region III will 
</p>
<p>not generally have this feature. At special energies, however, this can happen.] These 
</p>
<p>are the allowed energies and the corresponding functions are the allowed eigen-
</p>
<p>functions. Having found them, we can choose l{l(x0 ) such that they are normalized 
</p>
<p>to unity. For a nice numerical analysis of this problem see the book by Eisberg and 
</p>
<p>Resnick.&sect; 
</p>
<p>It is clear how these arguments generalize to a particle bound by some arbitrary 
</p>
<p>potential: if we try to keep lfl exponentially damped as x-+-oo, it blows up as x-+oo 
</p>
<p>(and vice versa), except at some special energies. It is also clear why there is no 
</p>
<p>quantization of energy for unbound states: since the particle is classically allowed 
</p>
<p>at infinity, lfl oscillates there and so we have two more parameters, one from each 
</p>
<p>end (why?), and so two solutions (normalizable to 8(0)) at any energy. 
</p>
<p>t We are not assuming E is quantized. 
&sect; R. Eisberg and R. Resnick, Quantum Physics of Atoms, Molecules, Solids, Nuclei and Particles, Wiley, 
</p>
<p>New York (1974). See Section 5.7 and Appendix F. 
</p>
<p>161 
</p>
<p>SIMPLE 
</p>
<p>PROBLEMS IN 
</p>
<p>ONE DIMENSION </p>
<p/>
</div>
<div class="page"><p/>
<p>162 
</p>
<p>CHAPTER 5 
</p>
<p>Let us now return to the problem of the particle in a box and discuss the fact 
that the lowest energy is not zero (as it would be classically, corresponding to the 
particle at rest inside the well) but 112rr 2/2mL2&bull; TP.e reason behind it is the uncertainty 
principle, which prevents the particle, whose position (and hence M) is bounded 
by lxl5.L/2, from having a well-defined momentum of zero. This in turn leads to a 
lower bound on the energy, which we derive as follows. We begin witht 
</p>
<p>so that 
</p>
<p>p2 
H=-
</p>
<p>2m 
</p>
<p>(H)= (P2) 
</p>
<p>2m 
</p>
<p>(5.2.18) 
</p>
<p>(5.2.19) 
</p>
<p>Now (P) = 0 in any bound state for the following reason. Since a bound state is a 
stationary state, (P) is time independent. If this (P) ;60, the particle must (in the 
average sense) drift either to the right or to the left and eventually escape to infinity, 
which cannot happen in a bound state. 
</p>
<p>Consequently we may rewrite Eq. (5.2.19) as 
</p>
<p>(H)= ((P- (P) )2) (!1P)2 
</p>
<p>2m 2m 
</p>
<p>If we now use the uncertainty relation 
</p>
<p>L1P &middot; Mzli/2 
</p>
<p>we find 
</p>
<p>Since the variable x is constrained by -Lj25.x5.L/2, its standard deviation M 
cannot exceed L/2. Consequently 
</p>
<p>In an energy eigenstate, (H)= E so that 
</p>
<p>(5.2.20) 
</p>
<p>The actual ground-state energy &pound; 1 happens to be rr2 times as large as the lower 
</p>
<p>t We are suppressing the infinite potential due to the walls of the box. Instead we will restrict x to the 
range lxi&lt;:;,L/2. </p>
<p/>
</div>
<div class="page"><p/>
<p>bound. The uncertainty principle is often used in this fashion to provide a quick 
</p>
<p>order-of-magnitude estimate for the ground-state energy. 
</p>
<p>If we denote by In) the abstract ket corresponding to lj/nCr), we can write the 
</p>
<p>propagator as 
</p>
<p>(5.2.21) 
</p>
<p>The matrix elements of U(l) in the X basis are then 
</p>
<p>&lt;xl U(t)lx') = U(x, t; x') 
</p>
<p>(5.2.22) 
</p>
<p>Unlike in the free-particle case, there exists no simple closed expression for this sum. 
</p>
<p>Exercise 5.2.1. * A particle is in the ground state of a box of length L. Suddenly the box 
expands (symmetrically) to twice its size, leaving the wave function undisturbed. Show that 
</p>
<p>the probability of finding the particle in the ground state of the new box is (8/37!')2 &bull; 
</p>
<p>Exercise 5.2.2. * (a) Show that for any normalized lift), &lt; llfl HI ty)?. E0 , where &pound; 0 is the 
lowest-energy eigenvalue. (Hint: Expand I ty) in the eigenbasis of H.) 
</p>
<p>(b) Prove the following theorem: Every attractive potential in one dimension has at least 
</p>
<p>one bound state. Hint: Since Vis attractive, if we define V( x) = 0, it follows that V(x) = 
-1 V(x)l for all x. To show that there exists a bound state with E&lt;O, consider 
</p>
<p>1 )1 4 
!lfa(X) =(0 
</p>
<p>,rr: 
</p>
<p>and calculate 
</p>
<p>IJ= I V(x)l 
</p>
<p>Show that E(a) can be made negative by a suitable choice of a. The d:s;red result follows 
</p>
<p>from the application of the theorem proved above. 
</p>
<p>Exercise 5.2.3. * Consider V(x) =-a V0 8(x). Show that it admits a bound state of energy 
E = -ma2 Vi /211 2 . Are there any other bound states? Hint: Solve Schri:idinger's equation out-
</p>
<p>side the potential for E &lt; 0, and keep only the solution that has the right behavior at infinity 
</p>
<p>and is continuous at x = 0. Draw the wave function and see how there is a cusp, or a discontinu-
ous change of slope at x = 0. Calculate the change in slope and equate it to 
</p>
<p>(where c is infinitesimal) determined from Schri:idinger's equation. 
</p>
<p>163 
</p>
<p>SIMPLE 
</p>
<p>PROBLEMS IN 
ONE DIMENSION </p>
<p/>
</div>
<div class="page"><p/>
<p>164 
</p>
<p>CHAPTER 5 
</p>
<p>Exercise 5. 2.4. Consider a particle of mass m in the state In) of a box of length L. Find 
</p>
<p>the force F= -iJEjoL encountered when the walls are slowly pushed in, assuming the particle 
</p>
<p>remains in the nth state of the box as its size changes. Consider a classical particle of energy 
</p>
<p>En in this box. Find its velocity, the frequency of collision on a given wall, the momentum 
</p>
<p>transfer per collision, and hence the average force. Compare it to -iJEjoL computed above. 
</p>
<p>Exercise 5.2.5. * If the box extends from x = 0 to L (instead of - L/2 to L/2) show that 
lfln(x)=(2/L) 112 sin(mrxjL), n= I, 2, ... , oo and En=Fhr2n2/2mL2 . 
</p>
<p>Exercise 5.2.6. * Square Well Potential. Consider a particle in a square well potential: 
</p>
<p>V(x)= {
0, 
</p>
<p>Vo, 
</p>
<p>lxl ;S,a 
</p>
<p>lxl~a 
</p>
<p>Since when V0 -.oc:, we have a box, let us guess what the lowering of the walls does to the 
</p>
<p>states. First of all, all the bound states (which alone we are interested in), will have E-5, V0 . 
</p>
<p>Second, the wave functions of the low-lying levels will look like those of the particle in a box, 
</p>
<p>with the obvious difference that if! will not vanish at the walls but instead spill out with an 
</p>
<p>exponential tail. The eigenfunctions will still be even, odd, even, etc. 
</p>
<p>(I) Show that the even solutions have energies that satisfy the transcendental equation 
</p>
<p>k tanka= K (5.2.23) 
</p>
<p>while the odd ones will have energies that satisfy 
</p>
<p>k cot ka= -K (5.2.24) 
</p>
<p>where kandiK are the real and complex wave numbers inside and outside the well, respectively. 
</p>
<p>Note that k and K are related by 
</p>
<p>(5.2.25) 
</p>
<p>Verify that as V0 tends to oo, we regain the levels in the box. 
</p>
<p>(2) Equations (5.2.23) and (5.2.24) must be solved graphically. In the (a= ka, f3 = Ka) 
plane, imagine a circle that obeys Eq. (5.2.25). The bound states are then given by the 
</p>
<p>intersection of the curve a tan a= f3 or a cot a= -13 with the circle. (Remember a and f3 are 
positive.) 
</p>
<p>(3) Show that there is always one even solution and that there is no odd solution unless 
</p>
<p>V0 ~ Pi2 11: 2 j8ma2 &bull; What is E when V0 just meets this requirement? Note that the general result 
</p>
<p>from Exercise 5.2.2b holds. 
</p>
<p>5.3. The Continuity Equation for Probability 
</p>
<p>We interrupt our discussion of one-dimensional problems to get acquainted with 
</p>
<p>two concepts that will be used in the subsequent discussions, namely, those of the 
</p>
<p>probability current density and the continuity equation it satisfies. Since the probability 
</p>
<p>current concept will also be used in three-dimensional problems, we discuss here a 
</p>
<p>particle in three dimensions. </p>
<p/>
</div>
<div class="page"><p/>
<p>As a prelude to our study of the continuity equation in quantum mechanics, let 
</p>
<p>us recall the analogous equation from electromagnetism. We know in this case that 
</p>
<p>the total charge in the universe is a constant, that is 
</p>
<p>Q(t) =const, independent of timet (5.3.1) 
</p>
<p>This is an example of a global conservation law, for it refers to the total charge 
</p>
<p>in the universe. But charge is also conserved locally, a fact usually expressed in the 
</p>
<p>form of the continuity equation 
</p>
<p>op(r, t) . 
- .. &middot;&middot;&middot;&middot;&middot;=-V&middot;J 
</p>
<p>at 
(5.3.2) 
</p>
<p>where p and j are the charge and current densities, respectively. By integrating this 
equation over a volume V bounded by a surface Sv we get, upon invoking Gauss's 
</p>
<p>law, 
</p>
<p>(5.3.3) 
</p>
<p>This equation states that any decrease in charge in the volume Vis accounted for 
</p>
<p>by the flow of charge out of it, that is to say, charge is not created or destroyed in 
</p>
<p>any volume. 
</p>
<p>The continuity equation forbids certain processes that obey global conservation, 
</p>
<p>such as the sudden disappearance of charge from one region of space and its immedi-
</p>
<p>ate reappearance in another. 
</p>
<p>In quantum mechanics the quantity that is globally conserved is the total prob-
</p>
<p>ability for finding the particle anywhere in the universe. We get this result by 
</p>
<p>expressing the in variance of the norm in the coordinate basis: since 
</p>
<p>then 
</p>
<p>( lf!(t)ilfl(t)) = ( lf/(0)1 ut (t) U(t)lw(O)) = ( llf(O)Ilf/(0)) 
</p>
<p>const = ( lfl{t)llfl(t)) =I I I ( lfl{t)lx, y, z)(x, y, zilfl(t)) dx dy dzt 
</p>
<p>=III (lf/(l)lr)(rllfl(t)) d 3r 
</p>
<p>=I J J lf/*(r, t) lf!(r, t) d 3r 
</p>
<p>=Iff P(r, t) d 3r 
</p>
<p>t The range of integration will frequently be suppressed when obvious. 
</p>
<p>(5.3.4) 
</p>
<p>165 
</p>
<p>SIMPLE 
</p>
<p>PROBLEMS IN 
</p>
<p>ONE DIMENSION </p>
<p/>
</div>
<div class="page"><p/>
<p>166 
</p>
<p>CHAPTER 5 
</p>
<p>This global-conservation law is the analog ofEq. (5.3.1). To get the analog of 
</p>
<p>Eq. (5.3.2), we turn to the Schrodinger equation 
</p>
<p>(5.3.5) 
</p>
<p>and its conjugate 
</p>
<p>(5.3.6) 
</p>
<p>Note that V has to be real if His to be Hermitian. Multiplying the first of these 
</p>
<p>equations by If/*, the second by If/, and taking the difference, we get 
</p>
<p>cP 
-= -V&middot;j 
ct 
</p>
<p>(5.3.7) 
</p>
<p>where 
</p>
<p>( 5.3.8) 
</p>
<p>is the probability current density, that is to say, the probability flow per unit time 
</p>
<p>per unit area perpendicular to i- To regain the global conservation law, we integrate 
Eq. (5.3.7) over all space: 
</p>
<p>(5.3.9) 
</p>
<p>where Soc is the sphere at infinity. For (typical) wave functions which are normaliz-
</p>
<p>able to unity, r 312 lfl-&gt;0 as r-&gt;oo in order that S l!f*lflr2 dr dO. is bounded, and the 
surface integral of j on Sex_ vanishes. The case of momentum eigenfunctions that do 
</p>
<p>not vanish on S~ is considered in one of the following exercises. 
</p>
<p>Exercise 5.3.1. Consider the case where V= V,-iV,, where the imaginary part V; is a 
</p>
<p>constant. Is the Hamiltonian Hermitian? Go through the derivation of the continuity equation 
</p>
<p>and show that the total probability for finding the particle decreases exponentially as 
</p>
<p>e-zv,t;R_ Such complex potentials are used to describe processes in which particles are absorbed 
</p>
<p>by a sink. </p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 5.2. The single-step potential. The dotted 
</p>
<p>line shows a more realistic potential idealized by 
</p>
<p>the step. which is mathematically convenient. The 
</p>
<p>total energy E and potential energy V are 
</p>
<p>measured along the y axis. 
</p>
<p>v 
</p>
<p>J rr '' 
----..L....-!:_-0------------&middot;_j_x 
</p>
<p>I 
</p>
<p>Exercise 5.3.2. Convince yourself that if 'I'= ciif, where c is constant (real or complex) 
</p>
<p>and iif is real, the corresponding j vanishes. 
</p>
<p>Exercise 5.3.3. Consider 
</p>
<p>Find j and P and compare the relation between them to the electromagnetic equation j = pv, 
v being the velocity. Since p and j are constant, note that the continuity Eq. (5.3.7) is trivially 
</p>
<p>satisfied. 
</p>
<p>Exercise 5.3.4.* Consider lji=A e'px1r.+Be&middot;--ipx;r, in one dimension. Show that)= 
</p>
<p>(IA1 2 -1 Bl 2)pjm. The absence of cross terms between the right- and left-moving pieces in 1/f 
allows us to associate the two parts of j with corresponding parts of II'&middot; 
</p>
<p>Ensemble Interpretation of j 
</p>
<p>Recall that j &middot; dS is the rate at which probability flows past the area dS. If we 
consider an ensemble of N particles all in some state !f1(r, t), then Nj &middot; dS particles 
</p>
<p>will trigger a particle detector of area dS per second, assuming that N tends to 
</p>
<p>infinity and that j is the current associated with !f1(r, t). 
</p>
<p>5.4. The Single-Step Potential: A Problem in Scattering~ 
</p>
<p>Consider the step potential (Fig. 5.2) 
</p>
<p>V(x)=O x &lt; 0 (region I) 
</p>
<p>x &gt; 0 (region H) (5.4.1) 
</p>
<p>Such an abrupt change in potential is rather unrealistic but mathematically 
</p>
<p>convenient. A more realistic transition is shown by dotted lines in the figure. 
</p>
<p>Imagine now that a classical particle of energy E is shot in from the left (region 
I) toward the step. One expects that if E&gt; V0 , the particle would climb the barrier 
and travel on to region II, while if E&lt; V0 , it would get reflected. We now compare 
this classical situation with its quantum counterpart. 
</p>
<p>:t This rather difficult section may be postponed till the reader has gone through Chapter 7 and gained 
more experience with the subject. It is for the reader or the instructor to decide which way to go. 
</p>
<p>167 
</p>
<p>SIMPLE 
</p>
<p>PROBLEMS IN 
</p>
<p>ONE DIMENSION </p>
<p/>
</div>
<div class="page"><p/>
<p>168 
</p>
<p>CHAPTER 5 
</p>
<p>a 
</p>
<p>t&raquo;&gt;a/(p0 /m) 
</p>
<p>-
Vo 
</p>
<p>~-&middot;&middot;&middot; 
</p>
<p>............................. J 
</p>
<p>Figure 5.3. A schematic description of 
</p>
<p>the wave function long before and long 
</p>
<p>after it hits the step. The area under 
</p>
<p>1'1'1 !' is unity. The areas under l~tRI' 
and I &bull; respectively. are the prob-
abilities for reflection and transmis-
</p>
<p>sion. 
</p>
<p>First of all, we must consider an initial state that is compatible with quantum 
</p>
<p>principles. We replace the incident particle possessing a well-defined trajectory with 
</p>
<p>a wave packed Though the detailed wave function will be seen to be irrelevant 
</p>
<p>in the limit we will consider, we start with a Gaussian, which is easy to handle 
</p>
<p>analytically&sect;: 
</p>
<p>(5.4.2) 
</p>
<p>This packet has a mean momentum p0 = 1ik0 , a mean position 
</p>
<p>take to be far away from the step), with uncertainties 
</p>
<p>= -a (which we 
</p>
<p>~ 
~X= ~P= 
</p>
<p>We shall be interested in the case of large ~. where the particle has essentially well-
</p>
<p>defined momentum tik0 and energy Eo"'" tek6/2m. We first consider the case &pound; 0 &gt; Ji(1 . 
After a time t~a[p 0 /m] 1 , the packet will hit the step and in general break into 
</p>
<p>two packets: lflfl, the reflected packet, and lj.lr, the transmitted packet (Fig. 5.3). 
</p>
<p>The area under llfl 11 ' 2 at large t is the probability of finding the particle in region I 
in the distant future, that is to say, the probability of reflection. Likewise the area 
</p>
<p>under I V'rl 2 at large tis the probability of transmission. Our problem is to calculate 
the reflection coefficient 
</p>
<p>t-&gt; :X (5.4.3) 
</p>
<p>and transmission coefficient 
</p>
<p>T= J ! lflrl~ dx, (-&gt; cc (5.4.4) 
Generally Rand Twill depend on the detailed shape of the initial wave function. 
</p>
<p>If, however, we go to the limit in which the initial momentum is well defined (i.e., 
</p>
<p>t A wave packet is any wave functton w1th reasonably well-defined position and momentum. 
&sect;This is just the wave packet in Eq. (5.1.14). displaced by an amount -a. </p>
<p/>
</div>
<div class="page"><p/>
<p>when the Gaussian in x space has infinite width), we expect the answer to depend 
</p>
<p>only on the initial energy, it being the only characteristic of the state. In the following 
</p>
<p>analysis we will assume that I!J.X = I!J./2 112 is large and that the wave function in k 
</p>
<p>space is very sharply peaked near k 0 &bull; 
</p>
<p>We follow the standard procedure for finding the fate of the incident wave 
</p>
<p>packet, lf/1 : 
</p>
<p>Step 1 : Solve for the normalized eigenfunction of the step potential Hamiltonian, 
</p>
<p>'I' E{x). 
</p>
<p>Step 2: Find the projection a(E)=&lt;'I'Eilf/J). 
</p>
<p>Step 3: Append to each coefficient a(E) a time dependence e-iEt/~ and get lf/(X, t) 
</p>
<p>at any future time. 
</p>
<p>Step 4: Identify lf/R and 'l'rin lfi(X, t--&gt;oo) and determineR and Tusing Eqs. (5.4.3) 
</p>
<p>and (5.4.4). 
</p>
<p>Step 1. In region I, as V=O, the (unnormalized) solution is the familiar one: 
</p>
<p>(5.4.5) 
</p>
<p>In region II, we simply replace E byE- V0 [see Eq. (5.2.2)], 
</p>
<p>k = 2m(E- V0) [ ]
1;2 
</p>
<p>2 '112 
( 5.4.6) 
</p>
<p>(We consider only E&gt; V0 ; the eigenfunction withE&lt; V0 will be orthogonal to lf/1 as 
</p>
<p>will be shown on the next two pages.) Of interest to us are eigenfunctions with D = 
</p>
<p>0, since we want only a transmitted (right-going) wave in region II, and incident 
</p>
<p>plus reflected waves in region I. If we now impose the continuity of 'I' and its 
</p>
<p>derivative at x = 0; we get 
</p>
<p>A+B=C (5.4.7) 
</p>
<p>(5.4.8) 
</p>
<p>In anticipation of future use, we solve these equations to express B and C in terms 
</p>
<p>of A: 
</p>
<p>B- I 2 A- 0 A (k
 -k) (Et/2_(&pound;- V.)l/2) 
</p>
<p>kt +k2 &pound; 112 + (E- V0) 1; 2 
(5.4.9) 
</p>
<p>(5.4.10) 
</p>
<p>169 
</p>
<p>SIMPLE 
</p>
<p>PROBLEMS IN 
</p>
<p>ONE DIMENSION </p>
<p/>
</div>
<div class="page"><p/>
<p>170 
</p>
<p>CHAPTER 5 
</p>
<p>Note that if Vo=O, B=O and C=A as expected. The solution with energy E is then 
</p>
<p>where 
</p>
<p>O(x) = 1 
</p>
<p>=0 
</p>
<p>ifx&gt;O 
</p>
<p>ifx&lt;O 
</p>
<p>(5.4.11) 
</p>
<p>Since to each E there is a unique k1 = + (2mE/1i2)u, we can label the eigenstates by 
k1 instead of E. Eliminating k2 in favor of k1, we get 
</p>
<p>lf/k, (x) =A[ (exp(ik1 x) +~ exp(- ik1 x) )e(- x) 
</p>
<p>+~ exp[i(kT- 2m V0 /1i 2) 112x]O(x) J ( 5.4.12) 
</p>
<p>Although the overall scale factor A is generally arbitrary (and the physics depends 
only on B/ A and C/ A), here we must choose A= (21Z')- 112 because lf/k has to be 
properly normalized in the four-step procedure outlined above. We shall verify 
shortly that A= (21Z')- 112 is the correct normalization factor. 
</p>
<p>Step 2. Consider next 
</p>
<p>(5.4.13) 
</p>
<p>The second integral vanishes (to an excellent approximation) since lf/I(x) is nonvan-
ishing far to the left of x = 0, while O(x) is non vanishing only for x &gt; 0. Similarly 
the second piece of the first integral also vanishes since lf/I in k space is peaked 
around k = + k0 and is orthogonal to (left-going) negative momentum states. [We 
can ignore the 8(-x) factor in Eq. (5.4.13) since it equals I where lf/I(x)#O.] So 
</p>
<p>( 
2)1 14 _ L\ . .-(k1 -k0)2ll,2;2 ilqa -- e e 
</p>
<p>1Z' 
(5.4.14) </p>
<p/>
</div>
<div class="page"><p/>
<p>is just the Fourier transform of lf/1 . Notice that for large A, a(kJ) is very sharply 
peaked at k1 =k0 &bull; This justifies our neglect of eigenfunctions withE&lt; Vo, for these 
correspond to k 1 not near ko . 
</p>
<p>Step 3. The wave function at any future time t is 
</p>
<p>lfi(X, t)= f"" a(ki) e-iE(kJ)t!~ lf/k1(x) dk1 
-xo 
</p>
<p>(5.4.15) 
</p>
<p>x {eik]xO( -x) + (~) e -;klxO(-x) 
</p>
<p>+ (&sect;) exp[i(kf- 2m Vo/1i2 ) 112x]e(x)} dk 1 (5.4.16) 
</p>
<p>You can convince yourse1fthat if we set t= 0 above we regain 1f11 (x), which corrobor-
ates our choice A=(2tr)-1/2&bull; 
</p>
<p>Step 4. Consider the first of the three terms. If 0(-x) were absent, we would 
be propagating the original Gaussian. After replacing x by x+a in Eq. (5.1.15), and 
inserting the 0( -x) factor, the first term of lfi(X, t) is 
</p>
<p>0(-x)tr-l/4(A+i1it)-l/2 ex [-(x+a-1ikot/m)2] 
m p 2A2(1 + i1it/mA2) 
</p>
<p>x exp[ik0 (x+a- ~~t) ]= 0( -x)G(-a, k0 , t) (5.4.17) 
</p>
<p>Since the Gaussian G( -a, k2 , t) is centered at x =-a+ 1ik0 t/m ~ 1ik0t/m as t-+oo, 
and 0(-x) vanishes for x&gt;O, the product OG vanishes. Thus the initial packet has 
disappeared and in its place are the reflected and transmitted packets given by the 
next two terms. In the middle term if we replace B/ A, which is a function of k1, by 
its value (B/A) 0 at k1 =k0 (because a(kJ) is very sharply peaked at k 1 =k0 ) and pull 
it out of the integral, changing the dummy variable from k 1 to -k1 , it is easy to see 
that apart from the factor (B/ A)0 0( -x) up front, the middle term represents the 
free propagation of a normalized Gaussian packet that was originally peaked at x = 
+a and began drifting to the left with mean momentum -1ik0 . Thus 
</p>
<p>'fin= 0(-x)G(a, -k0 , t)(B/A)0 (5.4.18) 
</p>
<p>171 
</p>
<p>SIMPLE 
</p>
<p>PROBLEMS IN 
</p>
<p>ONE DIMENSION </p>
<p/>
</div>
<div class="page"><p/>
<p>172 
</p>
<p>CHAPTER 5 
</p>
<p>As t-&gt;oo, we can set 0(-&middot;&middot;&middot;x) equal to I, since G is centered at x=a-11k 0 tjm~ 
</p>
<p>-11k0 t/m. Since the Gaussian G has unit norm, we get from Eqs. (5.4.3) and (5.4.9), 
</p>
<p>where 
</p>
<p>(5.4.19) 
</p>
<p>This formula is exact only when the incident packet has a well-defined energy E0 , 
</p>
<p>that is to say, when the width of the incident Gaussian tends to infinity. But it is an 
</p>
<p>excellent approximation for any wave packet that is narrowly peaked in momentum 
space. 
</p>
<p>To find T, we can try to evaluate the third piece. But there is no need to do so, 
since we know that 
</p>
<p>R+T= I (5.4.20) 
</p>
<p>which follows from the global conservation of probability. It then follows that 
</p>
<p>(5.4.2l) 
</p>
<p>By inspecting Eqs. (5.4.19) and (5.4.21) we see that both R and Tare readily 
</p>
<p>expressed in terms of the ratios (B/ A)0 and ( C/ A)o and a kinematical factor, 
</p>
<p>(E0 - V0) 112 /Eri'2 . Is there some way by which we can directly get to Eqs. (5.4.19) 
</p>
<p>and (5.4.21 ), which describe the dynamic phenomenon of scattering, from Eqs. 
</p>
<p>(5.4.9) and (5.4.10), which describe the static solution to Schri:idinger's equation? 
</p>
<p>Yes. 
</p>
<p>Consider the unnormalized eigenstate 
</p>
<p>lf/k0(X) = [Ao exp(ikox) + Bo exp( ikox)]O( ~~x) 
</p>
<p>. , 2mVo l. \1/2 J + Co exp r( ko- --~ 2 ) x __ 8(x) (5.4.22) 
The incoming plane wave A 
</p>
<p>to 
</p>
<p>has a probability current associated with it equal 
</p>
<p>, fiko 
.h= IAol~ ---
</p>
<p>m 
(5.4.23) </p>
<p/>
</div>
<div class="page"><p/>
<p>while the currents associated with the reflected and transmitted pieces are 
</p>
<p>and 
</p>
<p>. 2 'liko 
]R=iBoi --
</p>
<p>m 
</p>
<p>. 2 1i(k~-2m 
]T= I Col 
</p>
<p>m 
</p>
<p>(5.4.24) 
</p>
<p>(5.4.25) 
</p>
<p>(Recall Exercise 5.3.4, which provides the justification for viewing the two parts of 
</p>
<p>the j in region I as being due to the incident and reflected wave functions.) In terms 
</p>
<p>of these currents 
</p>
<p>(5.4.26) 
</p>
<p>and 
</p>
<p>(5.4.27) 
</p>
<p>Let us now enquire as to why it is that R and Tare calculable in these two 
</p>
<p>ways. Recall that R and T were exact only for the incident packet whose momentum 
was well defined and equal to 1ik0 . From Eq. (5.4.2) we see that this involves taking 
</p>
<p>the width of the Gaussian to infinity. As the incident Gaussian gets wider and wider 
(we ignore now the Ll- 112 factor up front and the normalization) the following things 
</p>
<p>happen: 
</p>
<p>( 1) It becomes impossible to say when it hits the step, for it has spread out to be a 
right-going plane wave in region I. 
</p>
<p>(2) The reflected packet also gets infinitely wide and coexists with the incident one, 
as a left-going plane wave. 
</p>
<p>(3) The transmitted packet becomes a plane wave with wave number 
(k6-2mV0 /1i 2 ) 112 in region II. 
</p>
<p>In other words, the dynamic picture of an incident packet hitting the step and 
</p>
<p>disintegrating into two becomes the steady-state process described by the eigenfunc-
tion Eq. (5.4.22). We cannot, however, find R and T by calculating areas under 
</p>
<p>Ill' T 12 and Ill' Rl 2 since all the areas are infinite, the wave packets having been trans-
formed into plane waves. We find instead that the ratios of the probability currents 
associated with the incident, reflected, and transmitted waves give us R and T. The 
equivalence between the wave packet and static descriptions that we were able to 
demonstrate in this simple case happens to be valid for any potential. When we come 
to scattering in three dimensions, we will assume that the equivalence of the two 
approaches holds. 
</p>
<p>173 
</p>
<p>SIMPLE 
</p>
<p>PROBLEMS IN 
</p>
<p>ONE DIMENSION </p>
<p/>
</div>
<div class="page"><p/>
<p>174 
</p>
<p>CHAPTER 5 
</p>
<p>Exercise 5.4.1 (Quite liard). Evaluate the third piece in Eq. (5.4 !6) and compare the 
</p>
<p>resulting Twith Eq. (5.4.21 ). [Hint: Expand the factor (k;! ----2m V0 ;fi2 ) 1 2 near k 1 =k0 , keeping 
</p>
<p>just the first derivative in the Taylor series.] 
</p>
<p>Before we go on to examine some of the novel features of the reflection and 
</p>
<p>transmission coefficients, let us ask how they are used in practice. Consider a general 
</p>
<p>problem with some V(.x), which tends to constants V+ and V. as .x--&gt; &plusmn; ::r. For 
</p>
<p>simplicity we take V.c = 0. Imagine an accelerator located to the far left (x-&gt; - cc) 
</p>
<p>which shoots out a beam of nearly monoenergetic particles with (P) = nk0 toward 
the potential. The question one asks in practice is what fraction of the particles will 
</p>
<p>get transmitted and what fraction will get reflected to x= -w, respectively. In gen-
</p>
<p>eral, the question cannot be answered because we know only the mean momenta of 
</p>
<p>the particles and not their individual wave functions. But the preceding analysis 
</p>
<p>shows that as long as 1he wave packets are loca!i:::ed sharply in momentum space, the 
</p>
<p>reflection and transmission probabilities (Rand T) depend only on the mean momentum 
</p>
<p>and not the detailed shape of the wave jimctions. So the answer to the question raised 
</p>
<p>above is that a fraction R(k0 ) will get reflected and a fraction T(ko) =I - R(ko) will 
</p>
<p>get transmitted. To find R and T we solve for the time-independent eigenfunctions 
</p>
<p>of H = T + V with energy eigenvalue E0 = l(k~j2m, and asymptotic behavior 
</p>
<p>and obtain from it R=!Bj..-!j and T=IC/AI~. Solutions with this asymptotic 
behavior (namely, free-pa .:de behavior) will always exist provided V vanishes rap-
</p>
<p>idly enough as lxi-&gt;CJJ. [Later we will see that this means lxV(x)l-&gt;0 as lxl-&gt;ccc.] 
</p>
<p>The general solution will also contain a piece D exp( -ikox) as x----&bull; CJJ, but we set 
</p>
<p>D = 0 here, for if a exp(ikox) is to be identified with the incident wave, it must only 
</p>
<p>produce a right-moving transmitted wave C e'kox as x----&bull; CJJ. 
</p>
<p>Let us turn to Eqs. (5.4.19) and (5.4.21) for Rand T. These contain many 
</p>
<p>nonclassical features. First of all we find that an incident particle with Eo&gt; Vo gets 
</p>
<p>reflected some of the time. It can also be shown that a particle with Eo&gt; Vo incident 
</p>
<p>from the right will also get reflected some of the time, contrary to classical 
</p>
<p>expectations. 
</p>
<p>Consider next the case Eo&lt; V0 &bull; Classically one expects the particle to be reflected 
</p>
<p>at x = 0, and never to get to region II. This is not so quantum mechanically. In 
</p>
<p>region II, the solution to 
</p>
<p>with E0 &lt; Vo is 
</p>
<p>lf/ll(x)=Ce (5.4.28) </p>
<p/>
</div>
<div class="page"><p/>
<p>(The growing exponential en does not belong to the physical Hilbert space.) Thus 
there is a finite probability for finding the particle in the region where its kinetic 
energy Eo- V0 is negative. There is, however, no steady flow of probability current 
into region II, since 1/fn (x) = Cifi, where tii is real. This is also corroborated by the 
fact the reflection coefficient in this case is 
</p>
<p>R- 0 0 0 - 0 II( - I I(E)I/2_(E-V.)I/212 lk-"12 
- (E0 ) 112 +(Eo- Vo) 112 - ko+iK -
</p>
<p>(5.4.29) 
</p>
<p>The fact that the particle can penetrate into the classically forbidden region leads 
to an interesting quantum phenomenon called tunneling. Consider a modification of 
Fig. 5.2, in which V= V0 only between x=O and L (region II) and is once again 
zero beyonci x=L (region III). If now a plane wave is incident on this barrier from 
the left with E &lt; V0 , there is an exponentially small probability for the particle to 
get to region III. Once a particle gets to region III, it is free once more and described 
by a plane wave. An example of tunneling is that of a particles trapped in the nuclei 
by a barrier. Every once in a while an a particle manages to penetrate the barrier 
and come out. The rate for this process can be calculated given V0 and L. 
</p>
<p>Exercise 5.4.2. (a)* Calculate Rand T for scattering of a potential V(x) = Voa8(x). (b) 
Do the same for the case V=O for lxl &gt;a and V= V0 for lxl &lt;a. Assume that the energy is 
positive but less than Vo. 
</p>
<p>Exercise 5.4.3. Consider a particle subject to a constant force fin one dimension. Solve 
for the propagator in momentum space and get 
</p>
<p>U(p, t;p', 0) = 8(p-p'-ft) ei(p '-p'&gt;l 6 m~f (5.4.30) 
</p>
<p>Transform back to coordinate space and obtain 
</p>
<p>U(x, t; x', 0)= --. exp - - ) +- ft(x+x')--( m )
112 
</p>
<p>{ i [m(x x' 2 1 f 2t3]} 
2n1ilt 1i 2t 2 24m 
</p>
<p>(5.4.31) 
</p>
<p>[Hint: Normalize '1/E(P) such that (EIE')=8(E-E). Note that E is not restricted to be 
positive.] 
</p>
<p>5.5. The Double-Slit Experiment 
</p>
<p>Having learned so much quantum mechanics, it now behooves us to go back 
and understand the double-slit experiment (Fig. 3.1). Let us label by I and II the 
regions to the left and right of the screen. The incident particle, which must really 
be represented by a wave packet, we approximate by a plane wave of wave number 
k=pjfz. The impermeable screen we treat as a region with V= oo, and hence the 
region of vanishing llf. Standard wave theory (which we can borrow from classical 
electromagnetism) tells us what happens in region II: the two slits act as sources of 
radially outgoing waves of the same wavelength. These two waves interfere on the 
</p>
<p>175 
</p>
<p>SIMPLE 
PROBLEMS IN 
</p>
<p>ONE DIMENSION </p>
<p/>
</div>
<div class="page"><p/>
<p>176 
</p>
<p>CHAPTER 5 
</p>
<p>line AB and produce the interference pattern. We now return to quantum mechanics 
</p>
<p>and interpret the intensity llJfl 2 as the probability density for finding the particle. 
</p>
<p>5.6. Some Theorems 
</p>
<p>Theorem 15. There is no degeneracy in one-dimensional bound states. 
</p>
<p>Proof Let lf1 1 and ljf2 be two solutions with the same eigenvalue E: 
</p>
<p>(5.6.1) 
</p>
<p>(5.6.2) 
</p>
<p>Multiply the first by lj/ 2 , the second by ljf 1 and subtract, to get 
</p>
<p>or 
</p>
<p>so that 
</p>
<p>dljf2 dlfll 
lj/1 ............ ~ l.f/2 &middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot; =c 
</p>
<p>dx dx 
(5.6.3) 
</p>
<p>To find the constant c, go to lxi&middot;-HYJ, where l.f/1 and lfl2 vanish, since they describe 
bound states by assumption.t Tt follows that c = 0. So 
</p>
<p>log 1f11 =log 1f12+d (dis a constant) 
</p>
<p>( 5.6.4) 
</p>
<p>t The theorem holds even if 'I' vanishes at either +co or --w. In a bound state it vanishes at both ends. 
But one can think of situations where the potential confines the wave function at one end hut not the 
</p>
<p>other. </p>
<p/>
</div>
<div class="page"><p/>
<p>Thus the two eigenfunctions differ only by a scale factor and represent the same 
state. Q.E.D. 
</p>
<p>What about the free-particle case, where to every energy there are two degenerate 
solutions with p = &plusmn; (2mEjli2) 112? The theorem doesn't apply here since lf/p(x) does 
not vanish at spatial infinity. [Calculate c in Eq. (5.6.3).] 
</p>
<p>Theorem 16. The eigenfunctions of H can always be chosen pure real in the 
coordinate basis. 
</p>
<p>Proof If 
</p>
<p>then by conjugation 
</p>
<p>Thus lfln and If/! are eigenfunctions with the same eigenvalue. It follows that the real 
and imaginary parts of "'n' 
</p>
<p>and 
</p>
<p>are also eigenfunctions with energy E. Q.E.D. 
The theorem holds in higher dimensions as well for Hamiltonians of the above 
</p>
<p>form, which in addition to being Hermitian, are real. Note, however, that while 
Hermiticity is preserved under a unitary change of basis, reality is not. 
</p>
<p>If the problem involves a magnetic field, the Hamiltonian is no longer real in 
the coordinate basis, as is clear from Eq. (4.3.7). In this case the eigenfunctions 
cannot be generally chosen real. This question will be explored further at the end of 
Chapter 11. 
</p>
<p>Returning to one dimension, due to nondegeneracy of bound states, we must 
have 
</p>
<p>lfl;= Clfln c, a constant 
</p>
<p>177 
</p>
<p>SIMPLE 
PROBLEMS IN 
</p>
<p>ONE DIMENSION </p>
<p/>
</div>
<div class="page"><p/>
<p>178 
</p>
<p>CHAPTER 5 
</p>
<p>Consequently, 
</p>
<p>Since the overall scale c is irrelevant, we can ignore it, i.e., work with real eigen-
functions with no loss of generality. 
</p>
<p>This brings us to the end of our study of one-dimensional problems, except for 
</p>
<p>the harmonic oscillator, which is the subject of Chapter 7. </p>
<p/>
</div>
<div class="page"><p/>
<p>6 
</p>
<p>The Classical Limit 
</p>
<p>It is intuitively clear that when quantum mechanics is applied to a macroscopic 
</p>
<p>system it should reproduce the results of classical mechanics, very much the way that 
relativistic dynamics, when applied to slowly moving (v/c&laquo;.l) objects, reproduces 
Newtonian dynamics. In this chapter we examine how classical mechanics is regained 
from quantum mechanics in the appropriate domain. When we speak of regaining 
</p>
<p>classical mechanics, we refer to the numerical aspects. Qualitatively we know that 
the deterministic world of classical mechanics does not exist. Once we have bitten 
the quantum apple, our loss of innocence is permanent. 
</p>
<p>We commence by examining the time evolution of the expectation values. We 
find 
</p>
<p>d d 
dt (Q)=dt('l'ir!i'l') 
</p>
<p>= &lt;vilr!l ~p) + &lt; ~p!r!llJi&gt; + &lt; wllil v,&gt;; (6.1) 
</p>
<p>In what follows we will assume that n has no explicit time dependence. We will 
therefore drop the third term ('I'IQI VJ). From the Schrodinger equation, we get 
</p>
<p>and from its adjoint, 
</p>
<p>i 
(vii = ( tftl H 
</p>
<p>1i 
</p>
<p>t If you are uncomfortable differentiating bras and kets, work in a ~.-'";is and convince yourself that this 
step is correct. 179 </p>
<p/>
</div>
<div class="page"><p/>
<p>180 
</p>
<p>CHAPTER 6 
</p>
<p>Feeding these into Eq. ( 6.1) we get the relation 
</p>
<p>d (' -i)' (0)= &middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot; &lt;llll[n,H]IIII&gt; 
dr J1 
</p>
<p>(6.2) 
</p>
<p>which is called Ehrenfest's theorem. 
</p>
<p>Notice the structural similarity between this equation and the corresponding 
</p>
<p>one from classical mechanics: 
</p>
<p>dro 
</p>
<p>dt 
(6.3) 
</p>
<p>We continue our investigation to see how exactly the two mechanics are related. Let 
</p>
<p>us, for simplicity, discuss a particle in one dimension. If we consider n =X we get 
</p>
<p>If we assume 
</p>
<p>then 
</p>
<p>Now 
</p>
<p>so that 
</p>
<p>p2 
H= + V(X) 
</p>
<p>2m 
</p>
<p>(,Y) = (&middot;~i) ([X, P 2 /2m]) 
</p>
<p>[.\:", P 2] = P[X, P] +[X, P]P 
</p>
<p>= 2inP 
</p>
<p>. (P'&gt; 
(X)= / 
</p>
<p>m 
</p>
<p>[from Eq. (1.5.10)] 
</p>
<p>(6.4) 
</p>
<p>(6.5) </p>
<p/>
</div>
<div class="page"><p/>
<p>The relation x=plm of classical mechanics now appears as a relation among the 
mean values. We can convert Eq. (6.5) to a more suggestive form by writing 
</p>
<p>P aH 
</p>
<p>m oP 
</p>
<p>where oH I ap is a formal derivative of II with respect to P, calculated by pretending 
that H, P, and X are just c numbers. The rule for finding such derivatives is just as 
in calculus, as long as the function being differentiated has a power series, as in this 
</p>
<p>case. We now get, in the place of Eq. (6.5), 
</p>
<p>Consider next 
</p>
<p>. jaH) 
&lt;X&gt;=\aP 
</p>
<p>. I 
(P)=-:- ([P, H]) 
</p>
<p>di 
</p>
<p>=_1_ ([P, V(X)]) 
ill 
</p>
<p>To find [P, V(X)] we go to the X basis, in which 
</p>
<p>and for any 1/f(X), 
</p>
<p>d 
p_,- ill- and V(X)-" V(x) 
</p>
<p>dx 
</p>
<p>[ d J dV -ill-, V(x) lfl(x) = -ill- lf!(x) 
dx dx 
</p>
<p>We conclude that in the abstract, 
</p>
<p>dV 
[P, V(X)] =- iti 
</p>
<p>dX 
</p>
<p>where dV I dX is again a formal derivative. Since dVI dX = aH I ax, we get 
</p>
<p>. \ DH) (P)= - .......... . 
a.:r 
</p>
<p>(6.6) 
</p>
<p>(6.7) 
</p>
<p>{6.8) 
</p>
<p>The similarity between Eqs. (6.6) and (6.8) and Hamilton's equations is rather strik-
ing. We would like to see how the quantum equations reduce to Hamilton's equations 
when applied to a macroscopic particle (of mass 1 g, say). 
</p>
<p>181 
</p>
<p>THE CLASSICAL 
</p>
<p>LIMIT </p>
<p/>
</div>
<div class="page"><p/>
<p>182 
</p>
<p>CHAPTER 6 
</p>
<p>First of ali, it is clear that we must consider an initial state that resembles the 
</p>
<p>states of classical mechanics, i.e., states with well-defined position and momentum. 
</p>
<p>Although simultaneous eigenstates of X and P do not exist, there do exist states 
</p>
<p>which we can think of as approximate eigenstates of both X and P. In these states, 
</p>
<p>labeled lxopoli), (X)=xo and (P)=po, with uncertainties M=ti and L1P::.:::fi/L1, 
</p>
<p>both of which are small in the macroscopic scale. A concrete example of such a state 
is 
</p>
<p>I \ 1,."4 
</p>
<p>lxopoli) -&gt;'P,u.ru.A = ( ~) 
JrA 1 
</p>
<p>(6.9) 
</p>
<p>If we choose L1"" 10- 13 em. say, which is the size of a proton, AP ~I 0 14 gem/sec. 
</p>
<p>For a particle of mass 1 g, this implies L1 V"" 10- 14 em/ sec, an uncertainty far below 
</p>
<p>the experimentally detectable range. In the classical scale, such a state can be said 
</p>
<p>to have well-defined values for X and P, namely, x 0 and p0 , since the uncertainties 
</p>
<p>(fluctuations) around these values are truly negligible. If we let such a state evolve 
</p>
<p>with time, the mean values xo(t) and po{t) will follow Hamilton's equations, once 
</p>
<p>again with negligible deviations. We establish this result as follows. 
</p>
<p>Consider Eqs. (6.6) and (6.8) which govern the evolution of (X)= x0 and (P) = 
p0 . These would reduce to Hamilton's equations if we could replace the mean values 
</p>
<p>of the functions on the right-hand side by the functions of the mean values: 
</p>
<p>( 6.10) 
</p>
<p>and 
</p>
<p>( 6.11) 
</p>
<p>If we consider some function of X and P, we will find in the same approximation 
</p>
<p>( 6.12) 
</p>
<p>Thus we regain classical physics as a good approximation whenever it is a good 
</p>
<p>approximation to replace the mean of the functions iJH/iJP, &middot;&middot;-aH/cX, and Q(X, P) 
</p>
<p>by the functions of the mean. 'fhis in turn requires that the fluctuations about the 
mean have to be smalL (The result is exact &pound;(there are no fluctuations.) Take as a 
concrete example Eqs. ( 6.10) and ( 6. 11). There is no approximation involved in the 
</p>
<p>first equation since (aH/iJP) is just (P/m)=p0/m. In the second one, we need to 
approximate (iJH/iJX)=(dV/dX)=(V'(X)) by V'(X=x0). To see when this is a 
good approximation, let us expand V' in a Taylor series around x 0 &bull; Here it is 
</p>
<p>convenient to work in the coordinate basis where V(X) = V(x). The series is </p>
<p/>
</div>
<div class="page"><p/>
<p>Let us now take the mean of both sides. The first term on the right-hand side, which 
alone we keep in our approximation, corresponds to the classical force at xo, and 
thus reproduces Newton's second law. The second vanishes in all cases, since the 
mean of x- x0 does. The succeeding terms, which are corrections to the classical 
approximation, represent the fact that unlike the classical particle, which responds 
only to the force F=- V' at x0 , the quantum particle responds to the force at 
neighboring points as well. (Note, incidentally, that these terms are zero if the poten-
tial is at the most quadratic in the variable x.) Each of these terms is a product of 
two factors, one of which measures the size or nonlocality of the wave packet and 
the other, the variation of the force with x. (See the third term for example.) At an 
intuitive level, we may say that these terms are negligible if the force varies very little 
over the "size" of the wave packet. (There is no unique definition of "size." The 
uncertainty is one measure. We see above that the uncertainty squared has to be 
much smaller than the inverse of the second derivative of the force.) In the present 
case, where the size of the packet is of the order of 10-13 em, it is clear that the 
classical approximation is good for any potential that varies appreciably only over 
macroscopic scales. 
</p>
<p>There is one apparent problem: although we may start the system out in a state 
with .1~ 10- 13 em, which is certainly a very small uncertainty, we know that with 
passing time the wave packet will spread. The uncertainty in the particle's position 
will inevitably become macroscopic. True. But recall the arguments of Section 5.1. 
We saw that the spreading of the wave packet can be attributed to the fact that any 
initial uncertainty in velocity, however small, will eventually manifest itself as a giant 
uncertainty in position. But in the present case (L1V~I0- 14 cmjsec) it would take 
300,000 years before the packet is even a millimeter across! (It is here that we invoke 
the fact that the particle is macroscopic: but for this, a small L1P would not imply 
a small L1 V.) The problem is thus of academic interest only; and besides, it exists in 
classical mechanics as well, since the perfect measurement of velocity is merely an 
idealization. 
</p>
<p>There remains yet another question. We saw that for a macroscopic particle pre-
pared in a state lx0p0L1), the time evolution of x0 and p0 will be in accordance with 
Hamilton's equations. Question: While it is true that a particle in such a conveniently 
prepared state obeys classical mechanics, are these the only states one encounters in 
classical mechanics? What if the initial position of the macroscopic particle is fixed 
to an accuracy of 10-27 em? Doesn't its velocity now have uncertainties that are 
classically detectable? Yes. But such states do not occur in practice. The classical 
physicist talks about making exact position measurements, but never does so in 
practice. This is clear from the fact that he uses light of a finite frequency to locate 
the particle's positions, while only light of infinite frequency has perfect resolution. 
For example light in the visible spectrum has a wavelength of A.~ 10-5 em and thus 
the minimum AX is ~ 10-5 em. If one really went towards the classical ideal and 
used photons of decreasing wavelength, one would soon find that the momentum of 
the macroscopic particle is affected by the act of measuring its position. For example, 
by the time one gets to a wavelength of 10-27 em, each photon would carry a momen-
tum of approximately 1 g cmjsec and one would see macroscopic objects recoiling 
under their impact. 
</p>
<p>In summary then, a typical macroscopic particle, described classically as possess-
ing a well-defined value of x and p, is in reality an approximate eigenstate lx0poi1), 
</p>
<p>183 
</p>
<p>THE CLASSICAL 
LIMIT </p>
<p/>
</div>
<div class="page"><p/>
<p>184 
</p>
<p>CHAPTER 6 
</p>
<p>where.'\ is at least w-s em if visible light is used to locate the particle. The quantum 
equations for the time evolution of these approximate eigenvalues xo and Po reduce 
to Hamilton's equations, up to truly negligible uncertainties. The same goes for any 
</p>
<p>other dynamical variable dependent on x and p. 
We conclude this chapter by repeating an earlier observation to underscore its 
</p>
<p>importance. Ehrenfest's theorem does not tell ~s that, in general, the expectation 
values of quantum operators evolve as do their classical counterparts. In particular, 
</p>
<p>(X)=x0 and (P)=p 0 do not obey Hamilton's equations in all problems. For them 
</p>
<p>to obey Hamilton's equations, we must be able to replace the mean values (expecta-
</p>
<p>tion values) of the functions uHjuP and DH/DX of X and P by the corresponding 
</p>
<p>functions of the mean values (X)= x0 and (P) =Po. For Hamiltonians that are at 
the most quadratic in X and P, this replacement can be done with no error for all 
</p>
<p>wave functions. In the general case, such a replacement is a poor approximation 
</p>
<p>unless the fluctuations about the means x0 and p0 are small. Even in those cases 
</p>
<p>where x0 and p0 obey classical equations, the expectation value of some dependent 
variable Q(X, P) need not, unless we can replace (Q(X, P)) by Q( (X), (P)) = 
</p>
<p>w(xo, Po). 
</p>
<p>Example 6.1. Consider (Q(X)&gt;, where Q=X 2 , in a state given by ij!(x)= 
</p>
<p>A exp[- (x- a) 2 /2~ 2 ]. Is (Q(X)&gt; = Q((X) )? No, for the difference between the two 
is (X 2)--(X)2 =(A.&yen;l#O. </p>
<p/>
</div>
<div class="page"><p/>
<p>7 
</p>
<p>The Harmonic Oscillator 
</p>
<p>7.1. Why Study the Harmonic Oscillator? 
</p>
<p>In this section I will put the hamwnic oscillator in its place-on a pedestaL Not 
</p>
<p>only is it a system that can be exactly solved (in classical and quantum theory) and 
</p>
<p>a superb pedagogical tool (which will be repeatedly exploited in this text), but it is 
</p>
<p>also a system of great physical relevance. As will be shown below, any system fluctu-
</p>
<p>ating by small amounts near a configuration of stable equilibrium may be described 
</p>
<p>either by an oscillator or by a collection of decoupled harmonic oscillators. Since 
</p>
<p>the dynamics of a collection of noninteracting oscillators is no more complicated 
</p>
<p>than that of a single oscillator (apart from the obvious N-fold increase in degrees 
</p>
<p>of freedom), in addressing the problem of the oscillator we are actually confronting 
</p>
<p>the general problem of small oscillations near equilibrium of an arbitrary system. 
</p>
<p>A concrete example of a single harmonic oscillator is a mass m coupled to a 
spring of force constant k. For small deformations x, the spring will exert the force 
</p>
<p>given by Hooke's law, F= -kx, (k being its force constant) and produce a potential 
</p>
<p>V= ~kx 2 &bull; The Hamiltonian for this system is 
</p>
<p>. . , p2 I . " 2 
&pound;= T+ v =-+-mw x 
</p>
<p>2m 2 
(7.1.1) 
</p>
<p>where w = (k/m) 1/ 2 is the classical frequency of oscillation. Any H::tmiltonian of the 
above form, quadratic in the coordinate and momentum, will be called the harmonic 
</p>
<p>oscillator Hamiltonian. Now, the mass-spring system is just one among the following 
</p>
<p>family of systems described by the oscillator Hamiltonian. Consider a particle moving 
</p>
<p>in a potential V(x). If the particle is placed at one of its minima x0 , it will remain 
</p>
<p>there in a state of stable, static equilibrium. (A maximum, which is a point of unstable 
static equilibrium, will not interest us here.) Consider now the dynamics of this 
</p>
<p>partide as it fluctuates by small amounts near x = x0 . The potential it experiences 
may be expanded in a Taylor series: 
</p>
<p>dvi 1 d2 vi V(x)= V(xo)+-. (x-xo)+--. (x-x0 ) 2 +&middot; &middot; &middot; 
dx xo 2! dx2 , 0 
</p>
<p>(7.1.2) 
185 </p>
<p/>
</div>
<div class="page"><p/>
<p>186 
</p>
<p>CHAPTER 7 
</p>
<p>Now, the constant piece V(x0 ) is of no physical consequence and may be 
dropped. [In other words, we may choose V(x0 ) as the arbitrary reference point for 
measuring the potential.] The second term in the series also vanishes since x0 is a 
minimum of V(x), or equivalently, since at a point of static equilibrium, the force, 
-dV/dx, vanishes. If we now shift our origin of coordinates to x0 Eq. (7.1.2) reads 
</p>
<p>(7.1.3) 
</p>
<p>For small oscillations, we may neglect all but the leading term and arrive at the 
potential (or Hamiltonian) in Eq. (7.1.1), d 2V/dx2 being identified with k=mm 2&bull; 
(By definition, xis small if the neglected terms in the Taylor series are small compared 
to the leading term, which alone is retained. In the case of the mass-spring system, 
xis small as long as Hooke's law is a good approximation.) 
</p>
<p>As an example of a system described by a collection of independent oscillators, 
consider the coupled-mass system from Example 1.8.6. (It might help to refresh your 
memory by going back and reviewing this problem.) The Hamiltonian for this system 
is 
</p>
<p>PT p~ 1 2 2 2 2 
Yf=-+-+-mm [xi+xz+(xi-xz)] 
</p>
<p>2m 2m 2 
</p>
<p>= Yfi + Yfz + ~mm 2 (XI- xz)2 (7.1.4) 
</p>
<p>Now this Yf is not of the promised form, since the oscillators corresponding to Yfi 
and Yf2 (associated with the coordinates xi and xz) are coupled by the (xi-xz)2 
</p>
<p>term. But we already know of an alternate description of this system in which it can 
be viewed as two decoupled oscillators. The trick is of course the introduction of 
normal coordinates. We exchange xi and x 2 for 
</p>
<p>(7.1.5a) 
</p>
<p>and 
</p>
<p>(7.1.5b) 
</p>
<p>By differentiating these equations with respect to time, we get similar ones for the 
velocities, and hence the momenta. In terms of the normal coordinates (and the 
corresponding momenta), 
</p>
<p>(7.1.6) 
</p>
<p>Thus the problem of the two coupled masses reduces to that of two uncoupled 
oscillators of frequencies m1 = m = (k/m)I 12 and mu = 3I12m = (3k/m)I 12. </p>
<p/>
</div>
<div class="page"><p/>
<p>Let us rewrite Eq. (7.1.4) as 
</p>
<p>(7.1.7) 
</p>
<p>where Vij are elements of a real symmetric (Hermitian) matrix Vwith the following 
values: 
</p>
<p>(7 .1.8) 
</p>
<p>In switching to the normal coordinates x1 and xu (and p1 and Pu ), we are going 
to a basis that diagonalizes V and reduces the potential energy to a sum of decoupled 
terms, one for each normal mode. The kinetic energy piece remains decoupled in 
both bases. 
</p>
<p>Now, just as the mass-spring system was just a representative element of a 
family of systems described by the oscillator Hamiltonian, the coupled-mass system 
is also a special case of a family that can be described by a collection of coupled 
harmonic oscillators. Consider a system with N Cartesian degrees of freedom 
x1 ... xN, with a potential energy function V(x1, . .. , xN ). Near an equilibrium point 
(chosen as the origin), the expansion of V, in analogy with Eq. (7.1.3), is 
</p>
<p>(7.1.9) 
</p>
<p>For small oscillations, the Hamiltonian is 
</p>
<p>(7.1.10) 
</p>
<p>where 
</p>
<p>(7.1.11) 
</p>
<p>are the elements of a Hermitian matrix V. (We are assuming for simplicity that the 
masses associated with all N degrees of freedom are equal.) From the mathematical 
theory of Chapter 1, we know that there exists a new basis (i.e., a new set of 
coordinates x 1 , xu, ... ) which will diagonalize V and reduce Jf' to a sum of N 
decoupled oscillator Hamiltonians, one for each normal mode. Thus the general 
problem of small fluctuations near equilibrium of an arbitrary system reduces to the 
study of a single harmonic oscillator. 
</p>
<p>This section concludes with a brief description of two important systems which 
are described by a collection of independent oscillators. The first is a crystal (in three 
dimensions), the atoms in which jiggle about their mean positions on the lattice. The 
second is the electromagnetic field in free space. A crystal with No atoms (assumed 
to be point particles) has 3N0 degrees offreedom, these being the displacements from 
</p>
<p>187 
</p>
<p>THE HARMONIC 
OSCILLATOR </p>
<p/>
</div>
<div class="page"><p/>
<p>188 
</p>
<p>CHAPTER 7 
</p>
<p>equilibrium points on the lattice. For small oscillations, the Hamiltonian will be 
</p>
<p>quadratic in the coordinates (and of course the momenta). Hence there wi!I exist 
3N0 normal coordinates and their conjugate momenta, in terms of which Jll' will be 
</p>
<p>a decoupled sum over oscillator Hamiltonians. What are the corresponding normal 
modes? Recall that in the case of two coupled masses, the normal modes corre-
</p>
<p>sponded to collective motions of the entire system, with the two masses in step in 
one case, and exactly out of step in the other. Likewise, in the present case, the 
</p>
<p>motion is collective in the normal modes, and corresponds to plane waves traveling 
</p>
<p>across the lattice. For a given wavevector k, the atoms can vibrate parallel to k 
(longitudinal polarization) or in any one of the two independent directions perpendic-
</p>
<p>ular to k (transverse polarization). Most books on solid state physics will tell you 
</p>
<p>why there are only N0 possible values for k. (This must of course be so, for with 
three polarizations at each k, we will have exactly 3N0 normal modes.) The modes, 
</p>
<p>labeled (k, A), where A is the polarization index (A= 1, 2, 3), form a complete basis 
</p>
<p>for expanding any state of the system. The coefficients of the expansion, a(k, A), are 
</p>
<p>the normal coordinates. The nonnal frequencies are labeled OJ(k, A).t 
</p>
<p>In the case of the electromagnetic field, the coordinate is the potential A(r, I) 
</p>
<p>at each point in space. [A(r, t) is the "velocity" corresponding to the coordinate 
A(r, t).] The normal modes are once again plane waves but with two differences: 
</p>
<p>there is no restriction on k, but the polarization has to be transverse. The quantum 
</p>
<p>theory of the field will be discussed at length in Chapter 18. 
</p>
<p>7 .2. Review of the Classical OscUla tor 
</p>
<p>The equations of motion for the oscillator are, from Eq. (7.1.1), 
</p>
<p>. o.Yf' P 
x=-=-~ 
</p>
<p>op m 
(7 .2.1) 
</p>
<p>(7.2.2) 
</p>
<p>By eliminating p, we arrive at the familiar equation 
</p>
<p>with the solution 
</p>
<p>x(t) =A cos OJt + B sin OJt = x0 cos(mt + &cent;) (7.2.3) 
</p>
<p>where x0 is the amplitude and &cent; the phase of oscillator. The conserved energy 
associated with the oscillator is 
</p>
<p>(7.2.4) 
</p>
<p>; To draw a parallel with the two-mass system, (k, A) is like I or II, a(k, A) is like x, or xn and ro(k, A) 
</p>
<p>is like (k/m) 112 or (3kjm) 112 </p>
<p/>
</div>
<div class="page"><p/>
<p>Since x0 is a continuous variable, so is the energy of the classical oscillator. The 
</p>
<p>lowest value for E is zero, and corresponds to the particle remaining at rest at the 
</p>
<p>ongm. 
By solving for ."i: in terms of E and x from Eq. (7.2.4) we obtain 
</p>
<p>(7.2.5) 
</p>
<p>which says that the particle starts from rest at a turning point (x = &plusmn;x0 ), picks up 
speed till it reaches the origin, and slows down to rest by the time it reaches the 
</p>
<p>other turning point. 
</p>
<p>You are reminded of these classical results, so that you may readily compare 
</p>
<p>and contrast them with their quantum counterparts. 
</p>
<p>7.3. Quantization of the Oscillator (Coordinate Basis) 
</p>
<p>We now consider the quantum oscillator, that is to say, a particle whose state 
</p>
<p>vector llfl) obeys the Schrodinger equation 
</p>
<p>with 
</p>
<p>Jill 
H=:lt'(x-+X J?-+P) = .......... + ---&middot; ma/X2 
</p>
<p>' &middot; 2m 2 
</p>
<p>As observed repeatedly in the past, the complete dynamics is contained in the propa-
</p>
<p>gator U(t), which in turn may be expressed in terms of the eigenvectors and eigenval-
</p>
<p>ues of H. In this section and the next, we will solve the eigenvalue problem in the 
</p>
<p>X basis and the H basis, respectively. In Section 7.5 the passage from the H basis 
</p>
<p>to the X basis will be discussed. The solution in the P basis, trivially related to the 
</p>
<p>solution in the X basis in this case, will be discussed in an exercise. 
</p>
<p>With an eye on what is to follow, let us first establish that the eigenvalues of H 
cannot be negative. For any llfl), 
</p>
<p>1 t 1 2 t 
=-&lt;lfiiPPilfl)+-mm (lfiiXXI~V) 
</p>
<p>2m 2 
</p>
<p>since the norms of the states I Pip') and IX IV) cannot be negative. If we now set llfl) 
equal to any eigenstate of H, we get the desired result. 
</p>
<p>189 
</p>
<p>THE HARMONIC 
</p>
<p>OSCTLLATOR </p>
<p/>
</div>
<div class="page"><p/>
<p>190 
</p>
<p>CHAPTER 7 
</p>
<p>Armed with the above result, we are now ready to attack the problem in the X 
basis. 
</p>
<p>We begin by projecting the eigenvalue equation, 
</p>
<p>onto the X basis, using the usual substitutions 
</p>
<p>and obtain 
</p>
<p>X-&gt;x 
</p>
<p>d 
P-&gt;-in --
</p>
<p>dx 
</p>
<p>IE)----; 1/ft:(X) 
</p>
<p>(The argument of V' and the subscript E are implicit.) 
</p>
<p>We can rearrange this equation to the form 
</p>
<p>(7.3.1) 
</p>
<p>(7.3.2) 
</p>
<p>(7 .3.3) 
</p>
<p>We wish to find all solutions to this equation that lie in the physical Hilbert space 
</p>
<p>(of functions normalizable to unity or the Dirac delta function). Follow the approach 
</p>
<p>closely-it will be invoked often in the future. 
</p>
<p>The first step is to write Eq. (7.3.3) in terms of dimensionless variables. We 
</p>
<p>look for a new variable y which is dimensionless and related to x by 
</p>
<p>_x-=b,;; (7 .3.4) 
</p>
<p>where b is a scale factor with units of length. Although any length b (say the radius 
</p>
<p>of the solar system) will generate a dimensionless variable y, the idea is to choose 
</p>
<p>the natural length scale generated by the equation itself. By feeding Eq. (7.3.4) into 
</p>
<p>Eq. (7.3.3), we arrive at 
</p>
<p>(7.3.5) </p>
<p/>
</div>
<div class="page"><p/>
<p>The last terms suggests that we choose 
</p>
<p>-( 1i )1/2 
b--
</p>
<p>mw 
(7.3.6) 
</p>
<p>Let us also define a dimensionless variable &pound;corresponding toE: 
</p>
<p>mEb2 E 
s=--=-
</p>
<p>1i2 1iw 
(7.3.7) 
</p>
<p>(We may equally well chooses= 2mEb2 j1i2. Constants of order unity are not uniquely 
suggested by the equation. In the present case, our choice of &pound; is in anticipation of 
the results.) In terms of the dimensionless variables, Eq. (7.3.5) becomes 
</p>
<p>Vl"+(2s-l)'l'=0 (7.3.8) 
</p>
<p>where the prime denotes differentiation with respect toy. 
Not only do dimensionless variables lead to a more compact equation, they also 
</p>
<p>provide the natural scales for the problem. By measuring x and E in units of 
(1ijmw) 112 and 1iw, which are scales generated intrinsically by the parameters enter-
ing the problem, we develop a feeling for what the words "small" and "large" mean: 
for example the displacement of the oscillator is large if y is large. If we insist on 
using the same units for all problems ranging from the atomic physics to cosmology, 
we will not only be dealing with extremely large or extremely small numbers, we will 
also have no feeling for the size of quantities in the relevant scale. (A distance of 
10-20 parsecs, small on the cosmic scale, is enormous if one is dealing with an atomic 
system.) 
</p>
<p>The next step is to examine Eq. (7.3.8) at limiting values of y to learn about 
the solution in these limits. In the limit y-+oo, we may neglect the 2&pound;'1' term and 
obtain 
</p>
<p>(7.3.9) 
</p>
<p>The solution to this equation in the same limit is 
</p>
<p>for 
</p>
<p>191 
</p>
<p>THE HARMONIC 
OSCILLATOR </p>
<p/>
</div>
<div class="page"><p/>
<p>192 
</p>
<p>CHAPTER 7 
</p>
<p>where we have dropped all but the leading power in y as y-&gt; oo. Of the two possibilit-
ies ym e&plusmn;Y'12 , we picky"' e&middot;--y'12 , for the other possibility is not a part of the physical 
</p>
<p>Hilbert space since it grows exponentially as y-&gt; oo. 
Consider next the y---&gt;0 limit. Equation (7.3.8) becomes, upon dropping the llfl 
</p>
<p>term, 
</p>
<p>lfl"+2clji=O 
</p>
<p>which has the solution 
</p>
<p>lji=A cos[J2iy]+Bsin[J2sy] 
</p>
<p>Since we have dropped the l term in the equation as being too small, consistency 
demands that we expand the cosine and sine and drop terms of order l and beyond. 
We then get 
</p>
<p>1{1 ----+ A + cy + 0( ./) 
y-&bull;0 
</p>
<p>where cis a new constant [=B(2c:) 112]. 
We therefore infer that lfl is of the form 
</p>
<p>l{l(y) = u(y) (7.3.10) 
</p>
<p>where u approaches A+ cy (plus higher powers) as y---&gt;0, andy"' (plus lower powers) 
as y-&gt;oo. To determine u(y) completely, we feed the above ansatz into Eq. (7.3.8) 
</p>
<p>and obtain 
</p>
<p>u" ---- 2yu' + (2c: -&middot;- 1 )u = 0 (7.3.11) 
</p>
<p>This equation has the desired features (to be discussed in Exercise 7.3.1) that indicate 
</p>
<p>that a power-series solution is possible, i.e., if we assume 
</p>
<p>u(y) = L Cny'' (7.3.12) 
n=O 
</p>
<p>the equation will determine the coefficients. [The series begins with n = 0, and not 
some negative n, since we know that as y-+0, u-&gt;A +cy+ 0(/).] Feeding this series 
</p>
<p>into Eq. (7.3.ll) we find 
</p>
<p>X 
</p>
<p>L Cn[n(n-l)yn- 2 -2ny"+(2e-l)yn]=O (7.3.13) 
n=O 
</p>
<p>Consider the first of three pieces in the above series: 
</p>
<p>J:~&middot; 
</p>
<p>L Cnn(n-l)yn-l 
n=O </p>
<p/>
</div>
<div class="page"><p/>
<p>Due to the n(n- 1) factor, this series also equals 
</p>
<p>00 
</p>
<p>L Cnn(n-l)yn- 2 
n=2 
</p>
<p>In terms of a new variable m = n- 2 the series becomes 
</p>
<p>L Cm+2(m+2)(m+ l)ym= L Cn+2(n+2)(n+ l)yn 
m=O n=O 
</p>
<p>since m is a dummy variable. Feeding this equivalent series back into Eq. (7.3.13) 
</p>
<p>we get 
</p>
<p>L yn[Cn+in+2)(n+l)+Cn(26-l-2n)]=O (7.3.14) 
n=O 
</p>
<p>Since the functions yn are linearly independent (you cannot express yn as a linear 
</p>
<p>combination of other powers of y) each coefficient in the linear relation above must 
</p>
<p>vanish. We thus find 
</p>
<p>Cn+ 2=Cn (2n+l-26) 
</p>
<p>(n+2)(n+ 1) 
(7.3.15) 
</p>
<p>Thus for any C0 and C1 , the recursion relation above generates C2 , C4 , C6 , &bull;&bull;&bull; and 
</p>
<p>C3 , C5 , C7 , &bull;&bull;&bull;&bull; The function u(y) is given by 
</p>
<p>c[t (1-26)/ (1-26) (4+1-26) 4 J 
u(y)= 0 + (0+2)(0+ 1) + (0+2)(0+ 1) (2+2)(2+ l)y +&middot; .. 
</p>
<p>c [ (2+ 1-26)/ (2+ 1-26) (6+ 1-26) 5 J 
+ I y+ (1+2){1+1) + (1+2)(1+1) (3+2)(3+l)y +&middot; .. 
</p>
<p>(7.3.16) 
</p>
<p>where C0 and C1 are arbitrary. 
</p>
<p>It appears as if the energy of the quantum oscillator is arbitrary, since &amp; has 
</p>
<p>not been constrained in any way. But we know something is wrong, since we saw at 
</p>
<p>the outset that the oscillator eigenvalues are nonnegative. The first sign of sickness 
</p>
<p>in our solution, Eq. (7.3.16), is that u(y) does not behave like ym as y-+oo (as 
</p>
<p>deduced at the outset) since it contains arbitrarily high powers of y. There is only 
</p>
<p>one explanation. We have seen that as y-+ oo, there are just two possibilities 
</p>
<p>If we write ljl(y) = u(y) e-y'12, then the two possibilities for u(y) are 
</p>
<p>193 
</p>
<p>THE HARMONIC 
</p>
<p>OSCILLATOR </p>
<p/>
</div>
<div class="page"><p/>
<p>194 
</p>
<p>CHAPTER 7 
</p>
<p>Clearly u(y) in Eq. (7.3.16), which is not bounded by any finite power of y as y-&gt;oo, 
corresponds to the latter case. We may explicitly verify this as follows. 
</p>
<p>Consider the power series for u(y) as y-&gt;oo. Just as the series is controlled by 
C0 (the coefficient of the lowest power of y) as y-&gt;0, it is governed by its coefficients 
Cn~~ as y-&gt; oo. The growth of the series is characterized by the ratio [see Eq. (7.3.15)] 
</p>
<p>Cn+2 2 
-~---+-
</p>
<p>Cn n---+'X&middot; n 
</p>
<p>Compare this to the growth of ym eY'. Since 
</p>
<p>so 
</p>
<p>x 2k+m 
In)'' "y ye=L..--
</p>
<p>k~o k! 
</p>
<p>Cn =coefficient of y" = 1/ k!; with n = 2k + m or k = (n - m) /2. Likewise 
</p>
<p>I 
Cn+2= &middot;~---
</p>
<p>[(n+2-m)/2]! 
</p>
<p>c+2 [(n- m)/2]! 2 
-~---+ ____::.c___-'-'.-_:_._ 
</p>
<p>c, n~oc [(n+2-m)/2]! (n-m+2)/2 n 
</p>
<p>(7.3.17) 
</p>
<p>In other words, u(y) in Eq. (7.3.16) grows as ymey', so that lfi(Y):!::yme&gt;' e-y'12 :!:: 
ym e+y'12 , which is the rejected solution raising its ugly head! Our predicament is now 
reversed: from finding that every e is allowed, we are now led to conclude that no 
e is allowed. Fortunately there is a way out. If e is one of the special values 
</p>
<p>2n+ I 
e=--
" 2 , 
</p>
<p>n=O, I, 2, ... (7.3.I8) 
</p>
<p>the coefficient c+2 (and others dependent on it) vanish. If we choose cl =0 when 
n is even (or C0 = 0 when n is odd) we have a finite polynomial of order n which 
satisfies the differential equation and behaves as yn as y-&gt; oo: 
</p>
<p>( ) ( ) -v'l2 &deg; 2Y 4Y n.Y -v2 12 { C + C 
2 + C &bull;4 + &middot; &middot; &middot; + C n } 
</p>
<p>lfiY =uy e&middot;&middot; = &middot;e&middot; 
C1y + C3/ + Cs/ + &middot; &middot; &middot; + C,.y" 
</p>
<p>(7.3.I9) 
</p>
<p>Equation ( 7. 3.18) tells us that energy is quantized: the only allowed values for 
E= e/i(J) (i.e., values that yield solutions in the physical Hilbert space) are 
</p>
<p>n=O, 1, 2, ... (7.3.20) </p>
<p/>
</div>
<div class="page"><p/>
<p>For each value of n, Eq. (7.3.15) determines the corresponding polynomials of nth 
</p>
<p>order, called Hermite polynomials, Hn(Y): 
</p>
<p>Ho(Y) = 1 
</p>
<p>H1(y) =2y 
</p>
<p>Hz(Y) = -2(1- 2/) 
</p>
<p>H 3(y) = -12(y-~y 3 ) 
</p>
<p>H4(y) = 12( 1-4)/ +}y4) 
</p>
<p>(7.3.21) 
</p>
<p>The arbitrary initial coefficients Co and C1 in H, are chosen according to a standard 
</p>
<p>convention. The normalized solutions are then 
</p>
<p>(7.3.22) 
</p>
<p>The derivation of the normalization constant 
</p>
<p>(7.3.23) 
</p>
<p>is rather tedious and will not be discussed here in view of a shortcut to be discussed 
</p>
<p>in the next section. 
</p>
<p>The following recursion relations among Hermite polynomials are very useful: 
</p>
<p>H~(y) = 2nH, -1 (7.3.24) 
</p>
<p>Hn+ J(y) =2yH,- 2nHn-l (7.3.25) 
</p>
<p>as is the integral 
</p>
<p>fXJ H,(y)Hn&middot;(y) 
-y_ 
</p>
<p>(7.3.26) 
</p>
<p>which is just the orthonormality condition of the eigenfunctions lf!n(x) and lfln{x) 
</p>
<p>written in terms of y= (mm/1i) 112x. 
We can now express the propagator as 
</p>
<p>CD 
1 moJ o \ (. mm 
</p>
<p>U(x,t;x',t')= 2: Anexpl--x&middot;JHn(x)Anexp ---
n ~ 0 \ 2/i ' . 2/i 
</p>
<p>) 
x Hn(x') exp[ -i(n + l/2)m(t- t')] (7.3.27) 
</p>
<p>195 
</p>
<p>THE HARMONIC 
OSCILLATOR </p>
<p/>
</div>
<div class="page"><p/>
<p>196 
</p>
<p>CHAPTER 7 
</p>
<p>Evaluation of this sum is a highly formidable task. We will not attempt it here since 
</p>
<p>we will find an extremely simple way for calculating U in Chapter 8, devoted to the 
path integral formalism. The result happens to be 
</p>
<p>where T=t-t'. 
</p>
<p>cos roT- 2xx' J 
2 sin roT 
</p>
<p>(7.3.28) 
</p>
<p>This concludes the solution of the eigenvalue problem. Before analyzing our 
results let us recapitulate our strategy. 
</p>
<p>Step 1. Introduce dimensionless variables natural to the problem. 
Step 2. Extract the asymptotic (y-+co, y-+0) behavior of'+'&middot; 
Step 3. Write vr as a product of the asymptotic form and an unknown function u. 
</p>
<p>The function u will usually be easier to find than lf!. 
Step 4. Try a power series to see if it will yield a recursion relation of the form Eq. 
</p>
<p>(7 .3.15). 
</p>
<p>Exercise 7.3.1. * Consider the question why we tried a power-series solution for Eq. 
(7.3.11) but not Eq. (7.3.8). By feeding in a series into the latter, verify that a three-term 
</p>
<p>recursion relation between Cn+z, Cn, and Cn-z obtains, from which the solution does not 
</p>
<p>follow so readily. The problem is that V'" has two powers of y less than 2t:lf!, while the 
</p>
<p>piece has two more powers of y. ln Eq. (7.3.11) on the other hand, of the three pieces u". 
</p>
<p>-2yu', and (2~::- 1 )u, the last two have the same powers of y. 
</p>
<p>Exercise 7.3.2. Verify that H3(y) and H 4(y) obey the recursion relation, Eq. (7.3.15). 
</p>
<p>Exercise 7.3.3. If Vt(X) is even and tj&gt;(x) is odd under x-&gt;-x, show that 
</p>
<p>r~~ lf!(x)r/J(x) dx=O 
</p>
<p>Use this to show that lf!2(x) and lf!1(x) are orthogonal. Using the values of Gaussian integrals 
</p>
<p>in Appendix A.2 verify that lf!2(x) and lf!o(x) are orthogonal. 
</p>
<p>Exercise 7.3.4. Using Eqs. (7.3.23)-(7.3.25), show that 
</p>
<p>( 
\ l/2 
</p>
<p>fl. ... 1 n ... l /2&middot; 
(n'IXIn)= &middot;2 .............. ) [On',n+l(n+l) &middot;-+on,n&middot;&middot;ln' I , mw 
</p>
<p>Exercise 7.3.5. * Using the symmetry arguments from Exercise 7.3.3 show that (niXIn) == 
(n!Pin)=O and thus that (X2 )=(AX)2 and (P 2 )=(Al')2 in these states. Show that 
(IIX2Il)=31i/2mw and (liP 2 1l)=~mwfi. Show that lf!o(x) saturates the uncertainty bound 
</p>
<p>AX&middot; AP;:;:.fi/2. </p>
<p/>
</div>
<div class="page"><p/>
<p>Exercise 7.3.6. * Consider a particle in a potential 
</p>
<p>x&gt;O 
</p>
<p>=oo, x~O 
</p>
<p>What are the boundary conditions on the wave functions now? Find the eigenvalues and 
eigenfunctions. 
</p>
<p>We now discuss the eigenvalues and eigenfunctions of the oscillator. The follow-
</p>
<p>ing are the main features: 
</p>
<p>(l) The energy is quantized. In contrast to the classical oscillator whose energy 
</p>
<p>is continuous, the quantum oscillator has a discrete set oflevels given by Eq. (7.3.20). 
</p>
<p>Note that the quantization emerges only after we supplement Schrodinger's equation 
</p>
<p>with the requirement that 'I' be an element of the physical Hilbert space. In this case 
</p>
<p>it meant the imposition of the boundary condition 'l'(lxl-+oo)-+0 [as opposed to 
</p>
<p>'l'(lxl-+oo)-+oo, which is what obtained for all but the special values of E). 
</p>
<p>Why does the classical oscillator seem to have a continuum of energy values? 
</p>
<p>The answer has to do with the relative sizes of the energy gap and the total energy 
</p>
<p>of the classical oscillator. Consider, for example, a mass of 2 g, oscillating at a 
</p>
<p>frequency of 1 radjsec, with an amplitude of 1 em. Its energy is 
</p>
<p>E= ~malx~= 1 erg 
</p>
<p>Compare this to the gap between allowed energies : 
</p>
<p>AE= fUJ) ~ 10-27 erg 
</p>
<p>At the macroscopic level, it is practically impossible to distinguish between a system 
</p>
<p>whose energy is continuous and one whose allowed energy levels are spaced 10-27 erg 
</p>
<p>apart. Stated differently, the quantum number associated with this oscillator is 
</p>
<p>E 1 27 
n=---~10 
</p>
<p>1im 2 
</p>
<p>while the difference inn between adjacent levels is unity. We have here a special case 
</p>
<p>of the correspondence principle, which states that as the quantum number tends to 
</p>
<p>infinity, we regain the classical picture. (We know vaguely that when a system is big, 
</p>
<p>it may be described classically. The correspondence principle tells us that the quantum 
</p>
<p>number is a good measure of bigness.) 
(2) The levels are spaced uniformly. The fact that the oscillator energy levels 
</p>
<p>go up in steps of 1im allows one to construct the following picture. We pretend that 
</p>
<p>associated with an oscillator of classical frequency a&gt; there exist fictitious particles 
</p>
<p>called quanta each endowed with energy 1im. We view the n1im piece in the energy 
formula Eq. (7 .3.20) as the energy of n such quanta. In other words, we forget about 
</p>
<p>the mass and spring and think in terms of the quanta. When the quantum number 
</p>
<p>n goes up (or down) by An, we say that An quanta have been created (or destroyed). 
</p>
<p>197 
</p>
<p>THE HARMONIC 
</p>
<p>OSCILLATOR </p>
<p/>
</div>
<div class="page"><p/>
<p>198 
</p>
<p>CHAPTER 7 
</p>
<p>Although it seems like a matter of semantics, thinking nf the oscillator in terms of 
</p>
<p>these quanta has proven very useful. 
</p>
<p>In the case of the crystal, there are 3No oscillators, labeled by the 3N0 values of 
</p>
<p>(k, A), with frequencies m(k, A). The quantum state of the crystal is specified by 
</p>
<p>giving the number of quanta, called phonons, at each (k, A). For a crystal whose 
</p>
<p>Hamiltonian is exactly given by a sum of oscillator pieces, the introduction of the 
</p>
<p>phonon concept is indeed a matter of semantics. If, however, we consider deviations 
</p>
<p>from this, say to take into account nonleading terms in the Taylor expansion of the 
</p>
<p>potential, or the interaction between the crystal and some external probe such as an 
</p>
<p>electron shot at it, the phonon concept proves very useful. (The two effects mentioned 
</p>
<p>above may be seen as phonon phonon interactions and phonon electron inter-
</p>
<p>actions, respectively.) 
</p>
<p>Similarly, the interaction of the electromagnetic field with matter may be viewed 
</p>
<p>as the interaction between light quanta or photons and matter, which is discussed in 
</p>
<p>Chapter 18. 
</p>
<p>(3) The lowest possible energy is lim /2 and not 0. Unlike the classical oscillator, 
</p>
<p>which can be in a state of zero energy (with x = p = 0) the quantum oscillator has a 
minimum energy of lim /2. This energy, called the zero-point energy, is a reflection 
of the fact that the simultaneous eigenstate I x = 0, p = 0) is precluded by the canonical 
commutation relation [X, P] = ifi. This result is common to all oscillators, whether 
they describe a mechanical system or a normal mode of the electromagnetic field, 
</p>
<p>since all these problems are mathematically identical and differ only in what the 
</p>
<p>coordinate and its conjugate momentum represent. Thus, a crystal has an energy 
</p>
<p>~ lim(k, },) in each mode (k, },) even when phonons are absent, and the electromag-
</p>
<p>netic field has an energy 21iw(k, .A) in each mode offrequency w even when photons 
</p>
<p>are absent. (The zero-point fluctuation of the field has measurable consequences, 
</p>
<p>which will be discussed in Chapter 18.) 
</p>
<p>In the following discussion let us restrict ourselves to the mechanical oscillator 
</p>
<p>and examine more closely the zero-point energy. We saw that it is the absence of 
</p>
<p>the state lx=O, p=O) that is responsible for this energy. Such a state, with AX= 
</p>
<p>!J.P = 0, is forbidden by the uncertainty principle. Let us therefore try to find a state 
that is quantum mechanically allowed and comes as close as possible (in terms of 
</p>
<p>its energy) to the classical state x=p=O. If we choose a wave function llf(x) that is 
</p>
<p>sharply peaked near x = 0 to minimize the mean potential energy dmw 2X 2 ), the 
wave function in P space spreads out and the mean kinetic energy (P 2 /2m) grows. 
</p>
<p>The converse happens if we pick a momentum space wave function sharply peaked 
</p>
<p>near p = 0. What we need then is a compromise If/ min(x) that minimizes the total 
</p>
<p>mean energy without violating the uncertainty principle. Let us now begin our quest 
</p>
<p>for 'l'min(x). We start with a nonnalized trial state I II') and consider 
</p>
<p>(7.3.29) 
</p>
<p>Now 
</p>
<p>(!J.P)" = (P 2 ) --- (P/ (7.3.30) 
</p>
<p>and 
</p>
<p>(7.3.31) </p>
<p/>
</div>
<div class="page"><p/>
<p>so that 
</p>
<p>(7.3.32) 
</p>
<p>The first obvious step in minimizing (H) is to restrict ourselves to states with (X)= 
(P) = 0. (Since (X) and (P) are independent of each other and of (A.&yen; f and (AP)2, 
such a choice is always possible.) For these states (from which we must pick the 
winner) 
</p>
<p>(AP)2 1 2 , 
(H)=--+ mw (AX)" 
</p>
<p>2m 2 
(7 .3.33) 
</p>
<p>Now we use the uncertainty relation 
</p>
<p>(7.3.34) 
</p>
<p>where the equality sign holds only for a Gaussian, as will be shmvn in Section 9.3. 
</p>
<p>We get 
</p>
<p>jj2 1 2 0 
(H)?:.-&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;+&middot;&middot;&middot;&middot;moJ (AX)-
</p>
<p>8m(AX)2 2 
</p>
<p>We minimize (H) by choosing a Gaussian wave function, for which 
</p>
<p>jj2 1 2 2 
(H)Gaussian = Sm(M)2 + 2 mw (AX) 
</p>
<p>(7.3.35) 
</p>
<p>(7.3.36) 
</p>
<p>What we have found is that the mean energy associated with the trial wave function 
is sensitive only to the corresponding AX and that, of all functions with the same 
AX, the Gaussian has the lowest energy. Finally we choose, from the family of 
</p>
<p>Gaussians, the one with the AX that minimizes (lf)Gaussian. By requiring 
</p>
<p>(7.3.37) 
</p>
<p>we obtain 
</p>
<p>(7.3.38) 
</p>
<p>and 
</p>
<p>(H) min= nw /2 (7.3.39) 
</p>
<p>199 
</p>
<p>THE HARMONIC 
</p>
<p>OSCILLATOR </p>
<p/>
</div>
<div class="page"><p/>
<p>200 
</p>
<p>CHAPTER 7 
</p>
<p>Thus, by systematically hunting in Hilbert space, we have found that the following 
normalized function has the lowest mean energy: 
</p>
<p>(mm)114 ( mmxz) IJ!min(X) = --;;r; exp -~ , (7.3.40) 
</p>
<p>If we apply the above result 
</p>
<p>to IIJI) = IIJ!o) =ground-state vector, we get 
</p>
<p>(IJ!miniHIIJ!min) ~(IJ!oiHIIJ!o)=Eo (7.3.41) 
</p>
<p>Now compare this with the result of Exercise 5.2.2: 
</p>
<p>If we set IIJI) = IIJ!min) we get 
</p>
<p>Eo= (IJ!oiHIIJ!o) ~ (IJ!miniHJ IJ!min) (7.3.42) 
</p>
<p>It follows from Eq. (7.3.41) and (7.3.42) that 
</p>
<p>(7.3.43) 
</p>
<p>Also, since there was only one state, IIJ!min), with energy 'lim /2, it follows that 
</p>
<p>IIJ!o) = IIJ!min) (7.3.44) 
</p>
<p>We have thus managed to find the oscillator ground-state energy and state vector 
without solving the Schrodinger equation. 
</p>
<p>It would be a serious pedagogical omission if it were not emphasized at this 
juncture that the uncertainty relation has been unusually successful in the above 
context. Our ability here to obtain all the information about the ground state using 
the uncertainty relation is a consequence of the special form of the oscillator Hamil-
tonian [which allowed us to write (H) in terms of (AX)2 and (!1P)2] and the fact 
that its ground-state wave function is a Gaussian (which has a privileged role with 
respect to the uncertainty relation). In more typical instances, the use of the uncer-
tainty relation will have to be accompanied by some hand-waving [before (H) can 
be approximated by a function of (AX)2 and (l1P)2] and then too will yield only an 
estimate for the ground-state energy. As for the wave function, we can only get an 
estimate for AX, the spread associated with it. </p>
<p/>
</div>
<div class="page"><p/>
<p>-4 
</p>
<p>Figure 7.1. Normalized eigenfunctions for n= 
</p>
<p>0, 1, 2, and 3. The small arrows at -4 
</p>
<p>IYI =(2n+ 1) 112 stand for the classical turning 
</p>
<p>points. Recall that y = (mco j1i) 112x. 
</p>
<p>4 
</p>
<p>4 y 
</p>
<p>(4) The solutions (Fig. 7.1) 'l'n(x) contain only even or odd powers of x, depend-
</p>
<p>ing on whether n is even or odd. Consequently the eigenfunctions are even or odd: 
</p>
<p>'l'i -x) = 'lfn{x), 
</p>
<p>= -'l'n(X), 
</p>
<p>neven 
</p>
<p>n odd 
</p>
<p>In Chapter 11 on symmetries it will be shown that the eigenfunctions had to have 
</p>
<p>this property. 
</p>
<p>(5) Thf' "';ave function does not vanish beyond the classical turning points, but 
</p>
<p>dies out exponentially as x-+oo. [Verify that the classical turning points are given 
</p>
<p>by y0 = &plusmn;(2n + 1) 112.] Notice, however, that when n is large (Fig. 7.2) the excursions 
outside the turning points are small compared to the classical amplitude. This expo-
</p>
<p>nentially damped a.nplitude in the classically forbidden region was previously 
</p>
<p>encountered in Chapter 5 when we studied tunneling. 
</p>
<p>(6) The probability distribution P(x) is very different from the classical case. 
</p>
<p>The position of a given classical oscillator is of course exactly known. But we could 
</p>
<p>ask the following probabilistic question: if I suddenly walk into a room containing 
</p>
<p>the oscillator, where am I likely to catch it? If the velocity of the oscillator at a point 
</p>
<p>x is v(x), the time it spends near the x, and hence the probability of our catching it 
</p>
<p>there during a random spot check, varies inversely with v(x): 
</p>
<p>(7.3.45) 
</p>
<p>which is peaked near &plusmn;x0 and has a minimum at the origin. In the quantum case, 
</p>
<p>for the ground state in particular, I 'l'(x)l 2 seems to go just the other way (Fig. 7.1). 
</p>
<p>There is no contradiction here, for quantum mechanics is expected to differ from 
</p>
<p>classical mechanics. The correspondence principle, however, tells us that for large n 
</p>
<p>201 
</p>
<p>THE HARMONIC 
</p>
<p>OSCILLATOR </p>
<p/>
</div>
<div class="page"><p/>
<p>202 
</p>
<p>CHAPTER 7 
</p>
<p>-6 
</p>
<p>\ 
' I 
</p>
<p>-2 0 2 
</p>
<p>' ' 
</p>
<p>I 
I 
I 
</p>
<p>I 
I 
</p>
<p>4 
</p>
<p>li'igure 7.2. Probability density in the staten&middot;~ II. 
</p>
<p>The broken curve gives the classical probability 
</p>
<p>6 Y distribution in a stale with the same energy. 
</p>
<p>the two must become indistinguishable. From Fig. 7.2, which shows the situations 
</p>
<p>at n = 11, we can see how the classical limit is reached: the quantum distribution 
P(x) = ilfl(x)l 2 wiggles so rapidly (in a scale set by the classical amplitude) that only 
its mean can be detected at these scales, and this agrees with Pc 1 (x). We are reminded 
</p>
<p>here of the double-slit experiment performed with macroscopic particles: there is a 
</p>
<p>dense interference pattern, whose mean is measured in practice and agrees with the 
classical probability curve. 
</p>
<p>A remark that was made in more general tenns in Chapter 6: the classical 
</p>
<p>oscillator that we often refer to, is a figment lodged in our imagination and doesn't 
</p>
<p>exist. In other words, all oscillators, including the 2-g mass and spring system, are 
</p>
<p>ultimately governed by the laws of quantum mechanics, and thus have discrete 
</p>
<p>energies, can shoot past the "classical" turning points, and have a zero-point energy 
</p>
<p>of ~:lim even while they play dead. Note however that what I am calling nonexistent 
is an oscillator that actually has the properties attributed to it in classical mechanics, 
</p>
<p>and not one that seems to have them when examined at the macroscopic level. 
</p>
<p>Exercise 7.3. 7. * The Oscillator in J.,fomentum Space. By setting up an eigenvalue equation 
for the oscillator in the P basis and comparing it to Eq. (7.3.2). show that the momentum 
</p>
<p>space eigenfunctions may be obtained from the ones in coordinate space through the substitu-
</p>
<p>tion x-&gt;p, mw-&gt;1/mw. Thus, for example, 
</p>
<p>There are several other pairs, such as b.X and b.P in the state In), which are related by the 
substitution mw--&gt; 1 jmw. You may wish to watch out for them. (Refer back to Exercise 7.3.5.) 
</p>
<p>7 .4. The OsciUator in the Energy Basis 
</p>
<p>Let us orient ourselves by recalling how the eigenvalue equation 
</p>
<p>(' P
2 1 2 2 '), 
</p>
<p>&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;+&middot;&middot;&middot;&middot; mmX 1E) =EIE) 
,2m 2 ; 
</p>
<p>(7.4.1) </p>
<p/>
</div>
<div class="page"><p/>
<p>was solved in the coordinate basis: (1) We made the assignments X ~x, 
P~-ifi d/ dx. (2) We solved for the components (xI E)= 'I' dx) and the eigenvalues. 
</p>
<p>To solve the problem in the momentum basis, we first compute the X and P 
operators in this basis, given their form in the coordinate basis. For instance, 
</p>
<p>(p'IXIp) =II V:j~ ~xi&middot;~E.? ~!!J dx dx' 
e -- tp .&lt;:;, Fi ._x-Q( X- x') e'PX i1i 
</p>
<p>(l?r~)''' (given) (2;;~)i~1 
</p>
<p>= -i1i8 '(p-p') 
</p>
<p>We then find P and H(X, P) in this basis. The eigenvalue equation, (7.4.1 ), will then 
</p>
<p>become a differential equation that we will proceed to solve. 
Now suppose that we want to work in the energy basis. We must first find the 
</p>
<p>eigenfunctions of H, i.e., (xi E), so that we can carry out the change of basis. But 
</p>
<p>finding (xiE)= '!'E(x) amounts to solving the full eigenvalue problem in the coordi-
nate basis. Once we have done this, there is not much point in setting up the problem 
</p>
<p>in the E basis. 
But there is a clever way due to Dirac, which allows us to work in the energy 
</p>
<p>basis without having to know ahead of time the operators X and P in this basis. All 
</p>
<p>we will need is the commutation relation 
</p>
<p>[X, P] = ifii= iti (7 .4.2) 
</p>
<p>which follows from X ~x, P~-ifi djdx, but is basis independent. The next few steps 
will seem rather mysterious and will not fit into any of the familiar schemes discussed 
</p>
<p>so far. You must be patient till they begin to pay off. 
Let us first introduce the operator 
</p>
<p>(
mm)li2 ( 1 \1;2 
</p>
<p>a = , 2i X+ i j;;;&middot;;;;&middot;tj) P (7.4.3) 
</p>
<p>and its adjoint 
</p>
<p>(7.4.4) 
</p>
<p>(Note that mm-&gt;Ijmm as X&lt;-&gt; P.) They satisfy the commutation relation (which 
you should verify) 
</p>
<p>(7.4.5) 
</p>
<p>203 
</p>
<p>THE HARMONIC 
</p>
<p>OSCILLATOR </p>
<p/>
</div>
<div class="page"><p/>
<p>204 
</p>
<p>CHAPTER 7 
</p>
<p>Note next that the Hermitian operator at a is simply related to H: 
</p>
<p>t mw 2 1 2 i 
aa=-X +--P +-[X,P] 
</p>
<p>211 2mw 11 211 
</p>
<p>H 
</p>
<p>11w 2 
</p>
<p>so that 
</p>
<p>(7.4.6) 
</p>
<p>[This method is often called the "method of factorization" since we are 
expressing H = P 2 + X 2 (ignoring constants) as a product of (X+ iP) =a and 
(X- iP) =at. The extra 11w /2 in Eq. (7.4.6) comes from the non-commutative nature 
of X and P.] 
</p>
<p>Let us next define an operator H, 
</p>
<p>- H t 
H=~=(a a+ 1/2) 
</p>
<p>11w 
(7.4.7) 
</p>
<p>whose eigenvalues s measure energy in units of 11w. We wish to solve the eigenvalue 
equation for ii: 
</p>
<p>(7.4.8) 
</p>
<p>where sis the energy measured in units of 11w. Two relations we will use shortly are 
</p>
<p>- t t [a, H)= [a, a a+ 1/2] =[a, a a] =a (7.4.9) 
</p>
<p>and 
</p>
<p>(7.4.10) 
</p>
<p>The utility of a and at stems from the fact that given an eigenstate of H, they generate 
others. Consider 
</p>
<p>Hal s) = (aH- [a, HJ)I s) 
</p>
<p>= (aH -a)l s) 
</p>
<p>= (s-1)al s) (7.4.11) </p>
<p/>
</div>
<div class="page"><p/>
<p>We infer from Eq. (7.4.11) that ale) is an eigenstate with eigenvalue e-1, i.e., 
</p>
<p>ale)=Cie-1) 
</p>
<p>where C, is a constant, and I e- 1) and I e) are normalized eigenketsJ 
Similarly we see that 
</p>
<p>so that 
</p>
<p>fiat I e)= (atfi- [at, H)) I e) 
</p>
<p>= (atfi +at) I e) 
</p>
<p>=(e+l)atle) 
</p>
<p>(7.4.12) 
</p>
<p>(7.4.13) 
</p>
<p>(7.4.14) 
</p>
<p>One refers to a and at as lowering and raising operators for obvious reasons. They 
</p>
<p>are also called destruction and creation operators since they destroy or create quanta 
</p>
<p>of energy rl(j}. 
We are thus led to conclude that if e is an eigenvalue of ii, so are 
</p>
<p>e+ I, e+ 2, e+ 3, ... , e+ oo; and e-1, ... , e- oo. The latter conclusion is in con-
flict with the result that the eigenvalues of H are nonnegative. So, it must be that 
</p>
<p>the downward chain breaks at some point: there must be a state I eo) that cannot 
be lowered further: 
</p>
<p>al eo)=O 
</p>
<p>Operating with at, we get 
</p>
<p>or 
</p>
<p>(H-l/2)le0)=0 [fromEq.(7.4.7)] 
</p>
<p>or 
</p>
<p>or 
</p>
<p>I 
eo= 2 
</p>
<p>t We are using the fact that there is no degeneracy in one dimension. 
</p>
<p>(7.4.15) 
</p>
<p>(7.4.16) 
</p>
<p>205 
</p>
<p>THE HARMONIC 
</p>
<p>OSCILLATOR </p>
<p/>
</div>
<div class="page"><p/>
<p>206 
</p>
<p>CHAPTER 7 
</p>
<p>We may, however, raise the state I c0 ) indefinitely by the repeated application of at. 
We thus find that the oscillator has a sequence of levels given by 
</p>
<p>Cn= (n+ 1/2), n =0, 1, 2, ... 
</p>
<p>or 
</p>
<p>En= (n + 1 /2)1im, n=O, 1, 2, ... (7.4.17) 
</p>
<p>Are these the only levels? If there were another family, it too would have to have a 
ground state 1 eo) such that 
</p>
<p>al cb) =0 
</p>
<p>or 
</p>
<p>or 
</p>
<p>(7.4.18) 
</p>
<p>But we know that there is no degeneracy in one dimension (Theorem 15). Conse-
quently it follows from Eqs. (7.4.16) and (7.4.18) that I c0 ) and I c0) represent the 
same state. The same goes for the families built from I c0 ) and I cb) by the repeated 
action of at. 
</p>
<p>We now calculate the constants Cc and Cc+ 1 appearing in Eqs. (7.4.12) and 
(7.4.14). Since c=n + 1/2, let us label the kets by the integer n. We want to determine 
the constant Cn appearing in the equation 
</p>
<p>aln)= Cnln-1) 
</p>
<p>Consider the adjoint of this equation 
</p>
<p>(nlat =(n-1IC~ 
</p>
<p>By combining these equations we arrive at 
</p>
<p>(nlataln)= (n-lln-1)C~C 
</p>
<p>(niH- ~In)= C~Cn (since In -1) is normalized) 
</p>
<p>(nlnln) =I Cnl 2 (since ffln) = (n+ 1/2)ln)) 
</p>
<p>1Cnl 2 =n 
</p>
<p>Cn=(n) 112 eit/' (cpisarbitrary) 
</p>
<p>(7.4.19a) 
</p>
<p>(7.4.19b) 
</p>
<p>(7.4.20) </p>
<p/>
</div>
<div class="page"><p/>
<p>It is conventional to choose &lt;/&gt; as zero. So we have 
</p>
<p>(7.4.21) 
</p>
<p>It can similarly be shown (by you) that 
</p>
<p>(7.4.22) 
</p>
<p>[Note that in Eqs. (7.4.21) and (7.4.22) the larger of then's labeling the two kets 
</p>
<p>appears under the square root.] By combining these two equations we find 
</p>
<p>(7.4.23) 
</p>
<p>In terms of 
</p>
<p>N=a'a (7.4.24) 
</p>
<p>called the number operator (since it counts the quanta) 
</p>
<p>-, I 
</p>
<p>H=1V+: (7.4.25) 
</p>
<p>Equations (7.4.21) and (7.4.22) are very important. They allow us to compute 
</p>
<p>the matrix elements of all operators in the In) basis. First consider a and at 
</p>
<p>themselves: 
</p>
<p>(7.4.26) 
</p>
<p>(7.4.27) 
</p>
<p>To find the matrix elements of X and P, we invert Eqs. (7.4.3) and (7.4.4) to obtain 
</p>
<p>(a+at) (7.4.28) 
</p>
<p>(7.4.29) 
</p>
<p>and then usc Eqs. (7.4.26) and (7.4.27). The details are left as an exercise. The two 
</p>
<p>basic matrices in this energy basis are 
</p>
<p>n=On=ln=2 
</p>
<p>n=O () 0 () 
</p>
<p>n=l 11/2 0 0 
t 
</p>
<p>&lt;-&gt; n=2 0 21;2 0 (7.4.30) a 
</p>
<p>0 () 
</p>
<p>207 
</p>
<p>THE HARMONIC 
OSCILLATOR </p>
<p/>
</div>
<div class="page"><p/>
<p>208 
</p>
<p>CHAPTER 7 
</p>
<p>and its adjoint 
</p>
<p>0 II /2 0 0 
</p>
<p>0 0 21/2 0 
a&lt;--+ 
</p>
<p>0 0 0 11/2 
(7 .4.31) 
</p>
<p>Both matrices can be constructed either from Eqs. (7.4.26) and (7.4.27) or Eqs. 
</p>
<p>(7.4.21) and (7.4.22) combined with our mnemonic involving images of the trans-
</p>
<p>formed vectors {/in) and aln). We get the matrices representing X and P by turning 
to Eqs. (7.4.28) and (7.4.29): 
</p>
<p>/ \ I:] 
</p>
<p>p &lt;--&gt; f!~~~ ) 
\ 2 I 
</p>
<p>0 112 0 0 
11/2 
</p>
<p>0 
II&middot; 2 
</p>
<p>0 
</p>
<p>0 
</p>
<p>0 
</p>
<p>0 
</p>
<p>0 
</p>
<p>i 2 
</p>
<p>0 
</p>
<p>-112 
</p>
<p>0 
</p>
<p>i2 
</p>
<p>0 
</p>
<p>0 
</p>
<p>0 0 
</p>
<p>0 
</p>
<p>0 -312 
</p>
<p>31 0 
</p>
<p>The Hamiltonian is of course diagonal in its own basis: 
</p>
<p>H &bull;&middot;&middot;&bull; 1iw 
0 
</p>
<p>0 
</p>
<p>0 0 0 
</p>
<p>3/2 0 0 
</p>
<p>0 5/2 
</p>
<p>(7.4.32) 
</p>
<p>(7.4.33) 
</p>
<p>( 7.4.34) 
</p>
<p>Equation (7.4.22) also allows us to express all normalized eigenvectors In) in terms 
</p>
<p>of the ground state IO): 
</p>
<p>(7.4.35) 
</p>
<p>The a and a&bull; operators greatly facilitate the calculation of the matrix of elements of 
</p>
<p>other operators between osciiiator eigenstates. Consider, for example, (3IX3 12). In </p>
<p/>
</div>
<div class="page"><p/>
<p>the X basis one would have to carry out the following integral: 
</p>
<p>whereas in the In) basis 
</p>
<p>( 1i \)
3 12 
</p>
<p>1 2 t &middot;~ l t 
= -- (31(a&middot; +a a +aa a+aa a 
</p>
<p>2mm 
</p>
<p>+at aa +a1aat +at at a+ atatat)l2) 
</p>
<p>Since a lowers n by one unit and at raises it by one unit and we want to go up by 
. f " 3 h I .b . f' t t t f one umt rom n = L. ton= , t eon y nonzero contn utwn comes rom a a a, aa a , 
</p>
<p>+ t 
and a aa. Now 
</p>
<p>so that 
</p>
<p>What if we want not some matrix element of X, but the probability of finding 
</p>
<p>the particle in In) at position x? We can of course fall back on Postulate III, which 
tells us to find the eigenvectors lx) of the matrix X [Eq. (7.4.32)] and evaluate the 
</p>
<p>inner product (xln). A more practical way will be developed in the next section. 
</p>
<p>Consider a remarkable feature of the above solution to the eigenvalue problem 
of H. Usually we work in the X basis and set up the eigenvalue problem (as a 
</p>
<p>differential equation) by invoking Postulate II, which gives the action of X and Pin 
</p>
<p>the X basis (X -&gt;x, P-.-ili djdx). In some cases (the linear potential problem), the 
P basis recommends itself, and then we use the Fourier-transformed version of 
Postulate II, namely, X -&gt;iii djdp, P-.p. In the present case we could not transform 
this operator assignment to the energy eigenbasis, for to do so we first had to solve 
for the energy eigenfunctions in the X basis, which was begging the question. Instead 
we used just the commutation relation [X, P] =in, which follows from Postulate II, 
but is true in all bases, in particular the energy basis. Since we obtained the complete 
</p>
<p>209 
</p>
<p>THE HARMONIC 
OSCJLLATOR </p>
<p/>
</div>
<div class="page"><p/>
<p>210 
</p>
<p>CHAPTER 7 
</p>
<p>solution given just this information, it would appear that the essence of Postulate II 
is just the commutator. This in fact is the case. In other words, we may trade our 
</p>
<p>present Postulate n for a more general version: 
</p>
<p>Postulate 11. The independent variables x and p of classical mechanics now 
</p>
<p>become Hermitian operators X and P defined by the canonical commutator 
</p>
<p>[X, P] =in. Dependent variables m(x, p) are given by operators n = 
m(x-+X, p--&gt; P). 
</p>
<p>To regain our old version, we go to the X basis. Clearly in its own basis X -&gt;x. 
</p>
<p>We must then pick P such that [X, P] = di. If we make the conventional choice P = 
-in d/ dx, we meet this requirement and arrive at Postulate II as stated earlier. But 
the present version of Postulate II allows us some latitude in the choice of P, for 
</p>
<p>we can add to --&middot;i1i d/dx any function of x without altering the commutator: the 
assignment 
</p>
<p>X------+ X 
X basis 
</p>
<p>d 
p -----&gt; - ifj -------- + f( X) 
</p>
<p>Xbasis dx 
</p>
<p>(7.4.36a) 
</p>
<p>(7.4.36b) 
</p>
<p>is equally satisfactory. Now, it is not at all obvious that in every problem (and not 
</p>
<p>just the harmonic oscillator) the same physics will obtain if we make this our starting 
</p>
<p>point. For example if we project the eigenvalue equation 
</p>
<p>Pip) =pip) (7.4.37a) 
</p>
<p>onto the X basis, we now get 
</p>
<p>l-ifl 1_ + j(x)_]lflp(X) =p~!p(X) dx __ (7.4.37b) 
from which it follows that ljlp(x) is no longer a plane wave x.eipx r,. How can the 
</p>
<p>physics be the same as before? The answer is that the wave function is never measured 
</p>
<p>directly. What we do measure are probabilities l&lt;mlij!)l 2 for obtaining some result 
m when n is measured, squares of matrix elements I&lt; lf/1l !11vr 2 )1 2 , or the eigenvalue 
spectrum of operators such as the Hamiltonian. In one of the exercises that follows, 
</p>
<p>you will be guided toward the proof that these measurable quantities are in fact left 
</p>
<p>invariant under the change to the nontraditional operator assignment Eq. (7 .4.36 ). 
Dirac emphasized the close connection between the commutation rule 
</p>
<p>[X, P] =in 
</p>
<p>of the quantum operators and the Poisson brackets (PB) of their classical 
</p>
<p>counterparts 
</p>
<p>{x,p}=l </p>
<p/>
</div>
<div class="page"><p/>
<p>which allows us to write the defining relation of the quantum operators as 
</p>
<p>[X, P] = i1i{x, p} = i1i (7.4.38) 
</p>
<p>The virtue of this viewpoint is that its generalization to the "quantization" of 
</p>
<p>a system of N degrees of freedom is apparent: 
</p>
<p>Postulate II (For N Degrees of Freedom). The Cartesian coordinates x 1 , &bull;&bull;&bull; , xN 
</p>
<p>and momenta p 1 , &bull;&bull; , PN of the classical description of a system with N degrees 
</p>
<p>of freedom now become Hermitian operators X1, .. , XN; P1, ... , PN obeying 
</p>
<p>the commutation rules 
</p>
<p>[X;, Pj] =i1i{x;,pj} =if18ii 
</p>
<p>[X;, Xj] = i1i{x;, xj} = 0 
</p>
<p>[P;, Pj] = i1i{p;,pj} = 0 
</p>
<p>Similarly ro(x,p)-+ro(x-+X,p-+P) = 0.. 
</p>
<p>(7.4.39) 
</p>
<p>[We restrict ourselves to Cartesian coordinates to avoid certain subtleties associated 
</p>
<p>with the quantization of non-Cartesian but canonical coordinates; see Exercise 
</p>
<p>(7.4.10). Once the differential equations are obtained, we may abandon Cartesian 
</p>
<p>coordinates in looking for the solutions.] 
</p>
<p>It if evident that the generalization provided towards the end of Section 4.2, 
</p>
<p>namely, 
</p>
<p>X;----+x; 
X basis 
</p>
<p>a 
P;----+-i1i-
</p>
<p>Xbasis OX; 
</p>
<p>is a choice but not the choice satisfying the canonical commutation rules, Eq. (7.4.39), 
</p>
<p>for the same reason as in the N = 1 case. 
</p>
<p>Given the commutation relations between X and P, the ones among dependent 
</p>
<p>operators follow from the repeated use of the relations 
</p>
<p>[0., Ar] = A[O., r] + [0., A]r 
</p>
<p>and 
</p>
<p>[OA, r]=O[A, r]+[O, r]A 
</p>
<p>Since PB obey similar rules (Ex~rcise 2.7.1) except for the lack of emphasis on 
</p>
<p>ordering of the classical variables, it turns out that if 
</p>
<p>{ro(x,p), A.(x,p)}=r(x,p) 
</p>
<p>211 
</p>
<p>THE HARMONIC 
</p>
<p>OSCILLATOR </p>
<p/>
</div>
<div class="page"><p/>
<p>212 
</p>
<p>CHAPTER 7 
</p>
<p>then 
</p>
<p>[Q(X, P), t\(X, P)] = i1ir(X, P) (7.4.40) 
</p>
<p>except for differences arising from ordering ambiguities; hence the formal similarity 
between classical and quantum mechanics, first encountered in Chapter 6. 
</p>
<p>Although the new form of postulate II provides a general, basis-independent 
specification of the quantum operators corresponding to classical variables, that is 
to say for "quantizing," in practice one typically works in the X basis and also 
ignores the latitude in the choice of P; and sticks to the traditional one, P;= 
-i1i 8/ox;, which leads to the simplest differential equations. The solution to the 
oscillator problem, given just the commutation relations (and a little help from 
Dirac) is atypical. 
</p>
<p>Exercise 7.4.1. * Compute the matrix elements of X and P in the In) basis and compare 
with the result from Exercise 7.3.4. 
</p>
<p>Exercise 7.4.2. * Find (X), (P), (X2), (P 2), M &middot; !J.P in the state In). 
</p>
<p>Exercise 7.4.3. * ( Virial Theorem). The virial theorem in classical mechanics states that 
for a particle bound by a potential V(r) = ar", the average (over the orbit) kinetic and potential 
energies are related by 
</p>
<p>T=c(k) V 
</p>
<p>when c(k) depends only on k. Show that c(k) =k/2 by considering a circular orbit. Using the 
results from the previous exercise show that for the oscillator (k = 2) 
</p>
<p>(T)=(V) 
</p>
<p>in the quantum state In). 
</p>
<p>Exercise 7.4.4. Show that (niX 4 In) = (1l/2mw )2[3 + 6n(n + 1)]. 
</p>
<p>Exercise 7.4.5.* At t=O a particle starts out in IVF(O))=I/z!i2(IO)+Il)). (I) Find 
I VF(t)); (2) find (X(O)) = &lt; VF(O)IXI VF(O)), (P(O)), (X(t)), (P(t)); (3) find (X(t)) and (P(t)) 
using Ehrenfest's theorem and solve for (X(t)) and (P(t)) and compare with part (2). 
</p>
<p>Exercise 7.4.6. * Show that (a(t)) = e-'"'' (a(O)) and that (at(t)) = e'"''(at(O)). 
</p>
<p>Exercise 7.4.7. Verify Eq. (7.4.40) for the case 
</p>
<p>(!) Q=X, A=X2 +P 2 
</p>
<p>(2) Q=X2, A=P 2 
</p>
<p>The second case illustrates the ordering ambiguity. </p>
<p/>
</div>
<div class="page"><p/>
<p>Exercise 7.4.8. * Consider the three angular momentum variables in classical mechanics: 
</p>
<p>ly=zpx-XP= 
</p>
<p>(I) Construct Lx, L,., and L=, the quantum counterparts, and note that there are no ordering 
</p>
<p>ambiguities. 
</p>
<p>(2) Verify that {lx. fy}=l= [see Eq. (2.7.3) for the definition of the PB]. 
</p>
<p>(3) Verify that [Lx, L,.]=ifiL=. 
</p>
<p>Exercise 7.4.9 (Important). Consider the unconventional (but fully acceptable) operator 
</p>
<p>choice 
</p>
<p>in the X basis. 
</p>
<p>X-&gt;x 
</p>
<p>d 
P-&gt;-ifi-+f(x) 
</p>
<p>dx 
</p>
<p>(1) Verify that the canonical commutation relation is satisfied. 
</p>
<p>(2) It is possible to interpret the change in the operator assignment as a result of a unitary 
</p>
<p>change of the X basis: 
</p>
<p>where 
</p>
<p>g(x) = r f(x') dx' 
First verify that 
</p>
<p>(.XIXIi') =xo(x- x') 
</p>
<p>i.e., 
</p>
<p>X X 
new X ba~is 
</p>
<p>Next verify that 
</p>
<p>213 
</p>
<p>THE HARMONIC 
</p>
<p>OSCILLATOR </p>
<p/>
</div>
<div class="page"><p/>
<p>214 
</p>
<p>CHAPTER 7 
</p>
<p>I.e .&bull; 
</p>
<p>d 
P &middot;iti--+f(x) 
</p>
<p>new J.;'' basis dx 
</p>
<p>This exercise teaches us that the ".X basis" is not unique; given a basis lx), we can get another 
</p>
<p>lx), by multiplying by a phase factor which changes neither the norm nor the orthogonality. 
The matrix elements of P change with f the standard choice corresponding to f= 0. Since the 
presence off is related to a change of basis, the in variance of the physics under a change in 
</p>
<p>f (from zero to nonzero) follows. What is novel here is that we are changing from one X basis 
to another X basis rather than to some other n basis. Another lesson to remember is that 
two different differential operators w(x, -ifi d/dx) and w(x, -ifid/dx+.f) can have the same 
</p>
<p>eigenvalues and a one-to-one correspondence between their eigenfunctions, since they both 
</p>
<p>represent the same abstract operator U(X, P). D 
</p>
<p>Exercise 7.4. !0. * Recall that we always quantize a system by promoting the Cartesian 
coordinates x,, ... , xN; and momenta p,, ... , PN to operators obeying the canonical commu-
</p>
<p>tation rules. If non-Cartesian coordinates seem more natural in some cases, such as the 
</p>
<p>eigenvalue problem of a Hamiltonian with spherical symmetry, we first set up the differential 
</p>
<p>equation in Cartesian coordinates and then change to spherical coordinates (Section 4.2). In 
</p>
<p>Section 4.2 it was pointed out that if :ft is written in terms of non-Cartesian but canonical 
</p>
<p>coordinates q1 &bull;&bull;&bull; q"'; p 1 .. &bull; pN; :tt(q,&middot;&middot;&middot;&bull;q,,p,&middot;&middot;&middot;&middot;&bull;-ifi does not generate the correct 
</p>
<p>Hamiltonian H, even though the operator assignment satisfies the canonical commutation 
</p>
<p>rules. In this section we revisit this problem in order to explain some of the subtleties arising 
</p>
<p>in the direct quantization of non-Cartesian coordinates without the use of Cartesian coordi-
</p>
<p>nates in intennediate stages. 
</p>
<p>(I) Consider a particle in two dimensions with 
</p>
<p>which leads to 
</p>
<p>4P _ p~. + p; + ( 2 + 2) I&bull; 2 
oft&middot;&middot;&middot;&middot;&middot; ax y 
</p>
<p>2m 
</p>
<p>.... tz2 
H-+&middot;--
</p>
<p>2m 
</p>
<p>in the coordinate basis. Since the problem has rotational symmetry we use polar coordinates 
</p>
<p>!/&gt;""tan-' (y/x) 
</p>
<p>in terms of which 
</p>
<p>- n2 1 ?2 1 i' 1 a' ') 
H ...... __, --(-2 +--::- + 2-=--:; + ap 
</p>
<p>co~~;~atc 2m \ap p cp p t-7&cent; ) 
(7.4.41) 
</p>
<p>Since p and 4&gt; are not mixed up as x andy are [in the (x2 +y2 ) 1 2 term] the polar version can 
</p>
<p>be more readily solved. </p>
<p/>
</div>
<div class="page"><p/>
<p>The question we address is the following: why not start with .Jif expressed in terms of 
</p>
<p>polar coordinates and the conjugate momenta 
</p>
<p>(where eP is the unit vector in the radial direction), and 
</p>
<p>i.e., 
</p>
<p>p41 =xpy-YPx (the angular momentum, also called l,) 
</p>
<p>2 2 
</p>
<p>.Jif=PP +__!'L+ap (verify this) 
2m 2mp2 
</p>
<p>and directly promote all classical variables p, PP, c/J, and p 41 to quantum operators obeying 
</p>
<p>the canonical commutations rules? Let's do it and see what happens. If we choose operators 
</p>
<p>a 
p -&gt;-i11-
</p>
<p>p ap 
</p>
<p>that obey the commutation rules, we end up with 
</p>
<p>(7.4.42) 
</p>
<p>which disagrees with Eq. (7.4.41). Now this in itself is not serious, for as seen in the last 
</p>
<p>exercise the same physics may be hidden in two different equations. In the present case this 
</p>
<p>isn't true: as we will see, the Hamiltonians in Eqs. (7.4.41) and (7.4.42) do not have the same 
</p>
<p>eigenvalued We know Eq. (7.4.41) is the correct one, since the quantization procedure in 
</p>
<p>terms of Cartesian coordinates has empirical support. What do we do now? 
</p>
<p>(2) A way out is suggested by the fact that although the choice Pp-&gt;-i11 Ojap leads to 
</p>
<p>the correct commutation rule, it is not Hermitian! Verify that 
</p>
<p>f'"'' J2" ( a )* # 0 0 -jfj a:' lf/2pdpdc/J 
= (Pplf/IIIf/2) 
</p>
<p>(You may assume plflflf!z-+0 as p-+0 or oo. The problem comes from the fact that pdp dc/J 
</p>
<p>and not dp dc/J is the measure for integration.) 
</p>
<p>t What we will see is that Pp= -ill djdp, and hence the H constructed with it, are non-Hermitian. 
</p>
<p>215 
</p>
<p>THE HARMONIC 
</p>
<p>OSCILLATOR </p>
<p/>
</div>
<div class="page"><p/>
<p>216 
</p>
<p>CHAPTER 7 
</p>
<p>Show, however, that 
</p>
<p>(7.4.43) 
</p>
<p>is indeed Hermitian and also satisfies the canonical commutation rule. The angular momentum 
P,r-&gt;-ili o/oc/J is Hermitian, as it stands, on single-valued functions: ljl(p, c/J)= 'l'(p, rjJ+2n). 
</p>
<p>(3) In the Cartesian case we saw that adding an arbitrary f(x) to -iii of ox didn't have 
any physical effect, whereas here the addition of a function of p to -iii a fop seems important. 
Why? [Isf(x) completely arbitrary? Mustn't it be real? Why? Is the same true for the -ili/2p 
piece?] 
</p>
<p>(4) Feed in the new momentum operator Pp and show that 
</p>
<p>-li2 ( i12 1 i1 1 I i1 2 ) H------+ -- -+- ---+-- +ap 
coordinate 2m i}p2 p op 4p2 p2 i1rp2 
</p>
<p>baSlS 
</p>
<p>which still disagrees with Eq. (7 .4.41 ). We have satisfied the commutation rules, chosen Hermi-
tian operators, and yet do not get the right quantum Hamiltonian. The key to the mystery 
lies in the fact that .Yt' doesn't determine H uniquely since terms of order li (or higher) may 
be present in H but absent in .Yt'. While this ambiguity is present even in the Cartesian case, 
it is resolved by symmetrization in all interesting cases. With non-Cartesian coordinates the 
ambiguity is more severe. There are ways of constructing H given .Yt' (the path integral 
formulation suggests one) such that the substitution Pp-+-ili(iJ/iJp+ i/2p) leads to Eq. 
(7.4.41). In the present case the quantum Hamiltonian corresponding to 
</p>
<p>2 2 
VP PP pq, 
.n. =-+--+ap 
</p>
<p>2m 2mp2 
</p>
<p>is given by 
</p>
<p>(7.4.44) 
</p>
<p>Notice that the additional term is indeed of nonzero order in li. 
</p>
<p>We will not get into a discussion of these prescriptions for generating H 
since they finally reproduce results more readily available in the approach we are 
adopting. 0 
</p>
<p>7 .5. Passage from the Energy Basis to the X Basis 
</p>
<p>It was remarked in the last section that although the In) basis was ideally suited 
for evaluating the matrix elements of operators between oscillator eigenstates, the 
amplitude for finding the particle in a state In) at the point x could not be readily 
computed: it seemed as if one had to find the eigenkets lx) of the operators X [Eq. 
(7.4.32)] and then take the inner product &lt;xln). But there is a more direct way to 
get tp'n(x) = (xln). </p>
<p/>
</div>
<div class="page"><p/>
<p>We start by projecting the equation defining the ground state of the oscillator 
</p>
<p>aiO)=O 
</p>
<p>on the X basis: 
</p>
<p>In terms of y = (mro jn) 112x, 
</p>
<p>For later use we also note that (since djdy is anti-Hermitian), 
</p>
<p>In the X basis Eq. (7.5.1) then becomes 
</p>
<p>or 
</p>
<p>or 
</p>
<p>or 
</p>
<p>dlf/o(Y) 
--=-ydy 
lf/o( y) 
</p>
<p>1/fo(Y) = Ao 
</p>
<p>(7.5.1) 
</p>
<p>(7.5.2) 
</p>
<p>(7.5.3) 
</p>
<p>(7.5.4) 
</p>
<p>(7.5.5) 
</p>
<p>217 
THE HARMONIC 
</p>
<p>OSCH.LATOR </p>
<p/>
</div>
<div class="page"><p/>
<p>218 
</p>
<p>CHAPTER 7 
</p>
<p>or (upon normalizing) 
</p>
<p>-('mw.) 1&middot;4 . (1 mc.ox2 \) --- -~ t:Xp ---
' 7[ 1i / ' \ 21i ! 
</p>
<p>(7.5.6) 
</p>
<p>By projecting the equation 
</p>
<p>, . (a+)" 
n)=--.,10) 
</p>
<p>&bull; (n!) I L 
</p>
<p>onto the X basis, we get the normalized eigenfunctions 
</p>
<p>[ 
( fi ,1:&bull;. "l l ; d\l"'. ,1,4 
</p>
<p>&lt;xln)=v;, x=l-) yj=--\:;. )--;-i(y--1 (.'-~OJ_) 
\moJ 1 __ (n!) 2 \ dy / n:fi 1 
</p>
<p>(7.5.7) 
</p>
<p>A comparison of the above result with Eq. (7.3.22) shows that 
</p>
<p>-2 
(7.5.8) 
</p>
<p>We now conclude our rather lengthy discussion of the oscillator. If you understand 
</p>
<p>this chapter thoroughly, you should have a good grasp of how quantum mechanics 
</p>
<p>works. 
</p>
<p>Exercise 7.5.1. Project Eq. (7.5.1) on the P basis and obtain lf/o(p). 
</p>
<p>Exercise 7.52 Project the relation 
</p>
<p>ain)=n' "In-!) 
</p>
<p>on the X basis and derive the recursion relation 
</p>
<p>H;,(y)=2nH, 1(y) 
</p>
<p>using Eq. (7.3.22)_ 
</p>
<p>E.&gt;cercise 7.5.3. Starting with 
</p>
<p>a-+-(/=2 1 \ 
</p>
<p>and 
</p>
<p>(a+ =n 12 ln l)-t(n-tl) 12ln+1) </p>
<p/>
</div>
<div class="page"><p/>
<p>and Eq. (7.3.22). derive the relation 
</p>
<p>Exercise 7.5.4. * Thermodynamics of Oscillators. The Boltzman formula 
</p>
<p>P(i)=e f3EUJ/Z 
</p>
<p>where 
</p>
<p>Z=L:e fJEuJ 
</p>
<p>gives the probability of finding a system in a state i with energy E(i), when it is in thermal 
</p>
<p>equilibrium with a reservoir of absolute temperature T = l / f3k, k = 1.4 x l 0 16 ergs;o K; being 
</p>
<p>Holtzman's constant. (The "probability" referred to above is in relation to a classical ensemble 
</p>
<p>of similar systems and has nothing to do with quantum mechanics.) 
</p>
<p>(I) Show that the thermal average of the system's energy is 
</p>
<p>- ----() 
E= L E(i)P(i) =-::;--In Z 
</p>
<p>i cf3 
</p>
<p>(2) Let the system be a classical oscillator, The index i is now continuous and corresponds 
</p>
<p>to the variables x and p describing the state of the oscillator, i.e., 
</p>
<p>i-&bull;x,p 
</p>
<p>and 
</p>
<p>~-&gt;JJdx dp 
</p>
<p>and 
</p>
<p>Show that 
</p>
<p>and that 
</p>
<p>Note that Ec1 is independent of m and w. 
</p>
<p>2rr 
</p>
<p>w/3 
</p>
<p>219 
</p>
<p>THE HARMONIC 
</p>
<p>OSCILLATOR </p>
<p/>
</div>
<div class="page"><p/>
<p>220 
</p>
<p>CHAPTER 7 
</p>
<p>(3) For the quantum oscillator the quantum number n plays the role of the index i. Show 
that 
</p>
<p>and 
</p>
<p>(4) It is intuitively clear that as the temperature Tincreases (and f3 = 1/kT decreases) the 
oscillator will get more and more excited and eventually (from the correspondence principle) 
</p>
<p>Verify that this is indeed true and show that "large T" means T&raquo;limjk. 
(5) Consider a crystal with No atoms, which, for small oscillations, is equivalent to 3N0 
</p>
<p>decoupled oscillators. The mean thermal energy of the crystal Ecrystal is Ec1 or Equ summed 
over all the normal modes. Show that if the oscillators are treated classicaly, the specific heat 
per atom is 
</p>
<p>c I ( T) =_I__ a Ecrystal = 3k 
c No oT 
</p>
<p>which is independent of T and the parameters of the oscillators and hence the same for all 
crystals.t This agrees with experiment at high temperatures but not as T --&gt;0. Empirically, 
</p>
<p>C(T)--&gt;3k (Tiarge) 
</p>
<p>--&gt;0 (T--&gt;0) 
</p>
<p>Following Einstein, treat the oscillators quantum mechanically, asuming for simplicity that 
they all have the same frequency m. Show that 
</p>
<p>where (}e= lim/k is called the Einstein temperature and varies from crystal to crystal. Show 
that 
</p>
<p>Cqu(T)-----+ 3k 
T:&gt;-::&gt;lJE 
</p>
<p>Although Cqu( T) --&gt;0 as T -+0, the exponential falloff disagrees with the observed 
C(T) --&gt;r-o T 3 behavior. This discrepancy arises from assuming that the frequencies of all 
</p>
<p>t More precisely, for crystals whose atoms behave as point particles with no internal degrees of freedom. </p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 7.3. Comparison of experiment with Einstein's 
</p>
<p>theory for the specific heat in the case of diamond. ( e E is 
chosen to be 1320 K.) 
</p>
<p>2k 
</p>
<p>C(-r) 
</p>
<p>I k 
</p>
<p>0 0.4 0.6 0.8 1.0 
T/BE 
</p>
<p>normal modes are equal, which is of course not generally true. [Recall that in the case of two 
</p>
<p>coupled masses we get w 1 = (k/m) 112 and w 11 = (3kjm) 112 .] This discrepancy was eliminated 
</p>
<p>by Debye. 
</p>
<p>But Einstein's simple picture by itself is remarkably successful (see Fig. 7.3). 
</p>
<p>221 
</p>
<p>THE HARMONIC 
</p>
<p>OSCILLATOR </p>
<p/>
</div>
<div class="page"><p/>
<p>The Path Integral Formulation 
</p>
<p>of Quantum Theory 
</p>
<p>8 
</p>
<p>We consider here an alternate formulation of quantum mechanics invented by 
</p>
<p>Feynman in the forties.t In contrast to the Schrodinger formulation, which stems 
</p>
<p>from Hamiltonian mechanics, the Feynman formulation is tied to the Lagrangian 
</p>
<p>formulation of mechanics. Although we are committed to the former approach, we 
</p>
<p>discuss in this chapter Feynman's alternative, not only because of its aesthetic value, 
</p>
<p>but also because it can, in a class of problems, give the full propagator with tremend-
</p>
<p>ous ease and also give valuable insight into the relation between classical and 
</p>
<p>quantum mechanics. 
</p>
<p>8.1. The Path Integral Recipe 
</p>
<p>We have already seen that the quantum problem is fully solved once the propa-
</p>
<p>gator is known. Thus far our practice has been to first find the eigenvalues and 
</p>
<p>eigenfunctions of H, and then express the propagator U(t) in terms of these. In the 
</p>
<p>path integral approach one computes U(t) directly. For a single particle in one 
</p>
<p>dimension, the procedure is the following. 
</p>
<p>To find U(x, t; x', t'): 
</p>
<p>(1) Draw all paths in the x-t plane connecting (x', t') and (x, t) (see Fig. 8.1). 
</p>
<p>(2) Find the action S[x(t)] for each path x(t). 
(3) U(x,t;x',t')=A I eiS[x(I)J/~ (8.1.1) 
</p>
<p>all paths 
</p>
<p>where A is an overall normalization factor. 
</p>
<p>t The nineteen forties that is, and in his twenties. An interesting account of how he was influenced by 
Dirac's work in the same direction may be found in his Nobel lectures. See, Nobel Lectures-Physics, 
</p>
<p>Vol. III, Elsevier Publication, New York (1972). 223 </p>
<p/>
</div>
<div class="page"><p/>
<p>224 
</p>
<p>CHAPTER 8 
</p>
<p>&bull; g (x,t) 
+ 
</p>
<p>:.-
</p>
<p>(x!t') Figure 8.1. Some of the paths that contribute to the propagator. The 
contribution from the path x(t) is Z=exp{iS[x(t)]/:li}. 
</p>
<p>8.2. Analysis of the Recipe 
</p>
<p>Let us analyze the above recipe, postponing for a while the proof that it repro-
duces conventional quantum mechanics. The most surprising thing about it is the 
fact that every path, including the classical path, xc1 (t), gets the same weight, that 
is to say, a number of unit modulus. How are we going to regain classical mechanics 
in the appropriate limit if the classical path does not seem favored in any way? 
</p>
<p>To understand this we must perform the sum in Eq. (8.1.1). Now, the correct 
way to sum over all the paths, that is to say, path integration, is quite complicated 
and we will discuss it later. For the present let us take the heuristic approach. Let 
us first pretend that the continuum of paths linking the end points is actually a 
discrete set. A few paths in the set are shown in Fig. 8.1. 
</p>
<p>We have to add the contributions Za = e;srx.(t)J/Ii fwm each path Xa(t). This 
summation is done schematically in Fig. 8.2. Since each path has a different action, 
it contributes with a different phase, and the contributions from the paths essentially 
cancel each other, until we come near the classical path. Since S is stationary here, 
the Z's add constructively and produce a large sum. As we move away from xc1 (t), 
destructive interference sets in once again. It is clear from the figure that U(t) is 
dominated by the paths near xc1 (t). Thus the classical path is important, not because 
it contributes a lot by itself, but because in its vicinity the paths contribute coherently. 
</p>
<p>How far must we deviate from Xc1 before destructive interference sets in? One 
may say crudely that coherence is lost once the phase differs from the stationary 
value S[xc1 (t)]j1i = Sc~/1i by about 1r. This in turn means that the action for the 
-;oherence paths must be within lire of Sc~. For a macroscopic particle this means a 
very tight constraint on its path, since Sc1 is typically ~ 1 erg sec~ 1 0271i, while for 
m electron there is quite a bit of latitude. Consider the following example. A free 
particle leaves the origin at t = 0 and arrives at x = 1 em at t = 1 second. The classical 
path is 
</p>
<p>x=t (8.2.1) 
</p>
<p>Figure 8.2. Schematic representation of the sum ~z &bull;. 
Paths near x" (I) contribute coherently since Sis station&middot; 
ary there, while others cancel each other out and may 
be ignored in the first approximation when we calculate 
U(l). </p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 8.3. Two possible paths connecting (0, 0) and ( 1, 1). The 
</p>
<p>action on the classical path x = t is m/2, while on the other. it is 
</p>
<p>2m/3. 
</p>
<p>Consider another path 
</p>
<p>which also links the two space-time points (Fig. 8.3.) 
</p>
<p>11''&middot;" ______ ..,. 
(8.2.2) 
</p>
<p>For a classical particle, of mass, say 1 g, the action changes by roughly 
</p>
<p>1.6 x 102611, and the phase by roughly l.6 x 1026 rad as we move from the classical 
</p>
<p>path x=t to the nonclassical path x=P. We may therefore completely ignore the 
nonclassical path. On the other hand, for an electron whose mass is ~ l0-27 g, as~ 
</p>
<p>11/6 and the phase change is just around a sixth of a radian, which is well within the 
coherence range 8S/1i-::; rr. It is in such cases that assuming that the particle moves 
along a well-defined trajectory, xc1 (t), leads to conflict with experiment. 
</p>
<p>8.3. An Approximation to U(t) for a Free Particle 
</p>
<p>Our previous discussions have indicated that, to an excellent approximation, we 
may ignore all but the classical path and its neighbors in calculating U(t). Assuming 
</p>
<p>that each of these paths contributes the same amount exp(iSc1/n), since 5' is station-
ary, we get 
</p>
<p>(8.3.1) 
</p>
<p>where A' is some normalizing factor which "measures" the number of paths in the 
</p>
<p>coherent range. Let us find U(t) for a free particle in this approximation and compare 
</p>
<p>the result with the exact result, Eq. (5.1.10). 
The classical path for a free particle is just a straight line in the x-t plane: 
</p>
<p>' ( ") , x&middot;----&middot;x . " , xc1 t = x + ~ - ( t - t ) 
t- t' 
</p>
<p>(8.3.2) 
</p>
<p>corresponding to motion with uniform velocity v=(x-x')/(t-t'). Since !&pound;'= 
mv2 /2 is a constant, 
</p>
<p>sci= I, !&pound;' dt" = -1- m (~--=:.\J~ 
( 2 t--t' 
</p>
<p>225 
</p>
<p>THE PATH 
</p>
<p>INTEGRAL 
</p>
<p>FORMULATlON 
</p>
<p>OF QUANTUM 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>226 
</p>
<p>CHAPTER 8 
</p>
<p>so that 
</p>
<p>, , , lim(x-x')2] U(x, t; x, t )=A exp 
21i(t- t') 
</p>
<p>(8.3.3) 
</p>
<p>To find A', we use the fact that as t-t' tends to 0, U must tend to 8(x-x'). 
Comparing Eq. (8.3.3) to the representation of the delta function encountered in 
Section 1.10 (see footnote on page 61), 
</p>
<p>s: &middot; , &bull; I l (x- x') 2] u(x-x )=hm 2 112 exp - 2 
A~o (n~ ) ~ 
</p>
<p>(valid even if~ is imaginary) we get 
</p>
<p>l ]: '2 A'= m 
2n1ii(t- r') 
</p>
<p>so that 
</p>
<p>( )1/2 l 2] , , m im(x-x') U(x, t; x, 0)= U(x, t; x) = -- exp 
2n1iit 21it 
</p>
<p>(8.3.4) 
</p>
<p>which is the exact answer! We have managed to get the exact answer by just comput-
ing the classical action! However, we will see in Section 8.6 that only for potentials 
of the form V=a+bx+cx2 +dx+exx is it true that U(t)=A(t) eiSdfli. Furthermore, 
we can't generally find A(t) using U(x, 0; x') = 8(x- x') since A can contain an 
arbitrary dimensionless functionfsuch that/--+ 1 as t---+0. Heref= 1 because we can't 
construct a nontrivial dimensionless fusing just m, 1i, and t (check this). 
</p>
<p>8.4. Path Integral Evaluation of the Free-Particle Propagator 
</p>
<p>Although our heuristic analysis yielded the exact free-particle propagator, we 
will now repeat the calculation without any approximation to illustrate path 
integration. 
</p>
<p>Consider U(xN, tN; x0, to). The peculiar labeling of the end points will be just-
ified later. Our problem is to perform the path integral 
</p>
<p>f'.v eiS[xUJ]:Iig[x(t)] 
xn 
</p>
<p>(8.4.1) 
</p>
<p>where 
</p>
<p>rN f2'[X(f)] 
X() </p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 8.4. The discrete approximation to a path 
</p>
<p>x(t). Each path is specified by N-l numbers 
</p>
<p>x(t1), ... , x(IN- 1). To sum over paths we must 
</p>
<p>integrate each X; from -oo to +oo. Once all inte-
</p>
<p>grations are done, we can take the limit N-+ oo. 
</p>
<p>x(t;) 
</p>
<p>is a symbolic way of saying "'mtegrate over all paths connecting x0 and xN (in the 
</p>
<p>interval t0 and tN )." Now, a path x(t) is fully specified by an infinity of numbers 
</p>
<p>x(t0), &bull;&bull;&bull; , x(t), ... , x(tN ), namely, the values of the function x(t) at every point t 
</p>
<p>in the interval t0 to tN. To sum over all paths we must integrate over all possible 
</p>
<p>values of these infinite variables, except of course x(t0) and x(tN ), which will be kept 
</p>
<p>fixed at x 0 and xN, respectively. To tackle this problem, we follow the idea that was 
</p>
<p>used in Section 1.10: we trade the function x(t) for a discrete approximation which 
</p>
<p>agrees with x(t) at the N + 1 points tn = t0 + ne, n =0, ... , N, where e= (tN- t0)/N. 
In this approximation each path is specified by N +I numbers x(t0), x(t1), . .. , x(tN ). 
</p>
<p>The gaps in the discrete function are interpolated by straight lines. One such path 
</p>
<p>is shown in Fig. 8.4. We hope that if we take the limit N-+oo at the end we will get 
</p>
<p>a result that is insensitive to these approximations.t Now that the paths have been 
</p>
<p>discretized, we must also do the same to the action integral. We replace the continu-
</p>
<p>ous path definition 
</p>
<p>by 
</p>
<p>N-1 ( )2 S= L 111_ X;+1-x; e 
;~o 2 e 
</p>
<p>(8.4.2) 
</p>
<p>where x;=x(t;). We wish to calculate 
</p>
<p>U(xN, tN; Xo, to)= f"' exp{iS[x(t)]/1i}2J[x(t)] 
xo 
</p>
<p>f cxc f"Xj f"' [ &middot; N-1 ( )2] _ l' A I m 'I;' X;+ 1- X; - 1m &middot; &middot; &middot; exp -- L.... 
N-oc 1i 2 &middot;-o e 
s-+0 -'X_- ~x -rx_, 1-
</p>
<p>(8.4.3) 
</p>
<p>t We expect that the abrupt changes in velocity at the points t0 +ne that arise due to our approximation 
will not matter because .!!' does not depend on the acceleration or higher derivatives. 
</p>
<p>227 
</p>
<p>THE PATH 
</p>
<p>INTEGRAL 
</p>
<p>FORMULATION 
</p>
<p>OF QUANTUM 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>228 
</p>
<p>CHAPTER 8 
</p>
<p>It is implicit in the above that x0 and xN have the values we have chosen at the 
outset. The factor A in the front is to be chosen at the end such that we get the 
correct scale for U when the limit N-+ oo is taken. 
</p>
<p>Let us first switch to the variables 
</p>
<p>We then want 
</p>
<p>where 
</p>
<p>(
2 fz )(N- I )/2 
</p>
<p>A' =A _____!_ 
m 
</p>
<p>(8.4.4) 
</p>
<p>Although the multiple integral looks formidable, it is not. Let us begin by doing the 
Y1 integration. Considering just the part of the integrand that involves y 1 , we get 
</p>
<p>(8.4.5) 
</p>
<p>Consider next the integration over Y2. Bringing in the part of the integrand involving 
y 2 and combining it with the result above we compute next 
</p>
<p>(8.4.6) 
</p>
<p>By comparing this result to the one from the y 1 integration, we deduce the pattern: 
if we carry out this process N -l times so as to evaluate the integral in Eq. (8.4.4), 
it will become </p>
<p/>
</div>
<div class="page"><p/>
<p>or 
</p>
<p>( . )(N-1)/2 
l'!r -rn(xN -xo)2/HsNi 
</p>
<p>Nl/2 e 
</p>
<p>Bringing in the factor A(2'1ielm)(N -ll/2 from up front, we get 
</p>
<p>U=A(2n'liei)N/
2
</p>
<p>( ~ ) 112 exp[im(xN-xo)2] 
m 2n'lilN e 2'/iN e 
</p>
<p>If we now let N-HXJ, e--+0, Ne--+tN-to, we get the right answer provided 
</p>
<p>[ J
-N/2 
</p>
<p>A= 2n;ei =B-N (8.4.7) 
</p>
<p>It is conventional to associate a factor 1 I B with each of the N- I integrations and 
the remaining factor 1 I B with the overall process. In other words, we have just learnt 
that the precise meaning of the statement "integrate over all paths" is 
</p>
<p>f . 1 foc f f fXo dx1 dx2 dxN-I !&raquo;[x(t)] = hm - &middot; &middot; &middot; - &middot; - &middot; &middot; &middot; --
s~o B B B B 
</p>
<p>N-+oo -:x: -a:) 
</p>
<p>where 
</p>
<p>( )
</p>
<p>1/2 
</p>
<p>B= 2n~ei (8.4.8) 
</p>
<p>8.5. Equivalence to the Schrodinger Equation 
</p>
<p>The relation between the Schrodinger and Feynrnan formalisms is quite similar 
</p>
<p>to that between the Newtonian and the least action formalisms of mechanics, in that 
</p>
<p>the former approach is local in time and deals with time evolution over infinitesimal 
</p>
<p>periods while the latter is global and deals directly with propagation over finite times. 
</p>
<p>In the Schrodinger formalism, the change in the state vector llfl) over an infin-
</p>
<p>itesimal time e is 
</p>
<p>-ie 
llf/(B)) -llf/(0)) =- Hllf/{0)) 
</p>
<p>'li 
</p>
<p>which becomes in the X basis 
</p>
<p>-ie [-'li2 ii J lf!(x, e) -lfi(X, 0) =- - - 2 + V(x, 0) lf!(X, 0) 
'li 2m ox 
</p>
<p>(8.5.1) 
</p>
<p>(8.5.2) 
</p>
<p>229 
</p>
<p>THE PATH 
</p>
<p>INTEGRAL 
</p>
<p>FORMULATION 
</p>
<p>OF QUANTUM 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>230 
</p>
<p>CHAPTER 8 
</p>
<p>to first order in e. To compare this result with the path integral prediction to the 
same order in e, we begin with 
</p>
<p>ljl(x, e)= f" U(x, e; x')ljl(x', 0) dx' 
-x_ 
</p>
<p>(8.5.3) 
</p>
<p>The calculation of U(e) is simplified by the fact that there is no need to do any 
integrations over intermediate x's since there is just one slice of time e between the 
start and finish. So 
</p>
<p>, m . m(x-x -
( )
</p>
<p>I 2 { [ ')" 
U(x, e; x) = 2-;.;--rz;;, exp 1 le (8.5.4) 
</p>
<p>where the (mj2;r;liie) 112 factor up front is just the 1/ B factor from Eq. (8.4.8). We 
take the time argument of V to be zero since there is already a factor of e before it 
and any variation of V with time in the interval 0 to e will produce an effect of 
second order in e. So 
</p>
<p>( )' I '2 f x [, ( ')2] [ , ( + , )] m 1m x-x le x x ljl(x, e)= --.- exp exp -- V ~-. 0 
2n1ite 2eli 1i 2 -x 
</p>
<p>X lji(X', 0) dx' (8.5.5) 
</p>
<p>Consider the factor exp[im(x- x') 2 j2eli]. It oscillates very rapidly as (x- x') varies 
since 8 is infinitesimal and 1i is so small. When such a rapidly oscillating function 
multiplies a smooth function like ljl(x', 0), the integral vanishes for the most part 
due to the random phase of the exponential. Just as in the case of the path integration, 
the only substantial contribution comes from the region where the phase is stationary. 
In this case the only stationary point is x=x', where the phase has the minimum 
value of zero. In terms of 17 = x'- x, the region of coherence is, as before, 
</p>
<p>or 
</p>
<p>I (
2e1i77:)I '2 
</p>
<p>117:5~-
m 
</p>
<p>(8.5.6) 
</p>
<p>Consider now 
</p>
<p>X lji(X + 17, 0) d17 (8.5.7) </p>
<p/>
</div>
<div class="page"><p/>
<p>We will work to first order in E: and therefore to second order in 1J [see Eq. (8.5.6) 
</p>
<p>above]. We expand 
</p>
<p>()VI 1]2 a2 VI 
VI(X+ 1], 0)= VI(X, 0)+ T/- + ~ ;;-+&middot; .. 
</p>
<p>ex 2 ax&middot;&middot; 
</p>
<p>exp[-(~)&amp;v(x+i,o)J=I-ii v(x+i,o)+&middot; &middot; &middot; 
</p>
<p>iE: 
=I - ~ V(x, 0) + &middot; &middot; &middot; 
</p>
<p>since terms of order 1JE: are to be neglected. Equation (8.5.7) now becomes 
</p>
<p>Consulting the list of Gaussian integrals in Appendix A.2, we get 
</p>
<p>( )1/2[ ; . )1/2 ( . )l/2 7 Vl(x, s) = ~ Vl(x, O) (2nfus _ ~"__ ~7rru~ ~-~ 
2n1its m 2mz m ox 
</p>
<p>iE: (2n1ii&amp;)
112 J - fz ---;;;---- v'(x, 0) VI(X, 0) 
</p>
<p>or 
</p>
<p>-iE: r -r? i? J 
VI(X, s)- VI(X, 0) = 1i L 2m ox2 + V(x, 0) VI(X, 0) 
</p>
<p>which agrees with the Schrodinger prediction, Eq. (8.5.1). 
</p>
<p>8.6. Potentials of the Form V =a + bx + cx2 + di +exit 
</p>
<p>We wish to compute 
</p>
<p>U(x, t; x') = f' eiS[x(t")]/nfl1[x(t")] 
x' 
</p>
<p>t This section may be omitted without loss of continuity. 
</p>
<p>(8.5.8) 
</p>
<p>(8.6.1) 
</p>
<p>231 
</p>
<p>THE PATH 
</p>
<p>INTEGRAL 
FORMULATTON 
</p>
<p>OF QUANTUM 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>232 
</p>
<p>CHAPTER 8 
</p>
<p>Let us write every path as 
</p>
<p>x(t") = Xci (t") + y(t") (8.6.2) 
</p>
<p>It follows that 
</p>
<p>x(t") = xc1 (t") + y( t") (8.6.3) 
</p>
<p>Since all the paths agree at the end points, y(O) = y(t) = 0. When we slice up the time 
into N parts, we have for intermediate integration variables 
</p>
<p>Since xc1 (t;') is just some constant at t;', 
</p>
<p>dx;=dy; 
</p>
<p>and 
</p>
<p>r f!&amp;[x(t")] = r f!&amp;[y(t")] (8.6.4) 
so that Eq. (8.6.1) becomes 
</p>
<p>(8.6.5) 
</p>
<p>The next step is to expand the functional S in a Taylor series about xc1: 
</p>
<p>II [ &bull; (a:t&gt;[ a2[ &middot;) = !f&gt;(XcJ,XcJ)+ ~ y+~. Y 
Q ax Xd ax Xc] 
</p>
<p>(8.6.6) 
</p>
<p>The series terminates here since :t&gt; is a quadratic polynominal. 
The first piece 2(xc~, .Xc~) integrates to give S[xcd = Sc~. The second piece, linear 
</p>
<p>in y andy, vanishes due to the classical equation of motion. In the last piece, if we 
recall 
</p>
<p>:t&gt; = ~ mx2 - a- bx- cx2 - dx- ex:~ (8.6.7) </p>
<p/>
</div>
<div class="page"><p/>
<p>we get 
</p>
<p>1 o2.P 
---=-c 
2 ox2 
</p>
<p>(8.6.8) 
</p>
<p>a2.P 
--=-e 
ax ax 
</p>
<p>(8.6.9) 
</p>
<p>1 o2 .P 
---=m 
2 a&pound; 
</p>
<p>(8.6.10) 
</p>
<p>Consequently Eq. (8.6.5) becomes 
</p>
<p>( iSc~) fo [i f' (1 .2 2 &middot;) '] U(x, t; x')=exp ---,;-
0 
</p>
<p>exp ~ 
0 2my -cy -eyy dt' 
</p>
<p>X !&raquo;[y(t")] (8.6.11) 
</p>
<p>Since the path integral has no memory of Xc1, it can only depend on t. So 
</p>
<p>(8.6.12) 
</p>
<p>where A(t) is some unknown function oft. Now if we were doing the free-particle 
</p>
<p>problem, we would get Eq. (8.6.11) with c=e=O. In this case we know that [see 
</p>
<p>Eq. (8.3.4)] 
</p>
<p>( )
</p>
<p>1/2 
</p>
<p>A(t)= ~ 
2trfllt 
</p>
<p>(8.6.13) 
</p>
<p>Since the coefficient b does not figure in Eq. (8.6.11 ), it follows that the same value 
</p>
<p>of A(t) corresponds to the linear potential V=a+bx as well. For the harmonic 
</p>
<p>oscillator, c = !mw2, and we have to do the integral 
</p>
<p>(8.6.14) 
</p>
<p>The evaluation of this integral is discussed in the book by Feynman and Hibbs 
</p>
<p>referred to at the end of this section. Note that even if the factor A(t) in lfl(x, t) is 
</p>
<p>not known, we can extract all the probabilistic information at time t. 
</p>
<p>Notice the ease with which the Feynman formalism yields the full propagator 
</p>
<p>in these cases. Consider in particular the horrendous alternative of finding the eigen-
</p>
<p>functions of the Hamiltonian and constructing from them the harmonic oscillator 
</p>
<p>propagator. 
The path integral method may be extended to three dimensions without any 
</p>
<p>major qualitative differences. In particular, the form &lt;Jf U in Eq. (8.6.12) is valid 
</p>
<p>for potentials that are at most quadratic in the coordinates and the velocities. An 
</p>
<p>233 
</p>
<p>THE PATH 
</p>
<p>INTEGRAL 
</p>
<p>FORMULATION 
</p>
<p>OF QUANTUM 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>234 
</p>
<p>CHAPTER 8 
</p>
<p>interesting problem in this class is that of a particle in a uniform magnetic field. For 
further details on the subject of path integral quantum mechanics, seeR. P. Feynman 
and A. R. Hibbs, Path Integrals and Quantum Mechanics, McGraw-Hill (1965), and 
Chapter 21. 
</p>
<p>Exercise 8.6.1. * Verify that 
</p>
<p>agrees with the exact result, Eq. (5.4.31), for V(x) = -jx. Hint: Start with xc1 (t") = 
xo+ Vol"+ ~(f/m)t" 2 and find the constants x0 and v0 from the requirement that xc1 (0) =x' 
and xc~(t)=x. 
</p>
<p>Exercise 8.6.2. Show that for the harmonic oscillator with 
</p>
<p>U(x, t; x') = A(t) exp { i~w [(x2 + x'2 ) cos wt- 2xx']} 
2/i sm wt 
</p>
<p>where A(t) is an unknown function. (Recall Exercise 2.8.7.) 
</p>
<p>Exercise 8.6.3. We know that given the eigenfunctions and the eigenvalues we can con-
struct the propagator: 
</p>
<p>U(x, t; x', t') =I 'l'n(x)vr!(x') e-&bull;E,&lt;t-z');n (8.6.15) 
</p>
<p>Consider the reverse process (since the path integral approach gives U directly), for the case 
of the oscillator. 
</p>
<p>(I) Set x=x'=t'=O. Assume that A(t)=(mw/2nilisinwt) 1i 2 for the oscillator. By 
expanding both sides of Eq. (8.6.15), you should find that E= liw /2, 5/iw /2, 9/iw /2, ... , etc. 
What happened to the levels in between? 
</p>
<p>(2) (Optional). Now consider the extraction of the eigenfunctions. Let x = x' and t' = 0. 
Find E0 , E 1 , I 'l'o(x)l', and I 11f1{x)l 2 by expanding in powers of a =exp(iwt). 
</p>
<p>Exercise 8.6.4. * Recall the derivation of the Schrodinger equation (8.5.8) starting from 
Eq. (8.5.4). Note that although we chose the argument of V to be the midpoint x + x' /2, it 
did not matter very much: any choice x +a TJ, (where 17 = x'- x) for 0::;; a::;; 1 would have 
given the same result since the difference between the choices is of order 17 &amp; ~ &amp;312 &bull; All this 
was thanks to the factor &amp; multiplying V in Eq. (8.5.4) and the fact that 1111 ~ &amp; 112, as per 
Eq. (8.6.5). </p>
<p/>
</div>
<div class="page"><p/>
<p>Consider now the case of a vector potential which will bring in a factor 
</p>
<p>to the propagator for one time slice. (We should really be using vectors for position and the 
</p>
<p>vector potential, but the one-dimensional version will suffice for making the point here.) Note 
</p>
<p>that t: now gets canceled, in contrast to the scalar potential case. Thus, going to order t: to 
</p>
<p>derive the Schrodinger equation means going to order r1 2 in expanding the exponential. This 
</p>
<p>will not only bring in an A 2 term, but will also make the answer sensitive to the argument of 
</p>
<p>A in the linear term. Choose a = 1 /2 and verify that you get the one-dimensional version of 
Eq. (4.3.7). Along the way you will sec that changing a makes an order&pound; difference to ij!(x, s) 
</p>
<p>so that we have no choice but to use a= I /2, i.e., use the midpoint prescriplion. This point 
</p>
<p>will come up in Chapter 21. 
</p>
<p>235 
</p>
<p>THE PATH 
</p>
<p>INTEGRAL 
</p>
<p>FORMULATION 
</p>
<p>OF QUANTUM 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>The Heisenberg 
</p>
<p>Uncertainty Relations 
</p>
<p>9.1. Introduction 
</p>
<p>9 
</p>
<p>In classical mechanics a particle in a state (x0 , p0) has associated with it well-
</p>
<p>defined values for any dynamical variable OJ(x,p), namely, OJ(xo,p0). In quantum 
</p>
<p>theory, given a state llfl), one can only give the probabilities P( OJ) for the possible 
</p>
<p>outcomes of a measurement of n. The probability distribution will be characterized 
by a mean or expectation value 
</p>
<p>(9.l.l) 
</p>
<p>and an uncertainty about this mean: 
</p>
<p>(9.1.2) 
</p>
<p>There are, however, states for which An= 0, and these are the eigenstates I OJ&gt; of n. 
If we consider two Hermitian operators n and A, they will generally have some 
</p>
<p>uncertainties An and AA in an arbitrary state. In the next section we will derive the 
</p>
<p>Heisenberg uncertainty relations, which will provide a lower bound on the product 
</p>
<p>of uncertainties, An&middot; AA. Generally the lower bound will depend not only on the 
</p>
<p>operators but also on the state. Of interest to us are those cases in which the lower 
</p>
<p>bound is independent of the state. The derivation will make clear the conditions 
</p>
<p>under which such a relation will exist. 
</p>
<p>9.2. Derivation of the Uncertainty Relations 
</p>
<p>Let n and A be two Hermitian operators, with a commutator 
</p>
<p>[n, A] =if' (9.2.1) 237 </p>
<p/>
</div>
<div class="page"><p/>
<p>238 
</p>
<p>CHAPTER 9 
</p>
<p>You may readily verify that r is also Hermitian. Let us start with the uncertainty 
product in a normalized state [If/): 
</p>
<p>where &lt;O&gt; = &lt;lfiiO[If/) and &lt;A&gt;= &lt;lfiiAIIf/). Let us next define the pair 
</p>
<p>n=n-&lt;n&gt; 
</p>
<p>A=A-&lt;A&gt; 
</p>
<p>(9.2.2) 
</p>
<p>(9.2.3) 
</p>
<p>which has the same commutator as nand A (verify this). In terms of nand A 
</p>
<p>since 
</p>
<p>and 
</p>
<p>(~Q) 2 (~A) 2 = &lt;lfiiQ2IIfl&gt;&lt;lfiiAZilf!) 
</p>
<p>= &lt;nlfl[nlfi&gt;&lt;AifiiAif/) 
</p>
<p>If we apply the Schwartz inequality 
</p>
<p>(9.2.4) 
</p>
<p>(9.2.5) 
</p>
<p>(9.2.6) 
</p>
<p>(where the equality sign holds only if I V1) = c[ V2), where c is a constant) to the 
states [Qif/) and [Aif/), we get from Eq. (9.2.4), 
</p>
<p>(9.2.7) 
</p>
<p>Let us now use the fact that 
</p>
<p>(9.2.8) 
</p>
<p>to rewrite the above inequality as 
</p>
<p>(9.2.9) 
</p>
<p>Now, we know that the commutator has to enter the picture somewhere. This we 
arrange through the following identity: 
</p>
<p>~ ~ nA+An nA-An 
QA= +---
</p>
<p>2 2 
</p>
<p>= ~rn. Al+ + ~rn, AJ (9.2.10) </p>
<p/>
</div>
<div class="page"><p/>
<p>where [fi, A]+ is called the anticommutator. Feeding Eq. (9.2.10) into the inequality 
</p>
<p>(9.2.9), we get 
</p>
<p>(9.2.11) 
</p>
<p>We next use the fact that 
</p>
<p>(1) since [fi, A]= ir, where r is Hermitian, the expectation value of the com-
mutator is pure imaginary; 
</p>
<p>(2) since [fi, A]+ is Hermitian, the expectation value of the anticommutator is 
</p>
<p>real. 
</p>
<p>(LlQ )2(LlA)2 ~*I&lt; 'I' I [fi, A]+l 'I')+ i&lt; '1'111 '1')1 2 
</p>
<p>~ h'l'l[fi, A]+l '1'/+ ~&lt;'1'111 '1')2 (9.2.12) 
</p>
<p>This is the general uncertainty relation between any two Hermitian operators and is 
</p>
<p>evidently state dependent. Consider now canonically conjugate operators, for which 
</p>
<p>1 = li. In this case 
</p>
<p>(9.2.13) 
</p>
<p>Since the first term is positive definite, we may assert that for any I 'I') 
</p>
<p>or 
</p>
<p>(9.2.14) 
</p>
<p>which is the celebrated uncertainty relation. Let us note that the above inequality 
</p>
<p>becomes an equality only if 
</p>
<p>and (9.2.15) 
</p>
<p>9.3. The Minimum Uncertainty Packet 
</p>
<p>In this section we will find the wave function ljl(x) which saturates the lower 
</p>
<p>bound of the uncertainty relation for X and P. According to Eq. (9.2.15) such a 
state is characterized by 
</p>
<p>(P- &lt;P) )I 'I') =c(X- (X) )I 'I') (9.3.1) 
</p>
<p>239 
</p>
<p>THE HEISENBERG 
</p>
<p>UNCERTAINTY 
RELATIONS </p>
<p/>
</div>
<div class="page"><p/>
<p>240 
</p>
<p>CHAPTER 9 
</p>
<p>and 
</p>
<p>('I'I(P- (P) )(X- (X)) +(X- (X) )(P- (P) )I '1')=0 (9.3.2) 
</p>
<p>where (P) and (X) refer to the state I 'If), implicitly defined by these equations. In 
the X basis, Eq. (9.3.1) becomes 
</p>
<p>or 
</p>
<p>(-in~ -&lt;P&gt;)'I'(x)=c(x-(X))'If(X) 
</p>
<p>dlji(X) i 
--=- [(P)+c(x-(X) )] dx 
'lf(x) n 
</p>
<p>(9.3.3) 
</p>
<p>Now, whatever (X) may be, it is always possible to shift our origin (to x= (X)) 
so that in the new frame of reference (X)=O. In this frame, Eq. (9.3.3) has the 
solution 
</p>
<p>Let us next consider the constraint, Eq. (9.3.2), which in this frame reads 
</p>
<p>('I'I(P- (P) )X+ X(P- (P) )I 'I') =0 
</p>
<p>If we now exploit Eq. (9.3.1) and its adjoint, we find 
</p>
<p>('l'lc*X 2 +cX 2I '1')=0 
</p>
<p>(c+ c*)('I'IX2 1 'I')= 0 
</p>
<p>from which it follows that c is pure imaginary: 
</p>
<p>c=ilcl 
</p>
<p>Our solution, Eq. (9.3.4) now becomes 
</p>
<p>In terms of 
</p>
<p>~ 2 =n/lcl 
</p>
<p>lji(X) = lji(O) ei(P)x/~ e-x'/21!.' 
</p>
<p>(9.3.4) 
</p>
<p>(9.3.5) 
</p>
<p>(9.3.6) </p>
<p/>
</div>
<div class="page"><p/>
<p>where A 2, like I cl, is arbitrary. If the origin were not chosen to make (X) zero, we 
would have instead 
</p>
<p>'lf(X) = 'lf((X)) ei&lt;P&gt; (x-&lt;X&gt;)!1i e -(x-&lt;X&gt;J2j2f1.2 (9.3.7) 
</p>
<p>Thus the minimum uncertainty wave function is a Gaussian of arbitrary width and 
center. This result, for the special case (X)=(P)=O, was used in the quest for the 
state that minimized the expectation value of the oscillator Hamiltonian. 
</p>
<p>9.4. Applications of the Uncertainty Principle 
</p>
<p>I now illustrate the use of the uncertainty principle by estimating the size of the 
ground-state energy and the spread in the ground-state wave function. It should be 
clear from this example that the success we had with the oscillator was rather atypical. 
</p>
<p>We choose as our system the hydrogen atom. The Hamiltonian for this system, 
assuming the proton is a spectator whose only role is to provide a Coulomb potential 
for the electron, may be written entirely in terms of the electron's variable as 
</p>
<p>(9.4.1)t 
</p>
<p>Let us begin by mimicking the analysis we employed for the oscillator. We evaluate 
(H) in a normalized state I 'I'): 
</p>
<p>(9.4.2) 
</p>
<p>Since 
</p>
<p>the first step in minimizing (H) is to work only with states for which (P;) = 0. For 
such states 
</p>
<p>(9.4.3) 
</p>
<p>t The operator (X 2 + Y2 +Z2r' 12 is just 1/r in the coordinate basis. We will occasionally denote it by 
1/r even while referring to it in the abstract, to simplify the notation. 
</p>
<p>241 
</p>
<p>THE HEISENBERG 
</p>
<p>UNCERTAINTY 
RELATIONS </p>
<p/>
</div>
<div class="page"><p/>
<p>242 
</p>
<p>CHAPTER 9 
</p>
<p>We cannot exploit the uncertainty relations 
</p>
<p>yet since (H) is not a function of AX and 11P. The problem is that 
</p>
<p>((X2 + Y 2 + Z 2}-112) is not simply related to AX, 11Y, and /1Z. Now the handwaving 
</p>
<p>begins. We argue that (see Exercise 9.4.2), 
</p>
<p>(9.4.4) 
</p>
<p>where the ~ symbol means that the two sides of Eq. (9.4.4) are not strictly equal, 
</p>
<p>but of the same order of magnitude. So we write 
</p>
<p>(H)_ (11Pxf + (!1Pyf + (11Pz) 2 e2 
</p>
<p>2m ((X2+ Y2+z2)I/2&gt; 
</p>
<p>Once again, we argue that 
</p>
<p>and ged 
</p>
<p>(H)_ (11Pxf + (11Py) 2 + (11Pzf e2 
</p>
<p>2m ( (X2) + &lt; yz&gt; + (Z2)) 112 
</p>
<p>From the relations 
</p>
<p>it follows that we may confine ourselves to states for which (X)=&lt; Y) = (Z) = 0 
</p>
<p>in looking for the state with the lowest mean energy. For such states 
</p>
<p>For a problem such as this, with spherical symmetry, it is intuitively clear that the 
</p>
<p>configuration of least energy will have 
</p>
<p>t We are basically arguing that the mean of the functions (of X, Y, and Z) and the functions of the 
mean ((X), ( Y), and (Z)) are of the same order of magnitude. They are in fact equal if there are no 
</p>
<p>fluctuations around the mean and approximately equal if the fluctuations are small (recall the discussion 
</p>
<p>toward the end of Chapter 6). </p>
<p/>
</div>
<div class="page"><p/>
<p>and 
</p>
<p>so that 
</p>
<p>(9.4.5) 
</p>
<p>Now we use 
</p>
<p>to get 
</p>
<p>We now differentiate the right-hand side with respect to AX to find its minimum: 
</p>
<p>or 
</p>
<p>(9.4.6) 
</p>
<p>Finally, 
</p>
<p>(9.4. 7) 
</p>
<p>What prevents us from concluding (as we did in the case of the oscillator), that the 
ground-state energy is -2me4 j91i2 or that the ground-state wave function is a Gauss-
ian [of width 3(3112)112 /4me2] is the fact that Eq. (9.4. 7) is an approximate inequality. 
However, the exact ground-state energy 
</p>
<p>(9.4.8) 
</p>
<p>differs from our estimate, Eq. (9.4.7), only by a factor ~2. Likewise, the true ground-
state wave function is not a Gaussian but an exponential lf!(x, y, z) = 
cexp[-(x 2 +l+~) 112 /a 0 ], where 
</p>
<p>243 
</p>
<p>THE HEISENBERG 
UNCERTAINTY 
</p>
<p>RELATIONS </p>
<p/>
</div>
<div class="page"><p/>
<p>244 
</p>
<p>CHAPTER 9 
</p>
<p>is called the Bohr radius. However, the LlX associated with this wave function is 
</p>
<p>(9.4.9) 
</p>
<p>which also is within a factor of 2 of the estimated LlX in (9.4.6). 
</p>
<p>In conclusion, the uncertainty principle gives us a lot of information about the 
</p>
<p>ground state, but not always as much as in the case of the oscillator. 
</p>
<p>Exercise 9.4.1. * Consider the oscillator in the state In= 1) and verify that 
</p>
<p>Exercise 9.4.2. (1) By referring to the table of integrals in Appendix A.2, verify that 
</p>
<p>is a normalized wave function (of the ground state of hydrogen). Note that in three dimensions 
</p>
<p>the normalization condition is 
</p>
<p>&lt;'1'1'1')= f 'l'*(r, 0, 1/&gt;)'!'(r, 0, if&gt;)r2 drd(cos 9) d&cent; 
</p>
<p>=4n f 'l'*(r)'l'(r)r2 dr= 1 
for a function of just r. 
</p>
<p>(2) Calculate (Mf in this state [argue that (M) 2 = ~&lt;r 2 )] and regain the result quoted 
</p>
<p>in Eq. (9.4.9). 
</p>
<p>(3) Show that &lt;I/r)::::I/&lt;r)::::me2/1i2 in this state. 
</p>
<p>Exercise 9.4.3. Ignore the fact that the hydrogen atom is a three-dimensional system and 
</p>
<p>pretend that 
</p>
<p>corresponds to a one-dimensional problem. Assuming 
</p>
<p>t1P&middot;t1R~1i/2 
</p>
<p>estimate the ground-state energy. 
</p>
<p>Exercise 9.4.4. * Compute AT&middot; M, where T= p2 /2m. Why is this relation not so famous? </p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 9.1. At the point x 1 , skater A throws the snowball 
towards skater B, who catches it at the point x2 &bull; 
</p>
<p>9.5. The Energy-Time Uncertainty Relation 
</p>
<p>There exists an uncertainty relation 
</p>
<p>AE&middot;At~1i/2 (9.5.1) 
</p>
<p>which does not follow from Eq. (9.2.12), since time t is not a dynamical variable 
but a parameter. The content of this equation is quite different from the others 
involving just dynamical variables. The rough meaning of this inequality is that the 
energy of a system that has been in existence only for a finite time At has a spread 
(or uncertainty) of at least AE, where AE and At are related by (9.5.1). To see how 
this comes about, recall that eigenstates of energy have a time dependence e-iEtl~, 
i.e., a definite energy is associated with a definite frequency, co= E/n. Now, only a 
wave train that is infinitely long in time (that is to say, a system that has been in 
existence for infinite time) has a well-defined frequency. Thus a system that has been 
in existence only for a finite time, even if its time dependence goes as e -;Et/~ during 
this period, is not associated with a pure frequency co= E/1i or definite energy E. 
</p>
<p>Consider the following example. At time t = 0, we turn on light of frequency co 
on an ensemble of hydrogen atoms all in their ground state. Since the light is supposed 
to consist of photons of energy Fico, we expect transitions to take place only to a 
level (if it exists) Fico above the ground state. It will however be seen that initially 
the atoms make transitions to several levels not obeying this constraint. However, 
as t increases, the deviation AE from the expected final-state energy will decrease 
according to AEc::::.Fijt. Only as t-+rxJ do we have a rigid law of conservation of 
energy in the classical sense. We interpret this result by saying that the light source 
is not associated with a definite frequency (i.e., does not emit photons of definite 
energy) if it has been in operation only for a finite time, even if the dial is set at a 
definite frequency co during this time. [The output of the source is not just e -imt but 
rather O(t) e-;"'', whose transform is not a delta function peaked at co.] Similarly 
when the excited atoms get deexcited and drop to the ground state, they do not emit 
photons of a definite energy E= E.- Eg (the subscripts e and g stand for "excited" 
and "ground") but rather with a spread AEc::::.1i/ At, At being the duration for which 
they were in the excited state. [The time dependence of the atomic wave function is 
not e-;E,t/~ but rather O(t)O(T- t) e-;E,t/~ assuming it abruptly got excited to this 
state at t = 0 and abruptly got deexcited at t = T.] We shall return to this point when 
we discuss the interaction of atoms with radiation in a later chapter. 
</p>
<p>Another way to describe this uncertainty relation is to say that violations in the 
classical energy conservation law by AE are possible over times At -Fi! AE. The 
following example should clarify the meaning of this statement. 
</p>
<p>Example 9.5.1. (Range of the Nuclear Force.) Imagine two ice skaters each equipped 
with several snowballs, and skating toward each other on trajectories that are parallel but 
separated by some perpendicular distance (Fig. 9.1). When skater A reaches some point x 1 
</p>
<p>245 
</p>
<p>THE HEISENBERG 
UNCERTAINTY 
</p>
<p>RELATIONS </p>
<p/>
</div>
<div class="page"><p/>
<p>246 
</p>
<p>CHAPTER 9 
</p>
<p>let him throw a snowball toward B. He (A) will then recoil away from Band start moving 
</p>
<p>along a new straight line. Let B now catch the snowball. He too will recoil as a result, as 
</p>
<p>shown in the figure. If this whole process were seen by someone who could not see the snow 
</p>
<p>balls, he would conclude that there is a repulsive force between A and B. If A (or B) can 
</p>
<p>throw the ball at most I 0 ft, the observer would conclude that the range of the force is I 0 ft, 
</p>
<p>meaning A and B will not affect each other if the perpendicular distance between them exceeds 
</p>
<p>10ft. 
This is roughly how elementary particles interact with each other: if they throw photons 
</p>
<p>at each other the force is called the electromagnetic force and the ability to throw and catch 
</p>
<p>photons is called "electric charge." If the projectiles are pions the force is called the nuclear 
</p>
<p>force. We would like to estimate the range of the nuclear force using the uncertainty principle. 
</p>
<p>Now, unlike the two skaters endowed with snowballs, the protons and neutrons (i.e., nucleons) 
</p>
<p>in the nucleus do not have a ready supply of pions, which have a mass J1 and energy J1C2 . A 
</p>
<p>nucleon can, however, produce a pion from nowhere (violating the classical law of energy 
</p>
<p>conservation by ~J.lc 2 ) provided it is caught by the other nucleon within a time !it such that 
</p>
<p>!it~fz/AE=fz/J1c 2 &bull; Even if the pion travels toward the receiver at the speed of light, it can 
</p>
<p>only cover a distance r = c !it= fz/ J.lC, which is called the Compton wavelength of the pion and 
</p>
<p>is a measure of the range of nuclear force. The value of r is approximately I Fermi= I o-' 3 em. 
The picture of nuclear force given here is rather simpleminded and should be taken with 
</p>
<p>a grain of salt. For example, neither is the pion the only particle that can be "exchanged" 
</p>
<p>between nucleons nor is the number of exchanges limited to one per encounter. (The pion is, 
</p>
<p>however, the lightest object that can be exchanged and hence responsible for the nuclear force 
</p>
<p>of the longest range.) Also our analogy with snowballs does not explain any attractive inter-
</p>
<p>action between particles. D </p>
<p/>
</div>
<div class="page"><p/>
<p>Systems with N Degrees 
of Freedom 
</p>
<p>10.1. N Particles in One Dimension 
</p>
<p>10 
</p>
<p>So far, we have restricted our attention (apart from minor digressions) to a 
system with one degree of freedom, namely, a single particle in one dimension. We 
now consider the quantum mechanics of systems with N degrees of freedom. The 
increase in degrees of freedom may be due to an increase in the number of particles, 
number of spatial dimensions, or both. In this section we consider N particles in one 
dimension, and start with the case N = 2. 
</p>
<p>The Two-Particle Hilbert Space 
</p>
<p>Consider two particles described classically by (xi, pi) and (x2, p2). The rule 
for quantizing this system [Postulate II, Eq. (7.4.39)] is to promote these variables 
to quantum operators (XI, P 1) and (X2 , P2) obeying the canonical commutation 
relations: 
</p>
<p>(i= I, 2) (IO.l.la) 
</p>
<p>(IO.l.lb) 
</p>
<p>(IO.l.lc) 
</p>
<p>It might be occasionally possible (as it was in the case of the oscillator) to extract 
all the physics given just the canonical commutators. In practice one works in a 
basis, usually the coordinate basis. This basis consists of the kets lxix2) which are 247 </p>
<p/>
</div>
<div class="page"><p/>
<p>248 
</p>
<p>CHAPTER 10 
</p>
<p>simultaneous eigenkets of the commuting operators X 1 and X 2 : 
</p>
<p>and are normalized ast 
</p>
<p>In this basis 
</p>
<p>We may interpret 
</p>
<p>Xdx1x2) =xdx1x2) 
</p>
<p>X2l x1x2) = x2lx1x2) 
(10.1.2) 
</p>
<p>(10.1.3) 
</p>
<p>(10.1.4) 
</p>
<p>(10.1.5) 
</p>
<p>as the absolute probability density for catching particle 1 near x 1 and particle 2 near 
</p>
<p>x2 , provided we normalize I IJI) to unity 
</p>
<p>(10.1.6) 
</p>
<p>There are other bases possible besides lx1x2). There is, for example, the momentum 
</p>
<p>basis, consisting of the simultaneous eigenkets I P1P2) of P1 and P2. More generally, 
we can use the simultaneous eigenkets lm1m2) of two commuting operators&sect; 
</p>
<p>QI(XI' PI) and Q2(X2, P2) to define then basis. We denote by 'W'I&reg;2 the two-particle 
</p>
<p>Hilbert space spanned by any of these bases. 
</p>
<p>'W'102 As a Direct Product Space 
</p>
<p>There is another way to arrive at the space 'W' 102 , and that is to build it out of 
</p>
<p>two one-particle spaces. Consider a system of two particles described classically by 
</p>
<p>(x1 , pi) and (x2 , p2). If we want the quantum theory of just particle 1, we define 
</p>
<p>operators X1 and P1 obeying 
</p>
<p>[XI, PI]= iii! (10.1.7) 
</p>
<p>The eigenvectors lx1 ) of X1 form a complete (coordinate) basis for the Hilbert space 
</p>
<p>j: Note that we denote the bra corresponding to lxlxD as (xix21. 
&sect;Note that any function of X, and P, commutes with any function of X2 and P 2 &bull; </p>
<p/>
</div>
<div class="page"><p/>
<p>\/1 of particle l. Other bases, such as lp1) of P1 or in general, lm 1) of 0 1(X1, P1) 
</p>
<p>are also possible. Since the operators X 1 , P1 , 0 1 , etc., act on \/ 1 , let us append a 
</p>
<p>superscript (1) to all of them. Thus Eq. ( 10.1.7) reads 
</p>
<p>(10.1.8a) 
</p>
<p>where /( 1 l is the identity operator on \1 1 . A similar picture holds for particle 2, and 
in particular, 
</p>
<p>( 10.1.8b) 
</p>
<p>Let us now turn our attention to the two-particle system. What will be the 
</p>
<p>coordinate basis for this system? Previously we assigned to every possible outcome 
</p>
<p>x1 of a position measurement a vector lx1) in \/ 1 and likewise for particle 2. Now a 
position measurement will yield a pair of numbers (x1, x2). Since after the measure-
</p>
<p>ment particle 1 will be in state I x1) and particle 2 in I x2), let us denote the correspond-
ing ket by lxi)&reg;Ixz): 
</p>
<p>I &gt; I &gt; {particle 1 at x1 X1 Q9 X2 .,_. 
particle 2 at Xz 
</p>
<p>( 10.1.9) 
</p>
<p>Note that lx1)&reg;lx2) is a new object, quite unlike the inner product (lf/d lf/2) 
or the outer product llf/1)(1f/2 1 both of which involve two vectors from the same 
</p>
<p>space. The product lx1)&reg;lx2), called the direct product, is the product of vectors 
</p>
<p>from two different spaces. The direct product is a linear operation: 
</p>
<p>The set of all vectors of the form lx1)&reg;lx2 ) forms the basis for a space which we call 
</p>
<p>\11 &reg; \12, and refer to as the direct product of the spaces \1 1 and \12. The dimensionality 
(number of possible basis vectors) of 'VJ0V2 is the product of the dimensionality 
</p>
<p>of V 1 and the dimensionality of \/2 . Although all the dimensionalities are infinite 
</p>
<p>here, the statement makes heuristic sense: to each basis vector lx1) of \/ 1 and lx2) 
</p>
<p>of 'Vz, there is one and only one basis vector lx,)0lx2) of \/,0%1 2 . This should be 
compared to the direct sum (Section 1.4) : 
</p>
<p>in which case the dimensionalities of \11 and \12 add (assuming the vectors of V 1 
are linearly independent of those of \/2). 
</p>
<p>The coordinate basis, !x1)&reg;lx2), is just one possibility; we can use the momen-
tum basis IP1)&reg;IP2), or, more generally, lm1)&reg;lm2). Although these vectors span 
V1&reg;V2, not every element of\1 10\12 is a direct product. For instance 
</p>
<p>I ljl) = lx!)&reg;lxD + lxl)&reg;lxD 
</p>
<p>249 
</p>
<p>SYSTEMS WITH 
N DEGREES 
</p>
<p>OF FREEDOM </p>
<p/>
</div>
<div class="page"><p/>
<p>250 
</p>
<p>CHAPTER 10 
</p>
<p>cannot be written as 
</p>
<p>where llf/ 1) and llf/2 ) are elements of 'l:J 1 and 'l:J2 , respectively. 
</p>
<p>The inner product of lxt)&reg;lx2) and lxi)&reg;lx2) is 
</p>
<p>((xi I &reg;(x2i)(lx,)&reg;lx2)) = (xilx,)(x21xz) 
</p>
<p>= o(xj- x,)o(x2- Xz) (10.1.11) 
</p>
<p>Since any vector in W 1 &reg; W 2 can be expressed in terms of the I x 1) &reg;I x2 ) basis, this 
defines the inner product between any two vectors in W 1 &reg; W 2 &bull; 
</p>
<p>It is intuitively clear that when two particles are amalgamated to form a single 
</p>
<p>system, the position and momentum operators of each particle, x\'&gt;, P/ 0 and X~ 2 &gt;, 
</p>
<p>pfl, which acted on W 1 and W 2 , respectively, must have counterparts in W 1 &reg; W 2 
and have the same interpretation. Let us denote by X\'J&reg;(ZJ the counterpart of 
</p>
<p>X\ 1 &gt;, and refer to it also as the "X operator of particle 1." Let us define its action 
</p>
<p>on 'l:J 1&reg;W2 &bull; Since the vectors lx1)&reg;lx2) span the space, it suffices to define its action 
</p>
<p>on these. Now the ket lx1)&reg;jx2) denotes a state in which particle l is at x 1 &bull; Thus 
</p>
<p>it must be an eigenket of x\'&gt;&reg;(2&gt; with eigenvalue x 1 : 
</p>
<p>(10.1.12) 
</p>
<p>Note that X\'J&reg;&lt;Z&gt; does not really care about the second ket jx2), i.e., it acts trivially 
</p>
<p>(as the identity) on lx2) and acts on lx1) just as xp&gt; did. In other words 
</p>
<p>( 10.1.13) 
</p>
<p>Let us define a direct product of two operators, r/ 1&gt; and A~ 2 &gt; (denoted by rf'&gt;&reg; 
A~ 2 &gt;), whose action on a direct product ket lm,)&reg;!mz) is 
</p>
<p>(10.1.14) 
</p>
<p>In this notation, we may write X\ 1&gt;&reg;m, in view of Eq. (10.1.13), as 
</p>
<p>(10.1.15) 
</p>
<p>We can similarly promote Pi21 , say, from 'l:J 2 to 'l:J 1&reg;'l:J2 by defining the momentum 
operator for particle 2, Pi1l&reg;&lt; 2&gt;, as 
</p>
<p>(10.1.16) 
</p>
<p>The following properties of direct products of operators may be verified (say 
</p>
<p>by acting on the basis vectors I x,) &reg;I xz)): </p>
<p/>
</div>
<div class="page"><p/>
<p>Exercise 10.1.1. * Show the following: 
</p>
<p>(1) (10.1.17a) 
</p>
<p>(operators of particle 1 commute with those of particle 2). 
</p>
<p>(2) (10.1.17b) 
</p>
<p>(3) If 
</p>
<p>then 
</p>
<p>(10.1.17c) 
</p>
<p>and similarly with 1-&gt;2. 
</p>
<p>(4) (10.1.17d) 
</p>
<p>The notion of direct products of vectors and operators is no doubt a difficult 
one, with no simple analogs in elementary vector analysis. The following exercise 
should give you some valuable experience. It is recommended that you reread the 
preceding discussion after working on the exercise. 
</p>
<p>Exercise 10.1.2. * Imagine a fictitious world in which the single-particle Hilbert space is 
two-dimensional. Let us denote the basis vectors by I+) and 1-). Let 
</p>
<p>+ -
</p>
<p>&lt;''-+[a rr, -
- c 
</p>
<p>bd] and 
</p>
<p>+ -
</p>
<p>(2)_ +[e 
CT2 -
</p>
<p>- g ~ J 
be operators in 'V 1 and 'V2, respectively (the &plusmn; signs label the basis vectors. Thus 
b=&lt;+lrrP'I-) etc.) The space 'V,&reg;'V2 is spanned by four vectors 1+)&reg;1+), 1+)&reg;1-), 
1-)&reg;1+), 1-)&reg;1-). Show (using the method of images or otherwise) that 
</p>
<p>++ +- -+ 
</p>
<p>+T 
0 b 
</p>
<p>~] 
(1) rr\''&reg;&lt;2'=rr\''&reg;J&lt; 2'=+- 0 a 0 
</p>
<p>-+ c 0 d 
</p>
<p>-- 0 c 0 
</p>
<p>251 
</p>
<p>SYSTEMS WITH 
N DEGREES 
</p>
<p>OF FREEDOM </p>
<p/>
</div>
<div class="page"><p/>
<p>252 
</p>
<p>CHAPTER 10 
</p>
<p>(Recall that (al&reg;(fJI is the bra corresponding to la)&reg;lfJ).) 
</p>
<p>(2) [
</p>
<p>e f 0 OJ 
0"(1)&reg;(2) = g h 0 0 
</p>
<p>2 0 0 e f 
0 0 g h 
</p>
<p>(3) [
</p>
<p>ae af be bfj 
</p>
<p>( CT CT )(1)&reg;(2&gt; = CT&lt;I&gt;.o.CT&lt;2&gt; = ag ah bg bh 
I 2 I 101 2 ce cf de df 
</p>
<p>cg ch dg dh 
</p>
<p>Do part (3) in two ways, by taking the matrix product of CT\'&gt;&reg;&lt;2&gt; and CT~ 1 &gt;&reg;&lt; 2 &gt; and by directly 
</p>
<p>computing the matrix elements of CTP&gt;&reg;CT~ 2 &gt;. 
</p>
<p>From Eqs. (10.1.17a) and (10.1.17c) it follows that the commutation relations 
</p>
<p>between the position and momentum operators on V 1 &reg; V 2 are 
</p>
<p>i,j= I, 2 
(10.1.18) 
</p>
<p>Now we are ready to assert something that may have been apparent all along: 
</p>
<p>the space V, &reg;V2 is just Vt&reg;2&bull; lx,)&reg;lx2) is just lx,x2), and x\'&gt;&reg;&lt;2&gt; is just X 1 , etc. 
</p>
<p>Notice first that both spaces have the same dimensionality: the vectors lx1x2) and 
</p>
<p>lx,)&reg;lx2) are both in one-to-one correspondence with points in the x 1-x2 plane. 
Notice next that the two sets of operators X 1 , &bull;&bull;&bull; , P2 and X~ 1 &gt;&reg;&lt; 2 &gt;, ... , PJ'&gt;&reg;&lt;2&gt; have 
</p>
<p>the same connotation and commutation rules [Eqs. (10.1.1) and (10.1.18)]. Since X 
</p>
<p>and P are defined by their commutators we can make the identification 
</p>
<p>xp&gt;&reg;&lt;2&gt;=x; 
</p>
<p>P?&gt;&reg;&lt;2&gt;=P; 
(10.1.19a) 
</p>
<p>We can also identify the simultaneous eigenkets of the position operators (since they 
</p>
<p>are nondegenerate): 
</p>
<p>(10.1.19b) 
</p>
<p>In the future, we shall use the more compact symbols occurring on the right-hand 
</p>
<p>side of Eqs. (10.1.19). We will, however, return to the concept of direct products of 
</p>
<p>vectors and operators on and off and occasionally use the symbols on the left-hand 
</p>
<p>side. Although the succinct notation suppresses the label (I &reg;2) of the space on </p>
<p/>
</div>
<div class="page"><p/>
<p>which the operators act, it should be clear from the context. Consider, for example, 
the CM kinetic energy operator of the two-particle system: 
</p>
<p>P'/;M= (Pt + P2)2 Pf+ Pi+ 2PtP2 
2M 2M 2M 
</p>
<p>which really means 
</p>
<p>2MT~~&reg;&lt;2l = (Pf)&lt;t&gt;&reg;&lt;2&gt; + (Pi)&lt;t&gt;&reg;&lt;2&gt; + 2PP&gt;&reg;&lt;2&gt;. p~t&gt;&reg;&lt;2&gt; 
</p>
<p>= (Pfl&gt;&reg;J&lt;2l)2 + (/o&gt;&reg;Pi2&gt;)2 + 2P1t&gt;&reg;pf&gt; 
</p>
<p>The Direct Product Revisited 
</p>
<p>Since the notion of a direct product space is so important, we revisit the forma-
tion of 'V 1 &reg;2 as a direct product of 'V 1 and 'V 2, but this time in the coordinate basis 
instead of in the abstract. Let Oi1&gt; be an operator on 'V1 whose nondegenerate 
eigenfunctions 1f1 ro,(x1) = co1(x1) form a complete ba,sis. Similarly let co2(x2) form a 
basis for 'V 2. Consider now a function 1f!(x1, x2), which represents the abstract 
ket I lf/) from 'V 1&reg;2. If we keep Xt fixed at some value, say Xt, then 1f1 becomes a 
function of x2 alone and may be expanded as 
</p>
<p>(10.1.20) 
</p>
<p>Notice that the coefficients of the expansion depend on the value of x1. We now 
expand the function Cro,(x1) in the basis co1(x1): 
</p>
<p>Cro,(Xt) = L Cro 1.ro2Wt(Xt) (10.1.21) 
OJt 
</p>
<p>Feeding this back to the first expansion and dropping the bar on x1 we get 
</p>
<p>(10.1.22a) 
</p>
<p>What does this expansion of an arbitrary 1f!(x1 , x2) in terms of co 1(xt) x co2(x2) imply? 
Equation (10.1.22a) is the coordinate space version of the abstract result 
</p>
<p>(10.1.22b) 
</p>
<p>which means 'Vt&reg;2='Vt&reg;'V2, for llfl) belongs to 'V1&reg;2 and lco1)&reg;lco2) spans 
Wt&reg;'i/2. If we choose !l=X, we get the familiar basis lxt)&reg;lx2). By dotting both 
sides of Eq. (10.1.22b) with these basis vectors we regain Eq. (10.1.22a). (In the 
coordinate basis, the direct product of the kets leo 1) and lco2) becomes just the 
ordinary product of the corresponding wave functions.) 
</p>
<p>Consider next the operators. The momentum operator on 'V 1 , which used to be 
-i1i dfdxt now becomes -i1i a;axt, where the partial derivative symbol tells us it 
</p>
<p>253 
</p>
<p>SYSTEMS WITH 
</p>
<p>N DEGREES 
</p>
<p>OF FREEDOM </p>
<p/>
</div>
<div class="page"><p/>
<p>254 
</p>
<p>CHAPTER 10 
</p>
<p>operates on x 1 as before and leaves x 2 alone. This is the coordinate space version of 
p(IJ&reg;( 2J=PPl&reg;J( 2 J_ You are encouraged to pursue this analysis further. 
</p>
<p>Evolution of the Two-Particle State Vector 
</p>
<p>The state vector of the system is an element of 
</p>
<p>to the equation 
</p>
<p>. It evolves in time according 
</p>
<p>l pl p~ l ifilljt)= &middot; 1&middot;&middot;+ --"--+ V(X1, X2) I if!)= HI If/) 
lm1 lm2 
</p>
<p>There arc two classes of problems. 
</p>
<p>Class A: His separable, i.e., 
</p>
<p>Class B: H is not separable, i.e., 
</p>
<p>and 
</p>
<p>( 10. 1.23) 
</p>
<p>(10.1.24) 
</p>
<p>(10.1.25) 
</p>
<p>Class A corresponds to two particles interacting with external potentials V1 and V2 
</p>
<p>but not with each other, while in class B there is no such restriction. We now examine 
</p>
<p>these two classes. 
</p>
<p>Class A: Separable Hamiltonians. Classically, the decomposition 
</p>
<p>means that the two particles evolve independently of each other. In particular, their 
</p>
<p>energies are separately conserved and the total energy E is E1 + E2. Let us see these 
results reappear in quantum theory. For a stationary state, 
</p>
<p>llfl(t)) =IE) e-/Er;ll (10.1.26) 
</p>
<p>Eq. (10.1.23) becomes 
</p>
<p>(10.1.27) </p>
<p/>
</div>
<div class="page"><p/>
<p>Since [HI. H2] = 0 [Eq. (10.1.17a)] we can find their simultaneous eigenstates, which 
are none other than 1Et)&reg;IE2)= IEtE2), where lEt) and IE2) are solutions to 
</p>
<p>(10.1.28a) 
</p>
<p>and 
</p>
<p>(10.1.28b) 
</p>
<p>It should be clear that the state IEt)&reg;IE2) corresponds to particle 1 being in 
the energy eigenstate lEt) and particle 2 being in the energy eigenstate IE2). Clearly 
</p>
<p>so that 
</p>
<p>(10.1.28c) 
</p>
<p>(The basis I Et &gt;&reg;I E2) is what we would get if in forming basis vectors of the direct 
product 'Vt&reg;'V2 , we took the energy eigenvalues from each space, instead of, say, 
the position eigenvectors.) Finally, feeding IE)=IEt)&reg;IE2), E=Et+E2 into Eq. 
(10.1.26) we get 
</p>
<p>(10.1.29) 
</p>
<p>It is worth rederiving Eqs. (10.1.28) and (10.1.29) in the coordinate basis to 
illustrate a useful technique that you will find in other textbooks. By projecting the 
eigenvalue Eq. (10.1.27) on this basis, and making the usual operator substitutions, 
Eq. (10.1.4), we obtain 
</p>
<p>where 
</p>
<p>(10.1.30) 
</p>
<p>We solve the equation by the method of separation of variables. We assume 
</p>
<p>(10.1.31) 
</p>
<p>The subscripts Et and E2 have no specific interpretation yet and merely serve as 
labels. Feeding this ansatz into Eq. (10.1.30) and then dividing both sides by 
</p>
<p>255 
</p>
<p>SYSTEMS WITH 
</p>
<p>N DEGREES 
OF FREEDOM </p>
<p/>
</div>
<div class="page"><p/>
<p>256 
</p>
<p>CHAPTER 10 
</p>
<p>(10.1.32) 
</p>
<p>This equation says that a function of x1 alone, plus one of x2 alone, equals a constant 
</p>
<p>E. Since x1 and x2 , and hence the two functions, may be varied independently, it 
</p>
<p>follows that each function separately equals a constant. We will call these constants 
</p>
<p>E 1 and E2 &bull; Thus Eq. (10.1.32) breaks down into three equations: 
</p>
<p>( 10.1.33) 
</p>
<p>Consequently 
</p>
<p>lf/ E(XJ, X2, t) = lf/ E(XJ, X2) e -iEt/~ 
</p>
<p>= lf/E,(XI) e-iE,t/~lf/E2(X2) e-iE,t/~ (10.1.34) 
</p>
<p>where lfl E, and lfl &pound; 2 are eigenfunctions of the one-particle Schrodinger equation with 
</p>
<p>eigenvalues E1 and E2 , respectively. We recognize Eqs. (10.1.33) and (10.1.34) to be 
</p>
<p>the projections ofEqs. (10.1.28) and (10.1.29) on lxix2)=1xi)&reg;Ix2). 
</p>
<p>Case B: Two Interacting Particles. Consider next the more general problem of 
</p>
<p>two interacting particles with 
</p>
<p>( 10.1.35) 
</p>
<p>where 
</p>
<p>Generally this cannot be reduced to two independent single-particle problems. If, 
</p>
<p>however, 
</p>
<p>V(x1, x2) = V(x1 - x2) (10.1.36) </p>
<p/>
</div>
<div class="page"><p/>
<p>which describes two particles responding to each other but nothing external, one can 
</p>
<p>always, by employing the CM coordinate 
</p>
<p>m1x1 +m2x2 
XcM= (10.1.37a) 
</p>
<p>and the relative coordinate 
</p>
<p>( 10.1.37b) 
</p>
<p>reduce the problem to that of two independent fictitious particles: one, the CM, 
</p>
<p>which is free, has mass M = m 1 + m2 and momentum 
</p>
<p>and another, with the reduced mass 11 = m 1m2/(m 1 + m2), momentum p = tLic, moving 
under the influence of V(x): 
</p>
<p>2 2 
.. . . PcM P 
</p>
<p>= Jfti_,M + ,Jif,'elative =-+-+ V( X) 
2M 211 
</p>
<p>(10.1.38) 
</p>
<p>which is just the result from Exercise 2.5.4 modified to one dimension. Since the new 
variables are also canonical (Exercise 2. 7 .6) and Cartesian, the quantization condi-
tion is just 
</p>
<p>[X eM, PcM] =iii 
</p>
<p>[X, P] =tn 
</p>
<p>and all other commutators zero. In the quantum theory, 
</p>
<p>and the eigenfunctions of H factorize: 
</p>
<p>eiPcM XcM/1! 
</p>
<p>l/fE(XcM, x) = (2rr1i) 1/ 2 &middot; lf/E",(x) 
</p>
<p>2 
</p>
<p>E=PcM+E 
2lvf rei 
</p>
<p>(10.1.39a) 
</p>
<p>(10.l.39b) 
</p>
<p>(10.1.40) 
</p>
<p>(10.1.41) 
</p>
<p>The real dynamics is contained in l/1 E,jx) which is the energy eigenfunction for a 
particle of mass 11 in a potential V(x). Since the CM drifts along as a free particle, 
one usually chooses to study the problem in the CM frame. In this case EcM = 
</p>
<p>257 
</p>
<p>SYSTEMS WITH 
</p>
<p>N DEGREES 
</p>
<p>OF FREEDOM </p>
<p/>
</div>
<div class="page"><p/>
<p>258 
</p>
<p>CHAPTER 10 
</p>
<p>p~~M/2M drops out of the energy, and the plane wave factor in lfl representing CM 
</p>
<p>motion becomes a constant. In short, one can forget all about the CM in the quantum 
theory just as in the classical theory. 
</p>
<p>N Particles in One Dimension 
</p>
<p>All the results but one generalize from N = 2 to arbitrary N. The only exception 
</p>
<p>is the result from the last subsection: for N&gt; 2, one generally cannot, by using CM 
and relative coordinates (or other sets of coordinates) reduce the problem to N 
</p>
<p>independent one-particle problems. There are a few exceptions, the most familiar 
</p>
<p>ones being Hamiltonians quadratic in the coordinates and momenta which may be 
</p>
<p>reduced to a sum over oscillator Hamiltonians by the use of normal coordinates. In 
</p>
<p>such cases the oscillators become independent and their energies add both in the 
</p>
<p>classical and quantum cases. This result (with respect to the quantum oscillators) 
</p>
<p>was assumed in the discussion on specific heats in Chapter 7. 
</p>
<p>Exercise 10. 1.3. * Consider the Hamiltonian of the coupled mass system. 
</p>
<p>Jif = 
</p>
<p>2m 
</p>
<p>l ' 0 , 
+ mw-[x!+x~+(x 1 -
</p>
<p>2m 2 
</p>
<p>We know from Example I .8.6 that .Jf' can be decoupled if we usc normal coordinates 
</p>
<p>and the corresponding momenta 
</p>
<p>(!) Rewrite .W in terms of normal coordinates. Verify that the normal coordinates are 
</p>
<p>also canonical, i.e., that 
</p>
<p>p1} = 8,1 etc.; i,j= !, ll 
</p>
<p>Now quantize the system, promoting these variables to operators obeying 
</p>
<p>i,j= L II 
</p>
<p>Write the eigenvalue equation for H in the simultaneous eigenbasis of X, and X 11 &bull; 
</p>
<p>(2) Quantize the system directly, by promoting x 1 , x2 , p 1 , and Jh to quantum operators. 
</p>
<p>Write the eigenvalue equation for H in the simultaneous eigenbasis of X1 and X 2 . Now change 
</p>
<p>from x 1 , x 2 (and of course D 1 i'x 1 , a; cx2 ) to x 1 &bull; x 11 (and 8 I cx1 , i!/ cx11 ) in the differential 
equation. You should end up with the result from part (I). 
</p>
<p>In general, one can change coordinates and then quantize or first quantize and then 
</p>
<p>change variables in the ditferential equation, if the change of coordinates is canonical. (We 
</p>
<p>are assuming that all the variables are Cartesian. As mentioned earlier in the book, if one wants </p>
<p/>
</div>
<div class="page"><p/>
<p>to employ non-Cartesian coordinates, it is best to first quantize the Cartesian coordinates and 
</p>
<p>then change variables in the differential equation.) 
</p>
<p>10.2. More Particles in More Dimensions 
</p>
<p>Mathematically, the problem of a single particle in two dimensions (in terms of 
Cartesian coordinates) is equivalent to that of two particles in one dimension. It is, 
however, convenient to use a different notation in the two cases. We will denote the 
two Cartesian coordinates of the single particle by x and y rather than x 1 and x 2 &bull; 
</p>
<p>Likewise the momenta will be denoted by Px and py. The quantum operators will be 
called X and Y; and P", and Pn their common eigenkets lxy), IPxPy). respectively, 
and so on. The generalization to three dimensions is obvious. We will also write a 
position eigenket as lr) and the orthonormality relation (xyzlx'y'z') = 
8(x-x')8(y-y')8(z-z') as (rlr')=o3(r-r'). The same goes for the momentum 
</p>
<p>eigenkets IP) also. When several particles labeled by numbers 1, ... , N are involved, 
this extra label will also be used. Thus lp1p2 ) will represent a two-particle state in 
which particle 1 has momentum p1 and particle 2 has momentum p2 and so 011. 
</p>
<p>Exercise 10.2.1 * (Particle in a Three-Dimensional Box). Recall that a particle in a one-
dimensional box extending from x = 0 to Lis confined to the region 0 ~ x ~ L; its wave function 
</p>
<p>vanishes at the edges x = 0 and Land beyond (Exercise 5.2.5). Consider now a particle confined 
</p>
<p>in a three-dimensional cubic box of volume L 3 . Choosing as the origin one of its corners, and 
</p>
<p>the x, y, and z axes along the three edges meeting there, show that the normalized energy 
</p>
<p>eigenfunctions are 
</p>
<p>( )
1;2 ( )(. ,li2 1 \( )1;2 &bull; \ 
</p>
<p>'I'E(X,y,z)= ~ sin '!:_x_'!__.&gt;:_ 1-?) sini:'IL'!.}') ?_ sin(:'l_c:'!_::) 
L ' L \L \ L I L, . L I 
</p>
<p>where 
</p>
<p>and n1 are positive integers. 
</p>
<p>Exercise 10.2.2. * Quantize the two-dimensional oscillator for which 
</p>
<p>p~ + p~ 1 , 2 I 2 2 
:Yf=----~+--mw:x +-mw,y 
</p>
<p>2m 2 2 
</p>
<p>(I) Show that the allowed energies are 
</p>
<p>E= (n,+ 1/2)1iwx+ (n, + l/2)1iw, n,, n,.=O, I, 2, ... 
</p>
<p>(2) Write down the corresponding wave functions in terms of single oscillator wave 
</p>
<p>functions. Verify that they have definite parity (even/odd) number x---&gt;-x, y---&gt;-y and that 
the parity depends only on n = nx + n,. 
</p>
<p>259 
</p>
<p>SYSTEMS WUH 
</p>
<p>N DEGREES 
</p>
<p>OF FREEDOM </p>
<p/>
</div>
<div class="page"><p/>
<p>260 
</p>
<p>CHAPTER 10 
</p>
<p>t=T 04 
</p>
<p>' 
</p>
<p>&gt; &lt;, 
,',' '\\ 
</p>
<p>' ' ' ' ,' \, 
,' ',, 
</p>
<p>d 'o 
t=O 2 
</p>
<p>Figure 10.1. Two identical billiard balls start near holes I and 2 and 
</p>
<p>end up in holes 3 and 4, respectively, as predicted by P 1 &bull; The pre-
</p>
<p>diction of P2 , that they would end up in holes 4 and 3, respectively, 
</p>
<p>is wrong, even though the two final configurations would be indis-
</p>
<p>tinguishable to an observer who walks in at t= T. 
</p>
<p>(3) Consider next the isotropic oscillator (rox=roy). Write explicit, normalized eigen-
functions of the first three states (that is, for the cases n = 0 and I). Reexpress your results in 
terms of polar coordinates p and 4J (for later use). Show that the degeneracy of a level with 
E=(n+l)liro is (n+l). 
</p>
<p>Exercise 10.2.3. * Quantize the three-dimensional isotropic oscillator for which 
</p>
<p>(I) Show that E= (n + 3/2)/iro; n = nx+ n,. + n=; nx, n,., n= = 0, I, 2, .... 
(2) Write the corresponding eigenfunctions in terms of single-oscillator wave functions 
</p>
<p>and verify that the parity of the level with a given n is (-I)". Reexpress the first four states 
</p>
<p>in terms of spherical coordinates. Show that the degeneracy of a level with energy E= 
(n+3/2)1iro is (n+ l)(n+2)/2. 
</p>
<p>10.3. Identical Particles 
</p>
<p>The formalism developed above, when properly applied to a system containing 
</p>
<p>identical particles, leads to some very surprising results. We shall say two particles 
</p>
<p>are identical if they are exact replicas of each other in every respect-there should 
</p>
<p>be no experiment that detects any intrinsict difference between them. Although the 
</p>
<p>definition of identical particles is the same classically and quantum mechanically, 
</p>
<p>the implications are different in the two cases. 
</p>
<p>The Classical Case 
</p>
<p>Let us first orient ourselves by recapitulating the situation in classical physics. 
</p>
<p>Imagine a billiard table with four holes, numbered 1 through 4 (Fig. 10.1). Near 
</p>
<p>holes 1 and 2 rest two identical billiard balls. Let us call these balls 1 and 2. The 
</p>
<p>difference between the labels reflects not any intrinsic difference in the balls (for they 
</p>
<p>are identical) but rather a difference in their environments, namely, the holes near 
</p>
<p>which they find themselves. 
</p>
<p>t By intrinsic I mean properties inherent to the particle, such as its charge or mass and not its location 
or momentum. </p>
<p/>
</div>
<div class="page"><p/>
<p>Now it follows from the definition of identity, that if these two balls are 
exchanged, the resulting configuration would appear exactly the same. Nonetheless 
these two configurations are treated as distinct in classical physics. In order for this 
distinction to be meaningful, there must exist some experiments in which these two 
configurations are inequivalent. We will now discuss one such experiment. 
</p>
<p>Imagine that at time t = 0, two players propel the balls toward the center of the 
table. At once two physicists P 1 and P2 take the initial-value data and make the 
following predictions: 
</p>
<p>P,: 
ball 1 goes to hole 3} 
</p>
<p>at t=T 
ball 2 goes to hole 4 
</p>
<p>Pz: 
ball 1 goes to hole 4} 
</p>
<p>at t= T 
ball 2 goes to hole 3 
</p>
<p>Say at time T we find that ball 1 ends up in hole 3 and ball 2 in hole 4. We 
declare that P 1 is correct and P2 is wrong. Now, the configurations predicted by 
them for t = T differ only by the exchange of two identical particles. If seen in 
isolation they would appear identical: an observer who walks in just at t = T and is 
given the predictions of P 1 and P2 will conclude that both are right. What do we 
know about the balls (that allows us to make a distinction between them and hence 
the two outcomes), that the newcomer does not? The answer of course is-their 
histories. Although both balls appear identical to the newcomer, we are able to trace 
the ball in hole 3 back to the vicinity of hole 1 and the one in hole 4 back to hole 
2. Similarly at t = 0, the two balls which seemed identical to us would be distin-
guishable to someone who had been following them from an earlier period. Now of 
course it is not really necessary that either we or any other observer be actually 
present in order for this distinction to exist. One imagines in classical physics the 
fictitious observer who sees everything and disturbs nothing; if he can make the 
distinction, the distinction exists. 
</p>
<p>To summarize, it is possible in classical mechanics to distinguish between ident-
ical particles by following their nonidentical trajectories (without disturbing them in 
any way). Consequently two configurations related by exchanging the identical parti-
cles are physically nonequivalent. 
</p>
<p>An immediate consequence of the above reasoning, and one that will play a 
dominant role in what follows, is that in quantum theory, which completely outlaws 
the notion of continuous trajectories for the particles, there exists no physical basis 
for distinguishing between identical particles. Consequently two configurations 
related by the exchange of identical particles must be treated as one and the same 
configuration and described by the same state vector. We now proceed to deduce 
the consequences of this restriction. 
</p>
<p>Two-Particle Systems-Symmetric and Antisymmetric States 
</p>
<p>Suppose we have a system of two distinguishable particles 1 and 2 and a position 
measurement on the system shows particle 1 to be at x =a and particle 2 to be at 
</p>
<p>261 
</p>
<p>SYSTEMS WITH 
N DEGREES 
</p>
<p>OF FREEDOM </p>
<p/>
</div>
<div class="page"><p/>
<p>262 
</p>
<p>CHAPTER 10 
</p>
<p>x =b. We write the state just after measurement as 
</p>
<p>IIJI) = lx1 =a, X2 =b)= lab) (10.3.1) 
</p>
<p>where we are adopting the convention that the state of particle l is described by the 
</p>
<p>first label (a) and that of particle 2 by the second label (b). Since the particles are 
</p>
<p>distinguishable, the state obtained by exchanging them is distinguishable from the 
</p>
<p>above. It is given by 
</p>
<p>IIJI&gt;=Iba) 
</p>
<p>and corresponds to having found particle 1 at b and particle 2 at a. 
</p>
<p>Suppose we repeat the experiment with two identical particles and catch one at 
</p>
<p>x=a and the other at x=b. Is the state vector just after measurement lab) or lba)? 
</p>
<p>The answer is, neither. We have seen that in quantum theory two configurations 
</p>
<p>related by the exchange of identical particles must be viewed as one and the same 
</p>
<p>and be described by the same state vector. Since IIJI) and allJI) are physically equiva-
</p>
<p>lent, we require that IIJI(a, b)), the state vector just after the measurement, satisfy 
</p>
<p>the constraint 
</p>
<p>IIJI(a, b))= allJI(b, a)) ( 10.3.2) 
</p>
<p>where a is any complex number. Since under the exchange 
</p>
<p>lab) &lt;-+lba) 
</p>
<p>and the two vectors are not multiples of each otheri (i.e., are physically distinct) 
</p>
<p>neither is acceptable. The problem is that our position measurement yields not an 
</p>
<p>ordered pair of numbers (as in the distinguishable particle case) but just a pair of 
</p>
<p>numbers: to assign them to the particles in a definite way is to go beyond what is 
</p>
<p>physically meaningful in quantum theory. What our measurement does permit us to 
</p>
<p>conclude is that the state vector is an eigenstate of X 1 + X 2 with eigenvalue a+ b, the 
sum of the eigenvalue being insensitive to how the values a and b are assigned to 
</p>
<p>the particles. In other words, given an unordered pair of numbers a and b we can 
</p>
<p>still define a unique sum (but not difference). Now, there are just two product vectors, 
</p>
<p>lab) and lba) with this eigenvalue, and the state vector lies somewhere in the two-
</p>
<p>dimensional degenerate (with respect to X 1 + X2) eigenspace spanned by them. Let 
IIJI(a, b))= fJiab) + rlba) be the allowed vector. If we impose the constraint 
Eq. (10.3.2): 
</p>
<p>f31ab) + rlba) = a[f31ba) + rlab)] 
</p>
<p>we find, upon equating the coefficients of lab) and lba) that 
</p>
<p>f3 = ar, r=af3 
</p>
<p>t We are assuming a#b. If a=b, the state is acceptable, but the choice we are agonizing over does not 
arise. </p>
<p/>
</div>
<div class="page"><p/>
<p>so that 
</p>
<p>a=&plusmn;l (10.3.3) 
</p>
<p>It is now easy to construct the allowed state vectors. They are 
</p>
<p>lab, S) =lab)+ lba) (10.3.4) 
</p>
<p>called the symmetric state vector (a = 1) and 
</p>
<p>lab, A)= lab) -lba) (10.3.5) 
</p>
<p>called the antisymmetric state vector (a= -1). (These are unnormalizedvectors. Their 
normalization will be taken up shortly.) 
</p>
<p>More generally, if some variable n is measured and the values m1 and m2 are 
obtained, the state vector immediately following the measurement is either lm 1m2, S) 
or lm 1m2 , A).t Although we have made a lot of progress in nailing down the state 
vector corresponding to the measurement, we have still to find a way to choose 
between these two alternatives. 
</p>
<p>Bosons and Fermions 
</p>
<p>Although both Sand A states seem physically acceptable (in that they respect 
the indistinguishability of the particles) we can go a step further and make the 
following assertion: 
</p>
<p>A given species of particles must choose once and for all between Sand A states. 
</p>
<p>Suppose the contrary were true, and the Hilbert space of two identical particles 
contained both S and A vectors. Then the space also contains linear combinations 
such as 
</p>
<p>which are neither symmetric nor antisymmetric. So we rule out this possibility. 
Nature seems to respect the constraints we have deduced. Particles such as the 
</p>
<p>pion, photon, and graviton are always found in symmetric states and are called 
bosons, and particles such as the electron, proton, and neutron are always found in 
antisymmetric states and are called fermions. 
</p>
<p>Thus if we catch two identical bosons, one at x =a and the other at x = b, the 
state vector immediately following the measurement is 
</p>
<p>I 'I')= lx1 =a, x2=b)+ lx1 =b, x2=a) 
</p>
<p>=lab)+ lba) =lab, S) 
</p>
<p>t We are assuming n is nondegenerate. If not, let m represent the eigenvalues of a complete set of 
commuting operators. 
</p>
<p>263 
</p>
<p>SYSTEMS WITH 
N DEGREES 
</p>
<p>OF FREEDOM </p>
<p/>
</div>
<div class="page"><p/>
<p>264 
</p>
<p>CHAPTER 10 
</p>
<p>Had the particles been fermions, the state vector after the measurement would have 
</p>
<p>been 
</p>
<p>I 'I')= lxt =a, Xz =b)-lxl =b, xz=a) =lab) -lba) 
</p>
<p>=lab, A) 
</p>
<p>Note that although we still use the labels x 1 and x2 , we do not attach them to the 
</p>
<p>particles in any particular way. Thus having caught the bosons at x=a and x=b, 
</p>
<p>we need not agonize over whether x 1 =a and x2 = b or vice versa. Either choice leads 
</p>
<p>to the same I 'I') for bosons, and to state vectors differing only by an overall sign 
for fermions. 
</p>
<p>We are now in a position to deduce a fundamental property of fermions, which 
</p>
<p>results from the antisymmetry of their state vectors. Consider a two-fermion state 
</p>
<p>Let us now set m1 = m2 = m. We find 
</p>
<p>lmm, A)=lmm)-lmm)=O (10.3.6) 
</p>
<p>This is the celebrated Pauli exclusion principle: Two identical fermions cannot be in 
</p>
<p>the same quantum state. This principle has profound consequences-in statistical 
</p>
<p>mechanics, in understanding chemical properties of atoms, in nuclear theory, astro-
</p>
<p>physics, etc. We will have occasion to return to it often. 
</p>
<p>With this important derivation out of our way, let us address a question that 
</p>
<p>may have plagued you: our analysis has only told us that a given type of particle, 
</p>
<p>say a pion, has to be either a boson or a fermion, but does not say which one. There 
</p>
<p>are two ways to the answer. The first is by further cerebration, to be specific, within 
</p>
<p>the framework of quantum field theory, which relates the spin of the particle to its 
</p>
<p>"statistics"-which is the term physicists use to refer to its bosonic or fermionic 
</p>
<p>nature. Since the relevant arguments are beyond the scope of this text I merely quote 
</p>
<p>the results here. Recall that the spin of the particle is its internal angular momentum. 
</p>
<p>The magnitude of spin happens to be an invariant for a particle (and thus serves as 
</p>
<p>a label, like its mass or charge) and can have only one of the following values: 0, 
</p>
<p>'li/2, 'li, 3'1ij2, 2'/i, .... The spin statistics theorem, provable in quantum field theory, 
</p>
<p>asserts that particles with (magnitude of spin) equal to an even multiple of 'li/2 are 
</p>
<p>bosons, and those with spin equal to an odd multiple of fz/2 are fermions. However, 
</p>
<p>this connection, proven in three dimensions, does not apply to one dimension, where 
</p>
<p>it is not possible to define spin or any form of angular momentum. (This should be 
clear classically.) Thus the only way to find if a particle in one dimension is a boson 
</p>
<p>or fermion is to determine the symmetry of the wave function experimentally. This 
</p>
<p>is the second method, to be discussed in a moment. 
Before going on to this second method, let us note that the requirement that 
</p>
<p>the state vector of two identical particles be symmetric or antisymmetric (under the 
</p>
<p>exchange of the quantum numbers labeling them) applies in three dimensions as 
</p>
<p>well, as will be clear by going through the arguments in one dimension. The only 
</p>
<p>difference will be the increase in the number of labels. For example, the position </p>
<p/>
</div>
<div class="page"><p/>
<p>eigenket of a spin-zero boson will be labeled by three numbers x, y, and z. For 
fermions, which have spin at least equal to fi/2, the states will be labeled by the 
</p>
<p>orientation of the spin as well as the orbital labels that describe spinless bosons.t 
We shall consider just spin-~ particles, for which this label can take only two values, 
</p>
<p>call them + and - or spin up and down (the meaning of these terms will be clear 
later). If we denote by m all the orbital labels and by s the spin label, the state vector 
of the fermion that is antisymmetric under the exchange of the particles, i.e., under 
</p>
<p>the exchange of all the labels, will be of the form 
</p>
<p>(10.3.7) 
</p>
<p>We see that the state vector vanishes if 
</p>
<p>and s1 =s2 (10.3.8) 
</p>
<p>Thus we find once again that two fermions cannot be in the same quantum state, 
</p>
<p>but we mean by a quantum state a state of definite m and s. Thus two electrons can 
be in the same orbital state if their spin orientations are different. 
</p>
<p>We now turn to the second way of finding the statistics of a given species of 
</p>
<p>particles, the method that works in one or three dimensions, because it appeals to a 
</p>
<p>simple experiment which determines whether the two-particle state vector is symmet-
</p>
<p>ric or antisymmetric for the given species. As a prelude to the discussion of such an 
experirn~nt, let us study in some detail the Hilbert space of bosons and fermions. 
</p>
<p>Bosonic and Fermionic Hilbert Spaces 
</p>
<p>We have seen that two identical bosons will always have symmetric state vectors 
and two identical fermions will always have antisymmetric state vectors. Let us call 
</p>
<p>the Hilbert space of symmetric bosonic vectors 'Vs and the Hilbert space of the 
</p>
<p>antisymmetric fermionic vectors VA. We first examine the relation between these 
</p>
<p>two spaces on the one hand and the direct product space 'V 102 on the other. 
</p>
<p>The space 'V1 0 2 consists of all vectors of the form lmlmz)=lm 1)&reg;lm2). To 
</p>
<p>each pair of vectors I m 1 =a, m2 =b) and I m 1 = b, m2 =a) there is one ( unnormalized) 
bosonic vector I m 1 =a, mz =b)+ I m 1 = h, m2 =a) and one fermionic vector I m 1 = 
a, mz =b) -I m 1 =a, m2 =b). If a= b; the vector I m 1 =a, m2 =a) is already symmetric 
and we may take it to be the bosonic vector. There is no corresponding fermionic 
vector (the Pauli principle). Thus 'V 102 has just enough basis vectors to form one 
</p>
<p>bosonic Hilbert space and one fermionic Hilbert space. We express this relation as 
</p>
<p>W 1 &reg;2 = 'V s\TJ 'VA ( 10.3.9) 
</p>
<p>t Since spin has no classical counterpart, the operator representing it is not a function of the coordinate 
and momentum operators and it commutes with any orbital operator n. Thus spin may be specified 
simultaneously with the orbital variables. 
</p>
<p>265 
</p>
<p>SYSTEMS WITH 
</p>
<p>N DEGREES 
</p>
<p>OF FREEDOM </p>
<p/>
</div>
<div class="page"><p/>
<p>266 
</p>
<p>CHAPTER 10 
</p>
<p>with 'V s getting slightly more than half the dimensionality of 'V 1 ca 2 .t Our analysis 
has shown that at any given time, the state of two bosons is an element of 'V s and 
</p>
<p>that of two fermions an element of \VA. It can also be shown that a system that 
</p>
<p>starts out in 'Ws('VA) remains in 'V 5 ('WA) (see Exercise 10.3.5). Thus in studying two 
</p>
<p>identical particles we need only consider or \V 4 . It is however convenient, for 
</p>
<p>bookkeeping purposes. to view 'Vs and w A as subspaces of 'V 1e2 and the elements 
of 'Vs or 'VA as elements also of w 1&reg;:'. 
</p>
<p>Let us now consider the normalization of the vectors in \J 5 . Consider first the 
</p>
<p>eigenkets I OJ 1 u)], S) corresponding to a variable Q with discrete eigenvalues. The 
unnormalized state vector is 
</p>
<p>Since I OJ 1 Cih) and I w 2w 1) are orthonormal states in 'W 1 cg 2 , the normalization factor 
is just T 112 , i.e., 
</p>
<p>(l 0.3.1 Oa) 
</p>
<p>is a normalized eigenvector. You may readily check that (m1m2, Slm1oh, S) = l. 
</p>
<p>The preceding discussion assumes OJ 1 T- w2 . If w 1 = m2 = m the product ket I ww) is 
itself both symmetric and normalized and we choose 
</p>
<p>lwm, S)=IOJ(O) ( l0.3.10b) 
</p>
<p>Any vector ll.fls) m 'V.s may be expanded in terms of this Q basis. As usual we 
identify 
</p>
<p>(10.3.11) 
</p>
<p>as the absolute probability of finding the particles in state lm1m2, S) when ann 
</p>
<p>measurement is made on a system in state llfls). The normalization condition of 
</p>
<p>ll.fls) and Ps(WJ. (02) may be written as 
</p>
<p>l=(l.flsll.fls)=I l(w,wz,SII.fls)l 2 
</p>
<p>clist 
</p>
<p>= '\' p &middot;(W ()) ) L.. ,) I ~ 2 (l0.3.l2a) 
dist 
</p>
<p>where Ictist denotes a sum over all physically distinct states. If w 1 and W2 take values 
</p>
<p>between OJ min and CO max, then 
</p>
<p>)= 
'-' 
</p>
<p>di:H 
</p>
<p>I I (10.3.12b) 
(0~ = Wmin (JJ 1 = Wmm 
</p>
<p>In this manner we avoid counting both lw 1w2 , S) and lm 2w1, S'), which are phys-
ically equivalent. Another way is to count them both and then divide by 2. 
</p>
<p>t Since every element of V sis perpendicular to every element of VA (you should check this) the dimension-
ality of 'W 102 equals the sum of the dimensionalities of 'Ws and 'VA. </p>
<p/>
</div>
<div class="page"><p/>
<p>What if we want the absolute probability density for some continuous variable 
such as X? In this case we must take the projection of Ill's) on the normalized 
position eigenket: 
</p>
<p>(10.3.13) 
</p>
<p>to obtain 
</p>
<p>(10.3.14) 
</p>
<p>The normalization condition for Ps(x., xz) and Ill's) is 
</p>
<p>(10.3.15) 
</p>
<p>where the factor 1/2 makes up for the double counting done by the dx1 dxz integra-
tion.t In this case it is convenient to define the wave function as 
</p>
<p>(10.3.16) 
</p>
<p>so that the normalization of l/f s is 
</p>
<p>(10.3.17) 
</p>
<p>However, in this case 
</p>
<p>( 10.3.18) 
</p>
<p>due to the rescaling. Now, note that 
</p>
<p>1 1 
l/fs(xl, x2) = il2 &lt;x&bull;xz, Sll/fs)=2 [(xlx2ll/ls)+(xzxd l/fs)] 
</p>
<p>= (xlx2ll/ls) (10.3.19) 
</p>
<p>where we have exploited the fact that Ill's) is symmetrized between the particles and 
has the same inner product with (x.x2 1 and (x2xd. Consequently, the normalization 
</p>
<p>t The points x 1 = x2 = x pose some subtle questions both with respect to the factor I /2 and the normaliza-
tion of the kets lxx, S). We do not get into these since the points on the line x 1 =x2 =x make only an 
infinitesimal contribution to the integration in the x 1 - x2 plane (of any smooth function). In the follow-
ing discussion you may assume that quantities such as P5 (x, x}, 1/ls(x, x) are all given by the limits 
x,--.x,-+x of P5 (x,, x2), 1/fs(x,, x2}, etc. 
</p>
<p>267 
</p>
<p>SYSTEMS WITH 
N DEGREES 
</p>
<p>OF FREEDOM </p>
<p/>
</div>
<div class="page"><p/>
<p>268 
</p>
<p>CHAPTER 10 
</p>
<p>condition Eq. (10.3.17) becomes 
</p>
<p>which makes sense, as llf/s) is an element of 'V1 0 2 as well. Note, however, that the 
</p>
<p>kets !x1x2) enter the definition of the wave function Eq. (10.3.19), and the normaliza-
</p>
<p>tion integral above, only as bookkeeping devices. They are not elements of 'V s and 
</p>
<p>the inner product (x1x2ilfl) would be of no interest to us, were it not for the fact 
</p>
<p>that the quantity that is of physical interest (x1x2, Sllf/ 5 ), is related to it by just a 
</p>
<p>scale factor of 2112. Let us now consider a concrete example. We measure the energy 
</p>
<p>of two noninteracting bosons in a box extending from x = 0 to x = L and find them 
</p>
<p>to be in the quantum states n = 3 and n = 4. The normalized state vector just after 
</p>
<p>measurement is then 
</p>
<p>I &gt;_13,4)+!4,3) lf/s - i12 (10.3.20) 
</p>
<p>in obvious notation. The wave function is 
</p>
<p>1 
=--112- [(x1x2!3, 4) + (xlx2!4, 3)+ (x2xd3, 4) + (x2xd4, 3)] 
</p>
<p>2(2 ) 
</p>
<p>1 
= 2(2112) [lf/J(xJ)lf/4(x2) + lf/4(xJ)lf/J(x2) + lf/3(x2)lf/4(x1) 
</p>
<p>+ lf/4(x2)lf/3(xJ)] 
</p>
<p>= T 112[ lf/3(xl) lf/4(x2) + lf/4(xJ) lf/3(x2)] 
</p>
<p>=(xlx2llf/s) (10.3.2la) 
</p>
<p>where in all of the above, 
</p>
<p>2 . mrx ( )
1/2 ( ) 
</p>
<p>lf!n(x)= L sm L (10.3.21b) 
</p>
<p>These considerations apply with obvious modifications to the fermionic space 
</p>
<p>'VA . The basis vectors are of the form 
</p>
<p>( 10.3.22) </p>
<p/>
</div>
<div class="page"><p/>
<p>(The case co 1 = co 2 does not arise here.) The wave function is once again 
</p>
<p>and as in the bosonic case 
</p>
<p>lf/A(xi, x2) =T11\x1x2, Allf/A) 
</p>
<p>= (XIX2Ilf/ A) 
</p>
<p>The normalization condition is 
</p>
<p>(10.3.23) 
</p>
<p>(10.3.24) 
</p>
<p>(10.3.25) 
</p>
<p>Returning to our example of two particles in a box, if we had obtained the 
values n = 3 and n = 4, then the state just after measurement would have been 
</p>
<p>I &gt;_13,4)-14,3) 
lf/A - i/2 (10.3.26) 
</p>
<p>(We may equally well choose 
</p>
<p>I &gt;_14,3)-13,4) lfl A - i/2 
</p>
<p>which makes no physical difference). The corresponding wave function may be 
written in the form of a determinant: 
</p>
<p>= TI/2,lf/3(xi) 
</p>
<p>lf/J(x2) 
</p>
<p>Had we been considering the state lco 1co2 , A) [Eq. (10.3.22)]J 
</p>
<p>( ) _ 2-I/21lfl.,,(xi) lf/A X1,X2-
lfl.,,(x2) 
</p>
<p>Determination of Particle Statistics 
</p>
<p>lfl ,,(xi) I 
lfl .,,(x2) 
</p>
<p>(10.3.27) 
</p>
<p>(10.3.28) 
</p>
<p>We are finally ready to answer the old question: how does one determine empir-
ically the statistics of a given species, i.e., whether it is a boson or fermion, without 
turning to the spin statistics theorem? For concreteness, let us say we have two 
identical noninteracting pions and wish to find out if they are bosons or fermions. 
</p>
<p>t The determinantal form of I{/ A makes it clear that I{/ A vanishes if x 1 =x2 or ro 1 =w2 &bull; 
</p>
<p>269 
</p>
<p>SYSTEMS WITH 
N DEGREES 
</p>
<p>OF FREEDOM </p>
<p/>
</div>
<div class="page"><p/>
<p>270 
</p>
<p>CHAPTER 10 
</p>
<p>We proceed as follows. We put them in a one-dimensional bod and make an energy 
</p>
<p>measurement. Say we find one in the state n = 3 and the other in the state n = 4. The 
</p>
<p>probability distribution in x space would be, depending on their statistics, 
</p>
<p>Ps;AXt, Xz) =21 'l's;Ax,, Xz)l 2 
</p>
<p>= 2IT 112[ 'l'3(xt) 'lfiXz) &plusmn;'If 4(Xt)'I'3(Xz)]l 2 
</p>
<p>=I 'l'3(x,)l 2 1 'l'4(xz)l 2 + I 'l'4(x,)l 2 1 'l'3(xz)l 2 
</p>
<p>&plusmn; ['lff(xt)'lf4(x,)'l'i(xz)'l'3(xz) + 'l'i(x,)'l'3(x,)'l'f&lt;xz)'l'4(xz)] (10.3.29) 
</p>
<p>Compare this situation with two particles carrying labels 1 and 2, but otherwise 
</p>
<p>identical,&sect; with particle 1 in state 3 and described by a probability distribution 
</p>
<p>I 'lf3(xW, and particle 2 in state 4 and described by the probability distribution 
</p>
<p>I 'l'4(x)l 2&bull; In this case, the first term represents the probability that particle 1 is at x 1 
</p>
<p>and particle 2 is at x2 , while the second gives the probability for the exchanged 
</p>
<p>event. The sum of these two terms then gives Pv(x1 , x2), the probability for finding 
</p>
<p>one at x1 and the other at x 2 , with no regard paid to their labels. (The subscript D 
</p>
<p>denotes distinguishable.) The next two terms, called interference terms, remind us 
</p>
<p>that there is more to identical particles in quantum theory than just their identical 
</p>
<p>characteristics: they have no separate identities. Had they separate identities (as in 
</p>
<p>the classical case) and we were just indifferent to which one arrives at x 1 and which 
</p>
<p>one at xz, we would get just the first two terms. There is a parallel between this 
</p>
<p>situation and the double-slit experiment, where the probabilities for finding a particle 
</p>
<p>at a given point x on the screen with both slits open was not the sum of the probabilit-
</p>
<p>ies with either slit open. In both cases, the interference terms arise, because in quan-
</p>
<p>tum theory, when an event can take place in two (or more) indistinguishable ways, 
</p>
<p>we add the corresponding amplitudes and not the corresponding probabilities. 
</p>
<p>Just as we were not allowed then to assign a definite trajectory to the particle 
</p>
<p>(through slits 1 or 2), we are not allowed now to assign definite labels to the two 
</p>
<p>particles. 
</p>
<p>The interference terms tell us if the pions are bosons or fermions. The difference 
</p>
<p>between the two cases is most dramatic as x 1-&gt;x2-+x: 
</p>
<p>PA(x 1-+x, x 2 -+x)-+O (Pauli principle applied to state lx)) (10.3.30) 
</p>
<p>whereas 
</p>
<p>(10.3.31) 
</p>
<p>which is twice as big as Pv(x1-+x, Xz-+X), the probability density for two distinct 
</p>
<p>label carrying (but otherwise identical) particles, whose labels are disregarded in the 
</p>
<p>position measurement. 
</p>
<p>One refers to the tendency of fermions to avoid each other (i.e., avoid the 
</p>
<p>state x 1 =x2 =x) as obeying "Fermi-Dirac statistics" and the tendency ofbosons to 
</p>
<p>:): We do this to simplify the argument. The basic idea works just as well in three dimensions. 
</p>
<p>&sect;The label can, for example, be the electric charge. </p>
<p/>
</div>
<div class="page"><p/>
<p>conglomerate as "obeying Bose-Einstein statistics," after the physicists who first 
</p>
<p>explored the consequences of the antisymmetrization and symmetrization require-
</p>
<p>ments on the statistical mechanics of an ensemble of fermions and bosons, respec-
tively. (This is the reason for referring to the bosonic/fermionic nature of a particle 
</p>
<p>as its statistics.) 
Given the striking difference in the two distributions, we can readily imagine 
</p>
<p>deciding (once and for all) whether pions are bosons or fennions by preparing an 
</p>
<p>ensemble of systems (with particles inn= 3 and 4) and measuring P(x1 , x2). 
</p>
<p>Note that P(x1 , x2) helps us decide not only whether the particles are bosons 
or fermions, but also whether they are identical in the first place. In other words, if 
</p>
<p>particles that we think are identical differ with respect to some label that we are not 
</p>
<p>aware of, the nature of the interference term will betray this fact. Imagine, for 
</p>
<p>example, two bosons, call them K and K, which are identical with respect to mass 
and charge, but different with respect to a quantum number called "hypercharge." 
</p>
<p>Let us assume we are ignorant of hypercharge. In preparing an ensemble that we 
</p>
<p>think contains N identical pairs, we will actually be including some (K, K) pairs, 
</p>
<p>some (K, K) pairs. If we now make measurements on the ensemble and extract 
</p>
<p>the distribution P(x1 , x2) (once again ignoring the hypercharge), we will find the 
</p>
<p>interference term has the + sign but is not as big as it should be. If the ensemble 
</p>
<p>contained only identical bosons, P(x, x) should be twice as big as P0 (x, x), which 
describes label-carrying particles; if we get a ratio less than 2, we know the ensemble 
</p>
<p>is contaminated by label-carrying particles which produce no interference terms. 
</p>
<p>From the above discussions, it is also clear that one cannot hastily conclude, 
</p>
<p>upon catching two electrons in the same orbital state in three dimensions that they 
</p>
<p>are not fermions. In this case, the label we are ignoring is the spin orientation s. As 
</p>
<p>mentioned earlier on, s can have only two values, call them + and -. If we assume 
</p>
<p>that s never changes (during the course of the experiment) it can serve as a particle 
</p>
<p>label that survives with time. If s = + for one electron and - for the other, they are 
</p>
<p>like two distinct particles and can be in the same orbital state. The safe thing to do 
</p>
<p>here is once again to work with an ensemble rather than an isolated measurement. 
</p>
<p>Since we are ignorant of spin, our ensemble will contain(+,+) pairs,(-,-) pairs, 
</p>
<p>and ( +, -) pairs. The ( +, +) and (-, -) pairs are identical fermions and will produce 
a negative interference term, while the ( +, -) pairs will not. Thus we will find P(r, r) 
</p>
<p>is smaller than Pn(r, r) describing labeled particles, but not zero. This will tell us 
that our ensemble has identical fermion pairs contaminated by pairs of distin-
</p>
<p>guishable particles. It will then be up to us to find the nature of the hidden degree 
of freedom which provides the distinction. 
</p>
<p>Systems of N Identical Particles 
</p>
<p>The case N = 2 lacks one feature that is found at larger N. We illustrate it by 
</p>
<p>considering the case of three identical particles in a box. Let us say that an energy 
measurement shows the quantum numbers of the particles to be n 1 , n2 , and n3 &bull; Since 
the particles are identical, all we can conclude from this observation is that the total 
energy is 
</p>
<p>271 
</p>
<p>SYSTEMS WITH 
</p>
<p>N DEGREES 
OF FREEDOM </p>
<p/>
</div>
<div class="page"><p/>
<p>272 
</p>
<p>CHAPTER 10 
</p>
<p>Now there are 3! =six product states with this energy: jn1n2n3), jn1n3n2), jn2n3n1), 
</p>
<p>ln2n1n3), jn3n2n1), and jn3n1n2). The physical states are elements of the six-dimen-
</p>
<p>sional eigenspace spanned by these vectors and distinguished by the property that 
</p>
<p>under the exchange of any two particle labels, the state vector changes only by a 
</p>
<p>factor a. Since double exchange of the same two labels is equivalent to no exchange, 
</p>
<p>we conclude as before that a = &plusmn; 1. There are only two states with this property: 
</p>
<p>1 
ln1n2n3, S) =--1-.2 [jn1non3) + jn1n3n2) + jn2n1n1) (3!) I - -
</p>
<p>+I n2n1n3) + I n3n2n1) + ln3n1n2)] (10.3.32) 
</p>
<p>called the totally symmetric state,t for which a=+ 1 for all three possible exchanges 
</p>
<p>(1 &lt;--&gt; 2, 2 &lt;--&gt; 3, 1 &lt;--&gt; 3); and 
</p>
<p>1 
ln1n2n3, A)= (3!) 112 [ln1n2n3) -1n1n3n2) + ln2n3n1) 
</p>
<p>-1n2n1n3) + ln3n1n2) -1n3n2n1)] (10.3.33) 
</p>
<p>called the totally antisymmetric state, for which a= -l for all three possible 
</p>
<p>exchanges. 
</p>
<p>Bosons will always pick the S states and fermions, the A states. It follows that 
</p>
<p>no two fermions can be in the same state. 
</p>
<p>As in the N = 2 case, the wave function in the X basis is 
</p>
<p>and 
</p>
<p>For instance, the wave function associated with ln1n2n3, SjA), Eqs. (10.3.33) and 
</p>
<p>(10.3.34), is 
</p>
<p>lf!n,n,n,(XJ' x2, XJ, S/A) 
</p>
<p>= (3!)- 112[ If! n,(xl) If! n,(x2) If! n3(X3) &plusmn; If! n1 (XJ) lfl n3(X2) lfl n,(XJ) 
</p>
<p>+ lf/n2(XJ)If!n3(X2)1f!n 1(XJ)&plusmn; lf!n2(XJ)lf/n 1(X2)lf/n3(X3) 
</p>
<p>+ lf/n 3(XJ)lf/n1(X2)lf/n,(X3) &plusmn; lf/n3(XJ)'I'n,(X2)lf/n 1(XJ)] ( 10.3.35) 
</p>
<p>t The normalization factor (3!)_,_, is correct only if all three n's are different. If. for example, n, =n,= 
n3 = n, then the product state I nnn) is normalized and symmetric and can be used as the S state. A 
</p>
<p>similar question does not arise for the fermion state due to the Pauli principle. </p>
<p/>
</div>
<div class="page"><p/>
<p>The fermion wave function may once again be written as a determinant: 
</p>
<p>1 
'l'n 1n2n3(XI, X2, X3, A)= (3!)!/Z 'l'n1(X2) 
</p>
<p>'l'n1(X3) 
</p>
<p>'l'n,(XI) 
</p>
<p>'I' mCx2) 
</p>
<p>'l'n,(X3) 
</p>
<p>'l'n3(XI) 
</p>
<p>'l'n3(X2) 
</p>
<p>'I' n,Cx3) 
</p>
<p>(10.3.36) 
</p>
<p>Using the properties of the determinant, one easily sees that 'I' vanishes if two of 
the x's or n's coincide. All these results generalize directly to any higher N. 
</p>
<p>Two questions may bother you at this point. 
</p>
<p>Question /. Consider the case N = 3. There are three possible exchanges here: 
(1 +-+ 2), (1 +-+ 3), and (2 +-+ 3). The S states pick up a factor a=+ 1 for all three 
exchanges, while the A states pick up a = -1 for all three exchanges. What about 
states for which some of the a's are + 1 and the others -1? Such states do not exist. 
You may verify this by exhaustion: take the 3! product vectors and try to form such 
a linear combination. Since a general proof for this case and all N involves group 
theory, we will not discuss it here. Note that since we get only two acceptable 
vectors for every N! product vectors, the direct product space for N??. 3 is bigger (in 
dimensionality) than 'W' sEB 'W' A &bull; 
</p>
<p>Question II. We have tacitly assumed that if two identical particles of a given 
species always pick the S (or A) state, so will three or more, i.e., we have extended 
our definition of bosons and fermions from N = 2 to all N. What if two pions always 
pick the S state while three always pick the A state? While intuition revolts at such 
a possibility, it still needs to be formally ruled out. We do so at the end of the next 
subsection. 
</p>
<p>When Can We Ignore Symmetrization and Antisymmetrization? 
</p>
<p>A basic assumption physicists make before they can make any headway is that 
they can single out some part of the universe (the system) and study it in isolation 
from the rest. While no system is truly isolated, one can often get close to this ideal. 
For instance, when we study the oscillations of a mass coupled to a spring, we ignore 
the gravitational pull of Pluto. 
</p>
<p>Classically, the isolation of the system is expressed by the separability of the 
Hamiltonian of the universe: 
</p>
<p>&pound;universe = &pound;.ys + Jlli:est (10.3.37) 
</p>
<p>where $.ys is a function of the system coordinates and momenta alone. It follows 
that the time evolution of the system's p's and q's are independent of what is going 
on in the rest of the universe. In our example, this separability is ruined (to give just 
one example) by the gravitational interaction between the mass and Pluto, which 
depends on their relative separation. If we neglect this absurdly small effect (and 
other such effects) we obtain separability to an excellent approximation. 
</p>
<p>273 
</p>
<p>SYSTEMS WITH 
N DEGREES 
</p>
<p>OF FREEDOM </p>
<p/>
</div>
<div class="page"><p/>
<p>274 
</p>
<p>CHAPTER 10 
</p>
<p>Quantum mechanically, separability of H leads to the factorization of the wave 
</p>
<p>function of the universe: 
</p>
<p>1JI universe = '.JI sys &middot; VI rest (10.3.38) 
</p>
<p>where lf/sys is a function only of system coordinates, collectively referred to as Xs. 
</p>
<p>Thus if we want the probability that the system has a certain coordinate x, and do 
</p>
<p>not care about the rest, we find (symbolically) 
</p>
<p>P(Xs) = J llfluniverse(X., Xrest)l 2 dXrest 
</p>
<p>= llf/sys(Xs)l 2 J llf/(Xrest)l 2 dXrest 
</p>
<p>=I IJ!sys(Xs)l 2 (10.3.39) 
</p>
<p>We could have obtained this result by simply ignoring lf/rest from the outset. 
</p>
<p>Things get complicated when the system and the "rest" contain identical parti-
</p>
<p>cles. Even if there is no interaction between the system and the rest, i.e., the Hamil-
</p>
<p>tonian is separable, product states are not allowed and only S or A states must be 
</p>
<p>used. Once the state vector fails to factorize, we will no longer have 
</p>
<p>P(x., Xrest) = P(xs)P(Xrest) ( 10.3.40) 
</p>
<p>(i.e., the systems will not be statistically independent), and we can not integrate out 
</p>
<p>P(xrest) and regain P(xs). 
Now it seems reasonable that at least in certain cases it should be possible to 
</p>
<p>get away with the product state and ignore the symmetrization or antisymmetrization 
</p>
<p>conditions. 
Suppose, for example, that at t = 0, we find one pion in the ground state of an 
</p>
<p>oscillator potential centered around a point on earth and another pion in the same 
</p>
<p>state, but on the moon. It seems reasonable that we can give the particles the labels 
</p>
<p>"earth pion" and "moon pion," which will survive with time. Although we cannot 
</p>
<p>follow their trajectories, we can follow their wave functions: we know the first wave 
</p>
<p>function is a Gaussian GE(xE) centered at a point in the lab on earth and that the 
</p>
<p>second is a Gaussian GM(xM) centered at a point on the moon. If we catch a pion 
</p>
<p>somewhere on earth at time t, the theory tells us that it is almost certainly the "earth 
</p>
<p>pion" and that the chances of its being the "moon pion" are absurdly small. Thus 
</p>
<p>the uncertainty in the position of each pion is compensated by a separation that is 
</p>
<p>much larger. (Even in classical mechanics, it is not necessary to know the trajectories 
</p>
<p>exactly to follow the particles; the band of uncertainty about each trajectory has 
</p>
<p>merely to be much thinner than the minimum separation between the particles during 
</p>
<p>their encounter.) We therefore believe that if we assumed 
</p>
<p>(10.3.41) </p>
<p/>
</div>
<div class="page"><p/>
<p>we should be making an error that is as negligible as is the chance of finding the 
earth pion on the moon and vice versa. Given this product form, the person on earth 
can compute the probability for finding the earth pion at some x by integrating out 
the moon pion: 
</p>
<p>P(xE) = IGE(x&pound;)1 2 fiGM(xM)I 2 dxM 
</p>
<p>= IGE(x&pound;)1 2 (10.3.42) 
</p>
<p>Likewise the person on the moon, who does not care about (i.e., sums over) the 
earth pion will obtain 
</p>
<p>(10.3.43) 
</p>
<p>Let us now verify that if we took a properly symmetrized wave function it leads 
to essentially the same predictions (with negligible differences). 
</p>
<p>Let us start w.ith 
</p>
<p>(10.3.44) 
</p>
<p>We use the labels x 1 and x2 rather than XE and xM to emphasize that the pions are 
indeed being treated as indistinguishable. Now, the probability (density) of finding 
one pa.rtcle near x 1 and one near x2 is 
</p>
<p>P(x., x2) =21 11'1 2 = I GE(x.)I 2IGM(x2)1 2+ I GM(x.)I 2IGE(x2)1 2 
</p>
<p>+ G1(x.)GM(x.)G.t{x2)GE(x2) 
</p>
<p>+ G ~(x.)GE(x.)G 1(x2)GM(x2) (10.3.45) 
</p>
<p>Let us ask for the probability of finding one particle near some point xE on the 
earth, with no regard to the other. This is given by setting either one of the variables 
(say x.) equal to xE and integrating out the other [since P(x1 , x 2) =P(x2, x 1)]. There 
is no need to divide by 2 in doing this integration (why?). We get 
</p>
<p>P(xE) =I GE(x&pound;)1 2 f I GM(x2)1 2 dx2 + IGM(XE )1 2 f IGE(x2)1 2 dx2 
</p>
<p>+ G1(xE)GM(xE) f G~(x2)GE(x2) dx2 
</p>
<p>+ G t(xE )GE(xE) J G !(xz)GM(xz) dx2 (10.3.46) 
</p>
<p>The first term is what we would get if we begin with a product wave function Eq. 
(10.3.41) and integrate out xM. The other three terms are negligible since GM is 
peaked on the moon and is utterly negligible at a point XE on the earth. Similarly if 
we asked for P(xM), where xM is a point on the moon, we will again get I GM(xM )1 2&bull; 
</p>
<p>275 
</p>
<p>SYSTEMS WITH 
N DEGREES 
</p>
<p>OF FREEDOM </p>
<p/>
</div>
<div class="page"><p/>
<p>276 
</p>
<p>CHAPTER 10 
</p>
<p>The labels "earth pion" and "moon pion" were useful only because the two 
</p>
<p>Gaussians remained well separated for all times (being stationary states). If the two 
</p>
<p>Gaussians had not been bound by the oscillating wells, and were wave packets 
</p>
<p>drifting toward each other, the labeling (and the factorized wave function) would 
</p>
<p>have become invalid when the Gaussians begin to have a significant overlap. The 
</p>
<p>point is that at the start of any experiment, one can always assign the particles some 
</p>
<p>labels. These labels acquire a physical significance only if they survive for some time. 
</p>
<p>Labels like "a particle of mass m and charge + 1" survive forever, while the longevity 
of a label like "earth pion" is controlled by whether or not some other pion is in 
</p>
<p>the vicinity. 
</p>
<p>A dramatic illustration of this point is provided by the following example. At 
t = 0 we catch two pions, one at x =a and the other at x =b. We can give them the 
labels a and b since the two delta functions do not overlap even if a and b are in the 
</p>
<p>same room. We may describe the initial state by a product wave function. But this 
</p>
<p>labeling is quite useless, since after the passage of an infinitesimal period of time, 
</p>
<p>the delta functions spread out completely: the probability distributions become con-
</p>
<p>stants. You may verify this by examining I U(x, t; a, 0)! 2 (the "fate" of the delta 
function)t or by noting that L\P= :tJ for a delta function (the particle has all possible 
</p>
<p>velocities from 0 to x) and which, therefore, spreads out in no time. 
</p>
<p>All these considerations apply with no modification to two fermions: the two 
cases differ in the sign of the interference term, which is irrelevant to these 
</p>
<p>considerations. 
</p>
<p>What if there are three pions, two on earth and one on the moon'J Since the 
</p>
<p>two on the earth (assuming that their wave functions appreciably overlap) can be 
confused with each other, we must symmetrize between them, and the total wave 
</p>
<p>function will be, in obvious notation, 
</p>
<p>(10.3.47) 
</p>
<p>The extension of this result to more particles and to fermions is obvious. 
</p>
<p>At this point the answer to Question n raised at the end of the last subsection 
becomes apparent. Suppose three-pion systems picked the A state while two-pion 
</p>
<p>systems picked the S state. Let two of the three pions be on earth and the third one on 
the moon. Then, by assumption, the following function should provide an excellent 
</p>
<p>approximation: 
</p>
<p>(10.3.48) 
</p>
<p>If we integrate over the moon pion we get 
</p>
<p>( 10.3.49) 
</p>
<p>We are thus led to conclude that two pions on earth will have a probability distribu-
tion corresponding to two fermions if there is a third pion on the moon and a 
distribution expected to two bosons if there is not a third one on the moon. Such 
</p>
<p>~ It is being assumed that the particles are tree. </p>
<p/>
</div>
<div class="page"><p/>
<p>absurd conclusions are averted only if the statistics depend on the species and not 
the number of particles. 
</p>
<p>A word of caution before we conclude this long discussion. If two particles have 
nonoverlapping wave functions in x space, then it is only in x space that a product 
wave function provides a good approximation to the exact symmetrized wave func-
tion, which in our example was 
</p>
<p>(10.3.50) 
</p>
<p>The formal reason is that for any choice of the arguments xi and x 2 , only one or 
the other of the two terms in the right-hand side is important. (For example, if xi 
is on the earth and x2 is on the moon, only the first piece is important.) Physically 
it is because the chance of finding one pion in the territory of the other is negligible 
and interference effects can be ignored. 
</p>
<p>If, however, we wish to switch to another basis, say the P basis, we must consider 
the Fourier transform of the symmetric function 'l's and not the product, so that we 
end up with a symmetrized wave function in p space. The physical reason for this is 
that the two pions have the same momentum distributions-with (P) = 0 and ident-
ical Gaussian fluctuations about this mean-since the momentum content of the 
oscillator is independent of its location. Consequently, there are no grounds in P 
space for distinguishing between them. Thus when a momentum measurement (which 
says nothing about the positions) yields two numbers, we cannot assign them to the 
pions in a unique way. Formally, symmetrization is important because the p-space 
wave functions of the pions overlap strongly and there exist values for the two 
momenta (both ~o) for which both terms in the symmetric wave function are 
significant. 
</p>
<p>By the same token, if there are two particles with nonoverlapping wave functions 
in p space, we may describe the system by a product wave function in this space 
(using labels like "fast" and "slow" instead of "earth" and "moon" to distinguish 
between them), but not in another space where the distinction between them is 
absent. It should be clear that these arguments apply not just to X or P but to any 
arbitrary variable n. 
</p>
<p>Exercise 10.3.1.* Two identical bosons are found to be in states 11/J) and 1'1'&gt;&middot; Write 
down the normalized state vector describing the system when (1/JI 'I') #0. 
</p>
<p>Exercise 10.3.2. * When an energy measurement is made on a system of three bosons in 
a box, the n values obtained were 3, 3, and 4. Write down a symmetrized, normalized state 
vector. 
</p>
<p>Exercise 10.3.3. * Imagine a situation in which there are three particles and only three 
states a, b, and c available to them. Show that the total number of allowed, distinct configura-
tions for this system is 
</p>
<p>( 1) 27 if they are labeled 
(2) 10 if they are bosons 
(3) 1 if they are fermions 
</p>
<p>277 
</p>
<p>SYSTEMS WITH 
N DEGREES 
</p>
<p>OF FREEDOM </p>
<p/>
</div>
<div class="page"><p/>
<p>278 
</p>
<p>CHAPTER 10 
</p>
<p>Exercise 10.3.4. * Two identical particles of mass m are in a one-dimensional box of 
length L. Energy measurement of the system yields the value Esys = fhr: 2 jmL 2 &bull; Write down 
</p>
<p>the state vector of the system. Repeat for E,ys = 51i2 rr:2 j2mL 2&bull; (There are two possible vectors 
</p>
<p>in this case.) You are not told if they are bosons or fermions. You may assume that the only 
</p>
<p>degrees of freedom are orbital. 
</p>
<p>Exercise 10.3.5. * Consider the exchange operator P12 whose action on the X basis is 
</p>
<p>(1) Show that P 12 has eigenvalues &plusmn;I. (It is Hermitian and unitary.) 
</p>
<p>(2) Show that its action on the basis ket I w, , w2) is also to exchange the labels I and 
</p>
<p>2, and hence that V s. A are its eigenspaces with eigenvalues &plusmn;I. 
</p>
<p>(3) Show that P12X,P,2=X2. P,2X2P12=X, and similarly for P 1 and P2. Then show that 
</p>
<p>Pd:l(X,, P,;X2. P2)P,2=il(X2, P2;X,, P.). [Consider the action on jx,, x:) or lp1 ,p,). As 
for the functions of X and P, assume they are given by power series and consider any term 
</p>
<p>in the series. If you need help, peek into the discussion leading to Eq. (11.2.22).] 
</p>
<p>( 4) Show that the Hamiltonian and propagator for two identical particles are left 
</p>
<p>unaffected under H-&gt;P 12HP,2 and U-&gt;P12UP,2. Given this, show that any eigenstate of P 12 
</p>
<p>continues to remain an eigenstate with the same eigenvalue as time passes, i.e., elements of 
</p>
<p>V s1 A never leave the symmetric or antisymmetric subspaces they start in. 
</p>
<p>Exercise 10.3.6. * Consider a composite object such as the hydrogen atom. Will it behave 
as a boson or fermion? Argue in general that objects containing an even/odd number of 
</p>
<p>fermions will behave as bosons/fermions. </p>
<p/>
</div>
<div class="page"><p/>
<p>Symmetries and 
Their Consequences 
</p>
<p>11.1. Overview 
</p>
<p>11 
</p>
<p>In Chapter 2, we explored the consequences of the symmetries of the Hamil-
tonian. We saw the following: 
</p>
<p>(I) If :Yf is invariant under the infinitesimal canonical transformation generated 
by a variable g(q,p), then g is conserved. 
</p>
<p>(2) Any canonical transformation that leaves :Yf invariant maps solutions to 
the equations of motion into other solutions. Equivalently, an experiment and its 
transformed version will give the same result if the transformation is canonical and 
leaves :Yf invariant. 
</p>
<p>Here we address the corresponding results in quantum mechanics.t 
</p>
<p>11.2. Translational Invariance in Quantum Theory 
</p>
<p>Consider a single particle in one dimension. How shall we define translational 
invariance? Since a particle in an arbitrary state has neither a well-defined position 
nor a well-defined energy, we cannot define translational invariance to be the invari-
ance of the energy under an infinitesimal shift in the particle position. Our previous 
experience, however, suggests that in the quantum formulation the expectation values 
should play the role of the classical variables. We therefore make the correspondence 
shown in Table 11.1. 
</p>
<p>Having agreed to formulate the problem in terms of expectation values, we still 
have two equivalent ways to interpret the transformations: 
</p>
<p>(X)-+ (X) + t: 
</p>
<p>(P)-+(P) 
</p>
<p>:j: It may be worth refreshing your memory by going through Sections 2.7 and 2.8. 
</p>
<p>(11.2.la) 
</p>
<p>(11.2.lb) 
</p>
<p>279 </p>
<p/>
</div>
<div class="page"><p/>
<p>280 
</p>
<p>CHAPTER 11 
</p>
<p>Table 11.1. Correspondence between Classical and Quantum Mechanical Concepts Related to 
Translational Invariance 
</p>
<p>Concept 
</p>
<p>Translation 
</p>
<p>Translational invariance 
Conservation law 
</p>
<p>Classical mechanics 
</p>
<p>X---&gt;x+c: 
</p>
<p>p---&gt;p 
</p>
<p>Xf---&gt;Yt 
</p>
<p>p=O 
</p>
<p>Quantum mechanics 
</p>
<p>(X)---&gt;(X)+ c: 
</p>
<p>(P)---&gt;(P) 
</p>
<p>(H)---&gt;(H) 
</p>
<p>(P)=O (anticipated) 
</p>
<p>The first is to say that under the infinitesimal translation, each state I If/) gets 
modified into a translated state, I If!.) such that 
</p>
<p>( 11.2.2a) 
</p>
<p>&lt;'I' .I PI If!.)= &lt;lfiiPI If/) (l1.2.2b) 
</p>
<p>In terms of T( &amp;), the translation operator, which translates the state (and which will 
be constructed explicitly in a while) 
</p>
<p>Eq. (11.2.2) becomes 
</p>
<p>T(t:)l If/)= I If/.) 
</p>
<p>&lt;'1'1 T\t:)XT(t:)i If/)= &lt;lfiiXI If/)+ t: 
</p>
<p>&lt;'1'1 T\t:)PT(t:)i If/)= &lt;'fliP! If/) 
</p>
<p>(11.2.3) 
</p>
<p>(11.2.4a) 
</p>
<p>(ll.2.4b) 
</p>
<p>This point of view is called the active transformation picture (in the terminology of 
Section 1. 7) and corresponds to physically displacing the particle to the right by &amp;. 
</p>
<p>The second point of view is to say that nothing happens to the state vectors; it 
is the operators X and P that get modified by T( &amp; ) as follows: 
</p>
<p>such that 
</p>
<p>X--&gt; Tt(t:)XT(t:) 
</p>
<p>P-&gt; Tt(t:)PT(t:) 
</p>
<p>T\t:)XT(t:) =X+ &amp;I 
</p>
<p>Tt(e)PT(t:)=P 
</p>
<p>(l1.2.5a) 
</p>
<p>(11.2.5b) 
</p>
<p>This is called the passive transformation picture. Physically it corresponds to moving 
the environment (the coordinate system, sources of external field if any, etc.) to the 
left by e. 
</p>
<p>Physically, the equivalence of the active and passive pictures is due to the fact 
that moving the particle one way is equivalent to moving the environment the other 
way by an equal amount. </p>
<p/>
</div>
<div class="page"><p/>
<p>Mathematically, we show the equivalence as follows. If we sandwich the operator 
equation (11.2.5) between ('1'1 and IIJI), we get Eq. (11.2.4). To go the other way, 
we first rewrite Eq. (11.2.4) as 
</p>
<p>('1'1 Tt(t:)XT(t:)- X- t:l] 'I') =0 
</p>
<p>&lt;'1'1 T\t:)PT(t:)- PI 'I'&gt; =0 
</p>
<p>We now reason as follows: 
</p>
<p>(I) The operators being sandwiched are Hermitian (verify). 
(2) Since I IJI) is arbitrary, we can choose it to be any of the eigenvectors of 
</p>
<p>these operators. It follows that all the eigenvalues vanish. 
(3) The operators themselves vanish, implying Eq. (11.2.5). 
</p>
<p>In what follows, we will examine both pictures. We will find that it is possible 
to construct T(t:) given either of Eqs. (11.2.4) or (11.2.5), and of course that the 
two yield the same result. The active transformation picture is nice in that we work 
with the quantum state IIJI), which now plays the role of the classical state (x,p). 
The passive transformation picture is nice because the response of the quantum 
operators X and P to a translation is formally similar to that of their classical 
counterparts. t 
</p>
<p>We begin by discussing translations in terms of active transformations. Let us 
examine how the ket I 1J1 ,:) is related to I IJI) or, equivalently, the action of the Hilbert 
space operator T(t:). The answer appears obvious if we work with kets of definite 
position, lx). In this case it is clear that 
</p>
<p>T(t:)lx) = lx+ t:) ( 11.2.6) 
</p>
<p>In other words, if the particle is originally at x, it must end up at x + t:. Notice that 
T(t:) is unitary: it acts on an orthonormal basis lx), -oo :::;;x:::;; oo, and gives another, 
lx+ t:), -oo :::;;x+ s:::;; oo. Once the action of T(t:) on a complete basis is known, its 
action on any ket I IJI) follows: 
</p>
<p>IIJI,)=T(t:)IIJI)=T(t:) f"' lx)(xiiJI)dx= Joo jx+t:)(xiiJI)dx 
-oo -oo 
</p>
<p>=fcc lx')(x'- t:l 'I') dx' 
-oo 
</p>
<p>(x'=x+ t:) (11.2. 7) 
</p>
<p>In other words if 
</p>
<p>(xi IJI)= IJI(x) 
</p>
<p>t As we shall see, it is this point of view that best exposes many formal relations between classical and 
quantum mechanics. 
</p>
<p>281 
</p>
<p>SYMMETRIES 
AND THEIR 
</p>
<p>CONSEQUENCES </p>
<p/>
</div>
<div class="page"><p/>
<p>282 
</p>
<p>CHAPTER II 
</p>
<p>then 
</p>
<p>(xi T( s)llf/) = lf!(X- s) (11.2.8) 
</p>
<p>For example, if lfl(x)~e-x' is a Gaussian peaked at the ongm, 
</p>
<p>lf!(x- s) ~e-&lt;x-&bull;l' is an identical Gaussian peaked at x= s. Thus the wave function 
</p>
<p>If! ,(x) is obtained by translating (without distortion) the wave function lf/(x) by an 
</p>
<p>amounts to the right. You may verify that the action of T(s) defined by Eq. (11.2.8) 
</p>
<p>satisfies the condition Eq. (11.2.la). How about the condition Eq. (11.2.lb)? It is 
</p>
<p>automatically satisfied: 
</p>
<p>= fx lfl*(x-s)( -i'li ~)lfl(x-s)dx 
-x 
</p>
<p>= f"' lf/*(x')( -iii d~')lfl(x') dx' (x'=x-s) 
-x 
</p>
<p>(11.2.9) 
</p>
<p>Now there is something odd here. Classically, translation is specified by two 
</p>
<p>independent relations 
</p>
<p>x--+x+s 
</p>
<p>p--+p 
</p>
<p>while in the quantum version we seem to find that in enforcing the former (on position 
</p>
<p>eigenkets), the latter automatically follows. The reason is that in our derivation we 
</p>
<p>have assumed more than what was explicitly stated. We reasoned earlier, on physical 
</p>
<p>grounds, that since a particle initially located at x must end up at x + s, it follows 
</p>
<p>that 
</p>
<p>T(Li)lx) = lx+ s) 
</p>
<p>While our intuition was correct, our implementation was not. As seen in chapter 7, 
</p>
<p>the X basis is not unique, and the general result consistent with our intuition is not 
</p>
<p>Eq. (11.2.6) but rather 
</p>
<p>(11.2.10) 
</p>
<p>(Note that as s--+0, T(s)lx)--+lx) as it should.) In ignoring g(x), we had essentially 
</p>
<p>assumed the quantum analog of p--+p. Let us see how. If we start with Eq. (11.2.10) </p>
<p/>
</div>
<div class="page"><p/>
<p>instead of Eq. ( 11.2.6), we find that 
</p>
<p>&lt;X) ---+ &lt;X) + e 
T(e) 
</p>
<p>(11.2.lla) 
</p>
<p>&lt;P)-+&lt;P) + e&lt;f(X)) (11.2.llb) 
</p>
<p>where f=g'. Demanding now that &lt;P)-+&lt;P), we eliminate f and reduce g to a 
harmless constant (which can be chosen to be 0). 
</p>
<p>Exercise 11.2.1. Verify Eq. (11.2.llb) 
</p>
<p>Note that there was nothing wrong with our initial choice Tjx)=lx+e)-it 
was too restrictive given just the requirement &lt;X&gt;-+&lt;X&gt; + &amp;, but not so if we also 
considered &lt;P&gt;-+&lt;P). This situation reappears when we go to two or three dimen-
sions and when we consider rotations. In all those cases we will make the analog of 
the naive choice T( e) I x) = I x + e) to shorten the derivations. 
</p>
<p>Having defined translations, let us now define translational invariance in the 
same spirit. We define it by the requirement 
</p>
<p>&lt;'Ill HI If/)= &lt;'II .I HI If!.&gt; (11.2.12) 
</p>
<p>To derive the conservation law that goes with the above equation, we must first 
construct the operator T(e) explicitly. Since s=O corresponds to no translation, we 
may expand T(e) to order e as 
</p>
<p>i&amp; 
T(s}=l-- G 
</p>
<p>1i 
(11.2.13) 
</p>
<p>The operator G, called the generator of translations, is Hermitian (see Exercise 11.2.2 
for the proof) and is to be determined. The constant (- i/fi) is introduced in anticipa-
tion of what is to follow. 
</p>
<p>Exercise 11.2.2. * Using Tt(e)T(e) =I to order e, deduce that Gt =G. 
</p>
<p>We find G by turning to Eq. (11.2.8): 
</p>
<p>&lt;xl T(s)l If/)= lf!(x- e) 
</p>
<p>Expanding both sides to order &amp;, we find 
</p>
<p>i&amp; dlfl 
&lt;xllllf/)-- &lt;xiGI If!)= lf!(x)-- &amp; 
</p>
<p>1i dx 
</p>
<p>283 
</p>
<p>SYMMETRIES 
AND THEIR 
</p>
<p>CONSEQUENCES </p>
<p/>
</div>
<div class="page"><p/>
<p>284 
</p>
<p>CHAPTER 11 
</p>
<p>so that 
</p>
<p>. dlJf 
(xiGIIJI)= -,rz_ 
</p>
<p>dx 
</p>
<p>Clearly G is the momentum operator, 
</p>
<p>and 
</p>
<p>G=P 
</p>
<p>is 
T(s) =/-~ P 
</p>
<p>1i 
(11.2.14) 
</p>
<p>We see that exactly as in classical mechanics, the momentum is the generator of 
</p>
<p>(infinitesimal) translations. 
</p>
<p>The momentum conservation law now follows from translational invariance, 
</p>
<p>Eq. ( 11.2.12), if we combine it with Eq. ( 11.2.14): 
</p>
<p>(IJIIHIIJI) = (IJI oiHIIJI &pound;) 
</p>
<p>= (T( s)IJIIHI T( t:) IJI) = &lt; IJII Tt ( t:)HT( t:)IIJI) 
</p>
<p>=(IJII (I+~P )H(I-~ P}IJI) 
</p>
<p>it: 
= (IJIIHIIJI) +~ (IJII[P, H]IIJI)+ O(t:2) 
</p>
<p>1i 
</p>
<p>so that, we get, upon equating the coefficient of s to zero, 
</p>
<p>(IJII[P, H]IIJI)=O 
</p>
<p>It now follows from Ehrenfest's theorem that 
</p>
<p>([P, H])=O--+(P)=O 
</p>
<p>Translation in Terms of Passive Transformations 
</p>
<p>Let us rederive T(t:), given that it acts as follows on X and P: 
</p>
<p>Tt(t:)XT(t:) =X+ t:l 
</p>
<p>T\t:)PT(t:) = P 
</p>
<p>(11.2.15) 
</p>
<p>(11.2.16) 
</p>
<p>(11.2.17a) 
</p>
<p>(11.2.17b) 
</p>
<p>The operator T\t:)XT(&amp;) is also a position operator, but it measures position 
</p>
<p>from a new origin, shifted to the left by &amp;: This is the meaning of Eq. (11.2.17a). </p>
<p/>
</div>
<div class="page"><p/>
<p>Equation ( 11.2.17b) states that under the shift in the origin, the momentum is 
</p>
<p>unaffected. 
Writing once again 
</p>
<p>icG 
T( s) =I- -&middot;&middot;--------
</p>
<p>fi 
</p>
<p>we find from Eq. (11.2.17a) (using the fact that Gt =G) 
</p>
<p>( icG) ( h:G) I+f: X 1-&middot;-f: =X+el 
</p>
<p>or 
</p>
<p>ic 
-~[X, G] = cl 
</p>
<p>[X,G]=ilil 
</p>
<p>This allows us to conclude that 
</p>
<p>G=P+f(X) 
</p>
<p>If we now tum to Eq. (ll.2.17b) we find 
</p>
<p>- 1i [P, G]=O 
</p>
<p>or 
</p>
<p>[P, G] =0 
</p>
<p>which eliminates f(X).t So once again 
</p>
<p>it:P 
T(c)=I--&middot;~ 
</p>
<p>1i 
</p>
<p>(1L2.18a) 
</p>
<p>(11.2.l8b) 
</p>
<p>(11.2.19) 
</p>
<p>(1L2.20a) 
</p>
<p>(ll.2.20b) 
</p>
<p>Having derived the translation operator in the passive transfom1ation p.icture, let us 
reexamine the notion of translational in variance. 
</p>
<p>We define translational invariance by the requirement 
</p>
<p>Tt(c)HT(c)=H (11.2.21) 
</p>
<p>t For the purists, it reduces/to a c number which commutes with X and P, which we choose to be zero. 
</p>
<p>285 
</p>
<p>SYMMETRIES 
</p>
<p>AND THEIR 
</p>
<p>CONSEQUENCES </p>
<p/>
</div>
<div class="page"><p/>
<p>286 
</p>
<p>CHAPTER 11 
</p>
<p>We can rewrite Eq. (11.2.21) in a form that is closer to the classical definition 
</p>
<p>of translational in variance. But first we need the following result: for any Q(X, P) 
</p>
<p>that can be expanded in a power series, and for any unitary operator U, 
</p>
<p>U'Q(X, P)U=Q(U+,~&middot;u, [/PU) 
</p>
<p>For the proof, consider a typical term in the series such as PX2P. We have, using 
uut =I, 
</p>
<p>Q.E.D. 
</p>
<p>Applying this result to the case U= T(E:) we get the response of any dynamical 
</p>
<p>variable to a translation: 
</p>
<p>(11.2.22) 
</p>
<p>Thus the transformed Q is found by replacing X by X+ sf and P by P. If we now 
</p>
<p>apply this to Eq. (11.2.21) we get the following definition of translation in variance: 
</p>
<p>H();' + d, P) = H(X, P) (11.2.23) 
</p>
<p>Not only does this condition have the same form as its classical counterpart 
</p>
<p>.Jif'(x+ s,p)=Jt(x,p) 
</p>
<p>but it is also satisfied whenever the classical counterpart is. The reason is simply that 
</p>
<p>H is the same function of X and P as :it is of x and p, and both sets of variables 
</p>
<p>undergo identical changes in a translation. 
</p>
<p>The conservation of momentum follows if we write T(&amp;) in Eq. (11.2.21) in 
</p>
<p>tenns of P and expand things out to first order in E: 
</p>
<p>0 = T+( &amp;)HT( E)-- H =(I+ i&amp;P/n)H(I- i&amp;P/n)- H 
</p>
<p>i&amp; 
= ---,;- [ H, P] (11.2.24) 
</p>
<p>which implies that &lt; i&bull;&gt; = 0, because of the Ehren fest's theorem. 
</p>
<p>A Digression on the Analogy with Classical Mechanicst 
</p>
<p>The passive transformation picture has the virtue that it bears a dose formal 
</p>
<p>resemblance to classical mechanics, with operators .Q in place of the classical variables 
</p>
<p>t In a less advanced course, the reader may skip this digression. </p>
<p/>
</div>
<div class="page"><p/>
<p>w [Eqs. (11.2.17), (11.2.22), (11.2.23)]. In fact, the infinitesimal unitary transforma-
tion T( &amp;) generated by Pis the quantum image of the infinitesimal canonical trans-
</p>
<p>formation generated by p: if we define the changes o)t and oP by 
</p>
<p>8X= Tt(s)XT(&amp;)- X 
</p>
<p>8P= Tt(s)PT(s) ----P 
</p>
<p>we get, on the one hand, from Eq. {11.2.17), 
</p>
<p>8X=X+d--X=d 
</p>
<p>8P=P-P=O 
</p>
<p>and on the other, from T= I- i&amp;P /n (working to tlrst order in s), 
</p>
<p>-i&amp; 
8X=(I+i&amp;P/n)X(I-it:P/n)-X=-----fi----- [X, P] 
</p>
<p>- ic: 
8P=(J+ic:P/n)P(l-i&amp;P/'fi)-P= [P, P] 
</p>
<p>n. 
</p>
<p>combining which we obtain 
</p>
<p>-i&amp; 
8X = ------------- [X, P] = E.! 
</p>
<p>ft 
</p>
<p>-is 
oP=-11-[P,P]=O 
</p>
<p>More generally, upon combining, Eq. (11.2.22) and T= I---- isP/n, we obtain 
</p>
<p>-is 
8Q = [Q, P] = Q(X + sf, P)- Q(X, P) 
</p>
<p>-n 
</p>
<p>These are the analogs of the canonical transformation generated by p: 
</p>
<p>8x=&amp;{x,p}=s 
</p>
<p>8p=;:;{p,p}=O 
</p>
<p>8w = &amp;{ w, p} = u&gt;(x+ &amp;, p)- ul(x, p) 
</p>
<p>If the problem is translationally invariant, we have 
</p>
<p>-i&amp; . 
8H= fi [H, P] = 0-+(P) =0 by Ehrenfest's theorem 
</p>
<p>287 
</p>
<p>SYMMETRIES 
AND THEIR 
</p>
<p>CONSEQUENCES </p>
<p/>
</div>
<div class="page"><p/>
<p>288 
</p>
<p>CHAPTER 11 
</p>
<p>while classically 
</p>
<p>o.if= c{.Jf,p} =O-+p=O by p= {p, x} 
</p>
<p>The correspondence is achieved through the substitution rules already familiar to 
</p>
<p>us: 
</p>
<p>In general, the infinitesimal canonical transformation generated by g(x, p ), 
</p>
<p>ow= t:{ w, g} 
</p>
<p>has as its image in quantum theory the infinitesimal unitary transformation UG( E)= 
1-ic:Gjfi in response to which 
</p>
<p>Now, we have seen that the transformation generated by any g(x, p) is canonicaL 
</p>
<p>i.e., it preserves the PB between the x's and the p's. In the quantum theory, the 
</p>
<p>quantities preserved are the commutation relations between the X's and the P's, for 
</p>
<p>if 
</p>
<p>then upon premultiplying by the unitary operator u;;( E) and postmultiplying by 
Uc;(c:), we find that the transformed operators obeyt 
</p>
<p>This completes the proof of the correspondence 
</p>
<p>transformation generated .,___. &lt; transformation generated {
</p>
<p>infinitesimal canonical )infinitesimal unitary]' 
</p>
<p>by g(x, p) l by G(X, P) 
</p>
<p>The correspondence holds for finite transformations as welJ, for these may be viewed 
as a sequence of infinitesimal transformations. 
</p>
<p>; More generally if [Q. lij&bull;= f, then a similar relation holds between the transformed operators u'nu. 
U1 e U, u'r U. This is the quantum version of the result that PB are invariant under canonical 
transformation. </p>
<p/>
</div>
<div class="page"><p/>
<p>The correspondence with unitary transformations also holds for regular canon-
</p>
<p>ical transformations which have no infinitesimal versions. For instance, in the 
</p>
<p>coupled oscillator problem, Exercise 1 0.1.3, we performed a canonical transformation 
</p>
<p>from x1, x2, P1, P2 to xi, xu, PI, and Pn, where, for example, xi= (x1 + xz) /2. In the 
quantum theory there will exist a unitary operator such that, for example, ut X 1 U = 
(X1 +X2)/2=XI and so on.t 
</p>
<p>We can see why we can either perform the canonical transformation at the 
</p>
<p>classical level and then quantize, or first quantize and then perform the unitary 
</p>
<p>transformation-since the quantum operators respond to the unitary transformation 
</p>
<p>as do their classical counterparts to the canonical transformation, the end effect will 
</p>
<p>be the same.&sect; 
</p>
<p>Let us now return to the problem of translational invariance. Notice that in a 
</p>
<p>problem with translational in variance, Eq. ( 11.2.24) tells us that we can find the 
</p>
<p>simultaneous eigenbasis of P and H. {This agrees with our result from Chapter 5, 
</p>
<p>that the energy eigenstates of a free particle could be chosen to be momentum 
</p>
<p>eigenstates as well.ll) If a system starts out in such an eigenstate, its momentum 
</p>
<p>eigenvalue remains constant. To prove this, first note that 
</p>
<p>[P,H]=O-+[P, U(t)]=O (11.2.25) 
</p>
<p>since the propagator is a function of just H.* 
</p>
<p>Suppose at t = 0 we have a system in an eigenstate of P: 
</p>
<p>Pip)=pip) (11.2.26) 
</p>
<p>After time t, the state is U(t)ip) and we find 
</p>
<p>PU(t)ip)= U(t)Pip)= U(t)pip)=pU(t}ip) (11.2.27) 
</p>
<p>In other words, the state at time t is also an eigenstate of P with the same eigenvalue. 
</p>
<p>For such states with well-defined momentum, the conservation law (P) = 0 reduces 
to the classical form p = 0. 
</p>
<p>Finite Translations 
</p>
<p>What is the operator T(a) corresponding to a finite translation a? We find it by 
</p>
<p>the following trick. We divide the interval a into N parts of size ajN. As N-+oo, 
</p>
<p>t If the transformation is not regular, we cannot find a unitary transformation in the quantum theory, 
since unitary transformations preserve the eigenvalue spectrum. 
</p>
<p>&sect; End of digression. 
</p>
<p>II Note that a single particle whose H is translationally invariant is necessarily free. 
*When His time inaependent, we know U(t) =exp(- iHtj1i). If H=H(t), the result is true if ?commutes 
</p>
<p>with H(t) for all t. (Why?) 
</p>
<p>289 
</p>
<p>SYMMETRIES 
</p>
<p>AND THEIR 
</p>
<p>CONSEQUENCES </p>
<p/>
</div>
<div class="page"><p/>
<p>290 
</p>
<p>CHAPTER II 
</p>
<p>a/ N becomes infinitesimal and we know 
</p>
<p>ia 
T(a!N)=l-&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;P 
</p>
<p>&middot; nN 
</p>
<p>Since a translation by a equals N translations by a/N, 
</p>
<p>T(a)= lim [T(a/N)t=e-&lt;aPit. 
.V---+cx 
</p>
<p>by virtue of the formula 
</p>
<p>( &bull;N 
</p>
<p>-ax 1. ~~ ax) e = 1m --
N~&middot;xc . 1V, 
</p>
<p>( 11.2.28) 
</p>
<p>(11.2.29) 
</p>
<p>We may apply this formula. true for c numbers, to the present problem, since P is 
the only operator in the picture and commutes with everything in sight, i.e., behaves 
</p>
<p>like a c number. Since 
</p>
<p>T(a)~ 
.\ basis 
</p>
<p>(11.2.30) 
</p>
<p>we find 
</p>
<p>, I ). &gt; ( dlf d 21f (/ \X T(a llf =ifx)- -a+-,-+ .. &middot; 
&middot; dx dx" 2! 
</p>
<p>(11.2.31) 
</p>
<p>which is the full Taylor series for Vl(x-a) about the point x. 
</p>
<p>A Consistency Check. A translation by a followed by a translation by b equals 
</p>
<p>a translation by a+ b. This result has nothing to do with quantum mechanics and 
</p>
<p>is true whether you are talking about a quantum system or a sack of potatoes. It is 
</p>
<p>merely a statement about how translations combine in space. Now, we have just 
</p>
<p>built operators T, which are supposed to translate quantum states. For this interpre-
</p>
<p>tation to be consistent, it is necessary that the law of combination of the translation 
</p>
<p>operators coincide with the law of combination of the translations they represent. 
</p>
<p>Now, although we presumed this [see Eq. (11.2.29), and the line above it] in the very 
</p>
<p>act of deriving the formula for T(a), let us verify that our result T(a) = exp(- iaP /n) 
satisfies 
</p>
<p>T(a)T(b) = T(a+ b)? (11.2.32) 
</p>
<p>We find that this is indeed so: 
</p>
<p>T(a)T(b) = e &middot;iaP;~. e ibP;~ = e iia+blP/ft = T(a +b) (11.2.33) 
</p>
<p>A Digression on Finite Canonical and Unitary Transformations~ 
</p>
<p>Though it is clear that the correspondence between canonical and unitary trans-
</p>
<p>formations, established for the infinitesimal case in earlier discussions, must carry 
</p>
<p>t Optional. </p>
<p/>
</div>
<div class="page"><p/>
<p>over to the finite case, let us nonetheless go through the details. Consider, for definite-
ness, the case of translations. In the quantum theory we have 
</p>
<p>Using the identity 
</p>
<p>1 1 
e-ABe+A=B+[B A]+-[[B A] A]+-&middot;&middot;&middot; 
</p>
<p>' 2! ' ' 3! 
</p>
<p>we find 
</p>
<p>For example, if we set n = X 2 we get X 2 --+(X+ a/)2 &bull; 
In the classical case, under an infinitesimal displacement oa, 
</p>
<p>om=oa{m,p} 
</p>
<p>or 
</p>
<p>dm 
da ={m,p} 
</p>
<p>Applying the above result to the variable dmjda, we get 
</p>
<p>d 
- (dmjda) =d2mjda2 = {dm/da,p} = { {m,p},p} 
da 
</p>
<p>( 11.2.34) 
</p>
<p>and so on. The response of m to the finite translation is given by the Taylor series 
about the point a= 0: 
</p>
<p>az 
m--+m +a{ m,p} +- { { m,p},p} + &middot; &middot; &middot; 
</p>
<p>2! 
(11.2.35) 
</p>
<p>which we see is in correspondence with Eq. (11.2.34) if we make the usual 
substitutions. 
</p>
<p>Exercise 11.2.3. * Recall that we found the finite rotation transformation from the infinite-
simal one, by solving differential equations (Section 2.8). Verify that if, instead, you relate 
the transformed coordinates x andy to x and y by the infinite string of Poisson brackets, you 
get the same result, x=x cos()- y sin 0, etc. (Recall the series for sin 0, etc.) 
</p>
<p>291 
</p>
<p>SYMMETRIES 
AND THEIR 
</p>
<p>CONSEQUENCES </p>
<p/>
</div>
<div class="page"><p/>
<p>292 
</p>
<p>CHAP'TER 11 
</p>
<p>System of Particles 
</p>
<p>We will not belabor the extension of the preceding ideas to a system of N 
</p>
<p>particles. Starting with the analog of Eq. ( 11.2.8), 
</p>
<p>( 11.2.36) 
</p>
<p>we find, on expanding both sides to order r;, that 
</p>
<p>(11.2.37) 
</p>
<p>from which it follows that 
</p>
<p>ir; ...... ir; 
T(r;)=l-~ L Pi=[- p 
</p>
<p>fti~l 1i 
(11.2.38) 
</p>
<p>where P is the total momentum operator. You may verify that 
</p>
<p>T\r;)P;T(G) =Pi, i=l, ... ,N (11.2.39) 
</p>
<p>Translational invariance means in this case (suppressing indices), 
</p>
<p>H(X, P)= Tt(r;)H(X, P)T(r;) = H(X + G!, P) (11.2.40) 
</p>
<p>Whereas in the single-particle cases this implied the particle was free, here it merely 
</p>
<p>requires that H (or rather V) be a function of the coordinate differences. Any system 
</p>
<p>whose parts interact with each other, but nothing external, will have this property. 
</p>
<p>There are some profound consequences of translational invariance besides 
</p>
<p>momentum conservation. We take these up next. 
</p>
<p>Implications of Translational Invariances 
</p>
<p>Consider a system with translational invariance. Premultiplying both sides of 
</p>
<p>Eq. (11.2.21) with Tand using its unitarity, we get 
</p>
<p>[T(a), H] =0 </p>
<p/>
</div>
<div class="page"><p/>
<p>U (I) 11/1(0)) U (t)T (a)lljr(O))&bull; T(o) U(!) 11/f{Ol) _.....--......_ ____......_ 
</p>
<p>Figure 11.1. A symbolic depiction of 
</p>
<p>translational invariance. The states 
</p>
<p>are represented schematically by 
</p>
<p>wave functions. 
</p>
<p>It follows that 
</p>
<p>' 
' 
</p>
<p>T(all'f(Ol) / 
</p>
<p>JX 
</p>
<p>[T(a),U(t)]=O or T(a)U(t)=U(t)T(a) 
</p>
<p>' 
</p>
<p>-
</p>
<p>' 
</p>
<p>(11.2.41) 
</p>
<p>The consequence of this relation is illustrated by the following example (Fig. 11.1). 
</p>
<p>At t = 0 two observers A and B prepare identical systems at x = 0 and x =a, respec-
tively. If ilf'(O)) is the state vector of the system prepared by A, then T(a)ilf'(O)) is 
</p>
<p>the state vector of the system prepared by B. The two systems look identical to the 
</p>
<p>observers who prepared them. After timet, the state vectors evolve into U(t)ilf/(0)) 
</p>
<p>and U(t)T(a)ltp(O)). Using Eq. (11.2.41) the latter may be rewritten as 
</p>
<p>T(a) U(t)ll!'(O)), which is just the translated version of A.'s system at timet. Therefore 
</p>
<p>the two systems, which differed only by a translation at t = 0, differ only by the same 
</p>
<p>translation at future times. In other words, the time evolution of each system appears 
</p>
<p>the same to the observer who prepared it. Translational invariance of H implies that 
</p>
<p>the same experiment repeated at two different places will give the same result (as 
</p>
<p>seen by the local observers). We have already seen this result in the classical frame-
</p>
<p>work. We pursue it further now. 
</p>
<p>Now it turns out that every known interaction~gravitational, weak, electromag-
</p>
<p>netic, and strong (e.g., nuclear )~is translationally invariant, in that every experi-
</p>
<p>ment, if repeated at a new site, will give the same result. Consider the following 
</p>
<p>illustrative example, which clarifies the meaning of this remark. A hydrogen atom 
</p>
<p>is placed between the plates of a charged condenser. The Hamiltonian is 
</p>
<p>( 11.2.42) 
</p>
<p>where the subscripts I and 2 refer to the electron and the proton and V(R)t to the 
</p>
<p>potential due to the plates. Now this problem has no translation invariance, i.e., 
</p>
<p>which in turn means that if the atom alone is translated (away from the condenser) 
</p>
<p>it will behave differently. But this does not correspond to repeating the same experi-
ment and getting a different result, since the condenser, which materially affects the 
</p>
<p>+Remember that R is the operator corresponding to the classical variable r. 
</p>
<p>293 
</p>
<p>SYMMETRIES 
</p>
<p>AND THEIR 
</p>
<p>CONSEQUENCES </p>
<p/>
</div>
<div class="page"><p/>
<p>294 
</p>
<p>CHAPTER II 
</p>
<p>dynamics, is left behind. To incorporate it in what is translated, we redefine our 
</p>
<p>system to include the (N) charges on the condenser and write 
</p>
<p>(11.2.43) 
</p>
<p>Now the charges on the condenser enter H, not via the external field which breaks 
translational in variance, but through the Coulomb interaction, which does not. Now 
it is true that (dropping indices), 
</p>
<p>H(R +E, P) = H(R, P) 
</p>
<p>which implies that if the atom and the condenser are moved to a new site, the 
</p>
<p>behavior of the composite system will be unaffected. This result should be viewed 
</p>
<p>not as obvious or self-evident, but rather as a profound statement about the Coulomb 
interaction. 
</p>
<p>The content of the assertion made above is that every known interaction has 
</p>
<p>translational invariance at the fundamentallevel---&middot;&middot;&middot;&middot;-ifwe expand our system to include 
</p>
<p>all degrees of freedom that affect the outcome of an experiment (so that there are 
</p>
<p>not external fields, only interactions between parts of the system) the total H is 
</p>
<p>translationally invariant. This is why we apply momentum conservation to every 
</p>
<p>problem whatever be the underlying interaction. The translational invariance of 
</p>
<p>natural laws reflects the uniformity or homogeneity of space. The fact that the 
</p>
<p>dynamics of an isolated:~ system (the condenser plus atom in our example) depends 
</p>
<p>only on where the parts of the system are relative to each other and not on where 
</p>
<p>the system is as a whole, represents the fact that one part of free space is as good 
as another. 
</p>
<p>It is translational invariance that allows experimentalists in difterent parts of 
</p>
<p>the earth to claim they all did the "same" experiment, and to corroborate, correct, 
</p>
<p>and complement each other. It is the invariance of the natural laws under translations 
</p>
<p>that allows us to describe a hydrogen atom in some distant star as we do one on 
</p>
<p>earth and to apply to its dynamics the quantum mechanical laws deduced on earth. 
</p>
<p>We will examine further consequences of translational invariance toward the end of 
</p>
<p>the next section. 
</p>
<p>11.3. Time Translational Invariance 
</p>
<p>Just as the homogeneity of space ensures that the same experiment performed 
at two different places gives the same result, homogeneity in time ensures that the 
</p>
<p>; To be exact, no system is truly "isolated" except the whole universe (and only its momentum is e.&lt;actly 
</p>
<p>conserved). But in practice one draws a line somewhere, between what constitutes the system and what 
</p>
<p>is irrelevant (for practical purposes) to its evolution. I use the term "isolated" in this practical sense. 
</p>
<p>The real utility of the concepts of translational invariance and momentum conservation lies in these 
</p>
<p>approximate situations. \Vho cares if the universe as a whole is translationa lly invariant and its momen-
</p>
<p>tum is conserved" What matters to me is that I can take my equipment to another town and get the 
</p>
<p>same results and that the momentum of my system is conserved (to a good accuracy). </p>
<p/>
</div>
<div class="page"><p/>
<p>same experiment repeated at two different times gives the same result. Let us see 
what feature of the Hamiltonian ensures this and what conservation law follows. 
</p>
<p>Let us prepare at time t1 a system in state I l/fo) and let it evolve for an infinitesi-
mal time e. The state at time t1 + e, to first order in e, will be 
</p>
<p>(11.3.1) 
</p>
<p>If we repeat the experiment at time t2 , beginning with the same initial state, the state 
at time t2 + e will be 
</p>
<p>The outcome will be the same in both cases if 
</p>
<p>0=1 V1Ct2+ e))-1 l/f(ti +e)) 
</p>
<p>=( -i)[H(t2)- H(tJ)]I l/lo) 
</p>
<p>Since I l/fo) is arbitrary, it follows that 
</p>
<p>Since t2 and !1 are arbitrary, it follows that His time-independent: 
</p>
<p>dH=O 
dt 
</p>
<p>(11.3.2) 
</p>
<p>(11.3.3) 
</p>
<p>(11.3.4) 
</p>
<p>( 11.3.5) 
</p>
<p>Thus time translational invariance requires that H have no t dependence. Now 
Ehrenfest's theorem for an operator Q that has no time dependencet is 
</p>
<p>ifz(Q) = ([Q, H)) 
</p>
<p>Applying it to Q = H in a problem with time translational invariance, we find 
</p>
<p>(H)=O (11.3.6) 
</p>
<p>which is the law of conservation of energy. 
</p>
<p>t If dn/dt#O there will be an extra piece ifr&lt;dO.jdt) on the right-hand side. 
</p>
<p>295 
</p>
<p>SYMMETRIES 
</p>
<p>AND THEIR 
</p>
<p>CONSEQUENCES </p>
<p/>
</div>
<div class="page"><p/>
<p>296 
</p>
<p>CHAPTER II 
</p>
<p>An important simplification that arises if dHjdt=O is one we have repeatedly 
</p>
<p>exploited in the past: Schrodinger's equation 
</p>
<p>(11.3.7) 
</p>
<p>admits solutions of the form 
</p>
<p>llf/(t)) =IE) e-iErl~ (11.3.8) 
</p>
<p>where the time-independent ket IE) satisfies 
</p>
<p>HIE)=EIE) ( 11.3.9) 
</p>
<p>The entire dynamics, i.e., the determination of the propagator U(t), boils down to 
</p>
<p>the solution of the time-independent Schrodinger equation (11.3.9). 
</p>
<p>The considerations that applied to space translation invariance apply here as 
</p>
<p>well. In particular, all known interactions-from gravitational to strong-are time 
</p>
<p>translational invariant. Consequently, if we suitably define the system (to include the 
</p>
<p>sources of external fields that affect the experiment) the total H will be independent of 
</p>
<p>t. Consider, for example, a hydrogen atom between the plates of a discharging con-
</p>
<p>denser. If the system includes just the electron and the proton, H will depend on 
</p>
<p>time-it will have the form ofEq. (11.2.42), with V= V(R, t). This simply means that 
</p>
<p>repeating the experiment without recharging the condenser, will lead to a different 
</p>
<p>result. If, however, we enlarge the system to include theN charges on the condenser, 
</p>
<p>we end up with the H in Eq. (11.2.43), which has no t dependence. 
</p>
<p>The space-time invariance of natural laws has a profound impact on our quest 
</p>
<p>for understanding nature. The very cycle of physics-of deducing laws from some 
</p>
<p>phenomena studied at some time and place and then applying them to other phenom-
</p>
<p>ena at a different time and place-rests on the assumption that natural laws are 
</p>
<p>space-time invariant. If nature were not to follow the same rules over space-time, 
</p>
<p>there would be no rules to find, just a sequence of haphazard events with no rhyme 
</p>
<p>or reason. By repeating the natural laws over and over through all of space-time, 
</p>
<p>nature gives tiny earthlings, who probe just a miniscule region of space for a fleeting 
</p>
<p>moment (in the cosmic scale), a chance of comprehending the universe at large. 
</p>
<p>Should we at times be despondent over the fact that we know so few of nature's 
</p>
<p>laws, let us find solace in these symmetry principles, which tell us that what little we 
</p>
<p>know is universal and eternal.t 
</p>
<p>t The in variance of the laws of nature is not to be confused with our awareness of them, which does 
not change with time. For example, Einstein showed that Newtonian mechanics and gravitation are 
</p>
<p>approximations to relativistic mechanics and gravitation. But this is not to say that the Newtonian 
</p>
<p>scheme worked till Einstein came along. In other words, the relation of Newton's scheme to Einstein's 
</p>
<p>(as a good approximation in a certain limit) has always been the same, before and after we learned of 
</p>
<p>it. </p>
<p/>
</div>
<div class="page"><p/>
<p>11.4. Parity Invariance 
</p>
<p>Unlike space~time translations, and rotations, (which we will study in the next 
chapter), parity is a discrete transformation. Classically, the parity operation corre-
sponds to reflecting the state of the particle through the origin 
</p>
<p>x----+ -x 
parity 
</p>
<p>p----+ -p 
panty 
</p>
<p>(11.4.1) 
</p>
<p>In quantum theory, we define the action of the parity operator on the X basis 
as follows 
</p>
<p>Illx)=l-x) (11.4.2) 
</p>
<p>in analogy with the classical case. Given this, 
</p>
<p>Illp)=l-p) (11.4.3) 
</p>
<p>follows, as you will see in a moment. 
Given the action of II on a complete (X) basis, its action on an arbitrary ket 
</p>
<p>follows: 
</p>
<p>Illl/f)=II Joo lx)&lt;xlllf)dx 
~oc;, 
</p>
<p>= Joo 1-x)&lt;xlllf)dx 
~cxo 
</p>
<p>= J~: lx')&lt;- x'l 1/f) dx' (where x' = - x) (11.4.4) 
</p>
<p>It follows that if 
</p>
<p>&lt;xi 1/f)= 1/f(X) 
</p>
<p>( 11.4.5) 
</p>
<p>The function 1/f(- x) is the mirror image of llf(x) about the origin. Applying Eq. 
(11.4.5) to a momentum eigenstate, it will be readily found that Illp) = 1-p). 
</p>
<p>297 
</p>
<p>SYMMETRIES 
AND THEIR 
</p>
<p>CONSEQUENCES </p>
<p/>
</div>
<div class="page"><p/>
<p>298 
</p>
<p>CHAPTER II 
</p>
<p>The eigenvalues of II are just &plusmn; 1. A moment's "reflection" will prove this. 
</p>
<p>Since 
</p>
<p>IIjx)= 1-x&gt; 
</p>
<p>II2jx)=j-(-x))=lx) 
</p>
<p>Since this is true for an entire basis, 
</p>
<p>Please note that 
</p>
<p>(1) II=II- 1 
</p>
<p>(2) The eigenvalues of II are &plusmn; 1. 
(3) II is Hermitian and unitary. 
(4) Orii-1 =IIt=II. 
</p>
<p>IIz=I (11.4.6) 
</p>
<p>The eigenvectors with eigenvalue &plusmn; 1 are said to have even/odd parity. In the X 
</p>
<p>basis, where 
</p>
<p>IJI(X) .......-..+ IJI(- x) 
n 
</p>
<p>even-parity vectors have even wave functions and odd-parity vectors have odd wave 
</p>
<p>functions. The same goes for the P basis since 
</p>
<p>lJI(P)----;;-+lJI(-p) 
</p>
<p>In an arbitrary .Q basis, IJI( w) need not be even or odd even if IIJI &gt; is a parity 
eigenstate (check this). 
</p>
<p>Rather than define II in terms of its action on the kets, we may also define it 
</p>
<p>through its action on the operators: 
</p>
<p>IItxii= -x 
</p>
<p>IItPII=-P 
</p>
<p>We say H(X, P) is parity invariant if 
</p>
<p>IItH(X, P)II=H( -X, -P)=H(X, P) 
</p>
<p>In this case 
</p>
<p>[II, H] =0 
</p>
<p>(11.4.7) 
</p>
<p>( 11.4.8) 
</p>
<p>and a common eigenbasis of II and H can be found. In particular, if we consider 
</p>
<p>just bound states in one dimension (which we saw are nondegenerate ), every eigenvec-
</p>
<p>tor of His necessarily an eigenvector of II. For example, the oscillator Hamiltonian </p>
<p/>
</div>
<div class="page"><p/>
<p>satisfies Eq. (11.4.8) and its eigenfunctions have definite parity equal to ( -1(, n 
being the quantum number of the state. The particle in a box has a parity-invariant 
</p>
<p>Hamiltonian if the box extends from --- L/2 to L/2. In this case the eigenfunctions 
have parity ( - 1 r + 1, II being the quantum number. If the box extends from 0 to L, 
V(x) is not parity invariant and the eigenfunctions 
</p>
<p>(i') 112 . ('n:rr:x) 1/fn(x) = - sm ---
\L; . L 
</p>
<p>have no definite parity. (When x-&gt;- x they vanish, smce l.fln is given by the sine 
function only between 0 and L, and vanishes outside.) 
</p>
<p>If H is parity invariant, then 
</p>
<p>llU(t)=U(t)fi (11.4.9) 
</p>
<p>This means that if at t= 0 I start with a system in a state I 1/f(O)), and someone 
else starts with a system in the parity operated state llll.f/(0) ), then at a later time 
the state of his system will be related to mine by the parity transformation. 
</p>
<p>Whereas all natural laws are invariant under space-time translations {and rota-
tions) some are not invariant under parity. These are the laws of weak interactions, 
</p>
<p>which are responsible for nuclear f3 decay (among other things). This means formally 
that the Hamiltonian cannot be made parity invariant by any redefinition of the 
</p>
<p>system if weak interactions are involved. Physically this means that if two observers 
prepare initial states I 1/f(O)) and Illl.f/(0)) which are mirror images of each other, 
the final states U(t)l 1/f(O)) and U(t)lll 1/f(O)) will not be mirror images of each other 
(since DU# UTI).t Consider the following concrete example of a f3 decay: 
</p>
<p>6&deg;Co-&gt; 60Ni + e--- + v 
</p>
<p>where e is an electron and v is an antineutrino. Now it turns out that the electron 
likes to come flying out in a direction opposite to the spin of 6&deg;Co-and this implies 
parity noninvariance. Let us see how. At t = 0 I prepare a system that consists of a 
6&deg;Co nucleus with its spin up along the z axis (Fig. 11.2) (experiment A). Although 
</p>
<p>you are not yet familiar with spin, you may pretend here that 6&deg;Co is spinning in 
the sense shown. Let another observer set up another system which is just the mirror 
image of mine (experiment B). Let M denote afictitious experiment, which is what 
I see in a mirror in front of me. Notice how the spinS gets reversed under a mirror 
reflection. Let the f3 decay take place. My electron comes out down the z axis. Of 
course the mirror also shows an electron coming down the z axis. In the other real 
experiment (B), the dynamics forces the electron to come up the z axis, since the 
initialS was down. Thus B starts out as the mirror image of A but ends up different. 
Consequently, what I see in the mirror (experiment M) does not correspond to what 
can happen in real life, i.e., is not a solution to the equations of motion. 
</p>
<p>t See Exercise II .4.4 for a discussion of why the parity transformation is essentially a mirror reflection 
in three dimensions. 
</p>
<p>299 
</p>
<p>SYMMETRIES 
</p>
<p>AND THEIR 
</p>
<p>CONSEQUENCES </p>
<p/>
</div>
<div class="page"><p/>
<p>300 
</p>
<p>CHAPTER II 
</p>
<p>A 
</p>
<p>mirror 
</p>
<p>I 
I 
I 
I 
I 
</p>
<p>mirror 
</p>
<p>M B 
</p>
<p>Initial 
</p>
<p>States 
</p>
<p>Final 
</p>
<p>States 
</p>
<p>Figure 11.2. An example of parity noninvariance. In experiment A, which I perform, the spin of the 
</p>
<p>nucleus points up the z axis. In its actual mirror image, it points down (experiment M). In experiment 
</p>
<p>B, which is a real experiment, the spin is chosen to be down, i.e., B starts out as the mirror image of A. 
</p>
<p>After the decay, the momentum of my electron, p., is down the z axis. The mirror image of course also 
</p>
<p>shows the electron coming down. But in the actual experiment B, the dynamics forces the electron to 
</p>
<p>come up the z axis, i.e., antiparallel to the initial nuclear spin S. 
</p>
<p>This then is the big difference between parity and other transformations such 
</p>
<p>as space-time translations and rotations. If a certain phenomenon can happen, its 
</p>
<p>translated or rotated version can also happen, but not its mirror-reflected version, 
</p>
<p>if the phenomenon involves weak interactions. In terms of conservation laws, if an 
</p>
<p>isolated system starts out in a state of definite parity, it need not end in a state of 
</p>
<p>same parity if weak interactions are at work. The possibility that weak interactions 
</p>
<p>could be parity noninvariant was discussed in detail by Lee and Yang in 1956 and 
</p>
<p>confirmed shortly thereafter by the experiments of C. S. Wu and collaborators.t 
</p>
<p>Exercise 11 .4.1. * Prove that if [ll, H] = 0, a system that starts out in a state of even/ odd 
parity maintains its parity. (Note that since parity is a discrete operation, it has no associated 
</p>
<p>conservation law in classical mechanics.) 
</p>
<p>Exercise 11.4.2. * A particle is in a potential 
</p>
<p>V(x) = V0 sin(2nx/a) 
</p>
<p>which is invariant under the translations x--&gt;x + ma, where m is an integer. Is momentum 
conserved? Why not? 
</p>
<p>Exercise 11.4.3. * You are told that in a certain reaction, the electron comes out with its 
spin always parallel to its momentum. Argue that parity is violated. 
</p>
<p>Exercise 11.4.4. * We have treated parity as a mirror reflection. This is certainly true in 
one dimension, where x--&gt;- x may be viewed as the effect of reflecting through a (point) 
</p>
<p>mirror at the origin. In higher dimensions when we use a plane mirror (say lying on the x- y 
</p>
<p>t T. D. Lee and C. N. Yang, Phys. Rev., 104, 254 (1956); C. S. Wu. E. Ambler, R. W. Hayward. and 
R. P. Hudson, Phys. Rev., 105, 1413 (1957). </p>
<p/>
</div>
<div class="page"><p/>
<p>plane), only one (z) coordinate gets reversed, whereas the parity transformation reverses all 
three coordinates. 
</p>
<p>Verify that reflection on a mirror in the x-y plane is the same as parity followed by 
180&deg; rotation about the z axis. Since rotational invariance holds for weak interactions, 
noninvariance under mirror reflection implies noninvariance under parity. 
</p>
<p>11.5. Time-Reversal Symmetry 
</p>
<p>This is a discrete symmetry like parity. Let us first understand what it means in 
classical physics. Consider a planet that is on a circular orbit around the sun. At t = 
0 it starts at () = 0 and has a velocity in the direction of increasing fJ. In other words, 
the orbit is running counterclockwise. Let us call the initial position and momentum 
x(O), p(O). (We should really be using vectors, but ignore this fact for this discussion.) 
</p>
<p>We now define the time-reversed state as one in which the position is the same 
but the momentum is reversed: 
</p>
<p>x,(t) = x(t) p,(t)= -p(t). 
</p>
<p>In general, any quantity like position or kinetic energy, which involves an even power 
of t in its definition is left invariant and any quantity like momentum or angular 
momentum is reversed in sign under the time-reversal operation. 
</p>
<p>Say that after time T the planet has come to a final state x(T), p(T) at fJ= 
rc /2 after doing a quarter of a revolution. Now Superman (for reasons best known 
to him) stops it dead in its tracks, reverses its speed, and lets it go. What will it do? 
We know it will retrace its path and at time 2T end up in the time-reversed state of 
the initial state: 
</p>
<p>x(2T)=x(O) p(2T) = -p(O) (11.5.1) 
</p>
<p>The above equation defines time-reversal invariance (TRI). 
We can describe TRI more graphically as follows. Suppose we take a movie of 
</p>
<p>the planet from t = 0 to t = T. At t = T, we start playing the film backward. The 
backward motion of the planet will bring it back to the time-reversal initial state at 
t=2T. What we see in the movie can really happen, indeed, it was shown how 
Superman could make it happen even as you are watching the movie. More generally, 
if you see a movie of some planetary motion you will have no way of knowing if 
the projector is running forwards or backward. In some movies they get a big laugh 
out of the audience by showing cars and people zooming in reverse. As a serious 
physics student you should not laugh when you see this since these motions obey 
Newton's laws. In other words, it is perfectly possible for a set of people and cars 
to execute this motion. On the other hand, when a cartoon character falling under 
gravity suddenly starts clawing his way upwards in thin air using sheer will power, 
you may laugh since this is a gross violation of Newton's laws. 
</p>
<p>While the correctness of Eq.( 11.5.1) is intuitively clear, we will now prove it 
with the help of Newton's Second Law using the fact that it is invariant under t-+- t: 
the acceleration is even in time and the potential or force has no reference tot. Here 
are the details. Just for this discussion let us use a new clock that has its zero at the 
</p>
<p>301 
</p>
<p>SYMMETRIES 
</p>
<p>AND THEIR 
CONSEQUENCES </p>
<p/>
</div>
<div class="page"><p/>
<p>302 
</p>
<p>CHAPTER II 
</p>
<p>point of time-reversal, so that t = 0 defines the point when the motion is time-reversed. 
When the movie is run backward we see the trajectory 
</p>
<p>x,(t) = x(- t) 
</p>
<p>In other words, 5 seconds after the reversal, the object is where it was 5 seconds 
</p>
<p>before the reversal. The reversal of velocities follows from this: 
</p>
<p>. . d.x( -t) dx(- t) . 
Xr(t)=&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;= -&middot;&middot;&middot;&middot;-&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;-= -x(-t) 
</p>
<p>dt d(- t) 
</p>
<p>and does not have to be additionally encoded. The question is this: Does this orbit 
</p>
<p>x,(t) obey Newton's Second Law 
</p>
<p>given that x(t) does? We find it does: 
</p>
<p>Not all problems are time-reversal invariant. Consider a positively charged par-
</p>
<p>ticle in the x-y plane moving under a magnetic field down the z-axis. Let us say it 
</p>
<p>is released at t = 0 just like the planet, with its velocity in the direction of increasing 
e. Due to the v x B force i &middot;,.&bull;tll go in a counterclockwise orbit. Let us wait till it has 
gone around by rr /2 and at this time, t = T, time-reverse its state. Will it return to 
the time-reserved initial state at t=2T' No, it is readily seen that starting from t= 
</p>
<p>Tit will once again go on a counterclockwise circular orbit tangential to the first at 
the point of reversal. We blame the magnetic interaction for this failure of TRI: the 
</p>
<p>force now involves the velocity which is odd under time-reversal. 
</p>
<p>We now ask how all this appears in quantum mechanics. The ideas will be 
</p>
<p>illustrated in the simplest context. Let us consider a particle in one dimension with 
</p>
<p>a time-independent Hamiltonian H. In the x-representation the wave equation is 
</p>
<p>Let us first note that 
</p>
<p>performs time-reversal. This is dear from the fact that the detailed probability distri-
bution in x is unaffected by this change. On the other hand, it is clear from looking 
at plane waves (or the momentum operator - ift(8/8x)) that p-&gt;- p under complex 
</p>
<p>conjugation. </p>
<p/>
</div>
<div class="page"><p/>
<p>If the system has TRI, we must find the analog ofEq. (I 1.5.1). So let us prepare 
a state ljl(x, 0), let it evolve for time T, complex conjugate it, let that evolve for 
</p>
<p>another time T and see if we end up with the complex conjugate of the initial state. 
</p>
<p>We find the following happens at each stage: 
</p>
<p>It is clear that in order for the end result, which is w(x, 21'), to obey 
</p>
<p>'!'(X, 21') = 'l'*(x, 0) 
</p>
<p>we require that 
</p>
<p>H(x) = 1-l*(x) ( 11.5.2) 
</p>
<p>i.e., that the Hamiltonian be real. For H=P 2/2m+ V(x) this is the case, even in 
higher dimensions. On the other hand, if we have a magnetic field, P enters linearly 
</p>
<p>and H(x) #- I-l*(x). 
If H has TRI, i.e., is real, we have seen at the end of Chapter 6 that every 
</p>
<p>eigenfunction implies a degenerate one which is its complex conjugate. 
Notice that the failure of TRI in the presence of a magnetic field does not 
</p>
<p>represent any fundamental asymmetry under time-reversal in electrodynamics. The 
</p>
<p>laws of electrodynamics are invariant under t-&gt;-t. The asymmetry in our example 
arose due to our treating the magnetic field as external to the system and hence not 
to be time-reversed. If we had included in our system the currents producing the 
magnetic field, and reversed them also, the entire system would have followed the 
time-reversed trajectory. Indeed, if you had taken a movie of the experiment and 
</p>
<p>played it back, and you could have seen the charges in the wire, you would have 
found them running backward, the field would have been reversed at t= T, and the 
charge we chose to focus on would have followed the time-reversed trajectory. 
</p>
<p>On the other hand, certain experiments together with general arguments from 
quantum field theory suggest that there exist interactions in this universe which do 
not have this symmetry at the fundamental level. 
</p>
<p>There are ways to formulate TRI in a basis-independent way but we will not 
do so here. For most problems where the coordinate basis is the natural choice the 
above discussion will do. There will be a minor twist when the problem involves 
spin which has no classical counterpart. This can be handled by treating spin as we 
would treat orbital angular momentum. 
</p>
<p>303 
</p>
<p>SYMMETRIES 
</p>
<p>AND THEIR 
</p>
<p>CONSEQUENCES </p>
<p/>
</div>
<div class="page"><p/>
<p>Rotational Invariance 
and Angular Momentum 
</p>
<p>12 
</p>
<p>In the last chapter on symmetries, rotational invariance was not discussed, not 
because it is unimportant, but because it is all too important and deserves a chapter 
on its own. The reason is that most of the problems we discuss involve a single 
particle (which may be the reduced mass) in an external potential, and whereas 
translational invariance of H implies that the particle is free, rotational invariance 
of H leaves enough room for interesting dynamics. We first consider two dimensions 
and then move on to three. 
</p>
<p>12.1. Translations in Two Dimensions 
</p>
<p>Although we are concerned mainly with rotations, let us quickly review transla-
tions in two dimensions. By a straightforward extension of the arguments that led to 
Eq. (11.2.14) from Eq. (11.2.13), we may deduce that the generators of infinitesimal 
translations along the x andy directions are, respectively, 
</p>
<p>Px 
. a 
</p>
<p>-tli-
coordinate ox 
</p>
<p>basis 
</p>
<p>(12.1.1) 
</p>
<p>Py 
. a 
</p>
<p>-z1i-
coordinate oy 
</p>
<p>basis 
</p>
<p>(12.1.2) 
</p>
<p>In terms of the vector operator P, which represents momentum, 
</p>
<p>(12.1.3) 
</p>
<p>Px and Py are the dot products of P with the unit vector (i or j) in the direction of 
the translation. Since there is nothing special about these two directions, we conclude 305 </p>
<p/>
</div>
<div class="page"><p/>
<p>306 
</p>
<p>CHAPTER 12 
</p>
<p>that in general, 
</p>
<p>(12.1.4) 
</p>
<p>is the generator of translations in the direction of the unit vector fl. Finite translation 
</p>
<p>operators are found by exponentiation. Thus T(a), which translates by a, is given 
</p>
<p>by 
</p>
<p>where a= a/a. 
The ConsistenC}' Test. Let us now ask if the translation operators we have 
</p>
<p>constructed have the right laws of combination, i.e., if 
</p>
<p>T(b)T(a)=T(a+b) (12.1.6) 
</p>
<p>or equivalently if 
</p>
<p>(12, 1.7) 
</p>
<p>This amounts to asking if P, and P, may be treated as c numbers in manipulating 
</p>
<p>the exponentials. The answer is yes, since in view of Eqs. ( 12.1.1) and (12.1.2 ), the 
</p>
<p>operators commute 
</p>
<p>[P,,P,]=O (12.1.8) 
</p>
<p>and their q number nature does not surface here. The commutativity of P, and P, 
</p>
<p>reflects the commutativity of translations in the x and y directions. 
</p>
<p>Exercise 12.1.1. * Verify that a&middot; P is the generator of infinitesimal translations along a by 
considering the relation 
</p>
<p>&lt;x. yif ... &middot;~ oa. PI w&gt; = VJ(X .. oa,, y- oa .. ) 
</p>
<p>12.2. Rotations in Two Dimensions 
</p>
<p>Classically, the effect of a rotation &cent;0 k, i.e., by an angle c/&gt;o about the ;; axis 
(counterclockwise in the x y plane) has the following effect on the state of a particle: 
</p>
<p>[ xj-+[&middot;~J =[cos ci&gt;o 
y y sm ci&gt;o 
</p>
<p>-sin c/&gt;oJ[xJ 
cos c/&gt;o y 
</p>
<p>(12.2.1) 
</p>
<p>[p,J [jJ'l [cos rf&gt;o 
p, -+ .. fi,. = sin cPo 
</p>
<p>-sin r/&gt;oJ .. [Pxj 
cos cPo p, .. 
</p>
<p>(12.2.2) </p>
<p/>
</div>
<div class="page"><p/>
<p>Let us denote the operator that rotates these two-dimensional vectors by R( &cent;ok). It 
</p>
<p>is represented by the 2 x 2 matrix in Eqs. (12.2.1) and (12.2.2). Just as T(a) is the 
</p>
<p>operator in Hilbert space associated with the translation a, let U[R(&cent;ok)] be the 
</p>
<p>operator associated with the rotation R( &cent;ok). In the active transformation picturet 
</p>
<p>ilJI) --&gt;llJIR) = U[R]ilJI) 
U(R] 
</p>
<p>( 12.2.3) 
</p>
<p>The rotated state llJI R) must be such that 
</p>
<p>(X)R= (X) cos &cent;o- ( Y) sin &cent;o (l2.2.4a) 
</p>
<p>( Y) R = (X) sin qJo + ( Y) cos qJo (12.2.4b) 
</p>
<p>( l2.2.5a) 
</p>
<p>( l2.2.5b) 
</p>
<p>where 
</p>
<p>and 
</p>
<p>(X)= ( lJIIX ilJI ), etc. 
</p>
<p>In analogy with the translation problem, we define the action of U[R] on position 
</p>
<p>eigenkets: 
</p>
<p>U[R]Ix, y) = lx cos &cent;o- y sin &cent;0 , x sin &cent;o + y cos &cent;0 ) (12.2.6) 
</p>
<p>As in the case of translations, this equation is guided by more than just Eq. (12.2.4), 
</p>
<p>which specifies how (X) and ( Y) transform: in omitting a possible phase factor 
</p>
<p>g(x, y), we are also ensuring that (Px) and (Py) transform as in Eq. (12.2.5). 
</p>
<p>One way to show this is to keep the phase factor and use Eqs. (12.2.5a) and 
</p>
<p>(12.2.5b) to eliminate it. We will take the simpler route of dropping it from the 
</p>
<p>outset and proving at the end that (P .. ) and (Py) transform according to Eq. 
(12.2.5). 
</p>
<p>Explicit Construction of U[RJ 
</p>
<p>Let us now construct U[R]. Consider first an infinitesimal rotation "'=k. In this 
case we set 
</p>
<p>is .. L 
U[R( s,k)] = l- -~ ... : 
</p>
<p>t We will suppress the rotation angle when it is either irrelevant or obvious. 
</p>
<p>( 12.2. 7) 
</p>
<p>307 
</p>
<p>ROTATION 
</p>
<p>INVARIANCE 
</p>
<p>AND ANGULAR 
</p>
<p>MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>308 
</p>
<p>CHAPTER 12 
</p>
<p>where L=, the generator of irifinitesimal rotations, is to be determined. Starting with 
Eq. (12.2.6), which becomes to first order in r;, 
</p>
<p>( 12.2.8) 
</p>
<p>it can be shown that 
</p>
<p>( 12.2.9) 
</p>
<p>Exercise 12.2.1. * Provide the steps linking Eq. ( 12.2.8) to Eq. ( 12.2.9). [Hint: Recall the 
derivation of Eq. (! 1.2.8) from Eq. (11.2.6).] 
</p>
<p>Expanding both sides to order B, 
</p>
<p>yiL,I Vt) =I x(- iti r}- y(- iii t)l Vt(X, y) L \ f'}; \ DX; 
So 
</p>
<p>(, a')&middot; (. a c-----&gt;x -i11~: -y -{fi-
coo;~;;~atc \ f/}.1 \ 
</p>
<p>(12.2.10) 
</p>
<p>or in the abstract 
</p>
<p>L,=XPy- YP, (12.2.11) 
</p>
<p>Let us verify that (P,) and (P,) transform according to Eq. (12.2.5). Since 
</p>
<p>L ----&lt;- (iii &middot;~. p . - il'i ~. p .) 
,_ ffiOnlCT?-1Um (;p .1&middot; ap , X 1 
</p>
<p>baSlS ~ - ) . 
</p>
<p>(12.2.12) 
</p>
<p>it is clear that 
</p>
<p>(12.2.13) 
</p>
<p>Thus I- it:,Lj11 rotates the momentum space wave function Vf(p" Pv) by &pound;, in 
momentum space, and as a result (P,) and (Py) transform just as (X&gt; and ( Y) 
</p>
<p>do, i.e., in accordance with Eq. (12.2.5). </p>
<p/>
</div>
<div class="page"><p/>
<p>We could have also derived Eq. (12.2.11) for L, by starting with the passive 
</p>
<p>transformation equations for an infinitesimal rotation: 
</p>
<p>ut[R]P, U[R] = Px- Pys, 
</p>
<p>ut[R]PyU[R] = Px&lt;-z + Pv 
</p>
<p>By feeding Eq. (12.2.7) into the above we can deduce that 
</p>
<p>[X, [.J=-diY 
</p>
<p>These commutation relations suffice to fix Lz as XPy ---- YP,. 
</p>
<p>(l2.2.14a) 
</p>
<p>(12.2.14b) 
</p>
<p>(12.2.15a) 
</p>
<p>( 12.2.15b) 
</p>
<p>(12.2.16a) 
</p>
<p>(12.2.l6b) 
</p>
<p>(12.2.17a) 
</p>
<p>( 12.2.17b) 
</p>
<p>Exercise 12.2.2. Using these commutation relations (and your keen hindsight) derive 
</p>
<p>Lz=XP,- YP,. At least show that Eqs_ (12.2.16) and (12.2.17) are consistent with L,= 
</p>
<p>XP,- YPx. 
</p>
<p>The finite rotation operator U[R( &cent;0 k)] is 
</p>
<p>( 12.2.18) 
</p>
<p>Given 
</p>
<p>/ 1) ( ~\ c (, 
L= _ x( - if1. -~ - y -iii-) 
</p>
<p>coob~;~ate \ oy , Ox 
</p>
<p>it is hard to see that e -i&lt;/&gt;oL,/~ indeed rotates the state by the angle &cent;0 &bull; For one 
thing, expanding the exponential is complicated by the fact that x(- ili.o/ oy) and 
y(- t/18 I ox) do not commute. So let us consider an alternative form for L"'. It can 
be shown, by changing to polar coordinates, that 
</p>
<p>a 
Lz -c-o&lt;-&gt;cd-.in-at-&gt;e - ifz ---------
</p>
<p>basis a&cent; 
( 12.2. I 9) 
</p>
<p>309 
</p>
<p>ROTATION 
</p>
<p>INVARIANCE 
</p>
<p>AND ANGULAR 
MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>310 
</p>
<p>CHAPTER 12 
</p>
<p>This result can also be derived more directly by starting with the requirement 
</p>
<p>that under an infinitesimal rotation t:zk, l{l(x, y) = l{l(p, &cent; ) becomes l{f(p, &cent;- sz). 
</p>
<p>Exercise 12.2.3. * Derive Eq. (12.2.19) by doing a coordinate transformation on Eq. 
(12.2.10), and also by the direct method mentioned above. 
</p>
<p>Now it is obvious that 
</p>
<p>(12.2.20) 
</p>
<p>rotates the state by an angle &cent;0 about the z axis, for 
</p>
<p>exp(- l/Joo/ol/J)If!(p, &cent;) = l{f(p, &cent;- l/Jo) 
</p>
<p>by Taylor's theorem. It is also obvious that U[R(&cent;0k)]U[R(&cent;0 k)] = 
U[R((&cent;0 + l/Jb)k)). Thus the rotation operators have the right law of combination. 
</p>
<p>Physical Interpretation of L=. We identify L= as the angular momentum opera-
</p>
<p>tor, since (i) it is obtained from l= = XPy- YPx by the usual substitution rule (Postulate 
</p>
<p>II), and (ii) it is the generator of infinitesimal rotations about the z axis. L= is 
</p>
<p>conserved in a problem with rotational invariance: if 
</p>
<p>( 12.2.21) 
</p>
<p>it follows (by choosing an infinitesimal rotation) that 
</p>
<p>(12.2.22) 
</p>
<p>Since X, Px, Y, and Py respond to the rotation as do their classical counterparts 
</p>
<p>[Eqs. (12.2.14) and (12.2.15)] and His the same function of these operators as Yf 
</p>
<p>is of the corresponding classical variables, H is rotationally invariant whenever 
</p>
<p>Yf is. 
Besides the conservation of (Lz), Eq. (12.2.22) also implies the following: 
</p>
<p>( 1) An experiment and its rotated version will give the same result if His rotationally 
</p>
<p>invariant. 
</p>
<p>(2) There exists a common basis for L= and H. (We will spend a lot of time discussing 
</p>
<p>this basis as we go along.) 
</p>
<p>The Consistency Check. Let us now verify that our rotation and translation 
</p>
<p>operators combine as they should. In contrast to pure translations or rotations, 
</p>
<p>which have a simple law of composition, the combined effect of translations and 
</p>
<p>rotations is nothing very simple. We seem to be facing the prospect of considering 
</p>
<p>every possible combination of rotations and translations, finding their net effect, and 
</p>
<p>then verifying that the product of the corresponding quantum operators equals the </p>
<p/>
</div>
<div class="page"><p/>
<p>operator corresponding to the result of all the transformations. Let us take one small 
</p>
<p>step in this direction, which will prove to be a giant step toward our goal. 
</p>
<p>Consider the following product of four infinitesimal operations: 
</p>
<p>where&pound;= c,i + ~&gt;yi- By subjecting a point in the x-y plane to these four operations 
we find 
</p>
<p>(12.2.23) 
</p>
<p>i.e., that the net effect is a translation by -.&lt;:yeo i + t:x coP In the above, we have 
ignored terms involving ~&gt;;, c;, c;, and beyond. We do, however, retain the Cx c= 
and &amp;yC= terms since they contain the first germ of noncommutativity. Note that 
</p>
<p>although these are second-order terms, they are fully determined in our approxima-
</p>
<p>tion, i.e. unaffected by the second-order tenus that we have ignored. Equation 
</p>
<p>( 12.2.23) imposes the following restriction on the quantum operators: 
</p>
<p>(12.2.24) 
</p>
<p>or 
</p>
<p>(12.2.25) 
</p>
<p>By matching coefficients (you should do this) we can deduce the following 
constraints: 
</p>
<p>[ P,, L=] = ---diP} 
</p>
<p>[P,, L=] = ifiP, 
</p>
<p>which are indeed satisfied by the generators [Eq. (12.2.17)]. 
</p>
<p>So our operators have passed this test. But many other tests are possible. How 
about the coefficients of terms such as &amp;x&amp;;, or more generally, how about finite 
</p>
<p>~ Note that if rotations and translations commuted, the fourfold product would equal /, as can be seen 
</p>
<p>by rearranging the factors so that the two opposite rotations and the two opposite translations cancel 
</p>
<p>each other. The deviation from this result of l is a measure of noncommutativity _ Given two symmetry 
</p>
<p>operations that do not commute, the fourfold product provides a nice characterization of their noncom-
</p>
<p>mutavity. As we shall see, this characterization is complete. 
</p>
<p>311 
</p>
<p>ROTATION 
</p>
<p>INVARIANCE 
</p>
<p>AND ANGULAR 
</p>
<p>MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>312 
</p>
<p>CHAPTER 12 
</p>
<p>rotations? How about tests other than the fourfold product, such as one involving 
14 translations and six rotations interlaced? 
</p>
<p>There is a single answer to all these equations: there is no need to conduct any 
</p>
<p>further tests. Although it is beyond the scope of this book to explain why this is so, 
</p>
<p>it is not hard to explain when it is time to stop testing. We can stop the tests when 
all possible commutators between the generators have been considered. In the present 
</p>
<p>case, given the generators P" Py. and Lo, the possible commutators are [Po Lz], 
[Pn Lz], and [P" Pvl&middot; We have just finished testing the first two. Although the third 
was tested implicitly in the past, let us do it explicitly again. If we convert the law 
</p>
<p>of combination 
</p>
<p>[xl I x + c,l [x + E,l I x l [xj 
y_ 7 L y - --:; y-+- Cy ~ l_y + &amp;y ---:-:_; y (12.2.26) 
</p>
<p>into the operator constraint 
</p>
<p>( 12.2.27) 
</p>
<p>we deduce that 
</p>
<p>[P,, P,]=O 
</p>
<p>which of course is satisfied by the generators P, and Py. [Although earlier on, we 
</p>
<p>did not consider the fourfold product, Eq. (12.2.27), we did verify that the arguments 
</p>
<p>of the T operators combined according to the laws of vector analysis. Equation 
</p>
<p>(12.2.26) is just a special case which brings out the commutativity of Px and Pr.l 
When I say that there are no further tests to be conducted, I mean the following: 
</p>
<p>( 1) Every consistency test will reduce to just another relation between the com-
</p>
<p>mutators of the generators. 
(2) This relation will be automatically satisfied if the generators pass the tests 
</p>
<p>we have finished conducting. The following exercise should illustrate this point. 
</p>
<p>Exercise 12.2.4. * Rederive the equivalent of Eq. (12.2.23) keeping terms of order 
(You may assume ~:,."-0.) Use this infom1ation to rewrite Eq. (12.2.24) to order 
</p>
<p>equating coefficients of this term deduce the constraint 
</p>
<p>This seems to conflict with statement (I) made above, but not really, in view of the identity 
</p>
<p>Using the identify, verify that the new constraint corning from the l:xt:; term is satisfted given 
</p>
<p>the commutation relations between P,, P, , and L. </p>
<p/>
</div>
<div class="page"><p/>
<p>Vector Operators 
</p>
<p>We call V = Vxi + Vyj a vector operator if Vx and Vy transform as components 
of a vector under a passive transformation generated by U[R]: 
</p>
<p>ut[R]V; u[RJ =I Rij Vj 
j 
</p>
<p>where Rij is the 2 x 2 rotation matrix appearing in Eq. (12.2.1). Examples of V are 
P=Pxi+Pyj and R=Xi+ Yj [see Eqs. (12.2.14) and (12.2.15)]. Note the twofold 
character of a vector operator such as P: on the one hand, its components are 
operators in Hilbert space, and on the other, it transforms as a vector in \P(R). 
</p>
<p>The same definition of a vector operator holds in three dimensions as well, with 
the obvious difference that Rij is a 3 x 3 matrix. 
</p>
<p>12.3. The Eigenvalue Problem of L .. 
</p>
<p>We have seen that in a rotationally invariant problem, Hand L= share a common 
basis. In order to exploit this fact we must first find the eigenfunctions of L=. We 
begin by writing 
</p>
<p>(12.3.1) 
</p>
<p>in the coordinate basis: 
</p>
<p>_ i1i Dlfl~o(P, 4&gt;) I A.) 
aq, = lfl,,(p, '+' (12.3.2) 
</p>
<p>The solution to this equation is 
</p>
<p>lf/t,(P, 4&gt;) = R(p) eil,&cent;!~ (12.3.3) 
</p>
<p>where R(p) is an arbitrary function normalizable with respect to J: p dp.t We shall 
have more to say about R(p) in a moment. But first note that I= seems to be arbitrary: 
it can even be complex since 4&gt; goes only from 0 to 2n. (Compare this to the 
eigenfunctions eipx/~ of linear momentum, where we could argue that p had to be 
real to keep llf/1 bounded as lxl-+oo.) The fact that complex eigenvalues enter the 
answer, signals that we are overlooking the Hermiticity constraint. Let us impose it. 
The condition 
</p>
<p>(12.3.4) 
</p>
<p>t This will ensure that IJI is normalizable with respect to 
</p>
<p>ff dxdy= r r~ pdpd&lt;/J 
0 0 
</p>
<p>313 
</p>
<p>ROTATION 
INVARIANCE 
</p>
<p>AND ANGULAR 
MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>314 
</p>
<p>CHAPTER 12 
</p>
<p>becomes in the coordinate basis 
</p>
<p>(12.3.5) 
</p>
<p>If this requirement is to be satisfied for all 1Jf 1 and 1Jf 2 , one can show (upon integrating 
</p>
<p>by parts) that it is enough if each IJl obeys 
</p>
<p>IJ.I(p, 0) = ljf(p, 27r) (12.3.6) 
</p>
<p>If we impose this constraint on the Lz eigenfunctions, Eq. (12.3.3), we find 
</p>
<p>(12.3.7) 
</p>
<p>This forces lz not merely to be real, but also to be an integral multiple of n: 
</p>
<p>m=O, &plusmn; 1, &plusmn;2, ... (12.3.8) 
</p>
<p>One calls m the magnetic quantum number. Notice that lc = mn implies that 1Jf IS a 
</p>
<p>single-valued function of cp. (However, see Exercise 12.3.2.) 
</p>
<p>Exercise 12.3.1. Provide the steps linking Eq. (12.3.5) to Eq. (12.3.6). 
</p>
<p>Exercise 12.3.2. Let us try to deduce the restriction on l, from another angle. Consider 
</p>
<p>a superposition of two allowed l, eigenstates: 
</p>
<p>l!f(p, &cent;} = A(p) e'&lt;~&gt;( li + B(p} e'"'1"" 
</p>
<p>By demanding that upon a 2rr rotation we get the same physical state (not necessarily the 
</p>
<p>same state vector), show that /,- ( = mft, where m is an integer. By arguing on the grounds 
</p>
<p>of symmetry that the allowed values of l, must be symmetric about zero, show that these 
values are either ... , 3fi/2, fij2, -fi/2, -3fi/2, ... or ... , 2ti, ti, 0, -ti, -211 ..... 1t is not 
possible to restrict !, any further this way. [] 
</p>
<p>Let us now return to the arbitrary function R(p) that accompanies the eigen-
</p>
<p>functions of Lz. Its presence implies that the eigenvalue '= = mfz does not nail down 
a unique state in Hilbert space but only a subspace 'Wm. The dimensionality of this 
space is clearly infinite, for the space of all normalizable functions R is infinite 
dimensional. The natural thing to do at this point is to introduce some operator that 
commutes with Lz and whose simultaneous eigenfunctions with Lz pick out a unique 
basis in each W m. We shall see in a moment that the Hamiltonian in a rotationally 
invariant problem does just this. Physically this means that a state is not uniquely 
specified by just its angular momentum (which only fixes the angular part of the 
wave function), but it can be specified by its energy and angular momentum in a 
rotationally invariant problem. </p>
<p/>
</div>
<div class="page"><p/>
<p>It proves convenient to introduce the functions 
</p>
<p>( 12.3.9) 
</p>
<p>which would have been nondegenerate eigenfunctions of L= if the p coordinate had 
</p>
<p>not existed. These obey the orthonormality condition 
</p>
<p>(12.3.10) 
</p>
<p>It will be seen that these functions play an important role in problems with rotational 
</p>
<p>in variance. 
</p>
<p>Exercise 12.3.3. * A particle is described by a wave function 
</p>
<p>l!'(P, t/J)=A 
</p>
<p>Show (by expressing cos"&cent; in terms of &lt;llml that 
</p>
<p>P(/=0)=2/3 
</p>
<p>P(l==-21i)= 1/6 
</p>
<p>(Hint: Argue that the radial part e-p'12"' is irrelevant here.) 
</p>
<p>Exercise 12.3.4. * A particle is described by a wave function 
</p>
<p>l!'(P, &cent;)=A e p', 2"' (~cos &cent;+sin &cent;) 
</p>
<p>Show that 
</p>
<p>P(l== n) =PUc.= -11) = i 
</p>
<p>Solutions to Rotationally Invariant Problems 
</p>
<p>Consider a problem where V(p, &cent;) = V(p). The eigenvalue equation for His 
</p>
<p>(12.3.11) 
</p>
<p>(We shall use J.1 to denote the mass, since m will denote the angular momentum 
</p>
<p>quantum number.) Since [H, LJ = 0 in this problem, we seek simultaneous eigen-
</p>
<p>functions of Hand L=. We have seen that the most general eigenfunction of L= with 
</p>
<p>315 
</p>
<p>ROTATION 
</p>
<p>INVARIANCE 
</p>
<p>AND ANGULAR 
</p>
<p>MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>316 
</p>
<p>CHAPTER 12 
</p>
<p>eigenvalue mfz is of the form 
</p>
<p>1/lm(p, &lt;/J)=R(p)(27r) 1 2e'"''"=R(p)&lt;f&gt;m{&cent;) 
</p>
<p>where R(p) is undetermined. In the present case R is determined by the requirement 
</p>
<p>that 
</p>
<p>(12.3.12) 
</p>
<p>be an eigenfunction of H as well, with eigenvalue E, i.e., that VJ Em satisfy Eq. 
</p>
<p>(12.3.11). Feeding the above form into Eq. (12.3.11), we get the radial equation that 
</p>
<p>determines REm(P) and the allowed values forE: 
</p>
<p>(12.3.13) 
</p>
<p>As we change the potential, only the radial part of the wave function, R, changes; 
</p>
<p>the angular part &lt;Dm is unchanged. Thus the functions &lt;Dm( &cent; ), which were obtained 
by pretending p does not exist, provide the angular part of the wave function in the 
eigenvalue problem of any rotationally invariant Hamiltonian. 
</p>
<p>Exercise 12.3.5* Note that the angular momentum seems to generate a repulsive poten-
</p>
<p>tial in Eq. (12.3.13). Calculate its gradient and identify it as the centrifugal force. 
</p>
<p>f-'xercise 12.3.6. Consider a particle of mass J.J constrained to move on a circle of radius 
</p>
<p>a. Show that H = L~/2fJ.a 2 &bull; Solve the eigenvalue problem of Hand interpret the degeneracy. 
</p>
<p>Exercise 1 2.3. 7. * ! The Isotropic Oscillator). Consider the Hamiltonian 
</p>
<p>(1) Convince yourself [H, L,]=O and reduce the eigenvalue problem of H to the radial 
</p>
<p>differential equation for Rr:m(p). 
</p>
<p>(2) Examine the equation as p-&gt;0 and show that 
</p>
<p>(3) Show likewise that up to powers of p 
</p>
<p>Rt.m\P)~ 
p-. --,_ </p>
<p/>
</div>
<div class="page"><p/>
<p>(4) Switch to dimensionless variables t:= Ejlim, y = (pmjli) 112p. 
(5) Convert the equation for R into an equation for U. (I suggest proceeding in two 
</p>
<p>stages: R= ylmlj;f=e-y'/2 U.) You should end up with 
</p>
<p>u" +[e 1 m~ + ')-2y ]u'+ (2t:-21ml-2)u=o 
</p>
<p>(6) Argue that a power series for U of the form 
</p>
<p>U(y)= I Cry' 
r=O 
</p>
<p>will lead to a two-term recursion relation. 
(7) Find the relation between C,+ 2 and C,. Argue that the series must terminate at some 
</p>
<p>finite r if they--&gt; oo behavior of the solution is to be acceptable. Show t:= r + lml + I leads to 
termination after r terms. Now argue that r is necessarily even-i.e., r=2k. (Show that if r is 
odd, the behavior of R as p--&gt;0 is not p 1m 1.) So finally you must end up with 
</p>
<p>E= (2k+ lml +!)lim, k=O, I, 2, ... 
</p>
<p>Define n=2k+lml, so that 
</p>
<p>E.=(n+ !)lim 
</p>
<p>(8) For a given n, what are the allowed values of lml? Given this information show that 
for a given n, the degeneracy is n + 1. Compare this to what you found in Cartesian coordinates 
(Exercise 10.2.2). 
</p>
<p>(9) Write down all the normalized eigenfunctions corresponding to n = 0, 1. 
( 10) Argue that the n = 0 function must equal the corresponding one found in Cartesian 
</p>
<p>coordinates. Show that the two n = 2 solutions are linear combinations of their counterparts 
in Cartesian coordinates. Verify that the parity of the states is ( -1 f as you found in Cartesian 
coordinates. 
</p>
<p>Exercise 12.3.8. * Consider a particle of charge q in a vector potential 
</p>
<p>A=~(-yi+xj) 
2 
</p>
<p>(1) Show that the magnetic field is B=Bk. 
(2) Show that a classical particle in this potential will move in circles at an angular 
</p>
<p>frequency m0 = qB/ pc. 
(3) Consider the Hamiltonian for the corresponding quantum problem: 
</p>
<p>H [Px+qYBj2c] 2 +[Py-qXBj2c]' 
</p>
<p>2p 2p 
</p>
<p>Show that Q=(cPx+qYB/2)/qB and P=(Py-qXBj2c) are canonical. Write Hin terms 
of P and Q and show that allowed levels are E=(n+!j2)1im0 . 
</p>
<p>317 
</p>
<p>ROTATION 
INVARIANCE 
</p>
<p>AND ANGULAR 
MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>318 
</p>
<p>CHAPTER 12 
</p>
<p>(4) Expand H out in terms of the original variables and show 
</p>
<p>I \ 
</p>
<p>( Wo J Wo ll = H &middot;&middot;~&middot;&middot;&middot;, J1 - &middot;&middot;:;- L, 
\ .:;.. ' ._ 
</p>
<p>where H(wo/2, p) is the Hamiltonian for an isotropic two-dimensional harmonic oscillator 
</p>
<p>of mass J1 and frequency ro0 /2. Argue that the same basis that diagonalizcd H( ro 0 /2, 11) will 
</p>
<p>diagonalize H By thinking in terms of this basis, show that the allowed levels for H are 
</p>
<p>E=(k+~lml-~m+~)l1wo, where k is any integer and m is the angular momentum. Con-
vince yourself that you get the same levels from this fonnula as from the earlier one 
</p>
<p>[E= (n +I /2)fiw 0 ]. We shall return to this problem in Chapter 21. 
</p>
<p>12.4. Angular Momentum in Three Dimensions 
</p>
<p>It is evident that as we pass from two to three dimensions, the operator L, picks 
</p>
<p>up two companions Lx and Ly which generate infinitesimal rotations about the x 
</p>
<p>andy axes, respectively. So we have 
</p>
<p>( 12.4.1 a) 
</p>
<p>Ly=ZP,.-XP, ( 12.4.1 b) 
</p>
<p>C=J:'P,- YP, (12.4.lc) 
</p>
<p>As usual, we subject these to the consistency test. It may be verified, (Exercise 12.4.2), 
</p>
<p>that if we take a point in three-dimensional space and subject it to the following 
</p>
<p>rotations: R(sxi), R(syj), R( -&middot; sxi) and lastly R(&middot;- &amp;yj). it ends up rotated by 
</p>
<p>- CxSyk. In other words 
</p>
<p>(12.4.2) 
</p>
<p>It follows that the quantum operators U[R] must satisfy 
</p>
<p>U[R(- &lt;:yj)] U[R(- t:.,J)] U[R( t:yj )] U[R( t:,i )] = U[R(- &lt;:xt:yk)] (12.4.3) 
</p>
<p>If we write each U to order F- and match coefficients of t:xsy, we will find 
</p>
<p>(l2.4.4a) 
</p>
<p>By considering two similar tests involving t:,..t:, and &pound;z&amp;x, we can deduce the 
</p>
<p>constraints 
</p>
<p>(12.4.4b) 
</p>
<p>( 12.4.4c) </p>
<p/>
</div>
<div class="page"><p/>
<p>You may verify that the operators in Eq. (12.4.1) satisfy these constraints. So they 
are guaranteed to generate finite rotation operators that obey the right laws of 
combination. 
</p>
<p>The three relations above may be expressed compactly as one vector equation 
</p>
<p>LX L=i1iL 
</p>
<p>Yet another way to write the commutation relations is 
</p>
<p>3 
</p>
<p>[L;, Lj]=i1i L &amp;ijkLk 
k~l 
</p>
<p>(12.4.5) 
</p>
<p>( 12.4.6) 
</p>
<p>In this equation, i and} run from 1 to 3, L 1 , L2 , and L 3 stand for Lx, Ly, and L=, 
respectivelyJ and &amp;ijk are the components of an antisymmetric tensor of rank 3, with 
the following properties: 
</p>
<p>(1) They change sign when any two indices are exchanged. Consequently no two 
indices can be equal. 
</p>
<p>(2) &pound; 123 = 1. 
</p>
<p>This fixes all other components. For example, 
</p>
<p>&pound;312=(-1)(-1)=+1 (12.4. 7) 
</p>
<p>and so on. In short, E;jk is +I for any cyclic permutation of the indices in &pound; 123 and 
-1 for the others. (The relation 
</p>
<p>c=axb 
</p>
<p>between three vectors from W3(R) may be written in component form as 
</p>
<p>3 3 
</p>
<p>c;= L L &pound;ukajbk 
j~ I k~ I 
</p>
<p>(12.4.8) 
</p>
<p>(12.4.9) 
</p>
<p>Of course ax a is zero if a is a vector whose components are numbers, but not zero 
if it is an operator such as L.) 
</p>
<p>Exercise 12.4.1. &bull; (I) Verify that Eqs. (12.4.9) and Eq. (12.4.8) are equivalent, given the 
definition of Euk. 
</p>
<p>(2) Let U, , Uz, and U3 be three energy eigenfunctions of a single particle in some 
potential. Construct the wave function If! A(x1 , x2 , x3) of three fermions in this potential, one 
of which is in U, , one in U2 , and one in U3 , usihg the Euk tensor. 
</p>
<p>Exercise 12.4.2. &bull; (I) Verify Eq. (12.4.2) by first constructing the 3 x 3 matrices corre-
sponding to R(&amp;xi) and R(&amp;yj), to order&amp;. 
</p>
<p>(2) Provide the steps connecting Eqs. (12.4.3) and (12.4.4a). 
</p>
<p>t We will frequently let the indices run over I, 2, and 3 instead of x, y, and z. 
</p>
<p>319 
</p>
<p>ROTATION 
</p>
<p>INVARIANCE 
</p>
<p>AND ANGULAR 
</p>
<p>MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>320 
</p>
<p>CHAPTER 12 
</p>
<p>(3) Verify that Lx and Ly defined in Eq. (12.4.1) satisfy 
</p>
<p>other commutators follows by cyclic pemmtation. 
(l2.4.4a). The proof for 
</p>
<p>We next define the total angular momentum operator squared 
</p>
<p>(12.4.10) 
</p>
<p>It may be verified (by you) that 
</p>
<p>[!/, L,] =0, i=x, y, or z (12.4.11) 
</p>
<p>Finite Rotation Operators. Rotations about a given axis commute. So a finite 
</p>
<p>rotation may be viewed as a sequence of infinitesimal rotations about the same axis. 
</p>
<p>What is the~ operator that rotates by angle 9, i.e., by an amount 8 about an axis 
</p>
<p>parallel to 8? If 9 = O,L then clearly 
</p>
<p>U[R(O,i)] =e~w,L.'fi 
</p>
<p>The same goes for 9 along the unit vectors j and k. What if 0 has some arbitrary 
</p>
<p>direction? We conjecture that Le = {j &middot; L (where {j = 0 I 8) is the generator of infin-
itesimal rotations about that axis and that 
</p>
<p>N 
</p>
<p>(
1 iO- \) .&middot; 
</p>
<p>U[R(O)]= lim /--&middot;-:;,
1 
</p>
<p>N.:_8&middot;L =e&middot;~tfJB&middot;Lin 
."'l-x \ fl 
</p>
<p>(12.4.12) 
</p>
<p>Our conjecture is verified in the following exercise. 
</p>
<p>Exercise 12.4.3. * We would like to show that 0 &middot; L generates rotations about the axis 
parallel to iJ. Let oG be an infinitesimal rotation parallel to G. 
</p>
<p>(I) Show that when a vector r is rotated by an angle 50, it changes to r + 80 x r. (It 
might help to start with r _LoG and then generalize.) 
</p>
<p>(2) We therefore demand that (to first order, as usual) 
</p>
<p>VJ(r)----&gt; VJ(r-5!1xr)=VJ(r)-(8Gxr)&middot;VVJ 
U[R(o6)] 
</p>
<p>Comparing to U [ R( 8!1)] = 1-- ( i M:l /'n)L11, show that L0 = fi &middot; L 
</p>
<p>Exercise 12.4.4. * Recall that V is a vector operator if its components V, transform as 
</p>
<p>rf[RJ v, U[R] = z: Rij Vj (12.4.13) </p>
<p/>
</div>
<div class="page"><p/>
<p>( 1) For an infinitesimal rotation 80, show, on the basis of the previous exercise, that 
</p>
<p>I Rij ~= v,+ (80 x v ),= v,+ I I Gijk(80)i vk 
j k 
</p>
<p>(2) Feed in U[R] =I- (i/fi)80&middot; L into the left-hand side of Eq. (12.4.13) and deduce 
that 
</p>
<p>[ v, Lj 1 = ifi I Gijk vk (12.4.14) 
k 
</p>
<p>This is as good a definition of a vector operator as Eq. (12.4.13). By setting V = L, we can 
obtain the commutation rules among the L's. 
</p>
<p>If the Hamiltonian is invariant under arbitrary rotations, 
</p>
<p>ut[R]HU[R]=H (12.4.15) 
</p>
<p>it follows (upon considering infinitesimal rotations around the x, y, and z axes) that 
</p>
<p>[H,L1]=0 ( 12.4.16) 
</p>
<p>and from it 
</p>
<p>( 12.4.17) 
</p>
<p>Thus L 2 and all three components of L are conserved. It does not, however, follow 
that there exists a basis common to H and all three L's. This is because the L's do 
not commute with each other. So the best one can do is find a basis common to H, 
L2, and one of the L's, usually chosen to beL=. 
</p>
<p>We now examine the eigenvalue problem of the commuting operators L2 and 
L=. When this is solved, we will turn to the eigenvalue problem of H, L2, and L=. 
</p>
<p>12.5. The Eigenvalue Problem of L 2 and Lz 
</p>
<p>There is a close parallel between our approach to this problem and that of the 
harmonic oscillator. Recall that in that case we (1) solved the eigenvalue problem 
of H in the coordinate basis; (2) solved the problem in the energy basis directly, 
using the a and at operators, the commutation rules, and the positivity of H; 
(3) obtained the coordinate wave function V' n(Y) given the results of part (2), by 
the following trick. We wrote 
</p>
<p>aiO)=O 
</p>
<p>in the coordinate basis as 
</p>
<p>321 
</p>
<p>ROTATION 
INVARJANCE 
</p>
<p>AND ANGULAR 
MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>322 
</p>
<p>CHAPTER 12 
</p>
<p>which immediately gave us IJ!o(Y)- e-_v' 2, up to a normalization that could be easily 
</p>
<p>determined. 
</p>
<p>Given the normalized eigenfunction IJ!o(y), we got IJI11(y) by the application of 
</p>
<p>the (diff~rential) operator (at)"/(n!) 1 2 --&gt;(y-iljcy)"/(2"n!) 112 &bull; 
</p>
<p>In the present case we omit part (I), which involves just one more bout with 
</p>
<p>differential equations and is not particularly enlightening. 
</p>
<p>Let us now consider part (2). It too has many similarities with part (2) of the 
</p>
<p>oscillator problem.t We begin by assuming that there exists a basis ja, {3) common 
</p>
<p>to L 2 and L=: 
</p>
<p>Cjaf3)=aiaf3) (12.5.1) 
</p>
<p>(12.5.2) 
</p>
<p>We now define raising and lower operators 
</p>
<p>(12.5.3) 
</p>
<p>which satisfy 
</p>
<p>( 12.5.4) 
</p>
<p>and of course (since L 2 commutes with L, and L,.) 
</p>
<p>( 12.5.5) 
</p>
<p>Equations (12.5.4) and (12.5.5) imply that L&plusmn; raise/lower the eigenvalue of L= by 
</p>
<p>1i, while leaving the eigenvalue of L 2 alone. For example, 
</p>
<p>and 
</p>
<p>L=(L+i a {3)) = (L+ L= + 1iL+)i af3) 
</p>
<p>= (L+ f3 + 1iL+)i af3) 
</p>
<p>= ({3 + 1i)(L+iaf3)) (12.5.6) 
</p>
<p>(12.5.7) 
</p>
<p>From Eqs. (12.5.6) and (12.5.7) it is clear that L+l af3) is proportional to the normal-
</p>
<p>ized eigenket I a, f3 + 1i): 
</p>
<p>( 12.5.8a) 
</p>
<p>t If you have forgotten the latter, you are urged to refresh your memory at this point. </p>
<p/>
</div>
<div class="page"><p/>
<p>It can similarly be shown that 
</p>
<p>Ll a/3) = C(a, f3)1a, f3- fr) (12.5.8b) 
</p>
<p>The existence of L&plusmn; implies that given an eigenstate I a /3) there also exist eigen-
states I a, f3 + fr), I a, f3 + 2fr), ... ; and I a, f3- fr), I a, f3- 2fr), .... This clearly 
signals trouble, for classical intuition tells us that the z component of angular momen-
tum cannot take arbitrarily large positive or negative values for a given value of the 
square of the total angular momentum; in fact classically I/= 1 5, (1 2 ) 112&bull; 
</p>
<p>Quantum mechanically we have 
</p>
<p>(12.5.9) 
</p>
<p>which implies 
</p>
<p>(since L; + L; is positive definite) or 
</p>
<p>(12.5.10) 
</p>
<p>Since /3 2 is bounded by a, it follows that there must exist a state I a f3max) such that 
it cannot be raised : 
</p>
<p>Operating with L_ and using L_ L+ = L 2 - L;- f!L=, we get 
</p>
<p>(L2 - L;- frLz)laf3max) = 0 
</p>
<p>C a - P~ax- F!f3max) I a f3ma&bull;&gt; = o 
</p>
<p>a= f3max(f3max + fr) 
</p>
<p>(12.5.11) 
</p>
<p>( 12.5.12) 
</p>
<p>Starting with I a f3max) let us operate k times with L_, till we reach a state I a f3m;n) 
that cannot be lowered further without violating the inequality (12.5.10): 
</p>
<p>Llaf3min) =0 
</p>
<p>L+Liaf3min)=O 
</p>
<p>(L2 - L; + 1iLz)laf3min) = 0 
</p>
<p>a= f3min(/3min -1i) 
</p>
<p>A comparison of Eqs. (12.5.12) and (12.5.13) shows (as is to be expected) 
</p>
<p>/Jmin=-/Jmax 
</p>
<p>(12.5.13) 
</p>
<p>(12.5.14) 
</p>
<p>323 
</p>
<p>ROTATION 
INVARIANCE 
</p>
<p>AND ANGULAR 
MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>324 
</p>
<p>CHAPTER 12 
</p>
<p>Table 12.1. Some Low-Angular-Momentum States 
</p>
<p>(Angular momentum) 
</p>
<p>k/2 /3max a laP&gt; 
</p>
<p>0 0 0 10, 0) 
</p>
<p>1/2 fr/2 (lj2)(3/2)fr2 1(3/4)fr2 , fr/2) 
</p>
<p>1(3/4)fr2, -fr/2) 
</p>
<p>tr (1)(2)fr2 12fr2, fl) 
</p>
<p>12fr2 , 0) 
</p>
<p>12fr2, -fl) 
</p>
<p>3/2 
</p>
<p>Since we got to la/Jmin) from laPmax) ink steps of 1i each, it follows that 
</p>
<p>/Jmax- /Jmin = 2/Jmax = fzk 
</p>
<p>k=O, 1, 2, (12.5.15a) 
</p>
<p>(12.5.15b) 
</p>
<p>We shall refer to (k/2) = (/Jmax/fz) as the angular momentum of the state. Notice 
</p>
<p>that unlike in classical physics, p;.,ax is less than a, the square of the magnitude of 
angular momentum, except when a= Pmax = 0, i.e., in a state of zero angular 
</p>
<p>momentum. 
Let us now take a look at a few of the low-angular-momentum states listed in 
</p>
<p>Table 12.1. 
At this point the astute reader raises the following objection. 
</p>
<p>A.R.: I am disturbed by your results for odd k. You seem to find that Lz can 
</p>
<p>have half-integral eigenvalues (in units of fz). But you just convinced us in Section 
</p>
<p>12.3 that Lz has only integral eigenvalues m (in units of fz). Where did you go wrong? 
</p>
<p>R.S.: Nowhere, but your point is well taken. The extra (half-integral) eigenval-
</p>
<p>ues arise because we have solved a more general problem than that of Lx, Ly, Lz, 
</p>
<p>and L2 (although we didn't intend to). Notice that nowhere in the derivation did we 
</p>
<p>use the explicit expressions for the L's [Eq. (12.4.1)] and in particular Lz-+- ifzo/ 
</p>
<p>aq,. (Had we done so, we would have gotten only integral eigenvalues as you expect.) 
We relied instead on just the commutation relations, L x L = ifzL. Now, these commu-
</p>
<p>tation relations reflect the law of combinations of infinitesimal rotations in three 
</p>
<p>dimensions and must be satisfied by the three generators of rotations whatever the 
</p>
<p>nature of the wave functions they rotate. We have so far considered just scalar wave 
</p>
<p>functions IJI(x, y, z), which assign a complex number (scalar) to each point. Now, 
</p>
<p>there can be particles in nature for which the wave function is more complicated, 
</p>
<p>say a vector field 'l'(x, y, z) = 1Jfx(x, y, z)i + IJ!y(x, y, z)j + IJ!z(x, y, x)k. The response 
of such a wave function to rotations is more involved. Whereas in the scalar case 
</p>
<p>the effect of rotation by 80 is to take the number assigned to each point (x, y, z) </p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 12.1. The effect of the infinitesimal rotations by B, 
on a vector 11f in two dimensions is to (I) first reassign it 
</p>
<p>to the rotated point (x', y') (2) and then rotate the vector 
</p>
<p>itself by the infinitesimal angle. The differential operator L, 
does the first part while a 2 x 2 spin matrix Sz does the 
second. 
</p>
<p>y 
</p>
<p>... 
"' 
</p>
<p>"''(x' ,y') ... 
1[ ,,- (x u) ....... _\,..!._........ ,, 
</p>
<p>.... 
1/t 
</p>
<p>... , ....... 
~-----------------------&middot; 
</p>
<p>and reassign it to the rotated point (x', y', z'), in the vector case the vector at (x, y, z) 
(i) must itself be rotated by 80 and (ii) then reassigned to (x', y', z'). (A simple 
example from two dimensions is given in Fig. 12.1.) The differential operators Lx, 
Ly, and Lz will only do part (ii) but not part (i), which has to be done by 3 x 3 
matrices Sx, Sy, and Sz which shuffle the components 'lfx, 'lfy, 'l'z of '1'. In such 
cases, the generators of infinitesimal rotations will be of the form 
</p>
<p>J;=L;+S; 
</p>
<p>where L; does part (2) and S; does part (1) (see Exercise 12.5.1 for a concrete 
example). One refers to L; as the orbital angular momentum, S; as the spin angular 
momentum (or simply spin), and J; as the total angular momentum. We do not yet 
know what J; or S; look like in these general cases, but we do know this: the 1/s 
must obey the same commutation rules as the L/s, for the commutation rules reflect 
the law of combination of rotations and must be obeyed by any triplet of generators 
(the consistency condition), whatever be the nature of wave function they rotate. So 
in general we have 
</p>
<p>JxJ=i1iJ (12.5.16) 
</p>
<p>with Las a special case when the wave function is a scalar. So our result, which 
followed from just the commutation relations, applies to the problem of arbitrary J 
and not just L. Thus the answer to the question raised earlier is that unlike Lz, Jz 
is not restricted to have integral eigenvalues. But our analysis tells us, who know 
very little about spin, that Sz can have only integral or half-integral eigenvalues if 
the commutation relations are to be satisfied. Of course, our analysis doesn't imply 
that there must exist particles with spin integral or half integral-but merely reveals 
the possible variety in wave functions. But the old maxim-if something can happen, 
it will-is true here and nature does provide us with particles that possess spin-i.e., 
particles whose wave functions are more complicated than scalars. We will study 
them in Chapter 14 on spin. 
</p>
<p>Exercise 12.5.1. * Consider a vector field 'l'(x, y) in two dimensions. From Fig. 12.1 it 
follows that under an infinitesimal rotation ezk, 
</p>
<p>325 
</p>
<p>ROTATION 
</p>
<p>INVARIANCE 
</p>
<p>AND ANGULAR 
</p>
<p>MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>326 
</p>
<p>CHAPTER 12 
</p>
<p>Show that (to order c,) 
</p>
<p>so that 
</p>
<p>where 1m is a 2 x 2 identity matrix with respect to the vector components, I 111 is the identity 
</p>
<p>operator with respect to the argument (x, y) of'l'(x, y). This example only illustrates the fact 
</p>
<p>that J, = L, + S= if the wave function is not a scalar. An example of half-integral eigenvalues 
will be provided when we consider spin in a later chapter. (In the present example, S, has 
</p>
<p>eigenvalues &plusmn; n.) 
</p>
<p>Let us return to our main discussion. To emphasize the generality of the results 
</p>
<p>we have found, we will express them in terms of J's rather than L's and also switch 
</p>
<p>to a more common notation. Here is a summary of what we have found. The 
</p>
<p>eigenvectors of the operators 1 2 and J, are given by 
</p>
<p>.1\jm)=j(j+ 1)1i\jm), j = 0, l /2, I, 3/2- ... (l2.5.17a) 
</p>
<p>JJim)=mfiiJm), m = j, j .... I, j .... 2, ... , .... j (l2.5.17b) 
</p>
<p>We shall call} the angular momentum of the state. Note that in the above m can be 
</p>
<p>an integer or half-integer depending on j. 
</p>
<p>The results for the restricted problem ,J = L that we originally set out to solve 
are contained in Eq. ( 12.5.17): we simply ignore the states with half-integral m and 
</p>
<p>j. To remind us in these cases that we are dealing with J = L, we will denote these 
states by 1/m). They obey 
</p>
<p>!=0, L 2, ... (J2.5.18a) 
</p>
<p>L,llm) =mfillm), m = l, l- 1, ... , - l (l2.5.18b) 
</p>
<p>Our problem has not been fully solved: we have only found the eigenvalues-
</p>
<p>the eigenvectors aren't fully determined yet. (As in the oscillator problem, finding 
</p>
<p>the eigenvectors means finding the matrices corresponding to the basic operators 
</p>
<p>whose commutation relations are given.) Let us continue our analysis in terms of 
</p>
<p>the J's. If we rewrite Eq. (12.5.8) in terms of J~J and m (instead of L1c, a, and 
</p>
<p>/3), we get 
</p>
<p>J,IJm) = C&plusmn;(j, m)lj, m&plusmn; I) ( 12.5.19) 
</p>
<p>where Cc1(j, m) are yet to be determined. We will determine them now. </p>
<p/>
</div>
<div class="page"><p/>
<p>If we take the adjoint of 
</p>
<p>l+IJm&gt; = c .. (J, m)IJ, m+ 1 &gt; 
</p>
<p>we get 
</p>
<p>(Jml 1~ = Cf(J, m)&lt;J, m +II 
</p>
<p>Equating the inner" product of the objects on the left-hand side to the product of the 
</p>
<p>objects on the right-hand side, we obtain 
</p>
<p>or 
</p>
<p>ori 
</p>
<p>&lt;Jml l~l+IJm) =I C+(j, m)I\J, m+ 11}, m +I) 
</p>
<p>=I C+(j, m)l 2 
</p>
<p>&lt;Jml 1 2 -1;- FiJ"IJm) = IC+(J, m)l 2 
</p>
<p>IC+(j, m)l 2 =j(j+ l)ti2 -m21i2 -m1(1 
</p>
<p>=li\j-m)(j+m+ 1) 
</p>
<p>C+(j, m) = 1i[ (j --- m )(} + m + 1)] 1 / 2 
</p>
<p>It can likewise be shown that 
</p>
<p>C(j, m) = n[(j + m)(j- m + l )t 2 
</p>
<p>so that finally 
</p>
<p>(12.5.20) 
</p>
<p>Notice that when J&plusmn; act on IJ, &plusmn;j) they kill the state, so that each family with a 
given angular momentum j has only 2} + I states with eigenvalues jn, 
u~ l)n, ... ' -(jn) for J". 
</p>
<p>Equation (12.5.20) brings us to the end of our calculation, for we can write 
</p>
<p>down the matrix elements of lx and ly in this basis: 
</p>
<p>&lt;., 'Ill. &gt; &lt;., '1 1 ++l~l. &gt; 1 m x Jm = J m --~---2- pn 
</p>
<p>x [(j+m)(j-m+ I)] (12.5.2la) 
</p>
<p>t There can be an overall phase factor in front of C., _ We choose it to be unity according to standard 
convention. 
</p>
<p>327 
</p>
<p>ROTATION 
</p>
<p>!NVAR!ANCE 
</p>
<p>AND ANGULAR 
</p>
<p>MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>328 
</p>
<p>CHAPTER 12 
</p>
<p>1+-J 
&lt;J'm'l 1y&middot;lfm) = &lt;J'm'l &middot;-&middot; -IJm) 
</p>
<p>2i 
</p>
<p>x [ (j + m) (j- m + 1) ]l/2} (12.5.2lb) 
</p>
<p>Using these (or our mnemonic based on images) we can write down the matrices 
</p>
<p>corresponding to 1 2 , 1" 1." and 1v in the IJm) basist: 
</p>
<p>~ (0, 0) d.~) d. -4) (I, 1) (1, 0) 
(l,-1) 
</p>
<p>m 
</p>
<p>(0, 0) 0 0 0 0 0 0 
</p>
<p>(~' 1&gt; () i 11~ 0 0 0 0 
12_. (b, -~) 0 0 ~ 1i2 0 0 0 
</p>
<p>(l. 1) 0 0 0 2112 0 0 
</p>
<p>(I, 0) 0 0 0 0 2112 0 
</p>
<p>(l, -1) 0 0 0 0 0 211 2 
</p>
<p>(12.5.22) 
</p>
<p>1c is also diagonal with elements mn. 
</p>
<p>0 0 0 0 0 0 
</p>
<p>0 0 fi/2 0 0 0 
</p>
<p>0 1112 0 0 0 0 
</p>
<p>1x_. 0 0 0 0 nj2u 0 (12.5.23) 
</p>
<p>0 0 0 n/i 2 0 fiji 2 
</p>
<p>0 0 0 0 fiji 2 0 
</p>
<p>0 0 0 0 0 0 
</p>
<p>0 0 -ifi/2 0 0 0 
</p>
<p>0 ili/2 0 0 0 () 
</p>
<p>1,--. 0 0 0 0 -ili/2 12 0 (12.5.24) 
</p>
<p>0 0 0 iii /i /l 0 -ifz/i 2 
</p>
<p>0 0 0 0 ili/21,2 0 
</p>
<p>Notice that although J, and J, are not diagonal in the lim ) basis, they are block 
</p>
<p>diagonal: they have no matrix elements between one value ofj and another. This is 
</p>
<p>t The quantum numbers} and m do not fully label a state; a stale is labeled by lajm). where a represents 
the remaining labels. In what follows, we suppress a but assume it is the same throughout. </p>
<p/>
</div>
<div class="page"><p/>
<p>because J&plusmn; (out of which they are built) do not change j when they act on [jm). 
Since the J's are all block diagonal, the blocks do not mix when we multiply them. 
</p>
<p>In particular when we consider a commutation relation such as [Jx, ly] = tliJ" it will 
be satisfied within each block. If we denote the (2}+ I) x (2}+ 1) block in J;, corre-
</p>
<p>sponding to a certain j, by J/11 , then we have 
</p>
<p>j=O,t 1, ... (12.5.25) 
</p>
<p>Exercise 12.5.2. (!) Verify that the 2 x 2 matrices ;ym, 1}; 1121 , and ;ym obey the com-
mutation rule [}~ 1121 , 1; 1121] = i'liJP!2l. 
</p>
<p>(2) Do the same for the 3 x 3 matrices ;,en. 
(3) Construct the 4 x 4 matrices and verify that 
</p>
<p>Exercise !2.5.3.* (l) Showthat(J,)=(Jy)=Oinastate[jm). 
</p>
<p>(2) Show that in these states 
</p>
<p>(use symmetry arguments to relate ( J};) to ( J:) ). 
</p>
<p>(3) Check that 1'1Jx &middot; 1'1Jy from part (2) satisfies the inequality imposed by the uncertainty 
principle [Eq. (9.2.9)]. 
</p>
<p>(4 1 Show that the uncertainty bound is saturated in the state [j, &plusmn;j). 
</p>
<p>Finite Rotations~ 
</p>
<p>Now that we have explicit matrices for the generators of rotations, lx, ly, and 
Jz, we can construct the matrices representing U[ Rl by exponentiating (- i9 &middot; J I 
1i). But this is easier said than done. The matrices J; are infinite dimensional and 
exponentiating them is not practically possible. But the situation is not as bleak as 
it sounds for the following reason. First note that since J; are block diagonal, so is 
the linear combination 9 &middot; J, and so is its exponential. Consequently, all rotation 
operators U[R] will be represented by block diagonal matrices. The (:q+ I)-dimen-
sional block at a given j is denoted by D(])[R]. The block diagonal form of the 
rotation matrices implies (recall the mnemonic of images) that any vector I VIJ) in 
the subspace W1 spanned by the (Zj + l) vectors IJJ), ... , 1J-j) goes into another 
element I VI}) of W1. Thus to rotate I VI), we just need the matrix DUl. More generally, 
if I VI) has components only in Wo, W,, W2 , &bull;.. , W1, we need just the first (j+ 1) 
matrices D(J). What makes the situation hopeful is that it is possible, in practice, to 
evaluate these if j is small. Let us see why. Consider the series representing nUl: 
</p>
<p>~The material from here to the end of Exercise 12.5.7 may be skimmed over in a less advanced course. 
</p>
<p>329 
</p>
<p>ROTATION 
INVARIANCE 
</p>
<p>AND ANGULAR 
</p>
<p>MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>330 
</p>
<p>CHAPTER 12 
</p>
<p>It can be shown (Exercise 12.5.4) that (fj &middot; J (])r for n&gt; 2} can be written as a linear 
combination of the first 2j powers of {} &middot; J U&gt;. Consequently the series representing 
</p>
<p>DUl may be reduced to 
</p>
<p>2j 
</p>
<p>nu&gt; =I Jn&lt; o)c e. J u&gt;r 
0 
</p>
<p>It is possible, in practice, to find closed expressions for fn( 0) in terms of trigonometric 
</p>
<p>functions, for modest values of j (see Exercise 12.5.5). For example, 
</p>
<p>n&lt; 112 &gt;[R] =cos(~)-~ 0 &middot; J &lt;t/Z&gt; sin(~) 
</p>
<p>Let us return to the subs paces VJ. Since they go into themselves under arbitrary 
</p>
<p>rotations, they are called invariant subspaces. The physics behind the invariance is 
</p>
<p>simple: each subspace contains states of a definite magnitude of angular momentum 
</p>
<p>squared j(j + 1)112, and a rotation cannot change this. Formally it is because 
[f, U[R]]=O and so U[R] cannot change the eigenvalue of I 2&bull; 
</p>
<p>The invariant subspaces have another feature: they are irreducible. This means 
</p>
<p>that V1 itself does not contain invariant subspaces. We prove this by showing that 
</p>
<p>any invariant subspace 0j of v, is as big as the latter. Let I 'I'&gt; be an element of 0,. 
Since we haven't chosen a basis yet, let us choose one such that I 'I') is one of the 
</p>
<p>basis vectors, and furthermore, such that it is the basis vector IJj), up to a normaliza-
</p>
<p>tion factor, which is irrelevant in what follows. (What if we had already chosen a 
</p>
<p>basis IJJ), ... , IJ, -j) generated by the operators I;? Consider any unitary trans-
</p>
<p>formation U which converts Iii&gt; into I ljl) and a different triplet of operators I! 
defined by I!= UI; ut. The primed operators have the same commutation rules and 
hence eigenvalues as the I;. The eigenvectors are just IJm)'= Uljm), with IJJ)'= 
</p>
<p>I ljl). In the following analysis we drop all primes.) 
</p>
<p>Let us apply an infinitesimal rotation 89 to I ljl). This gives 
</p>
<p>I 'I''&gt;= U[R(89)]1JJ&gt; 
=[I- (i/11)(89&middot;J)]IJJ&gt; 
</p>
<p>=[I- U/211)(80+L + oo_I+ + 280JJliJJ&gt; 
</p>
<p>where 
</p>
<p>Since I+IJJ&gt; = 0, IziJj) = Jf11JJ), and I-IJj) = 11(2j)112ij,j- 1 ), we get 
</p>
<p>Since 01 is assumed to be invariant under any rotation, I ljl') also belongs to 01. 
Subtracting (1- ijoOJIJJ&gt;, which also belongs to 0" from I '1''), we find that !J,J -1 &gt; 
also belongs to 01 . By considering more of such rotations, we can easily establish 
that the (2j+ 1) orthonormal vectors, IJJ), IJ,j-1), ... , IJ, -J&gt; all belong to 0;. </p>
<p/>
</div>
<div class="page"><p/>
<p>Thus W1 has the same dimensionality as \11 . Thus V1 has no invariant subspaces. (In 
a technical sense, \11 is its own subspace and is invariant. We are concerned here 
</p>
<p>with subspaces of smaller dimensionality.) 
</p>
<p>The irreducibility of \11 means that we cannot, by a change of basis within W1, 
further block diagonalize all the nul. We show that if this were not true, then a 
contradiction would arise. Let it be possible to block diagonalize all the n&lt;J&gt;, say, as 
follows: 
</p>
<p>+--- 2j + l --+ 
</p>
<p>r 
D&lt;ii[R] 2j+ 1 
</p>
<p>)jm) basis 
</p>
<p>l 
+-dt-+ +- dz--+ 
</p>
<p>t Co d, ! 
new basis i 
</p>
<p>0~ d. l 
(The boxed regions are generally nonzero). It follows that W1 contains two invariant 
</p>
<p>sub spaces of dimensionalities d1 and d2 , respectively. (For example, any vector with 
</p>
<p>just the first d1 components nonzero will get rotated into another such vector. Such 
</p>
<p>vectors form a d1-dimensional subspace.) We have seen this is impossible. 
</p>
<p>The block diagonal matrices representing the rotation operators U[R] are said 
</p>
<p>to provide an irreducible (matrix) representation of these operators. For the set of 
</p>
<p>all rotation operators, the elements of which do not generally commute with each 
</p>
<p>other, this irreducible form is the closest one can come to simultaneous diagonaliza-
</p>
<p>tion. All this is summarized schematically in the sketch below, where the boxed 
regions represent the blocks, n&lt;0 l, D0 l, ... etc. The unboxed regions contain zeros. 
</p>
<p>-E~---
0 
</p>
<p>U [ R] --:--:-+ 
ljm) baois 
</p>
<p>0 
</p>
<p>Consider next the matrix representing a rotationally invariant Hamiltonian in 
this basis. Since [H,J]=O, Hhas the same form as f, which also commutes with 
</p>
<p>331 
</p>
<p>ROTATION 
</p>
<p>INVARIANCE 
</p>
<p>AND ANGULAR 
</p>
<p>MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>332 
</p>
<p>CHAPTER 12 
</p>
<p>all the generators, namely, 
</p>
<p>(1) H is diagonaL since [II, 1 2] = 0, [H, lJ = 0. 
(2) Within each block, II has the same eigenvalue Ej, since [II, .I&plusmn;]= 0. 
</p>
<p>It follows from (2) that is an eigenspace of II with eigenvalue Ei, i.e., all states 
</p>
<p>of a given j are degenerate in a rotationally invariant problem. Although the same 
</p>
<p>result is true classically, the relation between degeneracy and rotational invariance 
</p>
<p>is different in the two cases. Classically, if we are given two states with the same 
</p>
<p>magnitude of angular momentum but different orientation, we argue that they are 
</p>
<p>degenerate because 
</p>
<p>(1) One may be rotated into the other. 
</p>
<p>(2) This rotation does not change the energy. 
</p>
<p>Quantum mechanically, given two elements of it is not always true that they 
</p>
<p>may be rotated into each other (Exercise 12.5.6). However, we argue as follows: 
</p>
<p>(1) One may be reached from the other (in general) by the combined action of J&plusmn; 
</p>
<p>and U[R]. 
</p>
<p>(2) These operators commute with H. 
</p>
<p>In short, rotational invariance is the cause of degeneracy in both cases, but the 
</p>
<p>degenerate states are not always rotated versions of each other in the quantum case 
</p>
<p>(Exercises 12.5.6 and 12.5.7). 
</p>
<p>Exercise 12.5.4. * (I) Argue that the eigenvalues of JY' and are the same as those or 
JY1, namely,jli, (j--l)ti, ... , ( -jti). Generalize the result to O&middot;J 
</p>
<p>(2) Show that 
</p>
<p>(J-Jn){J-(J-IJfiJ[J-(J-2JtiJ&middot; &middot; &middot; (.J +Jn)=O 
</p>
<p>where J = e &middot; J w (Hint: In the case J = J what happens when both sides are applied to an 
arbitrary eigenket l.im)? What about an arbitrary superpositions of such kets?) 
</p>
<p>(3) It follows from (2) that J"i+ 1 is a linear combination of .1&deg;, J 1, &bull; &bull; , f'. Argue that 
</p>
<p>the same goes for 1 21 ' ', k = 1, 2, .... 
</p>
<p>Exercise 12.5.5. (Hard). Using results from the previous exercise and Eq. ( 12.5.23 ), show 
</p>
<p>that 
</p>
<p>(I) D 1121 [R]=exp(-i0&middot;J' 121 /fi)~"cos(IJ/2)! 1 ' 21 -(2i/fi)sin(8/2)0&middot;J 11 21 
</p>
<p>(2) D111 [R] = exp(- ii)j: I) /n) =(cos 8,- l) e: "Y- i sin oJ_~t)) + 1 11 ) 
\ jj , I Ji 
</p>
<p>Exercise 12.5.6. Consider the family of states ijj) .... , !Jm), ... . ij, -j). One refers to 
them as states of the same magnitude but different orientation of angular momentum. If ones 
</p>
<p>takes this remark literally, i.e., in the classical sense, one is led to believe that one may rotate 
</p>
<p>these into each other, as is the case for classical states with these properties. Consider, for </p>
<p/>
</div>
<div class="page"><p/>
<p>instance, the family II, 1 ), II, 0), II, - 1 ). It may seem, for example, that the state with zero 
angular momentum along the z axis, 11,0), may be obtained by rotating 11, 1) by some 
</p>
<p>suitable d n?) angle about the x axis. Using D 0 )[R(OJ)] from part (2) in the last exercise 
show that 
</p>
<p>The error stems from the fact that classical reasoning should be applied to (J), which responds 
</p>
<p>to rotations like an ordinary vector, and not direcly to ljm), which is a vector in Hilbert 
</p>
<p>space. Verify that (J) responds to rotations like its classical counterpart, by showing that 
</p>
<p>(J) in the state D( 1J[R(Bxi)]ll, I) is ti[ --sin tlx.i +cos Bxk]. 
</p>
<p>It is not too hard to see why we can't always satisfy 
</p>
<p>IJm') = D(j)[R]IJm) 
</p>
<p>or more generally, for two normalized kets I vri&gt; and 1'1';), satisfy 
</p>
<p>by any choice of R. These abstract equations imply {2) + 1) linear, complex relations between 
the components of I vr}) and I vr) that can't be satisfied by varying R, which depends on only 
three parameters, Bx, {}nand tic. {Of course one can find a unitary matrix in W; that takes 
</p>
<p>IJm) into IJm') or [ vr,) into I vrj), but it will not be a rotation matrix corresponding to U[R].) 
</p>
<p>Exercise 12.5. 7: Euler Angles. Rather than parametrize an arbitrary rotation by the angle 
</p>
<p>0, which describes a single rotation by () about an axis parallel to 9, we may parametrize it 
</p>
<p>by three angles, y, f3, and a called Euler angles, which define three successive rotations: 
</p>
<p>(I) Construct D(ll[R(a, {3, y)] explicitly as a product of three 3 x 3 matrices. (Use the 
</p>
<p>result from Exercise 12.5.5 with Jx-&gt;J, .. ) 
</p>
<p>(2) Let it act on I I, I) and show that (J) in the resulting state is 
</p>
<p>(J) = ti(sin f3 cos cti +sin fJ sin aj +cos ,Bk) 
</p>
<p>(3) Show that for no value of a, f3, and y can one rotate 11, I) in to just 11, 0). 
( 4) Show that one can always rotate any II, m) into a linear combination that involves 
</p>
<p>ll,m'), i.e., 
</p>
<p>(I, m'ID( 1l[R(a, ,B, y)JII, m) 'i"'O 
</p>
<p>for some a, {3, y and any m, m'. 
</p>
<p>(5) To see that one can occasionally rotate IJm&gt; into l.im'), verify that a 180" rotation 
about the y axis applied to II, I) turns it in to II, - 1 &gt;. 
</p>
<p>Angular Momentum Eigenfunctions in the Coordinate Basis 
</p>
<p>We now turn to step (3) outlined at the beginning of this section, namely, the 
</p>
<p>construction of the eigenfunctions of L 2 and L, in the coordinate basis, given the 
</p>
<p>information on the kets 1/m). 
</p>
<p>333 
</p>
<p>ROTATION 
</p>
<p>INVARIANCE 
</p>
<p>AND ANGULAR 
</p>
<p>MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>334 
</p>
<p>CHAPTER l2 
</p>
<p>Consider the states corresponding to a given l. The "topmost" state Ill) satisfies 
</p>
<p>L+llf)=O (12.5.26) 
</p>
<p>If we write the operator L". = L, &plusmn; iL, in spherical coordinates we find 
</p>
<p>( 12.5.27) 
</p>
<p>Exercise 12.5.8 (Optional). Verify that 
</p>
<p>I, r c &middot;) r, ------+ if? . &middot;&middot;&middot;&middot;&middot; cos &lt;/J - + sin &cent; cot () -.. 
\ (1() (1&lt;/J 
</p>
<p>If we denote by l.fli(r, 0, cp) the eigenfunction corresponding to Ill). we find that it 
satisfies 
</p>
<p>Since lfl; is an eigenfunction of Lo with eigenvalue lfi, we let 
</p>
<p>and find that 
</p>
<p>or 
</p>
<p>(' 0 . ') &middot;I :-!cot 0 Ut=O 
.cO ; 
</p>
<p>dU~ d(sin 8) -&middot;- = t --&middot;- -&middot; 
u; sin (j 
</p>
<p>U~(r, 8) = R(r)(sin 0)1 
</p>
<p>(12.5.28) 
</p>
<p>(12.5.29) 
</p>
<p>( 12.5.30) 
</p>
<p>( 12.5.31) 
</p>
<p>where R(r) is an arbitrary (normalizable) function of r. When we address the eigen-
value problem of rotationally invariant Hamiltonians, we will see that 11 will nail 
</p>
<p>down R if we seek simultaneous eigenfunctions of H, L 2 , and L.,. But first let us 
</p>
<p>introduce, as we did in the study of L., in two dimensions, the function that would </p>
<p/>
</div>
<div class="page"><p/>
<p>have been the unique, nondegenerate solution in the absence ofthe radial coordinate: 
</p>
<p>[ J
-1/2 
</p>
<p>I I (2/+l)! l &middot; I il&cent; 
Y1(8 ;j&gt;)=(-1) --- --(smO) e 
</p>
<p>' 4rr il! 
(12.5.32) 
</p>
<p>Whereas the phase factor (- 1 )1 reflects our convention, the others ensure that 
</p>
<p>f f l f2" I Yil 2 dO. = I Yll 2 d( cos 0) d;j&gt; = 1 
-1 () 
</p>
<p>( 12.5.33) 
</p>
<p>We may obtain Y!- 1 by using the lowering operator. Since 
</p>
<p>Llll) = fi[(l+/)(1)] l, l-1) = fi(2l) 112 ll, /-1) 
</p>
<p>1 c - 1) [ _ _ ( a a )J Y!- 1(8, ;j&gt;)=~- 1 - 1 , -- fie&middot;-ul&gt; --;::--icot 0--;::- Y/ 
(2/) &bull; fi __ oO o;j&gt; _ 
</p>
<p>(12.5.34) 
</p>
<p>We can keep going in this manner until we reach Y1- 1 . The result is, form?:. 0, 
</p>
<p>ym( e, ;j&gt;) = ( -1)1[(2!+ 1) !]1.2 _1_ [-_(l+m) !_Jl/2 eim&lt;/&gt;(sin (;})-m 
' - 4n in (21)!(1-m) 1 
</p>
<p>di-m _ 
</p>
<p>x ----(sin (})21 
d(cos (;})t-rn 
</p>
<p>(12.5.35) 
</p>
<p>Form&lt; 0, see Eq. (12.5.40). These functions are called spherical harmonics and 
</p>
<p>satisfy the orthonormality condition 
</p>
<p>Another route to the Y/ is the direct solution of the L 2, L= eigenvalue problem 
</p>
<p>in the coordinate basis where 
</p>
<p>and of course 
</p>
<p>a 
L=---&gt; ---- ifi --
</p>
<p>3&cent;; 
</p>
<p>(12.5.36) 
</p>
<p>If we seek common eigenfunctions of the formt f( 8) im&lt;~&gt;, which are regular between 
</p>
<p>8 = 0 and rr, we will find that L 2 has eigenvalues of the form l (I+ I )fi2 , I= 0, I, 2, ... , 
</p>
<p>t We neglect the function R( r) that can tag along as a spectator. 
</p>
<p>335 
</p>
<p>ROTATION 
</p>
<p>lNVAR!ANCE 
</p>
<p>AND ANGULAR 
</p>
<p>MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>336 
</p>
<p>CHAPTER l2 
</p>
<p>where tc_ I mi. The Y!" functions are mutually orthogonal because they are nondegen-
erate eigenfunctions of I} and L=, which are Hermitian on single-valued functions 
of() and &cent;. 
</p>
<p>Exercise I 2.5. 9. Show that C above is Hermitian in the sense 
</p>
<p>The same goes for L, which is insensitive to e and is Hermitian with respect to the &cent; 
integration. 
</p>
<p>We may expand any IJ!(r, 0, &cent;) in terms of Y/"(0, &cent;) using r-dependent 
</p>
<p>coefficients [consult Eq. (10.1.20) for a similar expansion]: 
</p>
<p>IJI(r,O,&cent;)=I I C/"(r)Y/"(0,&cent;) ( 12.5.37a) 
1=0 m= -I 
</p>
<p>where 
</p>
<p>C/"(r)= f Y((B, &cent;)1J!(r, 8, &cent;) dr! ( 12.5.37b) 
</p>
<p>If we compute ( IJ!I eiiJ!) and interpret the result as a weighted average, we can 
readily see (assuming IJ! is normalized to unity) that 
</p>
<p>(' ., 
</p>
<p>P(L2 =1U+I)n2,Lc=m1i)= J .IC/"(r)l 2r 2 dr 
0 
</p>
<p>( 12.5.38) 
</p>
<p>It is clear from the above that C/" is the amplitude to find the particle at a radial 
distance r with angular momentum (!, m).% The expansion Eq. ( 12.5.37a) tells us 
</p>
<p>how to rotate any ljl(r, (), &cent;) by an angle e (in principle): 
(l) We construct the block diagonal matrices, exp(--i0&middot;L111 jti). 
</p>
<p>(2) Each block will rotate the C/" into linear combination of each other, i.e., 
under the action of U [ R], the coefficients C;n( r ), m = l, !- l, ... , -!; will get mixed 
</p>
<p>with each other by D~!"'. 
</p>
<p>In practice, one can explicitly carry out these steps only if IJ! contains only 
Y/"s with small l. A concrete example will be provided in one of the exercises. 
</p>
<p>t Note that r is just the eigenvalue of the operator (X'+ Y'+Z 2 ) 1'' which commutes with L' and L </p>
<p/>
</div>
<div class="page"><p/>
<p>Here are the first few Y/ functions: 
</p>
<p>Note that 
</p>
<p>Y8=(47r)-I/2 
</p>
<p>Yt = =F (3/87r) 112 sin(} e&plusmn;14&gt; 
Y?= (3/47r) 112 cos(} 
</p>
<p>Yi 2 = ( 15 /327r) 112 sin2 (} e&plusmn;2 ;q, 
</p>
<p>Yi 1 = =F (15/87r) 112 sin(} cos(} e&plusmn;14&gt; 
</p>
<p>yg = (5/l67r) 112(3 cos2 (} -1) 
</p>
<p>(12.5.39) 
</p>
<p>(12.5.40) 
</p>
<p>Closely related to the spherical harmonics are the associated Legendre polynomials 
Pi (with 05,m5,l) defined by 
</p>
<p>(12.5.41) 
</p>
<p>If m=O, P?(cos O)=P1(cos (})is called a Legendre polynomial. 
The Shape of the lim Functions. For large 1, the functions I Y;"l exhibit many 
</p>
<p>classical features. For example, I Y/1 ex: I sin1 01, is almost entirely confined to the x-y 
plane, as one would expect of a classical particle with all its angular momentum 
pointing along the z axis. Likewise, 1111 is, for large 1, almost entirely confined to 
the z axis. Polar plots of these functions may be found in many textbooks. 
</p>
<p>Exercise 12.5.10. Write the differential equation corresponding to 
</p>
<p>in the coordinate basis, using the L 2 operator given in Eq. (12.5.36). We already know fJ = 
mfi from the analysis of -ifi(o/oc/J ). So assume that the simultaneous eigenfunctions have 
the form 
</p>
<p>and show that P';; satisfies the equation 
</p>
<p>( I iJ . iJ a m
2 
</p>
<p>) --- sm () -+---- ?::(0)=0 
sin () i)() i)() 1i2 sin2 () a 
</p>
<p>337 
</p>
<p>ROTATION 
</p>
<p>INVARIANCE 
AND ANGULAR 
</p>
<p>MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>338 
</p>
<p>CHAPTER 12 
</p>
<p>We need to show that 
</p>
<p>a 
(I) ii-=1(1+ 1), 1=0, I, 2,, ... 
</p>
<p>(2) lml ~I 
</p>
<p>We will consider only part (I) and that too for the case m = 0. By rewriting the equation in 
terms of u=cos e, show that p~ satisfies 
</p>
<p>2 d 2 P~ dP~ (a ) 0 (J-u)-&middot;--2u-+--:- P =0 
du2 du 1f " 
</p>
<p>Convince yourself that a power series solution 
</p>
<p>'L 
</p>
<p>P~= I Cu'' 
n=O 
</p>
<p>will lead to a two-term recursion relation. Show that ( C + 2 / C,) -&gt;1 as n-.cXJ. Thus the series 
diverges when Jul -.1 (0-&gt;0 or rr). Show that if a/1'12 =(/)(l+ 1); 1=0, I, 2, ... , the series will 
terminate and be either an even or odd function of u. The functions 
</p>
<p>P~(u)=P~t+l)A'(u)=P?(u)=P 1 (u) are just the Legendre polynomials up to a scale factor. 
Determine Po, P1, and P2 and compare (ignoring overall scales) with the Y? functions. 
</p>
<p>Exercise 12.5.11. Derive Y11 starting from Eq.(l2.5.28) and normalize it yourself. 
[Remember the (---1)1 factor from Eq. (12.5.32).] Lower it to get Y? and Y! 1 and compare it 
with Eq. (12.5.39). 
</p>
<p>Exercise 12.5.12. * Since U and Lz commute with n. they should share a basis with it. 
Verify that under parity Yr' __,. ( -1)1Yr (First show that () _,. 1r - 8, &cent; ...... &cent; + 1T under 
parity. Prove the result for Y/- Verify that L ___ does not alter the parity, thereby proving the 
</p>
<p>result for all Yt) 
</p>
<p>Exercise 12.5.13. * Consider a particle in a state described by 
</p>
<p>where N is a normalization factor. 
(I) Show, by rewriting the YfLo functions in terms of x, y, z, and r, that 
</p>
<p>( 12.5.42) 
</p>
<p>Y'i=(&middot;~r2: 
4n, r 
</p>
<p>(2) Using this result, show that for a particle described by 'If above, P(lz = 0) = 2/3; 
P(lz = +1'1) = 1/6= P(lz = -Fi). </p>
<p/>
</div>
<div class="page"><p/>
<p>Exercise 12.5.14. Consider a rotation llxi. Under this 
</p>
<p>Therefore we must have 
</p>
<p>X-+X 
</p>
<p>y-+y cos llx- z sin llx 
</p>
<p>Z-+Z COS liz+ y sin llx 
</p>
<p>llt(X y z) lltR= llt(X, y COS llx- Z sin llx, Z COS llx+y sin llx) 
T ' ' U[R(O,I)] T T 
</p>
<p>Let us verify this prediction for a special case 
</p>
<p>which must go into 
</p>
<p>(1) Expand 'I' in terms of Y/, Y?, Yl 1 &bull; 
(2) Use the matrix e-&bull;o,L,/R to find the fate of 'I' under this rotation.t Check your result 
</p>
<p>against that anticipated above. [Hint: (1) 'I' -Y?, which corresponds to 
</p>
<p>[!] 
</p>
<p>(2) Use Fq_. (12.5.42).] 
</p>
<p>12.6. Solution of Rotationally Invariant Problems 
</p>
<p>We now consider a class of problems of great practical interest: problems where 
V(r, 0, r/)) = V(r). The Schrodinger equation in spherical coordinates becomes 
</p>
<p>[ -li
2 (_!_ i r 2 ~-+- 1 - _&pound;_sin (J _&pound;_+ 1 ~)+ V(r)] 
</p>
<p>2JJ r or or r sin (J o(J ofJ r sin2 (J or/)2 
X 1f!E{r, fJ, r/)) =Eif/E(r, fJ, r/)) (12.6.1) 
</p>
<p>Since [H, L] = 0 for a spherically symmetric potential, we seek simultaneous eigen-
functions of H, L 2, and Lz: 
</p>
<p>(12.6.2) 
</p>
<p>Feeding in this form, and bearing in mind that the angular part of V2 is just the L 2 
</p>
<p>operator in the coordinate basis [up to a factor ( -1i2r)- 1, see Eq. (12.5.36)], we get 
</p>
<p>t See Exercise 12.5.5. 
</p>
<p>339 
</p>
<p>ROTATION 
INVARIANCE 
</p>
<p>AND ANGULAR 
MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>340 
</p>
<p>CHAPTER 12 
</p>
<p>the radial equation 
</p>
<p>{ 1i
2 
</p>
<p>[ 1 0 2 0 l (l + 1 &gt;] } -- 2-r --::;-----2 - +V(r) REt=EREt 
2}1 r or or r 
</p>
<p>(12.6.3) 
</p>
<p>Notice that the subscript m has been dropped: neither the energy nor the radial 
</p>
<p>function depends on it. We find, as anticipated earlier, the (21+ I )-fold degeneracy 
</p>
<p>of H. 
</p>
<p>Exercise 12.6.1. * A particle is described by the wave function 
</p>
<p>(a0 =canst) 
</p>
<p>( 1) What is the angular momentum content of the state? 
</p>
<p>(2) Assuming IJI E is an eigenstate in a potential that vanishes as r---&gt;oo, find E. (Match 
</p>
<p>leading terms in Schrodinger's equation.) 
</p>
<p>(3) Having found E. consider finite rand find V(r). 
</p>
<p>At this point it becomes fruitful to introduce an auxiliary function UE1 defined 
</p>
<p>as follows: 
</p>
<p>( 12.6.4) 
</p>
<p>and which obeys the equation 
</p>
<p>( 12.6.5) 
</p>
<p>Exercise 12.6.2. * Provide the steps connecting Eq. (12.6.3) and Eq. (12.6.5). 
</p>
<p>The equation is the same as the one-dimensional Schrodinger equation except 
</p>
<p>for the following differences: 
</p>
<p>(1) The independent variable (r) goes from 0 to oo and not from -x to oo. 
</p>
<p>(2) In addition to the actual potential V(r), there is the repulsive centrifugal 
</p>
<p>barrier, l(l+ l)'li2/2Jlf2, in all but the 1=0 states. 
</p>
<p>(3) The boundary conditions on U are different from the one-dimensional case. 
</p>
<p>We find these by rewriting Eq. (12.6.5) as an eigenvalue equation 
</p>
<p>(12.6.6) </p>
<p/>
</div>
<div class="page"><p/>
<p>and demanding that the functions UEI be such that D1 is Hermitian with respect to 
them. In other words, if U1 and U2 are two such functions, then we demand that 
</p>
<p>This reduces to the requirement 
</p>
<p>( Ut* dUz_ Uz dU()Ioo =0 
dr dr o 
</p>
<p>(12.6.7b) 
</p>
<p>&bull; 
Exercise 12.6.3. Show that Eq. (12.6.7b) follows from Eq. (12.6.7a). 
</p>
<p>Now, a necessary condition for 
</p>
<p>to be normalizable to unity or the Dirac delta function is that 
</p>
<p>(12.6.8a) 
</p>
<p>or 
</p>
<p>(12.6.8b) 
</p>
<p>the first corresponding to bound states and the second to unbound states. In either 
case, the expression in the brackets in Eq. (l2.6.7b) vanishes at the upper limitt and 
the Hermiticity of D1 hinges on whether or not 
</p>
<p>[ Ut dUz_ Uz dUt] =O 
dr dr 0 
</p>
<p>(12.6.9) 
</p>
<p>Now this condition is satisfied if 
</p>
<p>c=const (12.6.10) 
</p>
<p>+ For the oscillating case, we must use the limiting scheme described in Section 1.1 0. 
</p>
<p>341 
</p>
<p>ROTATION 
INVARIANCE 
</p>
<p>AND ANGULAR 
MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>342 
</p>
<p>CHAPTER 12 
</p>
<p>If c is nonzero, then 
</p>
<p>v c 
R~---
</p>
<p>r r 
</p>
<p>diverges at the origin. This in itself is not a disqualification, for R is still square 
</p>
<p>integrable. The problem with c;ioO is that the corresponding total wave functiont 
</p>
<p>c 0 
IJI~- Yo 
</p>
<p>r 
</p>
<p>does not satisfy Schrodinger's equation at the origin. This is because of the relation 
</p>
<p>(12.6.11) 
</p>
<p>the proof of which is taken up in Exercise 12.6.4. Thus unless V(r) contains a delta 
</p>
<p>function at the origin (which we assume it does not) the choice c;ioO is untenable. 
</p>
<p>Thus we deduce that 
</p>
<p>Exercise 12.6.4. * (!) Show that 
</p>
<p>1 
83(r-r') =8(x- x')8(y- y')8(z-z') =-2-.- 8(r- r')8(0- 0')8(&cent;- &cent; ') 
</p>
<p>r sm 0 
</p>
<p>(consider a test function). 
</p>
<p>(2) Show that 
</p>
<p>(12.6.12) 
</p>
<p>(Hint: First show that V2(1/r)=O if r#O. To see what happens at r=O, consider a small 
</p>
<p>sphere centered at the origin and use Gauss's law and the identity V2&cent;=V&middot;V&cent;).&sect; 
</p>
<p>General Properties of U Et 
</p>
<p>We have already discussed some of the properties of VEt as r-+0 or oo. We shall 
</p>
<p>try to extract further information on VEt by analyzing the equation governing it in 
</p>
<p>these limits, without making detailed assumptions about V(r). Consider first the limit 
</p>
<p>r-+0. Assuming V(r) is less singular than r-2, the equation is dominated by the 
</p>
<p>t As we will see in a moment, 1#0 is incompatible with the requirement that lf/(r)-+r- 1 as r-+0. Thus 
</p>
<p>the angular part of If/ has to be Y1;= (4nT 112&bull; 
</p>
<p>&sect;Or compare this equation to Poisson's equation in electrostatics V2&cent;=-47rp. Here p=o3(r), which 
</p>
<p>represents a unit point charge at the origin. In this case we know from Coulomb's law that&cent;= 1/r. </p>
<p/>
</div>
<div class="page"><p/>
<p>centrifugal barrier: 
</p>
<p>u;~l(l~l)u, 
r 
</p>
<p>(12.6.13) 
</p>
<p>We have dropped the subscript E, since E becomes inconsequential in this limit. If 
</p>
<p>we try a solution of the form 
</p>
<p>we find 
</p>
<p>or 
</p>
<p>and 
</p>
<p>a(a-1)=/(l+I) 
</p>
<p>a=l+l or(-!) 
</p>
<p>(regular) 
</p>
<p>(irregular) 
(12.6.14) 
</p>
<p>We reject the irregular solution since it does not meet the boundary condition U(O) = 
</p>
<p>0. The behavior of the regular solutions near the origin is in accord with our expecta-
</p>
<p>tion that as the angular momentum increases the particle should avoid the origin 
more and more. 
</p>
<p>The above arguments are clearly true only if l of 0. If l = 0, the centrifugal barrier 
is absent, and the answer may be sensitive to the potential. In the problems we will 
</p>
<p>consider, U 1 ~o will also behave as r'+' with !=0. Although U0 (r)&middot;&middot;&middot;+0 as r-&middot;&bull;0, note 
that a particle in the l = 0 state has a nonzero amplitude to be at the origin, since 
R0 (r)= U0(r)/r&yen;0 at r=O. 
</p>
<p>Consider now the behavior of UE1 as r-+oo. If V(r) does not vanish as r-+w, 
it will dominate the result (as in the case of the isotropic oscillator, for which 
</p>
<p>V(r)cx:r2) and we cannot say anything in general. So let us consider the case where 
rV(r)-+0 as r-+oo. At larger the equation becomes 
</p>
<p>( l2.6.l5) 
</p>
<p>(We have dropped the subscript l since the answer doesn't depend on /.) There are 
now two cases: 
</p>
<p>1. &pound;&gt;0: the particle is allowed to escape to infinity classically. We expect UE to 
oscillate as r-.oo. 
</p>
<p>2. E &lt; 0: The particle is bound. The region r-+ ex: is classically forbidden and we 
expect UE to fall exponentially there. 
</p>
<p>343 
</p>
<p>ROTATION 
</p>
<p>lNVARIANCE 
</p>
<p>AND ANGULAR 
</p>
<p>MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>344 
</p>
<p>CHAPTER 12 
</p>
<p>Consider the first case. The solutions to Eq. (12.6.15) are of the form 
</p>
<p>that is to say, the particle behaves as a free particle far from the origin.t Now, you 
</p>
<p>might wonder why we demanded that rV(r)-+0 and not simply V(r)----&bull;0 as r----&bull;cc. 
</p>
<p>To answer this question. let us write 
</p>
<p>UE= f(r) e&plusmn;ikr 
</p>
<p>and see if f(r) tends to a constant as r-&gt;:~J. Feeding in this form of UE into Eq. 
(12.6.5) we find (ignoring the centrifugal barrier) 
</p>
<p>2pV(r) 
&plusmn; (2ik)f' -----~ 2 _f=O 
</p>
<p>Since we expectf(r) to be slowly varying as r--.:x, we ignore/"' and find 
</p>
<p>c((~iJ-l 
- = ..,. - -----;; V( r) dr 
f k fi&middot; 
</p>
<p>f(r)=f(ro) &middot; exp=f l~~ 2 I' V(r') dr'J 
,.,; f\1 -
</p>
<p>(12.6.16) 
</p>
<p>where r0 is some constant. [f V( r) falls faster than r 1, i.e .. r V(r) -&gt;0 as r--&gt;&lt;JJ, we 
</p>
<p>can take the limit as r-&gt;ex; in the integral andf(r) approaches a constant as r-&gt;CJJ. 
</p>
<p>If instead 
</p>
<p>e&middot; 
V(r)=-
</p>
<p>r 
</p>
<p>as in the Coulomb problem,&sect; then 
</p>
<p>- - lipe" (',-\1 j(r)=j(r0 ) exp&plusmn; 2 1n :_) 
kn }o 
</p>
<p>and 
</p>
<p>(12.6.17) 
</p>
<p>This means that no matter how far away the particle is from the origin. it is never 
completely free of the Coulomb potential. If V(r) falls even slower than a Coulomb 
</p>
<p>potential, this problem only gets worse. 
</p>
<p>t Although A and B are arbitrary in this asymptotic form, their rat.io is det.ermined by the requirement 
that if U1_ is continued inward to r "' 0, it must vanish. That there is just one free parameter in the 
</p>
<p>solution (the overall scale), and not two, is because D 1 is nondegenerate even for E&gt;O, which in turn 
</p>
<p>is due to the constraint 1/n(r=O)=O; see Exercise 12.6.5. 
</p>
<p>&sect;We are considering the case of equal and opposite charges with an eye on the next chapter. </p>
<p/>
</div>
<div class="page"><p/>
<p>Consider now the case E&lt;O. All the results from the E&gt;O case carry over with 
</p>
<p>the change 
</p>
<p>Thus 
</p>
<p>UE-&middot;-&gt; A e-&bull;-r + B e+.:r ( 12.6.18) 
y...__..ry:. 
</p>
<p>Again B I A is not arbitrary if we demand that U E continued inward vanish at r = 0. 
Now, the growing exponential is disallowed. For arbitrary E&lt;O, both e'" and e-"' 
</p>
<p>will be present in U E. Only for certain discrete values of E will the e"'' piece be 
</p>
<p>absent; these will be the allowed bound state levels. (If A/ B were arbitrary, we could 
choose B = 0 and get a normalizable bound state for every E &lt; 0.) 
</p>
<p>As before, Eq. (12.6.18) is true only if rV(r)-&gt;0. In the Coulomb case we expect 
</p>
<p>[from Eq. (12.6.17) with k-&gt;iK] 
</p>
<p>(12.6.19) 
</p>
<p>When we solve the problem of the hydrogen atom, we will find that this is indeed 
</p>
<p>the case. 
When E &lt; 0, the energy eigenfunctions are nonnalizable to unity. As the operator 
</p>
<p>D1(r) is nondegenerate (Exercise 12.6.5), we have 
</p>
<p>and 
</p>
<p>lJ!Etm(r, 0, cp)=REt(r)Y/'(0, &cent;) 
</p>
<p>obeys 
</p>
<p>We will consider the case E&gt; 0 in a moment. 
</p>
<p>Exercise 12.6.5. Show that D1 is nondegenerate in the space of functions U that vanish 
</p>
<p>as r-&gt;0. (Recall the proof of Theorem 15, Section 5.6.) Note that U,A is nondegenerate even 
forE&gt; 0. This means that E, l, and m, label a state fully in three dimensions. 
</p>
<p>345 
</p>
<p>ROTATION 
</p>
<p>INVARIANCE 
</p>
<p>AND ANGULAR 
</p>
<p>MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>346 
</p>
<p>CHAPTER 12 
</p>
<p>The Free Particle in Spherical Coordinates~ 
</p>
<p>lf we begin as usual with 
</p>
<p>lfiEtm(r, 0, cfJ)=REt(r)Y/'(8, &cent;) 
</p>
<p>and switch to U&pound;1, we end up with 
</p>
<p>2 2pE k =&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot; 
rz2 
</p>
<p>Dividing both sides by k 2, and changing to p = kr, we obtain 
</p>
<p>[ 
d. 2 '. /(l+ l)l 
</p>
<p>-~,~--- U1=U1 
dp" p2 . 
</p>
<p>( 12.6.20) 
</p>
<p>The variable k, which has disappeared, will reappear when we rewrite the answer in 
</p>
<p>terms of r = pjk. This problem looks a lot like the harmonic oscillator except for 
</p>
<p>the fact that we have a potential 1/ p2 instead of p 2 . So we define operators analogous 
</p>
<p>to the raising and lowering operators. These are 
</p>
<p>and its adjoint 
</p>
<p>d l+ 1 
d,=&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;+&middot;&middot;&middot;&middot; 
</p>
<p>dp p 
</p>
<p>+ d l+ 1 
dt = --+--
</p>
<p>dp p 
</p>
<p>(l2.6.21a) 
</p>
<p>(12.6.2lb) 
</p>
<p>(Note that d/dp is anti-Hermitian.) In terms of these, Eq. (12.6.20) becomes 
</p>
<p>( 12.6.22) 
</p>
<p>Now we premultiply both sides by dj to get 
</p>
<p>( 12.6.23) 
</p>
<p>You may verify that 
</p>
<p>( 12.6.24) 
</p>
<p>so that 
</p>
<p>( 12.6.25) 
</p>
<p>! The present analysis is a simplified version of the work of L Infeld. Phys. Rev .. 59. 737 ( 1941 ). </p>
<p/>
</div>
<div class="page"><p/>
<p>It follows that 
</p>
<p>(12.6.26) 
</p>
<p>where c1 is a constant. We choose it to be unity, for it can always be absorbed in 
the normalization. We see that di serves as a "raising operator" in the index /. Given 
U0 , we can find the others.t From Eq. (12.6.20) it is clear that if /=0 there are two 
independent solutions: 
</p>
<p>U~(p)=sinp, (12.6.27) 
</p>
<p>The constants in front are chosen according to a popular convention. Now ug is 
unacceptable at p=O since it violates Eq. (12.6.12). If, however, one is considering 
the equation in a region that excludes the origin, ug must be included. Consider 
now the tower of solutions built out of U~ and ug. Let us begin with the equation 
</p>
<p>(12.6.28) 
</p>
<p>Now, we are really interested in the functions R1= Uti p.&sect; These obey (from the 
above) 
</p>
<p>=/(-!!_)Rt 
dp p' 
</p>
<p>or 
</p>
<p>=(-~ ~)/+! Ro 
pdp Po 
</p>
<p>tIn Chapter 15, we will gain some insight into the origin of such a ladder of solutions. 
&sect;Actually we want R1= Ut/r=kUt/p. But the factor k may be absorbed in the normalization factors of 
</p>
<p>Uand R. 
</p>
<p>347 
</p>
<p>ROTATION 
</p>
<p>INVARIANCE 
AND ANGULAR 
</p>
<p>MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>348 
</p>
<p>CHAPTER 12 
</p>
<p>so that finally we have 
</p>
<p>( ~)I I I c Rl=(-p) -- Ro 
Pop 
</p>
<p>(12.6.29) 
</p>
<p>Now there are two possibilities for Ro: 
</p>
<p>These generate the functions 
</p>
<p>A . I I Slll p 
( 
</p>
<p>d 
)I( . ) R1 =11=(-p) pdp p (12.6.30a) 
</p>
<p>called the spherical Bessel functions of order 1, and 
</p>
<p>s 1 I d -cos p ( )I( ) R1 =n1=(-p) pdp -p- (12.6.30b) 
</p>
<p>called spherical Neumann functions of order /J Here are a few of these functions: 
</p>
<p>. sinp 
Jo(p)=-, 
</p>
<p>p 
</p>
<p>. ( ) sinp cosp 
}I p =-----, 
</p>
<p>p2 p 
</p>
<p>. ( 3 I ) . 3 cos p ]2(p) = --- Slll p---, 
p3 p p2 
</p>
<p>As p-+oo, these functions behave as 
</p>
<p>-cosp 
no(p)=--
</p>
<p>P 
</p>
<p>( 3 ~') 3 sin p nz(p)=- --- cosp---
p3 p p2 
</p>
<p>I . ( ltr) }1---+-sm p--
p~oo p 2 
</p>
<p>n,---+-_!_cos(p- 17r) 
p~oo p 2 
</p>
<p>(12.6.31) 
</p>
<p>( 12.6.32) 
</p>
<p>(12.6.32) 
</p>
<p>Despite the apparent singularities as p-+0, the j,(l) functions are finite and in fact 
</p>
<p>PI 
</p>
<p>j,(p)--;:;: (21+ 1)!! 
(12.6.33) 
</p>
<p>t One also encounters spherical Hankel functions h1 = }1 + in1 in some problems. </p>
<p/>
</div>
<div class="page"><p/>
<p>where (2/+ I)!!= (21+ 1)(2/-1)(2/- 3) ... (5)(3)(1). These are just the regular solu-
tions listed in Eq. (12.6.14). The Neumann functions, on the other hand, are singular 
</p>
<p>( ) (2/-1)!! n, p --+-
p-o pl+l 
</p>
<p>(12.6.34) 
</p>
<p>and correspond to the irregular solutions listed in Eq. (12.6.14). 
Free-particle solutions that are regular in all space are then 
</p>
<p>lf/Eim(r, 0, cp)=j,(kr)Yi(O, cp), (12.6.35) 
</p>
<p>These satisfy 
</p>
<p>(12.6.36) 
</p>
<p>We are using here the fact that 
</p>
<p>(12.6.37) 
</p>
<p>Exercise 12.6.6. * (1) Verify that Eqs. (12.6.21) and (12.6.22) are equivalent to Eq. 
(12.6.20) 
</p>
<p>(2) Verify Eq. (12.6.24). 
</p>
<p>Exercise 12.6.7. Verify thatj0 andjt have the limits given by Eq. (12.6.33). 
</p>
<p>Exercise 12.6.8. * Find the energy levels of a particle in a spherical box of radius r0 in 
the I= 0 sector. 
</p>
<p>Exercise 12.6.9. * Show that the quantization condition for /=0 bound states in a spher-
ical well of depth - V0 and radius r0 is 
</p>
<p>k' jrc =-tan k'r0 
</p>
<p>where k' is the wave number inside the well and ire is the complex wave number for the 
exponential tail outside. Show that there are no bound states for V0 &lt; n 21i2 /8w5. (Recall 
Exercise 5.2.6.) 
</p>
<p>Connection with the Solution in Cartesian Coordinates 
</p>
<p>If we had attacked the free-particle problem in Cartesian coordinates, we would 
have readily obtained 
</p>
<p>I . &middot;~ 
Ill (x y z)- e&bull;p&middot;r,n 
'~' E ' ' - (2n1i)3/2 , (12.6.38) 
</p>
<p>349 
</p>
<p>ROTATION 
INVARIANCE 
</p>
<p>AND ANGULAR 
MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>350 
</p>
<p>CHAPTER 12 
</p>
<p>Consider now the case which corresponds to a particle moving along the z axis with 
</p>
<p>momentum p. As 
</p>
<p>p&middot;r/1i = (pr cos 0)1i =kr cos() 
</p>
<p>we get 
</p>
<p>eikrcos(J 
</p>
<p>lJIE{r, (), f/&gt;) = (27r1i)3/Z' (12.6.39) 
</p>
<p>It should be possible to express this solution, describing a particle moving in the z 
</p>
<p>direction with energy E = 1i2e /2J1., as a linear combination of the functions lJI Elm 
which have the same energy, or equivalently, the same k: 
</p>
<p>XC / 
</p>
<p>eikrcosll= I I C{j,(kr)Y/'(0, f/&gt;) (12.6.40) 
t~o m~ -I 
</p>
<p>Now, only terms with m = 0 are relevant since the left-hand side is independent of 
</p>
<p>f/&gt;. Physically this means that a particle moving along the z axis has no angular 
</p>
<p>momentum in that direction. Since we have 
</p>
<p>0 (2/+ 1)1/2 
Yt(O)= 4n Pt(Cos ()) 
</p>
<p>% 
</p>
<p>cikrcosO= I Cdt(kr)Pt(COS ()), 
t~o 
</p>
<p>It can be show that 
</p>
<p>so that 
</p>
<p>Ct= i1 (21+ 1) 
</p>
<p>X 
</p>
<p>eikrcosll= I i/(2/+ l)jt(kr)Pt(COS ()) 
t~o 
</p>
<p>(12.6.41) 
</p>
<p>This relation will come in handy when we study scattering. This concludes our study 
</p>
<p>of the free particle. 
</p>
<p>Exercise 12.6.10. (Optional). Verify Eq. (12.6.41) given that 
</p>
<p>(I) r P,(cos {})Pr(cos tl) d(cos {})=[2/(2/+ 1)]8/1' 
-I 
</p>
<p>1 d 1(x2 -1)1 
</p>
<p>(2) P,(x) = 211! dxt 
</p>
<p>fl 2m d (2m)!! (3) o (1-x) x=(2m+1)!! 
Hint: Consider the limit kr---&gt;0 after projecting out C1&bull; </p>
<p/>
</div>
<div class="page"><p/>
<p>We close this section on rotationally invariant problems with a brief study of 
the isotropic oscillator. The most celebrated member of this class, the hydrogen 
atom, will be discussed in detail in the next chapter. 
</p>
<p>The Isotropic Oscillator 
</p>
<p>The isotropic oscillator is described by the Hamiltonian 
</p>
<p>If we write as usual 
</p>
<p>_ UE!(r) ym(e A.) 1/fE!m ___ I , 'I' 
r 
</p>
<p>we obtain the radial equation 
</p>
<p>As r--HXJ, we find 
</p>
<p>where 
</p>
<p>( )
</p>
<p>1/2 
</p>
<p>y= f.l1iOJ r 
</p>
<p>is dimensionless. So we let 
</p>
<p>and obtain the following equation for v(y): 
</p>
<p>v"-- 2yv' + [2A.-1- 1(/; l) }=o, E A.=-
1iro 
</p>
<p>(12.6.42) 
</p>
<p>(12.6.43) 
</p>
<p>(12.6.44) 
</p>
<p>(12.6.45) 
</p>
<p>(12.6.46) 
</p>
<p>(12.6.47) 
</p>
<p>(12.6.48) 
</p>
<p>It is clear upon inspection that a two-term recursion relation will obtain if a power-
series solution is plugged in. We set 
</p>
<p>00 
</p>
<p>v(y)=/+1 L Cnyn (12.6.49) 
n=O 
</p>
<p>where we have incorporated the known behavior [Eq. (12.6.14)] near the origin. 
</p>
<p>351 
</p>
<p>ROTATION 
INVARIANCE 
</p>
<p>AND ANGULAR 
MOMENTUM </p>
<p/>
</div>
<div class="page"><p/>
<p>352 
</p>
<p>CHAPTER 12 
</p>
<p>By going through the usual steps (left as an exercise) we can arnve at the 
</p>
<p>following quantization condition: 
</p>
<p>E= (2k+ I+ 3/2)fzro, k=O, I, 2, ... (12.6.50) 
</p>
<p>If we define the principal quantum number (which controls the energy) 
</p>
<p>n=2k+l (12.6.51) 
</p>
<p>we get 
</p>
<p>E= (n + 3/2)fzro (12.6.52) 
</p>
<p>At each n, the allowed I values are 
</p>
<p>I= n- 2k = n, n - 2, ... , I or 0 (12.6.53) 
</p>
<p>Here are the first few eigenstates: 
</p>
<p>n=O 1=0 m=O 
</p>
<p>n=1 I= I m= &plusmn; 1, 0 
</p>
<p>n=2 1=0, 2 m=O; &plusmn;2, &plusmn; 1, 0 
</p>
<p>n=3 I= 1, 3 m=&plusmn;l,O; &plusmn;3, &plusmn;2, &plusmn;1,0 
</p>
<p>Of particular interest to us is the fact that states of different I are degenerate. The 
</p>
<p>degeneracy in m at each I we understand in terms of rotational invariance. The 
</p>
<p>degeneracy of the different I states (which are not related by rotation operators or the 
</p>
<p>generators) appears mysterious. For this reason it is occasionally termed accidental 
</p>
<p>degeneracy. This is, however, a misnomer, for the degeneracy in I can be attributed 
</p>
<p>to additional invariance properties of H. Exactly what these extra invariances or 
</p>
<p>symmetries of Hare, and how they explain the degeneracy in /, we will see in Chapter 
</p>
<p>15. 
</p>
<p>Exercise 12.6.11. * (I) By combining Eqs. (12.6.48) and (12.6.49) derive the two-term 
recursion relation. Argue that C0 # 0 if U is to have the right properties near y = 0. Derive the 
</p>
<p>quantizations condition, Eq. (12.6.50). 
</p>
<p>(2) Calculate the degeneracy and parity at each n and compare with Exercise 10.2.3, 
</p>
<p>where the problem was solved in Cartesian coordinates. 
</p>
<p>(3) Construct the normalized eigenfunction 'I' nlm for n = 0 and I. Write them as linear 
</p>
<p>combinations of the n = 0 and n = 1 eigenfunctions obtained in Cartesian coordinates. </p>
<p/>
</div>
<div class="page"><p/>
<p>13 
</p>
<p>The Hydrogen Atom 
</p>
<p>13.1. The Eigenvalue Problem 
</p>
<p>We have here a two-body problem, of an electron of charge -e and mass m, 
and a proton of charge +e and mass M. By using CM and relative coordinates and 
working in the CM frame, we can reduce the problem to the dynamics of a single 
particle whose mass J-1 = mM /(m + M) is the reduced mass and whose coordinate r 
is the relative coordinate of the two particles. However, since m/ M ~ 1/2000, as a 
result of which the relative coordinate is essentially the electron's coordinate and the 
reduced mass is essentially m, let us first solve the problem in the limit M-+ oo. In 
this case we have just the electron moving in the field of the immobile proton. At a 
later stage, when we compare the theory with experiment, we will see how we can 
easily take into account the finiteness of the proton mass. 
</p>
<p>Since the potential energy of the electron in the Coulomb potential 
</p>
<p>f/&gt;=e/r (13.1.1) 
</p>
<p>due to the proton is V = -e2 I r, the Schrodinger equation 
</p>
<p>(13.1.2) 
</p>
<p>determines the energy levels in the rest frame of the atom, as well as the wave 
functionst 
</p>
<p>If/ Etm(r, (), 4&gt;) = REt(r) Y'!'( (), 4&gt;) = U El(r) Y/( (), 4&gt;) (13.1.3) 
r 
</p>
<p>It is clear upon inspection of Eq. ( 13.1.2) that a power series ansatz will lead 
to a three-term recursion relation. So we try to factor out the asymptotic behavior. 
</p>
<p>t It should be clear from the context whether m stands for the electron mass or the z component of 
angular momentum. 353 </p>
<p/>
</div>
<div class="page"><p/>
<p>354 
</p>
<p>CHAPTER 13 
</p>
<p>We already know from Section 12.6 that up to (possibly fractional) powers of r 
</p>
<p>[Eq. (12.6.19)], 
</p>
<p>Um ~ exp[-(2mW/fi2) 1/ 2r] (13.1.4) 
r--+ ~;r:_ 
</p>
<p>where 
</p>
<p>W=-E 
</p>
<p>is the binding energy (which is the energy it would take to liberate the electron) and 
</p>
<p>that 
</p>
<p>(13.1.5) 
</p>
<p>Equation (13.1.4) suggests the introduction of the dimensionless variable 
</p>
<p>(13.1.6) 
</p>
<p>and the auxiliary function v m defined by 
</p>
<p>(13.1.7) 
</p>
<p>The equation for v is then 
</p>
<p>( 13.1.8) 
</p>
<p>where 
</p>
<p>(13.1.9) 
</p>
<p>and the subscripts on v are suppressed. You may verify that if we feed in a series 
</p>
<p>into Eq. (13.1.8), a two-term recursion relation will obtain. Taking into account the 
</p>
<p>behavior near p=O [Eq. (13.1.5)] we try 
</p>
<p>k~O 
</p>
<p>and obtain the following recursion relation between successive coefficients: 
</p>
<p>The Energy Levels 
</p>
<p>Since 
</p>
<p>ck+l -e2JL+2(k+/+ 1) 
- --- --
</p>
<p>ck (k+l+2)(k+l+l)-&middot;&middot;l(l+l) 
</p>
<p>ck+l 2 
------------ -----+ &middot;~ 
</p>
<p>ck k &middot;w k 
</p>
<p>(13.1.10) 
</p>
<p>(13.1.11) 
</p>
<p>(13.1.12) </p>
<p/>
</div>
<div class="page"><p/>
<p>is the behavior of the series pme2P, and would lead to u~e-pv~pme-pe 2 P-pmep 
</p>
<p>as p-HfJ, we demand that the series terminate at some k. This will happen if 
</p>
<p>(13.1.13) 
</p>
<p>or [from Eq. (13.1.9)] 
</p>
<p>k=O, 1, 2, ... : l=O, 1, 2,... (13.1.14) 
</p>
<p>In terms -.:&gt;f the principal quantum number 
</p>
<p>n=k+l+ 1 ( 13.1.15) 
</p>
<p>the allowed energies are 
</p>
<p>n=1,2,3, ... (13.1.16) 
</p>
<p>and at each n the allowed values of I are, according to Eq. (13.1.15), 
</p>
<p>l = n - k - 1 = n - 1, n - 2, ... , 1 , 0 ( 13.1.17) 
</p>
<p>That states of different I should be degenerate indicates that H contains more symmet-
</p>
<p>ries besides rotational invariance. We discuss these later. For the present, let us note 
</p>
<p>that the degeneracy at each n is 
</p>
<p>n-1 
</p>
<p>L (21+ l)=n2 (13.1.18) 
t~o 
</p>
<p>It is common to refer to the states with l = 0, 1, 2, 3, 4, ... as s, p, d,j, g, h, ... states. 
In this spectroscopic notation, Is denotes the state (n = l, l = 0); 2s and 2p the l = 0 
and l= 1 states at n=2; 3s, 3p, and 3d the /=0, 1, and 2 states at n=3, and so on. 
No attempt is made to keep track of m. 
</p>
<p>It is convenient to employ a natural unit of energy, called a Rydberg (Ry), for 
measuring the energy levels of hydrogen: 
</p>
<p>4 
R _me 
y- 21i2 (13.1.19) 
</p>
<p>355 
</p>
<p>THE 
HYDROGEN 
</p>
<p>ATOM </p>
<p/>
</div>
<div class="page"><p/>
<p>356 
</p>
<p>CHAPTER 13 
</p>
<p>d f - --E t 0 -1/16 -n=4 -1/9 -n=3 
-1/4 -n=2 
</p>
<p>-1 -n=l 
</p>
<p>in terms of which 
</p>
<p>Figure 13.1. The first few eigenstates of hydrogen. The energy 
</p>
<p>is measured in Rydbergs and the states are labelled in the spec-
</p>
<p>troscopic notation. 
</p>
<p>-Ry 
E=-n 2 
</p>
<p>n 
(13.1.20) 
</p>
<p>Figure 13.1 shows some of the lowest-energy states of hydrogen. 
</p>
<p>The Wave Functions 
</p>
<p>Given the recursion relations, it is a straightforward matter to determine the 
</p>
<p>wave functions and to normalize them. Consider a given n and /. Since the series in 
</p>
<p>Eq. (13.1.10) terminates at 
</p>
<p>k=n-1-1 ( 13.1.21) 
</p>
<p>the corresponding function v1 is /+ 1 times a polynomial of degree n -I- 1. This 
polynomial is called the associated Laguerre polynomial, L~ 1 ~/-1 (2p ).t The corre-
sponding radial function is 
</p>
<p>(13.1.22) 
</p>
<p>Recall that 
</p>
<p>( )112 [ ( 4 )]1/2 2mW &middot; 2m me 
p = T r = f1 21iV r 
</p>
<p>(13.1.23) 
</p>
<p>L~=e'(dP /dxP)V'x"). </p>
<p/>
</div>
<div class="page"><p/>
<p>In terms of the length 
</p>
<p>-n2 
ao=--- 2 
</p>
<p>me 
(13.1.24) 
</p>
<p>called the Bohr radius, which provides the natural distance scale for the hydrogen 
</p>
<p>atom, 
</p>
<p>R ( ) -r;nao ( r ')' L2t+ 1 (
1 2r \) 
</p>
<p>nl r ~e - n---1--l -
.nao !lao 
</p>
<p>(13.1.25) 
</p>
<p>As r-+ oc, L will be dominated by the highest power, r" I---I, and 
</p>
<p>Rnt - (rf- 1 e-r/nao (independent of/) (13.1.26) 
r--+% 
</p>
<p>(If l=n-1, this form is valid at all r since L6'~ 1 is a constant.) Equation (13.1.26) 
</p>
<p>was anticipated in the last chapter when we considered the behavior of UEt as r-+ oc, 
</p>
<p>in a Coulomb potential (see Exercise 13.1.4). 
The following are the first few normalized eigenfunctions, ~~Elm= lJf ntm : 
</p>
<p>r )1/2 
lJf 1.0,0 = ( - ~ 
</p>
<p>Jrao 
</p>
<p>( )l/2( ) 1 r -r/2ao lJI2.o.o = --1 2------ e - -
32na0, , a0 , 
</p>
<p>(13.1.27) 
</p>
<p>cos e 
</p>
<p>( 
</p>
<p>\ 1/2 
1 &middot; r 
</p>
<p>lJI2,1,&plusmn;l = 'f 6-4 3) ~ 
nao ao 
</p>
<p>Exercise 13.1.1. Derive Eqs. (13.1.11) and ( 13. l. 14) starting from Eqs. (13.1.8) (13.1.10). 
</p>
<p>Exercise 13.1.2. Derive the degeneracy formula, Eq. (13.1.18). 
</p>
<p>Exercise 13.1.3. Starting from the recursion relation, obtain lflm&bull; (normalized). 
</p>
<p>Exercise 13.1.4. Recall from the last chapter [Eq. (12.6.19)] that as ,._,.ex;, 
UF- (r)me'/Kfi' e-"' in a Coulomb potential V= -e2 jr [K =(2m W/112 ) 112]. Show that this agrees 
</p>
<p>with Eq. (13.1.26). 
</p>
<p>357 
</p>
<p>THE 
HYDROGEN 
</p>
<p>ATOM </p>
<p/>
</div>
<div class="page"><p/>
<p>358 
</p>
<p>CHAPTER 13 
</p>
<p>Let us explore the statement that a0 provides a natural length scale for the 
</p>
<p>hydrogen atom. Consider the state described by 
</p>
<p>lj/n,n-l,mr:&pound;e-r/tuto ,.n---1 Y:~r __ J( fJ, c/&gt;) (13.1.28) 
</p>
<p>Let us ask for the probability of finding the electron in a spherical shell of radius r 
</p>
<p>and thickness dr: 
</p>
<p>J P(r)r2 dr df.1oce Zrnao r2n dr 
n 
</p>
<p>The probability density in r reaches a maximum when 
</p>
<p>or 
</p>
<p>( J 3.1.29) 
</p>
<p>( 13.1.30) 
</p>
<p>When n = 1, this equals a0 . Thus the Bohr radius gives the most probable value of 
</p>
<p>r in the ground state and this defines the "size" of the atom (to the extent one may 
</p>
<p>speak of it in quantum theory). If n &gt; 1 \Ve see that the size grows as n2 , at least in 
the state of I= n- l. If l =Fn- 1, the radial function has n -I- 1 zeros and the density 
</p>
<p>in r has several bumps. In this case, we may define the size by (r).! It can be shown, 
</p>
<p>by using properties of L~'~~~ 1 that 
</p>
<p>(13.1.31) 
</p>
<p>Rather than go through the lengthy derivation of this formula let us consider the 
</p>
<p>following argument, which indicates that the size grows as n2a0 &bull; In any eigenstate 
</p>
<p>(ll) = E= Cf') + ( V) = (P2/2m) -- (13.1.32) 
</p>
<p>It can be shown (Exercise 13.1.5) that 
</p>
<p>(T)=- V) (13.1.33) 
</p>
<p>which is just the quantum version of the classical virial theorem, which states that 
</p>
<p>if V= cl, then the averages f and U are related by 
</p>
<p>- k -
T=- V 
</p>
<p>2 
</p>
<p>t Even though r represents the abstract operator (X1 + Y 1 only in the coordinate basis, we shall 
use the same symbol to refer to it in the abstract, so as to keep the notation simple. </p>
<p/>
</div>
<div class="page"><p/>
<p>It follows that 
</p>
<p>(13.1.34) 
</p>
<p>Now, in the state labeled by n, 
</p>
<p>(13.1.35) 
</p>
<p>from which it follows that 
</p>
<p>(13.1.36) 
</p>
<p>Although 
</p>
<p>the two are of the same order of magnitude (see Exercise 9.4.2) and we infer that 
</p>
<p>(13.1.37) 
</p>
<p>which agrees with the result Eq. (13.1.31). (One must be somewhat cautious with 
statements like (1/r) ~ 1/ (r). For example, it is not true in an s state that (l/r4 ) ~ 1/ 
(r4), since (l/r4 ) is divergent while 1/(r4 ) is not. In the present case, however, (1/ 
r) is well defined in all states and indeed (1/r) and 1/(r) are of the same order of 
magnitude.) 
</p>
<p>This completes our analysis of the hydrogen spectrum and wave functions. 
Several questions need to be answered, such as (1) What are the numerical values 
of En, a0 , etc.? (2) How does one compare the energy levels and wave functions 
deduced here with experiment? 
</p>
<p>These questions will be taken up in Section 13.3. But first let us address a 
question raised earlier: what is the source of the degeneracy in l at each n? 
</p>
<p>Exercise 13.1.5. * ( Virial Theorem). Since In, I, m) is a stationary state, (Q) = 0 for any 
n. Consider n=R &middot; P and use Ehrenfest's theorem to show that (T) = ( -1/2)(V) in the state 
In, /,m). 
</p>
<p>13.2. The Degeneracy of the Hydrogen Spectrum 
</p>
<p>The hydrogen atom, like the oscillator, exhibits "accidental degeneracy." 
Quotation marks are used once again, because, as in the case of the oscillator, the 
degeneracy can be explained in terms of other symmetries the Hamiltonian has 
besides rotational invariance. Now, we have seen that the symmetries of H imply 
</p>
<p>359 
</p>
<p>THE 
HYDROGEN 
</p>
<p>ATOM </p>
<p/>
</div>
<div class="page"><p/>
<p>360 
</p>
<p>CHAPTER 13 
</p>
<p>the conservation of the generators of the symmetries. Consequently, if there is an 
extra symmetry (besides rotational invariance) there must be some extra conserved 
</p>
<p>quantities (besides angular momentum). Now it is well known classically that the 
</p>
<p>Coulombt potential is special (among rotationally invariant potentials) in that it 
conserves the Runge-Lenz vector 
</p>
<p>p xI e2 
n= &middot;&middot;&middot;&middot;&middot; r (13.2.1) 
</p>
<p>m r 
</p>
<p>The conservation ofn implies that not only is the orbit confined to a plane perpendic-
ular to l (as in any rotationally invariant problem) it is also closed (Exercise 13.2.1). 
</p>
<p>In quantum theory then, there will be an operator N which commutes with H: 
</p>
<p>[N, H] =0 (13.2.2) 
</p>
<p>and is given by&sect; 
</p>
<p>(13.2.3) 
</p>
<p>We have seen that the conservation of L implies that [L&plusmn;, H] = 0, which means 
that we can raise and lower them values at a given l without changing the energy. 
This is how the degeneracy in m is "explained" by rotational invariance. 
</p>
<p>So it must be that since [N, H] = 0, we must be able to build some operator out 
</p>
<p>of the components of N, which commutes with H and which raises l by one unit. 
</p>
<p>This would then explain the degeneracy in l at each n. Precisely what this operator 
is and how it manages to raise l by one unit will be explained in Section 15.4, devoted 
</p>
<p>to the study of "accidental" degeneracy. You will also find therein the explanation 
</p>
<p>of the degeneracy of the oscillator. 
</p>
<p>Exercise 13.2.1. Let us see why the conservation of the Runge-Lenz vector n implies 
</p>
<p>closed orbits. 
</p>
<p>(1) Express n in terms of rand p alone (get rid of l). 
</p>
<p>(2) Since the particle is bound, it cannot escape to infinity. So, as we follow it from some 
</p>
<p>arbitrary time onward, it must reach a point r max where its distance from the origin stops 
</p>
<p>growing. Show that 
</p>
<p>:j: Or generally any ljr potential, say. gravitationaL 
</p>
<p>&sect;Since [P, L] #0, we have used the symmetrization rule to construct N from n. i.e., 
</p>
<p>p x J-t[(P x L) + (P x L)1] = ~[P x L- L x P] (verify this). </p>
<p/>
</div>
<div class="page"><p/>
<p>at this point. (Use the law of conservation of energy to eliminate p2 .) Show that, for similar 
reasons, if we wait some more, it will come to r min, where 
</p>
<p>n = rmin(2E+L) 
7mm 
</p>
<p>Thus rmax and rmin are parallel to each other and to n. The conservation or constancy of n 
implies that the maximum (minimum) separation is always reached at the same point 
rmax(rmin), i.e., the orbit is closed. In fact, all three vectors fmax. rmin, and n are aligned with 
the major axis of the ellipse along which the particle moves; n and rmin are parallel, while n 
and rmax are antiparallel. (Why?) Convince yourself that for a circular orbit, n must and does 
vanish. 
</p>
<p>13.3. Numerical Estimates and Comparison with Experiment 
</p>
<p>In this section we (1) obtain numerical estimates for various quantities such as 
the Bohr radius, energy levels, etc.; (2) ask how the predictions of the theory are 
actually compared with experiment. 
</p>
<p>Numerical Estimates 
</p>
<p>Consider first the particle masses. We will express the rest energies of the particles 
in million-electron volts or MeV: 
</p>
<p>mc 2 ~0.5 Mev (0.511 is a more exact value) 
</p>
<p>Mc2 = 1000 MeV (938.3)t 
</p>
<p>m/M~ l/2000 (l/1836)t 
</p>
<p>Consequently the reduced mass J.l and electron mass m are almost equal: 
</p>
<p>mM mM 
p=--~--=m 
</p>
<p>m+M M 
</p>
<p>as are the relative coordinate and the electron coordinate. 
Consider now an estimate of the Bohr radius 
</p>
<p>t A more exact value. 
</p>
<p>(13.3.1) 
</p>
<p>(13.3.2) 
</p>
<p>(13.3.3) 
</p>
<p>(13.3.4) 
</p>
<p>(13.3.5) 
</p>
<p>361 
</p>
<p>THE 
HYDROGEN 
</p>
<p>ATOM </p>
<p/>
</div>
<div class="page"><p/>
<p>362 
</p>
<p>CHAPTER 13 
</p>
<p>To find this we need the values of 1i and e. It was mentioned earlier that 
</p>
<p>1i = 1.054 x 1 o-27 erg sec 
</p>
<p>A more useful thing to remember for performing quick estimates ist 
</p>
<p>lic~2000 eVA (1973.3) (13.3.6) 
</p>
<p>where 1 angstrom (A)= 1 o-s em. The best way to remember e2 is through the fine-
structure constant: 
</p>
<p>(13.3. 7) 
</p>
<p>This constant plays a fundamental role in quantum mechanical problems involving 
</p>
<p>electrodynamics. Since it is dimensionless, its numerical value has an absolute signifi-
</p>
<p>cance: no matter what units we use for length, mass, and time, a will be 1/137. 
</p>
<p>Thus, although no one tries to explain why c = 3 x 1010 em/ sec, several attempts have 
</p>
<p>been made to arrive at the magic figure of 1/137. Since it is a God-given number 
</p>
<p>(independent of mortal choice of units) one tries to relate it to fundamental numbers 
</p>
<p>such as 1!, e, e", 7!', the number of space-time dimensions, etc. 
</p>
<p>Anyway, returning to our main problem, we can now estimate a0 : 
</p>
<p>a ~!f_=~(lic)= (2000)(137) A~o. 55 A 
0 me2 mc2 e2 0.5 x 106 
</p>
<p>(0.53) 
</p>
<p>Consider next the energy levels 
</p>
<p>We estimate 
</p>
<p>Ry= ;::=mt (::r 
0.25 X 106 
</p>
<p>~ 2 eV ~ 13.3 eV 
(137) 
</p>
<p>(13.6) 
</p>
<p>So, using the more accurate value of Ry, 
</p>
<p>-13.6 v 
En=--2-e 
</p>
<p>n 
</p>
<p>t Many of the tricks used here were learned from Professor A. Rosenfeld at the University of California, 
Berkeley. </p>
<p/>
</div>
<div class="page"><p/>
<p>The electron in the ground state needs 13.6 eV to be liberated or ionized. One may 
imagine that it is 13.6 eV down the infinitely deep Coulomb potential. 
</p>
<p>Let us digress to consider two length scales related to a0 &bull; The first 
</p>
<p>fiz ez fi 
aoa=-z&middot;-=-=Xe 
</p>
<p>me fie me 
(13.3.8) 
</p>
<p>is called the Compton wavelength of the electron and is 137 times smaller than the 
Bohr radius. What does Ae represent? In discussing the nuclear force, it was pointed 
out that the Compton wavelength of the pion was the distance over which it could 
be exchanged. It can also be defined as the lower limit on how well a particle can 
be localized. In the nonrelativistic theory we are considering, the lower limit is zero, 
since we admit position eigenkets lx). But in reality, as we try to locate the particle 
better and better, we use more and more energetic probes, say photons to be specific. 
To locate it to some M, we need a photon of momentum 
</p>
<p>fi 
11P--
</p>
<p>M 
</p>
<p>Since the photon is massless, the corresponding energy is 
</p>
<p>fie 
11E--
</p>
<p>M 
</p>
<p>in view of Einstein's formula E2 = c2p 2 + m 2c4&bull; 
If this energy exceeds twice the rest energy of the particle, relativity allows the 
</p>
<p>production of a particle-antiparticle pair in the measurement process. So we demand 
</p>
<p>or 
</p>
<p>fi fi 
M~----
</p>
<p>2me me 
</p>
<p>If we attempt to localize the particle any better, we will see pair creation and we will 
have three (or more) particles instead of the one we started to locate. 
</p>
<p>In our analysis of the hydrogen atom, we treated the electron as a localized 
point particle. The preceding analysis shows that this is not strictly correct, but it 
</p>
<p>363 
</p>
<p>THE 
HYDROGEN 
</p>
<p>ATOM </p>
<p/>
</div>
<div class="page"><p/>
<p>364 
</p>
<p>CHAPTER 13 
</p>
<p>also shows that it is a fair approximation, since the "fuzziness" or "size" of the 
</p>
<p>electron is a times smaller than the size of the atom, a0 
</p>
<p>tijmc 1 
~~~~~~ = a ~, 
</p>
<p>ao . - 137 
</p>
<p>Had the electric charge been 10 times as big, a would have been of order unity, and 
the size of the electron and the size of its orbit would have been of the same order 
</p>
<p>and the point particle approximation would have been untenable. Let us note that 
</p>
<p>1 1 
Ae= a&middot; ao'-"'0.5 x ~~~~A ::o.: ~~~~ A c::.::4 x 10 ~ 3 A 
</p>
<p>137 250 
</p>
<p>If we multiply Xe by a we get another length, called the classical radius of the electron: 
</p>
<p>(13.3.9) 
</p>
<p>If we imagine the electron to be a spherical charge distribution, the Coulomb energy 
of the distribution (the energy it takes to assemble it) will be of the order i 
where re is the radius of the sphere. If we attribute the rest energy of the electron to 
this Coulomb energy, we arrive at the classical radius. In summary, 
</p>
<p>--'&gt; 
a 
</p>
<p>ro 
</p>
<p>Let us now return to the hydrogen atom. The mnemonics discussed so far are 
</p>
<p>concerned only with the numbers. Let us now consider mnemonics that help us 
</p>
<p>remember the dynamics. These must be used with caution, for they are phrased in 
</p>
<p>terms not allowed in quantum theory. 
</p>
<p>The source of these mnemonics is the Bohr model of the hydrogen atom. About 
</p>
<p>a decade or so prior to the formulation of quantum mechanics as described in this 
text, Bohr proposed a model of the atom along the following lines. Consider a 
</p>
<p>particle of mass m in V(r) = , moving in a circular orbit of radius r. The 
</p>
<p>dynamical equation is 
</p>
<p>or 
</p>
<p>2 e2 
mv =-
</p>
<p>r 
</p>
<p>( 13.3.10) 
</p>
<p>(13.3.11) </p>
<p/>
</div>
<div class="page"><p/>
<p>Thus any radius is allowed if r satisfies this equation. It also follows that any energy 
</p>
<p>is allowed since 
</p>
<p>I z e2 e2 1 z 
E=-mv --&middot;&middot;= ---= --&middot;&middot;mv 
</p>
<p>2 r 2r 2 
( 13.3.12) 
</p>
<p>Bohr conjectured that the only allowed orbits were those that had integral 
</p>
<p>angular momentum in units of 1i : 
</p>
<p>mvr=nn 
</p>
<p>Feeding this into Eq. (13.3.11) we get 
</p>
<p>or 
</p>
<p>and 
</p>
<p>n21iz ez 
m&middot;--=-
</p>
<p>m2r2 r 
</p>
<p>E = - e2 = ..... _i:_ . ( 12) 
n 2r 2ao \n ' 
</p>
<p>( 13.3.13) 
</p>
<p>(13.3.14) 
</p>
<p>( 13.3.15) 
</p>
<p>Thus, if you ever forget the formula for a0 or En, you can go back to this model for 
the formulas (though not for the physics, since it is perched on the fence between 
classical and quantum mechanics; it speaks of orbits, but quantizes angular momen-
</p>
<p>tum and so on). The most succinct way to remember the Bohr atom (i.e., a mnemonic 
</p>
<p>for the mnemonic) is the equation 
</p>
<p>a=f3 (13.3.16) 
</p>
<p>where f3 is the velocity of the electron in the ground state of hydrogen measured in 
</p>
<p>units of velocity of light (/3 =vIc). Given this, we get the ground state energy as 
</p>
<p>I 2 1 , 2 1 .,, 1 22 
E1 =--mv =--mc~(vjc) = .... &middot;-mc"f3'&middot;= .... -mc a 
</p>
<p>2 2 2 2 
</p>
<p>(13.3.17) 
</p>
<p>365 
</p>
<p>THE 
HYDROGEN 
</p>
<p>ATOM </p>
<p/>
</div>
<div class="page"><p/>
<p>366 
</p>
<p>CHAPTER 13 
</p>
<p>Given this, how could one forget that the levels go as n - 2, i.e., 
</p>
<p>If we rewrite E 1 as -e2 /2a0 , we can get the formula for a0 &bull; The equation a= f3 also 
justifies the use of nonrelativistic quantum mechanics. An equivalent way (which 
</p>
<p>avoids the use of velocity) is Eq. (13.3.17), which states that the binding energy is 
</p>
<p>~o I 13 7)2 times the rest energy of the electron. 
</p>
<p>Exercise 13.3.1. * The pion has a range of I Fermi= 10- 5 A as a mediator of nuclear 
force. Estimate its rest energy. 
</p>
<p>Exercise 13.3.2. * Estimate the de Broglie wavelength of an electron of kinetic energy 
200 eV. (Recall )c= 2lr1i/p.) 
</p>
<p>Comparison with Experiment 
</p>
<p>Quantum theory makes very detailed predictions for the hydrogen atom. Let us 
</p>
<p>ask how these are to be compared with experiment. Let us consider first the energy 
</p>
<p>levels and then the wave functions. In principle, one can measure the energy levels 
</p>
<p>by simply weighing the atom. In practice, one measures the differences in energy 
</p>
<p>levels as follows. If we start with the atom in an eigenstate ln/m), it will stay that 
</p>
<p>way forever. However, if we perturb it for a time T, by turning on some external 
</p>
<p>field (i.e., change the Hamiltonian from if!, the Coulomb Hamiltonian, to l-f + H 1) 
its state vector can start moving around in Hilbert space, since lnlm) is not a station-
</p>
<p>ary state of if!+ H 1&bull; If we measure the energy at time t &gt; T, we may find it corre-
sponds to another state with n' i=n. One measures the energy by detecting the photon 
emitted by the atom. The frequency of the detected photon will be 
</p>
<p>En-En&middot; 
m .=---nn fj 
</p>
<p>Thus the frequency of light coming out of hydrogen will be 
</p>
<p>(13.3.18) 
</p>
<p>(13.3.19) 
</p>
<p>For a fixed value n' = 1, 2, 3, ... , we obtain a family of lines as we vary n. These 
</p>
<p>families have in fact been seen, at least for several values of n'. The n' =I family is </p>
<p/>
</div>
<div class="page"><p/>
<p>called the Lyman series (it corresponds to transitions to the ground state from the 
</p>
<p>upper ones) : 
</p>
<p>(13.3.20) 
</p>
<p>The n' = 2 family is called the Balmer series and corresponds to transitions to the 
states 12/m) from n = 3, 4, ... , etc. The n' = 3 family called the Paschen series, etc. 
Let us estimate the wavelength of a typical line in the Lyman series, say the one 
</p>
<p>corresponding to the transition n = 2-+n' =I : 
</p>
<p>l3.5ev(&middot; 1) 
ill21 = -~jj- I --4, 
</p>
<p>The wavelength is estimated to be 
</p>
<p>10 
~-eV 
</p>
<p>li 
</p>
<p>2:rrc 2:rr 
A.=---~--------=~ (Fie) ~ 1200 A 
</p>
<p>(() 10 
</p>
<p>A more refined estimate gives a value of 1216 A, in very good agreement with 
experiment. Equally good is the agreement for all other observed lines. However, 
</p>
<p>there are, in all cases, small discrepancies. Much of these may be explained by 
</p>
<p>corrections that are calculable in theory. First we must correct for the fact that the 
</p>
<p>proton is not really immobile; that we have here a two-body problem. As explained 
</p>
<p>in Chapter 10, this is done by writing Schrodinger's equation for the relative (and 
</p>
<p>not electron) coordinate and working in the CM frame. This equation would differ 
</p>
<p>from Eq. (13.1.2) only in that m would be replaced by fJ.. This in fact would be the 
</p>
<p>only change in all the formulas that follow, in particular Eq. (13.1.16) for the energy 
</p>
<p>levels. This would simply rescale the entire spectrum by a factor p/m = ll{/(1\-f + m), 
which differs from 1 by less than a tenth of a percent. This difference is, however, 
</p>
<p>observable in practice: one sees it in the difference between the levels of hydrogen 
and deuterium (whose nucleus has a proton and a neutron). 
</p>
<p>Then there is the correction due to the fact that the kinetic energy of the electron 
</p>
<p>is not ~mv 2 = p2 /2m in Einstein's theory, but instead mc2 [(1- v2 / c2 ) 112 ~~ 1 ], which 
is the difference between the energy at velocity v and the energy at rest. The ~mv 2 
</p>
<p>term is just the first in the power series expansion of the above, in the variable v2 / 
c). In Chapter 17 we will take into account the effect of the next term, which is 
</p>
<p>-3mv4/8c2, or in terms of the momentum, -3p4 /8m 3c1 &bull; This is a correction of order 
v2 / c2 relative to the p 2 /2m piece we included, or since v /c ~a, a correction of order 
a 2 relative to main piece. There are other corrections of the same order, and these 
go by the name of fine-structure corrections. They will be included (in some approxi-
mation) in Chapter 17. The Dirac equation, which we will not so.lve in this book, 
takes into account the relativistic corrections to all orders in However, it too 
doesn't give the full story; there are tiny corrections due to quantum fluctuations of 
</p>
<p>367 
</p>
<p>THE 
HYDROGEN 
</p>
<p>ATOM </p>
<p/>
</div>
<div class="page"><p/>
<p>368 
</p>
<p>CHAPTER l3 
</p>
<p>the electromagnetic field (which we have treated classically so far). These corrections 
</p>
<p>are calculable in theory and measurable experimentally. The agreement between 
theory and experiment is spectacular. It is, however, important to bear in mind that 
</p>
<p>all these corrections are icing on the cake; that the simple nonrelativistic Schrodinger 
</p>
<p>equation by itself provides an excellent description of the hydrogen spectrum. (Much 
</p>
<p>of the present speculation on what the correct theory of elementary particles is will 
</p>
<p>be put to rest if one can come up with a description of these particles that is half as 
</p>
<p>good as the description of the hydrogen atom by Schrodinger's equation.) 
</p>
<p>Consider next the wave functions. To test the predictions, one once again relies 
</p>
<p>on perturbing the system. The following example should give you a feeling for how 
</p>
<p>this is done. Suppose we apply an external perturbation H 1 for a short time F:. During 
this time, the system goes from I nlm) to 
</p>
<p>(' iE:E, ir:H
1 )\ 
</p>
<p>= I nlm) - , Ji + &middot;&middot;&middot;&middot;&middot;&middot;&middot;~&middot;&middot;&middot;&middot;&middot;&middot; I nlm) 
</p>
<p>The probability of it being in a state ln'l'm') (assuming ln'l'm') is different from 
</p>
<p>lnlm)) is 
</p>
<p>Thus quantum theory can also determine for us the rate of transition to the state 
ln'l'm'). This rate is controlled by the matrix element (n'l'nliH11nlm), which in 
</p>
<p>coordinate space, will be some integral over lfl:&middot;rm&middot; and lflntm with H 1 sandwiched 
between them. The evaluation of the integrals entails detailed knowledge of the wave 
</p>
<p>functions, and conversely, agreement of the calculated rates with experiment is a 
</p>
<p>check on the predicted wave functions. We shall see a concrete example of this when 
</p>
<p>we discuss the interaction of radiation with matter in Chapter 18. 
</p>
<p>Exercise 13. 3. 3. Instead of looking at the emission spectrum, we can also look at the 
</p>
<p>absorption spectrum of hydrogen. Say some hydrogen atoms are sitting at the surface of the 
</p>
<p>sun. From the interior of the sun, white light tries to come out and the atoms at the surface 
</p>
<p>absorb what they can. The atoms in the ground state will now absorb the Lyman series and 
</p>
<p>this will lead to dark lines if we analyze the light coming from the sun. The presence of these 
</p>
<p>lines will tell us that there is hydrogen at the surface of the sun. We can also estimate the 
</p>
<p>surface temperature as follows. Let T be the surface temperature. The probabilities P(n oo I) 
and P(n = 2) of an atom being at 11 = 1 and 11 = 2, respectively, are related by Boltzmann's 
</p>
<p>formula 
</p>
<p>P(n =2) =4 e &middot;tf:,&middot;&middot;E,) kT 
P(n= I) 
</p>
<p>where the factor 4 is due to the degeneracy of the n = 2 level. Now only atoms in n = 2 can 
</p>
<p>produce the Balmer lines in the absorption spectrum. The relative strength of the Balmer and </p>
<p/>
</div>
<div class="page"><p/>
<p>Lyman lines will tell us P(n=2)/P(n= 1), from which we may infer T. Show that forT= 
6000 K, P(n = 2)/ P(n = 1) is negligible and that it becomes significant only for T&lt;::!!.I05 K. (The 
Boltzmann constant is k':!:.9 x 10-5 eV /K. A mnemonic is kT':!:.;fo eV at room temperature, 
T=300K.) 
</p>
<p>13.4. Multielectron Atoms and the Periodic Table 
</p>
<p>It is not possible to treat multielectron atoms analytically even if we treat the 
nucleus as immobile. Although it is possible, in principle, to treat an arbitrarily 
complex atom by solving the exact Schrodinger equation numerically, a more practi-
cal method is to follow some approximation scheme. Consider the one due to Hartree. 
Here one assumes that each electron obeys a one-particle Schrodinger equation 
wherein the potential energy V = -elf&gt; (r) is due to the nucleus and the other electrons. 
In computing the electronic contribution to q, (r), each electron is assigned a charge 
distribution which is (-e) times the probability density associated with its wave 
function. And what are the wave functions? They are the eigenstates in the potential 
q, (r)! To break the vicious circle, one begins with a reasonable guess for the potential, 
call it l/J0(r), and computes the allowed energy eigenstates. One then fills them up in 
the order of increasing energy, putting in just two electrons in each orbital state, 
with opposite spins (the Pauli principle will not allow any more)t until all the 
electrons have been used up. One then computes the potential lf&gt;t(r) due to this 
electronic configuration.&sect; If it coincides with l/&gt;0{r) (to some desired accuracy) one 
stops here and takes the configuration one got to be the ground state of the atom. 
If not, one goes through one more round, this time starting with l/J1(r). The fact that, 
in practice, one soon finds a potential that reproduces itself, signals the soundness of 
this scheme. 
</p>
<p>What do the eigenstates look like? They are still labeled by (nlm) as in hydrogen, 
with states of different m degenerate at a given n and I. [This is because q, (r) is 
rotationally invariant.] The degeneracy in I is, however, lost. Formally this is because 
the potential is no longer 1/r and physically this is because states with lower angular 
momentum have a larger amplitude to be near the origin and hence sample more of 
the nuclear charge, while states of high angular momentum, which are suppressed 
at the origin, see the nuclear charge shielded by the electrons in the inner orbits. As 
a result, at each n the energy goes up with I. The "radius" of each state grows with 
n, with a slight dependence on I. States of a given n are thus said to form a shell 
(for, in a semiclassical sense, they may be viewed as moving on a sphere of radius 
equal to the most probable value of r). States of a given I and n are said to form a 
sub shell. 
</p>
<p>Let us now consider the electronic configurations of some low Z ( Z is the nuclear 
charge) atoms. Hydrogen CH) has just one electron, which is in the Is state. This 
configuration is denoted by ls1&bull; Helium eHe) has two electrons in the Is state with 
opposite spins, a configuration denoted by Ii. 2He has its n = 1 shell filled. Lithium 
CLi) has its third electron in the 2s state, i.e., it is in the configuration ls22s 1&bull; (Recall 
</p>
<p>t In this discussion electron spin is viewed as a spectator variable whose only role is to double the states. 
This is a fairly good approximation. 
</p>
<p>&sect;If necessary, one averages over angles to get a spherically symmetric 1/J. 
</p>
<p>369 
</p>
<p>THE 
HYDROGEN 
</p>
<p>ATOM </p>
<p/>
</div>
<div class="page"><p/>
<p>370 
</p>
<p>CHAPTER 13 
</p>
<p>that the s state is lower than the p state.) We keep going this way through beryllium 
</p>
<p>( 4Be), boron CB), carbon (6C), nitrogen CN), oxygen (80), and fluorine (9F), till 
</p>
<p>neon C~e). Neon is in the configuration ls22s22p6, i.e., has its n=2 shell filled. The 
</p>
<p>next element, sodium C1Na), has a solitary electron in the 3s state. The 3s and 3p 
subshells are filled when we get to argon C8Ar). The next one, potassium C9K) has 
its 19th electron in the 4s and not 3d state. This is because the growth in energy due 
</p>
<p>to a change in n from 3 to 4 is less than the growth due to change in I from I to 2 
</p>
<p>at n = 3. This phenomenon occurs often as we move up in Z. For example. in the 
</p>
<p>"rare earth" elements, the 6s shell is filled before the 4f shell. 
Given the electronic configurations, one can anticipate many of the chemical 
</p>
<p>properties of the elements. Consider an element such as 1 ~e, which has a closed 
</p>
<p>outer shell. Since the total electronic charge is spherically symmetric 
</p>
<p>(1Rn1l2 L~~-tl Yil 2 is independent of() and l/&gt;), it shields the nuclear charge very 
effectively and the atom has no significant electrostatic affinity for electrons in other 
</p>
<p>atoms. If one of the electrons in the outer shell could be excited to a higher level, 
</p>
<p>this would change, but there is a large gap in energy to cross. Thus the atom is rarely 
</p>
<p>excited and is chemically inert. On the other hand, consider an. element like 11 Na, 
</p>
<p>which has one more electron, which occupies the 3s state. This electron sees a charge 
</p>
<p>of +e when it looks inward (the nuclear charge of II shielded by the I 0 electrons 
</p>
<p>in then= I and 2 shells) and is thus very loosely bound. Its binding energy is 5.1 eV 
</p>
<p>compared to ann= 2 electron in Ne, which has a binding energy of 21.6 eV. If 11Na 
</p>
<p>could get rid of this electron, it could reach a stable configuration with a closed n = 
</p>
<p>2 shell. If we look one place to the left (in Z) of 1 ~e, we see a perfect acceptor for 
</p>
<p>this electron: we have here 9F, whose n = 2 shell is all full except for one electron. 
</p>
<p>So when 11Na and 9F get together, Na passes on its electron to F and the system as 
</p>
<p>a whole lowers its energy, since the binding energy in F is 17.4 eV. Having carried 
</p>
<p>out the transfer, the atoms cannot part company, for they have now become charged 
</p>
<p>ions, Na + and F-, which are held together by electrostatic attraction, called the ionic 
</p>
<p>bond and form the NaF molecule. 
</p>
<p>Once we grasp that the chemical behavior is dictated by what is happening in 
</p>
<p>the outermost shell, we can see that several elements will have similar chemical 
</p>
<p>properties be.cause they have similar outer shells. For example, we expect all elements 
</p>
<p>with filled outer shells to be chemically inert. This is true. It is also true that some 
</p>
<p>elements with filled subshells are also inert, such as 18 Ar, in which just the 3s and 3p 
</p>
<p>subshells are filled. The origin of this inertness is the same as in the case with filled 
</p>
<p>shells: a spherically symmetric electronic charge distribution and a large excitation 
</p>
<p>energy. If we move one place to the right of the inert elements, we meet those that 
</p>
<p>behave like Na, i.e., eager to give up an electron, while if we move one place to the 
</p>
<p>left, we meet the likes ofF, eager to accept an electron. If we move two places to 
</p>
<p>the left, we see the likes of oxygen, which want two electrons, while two places to 
</p>
<p>the right we have elements like magnesium, which want to get rid of two electrons. 
</p>
<p>It follows that as we move in Z, we see a certain chemical tendency over and over 
</p>
<p>again. This quasiperiodic behavior was emphasized in 1869 by Mendeleev, who 
</p>
<p>organized the elements into a periodic table, in which the elements are arranged into 
</p>
<p>a matrix, with all similar elements in the same column. As we go down the first 
</p>
<p>column, for example, we see H, Li, Na, etc., i.e., elements with one electron to spare. 
</p>
<p>In the last column we see the inert elements, He, Ne, etc. Given the maxim that 
</p>
<p>happiness is a filled outer shell, we can guess who will interact with whom. For </p>
<p/>
</div>
<div class="page"><p/>
<p>instance, not only canNa give its electron to F, it can give to Cl, which is one shy 
of a filled 3p subshell. Likewise F can get its electron from K as well, which has a 
lone electron in the 4s state. More involved things can happen, such as the formation 
of H 20 when two H atoms get together with an oxygen atom, forming the covalent 
bond, in which each hydrogen atom shares an electron with the oxygen atom. This 
way all three atoms get to fill their outer shells at least part of the time. 
</p>
<p>There are many more properties of elements that follow from the configuration 
of the outer electrons. Consider the rare earth elements, 58Ce through 71 Lu, which 
have very similar chemical properties. Why doesn't the chemical behavior change 
with Z in this range? The answer is that in these elements the 6s subshell is filled 
and the 4f subshell, deep in the interior (but of a higher energy), is being filled. Since 
what happens in the interior does not affect the chemical properties, they all behave 
alike. The same goes for the actinides, 9&lt;Th to 103Lw, which have a filled 7s subshell 
and a 5/ subshell that is getting filled up. 
</p>
<p>Since we must stop somewhere, let us stop here. If you want to know more, 
you must consult books devoted to the subjecd 
</p>
<p>Exercise 13.4.1. * Show that if we ignore interelectron interactions, the energy levels of 
a multielectron atom go as Z 2 . Since the Coulomb potential is Zejr, why is the energy rx:Z2? 
</p>
<p>Exercise 13.4.2. * Compare (roughly) the sizes of the uranium atom and the hydrogen 
atom. Assume levels fill in the order of increasing n, and that the nonrelativistic description 
holds. Ignore interelectron effects. 
</p>
<p>Exercise 13.4.3. * Visible light has a wavelength of approximately 5000 A. Which of the 
series-Lyman, Balmer, Paschen--do you think was discovered first? 
</p>
<p>t See, for a nice trip through the periodic table, U. Fano and L. J:&lt;ano, Basic Physics of Atoms and 
Molecules, Chapter 18, Wiley, New York (1959). 
</p>
<p>371 
</p>
<p>THE 
HYDROGEN 
</p>
<p>ATOM </p>
<p/>
</div>
<div class="page"><p/>
<p>14 
</p>
<p>Spin 
</p>
<p>14.1. Introduction 
</p>
<p>In this chapter we consider a class of quantum phenomena that cannot be 
handled by a straightforward application of the four postulates. The reason is that 
these phenomena involve a quantum degree of freedom called spin, which has no 
classical counterpart. Consequently, neither can we obtain the spin operator by 
turning to Postulate II, nor can we immediately write down the quantum Hamil-
tonian that governs its time evolution. The problem is very important, for most 
particles-electrons, protons, neutrons, photons-have the spin degree of freedom. 
Fortunately the problem can be solved by a shrewd mixture of classical intuition 
and reasoning by analogy. In this chapter we study just electron spin. The treatment 
of the spins of other particles is quite similar, with the exception of the photon, 
which moves at speed c and can't be treated nonrelativistically. Photon spin will be 
discussed in Chapter 18. 
</p>
<p>In the next three sections we address the following questions: 
(I) What is the nature of this new spin degree of freedom? 
(2) How is the Hilbert space modified to take this new degree of freedom into 
</p>
<p>account? What do the spin operators look like in this space (kinematics of spin)? 
(3) How does spin evolve with time, i.e., how does it enter the Hamiltonian 
</p>
<p>(dynamics of spin)? 
</p>
<p>14.2. What is the Nature of Spin? 
</p>
<p>The best way to characterize spin is as a form of angular momentum. It is, 
however, not the angular momentum associated with the operator L, as the following 
experiment shows. An electron is prepared in a state of zero linear momentum, i.e., 
in a state with a constant (space-independent) wave function. As the operators Lx, 
Ly, and Lz will give zero when acting on it, our existing formalism predicts that if 
the angular momentum along, say the z direction, is measured, a result of zero will 
obtain. The actual experiment, however, shows that this is wrong, that the result is 373 </p>
<p/>
</div>
<div class="page"><p/>
<p>374 
</p>
<p>CHAPTER 14 
</p>
<p>&plusmn;lij2J It follows that the electron has "intrinsic" angular momentum, not associated 
</p>
<p>with its orbital motion. This angular momentum is called spin, for it was imagined 
</p>
<p>in the early days that if the electron has angular momentum without moving through 
</p>
<p>space, then it must be spinning like a top. We adopt this nomenclature, but not the 
</p>
<p>mechanical model that goes with it, for a consistent mechanical model doesn't exist. 
</p>
<p>Fortunately one can describe spin and its dynamics without appealing to any model, 
</p>
<p>starting with just the observed fact that it is a form of angular momentum. Let us 
</p>
<p>now develop the formalism that deals with spin and, in particular, allows us to 
</p>
<p>understand the above experiment. 
</p>
<p>14.3. Kinematics of Spin 
</p>
<p>The discussion following the general solution to the eigenvalue problem of 
</p>
<p>angular momentum (Section 12.5) suggests the way for treating particles with 
</p>
<p>intrinsic angular momentum or spin. Recall that if a particle is described by a wave 
</p>
<p>function with many (n) components, the generator of infinitesimal rotation is not 
</p>
<p>just L but something more. The reason is that under an infinitesimal rotation two 
</p>
<p>things happen to the wave function: ( 1) the values at each spatial point are re-
</p>
<p>assigned to the rotated point, and (2) the components of the wave function get 
</p>
<p>transformed into linear combinations of each other. 
</p>
<p>The differential operator L does part (1 ), while an n x n matrix S is responsible 
</p>
<p>for part (2). 
By generalizing our findings from Exercise 12.5.1 to an n component wave 
</p>
<p>function in three dimensions, we can say that under an infinitesimal rotation around 
</p>
<p>the z axis, the wave function is transformed as follows: 
</p>
<p>(14.3.1) 
</p>
<p>where S= is an n x n matrix. In abstract form, this equation reads&sect; 
</p>
<p>llJt') = [1- i: (L=+ S=) }I'&gt; 
</p>
<p>=[I-i: J=}ll'&gt; ( 14.3.2) 
We identify J=, the generator of infinitesimal rotations about the z axis, as the 
</p>
<p>z component of angular momentum. We see it has two parts: 
</p>
<p>t In practice one measures not the angular momentum, but a related quantity called magnetic moment. 
More on this later. Also spin was first discovered on the basis of spectroscopic evidence and not from 
</p>
<p>an experiment of the above type. 
</p>
<p>&sect;The spin operators will be denoted by the same symbol (S) whether they are referred to in the abstract 
</p>
<p>or as matrices in some basis. </p>
<p/>
</div>
<div class="page"><p/>
<p>and more generally 
</p>
<p>J=L+S (14.3.3) 
</p>
<p>Our problem is to find the number (n) of components appropriate to the electron 
and the three spin matrices that rotate its components. We proceed as follows. 
</p>
<p>Since J; are generators of rotations, they must obey the consistency condition 
</p>
<p>[J;, Jj] =ill L Gijk&middot;h 
k 
</p>
<p>(14.3.4) 
</p>
<p>Since LandS act on different parts of the wave function (the former on x, y, z, the 
latter on the indices i= 1, ... , n) they commute, and we may infer from Eq. (14.3.4) 
that 
</p>
<p>(14.3.5) 
</p>
<p>Using the known commutation rules of the L;, we deduce 
</p>
<p>[S;, sj] = i1i I sij"s" (14.3.6) 
k 
</p>
<p>Now recall that in Chapter 12 we found matrices Jx, JY, and Jz [Eqs. (12.5.22)-
(12.5.24)] that obey precisely these commutation relations. But these matrices were 
infinite dimensional. However, the infinite-dimensional matrices were built out of 
(2j+ 1) x (2j+ 1) blocks, withj=O, 1/2, 1, 3/2, ... , and the commutation relations 
were satisfied block by block. So which block shall we pick for the electron spin 
operators? The answer is given by the empirical fact that Sz has only the eigenvalues 
&plusmn;1ij2. This singles out the 2 x 2 blocks in Eqs. (12.5.22)-(12.5.24): 
</p>
<p>1i [0 -i] 
Sy=l i 0 ' (14.3.7) 
</p>
<p>Thus, the way to describe the electron is through a two-component wave function 
called a spinor: 
</p>
<p>If/= [If! +(x, y, z) J 
lfl-(x, y, z) 
</p>
<p>(14.3.8a) 
</p>
<p>(14.3.8b) 
</p>
<p>If If!_= 0, If!+# 0, we have an eigenstate of S= with eigenvalue 1ij2; if If!_# 0, If/+= 
0, the S= eigenvalue is ( -1ij2). 
</p>
<p>375 
</p>
<p>SPIN </p>
<p/>
</div>
<div class="page"><p/>
<p>376 
</p>
<p>CHAPTER 14 
</p>
<p>Let us now proceed to interpret the experiment mentioned earlier. Since we 
</p>
<p>prepared a state of zero momentum, we want the operator P to give zero when 
</p>
<p>acting on ljl. The operator P simply differentiates both components of V': 
</p>
<p>[ 
-ifzV 
</p>
<p>p ..... 
0 
</p>
<p>We deduce from P llJI) =0, i.e., 
</p>
<p>(14.3.9) 
</p>
<p>(14.3.10) 
</p>
<p>that ljl + and lj/- are independent of x, y, and :::. It follows that L= acting on 111 gives 
</p>
<p>zero. However, Sz doesn't: there is an amplitude IJI&plusmn; for obtaining &plusmn;fi/2. 
</p>
<p>The electron spinor is a two-component object, which puts it between a scalar, 
</p>
<p>which has one component, and a vector, which has three. However, the components 
</p>
<p>of the spinor are complex. 
</p>
<p>A significant difference between spin and orbital angular momentum is this: we 
</p>
<p>can change the magnitude of orbital angular momentum of a particle (by applying 
</p>
<p>external fields) but not the magnitude of its spin. The S2 operator is 
</p>
<p>0 - l fz2 I J [
. 
</p>
<p>(2)d+l) - 4 0 
(14.3.11) 
</p>
<p>and yields a value ~11 2 on any state lJI. [For any particle, the magnitude of spin is 
decided by the number of components in the wave function and is an invariant. Thus 
</p>
<p>the spin of the electron is always 1/2 (in units of fi) and serves as an invariant label 
</p>
<p>of the particle, like its charge or rest mass.] 
</p>
<p>We have deduced that the electron is to be described by a two-component wave 
</p>
<p>function in the coordinate basis.+ Let us restate this result in Hilbert space. First, it 
</p>
<p>is easy to see that the introduction of spin has doubled the size of Hilbert space; if 
it was oo dimensional before, now it is 2c:o dimensional, if you know what I mean. 
The basis vectors lxyzs=&gt; diagonalize the mutually commuting operators X, Y, Z, 
</p>
<p>and So (one can also think of other bases such as IPso) or Ips,) etc.). The state vector 
</p>
<p>t We made the deduction given the empirical input from experiment. When we come to the Dirac equation. 
we will see that incorporating relativistic kinematics will automatically lead to a multicomponenl wave 
</p>
<p>function. i.e .. lead to spin, if we demand that the equation be first order in time and space. </p>
<p/>
</div>
<div class="page"><p/>
<p>llfl) is a 2co-dimensional column vector in this basis: 
</p>
<p>lf!(x= -:-co, y= -co, z= -co, s== +1ij2) 
</p>
<p>lf!(X, y, _z, s= = +li/2) 
</p>
<p>lfi(X =co, y =co, z =co, s= = +li/2) 
(14.3.12) llf/) ---&gt; (xyzs= llf/) = 
</p>
<p>R.S= basis 
lfl(x =-:-co, y =-co, z =-co, s= = -1ij2) 
</p>
<p>lf!(X, y, _z, Sz = -1ij2) 
</p>
<p>lf!(x= co, y= co, z= co, s== -1ij2) 
</p>
<p>Clearly lf/(r, &plusmn;1ij2) gives the amplitude to find the electron at r with Sz = &plusmn;1ij2. The 
horizontal dashed line separates the components with s= = 1ij2 from those with s= = 
-1ij2. Now if s= is fixed at li/2 and we vary x, y, z from -co to co, the component 
of llf/) will vary smoothly, i.e., define a continuous function 1f1 +(x, y, z). Likewise 
the components below the dotted line define a function lfl-(x, y, z). In terms of these 
functions, we may compactify Eq. (14.3.12) to the form 
</p>
<p>I) [lfl+(x,y,z)] 
lfl R,S, basis lf/-(X, y, z) 
</p>
<p>(14.3.13) 
</p>
<p>This notation blends two notations we have used so far: if the vector has components 
labeled by a discrete index i (i= 1, ... , n) we denote it as a column vector, while if 
it is labeled by a continuous index such as x, we denote it by a function lf!(x); but 
here, since it depends on discrete (sz) as well as continuous (x, y, z) indices, we write 
it as a column vector whose components are functions. The normalization condition 
IS 
</p>
<p>1 = ( lf/1 lfl) = ~ I ( lf/1 xyzs=) (xyzs= I lfl) dx dy d: 
</p>
<p>=I (llf/+1 2 + llf/-1 2) dx dy dz 
</p>
<p>In the compact notation, S= is a 2 x 2 matrix: 
</p>
<p>S.l &gt; !!_[1 O][lfl+(x,y,z)] 
. lfl R,S,basis 2 0 -1 lfl-(x,y, z) 
</p>
<p>(14.3.14) 
</p>
<p>(14.3.15a) 
</p>
<p>377 
</p>
<p>SPIN </p>
<p/>
</div>
<div class="page"><p/>
<p>378 
</p>
<p>CHAPTER 14 
</p>
<p>whereas in its full glory, it is a 2cc -dimensional matrix: 
</p>
<p>n 
S-Ill') -------&bull; --
</p>
<p>- RS::: bas1s 2 -1 
</p>
<p>0 
</p>
<p>o 1!'(-w,n/2) 
</p>
<p>-1 
</p>
<p>-I l!'(cc, -n/2) 
</p>
<p>(14.3.15b) 
</p>
<p>What about the familiar operators n(R, P)? Equation (14.3.9) gives Pin the compact 
</p>
<p>notation. Likewise, L= becomes 
</p>
<p>I ) __ [-ifi/3/c&cent; 0 J[V'+(x,y,z)J 
L= II' R,S,basis 0 -ifit3/D&cent; 1/f-(X,y,z)_ 
</p>
<p>( 14.3.16) 
</p>
<p>The forms of these operators are consistent with the requirement that operators 
</p>
<p>built out of R and P commute with the spin operators. Observe that the Hilbert 
</p>
<p>space We of the electron may be viewed as a direct product of an infinite-dimensional 
</p>
<p>space W 0 , which describes a particle with just orbital degrees of freedom, and a two-
</p>
<p>dimensional space W,., which describes a particle with just spin degrees of freedom: 
</p>
<p>(14.3.17) 
</p>
<p>The basis vector I x, y, z, so) of We is just a direct product 
</p>
<p>ix, J\ z, = lxyz)01s =I /2, so) (14.3.18) 
</p>
<p>Of course W 0 and V, do not describe two particles which are amalgamated into a 
single system, but, rather, two independent degrees of freedom of the electron. 
</p>
<p>Since we already know how to handle the orbital degrees of freedom, let us 
</p>
<p>pretend from now on that only the spin degree of freedom exists. Or, to be more 
</p>
<p>precise, let us assume the orbital degree of freedom exists but evolves independently. 
</p>
<p>Formally this means that the Hamiltonian is separable: 
</p>
<p>ll=Ho+lf,. (14.3.19) </p>
<p/>
</div>
<div class="page"><p/>
<p>where H0 and Hs depend on just the orbital and spin operators, respectively. Conse-
quently the state vector factorizes intot 
</p>
<p>llf/(t)) = llf/o(t))&reg;lxs(t)) (14.3.20) 
</p>
<p>where llf/o) and IXs) are elements of '\1/o and '\\Is, respectively. Now llf/o(t)) evolves 
in response to H0 , while the evolution of lxs(t)) is dictated by Hs. We will follow 
just the evolution of IXs). The product form of IV') ensures that the spin and orbital 
degrees of freedom are statistically independent. Of course, there are many interesting 
cases in which H is not separable, and the orbital and spin degrees are coupled in 
their evolution. We will tackle them in a later chapter. 
</p>
<p>With this assumption, we have just a (complex) two-dimensional Hilbert space 
'\\Is to work with. A complete basis is provided by the vectors Is, sz) =Is, m1i) =Is, m). 
They are 
</p>
<p>ls,m)=ll/2, 1/2)~[~] (14.3.21a) 
</p>
<p>ls,m)=ll/2, -1/2)~ [~] (14.3.21b) 
</p>
<p>Any ket IX) in '\\Is may be expanded as 
</p>
<p>lx)=a11/2, 1/2)+PI1/2, -1/2)--::-&gt;[a] 
S:: basts {3 
</p>
<p>(14.3.22) 
</p>
<p>The normalization condition is 
</p>
<p>(14.3.23) 
</p>
<p>If one calculates (S) in the eigenstates of Sz, one finds 
</p>
<p>(1/2, &plusmn;1/21811/2, &plusmn;1/2) = &plusmn;(1i/2)k (14.3.24) 
</p>
<p>One refers to these as states with spin pointing up/down the z axis. More generally, 
the eigenstates In,&plusmn;) of n&middot; s with eigenvalues &plusmn;1i/2, in which 
</p>
<p>&lt;n, &plusmn;I Sin,&plusmn;)= &plusmn;(1i/2)n (14.3.25) 
</p>
<p>are said to be states with spin up/down the direction of the unit vector fl. Let us 
address the determination (in the S= basis) of the components of In,&plusmn;) and the 
verification of Eq. (14.3.25). 
</p>
<p>tIn the R, Sz basis, this means 1j!(x, y, z, s" t) = 'l'o(x, y, z, t)z(t) where x is a two-component spinor 
independent of x, y, and z. 
</p>
<p>379 
</p>
<p>SPIN </p>
<p/>
</div>
<div class="page"><p/>
<p>380 
</p>
<p>CHAPTER 14 
</p>
<p>Let us say fi points in the direction ( (!, &cent; ), i.e., that 
</p>
<p>fi= =cos (! 
</p>
<p>fix= sin (! cos &cent; 
</p>
<p>fiy = sin (! sin &cent; 
</p>
<p>The kets I fi, &plusmn;) are eigenvectors of 
</p>
<p>sin(! e-'4&gt;] 
-cos(! 
</p>
<p>( 14.3.26) 
</p>
<p>(14.3.27) 
</p>
<p>It is a simple matter to solve the eigenvalue problem (Exercise 14.3.2) and to find 
</p>
<p>A = A+ =[cos(0/2) e-'4&gt;/2] 
lnup)-ln ) sin(0/2)e'4&gt;!2 
</p>
<p>A A [-sin((! /2) e -it/&gt;12 J 
In down) = In-) = cos((! 12) e;q,n 
</p>
<p>You may verify that as claimed 
</p>
<p>(fi&plusmn;ISifi&plusmn;) = &plusmn;(1i/2)(i sin (!cos&cent;+ j sin (!sin&cent;+ k cos 6) 
</p>
<p>= &plusmn;(1i/2)fi 
</p>
<p>( 14.3.28a) 
</p>
<p>(14.3.28b) 
</p>
<p>(14.3.29) 
</p>
<p>An interesting feature of 'Ills is that not only can we calculate (S) given a state, 
</p>
<p>but we can also go the other way, i.e, deduce the state vector given (S). This has 
</p>
<p>to do with the fact that any element of 'Ills has only two (complex) components a 
</p>
<p>and /3, constrained by the normalization requirement lal 2+ 1/31 2= 1, i.e., three real 
degrees of freedom, and (S) contains exactly three pieces of information. If we write 
</p>
<p>(S) as (1i/2)fi, then the corresponding ket is I fi, +) or if you want 1-fi, - ). Another 
</p>
<p>way to state this result is as follows. Instead of specifying a state by a and /3, we 
can give the operator fi &middot; S of which it is an eigenvector with eigenvalue 1i/2. An 
</p>
<p>interesting corollary is that every spinor in 'Ills is an eigenket of some spin operator 
</p>
<p>fi &middot; S with eigenvalue 1i/2. 
</p>
<p>Exercise 14.3.1. Let us verify the above corollary explicitly. Take some spinor with com-
</p>
<p>ponents a= Pte'"'' and f3 = P2 e'"''. From &lt;xlx&gt; =I, deduce that we can write Pt =cos(ll/2) 
and p2 =sin(ll/2) for some 0. Next pull out a common phase factor so that the spinor takes 
</p>
<p>the form in Eq. (14.3.28a). This verifies the corollary and also fixes fl. </p>
<p/>
</div>
<div class="page"><p/>
<p>So much for the state vectors in V,. How about the operators on this space? 
Let us commence with Sx, Sy, and Sz. It is convenient to introduce the Pauli matrices 
</p>
<p>o-, defined by 
</p>
<p>so that 
</p>
<p>!i 
S=-o-
</p>
<p>2 
(14.3.30) 
</p>
<p>(14.3.31) 
</p>
<p>It is worth memorizing these matrices. Here are some of their important properties. 
</p>
<p>( 1) They anticommute with each other: 
</p>
<p>(14.3.32) 
</p>
<p>(2) From the commutation rules for the spin operators S, we get, upon using 
</p>
<p>the anticommutativity of the Pauli matrices, 
</p>
<p>u xO"y = iu = and cyclic permutations (14.3.33) 
</p>
<p>(3) They are traceless 
</p>
<p>Tr ui = 0, i=.x,y,z ( 14.3.34) 
</p>
<p>(See Exercise 14.3.3 for the proof.) 
</p>
<p>(4) The square of any Pauli matrix equals 1: 
</p>
<p>ui=I (14.3.35) 
</p>
<p>or more generally, 
</p>
<p>(14.3.36) 
</p>
<p>Proof Since S= has eigenvalues &plusmn;n/2, it follows that 
</p>
<p>381 
</p>
<p>SPIN </p>
<p/>
</div>
<div class="page"><p/>
<p>382 
</p>
<p>CHAPTER 14 
</p>
<p>in this Hilbert space.t But since what we call the z axis is arbitrary, it must be true 
</p>
<p>that 
</p>
<p>or 
</p>
<p>or 
</p>
<p>( 
A S Jfi )( A Jfi ) n&middot; +2 n&middot;S-2 =0 
</p>
<p>A s )2 1i2 
(n- =-[ 
</p>
<p>4 
</p>
<p>(5) We can combine Eqs. (14.3.32) and (14.3.35) into 
</p>
<p>(6) Combining this relation with the commutation rules 
</p>
<p>[a" ay] = 2ia= and cyclic permutations 
</p>
<p>we may establish a very useful identity (Exercise 14.3.4): 
</p>
<p>(A &middot;a)(B&middot;a) =A&middot; BI+i(A x B) &middot;a 
</p>
<p>where A and B are vectors or vector operators that commute with a. 
</p>
<p>(7) Combining Eqs. (14.3.33), (14.3.34), and (14.3.35) we find that 
</p>
<p>i,j=x,y, z 
</p>
<p>Q.E.D. 
</p>
<p>(14.3.37) 
</p>
<p>( 14.3.38) 
</p>
<p>(14.3.39) 
</p>
<p>( 14.3.40a) 
</p>
<p>Let us view the identity, I, as the fourth Pauli matrix. If we call it a 0 , then 
</p>
<p>(a, f3 =x, y, z, O)&sect; (l4.3.40b) 
</p>
<p>This equation implies that the a a matrices are linearly independent. By this I mean 
</p>
<p>as usual that 
</p>
<p>for all a (14.3.41) 
</p>
<p>To prove this for say cp, multiply both sides by ap and take the trace. 
</p>
<p>t See Exercise 12.5.4. 
&sect;From now on a, f3 will run over four values x, y, z, 0; while i, j will run over just x, y, and ~. </p>
<p/>
</div>
<div class="page"><p/>
<p>Since any 2 x 2 matrix M has only four independent (complex) degrees of 
</p>
<p>freedom, it may be written as 
</p>
<p>(14.3.42) 
</p>
<p>To find mfl, we multiply by a fl and take the trace, to find 
</p>
<p>(14.3.43) 
</p>
<p>(The coefficients rna will be complex in general, and real if ,&gt;yf is Hermitian.) 
</p>
<p>Thus, any operator in 'W s may be expressed in terms of the a a, which form a 
basis that is orthonormal with respect to the inner product ~ Tr(aaap).t 
</p>
<p>Explicit Forms of Rotation Operators 
</p>
<p>The fact that (n &middot; oY =I greatly simplifies many computations and allows us to 
compute in closed form several operators such as U(t)=exp(-iHt/fz), U[R(O)]= 
</p>
<p>exp( -iO &middot; S/fz), which are intractable in infinite-dimensional spaces. In this section 
we consider the rotation operators, and in the next, the propagator. 
</p>
<p>Consider 
</p>
<p>U[R(O)) = exp( -i0&middot;S/1i) =exp( -iEhr/2) 
</p>
<p>Grouping together the coefficients of I and {) &middot; cr, we get 
</p>
<p>U[R(O)] =cos( e /2)!- i sin( e /2)0 &middot;a (14.3.44) 
</p>
<p>Let us put this operator to a test. Suppose we have a particle with spin up along 
</p>
<p>the z direction, i.e., in the state[~]. If we want to get from this a particle in the state 
In, + ), it is clear that we must rotate [b] by an angle e about an axis perpendicular 
to the z axis and the n axis. Thus the rotation angle is 
</p>
<p>(14.3.45) 
</p>
<p>i The inner product between two matrices M and M' acting on '\./,. is actually Tr(MA1't). However, the 
dagger is irrelevant for the Hermitian O''s. It is an interesting exercise to check that this inner product 
</p>
<p>obeys the three axioms. 
</p>
<p>383 
</p>
<p>SPIN </p>
<p/>
</div>
<div class="page"><p/>
<p>384 
</p>
<p>CHAPTER 14 
</p>
<p>where k is the unit vector along the z axis. Since fi =(sin 8 cos &cent;, sin 8 sin &cent;, cos 0), 
it follows that 
</p>
<p>(j = : 1 &middot; (-sine sin&cent;, sine cos&cent;, 0) = (&middot;&middot;&middot;sin&cent;, cos&cent;, 0) 
sm e 
</p>
<p>The rotation matrix is, from Eq. (14.3.44), 
</p>
<p>exp ('- ~~ iJ. (f)= cos ( e /2) ' ' [ 
\ 2 1 sin(0/2) e"" 
</p>
<p>-sin( e /2) e i&cent;] 
cos(e /2) 
</p>
<p>( 14.3.46) 
</p>
<p>(14.3.47) 
</p>
<p>According to our mnemonic, the first column gives the rotated version of [b]. 
</p>
<p>We see that it agrees with lrl, + &gt; given in Eq. (14.3.28) up to an overall phase. 
Here is a summary of useful formulas that were derived or simply stated: 
</p>
<p>1i 
S=--- (f 
</p>
<p>2 
</p>
<p>[a,, CTj] =2iL; CijkCTk 
k 
</p>
<p>Trcr,=O 
</p>
<p>(a,[j=x,y,z,O) 
</p>
<p>( o- &middot;) le') 18\-exp ---i-O&middot;G =cos(- Iisini_-;:;!B&middot;&lt;f 
2 / ,2 \ ... ! 
</p>
<p>(A &middot;&lt;f)(B&middot;a) =(A&middot; B)I+ i(A x B)&middot; a 
</p>
<p>Exercise 14.3.2. * (I) Show that the eigenvectors of a&middot; fi are given by Eq. (14.3.28). 
(2) Verify Eq. (14.3.29). 
</p>
<p>Exercise 14.3.3. &bull; Using Eqs. (14.3.32) and (14.3.33) show that the Pauli matrices are 
</p>
<p>traceless. 
</p>
<p>Exercise 14.3.4. * Derive Eq. (14.3.39) in two different ways. 
(!) Write a;&lt;:rJ in terms of [a,, a1 ]+ and [a;, a1 ]. 
</p>
<p>(2) Use Eqs. (14.3.42) and ( 14.3.43). </p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 14.1. In the figure, B is the magnetic field and 11 is the magnetic moment 
of the loop. The direction of the arrows in the loop is that of the current. 
</p>
<p>Exercise 14.3.5. Express the following matrix Min terms of the Pauli matrices: 
</p>
<p>M=[; i] 
Exercise 14.3.6. (I) Argue that In, +) = U[R(I/Jk)] U[R(9j)]Jsz= li/2). (2) Verify by 
</p>
<p>explicit calculation. 
</p>
<p>Exercise 14.3. 7. Express the following as linear combinations of the Pauli matrices 
and/: 
</p>
<p>(1) (J+iax) 112&bull; (Relate it to half a certain rotation.) 
(2) (2J+ O"x )- 1&bull; 
</p>
<p>(3) CJ"~I 
</p>
<p>Exercise 14.3.8. * (I) Show that any matrix that commutes with a is a multiple of the 
unit matrix. 
</p>
<p>(2) Show that we cannot find a matrix that anticommutes with all three Pauli matrices. 
(If such a matrix exists, it must equal zero.) 
</p>
<p>14.4. Spin Dynamics 
</p>
<p>Since the quest for the spin Hamiltonian is based on classical analogy, let us 
recall some basic ideas from classical magnetostatics. Consider a square loop 
(Fig. 14.1) carrying a current/, in a magnetic field B. From standard magnetostatics 
(force per unit length on a current-carrying conductor etc.) one can show that the 
torque on the loop is 
</p>
<p>where Jl, the magnetic moment, is given by 
</p>
<p>/&middot;A 
Jl=--e.L 
</p>
<p>c 
</p>
<p>(14.4.1) 
</p>
<p>(14.4.2) 
</p>
<p>where A is the area of the loop, c is the velocity of light, and e.L is a unit vector 
perpendicular to the plane of the loop.t The effect of T will be to rotate the loop 
until J1 and B are parallel. 
</p>
<p>Since we finally wish to address a quantum mechanical problem, it is preferable 
to summarize the interaction between the loop and the magnetic field in terms of 
</p>
<p>t The sense of e.L is related to the current flow by the right-hand rule. 
</p>
<p>385 
</p>
<p>SPIN </p>
<p/>
</div>
<div class="page"><p/>
<p>386 
</p>
<p>CHAPTER 14 
</p>
<p>the potential energy associated with the torque: If (} is the angle between Jl and B, 
</p>
<p>the interaction energy ist 
</p>
<p>&pound;'int= f T((})d(}= fpBsin8dO=-pBcos(}=-wB ( 14.4.3) 
</p>
<p>As we would expect, this energy is minimized, i.e., a stable configuration obtains, 
</p>
<p>when Jl and B are parallel. 
</p>
<p>Although we derived the above equations for a square loop, they are true for 
</p>
<p>any tiny planar loop, over whose extent B is constant. So we may apply it to the 
</p>
<p>following problem. Imagine a particle of mass m, charge q, moving in a circular 
</p>
<p>orbit of radius r. The current associated with this charge is 
</p>
<p>I= charge flow past any point in the circle per second 
</p>
<p>qv 
</p>
<p>2nr 
</p>
<p>and the magnetic moment has a magnitude 
</p>
<p>J.l =.!]!J_. nr2 = qvr =(..!f._) mvr=..!L &middot;I 
2nr c 2c 2mc 2mc 
</p>
<p>(14.4.4) 
</p>
<p>( 14.4.5) 
</p>
<p>where l is the magnitude of the angular momentum. Since Jl and I are parallel, 
</p>
<p>Jl =(..!f._) I 
2mc 
</p>
<p>(14.4.6) 
</p>
<p>The ratio of Jl to I is called the gyromagnetic ratio y. For the particle considered 
</p>
<p>above, 
</p>
<p>(14.4.7) 
</p>
<p>In the case of the current loop, it was stated that the effect of the torque T is to 
</p>
<p>cause 11 to align with B. This picture changes when Jl has its origin in angular 
</p>
<p>momentum, as is the case for the particle in question. In this case, T causes a 
</p>
<p>t This is not the full Hamiltonian (for it does not include the kinetic energy of the loop) but just the 
potential energy of interaction with the magnetic field. </p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 14.2. In a small time IJ.t, the tip of the I vector precesses by an 
</p>
<p>angle !J.&cent; around the magnetic field vector. 
</p>
<p>precession of J.1 around B. We may see this as follows (see Fig. 14.2). The equation 
of motion is 
</p>
<p>So in a small time !lt, 
</p>
<p>or 
</p>
<p>dl 
T=-=p. X B= y(l X B) 
</p>
<p>dt 
</p>
<p>Ill= y(l X B)!ll 
</p>
<p>M= y!B sin () !lt 
</p>
<p>Since Ill is perpendicular to I, the tip of the I vector moves by an angle 
</p>
<p>( -Ill )' !lcf&gt; = -. - = (- r B) !lt 
I sm () 
</p>
<p>i.e., precesses at a frequency 
</p>
<p>roo= -yB 
</p>
<p>Orbital Magnetic Moment in Quantum Theory 
</p>
<p>{14.4.8) 
</p>
<p>(14.4.9) 
</p>
<p>(14.4.10) 
</p>
<p>These ideas reemerge in the quantum theory. The Hamiltonian for a particle of 
mass m and charge q in a magnetic field is 
</p>
<p>(P-qA/cf IPI 2 q . l1AI 2 
H= ----~=----- (P&middot;A + A&middot;P) +---
</p>
<p>2m 2m 2mc 2mc2 
(14.4.11) 
</p>
<p>387 
</p>
<p>SPIN </p>
<p/>
</div>
<div class="page"><p/>
<p>388 
</p>
<p>CHAPTER 14 
</p>
<p>Let 
</p>
<p>A= i ( -yi+ xj) (14.4.12) 
</p>
<p>so that 
</p>
<p>VxA=B=Bk ( 14.4.13) 
</p>
<p>is constant and along the z axis. We will assume B is small and drop the last term 
</p>
<p>in H, quadratic in B. When the middle term acts on any I IJI), 
</p>
<p>(P&middot; A)l IJI)-+-tliV &middot; (AVt) 
</p>
<p>= -ifi[(V &middot; A)IJI +A&middot; VIJI] 
</p>
<p>= (-iliA&middot; V)IJI -+(A&middot; P)l Vt) 
</p>
<p>since V &middot;A= 0 here.t Thus the interaction Hamiltonian is 
</p>
<p>so that 
</p>
<p>q 
H =--&middot;&middot;(2A&middot;P) 
</p>
<p>mt 2mc 
</p>
<p>q B 
=--- (- YPx+XPy) 
</p>
<p>me 2 
</p>
<p>= - _!1_ L &middot; B = -11&middot; B 
2mc 
</p>
<p>(14.4.14) 
</p>
<p>(14.4.15) 
</p>
<p>exactly as in the classical case. (We use the same symbol 11 to denote the classical 
</p>
<p>variable and the quantum operator. We will occasionally violate our convention in 
</p>
<p>this manner, so as to follow other widely used conventions.) 
</p>
<p>If we project this relation along the z axis, we get 
</p>
<p>q qfi 
JJ.==&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot; L==- (0, &plusmn;1, &plusmn;2, ... ) 
</p>
<p>2mc 2mc 
</p>
<p>+ It is shown in Section 18.4 that A corresponding to a given B can always be chosen divergenceless. </p>
<p/>
</div>
<div class="page"><p/>
<p>The quantity qftj2mc is called the Bohr magneton of the particle. The electron Bohr 
magneton, simply called the Bohr magneton, has a magnitude 
</p>
<p>eft _ 
-~o.6x 10 8 eV/G 
2mc 
</p>
<p>(14.4.16) 
</p>
<p>where m is the mass of the electron and G stands for gauss. The nucleon Bohr 
magnet on is about 2000 times smaller: 
</p>
<p>eft _ 
--~0.3 x 10 11 eV/G 
2Mc 
</p>
<p>(14.4.17) 
</p>
<p>where M is the nucleon (proton or neutron)t mass. (The nucleon Bohr magneton 
is also called the nuclear Bohr magneton.) 
</p>
<p>It may be verified, by the use ofEhrenfest's theorem, that (L) precesses around 
the constant field B just as I would (Exercise 14.4.1). 
</p>
<p>Spin Magnetic Moment 
</p>
<p>Armed with all this knowledge, we now address the problem of how the electron 
interacts with an external magnetic field. We assume once again that there is a 
magnetic moment operator J1 associated with the spin angular momentum. Since any 
operator on V s is a linear combination of the identity and the spin operators, and 
since J1 is a vector operator, we conclude that 
</p>
<p>Jl=rs 
</p>
<p>where r is a constant. Since r= -ej2mc for the orbital case, let us write 
</p>
<p>Jl= g( -ej2mc)S 
</p>
<p>where g is a constant. We also assume that 
</p>
<p>ge 
H =-wB=-S&middot;B 
</p>
<p>mt 2mc 
</p>
<p>geft 
=-a&middot;B 
</p>
<p>4mc 
</p>
<p>(14.4.18a) 
</p>
<p>(14.4.18b) 
</p>
<p>(14.4.19) 
</p>
<p>The intrinsic magnetic moment due to spin is g/2 magnetons. Our present 
formalism does not tell us what g is; to find it we must confront the above H with 
experiment and hope that for some value of g it gives the right physics. This happens 
to be the case, and the experimental value for g is very close to 2. We assume 
</p>
<p>t Recall that these two are nearly equal: MPc2 =938.28 MeV, while M.c2 =939.57 MeV. 
</p>
<p>389 
</p>
<p>SPIN </p>
<p/>
</div>
<div class="page"><p/>
<p>390 
</p>
<p>CHAPTER 14 
</p>
<p>hereafter that 
</p>
<p>g=2 (14.4.20) 
</p>
<p>Thus the gyromagnetic ratio for spin is twice as big as for orbital angular momentum. 
</p>
<p>Why is g ~ 2? And why isn't it exactly equal to 2, which would be much prettier? 
</p>
<p>Our formalism doesn't tell us. But it is irresistible to digress and mention that the 
</p>
<p>Dirac equation, which we will discuss in Chapter 20, predicts that g = 2 exactly. 
</p>
<p>Quantum electrodynamics, which we will not discuss in this book, predicts that the 
</p>
<p>Dirac result will receive corrections that can be calculated in a power series in a, 
</p>
<p>the fine-structure constant. The physics behind the corrections is the following. Recall 
</p>
<p>that the interaction between the electron and other charged particles is mediated by 
</p>
<p>the exchange of photons. Occasionally, an electron will recapture the photon it 
</p>
<p>emitted. Between the emission and reabsorption, the system that originally contained 
</p>
<p>just the electron will contain an electron and the photon. If the magnetic moment 
</p>
<p>of the system is probed at this time, we can get a result that corresponds to g #- 2, 
</p>
<p>since the electron in the two-particle system has both spin and orbital angular 
</p>
<p>momentum. In fact, quantum electrodynamics predicts that what we call the electron 
</p>
<p>is really a superposition of states that contain one Dirac electron, a Dirac electron 
</p>
<p>and a photon, a Dirac electron, several photons, and several electron-positron pairs, 
</p>
<p>etc.t The reason the observed value of g is so close to the Dirac value of 2 is that 
</p>
<p>configurations of increasing complexity are suppressed by increasing powers of the 
</p>
<p>fine-structure constant in the superposition. Thus the simplest configuration, with 
</p>
<p>just the Dirac electron, will dominate the picture and the complicated states will 
</p>
<p>provide smaller and smaller corrections to the result g = 2. The corrections may be 
</p>
<p>calculated in a power series in a : 
</p>
<p>which has been evaluated to order a 3&bull; The result is&sect; 
</p>
<p>gtheory = 2[1.001159652140(&plusmn;28)] 
</p>
<p>where the error &plusmn;28 in the last two digits is mostly due to uncertainties in the value 
</p>
<p>of a itself and in the numerical evaluation of some of the integrals in the calculation. 
</p>
<p>In addition to higher-order corrections, this result also receives corrections due 
</p>
<p>to other interactions of the electron, i.e., due to its ability to exchange other quanta 
</p>
<p>such as the graviton. But these effects are negligible to the accuracy considered above. 
</p>
<p>The experimental value of g is II 
</p>
<p>gexp = 2[1.0011596521884(&plusmn;43)] 
</p>
<p>t The time-energy uncertainty relation allows the production of these particles for short times. 
&sect; T. Kinoshita and W. B. Lindquist, Phys. Rev. 042, 636, 1990. 
</p>
<p>11 R. S. VanDyck, P. B. Schwinberg, and H. G. Dehmelt, Phys. Rev. Lett. 59, 26, 1987. </p>
<p/>
</div>
<div class="page"><p/>
<p>in splendid agreement with theory. Feynman has pointed out that this is equivalent 
</p>
<p>to predicting and measuring the distance between New York and Los Angeles to 
within the width of a human hair! 
</p>
<p>The theoretical situation is bad for the nucleons. The reason is that these partici-
pate in strong interactions as well, i.e., can emit and absorb pions etc., and the 
counterpart of a is large ( ::::15). In other words, the state with just the Dirac particle 
no longer dominates, and the corrections are no longer tiny. We can of course 
measure g experimentally, and the result is (to two places) 
</p>
<p>Yproton = 5.6 (e/2Afc) 
</p>
<p>Yneutron =-3.8 (e/2Mc) 
</p>
<p>Dirac theory predicts r = e I M c or g = 2 for the proton and r = 0 for the neutral 
neutron. The nonzero y of the neutron reflects the fact that the neutron can be in 
a state that has particles with compensating electrical charges but not necessarily 
compensating magnetic moments. 
</p>
<p>Because of their large masses, the magnetic moments of the nucleons are negli-
gible compared to that of the e\ectron.t 
</p>
<p>Let us now return to the dynamics of spin in a magnetic field B. All we need 
from now on is the Hamiltonian 
</p>
<p>where 
</p>
<p>H=-wB=-yS&middot;B 
</p>
<p>----e&middot; 2 -----e 
r=--=-
</p>
<p>2mc me 
</p>
<p>Let IIJI(O)) be the initial state of the electron. The state at a later time is 
</p>
<p>I'!'( t)) = U( t) I'Jf(O) &gt; 
</p>
<p>where 
</p>
<p>U(t) = e&middot;-iflti~ = e +iyt(S&middot;IIJ!" 
</p>
<p>(14.4.21) 
</p>
<p>(14.4.22) 
</p>
<p>( 14.4.23) 
</p>
<p>Since exp(-i9&middot;S/1i) is the operator that rotates by 9, the effect of U(t) is clearly to 
rotate the state by an angle 
</p>
<p>9(t) = -yBt (14.4.24) 
</p>
<p>t End of digression. 
</p>
<p>391 
</p>
<p>SPIN </p>
<p/>
</div>
<div class="page"><p/>
<p>392 
</p>
<p>CHAPTER 14 
</p>
<p>It follows that (S) will precess around Bat a frequency ro0 = -rB. If this seems too 
</p>
<p>abstract, let us consider a concrete example. Let B be along the z axis: B = Bk. In 
</p>
<p>this case 
</p>
<p>Since a= is diagonal, 
</p>
<p>U(t) = exp(iytS=Bjfz) 
</p>
<p>= exp(im0taj2) (mo= yB) 
</p>
<p>Consider an electron that starts out in the state lfi, + &gt;: 
</p>
<p>in which case 
</p>
<p>, [cos(() /2) e -i&lt;/J/2 J 
llj/(O))=In, +)-+ sin(fJ/2) ei&lt;/J/2 
</p>
<p>[
cos(() /2) e -i(&lt;/J- root)/2] 
</p>
<p>jljl( t)) = U( t)jlji(O) )-+ sin(() / 2) ei(&lt;/J- ro0r)/2 
</p>
<p>i.e., ljJ decreases at a rate m0 &bull; 
</p>
<p>Paramagnetic Resonance 
</p>
<p>Consider a classical magnetic moment J1 in a field Bo = B0k. It will precess around 
B0 at a frequency 
</p>
<p>roo=-rBo 
</p>
<p>Suppose we view this process in a frame that is rotating at a frequency ro parallel 
</p>
<p>to B0 . In this rotating frame, the precession frequency will be 
</p>
<p>ro,= roo- ro = -rBo- ro = -y(Bo + roj r) ( 14.4.25) 
</p>
<p>Thus the effective field in this rotating frame will be 
</p>
<p>B,= Bo+ ro/r (14.4.26) </p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 14.3. The situation in the rotating frame. The effective magnetic 
field is 8,. The magnetic moment starts out along the z axis (but is 
slightly displaced in the figure for clarity) and precesses around 8,. 
The z component of the moment oscillates with an amplitude JJ sin2 a, 
where a is the opening angle of the cone. At resonance, 8, lies along 
the x axis and I' precesses in the plane normal to it. The amplitude of 
the JJz oscillation is then at its maximum value of JJ. 
</p>
<p>2 
Jl- OCt&amp; Q 
</p>
<p>8, 
</p>
<p>This result is valid even if ro and B0 are not parallel (Exercise 14.4.5). Consider now 
the problem of interest, where, in a nonrotating (lab) frame 
</p>
<p>B = B cos roti- B sin rotj + B0k (B&laquo;&lt;Bo) (14.4.27) 
</p>
<p>and at t=O, 
</p>
<p>J.L(O) = ,uk (14.4.28) 
</p>
<p>We womd like to find out the evolution of J.l(f). Since B depends on time, it proves 
advantageous to first consider the problem in a frame that rotates at the same 
frequency ro --= --rok as the tiny clockwise rotating field B. In this frame, the rotating 
component of B gets frozen (say along the x axis) and the constant component Bok 
gets reduced as per Eq. (14.4.26) so that the effective, time-independent field is 
</p>
<p>B,= Bi,+ (B0 - rojy)k (14.4.29) 
</p>
<p>where i, is the unit vector in the x direction in the rotating frame. (k = k, of course.) 
In this frame, J.l will precess around B, at a frequency 
</p>
<p>ro,=-yB, (14.4.30a) 
</p>
<p>where 
</p>
<p>(14.4.30b) 
</p>
<p>It is a simple matter to deduce from Fig. 14.3 that .U= oscillates as follows: 
</p>
<p>.uz(t) = .u cos2 a+ .u sin2 a cos ro,t 
</p>
<p>=.u-(o)[ (roo-rof + r2Fcosro,t J 
- (roo- ro )2 + Y2 F (roo- ro )2 + r 2 F 
</p>
<p>(14.4.31) 
</p>
<p>393 
</p>
<p>SPIN </p>
<p/>
</div>
<div class="page"><p/>
<p>394 
</p>
<p>CHAPTER 14 
</p>
<p>This formula for f.lz(t) applies in the lab frame as well, since f.l= is invariant under:; 
</p>
<p>rotations. As ro increases from 0, the z component of B, steadily decreases; a, the 
</p>
<p>opening angle of the cone, increases, and the amplitude of oscillation, f.1 sin2 a, grows. 
</p>
<p>At paramagnetic resonance, ro = ro 0 , B, = Bi,, a= n /2, the cone becomes a circle in 
</p>
<p>the y-z plane, and f.l= oscillates with the largest amplitude f.1 at a frequency yB. The 
</p>
<p>behavior for ro &gt; ro0 is obvious. 
What if we apply the rotating field at the resonance frequency, but for a time 
</p>
<p>r such that 
</p>
<p>yBr = n/2? 
</p>
<p>Such a pulse, called a 90&deg; pulse, will swing the magnetic moment into the x-y plane 
</p>
<p>(in either frame). Thereafter 1.1 will precess around B0k at the frequency ro0 in the 
</p>
<p>lab frame. If we apply a 180&deg; pulse, i.e., choose r such that 
</p>
<p>yBr=n 
</p>
<p>the pulse will reverse the sign of 1.1 and leave it pointing down the z axis, where it 
</p>
<p>will stay (in either frame). 
</p>
<p>These results for the classical moment 1.1 apply to the expectation value (1.1) in 
</p>
<p>the quantum problem, as you may verify by doing Exercise 14.4.1, where it is proved 
</p>
<p>in general, and Exercise 14.4.3, where the explicit verification in this case is discussed. 
</p>
<p>Negative Absolute Temperature (Optional Digression) 
</p>
<p>The absolute zero of temperature, 0 K, ( ~ -273&deg;C) is defined so that nothing 
</p>
<p>can be colder, yet here we speak of negative absolute temperatures! There is no 
</p>
<p>conflict, however, since we will see that negative temperatures are hotter than positive 
</p>
<p>temperatures! Before you give up all faith, let us quickly sort this thing out. 
</p>
<p>The absolute temperature Tis defined as follows: 
</p>
<p>13 =J_=~ as= a In Q(E) 
kT k aE oE 
</p>
<p>(14.4.32) 
</p>
<p>where j3 is the thermodynamic temperature, k is Boltzmann's constant, S=k In Q is 
</p>
<p>the entropy and Q(E) is the number of states available to the system as a function 
</p>
<p>of its energy. (Q depends on other variables, assumed to be fixed.) In most systems, 
</p>
<p>j3 is positive because adding energy only opens up more states and increases n. For 
instance, if we have a box of gas molecules, they all stay in the ground state at T= 
</p>
<p>0. So, S = k In n =kIn 1 = 0. As we pump in energy, they can occupy higher states, 
and S and n can increase without limit. 
</p>
<p>Consider now a collection of N spin-half particles sitting on some crystal lattice 
</p>
<p>which is immersed in a field B = B0k. Each magnetic moment (or spin) has two states 
</p>
<p>only, with energies E= &plusmn;f.1B0 , where f.1 is the magnitude of the magnetic moment. 
</p>
<p>At T= 0 K, all are in the ground state (1.1 parallel to B); n = 1, and S = 0. The system 
has a magnetic moment M = nJ.lk. If we pump in energy 2f.1Bo, one of the moments 
</p>
<p>can move to the upper energy state; there are N ways to pick the one that moves </p>
<p/>
</div>
<div class="page"><p/>
<p>up, so that n = Nand S = k In N. Clearly fJ and Tare positive. As we pump in more 
and more energy, S keeps growing until half are up and half are down. At this 
</p>
<p>point, s reaches a maximum, fJ =as I aE = 0, and T = +-en. The system has no mean 
magnetic moment along the z axis. Pumping in more energy only reduces S, with 
</p>
<p>more and more particles piling up in the upper state. So fJ and T become negative. 
Finally, when E= NJ-i.Bo, all moments are in the upper energy state (antiparallel to 
B), M =-N f-l.k, there is only one such state; n = 1 and S = 0. This corresponds 
to fJ = -oo, T=O-. Thus the sequence of temperatures is T= 
o+, ... , 300, ... , oo, -oo, ... , -300, ... , 0 . In terms of fJ, there is more continu-
ity: fJ= oo, ... , 0+, o-, ... , -oo. (We should have chosen ----{J as the temperature, 
for it rises monotonically from - oo to + XJ as we heat the system.) It should be clear 
that negative temperatures are hotter than positive temperatures since we go from 
</p>
<p>the latter to the former by pumping in energy. We can also see this by imagining a 
</p>
<p>system at T= -300 K brought in contact with identical system at T= +300 K. Since 
</p>
<p>the populations of the two systems are identical, except for the change, parallel &lt;---&gt; 
</p>
<p>antiparallel, they can increase their entropies by moving toward the state with equal 
</p>
<p>numbers up and down. In this process energy clearly flows from the negative tempera-
</p>
<p>ture system to the positive temperature system, i.e., the former is hotter. Also note 
</p>
<p>that the final equilibrium temperature is not 0 K but u:; K. 
</p>
<p>How does one prepare negative temperatures in the lab? One takes a sample 
</p>
<p>at room temperature, say at T= 300 K. It will have more moments parallel than 
antiparallel: 
</p>
<p>N(parallel) 
</p>
<p>N( anti parallel) 
( 14.4.33) 
</p>
<p>and a net magnetic moment M along the z axis. lf one applies a 180&deg; pulse, there 
</p>
<p>will be population inversion (parallel+-&gt; antiparallel), which amounts to a change in 
</p>
<p>the sign of fJ and T [see Eq. (14.4.33)]. The spin system cannot stay in this hot state 
(T=- 300 K) forever, because it is in contact with the lattice, which will eventually 
</p>
<p>cool it down to room temperature. 
</p>
<p>The return to thermal equilibrium is easier to observe if one applies a 900 pulse 
</p>
<p>which swings Minto the x-y plane. The temperature now is T= oo K, since Mo= 
0--+ N(parallel) = N(antiparalle1)--+ T= oo. Thus M, which will initially begin to pre-
cess around B = Bok, will eventually realign itself with B. The decay of its rotating 
components in the x-y plane may be observed as follows. Suppose the specimen is 
</p>
<p>a long cylinder whose axis lies in the x-y plane. If one winds a coil around it, the 
transverse (x-y) components of M, which simulate a bar magnet rotating in the x 
</p>
<p>y plane, will induce an oscillating voltage in the coil. The frequency of the (damped) 
</p>
<p>oscillation will be w0 and the half-life will be a time-r, called the transverse relaxation 
time.t 
</p>
<p>Exercise 14.4.1. * Show that if H = -yL&middot; B, and B is position independent, 
</p>
<p>d&lt;L) 
--= &lt;1-l X B)= &lt;11&gt; X B 
</p>
<p>dt 
</p>
<p>! The transverse components of M decay for other reasons, besides restoration of thermal equilibrium. 
SeeR. Schumacher, Magnetic Resonance, W. A. Benjamin. New York (!970). 
</p>
<p>395 
</p>
<p>SPIN </p>
<p/>
</div>
<div class="page"><p/>
<p>396 
</p>
<p>CHAPTER 14 
</p>
<p>Comparing this to Eq. (14.4.8), we see that &lt;11&gt; evolves exactly like Jl. Notice that this 
conclusion is valid even if B depends on time and also if we are talking about spin instead of 
</p>
<p>orbital angular momentum. A more explicit verification follows in Exercise 14.4.3. 
</p>
<p>Exercise 14.4.2. Derive (14.4.31) by studying Fig. 14.3. 
</p>
<p>Exercise 14.4.3. * We would like to study here the evolution of a state that starts out as 
(b) and is subject to the B field given in Eq. (14.4.27). This state obeys 
</p>
<p>d 
ili-IIJI(t))=HIIJI&gt; 
</p>
<p>dt 
(14.4.34) 
</p>
<p>where H = - yS &bull; B, and B is time dependent. Since classical reasoning suggests that in a frame 
rotating at frequency ( -rok) the Hamiltonian should be time independent and governed by 
</p>
<p>B, [Eq. (14.4.29)), consider the ket in the rotating frame, IIJI,(t)), related to IIJI(t)) by a 
</p>
<p>rotation angle rot: 
</p>
<p>(14.4.35) 
</p>
<p>Combine Eqs. (14.4.34) and (14.4.35) to derive Schrodinger's equation for IIJI,(t)) in the S= 
</p>
<p>basis and verify that the classical expectation is borne out. Solve for IIJI,{t)) = U,(t)IIJf,(O)) 
</p>
<p>by computing U,(t), the propagator in the rotating frame. Rotate back to the lab and show 
</p>
<p>that 
</p>
<p>[[ 
( ro,t) .wo-w . (ro,t)J +irot/2] cos- +c--sm- e 
</p>
<p>2 W, 2 
</p>
<p>IIJI(f)) s. basis 
. iyB . (w,t) -iror;2 
</p>
<p>-sm- e 
w, 2 
</p>
<p>(14.4.36) 
</p>
<p>Compare this to the state lfi, +) and see what is happening to the spin for the case Wo = w. 
</p>
<p>Calculate (Jlo{l)) and verify that it agrees with Eq. (14.4.31). 
</p>
<p>Exercise 14.4.4. At t = 0, an electron is in the state with s, = li/2. A steady field B = Bi, 
B= 100 G, is turned on. How many seconds will it take for the spin to flip? 
</p>
<p>Exercise 14.4.5. We would like to establish the validity of Eq. (14.4.26) when ro and Bo 
</p>
<p>are not parallel. 
(1) Consider a vector V in the inertial (nonrotating) frame which changes by AV in a 
</p>
<p>time At. Argue, using the results from Exercise 12.4.3, that the change as seen in a frame 
</p>
<p>rotating at an angular velocity ro, is AV- ro x VAt. Obtain a relation between the time deriva-
</p>
<p>tives of V in the two frames. 
(2) Apply this result to the case of I [Eq. (14.4.8)), and deduce the formula for the 
</p>
<p>effective field in the rotating frame. </p>
<p/>
</div>
<div class="page"><p/>
<p>Exercise 14.4.6 (A Density Matrix Problem). (1) Show that the density matrix for an 
</p>
<p>ensemble of spin-1/2 particles may be written as 
</p>
<p>p= ~(l+a&middot;o") 
</p>
<p>where a is a c-number vector. 
</p>
<p>(2) Show that a is the mean polarization, &lt;&lt;J). 
(3) An ensemble of electrons in a magnetic field B=Bk, is in thermal equilibrium at 
</p>
<p>temperature T. Construct the density matrix for this ensemble. Calculate (j1). 
</p>
<p>14.5. Return of Orbital Degrees of Freedom 
</p>
<p>Let us now put back the orbital degrees of freedom. The simplest case is when 
</p>
<p>H is separable: 
</p>
<p>(14.5.1) 
</p>
<p>so that the energy eigenstates factorize 
</p>
<p>IIJI) = llflo)01Xs) 
</p>
<p>An example is provided by the hydrogen atom, where the Coulomb interaction is 
</p>
<p>independent of spin: 
</p>
<p>H=Ho ( 14.5.2) 
</p>
<p>Here the spin is a constant in time, and all that happens is that we attach a constant 
</p>
<p>spinor x to the wave functions we found in Chapter 13. If we choose x to be an 
eigenstate of s=' we havet 
</p>
<p>lnlmm,.= 1/2)-&gt;IJI,,m(r, B, &lt;fi)X, [X+=[~ JJ 
</p>
<p>[ [OJJ x-= 
.l ... 
</p>
<p>( 14.5.3) 
</p>
<p>lnlmm,= -1/2)-&gt; IJinlm(r, e, &cent;)X .. 
</p>
<p>The energy levels are of course unaffected. All we have is a doubling of states, with 
the electron spin being up or down (the z axis) in each of the orbital states (nlm). 
</p>
<p>Consider next the problem of the hydrogen atom in a weak magnetic field B = 
Bk. Although both the proton and the electron couple to B, the smallness of the 
</p>
<p>ratio m/ Mallows us to ignore, in the first approximation, the coupling of the proton's 
intrinsic and orbital magnetic moments [these are of order m/M and (m/ M) 2 relative 
</p>
<p>:j: We use the subscript s on m, to remind us that it measures the ,1pin projection: s, '" m,n. It will be 
dropped whenever it is obvious that we are dealing with spin. 
</p>
<p>397 
</p>
<p>SPIN </p>
<p/>
</div>
<div class="page"><p/>
<p>398 
</p>
<p>CHAPTER 14 
</p>
<p>to that of the electron; see Exercise 14.5.1]. Thus we have, from Eqs. (14.4.14) and 
(14.4.19), 
</p>
<p>(
'-eB\ (i&middot;&middot;&middot;&middot;&middot;eB.) 
</p>
<p>JJ=Jfcoulumb- --&middot;&middot;&middot;&middot;&middot;- ILc- -- So 
.2mc J me; 
</p>
<p>(14.5.4) 
</p>
<p>Since the additional terms in H commute with Hcoulomb, L 2, L", and So, this His 
diagonalized by the same states as before, namely, lnlmms). The eigenvalues are, 
</p>
<p>however, different: 
</p>
<p>l&middot;-&middot;Ry eBti J Hlnlmm.)= --+-(m+2m) 'nlmm.) 
&middot;' n2 2mc s - I ' 
</p>
<p>(14.5.5) 
</p>
<p>The degeneracy is greatly reduced by the B field. The ground state, which was twofold 
</p>
<p>degenerate, splits into two levels: 
</p>
<p>etiB 
En~ I =-Rv&plusmn; 
</p>
<p>" 2mc 
</p>
<p>The second, which was eightfold degenerate, splits into five levels: 
</p>
<p>Ry eB!i 
En~2 =&middot;&middot;&middot;&middot;&middot;-+--X 
</p>
<p>4 2mc 
</p>
<p>2(m=l,m,=l 
</p>
<p>l(m=O, m,= 1/2)(1=0 or l) 
</p>
<p>O(m = 1, ms = -1 or m = -1, 1111 = 1 
-l(m =0, m,= -1/2) (1=0 or I) 
</p>
<p>&middot;&middot;&middot;&middot;&middot; 2( m = &middot;&middot;&middot;&middot;&middot; 1, m, = --I /2) 
</p>
<p>(14.5.6) 
</p>
<p>(14.5.7) 
</p>
<p>and so on. In a multielectron atom, one simply adds the contributions from all the 
</p>
<p>electrons. The splitting of levels leads to an increase in the number of spectral lines; 
</p>
<p>where there was one, there will now be several, and the spacing between them may 
</p>
<p>be varied by varying B. This phenomenon is called the Zeeman effect. 
</p>
<p>Consider lastly the Hamiltonian 
</p>
<p>H=Hcoulomb+aL&middot;S (14.5.8) 
</p>
<p>whose origin will be explained in a later chapter. For the present, we note that it is 
</p>
<p>not separable, and consequently the spin and orbital degrees of freedom are coupled 
in their time evolution. The eigenstates of H will not be simply products of orbital 
and spin parts, but instead superpositions of such states that diagonalize L &middot; S. The 
</p>
<p>details will be explained in the next chapter. 
</p>
<p>Exercise 14.5.1. &bull; (I) Why is the coupling of the proton's intrinsic moment to Ban order 
</p>
<p>mj M correction to Eq. (14.5.4 )'? 
(2) Why is the coupling of its orbital motion an order (m/ M )2 correction? (You may 
</p>
<p>reason classically in both parts.) </p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 14.4. The Stem-Gerlach 
</p>
<p>experiment. A beam of particles 
</p>
<p>endowed with magnetic moments 
</p>
<p>enters the inhomogeneous field. 
</p>
<p>Classically the beam is expected to 
</p>
<p>fan out and produce a continuous 
</p>
<p>trace (A) on the screen. What one 
</p>
<p>observes is a set of discrete dots 
</p>
<p>(B). This implies the quantization 
</p>
<p>of magnetic moment and angular 
</p>
<p>momentum. 
</p>
<p>z tcy 
X 
</p>
<p>Exercise 14.5.2. * ( 1) Estimate the relative size of the level splitting in the n = l state to 
the unperturbed energy of the n = 1 state, when a field B = 1 OOOkG is applied. 
</p>
<p>(2) Recall that we have been neglecting the order If term in H. Estimate its contribution 
in the n = 1 state relative to the linear ( -p &middot;B) term we have kept, by assuming the electron 
</p>
<p>moves on a classical orbit of radius a0 &bull; Above what IBI does it begin to be a poor 
approximation? 
</p>
<p>The Stern-Gerlach (SG) Experiment 
</p>
<p>We now consider (in simplified form) the SG experiment, which clearly displays 
</p>
<p>the quantization of angular momentum (along any direction). The apparatus (Fig. 
</p>
<p>14.4) consists of north and south pole pieces, between which is an inhomogeneous 
</p>
<p>magnetic field. A beam of (particles with) magnetic moments, traveling along they 
</p>
<p>axis, enters the apparatus in a region where B is predominantly along the z axis and 
</p>
<p>aBz/ az &lt; 0. What do we expect will happen classically? If we pretend that the magnetic 
</p>
<p>moment is due to a pair of equal and opposite (fictitious) magnetic charges, it is clear 
</p>
<p>that any inhomogeneity in B can lead to a net force on the dipole. This is confirmed 
</p>
<p>if we calculate the force associated with the gradient of the interaction energy 
</p>
<p>8Bz 
F= -VJf=V(p&middot;B)= CwV)B= JJza; k (14.5.9) 
</p>
<p>[We have used the identity V(w B)= (Jl" V)B + (B &middot; V)p + p x (V x B)+ B x (V x p). In 
the present case, pis not a function of r, and by Maxwell's equations, V x B=O. 
</p>
<p>Both Fx and Fy vanish on average due to the precession of spin in the x- y plane.] 
</p>
<p>Classically, since JJz is continuous, the beam is expected to fan out and produce a 
</p>
<p>continuous trace (A in figure) on a screen placed behind the magnet. The actual 
</p>
<p>experiment performed with atoms reveals a series of discrete dots (B in figure). We 
</p>
<p>understand this in semiclassical terms, by saying that JJz in Eq. (14.5.9) is discrete 
</p>
<p>and therefore so is the angular momentum along the z axis. 
This experiment can also be used to reveal the existence of electron spin. For 
</p>
<p>example, if we send in a beam of hydrogen atoms in their ground state, the beam 
</p>
<p>will split into two parts. 
</p>
<p>399 
</p>
<p>SPIN </p>
<p/>
</div>
<div class="page"><p/>
<p>400 
</p>
<p>CHAPTER 14 
</p>
<p>Let us describe the above-mentioned hydrogen atom experiment in quantum 
mechanical terms. Suppose the initial state of a hydrogen atom is 
</p>
<p>(14.5.10) 
</p>
<p>where lfl&bull;&middot; is a wave packet drifting along the y axis that describes the CM motion, 
If/too is the ground state wave function, and [ ~] is the electron spinor. (The proton 
spin is ignored, for the associated magnetic moment is too small to affect the dynam-
ics.) Since the electron spin is up, its Jl= is down. Since 8Bj8z&lt;O, the classical force 
on the atom is up. So by Ehrenfest's theoremt we expect the atom to emerge from 
the apparatus in a state (up to a phase factor) 
</p>
<p>lfl out= lfly.+=(rcM) lf/too(r{ ~ J (14.5.11) 
</p>
<p>where lfly.+= describes a wave packet that is displaced (relative to the incoming one) 
along the positive z axis and has also a small velocity in the same direction. Likewise, 
if the electron spin or had initially been [ ?J, the CM would have emerged in the state 
lf!_v.-= (in the same notation). More generally, if 
</p>
<p>(14.5.12) 
</p>
<p>then, by the linearity of Schrodinger's equation 
</p>
<p>(14.5.13) 
</p>
<p>Assuming 1f1 _v.&plusmn;= are narrow packets with no overlap, we see that the SG apparatus 
has introduced a correlation between the spin and orbital coordinates: if we catch 
(by placing a screen) the outgoing atom above the original line of flight (i.e., in a 
region where lf/_v,+= is peaked) it will have spin up, while if we catch it below, the 
spin is down. 
</p>
<p>The SG apparatus can be used to prepare a state of definite spin orientation: 
to get a pure spin up/down beam we simply block the lower/upper beam. But note 
that the filtering process changes the average z component oflinear momentum. This 
can be undone and the particle restored its original momentum (but filtered with 
respect to spin) if we place some more magnets (with B along the z axis) behind this 
apparatus. With this modification (which is assumed in the following exercises) the 
</p>
<p>:j: Recall the warning at the end of Chapter 6. In the present case, the system follows the classical trajectory 
(approximately) thanks to the massive proton. If we send in just the electron, quantum fluctuations 
would wipe out the effect. See, for example, pages 324-330 of G. Baym, Lectures on Quantum Mechanics, 
Benjamin, New York (1969). </p>
<p/>
</div>
<div class="page"><p/>
<p>only effect of the SG apparatus with one or the other beams blocked is to filter the 
</p>
<p>spin without affecting the orbital motion. 
</p>
<p>Exercise 14.5. 3. * A beam of spin-1 /2 particles moving along they axis goes through two 
collinear SG apparatuses, both with lower beams blocked. The first has its B field along the 
</p>
<p>z axis and the second has its B field along the x axis (i.e., is obtained by rotating the first by 
</p>
<p>an angle n /2 about they axis). What fraction of particles leaving the first will exit the second? 
If a third filter that transmits only spin up along the z axis is introduced, what fraction of 
</p>
<p>particles leaving the first will exit the third? If the middle filter transmits both spins up and 
</p>
<p>down (no blocking) the x axis, but the last one transmits only spin down the z axis, what 
</p>
<p>fraction of particles leaving the first will leave the last? 
</p>
<p>Exercise 14.5.4. A beam of spin-! particles, moving along they axis, is incident on two 
</p>
<p>collinear SG apparatuses, the first with B along the z axis and the second with B along the 
</p>
<p>z' axis, which lies in the x-z plane at an angle () relative to the z axis. Both apparatuses 
</p>
<p>transmit only the uppermost beams. What fraction leaving the first will pass the second? 
</p>
<p>401 
</p>
<p>SPIN </p>
<p/>
</div>
<div class="page"><p/>
<p>15 
</p>
<p>Addition of Angular Momenta 
</p>
<p>15.1. A Simple Example 
</p>
<p>Consider a system of two spin-1 /2 particles (whose orbital degrees of freedom 
we ignore). If S 1 and S2t are their spin operators, the two-particle Hilbert space 
vl@2 is spanned by the four vectors 
</p>
<p>(15.1.1) 
</p>
<p>which obey 
</p>
<p>(15.1.2a) 
</p>
<p>(i=l,2) ( 15.1.2b) 
</p>
<p>Since s; = l /2, and m; = &plusmn; 1/2 has freedom only in sign, let us use the compact notation 
1++), 1+-), 1-+), 1--) to denote the states. For instance, 
</p>
<p>(15.1.3) 
</p>
<p>and so on. These four vectors form the product basis. They represent states that have 
</p>
<p>well-defined values for the magnitude and z component of the individual spins. 
Suppose now that we choose not to look at the individual spins but the system 
</p>
<p>as a whole. What are the possibk values for the magnitude and z component of the 
system spin, and what are the s~ates that go with these values? This is a problem in 
addition of angular momenta, which is the topic of this chapter. 
</p>
<p>; In terms of the operators s['&gt; and Si2&gt; which act on the one-particle spaces, S, =S[ 11 &reg;Im and S,= 
J&lt;I&gt;&reg;s?'. 403 </p>
<p/>
</div>
<div class="page"><p/>
<p>404 
CHAPTER 15 
</p>
<p>Consider the operator 
</p>
<p>(15.1.4) 
</p>
<p>which we call the total angular momentum operator. That Sis indeed the total angular 
momentum operator is supported by (1) our intuition; (2) the fact that it is the 
generator of rotation for the product kets, i.e., rotations of the whole system; (3) the 
fact that it obeys the commutation rules expected of a generator of rotations, namely, 
</p>
<p>(15. 1.5) 
</p>
<p>as may be readily verified. Our problem is to find the eigenvalues and eigenvectors of 
S 2 and S=. Consider first 
</p>
<p>(15.1.6) 
</p>
<p>which commutes with si' sL S~:. and s2z&middot; We expect it to be diagonal in the 
product basis. This is readily verified: 
</p>
<p>(.n !i) S=I++)=(S1=+S2JI++)= 2+2
1 
</p>
<p>I++) 
</p>
<p>Szf+ )=01+-) (15.1.7) 
</p>
<p>S=l-+)=01-+) 
</p>
<p>Szf--)=-nl -) 
</p>
<p>Thus the allowed values for the total z component are n, 0, and -fi. 
By the method of images (or any other method) 
</p>
<p>++ ,- -+ 
</p>
<p>[ I 
</p>
<p>0 0 
</p>
<p>JJ 
0 0 0 (15.1.8) s _ ____. 1i 
</p>
<p>- prod~ct 0 0 0 
basts 
</p>
<p>0 0 0 
</p>
<p>Note that the eigenvalue .s&middot;= = 0 is twofold degenerate, and the eigenspace is spanned 
by the vectors i +-) and -+ ). If we form some linear combination, 
af&middot;t-----)+131-+), we still get an eigenstate with s==O, but this state will not have 
definite values for s~., and 82, (unless a or J3 = 0). 
</p>
<p>Consider next the operator 
</p>
<p>(15.1.9) </p>
<p/>
</div>
<div class="page"><p/>
<p>Although S 2 commutes with Si and S~, it does not commute with S1: and Szz 
</p>
<p>because of the S 1 &middot;S2 tenn, which has S1x, S1y, etc. in it. By explicit computation, 
</p>
<p>++ +- &middot;-+ 
</p>
<p>[ 2 
</p>
<p>0 0 
</p>
<p>n s2----&gt; n.z 0 (15.1.10) product 0 1 1 basis O 0 0 
Thus we see that although I++) and 1---) are eigenstates of S 2[s(s+ 1)=2], the 
states of zero Sz, namely, 1+-) and 1-+), are not. However, the following linear 
combinations are: 
</p>
<p>1+-)+1&middot;&middot;&middot;&middot;&middot;&bull;&middot;&gt; 
(s= 1) 
</p>
<p>(15.1.11) 
1+-)-1&middot;-&middot;+) 
</p>
<p>21/2 
(s=O) 
</p>
<p>Exercise 15.1.1. *Derive Eqs. (15.1.10) and (15.1.11). It might help to use 
</p>
<p>(15.1.12) 
</p>
<p>This completes the solution to the problem we undertook. The allowed values 
</p>
<p>for total spin are s =I and 0, while the allowed values of s= are 11., 0, and -11.. The 
corresponding eigenstates in the product basis are 
</p>
<p>Is= 1m= 1, S1 = 1/2 sz= 1/2)=1++) 
</p>
<p>Is= 1 m=O, Sj = 1/2 s2= 1/2) = T 112 [1 +-&gt; + 1-+ &gt; l 
Is= 1m= -1, s1 = 1/2 s2 = 1/2)= 1--) 
</p>
<p>ls=Om=O, sl=l/2sz=l/2)=2 112[1+-)-l-+)] 
</p>
<p>(15.1.13) 
</p>
<p>These vectors represent states with well-defined total angular momentum; they form 
</p>
<p>the total-s basis. The three spin-1 states are called triplets and the solitary spin-0 
</p>
<p>state is called the singlet. The problem of adding angular momenta is essentially a 
</p>
<p>change of basis, from one that diagonalizes (St' sL slz, Sz,) to one that diagonalizes 
(S 2 , S=, Sf, S~). We can describe our findings symbolically as 
</p>
<p>1/2@ 1/2= lEBO (15.1.14) 
</p>
<p>which means that the direct product of two spin-1 /2 Hilbert spaces is a direct sum 
</p>
<p>of a spin-! space and a spin-0 space. The way the dimensionalities work out in 
</p>
<p>405 
</p>
<p>ADDITION OF 
</p>
<p>ANGULAR 
</p>
<p>MOMENTA </p>
<p/>
</div>
<div class="page"><p/>
<p>406 
</p>
<p>CHAPTER 15 
</p>
<p>Eq. (15.l.14) is as follows: 
</p>
<p>left-hand side: (2s1 +1)(2s2 +1)=(2x l/2+1)(2x 1/2+1)=4 
I 
</p>
<p>right-hand side: I (2s+ 1)= 1 +3=4 ( 15.l.15) 
s=O 
</p>
<p>The decomposition of the direct product space into a sum over spaces with well-
defined total spin can also be viewed this way. The rotation operators for the entire 
system will be 4 x 4 matrices in the product basis. These matrices are, however, 
reducible: by changing to the total-s basis, they may be block diagonalized into a 
3 x 3 block (spin-! sector) and a 1 x 1 block (spin-0 sector). The total-s basis is, 
however, irreducible; we cannot further subdivide the spin-1 space into parts that 
do not mix under rotations. 
</p>
<p>The total-s states have another property: they have definite symmetry under the 
exchange of the two particles. The triplets are symmetric and the singlet is antisym-
metric. Now, the state vector for two identical spin-1/2 particles must be antisymmet-
ric under the exchange of particle labels, i.e., under the exchange of their spin and 
orbital degrees of freedom. We already know that ifQ is some orbital operator (built 
out of coordinates and momenta), then 
</p>
<p>and 
</p>
<p>are symmetric and antisymmetric, respectively, under the exchange of the orbital 
variable. To form the complete state vector, we simply multiply orbital and spin 
states of opposite symmetry: 
</p>
<p>( 15.1.16) 
</p>
<p>These vectors provide a complete basis for the Hilbert space of two identical spin-
! /2 particles. As an example, consider the ground state of the He atom, which has 
two electrons. In connection with the periodic table it was said that in this state of 
lowest energy, both electrons are in the lowest orbital state In= 1, /=0, m=O)t and 
</p>
<p>t If we neglect interelectron forces, the states allowed to the electrons are hydrogenlike, in that they are 
labeled In, I, m). But the energies and wave functions are obtained upon making the replacement 
e2 -&gt;Ze2 = 2e2. </p>
<p/>
</div>
<div class="page"><p/>
<p>have opposite spins. We can sharpen that statement now. The orbital part of the 
</p>
<p>ground-state ket is just the direct product, 
</p>
<p>llf/o) = !100)(8)!100) (15.1.17) 
</p>
<p>which is already symmetric. So the spin part must be 
</p>
<p>1Xs)=2 112(!+-)-! (15.1.18) 
</p>
<p>and so 
</p>
<p>llf! ground)= llfl,.) &reg;I X,) (15.1.19) 
</p>
<p>In this state, both the orbital and spin angular momenta are zero. 
</p>
<p>Let us now return to the problem of just the two spins (and no orbital coordi-
nates). Now that we have two bases, which one should we use? The answer depends 
</p>
<p>on the Hamiltonian. For instance, if the two spins only interact with an external 
</p>
<p>field B = B0k, 
</p>
<p>(15.1.20) 
</p>
<p>the product basis, which diagonalizes S1, and S2= is the obvious choice. (If, however, 
</p>
<p>Y1 = y2, then HocS=, and we can use the total-s basis as well.) On the other hand, 
if the spins are mutually interacting and, say, 
</p>
<p>(15.1.21) 
</p>
<p>the total-s basis diagonalizes H. 
</p>
<p>Exercise 15.1.2. * In addition to the Coulomb interaction, there exists another, called the 
hyperfine interaction, between the electron and proton in the hydrogen atom. The Hamiltonian 
</p>
<p>describing this interaction, which is due to the magnetic moments of the two particles is, 
</p>
<p>(A&gt;O) ( 15.1.22) 
</p>
<p>(This formula assumes the orbital state of the electron is 11, 0, 0).) The total Hamiltonian is 
</p>
<p>thus the Coulomb Hamiltonian plus Hhf&middot; 
</p>
<p>( 1) Show that Hhf splits the ground state into two levels: 
</p>
<p>1i2A 
E+=-Ry+-&middot; 
</p>
<p>4 
</p>
<p>11i2 A 
&pound;_=-Ry--&middot;-
</p>
<p>4 
</p>
<p>and that corresponding states are triplets and singlet, respectively. 
</p>
<p>( 15.1.23) 
</p>
<p>407 
</p>
<p>ADDITION OF 
</p>
<p>ANGULAR 
</p>
<p>MOMENTA </p>
<p/>
</div>
<div class="page"><p/>
<p>408 
</p>
<p>CHAPTER 15 
</p>
<p>(2) Try to estimate the frequency of the emitted radiation as the atom jumps from the 
triplet to the singlet. To do so, you may assume that the electron and proton are two dipoles 
J.le and J.lp separated by a distance a0 , with an interaction energy of the ordert 
</p>
<p>Show that this implies that the constant in Eq. (15.1.22) is 
</p>
<p>2e (5.6)e I 
A~~~--
</p>
<p>2mc 2Mc a~ 
</p>
<p>(where 5.6 is the g factor for the proton), and that 
</p>
<p>is a correction of order (m/ M)a 2 relative to the ground-state energy. Estimate that the 
frequency of emitted radiation is a few tens of centimeters, using the mnemonics from Chapter 
13. The measured value is 21.4 em. This radiation, called the 21-cm line, is a way to detect 
hydrogen in other parts of the universe. 
</p>
<p>(3) Estimate the probability ratio P(triplet)/ P(singlet) of hydrogen atoms in thermal 
equilibrium at room temperature. 
</p>
<p>15.2. The General Problem 
</p>
<p>Consider now the general problem of adding two angular momenta J 1 and J 2 &bull; 
What are the eigenvalues and eigenkets of J 2 and J=, where J = J 1 + J2? One way to 
find out is to mimic the last section: construct the (2jt + I) &middot; (2jz + I )-dimensional 
matrices J 2 and J= and diagonalize them. Now, J= will be diagonal in the product 
basis itself, for 
</p>
<p>(15.2.1) 
</p>
<p>It will be a degenerate operator, for there are many ways to build up a total m = 
m1 + m2 , except when m = &plusmn;(jt + j 2) when both angular momenta have maximal pro-
jections up/down the z axis. For instance, if m = h + jz- 2, there are three product 
kets: (m 1 =j1 ,m2 =jz-2), (mt=j1 -l,m2=jz-l), and (mt=jt-2,m2=jz). In each 
of the degenerate eigenspaces of J=, we must choose a basis that diagonalizes f 
(and undiagonalizes J1= and J2=). We can do this by constructing the matrix J 2 and 
then diagonalizing it. But this can be a tedious business. (If you have done Exercise 
15.1.1 you will know that the construction of S 2 is quite tedious even in this four-
dimensional case.) There is, however, a more efficient alternative to be described 
now. 
</p>
<p>As a first step, we need to know the allowed values for j. Our intuition and our 
experience from the last section suggest that j can take on values jt + h, 
</p>
<p>+ The description here is oversimplified; both .Yt,.f and Hhf are rather tricky to derive. Our aim is just to 
estimate IAI and not to get into its precise origin. </p>
<p/>
</div>
<div class="page"><p/>
<p>} 1 + }2 -1, ... ,}1 - h (assuming }1 ;;::}2).t Let us check this. The number of product 
kets is (2}1 + 1) &middot; (2}2 + 1 ). This must equal the number of total-} kets. According to 
</p>
<p>our conjecture, this number is 
</p>
<p>il+h h+h it-Jz-1 
</p>
<p>:L (2J+l)= :L (2J+l)- :L (2J+l)=(2Jt+1)(2h+l) (15.2.2) 
</p>
<p>using the formula 
</p>
<p>~ n=N(N+ 1) 
</p>
<p>n~o 2 
</p>
<p>We take this to be proof of our conjecture: 
</p>
<p>In other words, the total-} kets are 
</p>
<p>Let us write them in the form of an array: 
</p>
<p>j 
</p>
<p>m~}2 
! lit+h,}t+h&gt; 
</p>
<p>lit+h,}t+iz-1) 
</p>
<p>lit+ }2,}1 + }2-2) 
</p>
<p>lit+}2-1,}t+h-l) 
</p>
<p>lit +}2-l,}t +}2-2) 
</p>
<p>lit+h,-UI+h-2)) 1}!+}2-1,-(}t+h-2)) 
</p>
<p>IJI+Jz, -(jt+h-1)) 1}1+}2-1, -(}!+}2-1)) 
</p>
<p>lit+ }2, -(jl + }2)) 
</p>
<p>(15.2.3) 
</p>
<p>(15.2.4) 
</p>
<p>(15.2.5) 
</p>
<p>(Note that the labels}I}2 are suppressed on the total-jkets. We shall do so frequently 
</p>
<p>to simplify the notation.) 
</p>
<p>Our problem is to express each of these kets as a linear combination of product 
</p>
<p>kets. To get an idea of how one goes about doing this, let us consider the problem 
</p>
<p>t There is no loss of generality, for we can always call the larger one }1 &bull; 
</p>
<p>409 
</p>
<p>ADDITION OF 
ANGULAR 
</p>
<p>MOMENTA </p>
<p/>
</div>
<div class="page"><p/>
<p>410 
</p>
<p>CHAPTER 15 
</p>
<p>solved in the last section (}1 =}2= 1/2). In this case the states are 
</p>
<p>j 
</p>
<p>m~ 
l II, I) 
</p>
<p>II, 0) 
</p>
<p>II, -I) 
</p>
<p>0 
</p>
<p>10, 0) 
</p>
<p>Consider the top state in the first column, II, I), which has the largest possible z 
component. There is only one product state with the right value of m, namely, both 
spins up. So by inspection, 
</p>
<p>II, 1)=1++) 
</p>
<p>We can multiply the right-hand side by a phase factor, but we follow the convention, 
called the Condon-Shortley convention, in which the coefficient of this top state is 
chosen to be unity. Consider next the state below this one, namely, II, 0). There are 
two product states with m = 0, namely, I +-) and 1-+); and II, 0) must be a linear 
combination of these. We find the combination as follows. We know thatt 
</p>
<p>so that 
</p>
<p>But we do not want II, 0) in terms of II, 1), we want it in terms of the product 
kets. So we rewrite the right-hand side as 
</p>
<p>I I 
=-.-2 (S._+Sz-)1++)=-1 ,2 (1il-+)+1il+-)) 
</p>
<p>2 1 1i 2 1 1i 
</p>
<p>so that 
</p>
<p>II, 0) =T 112 (1 +-) + 1-+)) 
</p>
<p>in accordance with our earlier result. 
The next state 11, -I) can be obtained by lowering this one more step in the 
</p>
<p>above sense, or more simply by noting that there is only one ket with m maximally 
negative, namely, 1--). So 
</p>
<p>II, -1)=1--) 
</p>
<p>Our phase convention is such that this is what you would get if you lowered II, 0). 
</p>
<p>t Recall J"IJ. m) = li[(j'fm)(j&plusmn;m+ I )] 1 'IJ. m&plusmn; I). </p>
<p/>
</div>
<div class="page"><p/>
<p>This takes care of the j = 1 states. Consider next j = 0. The state I 0, 0) has m = 
0 and is also a linear combination of I+-) and 1-+ ). We find the combination 
</p>
<p>using two constraints: (1) The combination must be orthogonal to the one that 
</p>
<p>forms the other state with m = 0, namely, 11, 0) and have real coefficients.t (2) The 
</p>
<p>combination is normalized to unity. If we call the combination af+-)+fll-+), 
these constraints tell us that 
</p>
<p>It follows that 
</p>
<p>a+fi=O 
</p>
<p>az+Pz=l 
</p>
<p>10, 0) =T 112 (1 +-)-I-+)) 
</p>
<p>Note that we could still have multiplied the state by ( -1). Our convention is as 
</p>
<p>follows: in each column in Eq. (15.2.5) the top state is given the overall sign which 
</p>
<p>makes the coefficient of the product ket with m 1 = j 1 positive. 
Let us now turn to the general problem, Eq. (15.2.5). Once again the top state 
</p>
<p>in the first column, with m equal to its maximum value of }1 + }2, can be built out 
of only one product ket, the one in which both angular momenta take on maximum 
</p>
<p>possible projections along the z axis: 
</p>
<p>( 15.2.6) 
</p>
<p>The other m states at this value of j are obtained by lowering. Let us consider going 
</p>
<p>down just one step. Since 
</p>
<p>we have, as in the spin-(1/2@1/2) problem 
</p>
<p>IJ1+}2,}1+}z-l) 
</p>
<p>1 . (J +J )I . . . . &gt; 
[2(}1 +h)JI/211 1- 2- }1}1 ,)2)2 
</p>
<p>[2(}1 + ~z) f ;2 11 [11(2}1) 112IJ1 U1 - 1 ), }z.h) + 1i(2}z) 1/ 21}1}1 ,}2(}2- 1))] 
</p>
<p>( 
. ) I /2 (' . \ I /2 
</p>
<p>= . 1+1 &bull; fj,(jl-l),Jz)2)+ . 1+2 .) IJI),,,h(h-1)) 
}I }2 }1 }2 
</p>
<p>(15.2.7) 
</p>
<p>Proceeding in this manner we can get to the bottom state in the first column.&sect; 
Now for the top state in the second column. Since it has m = }1 + }z -1, there 
</p>
<p>are two product kets that are eligible to enter the linear combination; they are 
</p>
<p>t This is a matter of convention. 
&sect;In practice one goes only to m = 0. The states of negative m can be found using special properties of 
</p>
<p>the expansion, to be discussed shortly. 
</p>
<p>411 
</p>
<p>ADDITION OF 
</p>
<p>ANGULAR 
</p>
<p>MOMENTA </p>
<p/>
</div>
<div class="page"><p/>
<p>412 
</p>
<p>CHAPTER 15 
</p>
<p>lir}I ,/2(}2- I)) and 1./rUr- I ),h.h). The combination must be nom1alized to unity, 
be orthogonal to the other state fonned out of these kets, namely, IJ1 + }2, } 1 + h- I) 
[see Eq. (15.2.7)], and by convention have real coefficients. The answer is, by 
</p>
<p>inspection, 
</p>
<p>IJ1 +h-l,JJ +h-1)= 
</p>
<p>. \ l i2 
</p>
<p>_)
1
.) 1Jr}J,}2(}2-I)) 
</p>
<p>+}2, 
</p>
<p>1 &lt;~ 
</p>
<p>h) 1&middot;(&middot; 1&middot; &middot;&middot;&gt; - -+ . }l }1- ),}2]2 
}21 
</p>
<p>(15.2.8) 
</p>
<p>The overall sign is fixed by requirement that the coefficient of the product ket with 
</p>
<p>m 1 = ) 1 be positive. Given the top state, the rest of the second column may be obtained 
by lowering. Let us go just one more column. The top state in the third column, 
</p>
<p>lir + h- 2,)r + }2- 2), can be a superposition of three product kets. The three (real) 
coefficients are determined by these three requirements: orthogonality to the two 
</p>
<p>preceding total-) kets of the same m, and unit normalization. It is clear that there 
</p>
<p>are always enough constraints to determine the top states of each column, and once 
</p>
<p>the top states are known, the rest follow by lowering. 
</p>
<p>Exercise 15.2J { l) Verify that 1)1) 1 ,)2}2) is indeed a state of j = j, +h by letting 
1 2 =1T+J~+2Jlj2c+Jl+h +J, J2&bull; act on it. 
</p>
<p>(2) (optional) Verify that the right-hand side of Eq. ( 15.2.8) indeed has angular momen-
tumj=j, +)2-1. 
</p>
<p>Clebsch-Gordan (CG) Coefficients 
</p>
<p>The completeness of the product kets allows us to write the total-) kets as 
</p>
<p>The coefficients of the expansion 
</p>
<p>are called Clebsch Gordan coefficients or vector addition coefficients. (Since the label.s 
</p>
<p>.id2 appear in the bra, we suppress them in the ket.) Here are some properties of 
these coefficients : 
</p>
<p>( 15.2.9) 
</p>
<p>(This is called the triangle inequality, for geometrically it means that we must be able 
</p>
<p>to form a triangle with sides )J, )2, and j). 
</p>
<p>(2) ()rmt,)2m21Jm)r0 onlyif m,+m2=m 
</p>
<p>(3) they are real (conventional) 
</p>
<p>(15.2.10) </p>
<p/>
</div>
<div class="page"><p/>
<p>( 4) (}1}1 ,}2(}-j 1)IJJ) is positive (conventional) 
</p>
<p>(This condition fixes the overall sign in the expansion of each top state and was 
</p>
<p>invoked in the preceding discussion.) 
</p>
<p>(15.2.11) 
</p>
<p>This relation halves the work we have to do: we start at the top state and work our 
</p>
<p>way down to m=O (or 1/2 ifj is half-integral). The coefficients for the negative m 
</p>
<p>states are then determined by this relation. 
</p>
<p>Exercise 15.2.2. * Find the CG coefficients of 
</p>
<p>(I) ~&reg; 1 =hEd 
</p>
<p>(2) 1&reg;1=2EBIEB0 
</p>
<p>Exercise 15. 2.3. Argue that ~ &reg; ~ &reg; ~ = ~ EB ~ EB t 
</p>
<p>If we assemble the CG coefficients into a matrix, we find it is orthogonal (real 
</p>
<p>and unitary). This follows from the fact that it relates one orthonormal basis to 
</p>
<p>another. If we invert the matrix, we can write the product kets in terms of total-} 
</p>
<p>kets. The coefficients in this expansion are also CG coefficients: 
</p>
<p>because the CG coefficients are real. As an example, consider the i@ i problem. 
There we have 
</p>
<p>IJm) lm1m2) 
</p>
<p>[ II, I) J [I 0 0 I++)J IL 0) = o 1!i 12 1 nl/2 0 1+-) !~ 
11,-1) 0 0 0 1 1-+) 
10, 0) 0 l !'")1/2 -1ji 12 0 1--) ;~ 
</p>
<p>(Notice that the columns contain not the components of vectors, but the basis vectors 
</p>
<p>themselves.) We can invert this relation to get 
</p>
<p>[
I++&gt;] [1 1+-) = 0 
1-+) 0 
1--&gt; 0 
</p>
<p>0 
</p>
<p>1/21/2 
</p>
<p>1/21/2 
</p>
<p>0 
</p>
<p>0 0 ][11,1)] 0 1/2112.&middot; 11.,0) 
o -1/21 ' 2 IL-l) 
l 0 10,0) 
</p>
<p>413 
</p>
<p>ADDITION OF 
</p>
<p>ANGULAR 
</p>
<p>MOMENTA </p>
<p/>
</div>
<div class="page"><p/>
<p>414 
</p>
<p>CHAPTER 15 
</p>
<p>Thus we can write 
</p>
<p>1+-)=2 1 11,0)+10,0)) 
</p>
<p>etc. In practice one uses CG coefficients to go both ways, from the product to the 
</p>
<p>total-} basis and vice versa. 
</p>
<p>Addition of L and S 
</p>
<p>Consider an electron bound to a proton in a state of orbital angular moment 
</p>
<p>l. Since the electron has spin l its total angular momentum J = L + S can have 
values of j =I&plusmn; 1 /2. We wish to express the total-} states in terms of product states 
I fmo, sms) .t Since m, = &plusmn; l /2, at each m there will be at the most two eligible product 
kets.&sect; Let 
</p>
<p>1J = l + J /2. m) =a I I, m- 1 ; l/2, I /2) + ..611, m + I ; 1 /2, -1 (15.2.12) 
</p>
<p>l.i=l-1/2, m) = a'll, m-l ; 1/2, l + fJ'Il, m + 1 ; l -1/2) (1.5.2.13) 
</p>
<p>The requirement that these states be orthonormal tells us that 
</p>
<p>( 15.2.14) 
</p>
<p>(15.2.15) 
</p>
<p>aa'+fJfl'=O (15.2.16) 
</p>
<p>So we only need one more constraint, say the ratio ajfJ. We find it by demanding 
</p>
<p>that 
</p>
<p>m) = ti. 2(l+ l/2)(/+ 3/2)1.i=/+ 1/2, m) (15.2.17) 
</p>
<p>Writing 
</p>
<p>(15.2.18) 
</p>
<p>we can deduce that 
</p>
<p>( )
</p>
<p>L2 
</p>
<p>/2= /+ 1/2&middot;&middot;&middot;&middot;&middot; m 
a J+ 1/2+m 
</p>
<p>(15.2.19) 
</p>
<p>t Here, mv, m." and m stand for orbital, spin, and total projections along the = axis. 
&sect;It might help to construct the table as in Eq. (15.2.5). It will contain just two columns, one fori=/+ 1/ 
</p>
<p>2 and one for j= 1- I/2. </p>
<p/>
</div>
<div class="page"><p/>
<p>Given this, and our convention for the overall sign, 
</p>
<p>1 
Jj=l&plusmn; 1/2, m)= 112 [&plusmn;(/+ lj2&plusmn;m)112\l, m-1/2; 1/2, 1/2) 
</p>
<p>(21+ 1) 
</p>
<p>+(I+ 1/2 =F m) 112\l, m + 1/2; 1/2, -1/2)] (15.2.20) 
</p>
<p>[Notice that ifj= I+ 1/2, m =&plusmn;(I+ 1/2); only one term survives with unit coefficient.] 
</p>
<p>If the Hamiltonian contains just the Coulomb interaction, or, in addition, an inter-
</p>
<p>action with a weak constant magnetic field, the product basis is adequate. The total-
</p>
<p>j basis will come in handy when we study the spin-orbit interaction [which involves 
</p>
<p>the operator L&middot; S = 1(12 - L 2 - S 2)] in Chapter 17. 
</p>
<p>Exercise 15.2.4. Derive Eqs. (15.2.19) and (15.2.20). 
</p>
<p>Exercise 15.2.5. * (1) Show that IP' 1 = ~/+(S 1 &middot;S 2 )/f'z 2 and IP0 = ii- (S1 &bull;Sz)/f'z2 are projec-
tion operators, i.e., obey IP',IP'j= oijiP'j [use Eq. (14.3.39)]. 
</p>
<p>(2) Show that these project into the spin-1 and spin-0 spaces in ~ &reg; ~ = 1 $0. 
</p>
<p>Exercise 15.2.6. Construct the project operators IP&plusmn; for thej=/&plusmn;1/2 subspaces in the 
</p>
<p>addition L+S=J. 
</p>
<p>Exercise 15.2. 7. Show that when we add j 1 to j 1 , the states with j= 2j1 are symmetric. 
</p>
<p>Show that the states withj=2j1 = 1 are antisymmetric. (Argue for the symmetry of the top 
</p>
<p>states and show that lowering does not change symmetry.) This pattern of alternating symme-
</p>
<p>try continues as j decreases, but is harder to prove. 
</p>
<p>The Modified Spectroscopic Notation 
</p>
<p>In the absence of spin, it is sufficient to use a single letter such as s, p, d, ... to 
</p>
<p>denote the (orbital) angular momentum of a particle. In the presence of spin one 
</p>
<p>changes the notation as follows: 
</p>
<p>(1) Use capital letters S, P, D, ... (let us call a typical letter L), to indicate the value 
</p>
<p>of the orbital angular momentum. 
(2) Append a subscript J to the right of L to indicate the j value. 
</p>
<p>(3) Append a superscript 2S + 1 to the left of L to indicate the multiplicity due to 
spin projections. 
</p>
<p>Thus, for example 
</p>
<p>denotes a state with I= 1, s = 1/2, j = 3/2. For a single electron the 2S + I label 
is redundant and always equals 2. For a multielectron system, Sand L stand 
</p>
<p>415 
</p>
<p>ADDITION OF 
ANGULAR 
MOMENTA </p>
<p/>
</div>
<div class="page"><p/>
<p>416 
</p>
<p>CHAPTER 15 
</p>
<p>for total spin and total orbital angular momentum, and J for their sum. Thus 
in the ground state of He, 
</p>
<p>15.3. Irreducible Tensor Operators 
</p>
<p>We have already discussed scalar and vector operators. A scalar operator S 
transforms like a scalar under rotations, i.e., remains invariant: 
</p>
<p>S-.S' = ut[R]SU[R] = S 
</p>
<p>By considering arbitrary infinitesimal rotations we may deduce that 
</p>
<p>or in a form that will be used later 
</p>
<p>[J;, S]=O 
</p>
<p>[J&plusmn;,S]=O 
</p>
<p>[J=, S] =0 
</p>
<p>(15.3.1) 
</p>
<p>( 15.3.2) 
</p>
<p>Examples of S are rotationally invariant Hamiltonians such as the Coulomb or 
isotropic oscillator Hamiltonian. A vector operator V was defined as a collection of 
three operators (Vx, Vy, Vo) which transform as the components of a vector in W3(R): 
</p>
<p>( 15.3.3) 
</p>
<p>where R is the usual 3 x 3 rotation matrix. By considering infinitesimal rotations, we 
may deduce that [Eq. (12.4.14)]: 
</p>
<p>[ V;, Jj] = i1i I &amp;ukvk 
k 
</p>
<p>(15.3.4) 
</p>
<p>Let us rewrite Eq. (15.3.3) in an equivalent form. ReplaceR by R- 1 = Rr everywhere 
to get 
</p>
<p>(15.3.5) 
</p>
<p>Notice that we are summing now over the first index of R. This seems peculiar, for 
we are accustomed to the likes of Eq. (15.3.3) where the sum is over the second 
index. The relation ofEq. (15.3.3) to Eq. (15.3.5) is the following. Let 11), 12), and 
13) be basis kets in W3(R) and R a rotation operator on it. If I V) is some vector </p>
<p/>
</div>
<div class="page"><p/>
<p>with components v;= (il V), its rotated version IV')= Rl V) has components 
</p>
<p>v; = (iiRI V) = 2: (iiRU)(JI V) = 2: Rijvj (15.3.6) 
j j 
</p>
<p>If instead we ask what R does to the basis, we find I i) -+I i') = Rl i) where 
</p>
<p>li')=Rii)=l: IJ&gt;&lt;JIRii)=l: Rj;IJ&gt; (15.3.7) 
j j 
</p>
<p>Since Rj;= (R- 1)ij, we see that vector components and the basis vectors transform in 
</p>
<p>"opposite" ways. Equation (15.3.3) defines a vector operator as one whose compo-
</p>
<p>nents transform under V;-+ utV;U as do components of a vector IV) under 
</p>
<p>I V)-+RI V), while Eq. (15.3.5) defines it as one whose components V; transform 
</p>
<p>under V;-+ UV;Ut as do the kets I i) under I i)-+ Rl i). Both definitions are of course 
</p>
<p>equivalent. The first played a prominent role in the past and the second will play a 
</p>
<p>prominent role in what follows. 
</p>
<p>Tensor Operators 
</p>
<p>We know that a vector IV) is an element of V3(R), i.e., may be written as 
</p>
<p>3 
</p>
<p>IV)= L V;li) (15.3.8) 
i=l 
</p>
<p>in terms of its components V; and the basis kets I i). A second-rank tensor I r&lt;2l) is 
an element of the direct product space V 3(R)&reg;V3(R), spanned by the nine kets 
</p>
<p>li)&reg;IJ): 
</p>
<p>3 3 
</p>
<p>I T(2)) = L L tijli)&reg;IJ&gt; (15.3.9) 
i~l j~l 
</p>
<p>One refers to tij as the components of IT(2l) in the basis li)&reg;IJ). 
</p>
<p>As in the case of vectors, a tensor operator of rank 2 is a collection of nine 
</p>
<p>operators Tij which, under Tij-+ utriju, respond as do the tensor components tij, or, 
</p>
<p>equivalently, under Tij-+UTijut, respond as do the basis kets li)&reg;IJ). Tensors and 
</p>
<p>tensor operators of rank n &gt; 2 are defined in a similar way. (Note that a vector may 
</p>
<p>be viewed as a tensor of rank 1.) We shall call these tensors Cartesian tensors. 
</p>
<p>Of greater interest to us are objects called spherical tensor operators. A spherical 
</p>
<p>tensor operator of rank k has 2k+ 1 components Tk, q= +k, (k-1), ... , -k, which, 
</p>
<p>417 
</p>
<p>ADDITION OF 
</p>
<p>ANGULAR 
</p>
<p>MOMENTA </p>
<p/>
</div>
<div class="page"><p/>
<p>418 
</p>
<p>CHAPTER 15 
</p>
<p>under Tf-.UT%U1 respond like the angular momentum eigenkets [j=k, m=q)= 
</p>
<p>fkq)t: 
</p>
<p>U[RJ rz ut[RJ =I n;{,Jr'{ (15.3.10) 
r./ 
</p>
<p>Since the 2k + 1 kets [kq) transform irreducibly. so do the operators rr. For this 
reason, they are also called irreducible tensor operators. 
</p>
<p>By considering infinitesimal rotations, we may deduce from Eq. (15.3.10) that 
</p>
<p>(Exercise 15.3.1): 
</p>
<p>[J&plusmn;, T~] = &plusmn;ii[(k =j= q)(k &plusmn; q --1- 1 ){ 2 Tk&plusmn; I 
</p>
<p>[Jo, Tf]=/iqTk 
(15.3.1 I) 
</p>
<p>Notice that commuting a J with Tic is like letting J act on the ket [kq). 
Why are irreducible tensor operators interesting? Consider the effect of acting 
</p>
<p>on a state [alm) with TZ. (Here a denotes labels besides angular momentum.) Let 
us rotate the resulting state and see what happens: 
</p>
<p>U[R] Ti,[jm) = U[RJT%Ut[R] U[R][jm) 
</p>
<p>=I D~~}TZ I n;~;,[jm') 
q' 
</p>
<p>= '\ ,. D 1 ~ 1 DU.i 1&middot;q&middot;[ . ')-
'- ~ if if tn r!l k Jfn (15.3.12) 
q' n;' 
</p>
<p>We find that TZfJm) responds to rotations like the product ket [kq)@[jm). Thus, 
when we act on a state with T~. we add angular momentum (k, q) to the state. In 
</p>
<p>other words, an irreducible tensor operator TZ imparts a definite amount of angular 
momentum (k, q) to the state it acts on. This allows us to say the following about 
</p>
<p>matrix elements of Ti between angular momentum eigenstates: 
</p>
<p>(aj'm'[ T%fajm) = 0 unless k + j?.j' ?.[k- j[, m'=m+q (15.3.13) 
</p>
<p>This is because T%1 ajm) contains only those angular momenta that can be obtained 
by adding (k, q) and (j, m); so I aj'm') is orthogonal to TkiJm) unless (j', m') is 
one of the possible results of adding (k, q) and (j, m). Equation (15.3.13) is an 
</p>
<p>example of a selection rule. 
Let us consider some examples, starting with the tensor operator of rank 0. It 
</p>
<p>has only one component T8. which transforms like [00), i.e., remains invariant. 
</p>
<p>t Recall that 
</p>
<p>!kq) &gt; U[R]Ikq) ~ L L lk'q')&lt;k q'l U[R]Ikq) 
k' q' </p>
<p/>
</div>
<div class="page"><p/>
<p>Thus rg is just a scalar operator S, discussed earlier. Our selection rule tells us that 
</p>
<p>(a]'m'IT81a}m)=O unless j=j', m=m' (15.3.14) 
</p>
<p>Consider next T! (q= 1, 0, -1). Here we have three objects that go into each 
</p>
<p>other under rotations. Since a vector operator V also has three components that 
</p>
<p>transform irreducibly (why?) into each other, we conjecture that some linear combi-
</p>
<p>nations of the vector operator components should equal each T!. In fact 
</p>
<p>T&plusmn;l V,&plusmn;i~. y&plusmn;&bull;1 
1 = =F i/2 
</p>
<p>(15.3.15)t 
</p>
<p>T?= V== V? 
</p>
<p>Given Eq. (15.3.4) and the above definitions, it may be readily verified that vt&bull; and 
Vl obey Eq. (15.3.11) with k= 1, q= &plusmn;1, 0. The selection rule for, say, V, is 
</p>
<p>v-1-v1 
(a)'m'l Vxla}m)=(a]'m'l 12112 
</p>
<p>1 la}m) 
</p>
<p>= 0 unless j + 1 ~j' ~ IJ- 11, 
</p>
<p>and likewise 
</p>
<p>(a]'m'l Vzla}m)=(a}'m'l V?la}m) 
</p>
<p>=0 unless j+ 1 ~J'~IJ-11, 
</p>
<p>m'=m&plusmn;l (15.3.16a) 
</p>
<p>m'=m (15.3.16b) 
</p>
<p>Once we go beyond rank 1, it is no longer possible to express Cartesian and 
</p>
<p>spherical tensors of the same rank in terms of each other. A Cartesian tensor of rank 
</p>
<p>n has 3n components, whereas a spherical tensor of rank k has (2k + 1) components. 
For n = 0 and n = 1, the Cartesian tensors happened to have the same number of 
components as spherical tensors of rank k=O and 1, respectively, and also trans-
</p>
<p>formed irreducibly. But consider higher ranks, say rank 2. The tensor n has five 
components that transform irreducibly. The tensor Tij has nine r..:omponents which 
</p>
<p>transform reducibly, i.e., it is possible to form combinations of Tij such that some 
</p>
<p>of them never mix with others under rotations. There is one combination that is 
</p>
<p>invariant, i.e., transforms like T8; there are three combinations that transform like 
</p>
<p>a vector or in light of Eq. (15.3.15) like TT; and finally there are five that transform 
like T1. We will see what these combinations are when we study the degeneracy of 
the isotropic oscillator of a few pages hence. Cartesian tensors of higher rank are 
</p>
<p>likewise reducible. Let us now return to the selection rule, Eq. (15.3.13). 
We can go a step further and relate the nonvanishing matrix elements. Con-
</p>
<p>sider the concrete example of R1, the position operator in spherical form. We have 
</p>
<p>419 
</p>
<p>ADDITION OF 
</p>
<p>ANGULAR 
</p>
<p>MOMENTA </p>
<p/>
</div>
<div class="page"><p/>
<p>420 
</p>
<p>CHAPTER 15 
</p>
<p>(assuming no spin, so J=L) 
</p>
<p>(a 2I2m2 I Rll a1llm1) 
</p>
<p>= rR~,t,(r) Y7;;( 0, &cent; )r( 4n)112 YIR,, 1,(r) Y~''( 0, &cent; )r2 dr d0.. 
&bull; \ 3 
</p>
<p>(l5.3.17)t 
</p>
<p>where (a)2IIRd I a tlt ), the reduced matrix element, is independent of m 1, m2&bull; and q. 
which appear only in the CG coetlicient, which is essentially the angular integral (up to a 
</p>
<p>factor independent of m 1&bull; m2, and q). 
This example illustrates a general result (not proven here): 
</p>
<p>(15.3.18) 
</p>
<p>This is called the Wigner-Eckart theorem. It separates the dependence of the matrix 
</p>
<p>element on spatial orientation (on m2 , m 1 , and q) from the rest. The former is 
</p>
<p>expressed entirely in terms of the CG coefficients. 
</p>
<p>Exercise 15.3.1. (1) Show that Eq. (15.3.11) follows from Eq. (15.3.10) when one consid-
</p>
<p>ers infinitesimal rotations. (Hint: D~~d=(kq'll-(io9&middot;J)/filkq). Pick 89 along, say, the x 
</p>
<p>direction and then generalize the result to the other directions.) 
</p>
<p>(2) Verify that the spherical tensor V1 constructed out of Vas in Eq. (15.3.15) obeys 
Eq. (15.3.11 ). 
</p>
<p>Exercise 15.3.2. It is claimed that Lq (-l)qS~Tl-qJ is a scalar operator. 
(I) Fork= l, verify that this is just S&middot;T. 
</p>
<p>(2) Prove it in general by considering its response to a rotation. [Hint: 
D'l/,,-m' = ( -J)m&middot;&middot;&middot;m(D~.~,.)* .] 
</p>
<p>Exercise 15. 3. 3. ( 1) Using (jj l.fj, 10) = U/ (j + !) ] 112 show that 
</p>
<p>(a)m'IJ&middot;Aiajm) =c&lt;a)IIAIIaj) 
</p>
<p>where cis a constant independent of a. a' and A. Show that c "' nfj(j+ 1 )] 1128m.m'. 
(3) Using the above, show that 
</p>
<p>(15.3.19) 
</p>
<p>! Note that R~ is the tensor operator and Ra1 (r) is the radial part of the wave function. We have also 
used Eq. (12.5.42) to obtain R7. </p>
<p/>
</div>
<div class="page"><p/>
<p>Exercise 15.3.4. * (I) Consider a system whose angular momentum consists of two parts 
J 1 and J2 and whose magnetic moment is 
</p>
<p>In a state IJm,)th) show, using Eq. (15.3.19), that 
</p>
<p>(2) Apply this to the problem of a proton (g= 5.6) in a 2Pt/2 state and show that (J..Iz) = 
</p>
<p>&plusmn;0.26 nuclear magnetons. 
</p>
<p>(3) For an electron in a 2 P 112 state show that (J..Iz) = &plusmn; ~ Bohr magnetons. 
</p>
<p>Exercise 15.3.5. * Show that (Jml T:IJm) = 0 if k &gt; 2). 
</p>
<p>15.4. Explanation of Some "Accidental" Degeneracies 
</p>
<p>In this section the degeneracy of states of different l at a given value of n in the 
</p>
<p>hydrogen atom and the isotropic oscillator (see Section 12.6) will be explained. But 
</p>
<p>first let us decide what it means to explain any degeneracy. Consider for example 
</p>
<p>the (2! + 1 )-fold degeneracy of the different m states at a given lin both these prob-
lems. We explain it in terms of the rotational invariance of the Hamiltonian as 
</p>
<p>follows: 
</p>
<p>(1) For every rotation R(O) on w3(R) there exists a unitary operator U[RJ 
which rotates the vector operators 
</p>
<p>(15.4.1) 
</p>
<p>If the Hamiltonian depends only on the "lengths" of various vector operators like 
P, R, L etc., then it is rotationally invariant: 
</p>
<p>(15.4.2) 
</p>
<p>i.e., rotations are symmetries of H. This is the case for the two problems in question. 
</p>
<p>(2) If we write this relation in infinitesimal form, we find 
</p>
<p>[H,L;]=O, i=l,2,3 (15.4.3) 
</p>
<p>where L; are the generators of rotation. For every free parameter that defines a 
rotation (8,, 8y, and 8J there is a corresponding generator. They are all conserved. 
</p>
<p>421 
</p>
<p>ADDITION OF 
</p>
<p>ANGULAR 
MOMENTA </p>
<p/>
</div>
<div class="page"><p/>
<p>422 
</p>
<p>CHAPTER 15 
</p>
<p>(3) From the three generators we construct the operator 
</p>
<p>(15.4.4) 
</p>
<p>which lowers the m value: 
</p>
<p>Ll/, m)=cl/, m-1) (15.4.5) 
</p>
<p>Since [L, H] = 0, the lowering operation does not change the energy. 
This explains the degeneracy in m, for, starting with the state of highest mat a 
</p>
<p>given /, we can go down all the way to the lowest m without changing the energy. 
(We can equally well work with L+ .) 
</p>
<p>Let us try to do the same for the two problems in question. We follow these 
steps: 
</p>
<p>Step (I): Identify symmetries of H besides rotational invariance. 
Step (2): Find the generators of the symmetry transformations. 
Step (3): Construct an operator from these generators that can change I by one unit 
</p>
<p>in the case of hydrogen and two units in the case of the oscillator. 
</p>
<p>Hydrogen 
</p>
<p>Steps (1) and (2). Unfortunately the only obvious symmetry of the Coulomb 
Hamiltonian is rotational invariance. The additional symmetry, the one we are after, 
is very subtle and clearest in momentum space. We will not discuss it. But how then 
do we go to step (2)? The answer lies in the fact that the generators of the symmetry 
are conserved quantities. Now we have seen that the Coulomb problem admits an 
extra conserved quantity, the Runge-Lenz vector. Thus the three components of 
</p>
<p>(15.4.6) 
</p>
<p>must be the generators of the additional symmetry transformations (or linear combi-
nations thereof). 
</p>
<p>Step (3). Since we wish to talk about angular momentum let us write N in 
spherical form: 
</p>
<p>(15.4.7) 
</p>
<p>Consider the state I nil) of the H-atom. Acting on it with Nl, we get another state 
of the same energy or same n (since [ H, N l] = 0) but with higher angular momentum: 
Nllnll) behaves as 111)&reg;1/l) =II+ 1, /+ 1 ). So 
</p>
<p>Nlln, l, l)=cln, l+ 1, l+ 1) (15.4.8) </p>
<p/>
</div>
<div class="page"><p/>
<p>(It will turn out that c vanishes when l= lmax = n -1.) Using Nl we can connect all 
the different l states at a given n, and using L_ we can connect all the m states at a 
</p>
<p>given I. For example, at n = 3 the network that connects degenerate states is as 
follows: 
</p>
<p>3 2 2 
</p>
<p>(3, 0, 0) 
</p>
<p>3 2 (-2) 
</p>
<p>The Oscillator 
</p>
<p>/ 
~i 
</p>
<p>0 
</p>
<p>Step (1). To find the extra symmetry of H, let us look at it again: 
</p>
<p>(15.4.9) 
</p>
<p>We say His rotationally invariant because it depends only on the lengths squared 
of the (real) vectors P and R. Let us now rewrite H in a way that reveals the extra 
</p>
<p>symmetry. Define a complex vector (operator) whose real and imaginary parts are 
proportional to R and P: 
</p>
<p>1 
a= . 1 , 2 (pwR + iP) 
</p>
<p>(2pwfi) ' 
</p>
<p>and its adjoint, whose components are complex conjugates of those of a: 
</p>
<p>t 1 . 
a=---. (pwR-lP) 
</p>
<p>(2pwfi)ll2 
</p>
<p>(15.4.10) 
</p>
<p>(15.4.11) 
</p>
<p>The components of a and at are just the lowering and raising operators for the x, 
y, and z oscillators. They obey 
</p>
<p>423 
</p>
<p>ADDITION OF 
</p>
<p>ANGULAR 
</p>
<p>MOMENTA </p>
<p/>
</div>
<div class="page"><p/>
<p>424 
</p>
<p>CHAPTER 15 
</p>
<p>In terms of a and at, 
</p>
<p>(15.4.12) 
</p>
<p>Thus we find that H is a function of the length squared of a complex three-dimen-
sional vector a. So it is invariant under "rotations" in 'W'\ C), i.e., under unitary 
transformations in 'W'3( C). Just as we denoted the rotations in 'W' 3(R) by R, let us 
call these c.t For every "rotation" C (unitary transformation) in 'W' 3( C), there will 
exist Hilbert space operators U[ C] which rotate the complex vector operator a: 
</p>
<p>ai-&gt;a; = ut[ C]aiU[ C] = L Cijaj 
j 
</p>
<p>(15.4.13) 
</p>
<p>where CiJ are matrix elements of the unitary operator C in 'W' 3( C). Since H depends 
only on the norm squared of a, 
</p>
<p>ut[C]HU[C] =H (15.4.14) 
</p>
<p>Step (2). How many generators of U[C] are there and what are they? The 
answer to the first part is the number of parameters that define a rotation in 'W' 3( C), 
i.e., the number of independent parameters in a 3 x 3 unitary matrix C. Now any 
such matrix can be written as 
</p>
<p>(15.4.15) 
</p>
<p>where n is a 3 x 3 Hermitian matrix. It is easy to see that n has three real diagonal 
elements and three independent complex off-diagonal elements. Thus it depends on 
nine real parameters. So there are nine conserved generators. What are they? Rather 
than deduce them (as we did the L's by considering the effect of infinitesimal rotations 
on 'II) we write down the nine conserved quantities by inspection. It is clear that in 
the oscillator case, the nine operators 
</p>
<p>(i,j= x, y, or z) ( 15.4.16) 
</p>
<p>are conserved. The proof is simple: aj destroys a j quantum and aJ creates an i 
quantum and this leaves the energy invariant since the x, y, and z oscillators have the 
same w (isotropy). To see what impact TiJ has on I degeneracy, we must decompose TiJ 
into its irreducible parts. 
</p>
<p>Consider first the combination 
</p>
<p>( 15.4.17) 
</p>
<p>This is clearly a scalar, i.e., transforms like rg. The fact that it commutes with H 
does not explain the degeneracy in l because it "carries" no angular momentum. In 
fact at&middot; a is just H up to a scale factor and an additive constant. 
</p>
<p>t We should really be calling these U. But that will complicate the notation. </p>
<p/>
</div>
<div class="page"><p/>
<p>Consider next the three antisymmetric combinations 
</p>
<p>Txy- Tyx=a:ay-a;.ax= (at x a): 
</p>
<p>Tyz- Tzy= (at X a)x 
</p>
<p>Tzx- Txz= (at X a)y 
</p>
<p>(15.4.18) 
</p>
<p>&middot;I 
These clearly transform as a vector V =a x a. There seems to be a problem here. 
</p>
<p>Suppose we form the operator Vl=-(Vx+iVy)/i 12&bull; Then we expect 
</p>
<p>VlJnll)=cJn, l+ 1, l+ 1) (15.4.19) 
</p>
<p>as in Eq. (15.4.8). This would mean that states differing by one unit in l are degener-
</p>
<p>ate. But we know from Section 12.6 that states differing by two units in l are degener-
</p>
<p>ate. So how do we get out of the fix? To find out, you must work out any one of 
</p>
<p>the components of the operator V =at x a in terms of R and P. If you do, you will 
</p>
<p>see that c in Eq. (15.4.19) is really zero, and the paradox will be resolved. 
We are now left with 9- 1- 3 = 5 degrees of freedom out of the original nine 
</p>
<p>Ty's. We argue that these must transform irreducibly. Why? Suppose the contrary is 
</p>
<p>true. Then it must be possible to form irreducible tensors with fewer than five compo-
</p>
<p>nents out of these residual degrees of freedom. The only possibilities are tensors with 
</p>
<p>1 or 3 components, that is to say, scalars or vectors. But we know that given two 
</p>
<p>vectors at and a we can form only one scalar, at&middot; a and only one vector at x a, both 
</p>
<p>of which we have already used up. So we are driven to the conclusion that the five 
</p>
<p>residual degrees of freedom are linear combinations of some T3. One usually refers 
to this object as the quadrupole tensor Q~. AH we need here is the component Qi, 
</p>
<p>smce 
</p>
<p>Qilnll) = cJn, l+ 2, l+ 2) (15.4.20) 
</p>
<p>which explains the degeneracy in l at each n. (When l = n = lmax, c vanishes.) 
Let us explicitly construct the operator Qi in terms of aiaj to gain some experi-
</p>
<p>ence. Now a and at are vector operators from which we can fom1 the tensor operators 
</p>
<p>a'{ and (at)1 which behave like ll, q). The product aiaj then behaves like the direct 
</p>
<p>product of (linear combinations) of two spin-1 objects. Since Qi behaves like 122) 
</p>
<p>and since 122)=111)&reg;111), we deduce that 
</p>
<p>(15.4.21) 
</p>
<p>Other components of Q3 may be constructed by similar techniques. (It is just a 
matter of adding angular momenta 1 &reg; l to get 2.) Starting with the smallest value 
</p>
<p>425 
</p>
<p>ADDITION OF 
</p>
<p>ANGULAR 
</p>
<p>MOMENTA </p>
<p/>
</div>
<div class="page"><p/>
<p>426 
</p>
<p>CHAPTER 15 
</p>
<p>of I at each n (namely, 0 or 1), we can move up in steps of 2 until we reach l=n, at 
which point c in Eq. (15.4.20) will vanish. The network for n=4 is shown below: 
</p>
<p>0 
</p>
<p>4 4 4 ~ 
4 2 2 
</p>
<p>y 
)' ;/4 lL 2 I 
</p>
<p>(4 0 0) 
2 . (-2) 4 . ( -4) 4 4 
</p>
<p>This completes the explanation of the degeneracy of the oscillator. 
</p>
<p>The Free-Particle Solutions 
</p>
<p>We examine the free-particle solutions from Section 12.6 in the light of the 
preceding discussion. Here again we have a case where states with different I, in fact 
an infinite number of them, are degenerate at each energy E= 1i2k2 /2p. This degener-
acy is, however, not "accidental," since the extra symmetry of the free-particle Hamil-
tonian, namely, translational invariance, is obvious. We therefore have a conserved 
vector operator P from which we can form P + ,t which can raise l and m by one 
unit. Thus, given the state with I= m = 0, we can move up in I using 
</p>
<p>lkll) = c(P +)'lkOO) 
</p>
<p>where c is some normalization constant. 
Recall that in the coordinate basis it was easy to find 
</p>
<p>Uo(p) 0 
lk00)-+1/hoo=-- Yo 
</p>
<p>p 
</p>
<p>(15.4.22) 
</p>
<p>(15.4.23) 
</p>
<p>where p=kr, and U0(p) is sin p or -cos p (regular or irregular solutions). It is easy 
to verify that 
</p>
<p>1 d [Uo(p)J 0 P+lkOO) -i1i(x+iy)-&middot;- --Yo 
coordinate r dr p 
</p>
<p>basis 
</p>
<p>. 1 d [Uo(P)] =C1(x+zy)-- --
pdp p 
</p>
<p>(15.4.24) 
</p>
<p>t P+=Px+iPy is, up to a scale factor (-21/ 2 ) which does not change its rotational properties, just Pi. </p>
<p/>
</div>
<div class="page"><p/>
<p>where Ct has absorbed all the factors that have no p dependence. If we operate once 
</p>
<p>again with P+ and use [P+, R+]=O (where R+=Rx+iRy'xR 11), we get 
</p>
<p>( 15.4.25) 
</p>
<p>and so finally 
</p>
<p>- . ' uq, I 1 d Uo(P) ( )
I 
</p>
<p>=C,(sm (}) e p pdp -p-
</p>
<p>= ciY;p'G ~)' ~uc~P2 
</p>
<p>=R,Yi (1 5.4.26) 
</p>
<p>where 
</p>
<p>)I ( )I "' 1 1 d U0(p) "' 1 1 d R1=C1p(-- ~&middot;&middot;-=Cip -- Ro(P) 
,pdp p pdp 
</p>
<p>( 15.4.27) 
</p>
<p>This agrees with Eq. (12.6.29) if we set C1=(-l/ 
</p>
<p>427 
</p>
<p>ADDITION OF 
</p>
<p>ANGULAR 
</p>
<p>MOMENTA </p>
<p/>
</div>
<div class="page"><p/>
<p>The Variational and 
</p>
<p>WKB Methods 
</p>
<p>16.1. The Variational Method 
</p>
<p>16 
</p>
<p>More often than not, it is impossible to find exact solutions to the eigenvalue 
</p>
<p>problem of the Hamiltonian. One then turns to approximation methods, some of 
</p>
<p>which will be described in this and the following chapters. In this section we consider 
</p>
<p>a few examples that illustrate the variational method. 
</p>
<p>Our starting point is the inequality 
</p>
<p>(16.1.1) 
</p>
<p>where E0 is the lowest eigenvalue of H, i.e., the ground-state energy. Although this 
</p>
<p>result was proved earlier, let us recall the idea behind it. E ['I'] is just the mean value 
</p>
<p>of the energy in the state I 'If). The inequality states that the mean value cannot be 
less than the lowest value that enters the average. More formally, if I 'I') is expanded 
in terms of the eigenfunctions lEn) of H, 
</p>
<p>(16.1.2) 
</p>
<p>This inequality suggests a way (at least in principle) of determining the ground-
</p>
<p>state energy and eigenket. We take all the kets in the Hilbert space one by one and 
</p>
<p>make a table of the corresponding E[fl']. At the end we read off the lowest entry 
</p>
<p>and the ket that goes with it. Clearly this is not a practical algorithm. What one 
</p>
<p>does in practice is to consider just a subset (not necessarily a subspace) of vectors 
</p>
<p>which are parametrized by some variables (a, /3, y, .. . ) and which have the general 
features one expects of the true ground-state ket. In this limited search E[ 'I'] reduces 
</p>
<p>to a function of the parameters, E(a, f3, .. . ). We then find the values (a 0 , /3 0 , &bull;&bull;&bull; ) 
which minimize E. This minimun E( a 0 , /30 , &bull;&bull;&bull; ) provides an upper bound on Eo. 429 </p>
<p/>
</div>
<div class="page"><p/>
<p>430 
</p>
<p>CHAPTER 16 
</p>
<p>The name of the game is finding the lowest upper bound for a given amount of 
work. If H happens to be positive definite, Eo~ 0, and we will be able to restrict Eo 
to the range E(ao, f3o, .. . )~Eo~O. 
</p>
<p>As an example, consider the problem of a particle in a potential V(x)=A.x 4&bull; 
Here are the features we expect of the ground state. It will have definite parity, and, 
since the ground-state function will have no nodes (more nodes-+more wiggles-+ 
more kinetic energy), it will have even parity. It will be peaked at x = 0 so as to 
minimize (V). And of course it will vanish as lxl-+oo. A trial function that has all 
these features (and is also easy to differentiate and integrate) is 
</p>
<p>IJI(X, a)= e -ax'/2 (16.1.3) 
</p>
<p>where a is a free parameter that determines the width of the Gaussian. The energy 
as a function of a is 
</p>
<p>We see here the familiar struggle between the kinetic and potential energy terms. 
The former would like to see a -+0 so that the wave function is wide and has only 
large wavelength (small momentum) components, while the latter would like to see 
a-+oo, so that the wave function is a narrow spike near x=O, where the potential 
is a minimum. The optimal point, both effects included, is 
</p>
<p>The corresponding energy is 
</p>
<p>-(6mA.)I/3 
ao- --
</p>
<p>fz2 
</p>
<p>3 (6fz4A)l/3 
E(ao)=--
</p>
<p>8 m2 
</p>
<p>Since His positive definite, we conclude 
</p>
<p>05;Eo5;E (ao) 
</p>
<p>(16.1.4) 
</p>
<p>(16.1.5) 
</p>
<p>(16.1.6) 
</p>
<p>The best approximation to the wave function of the ground state (among all Gauss-
ians) is IJI(X, a 0 )=exp{-~a 0 x 2 ). 
</p>
<p>The inequality (16.1.6) is of course rigorous, but is utility depends on how close 
E ( a0 ) is to Eo. Our calculation does not tell us this. All we know is that since we 
paid attention to parity, nodes, etc., our upper bound E(a 0 ) is lower than that 
obtained by someone whose test functions had odd parity and 15 nodes. For instance, 
if V(x) had been ~mm 2 x 2 instead of A.x 4 , we would have found a 0 =(mmjfz) 112 and 
E(a0 ) = fzm/2. Although this is the exact answer, our calculation would not tell us 
this. The way to estimate the quality of the bound obtained is to try to lower it 
further by considering a trial function with more parameters. If this produces sub-</p>
<p/>
</div>
<div class="page"><p/>
<p>stantiallowering, we keep going. On the other hand, if we begin to feel a "resistance" 
</p>
<p>to the lowering of the bound as we try more elaborate test functions, we may suspect 
</p>
<p>that &pound; 0 is not too far below. In the case of V(x) = ~mro 2 x 2 , it will eventually be 
</p>
<p>found that there is no way of going below E ( ao) = liw /2. 
</p>
<p>Our faith in the variational method stems from its performance in cases where 
</p>
<p>the exact answer is known either analytically or experimentally. Let us consider two 
</p>
<p>examples. The first is that of the electron in a Coulomb potential V = -e2 jr. We 
</p>
<p>expect the ground-state wave function to have no angular momentum, no nodes, 
</p>
<p>behave like r 0 as r-+0, and vanish as r-+oo. So we choose lf!(r, (), f/J, a) =exp( -ar2).t 
We find (upon ignoring the irrelevant angular variables throughout) 
</p>
<p>E( ) _ f [ -ar2 ( fz2 1 d 2 d e2 ) -ar2] 2 d If -2ar2 2 d a - e --- -r --- e r r e r r 
2mr2 dr dr r 
</p>
<p>which is minimized by 
</p>
<p>The upper bound is then 
</p>
<p>me4 8 
E(ao) = -- -= -0.85 Ry 
</p>
<p>21i2 3n-
</p>
<p>(16.1.7) 
</p>
<p>(16.1.8) 
</p>
<p>(16.1.9) 
</p>
<p>(16.1.10) 
</p>
<p>which is slightly above&sect; the true energy. The true wave function is of course not a 
</p>
<p>Gaussian, but the general features are the same. For example lf!(r, a0) = e -aor2 predicts 
an uncertainty M= (9n-/32) 112a0 =0.94a0 , while the exact result is M=a0 (the Bohr 
</p>
<p>radius).ll 
</p>
<p>The second example deals with the ground state of He. Ignoring nuclear motion 
</p>
<p>(m/ M -+0), the Hamiltonian in the coordinate basis is 
</p>
<p>(16.1.11) 
</p>
<p>where '&bull; and rz are the radial coordinates of the two electrons and r12 is the radial 
</p>
<p>separation between them. We have already seen that if the mutual repulsion (e2 jr12) 
</p>
<p>is ignored, the ground-state wave function is just 
</p>
<p>t We could also choose e-ar, which would give the exact answer. But let us not. 
&sect; Remember that we are dealing with negative energies here. 
</p>
<p>(16.1.12) 
</p>
<p>II This agreement is rather fortuitous. In general, the variational method provides much better approxima-
tions to the energies than to the wave functions. The reason follows shortly. 
</p>
<p>431 
</p>
<p>THE 
</p>
<p>VARIATIONAL 
</p>
<p>ANDWKB 
</p>
<p>METHODS </p>
<p/>
</div>
<div class="page"><p/>
<p>432 
</p>
<p>CHAPTER 16 
</p>
<p>where the singlet spin wave function is suppressed, and 'I' 100 is the hydrogen-like 
wave function with fl_.zfl: 
</p>
<p>'lfloo= ~ e-Zr/ao ( 
3 )1/2 
</p>
<p>n-a~ 
(Z=2) (16.1.13) 
</p>
<p>Consequently 
</p>
<p>(Z=2) (16.1.14) 
</p>
<p>The energy that goes with this simplified picture is 
</p>
<p>( m(2e
2) 2) 
</p>
<p>E=2 -~ =-8Ry~-108.8eV 
</p>
<p>which is far below the measured value of -78.6 eV.t So we find that omitting the 
</p>
<p>Coulomb repulsion between the electrons is a bad approximation. But if we include 
</p>
<p>the t? /r12 term, the problem cannot be solved analytically. So we apply to it the 
variational method for a trial wave function, we use just the product function in Eq. 
</p>
<p>(16.1.14) but treat Z as a variable rather than setting it equal to 2. The idea is that 
as each electron shields the nuclear charge seen by the other, the effective Z is less 
</p>
<p>than 2. This is borne out by the calculation of 
</p>
<p>[f 'l'(r1r2Z)[- !_ (V~ + V~)- 2tl(_!_+_!_) +{J] 2m r1 r2 r12 
x 'l'(r1r2Z) d3r1 d3r2 
</p>
<p>E(Z)=--------r------------------
</p>
<p>f I 'l'(rlr2Z)I 2 d3r1 d3r2 
</p>
<p>= -2 Ry[4Z-Z2 -iZl (16.1.15) 
</p>
<p>whose minimum lies at Z=2 but at Z=2-5jl6. The corresponding energy is 
</p>
<p>E(2- 5/16)= -2(2- 5/16)2 Ry~ -77.5 eV (16.1.16) 
</p>
<p>which is much closer to the real answer. Notice also that it lies above it, as demanded 
by the inequality (16.1.1). By considering trial functions with more parameters, one 
</p>
<p>can get closer to the exact answer, and one can also feel the "resistance" to further 
</p>
<p>lowering. 
</p>
<p>t This is not in contradiction with Eq. ( 16.1.1) since we are using the wrong Hamiltonian when we neglect 
the Coulomb repulsion between the electrons. </p>
<p/>
</div>
<div class="page"><p/>
<p>A virtue of the variational method is that even a poor approximation to the 
</p>
<p>actual wave function can yield an excellent approximation to the actual energy. The 
</p>
<p>reason is the following. Suppose we had chosen a trial function 
</p>
<p>which contains a 10% contamination from the state IE.). The estimated energy 
</p>
<p>would have been 
</p>
<p>(EoiHIEo) +rlio(E.IHIE&bull;) Eo+O.OlE1 
</p>
<p>E(lf/) 1 + 1 ~ 1.01 
</p>
<p>~0.9~Eo+0.01EI 
</p>
<p>which is off by just 1%. (We are assuming that E 1 is not anomalously large.) 
</p>
<p>More generally, let 
</p>
<p>llfi)=IEo)+lolfl) (16.1.17a) 
</p>
<p>be a trial ket. Let us decompose I o VI) into parts parallel to and perpendicular to 
lEo): 
</p>
<p>In this state 
</p>
<p>I olfl) =I 81f/u) +I Olfl .L) 
</p>
<p>=a lEo)+ I Olf/ .L) 
</p>
<p>E[lf/] Eol1+al 2 +(olf/.LIHiolf/.L) 
</p>
<p>11 + al 2 + (Oif/ .LI Olfl .L) 
</p>
<p>=Eo+ 0(01[1 .L)2 
</p>
<p>(16.1.17b) 
</p>
<p>(16.1.18) 
</p>
<p>Thus the error in energy is of the second order in the error in the state vector. Notice 
</p>
<p>that I o If! 11 ) produces no error in energy. This is because rescaling the normalized 
eigenket does not change the mean energy. 
</p>
<p>All these results are true for any eigenket of H. If 
</p>
<p>is an approximation to IE,.), then by similar reasoning 
</p>
<p>E[lf/,.] = E, + O[(Oif/,.)2] (16.1.19) 
</p>
<p>Thus the eigenkets of H are characterized by the fact that when they are changed 
</p>
<p>to first order, there is no energy change to first order: the eigenkets of Hare stationary 
</p>
<p>points of E[lf!]. (The ground state happens, in addition, to be an absolute minimum.) 
</p>
<p>If we could carry out the impossible task of tabulating all the E[lf/] we can then 
</p>
<p>read off all the eigenstates by looking for the stationary points. This is of course not 
</p>
<p>433 
</p>
<p>THE 
</p>
<p>VARIATIONAL 
ANDWKB 
</p>
<p>METHODS </p>
<p/>
</div>
<div class="page"><p/>
<p>434 
</p>
<p>CHAPTER 16 
</p>
<p>a practical proposition. In practice, we use the following trick for finding the higher 
eigenvalues and eigenkets. Consider the case V=A.x 4 &bull; Since His parity invariant, 
the states will occur with alternating parity. Suppose we take a trial state with odd 
parity. Then in the expansion I'!')= L Cn In), Cn = &lt;nl '!') = 0 for all even n, because 
the integral of an even and an odd function is zero. Consequently the lowest energy 
that enters the averaging is E1 and we have the inequality 
</p>
<p>( 16.1.20) 
</p>
<p>So we expect that if we take a trial state with odd parity, one node (in one dimension, 
there is one extra node for each upward step in energy), and the usual behavior as 
lxl-+co, we can get a good estimate E1 and a rough picture of the corresponding 
wave function. What if we want to get a bound on E2? The general idea is of course 
the same, to consider trial states in whose expansion I E0 ) and I E1) do not appear. 
But this cannot be done simply by choosing trial states of some definite parity. What 
we can do is the following. We have approximate wave functions for the first two 
levels from the variational energy estimates. We can choose our trial states to be 
orthogonal to these. The corresponding bounds are not rigorous, for we do not 
know lEo) and IE1) exactly, but they may still be useful. 
</p>
<p>This general idea is easier to implement in three dimensions if His rotationally 
invariant. In this case the energy eigenstates have definite angular momentum. The 
ground state will have I= 0. By varying spherically symmetric trial functions we can 
estimate the ground-state energy. If we next choose I= 1 trial functions 
[l!'=R(r)Y7'], E[l!'] wil obey 
</p>
<p>where E 1 ~ 1 is the lowest energy level with l= I. We can clearly keep going up in l. 
Suppose we do this for the Coulomb problem. We know that at each /, the lowest 
energy corresponds to n =I+ 1. The variational method applied to I= 0, 1, 2, ... will 
yield energies close to those of the n = 1, 2, ... levels. Of course we must pay attention 
to the radial part of 'I' as well. For instance R (r) must behave like r1 as r-+0 in the 
angular momentum l sector. It must have the least number of nodes, namely zero, 
if it is to have the lowest energy for the given l. With these features built in, both 
the energy and wave function will come close to 'I' n,n- l.m. 
</p>
<p>We can also use any other operator that commutes with H in choosing trial 
functions. The angular momentum is especially convenient because its eigenfunctions 
are easy to write down and its eigenvalues are correlated (grow) with energy. 
</p>
<p>Exercise 16.1.1. * Try If! =exp(- ax 2 ) for V= ~ mro 2x 2 and find a 0 and E(ao). 
</p>
<p>Exercise 16.1.2. * For a particle in a box that extends from -a to a, try (within the box) 
1f!=(x-a)(x+a) and calculate E. There is no parameter to vary, but you still get an upper 
bound. Compare it to the true energy, Eo. (Convince yourself that the singularities in If!" at 
x =&plusmn;a do not contribute to the energy.) 
</p>
<p>Exercise 16.1.3. * For the attractive delta function potential V= -a V0 8(x) use a Gaussian 
trial function. Calculate the upper bound on Eo and compare it to the exact answer 
( -ma2 V~/21i 2 ) (Exercise 5.2.3). </p>
<p/>
</div>
<div class="page"><p/>
<p>Exercise 16.1.4 (Optional). For the oscillator choose 
</p>
<p>'l'=(x-a) 2(x+af, 
</p>
<p>=0, 
</p>
<p>Calculate E(a), minimize it and compare to fuil/2. 
</p>
<p>[x[sa 
</p>
<p>[x[ &gt;a 
</p>
<p>Exercise 16.1.5. * Solve the variational problem for the I= I states of the electron in a 
potential V= - e2 jr. In your trial function incorporate (i) correct behavior as r-+0, appropri-
ate to I= 1, (ii) correct number of nodes to minimize energy, (iii) correct behavior of wave 
</p>
<p>function as r-+ oo in a Coulomb potential (i.e., exponential instead of Gaussian damping). 
</p>
<p>Does it matter what m you choose for Y7'? Comment on the relation of the energy bound 
</p>
<p>you obtain to the exact answer. 
</p>
<p>16.2. The Wentzel-Kramers--Brillouin Method 
</p>
<p>Consider a particle of energy E in one dimension moving in a constant potential 
</p>
<p>V. The energy eigenfunctions are 
</p>
<p>p=[2m(E- V)]u2 (16.2.1) 
</p>
<p>where the &plusmn; signs correspond to right- and left-moving plane waves. The general 
</p>
<p>solution is a combination of both waves. The real and imaginary parts of lfl oscillate 
</p>
<p>in space with a wavelength ?c=27di/p or equivalently, the phase change per unit 
</p>
<p>length is a constant, p /fl. Suppose now that V, instead of being a constant, varies 
very slowly. We then expect that over a small region [small compared to the distance 
</p>
<p>over which V(x) varies appreciably] VJ will still behave like a plane wave, with the 
</p>
<p>local value of the wavelength: 
</p>
<p>).(x)=2n:fl= 2n:ti 
1 
</p>
<p>o 
</p>
<p>p(x) {2m[&pound;- V(x)]} 1" 
(16.2.2) 
</p>
<p>Since A varies with .x, the accumulated phase shift between x=O and x=x is given 
</p>
<p>by an integral, so that 
</p>
<p>lf!(X) = lf/(0) exp[ &plusmn; (i/n) r p(x') dx'] 
0 
</p>
<p>or more generally 
</p>
<p>lf/(X) = lf!(Xo) exp[ &plusmn; (i/fi) J .. ,' p(x') dx'] 
(l 
</p>
<p>(16.2.3) 
</p>
<p>Once again the &plusmn; stand for right- and left-moving waves, and the general solution 
</p>
<p>is formed by taking an arbitrary linear combination d both. As mentioned above, 
</p>
<p>we trust this formula only if the wavelength varies slowly. How slow is slow enough? 
</p>
<p>Note that although there is a well defined function A (x) at each x, it makes no sense 
</p>
<p>435 
</p>
<p>THE 
VARIATIONAL 
</p>
<p>ANDWKB 
</p>
<p>METHODS </p>
<p/>
</div>
<div class="page"><p/>
<p>436 
</p>
<p>CHAPTER 16 
</p>
<p>to speak of the wavelength at a point. The wavelength is a characteristic of a repetitive 
phenomenon and consequently defined only over a region that contains many repeti-
tions. Thus the statement "a position-dependent wavelength A. (x)" makes sense only 
if 8A. over a length A. is negligible compared to A.: 
</p>
<p>(16.2.4) 
</p>
<p>Let us now derive all of the above results more formally. The derivation will 
also provide corrections to this result and clarify the nature of the approximation. 
Our problem is to solve the equation 
</p>
<p>{ d
2 2m } 
</p>
<p>- 2+-2 [E- V(x)] lJf(x)=O 
dx Pi 
</p>
<p>or 
</p>
<p>Let us write 
</p>
<p>lJ!(x) = exp[icf&gt; (x)JPi] (16.2.5) 
</p>
<p>Since cf&gt; (x) is not assumed to be real, there is no loss of generalityj Feeding this 
form into the equation, we get 
</p>
<p>(16.2.6) 
</p>
<p>We now expand cf&gt; in a power series in Pi: 
</p>
<p>(16.2.7) 
</p>
<p>The logic is the following. If Pi--&gt;0, the wavelength A.=2nPijp tends to zero. Conse-
quently any potential can be considered slowly varying in this limit and our approxi-
mation Eq. (16.2.3) should become increasingly reliable. Conversely, any corrections 
to this formula can be traced to the fact that Pi isn't really zero. In situations where 
Pi may be treated as a small number, we hope that corrections may be computed in 
powers of Pi. 
</p>
<p>The WKB approximation (also called the semiclassical approximation) consists 
of keeping just the first two terms in Eq. (16.2.7). If we feed the truncated expansion 
</p>
<p>tIn other words, any complex number 'I'= p e';;=e1&cent;+Jnp=e'&cent;, where&cent;={$- i In p. </p>
<p/>
</div>
<div class="page"><p/>
<p>into Eq. (16.2.6) and group terms with the same fi dependence, we get 
</p>
<p>(I 6.2.8) 
</p>
<p>In the first approximation we concentrate on just the rz-2 term. This gives 
</p>
<p>c/Jb= &plusmn;p(x) 
</p>
<p>or 
</p>
<p>c/Jo(x) = &plusmn; r p(x') dx' (16.2.9) 
and 
</p>
<p>lf!(X) =A exp[ &plusmn; (i/1i) r p(x') dx'] 
= lf/(Xo) exp[ &plusmn; (i/fi) r p(x') dx'] 
</p>
<p>xu 
</p>
<p>(16.2.10) 
</p>
<p>where A was found by setting x=x0 in the first equation. All this agrees with our 
previous result. But we can go a step further and include the fi- 1 term in Eq. (16.2.8). 
We still choose cp 0 so that the fi 2 term continues to vanish. For the fi- 1 term to 
vanish, we need 
</p>
<p>ic/J&amp; = 24&gt;; c/Jb 
A, II 
</p>
<p>;~ =- 2icp] (16.2.11) 
</p>
<p>In c/Jb=- 2icpl + c 
</p>
<p>c/J1 = + iln( c/Jo) 112 + cj2i= iln p112 + c 
</p>
<p>To this order in fi, 
</p>
<p>(16.2.12) 
</p>
<p>437 
</p>
<p>THE 
VARIATIONAL 
</p>
<p>ANDWKB 
</p>
<p>METHODS </p>
<p/>
</div>
<div class="page"><p/>
<p>438 
</p>
<p>CHAPTER 16 
</p>
<p>or 
</p>
<p>[ Jl/2 [ f' J p(xo) i &middot; lf!(x) = 1f!(x0 ) p(x) exp &plusmn;lj p(x') dx' 
xo 
</p>
<p>(16.2.13) 
</p>
<p>The probability density associated with lf!(X) behaves as [p(x)r 1&bull; This inverse 
dependence on the classical velocity is familiar to us from the classical probability 
distribution Pc1 (x) we studied in connection with the oscillator. Conversely, we could 
have written down Eq. (16.2.13) by combining classical and quantum reasoning, i.e., 
by semiclassical reasoning. We know classically that if a particle starts at x0 
with momentum p(x0 ) = {2m[E- V(x0)J} 112, it will have a momentum p(x) = 
{2m[E- V(x)J} 112 when it gets to x. Now we argue that sincep/fi is the phase change 
per unit length of the quantum wave function, its phase must be (1/fi) J p(x') dx'. 
As for its amplitude, we argue that since the probability Pc1 (x) -1/v(x), ilfli ~ 
lj[v(x)] 112 ~ 1/[p(x)f 12. 
</p>
<p>Whenever we do an approximate calculation, it behooves us to verify at the end 
that the solution we obtain is consistent with the assumptions that went into its 
derivation. Our fundamental assumption in the recursive approach to Eq. (16.2.8) 
has been that the part of the equation with less powers of 1i is more important than 
the part with more powers, because 1i is so small. This is fine as long as the coefficients 
of the various powers of 1i are not anomalously big or small. For instance, if the 
coefficient of the fi- 2 term [- ( &cent;0)2 + p2(x)] is very small, of the order of, say, 1i, then 
it makes no sense to ignore the fi- 1 term in comparison. The same is true if the 
coefficient of the fi- 1 is as large as fi- 1&bull; So we demand that the absolute magnitude 
of the first term be much bigger than that of the second. Since in the solution, 
cp0 = p(x), we choose ( t/J0/fi) 2 as a measure of the first term and for similar reasons 
cp0j1i as a measure of the second. The condition for the validity of the WKB approxi-
mation (to this order) is 
</p>
<p>l~ol&laquo;l~or (16.2.14) 
</p>
<p>or 
</p>
<p>(16.2.15) 
</p>
<p>which agrees with our heuristic expectation, Eq. (16.2.4). 
</p>
<p>Connection with the Path Integral Formalism 
</p>
<p>Let us now rederive the semiclassical wave functions ofEq. (16.2.10) in the path 
integral approach, in the semiclassical approximation, in which one writes 
</p>
<p>Uc~(xt, x'O) =A e(ifli)ScJ[xt;x"O] (16.2.16) </p>
<p/>
</div>
<div class="page"><p/>
<p>where Sc1[xt; x'O] is the action for the classical path connecting the end points (x'O) 
</p>
<p>and (xt). We have chosen the initial time t' = 0, assuming the problem has time 
translation invariance. The prefactor A has no dependence on x, x', or t in our 
approximation in which the "area" under the functional integral is replaced by the 
</p>
<p>value of the integrand at the stationary point times some constant. We will illustrate 
</p>
<p>the procedure in the following situation. 
</p>
<p>(I) The potential is always negative and goes to a constant, chosen for convenience 
</p>
<p>to be zero, as lxl-.oo. 
(2) The particle is in a quantum state with energy E&gt; 0. This means that at the 
</p>
<p>classical level there are no turning points, a fact which will prove significant. 
</p>
<p>Our strategy will be as follows. We will nrst show how to project out the exact 
</p>
<p>wave functions from the exact propagator by performing some integrals. Then we 
</p>
<p>will compute the propagator in the semiclassical approximation and project out the 
</p>
<p>corresponding (approximate) wave functions of Eq. (16.2.10)). 
</p>
<p>How are the wave functions to be extracted from the propagator? In general, 
</p>
<p>we have 
</p>
<p>where the sum may have to be an integral if the spectrum is continuous. Indeed, in 
</p>
<p>the present problem this is the case. In the asymptotic region lxl-.oo, the solutions 
</p>
<p>must be plane waves with momentum 
</p>
<p>So we can use p as a label for the E&gt; 0 eigenstates. (Hereafter the subscript on Poo 
</p>
<p>will be dropped.) At each energy E, there will be two solutions just as in the free 
</p>
<p>particle case, with E = p 2 /2m. The wave functions will, however, not be plane waves 
</p>
<p>in the interior and these are what we are after. So let us begin with 
</p>
<p>U(xt;x')= foo 2 ~fz lf/p(X)lf/;(x')e-ip'tf2m"+BS 
-oo 
</p>
<p>(16.2.17) 
</p>
<p>where BS stands for the sum over bound states. These will be dropped since we will 
</p>
<p>soon be projecting out the states at some positive energy. Factors like rr in the p-
integration do not matter too much, we just want to get the right x-dependence of 
</p>
<p>the wave functions and not their normalization. 
Remember that U(t) has been constructed to propagate the system forward in 
</p>
<p>time, i.e., it is to be used for t &gt; 0. Let us now define its transform 
</p>
<p>U(x, x', z)= {"" dt U(xt; x') eizt/1i z=E+ie (16.2.18a) 
</p>
<p>439 
</p>
<p>THE 
</p>
<p>VARIATIONAL 
</p>
<p>ANDWKB 
</p>
<p>METHODS </p>
<p/>
</div>
<div class="page"><p/>
<p>440 
</p>
<p>CHAPTER 16 
</p>
<p>where cis a positive infinitesimal introduced to ensure convergence as t-+ c:JJ. In what 
follows we will not distinguish between s and a finite positive factor times t: and 
will ignore terms of higher order in t:. 
</p>
<p>It is readily seen by combining the two equations above that 
</p>
<p>J. 1E v p(x) vr;(x') U(x, x', z) =2m .. 2 
" 2rr:i p - 2mE- it: 
</p>
<p>Writing (to order s) 
</p>
<p>we factorize as follows: 
</p>
<p>and use the formula (derived and explained in Appendix A4) 
</p>
<p>to obtain 
</p>
<p>I 
&middot;21--&plusmn;ino(x-a) 
</p>
<p>(x-a)'fis .x-a 
</p>
<p>x [.:J&gt;( &middot;~. :; )+ino(p-.ji;E) 
\p- v' 2mE, 
</p>
<p>:.l.)l~~~E ) + in i) (p + .j?.;F;) J 
</p>
<p>(l6.2.18b) 
</p>
<p>where['}' means the principal value integral is to be evaluated. As it stands, U depends 
</p>
<p>on the eigenfunctions not just at the energy E, but nearby energies also, due to 
</p>
<p>the principal value integral. So we form the combination which singles out the 
</p>
<p>eigenfunctions at just one energy: 
</p>
<p>U(x, x', z) + [ U(x', x, z)]* 
</p>
<p>= J7j f' dp(~pp(x)vr;(:x'){8(p- ,J2mE) + 8(p+ .j2mE) J 
---,x: --
</p>
<p>= {; [~p 1-(x)ij!*.-, -(x') + v 1 --,-(x)~p* . (x')] 'J 2&pound; v' 2mE -v' ""mE -,.: 2mE --- v 2m&pound; (16.2.19) </p>
<p/>
</div>
<div class="page"><p/>
<p>We now compare this with Vel (x, x', z): 
</p>
<p>Vcl(x, x', z) = f oo dt Vel (x, x', t) eUI~HE +i&bull;Jt = f oo dt eU/R)So~[x,x',tJ e&lt;ii~HE +i&bull;Jt 
0 0 
</p>
<p>Since Vel was itself evaluated in the stationary point approximation of a functional 
</p>
<p>integral (on the basis of the smallness of 1i), to be consistent, we must evaluate the 
</p>
<p>ordinary integral in t also by stationary phase, by setting the argument of the expon-
</p>
<p>ent to its value at the point t * defined by 
</p>
<p>as 
-+E=-E 1+E=O 
iJt c 
</p>
<p>(16.2.20) 
</p>
<p>and equating the integral to the integrand at this point times some constant. For the 
</p>
<p>stationary point, we have recalled Eq. (2.8.18) and dropped the convergence factor e 
</p>
<p>since it is not needed. What the stationary point does is to choose from all trajectories 
</p>
<p>connecting the two given end points (with various travel times) the one whose class-
</p>
<p>ical energy equals the energy E of the quantum state [and whose travel time is t* = 
</p>
<p>t*(E)]. 
</p>
<p>Note that previously we were interested in trajectories that connected x' toxin 
</p>
<p>a fixed time t. Given the equations of motion are second order in time, there will be 
</p>
<p>just one trajectory that can meet these requirements. But now we are asking for a 
</p>
<p>trajectory with a fixed energy connecting the two points x' and x with no restriction 
</p>
<p>on travel time. This can generally have many solutions. For example, in a confining 
</p>
<p>potential like the oscillator (or in any bound state) a particle leaving x' with some 
</p>
<p>energy E will hit any other point x (within the turning points) an infinite number 
</p>
<p>of times as it rattles back and forth. Each such orbit will have a different travel time 
</p>
<p>and Vel (x, x', E) will receive contributions from an infinite number of stationary 
</p>
<p>points obeying Eq. (16.2.20). 
</p>
<p>In the present problem with no turning points, there will be just two solutions-
</p>
<p>call them R and L-which are moving to the right or to the left. The right mover 
</p>
<p>can go from x' to x if x' &lt; x and the left mover if x' &gt; x. 
Let us proceed with our calculation bearing all this in mind, and start with 
</p>
<p>Ucl(x, x', E)= A 1 I e(i/R)[So~(x,x',t*)+Et*] 
R,L 
</p>
<p>where A' is a new constant. We now manipulate as follows bearing in mind that 
</p>
<p>Ec1 = E is conserved on the classical path: 
</p>
<p>Sel[x,x',t*]= r&middot; (T-V)dt= r&middot; 2Tdt-Et* 
= f'&bull; p(x(t)) dx dt- Et* = fx p(x") dx"- Et* 
</p>
<p>0 ~ x' 
</p>
<p>= W[x, x', E]- Et* (16.2.2la) 
</p>
<p>441 
THE 
</p>
<p>VARIATIONAL 
ANDWKB 
</p>
<p>METHODS </p>
<p/>
</div>
<div class="page"><p/>
<p>442 
</p>
<p>CHAPTER l6 
</p>
<p>It follows that 
</p>
<p>[ 
</p>
<p>' ~X ] 
Uc1(x, x', E) =A' L exp 1 j p(x") dx" 
</p>
<p>R. L fi , ,&middot; 
</p>
<p>/ . \ 
</p>
<p>=A' L expl !_ W[x, x', E]) 
R. L \ fi 
</p>
<p>(16.2.21 b) 
</p>
<p>If x &gt; x', the classical momentum has to be positive and we set p(x) = 
J2m(E- V(x)) whereas if x &lt; x' we make the other choice for the square root. Thus 
</p>
<p>Uc1 (x, x', E)= O(x- x}A' expG t f -./2m(E- V(x")) dx"]) 
+ O(x'- x)A' exp( - ~ r-r .,j2m(E- V(x")) dx" J) ( 16.2.22) 
</p>
<p>We now find 
</p>
<p>Uc~(x, x', E)+ U,~ (x'x, E) =A' exp(~ [ r-./2m(E- V(x")) dx" J) 
X 
</p>
<p>+A'exp( -~[JxJ2m(.E-V(x'')) dx"]) (16.2.23) 
\ 
</p>
<p>Comparing this to Eq. (16.2.19) we find 
</p>
<p>lfl _.-_(X)lfl* .&middot;-. (x') '-"exi &plusmn;~ r rx_J2~(E --- V(x")) dx"]' )\ 
&plusmn;-v'2mE &plusmn;v2mE \ n LJX 
</p>
<p>It is easily seen that this may be written as 
</p>
<p>lfl &plusmn; (x) = lfi(Xo) exp( &plusmn; ~ [ rJ2m(E- V(x")) dx" ]) 
_l('o 
</p>
<p>Several comments are in order. 
</p>
<p>First, if we want to get Eq. (16.2.13), with the factor p 112 , we need to do a 
</p>
<p>more accurate calculation. So far we have evaluated the functional integral and the 
</p>
<p>ordinary t integral by setting the integral equal to the value of the integrand at the 
stationary point, times some constant to represent the "area" around this point. To 
</p>
<p>get Eq. (16.2.13) we must approximate the integrands at the stationary point by 
Gaussians and do the Gaussian integrals. If you are interested in the details, consult 
</p>
<p>one of the works in the Bibliography at the end of Chapter 21. 
Second, note that in going from U(t) to U(E), (at fixed x', x), we shifted 
</p>
<p>our attention from paths with a fixed time to paths with a fixed energy. Since </p>
<p/>
</div>
<div class="page"><p/>
<p>E= -i3Sc~/ot, one is trading t for the derivative of Sc1 with respect tot. This clearly 
requires a Legendre transformation, as explained in Section 2.5. It is clear from Eq. 
</p>
<p>(16.2.2la) that W(E) is the Legendre transform in question. This idea is pursued 
further in Exercise (16.2.1). 
</p>
<p>Finally, let us look at the combination U (x, x', E)+ U*(x', x, E) we were led 
to in our attempt to filter out one energy. From our discussion of time reversal you 
should expect that the complex conjugated version of U (with initial and final points 
exchanged) stands for the time-reversed propagator. If so, the integral of this combi-
nation over positive times is the integral of U for all times. It is now dear that such 
an integration done on Eq. (16.2.17) will indeed produce a factor o(E-p 2 /2m) which 
projects out states at the energy E. This point is pursued in Exercise (16.2.3). 
</p>
<p>Exercise 16.2.1. Consider the function W(E) introduced in Eq. (l6.2.2la). Since -E is 
</p>
<p>the t derivative of S(t), it follows that W(E), must be the Legendre transform of S. In this 
</p>
<p>case t must emerge as the derivative of W(E) with respect to E. Verify that differentiation of 
</p>
<p>the formula 
</p>
<p>r
' 
</p>
<p>W(E) = j2~ui::.. V(x")) dx" 
&bull; x' 
</p>
<p>gives the time t taken to go from start to finish at this preassigned energy. 
</p>
<p>Exercise 16.2.2. Consider the free particle problem using the approach given above. Now 
</p>
<p>that you know the wave functions explicitly, evaluate the integral in Eq. (l6.2.18b) by contour 
</p>
<p>integration to obtain 
</p>
<p>U(x, x', t) = O(x- x') e\'ln)-!2mE(x &middot;&middot;x') + (}(x' _ x) ei&middot;&middot; ifijJ,I2mE(x ,') 
</p>
<p>In doing the contour integrals, ask in which half-plane you can dose the contour for a given 
</p>
<p>sign of x-x'. Compare the above result to the semiclassical result and make sure it all works 
</p>
<p>out. Note that there is no need to form the combination U(x, x', t) + U*(x, x', t); we can 
calculate both the principal part and the delta function contributions explicitly because we 
</p>
<p>know the p-dependence of the integrand explicitly. To see this more clearly, avoid the contour 
</p>
<p>integral approach and use instead the formula for (x :&plusmn;:.is) 1 given above. Evaluate the principal 
</p>
<p>value integral and the contribution from the delta function and see how they add up to the 
</p>
<p>result of contour integration. Although both routes are possible here, in the problem with 
</p>
<p>V # 0 contour integration is not possible since the p-dependence of the wave function in the 
</p>
<p>complex p plane is not known. The advantage of the U + U* approach is that it only refers 
to quantities on the real axis and at just one energy. 
</p>
<p>Exercise 16.2.3. Let us take a second look at our derivation. We worked quite hard to 
</p>
<p>isolate the eigenfunctions at one energy: we formed the combination U(x, x', z) + U*(x, x', z) 
to get rid of the principal part and filter out the delta function. Now it is clear that if in Eq. 
</p>
<p>(16.2.18a) we could integrate in the range -oo-:::;, 1 s oo, we would get the 8 function we want. 
What kept us from doing that was the fact the U ( t) was constructed to be used for t &gt; 0. 
However, if we use the time evolution operator e .. u;&bull;Jm for negative times, it will simply tell 
</p>
<p>us what the system was doing at earlier times, assuming the same Hamiltonian. Thus we can 
</p>
<p>make sense of the operator for negative times as well and define a transform that extends 
over all times. This is true in classical mechanics as well. For instance, if a stone is thrown 
</p>
<p>straight up from a tall building and we ask when it will be at ground level, we get two answers, 
</p>
<p>443 
</p>
<p>THE 
</p>
<p>VARIATIONAL 
</p>
<p>ANDWKB 
</p>
<p>METHODS </p>
<p/>
</div>
<div class="page"><p/>
<p>444 
</p>
<p>CHAPTER 16 
</p>
<p>V(x l 
</p>
<p>Vmox ------
</p>
<p>-Vol----' 
</p>
<p>Figure 16.1. A typical barrier penetration problem. The particle 
has energy &pound;&gt;0, but is restrained by the barrier since Vmax&gt;E. 
But there is an amplitude for it to tunnel through the barrier 
and escape to infinity as a free particle. 
</p>
<p>one with negative t corresponding to the extrapolation of the given initial conditions to earlier 
times. Verify that U(x, x', z) + U*(x, x', z) is indeed the transform of U(t) for all times by 
seeing what happens to (xle-u;n&gt;Hrlx') under complex conjugation and the exchange x +-+ x'. 
Likewise, ask what happens to Uc1 when we transform it for all times. Now you will find that 
no matter what the sign of x- x' a single right moving trajectory can contribute-it is just 
that the time of the stationary point, t*, will change sign with the sign of x- x'. (The same 
goes for a left moving trajectory.) 
</p>
<p>Tunneling Amplitudes 
</p>
<p>The WKB formula can also be used to calculate tunneling amplitudes provided 
x(x) = {2m[V(x)- &pound;]} 112 varies slowly. As an example, consider a particle trapped 
in a potential shown in Fig. 16.1. If its energy is positive, there exists some probability 
for it to penetrate the barrier and escape to infinity as a free particle. In the first 
approximation, the ratio of 'I' at the point of escape, Xe and at the outer wall of the 
well, x0 , is 
</p>
<p>'lf(Xe) = 'lf(Xo) exp( ~ f' i{2m[ V(x)- El} 112 dx) 
xo 
</p>
<p>(16.2.24) 
</p>
<p>( 16.2.25) 
</p>
<p>The mean lifetime of the particle inside the well may be estimated by the following 
semiclassical computation. Since the particle inside the well has a kinetic energy T= 
E- V=E+ V0 , its velocity is v=[2m(E+ V0)] 112/m and it bangs against the outer 
wall at a frequency f=vj2x0 &bull; Upon collision, there is a probability of escape e-r. 
Consequently probability of escape in 1 second is 
</p>
<p>R=[2m(E+ V0)] 112 e-r 
</p>
<p>2mxo 
</p>
<p>The mean lifetime is then r = I I R. 
</p>
<p>(16.2.26) 
</p>
<p>Note that just as a particle inside can tunnel out a particle withE&lt; Vmax can 
tunnel in from outside and get captured. A standard example of tunneling and 
capture is provided by a decay in which a nucleus emits an a particle.t At short 
distances, the force between the a particle and the nucleus is attractive (the nuclear 
force beats the Coulomb repulsion), while at larger distances, the Coulomb repulsion 
</p>
<p>t The a particle has two protons and two neutrons in it. </p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 16.2. A typical bound state problem. The WKB approximation to the wave function works nicely 
</p>
<p>except in the shaded bands near the classical turning points x, and x2. To join ll'n to 1/ft and 1/fm, one 
</p>
<p>solves the Schrodinger equation in these bands after approximating the potential by a linear function 
</p>
<p>within each band. 
</p>
<p>dominates. The sum of the two potentials is roughly described by a potential of the 
</p>
<p>type shown in Fig. 16.1, with x playing the role of the radial coordinate. (The 
</p>
<p>centrifugal barrier must be added onto the two potentials if the a particle comes out 
with nonzero orbital angular momentum.) Thus a particles withE&lt; Vmax can tunnel 
out from within, or get captured from without. 
</p>
<p>Exercise 16.2.4. Alpha particles of kinetic energy 4.2 MeV come tunneling out of a 
</p>
<p>nucleus of charge Z=90 (after emission). Assume that V0 = 0, x0 = w-&bull;z em, and V(x) is just 
Coulombic for x;:: x0 &bull; (See Fig. 16.1.) Estimate the mean lifetime. {Hint: Show y = 
</p>
<p>(8Ze2/liv)[cos-&bull; y112 -y112(1-y) 112], where y=x0 lxe. Show that y&laquo;l and use cos- 1 y 112 ~ 
hr- y 112 before calculating numbers.} 
</p>
<p>The derivation of the tunneling amplitude, Eq. (16.2.24), is not straightforward 
</p>
<p>in the path integral formalism due to the fact that there exists no classical path that 
</p>
<p>can take the particle across the barrier. There is, however, such a path in the "imagin-
</p>
<p>ary time" formalism. This will be detailed in Chapter 21. 
</p>
<p>Bound States 
</p>
<p>The WKB method can be applied to approximate bound state energies and 
</p>
<p>wave functions. Consider a particle (Fig. 16.2) bound by a potential V(x). In the 
</p>
<p>figure, x 1 and x2 are the classical turning points for the energy E. Let us see how 
</p>
<p>the quantization of E emerges in this formalism. We know that in the classically 
</p>
<p>forbidden regions I and III, the wave function will be a damped exponential. For 
instance 
</p>
<p>'l'm(x)~ 1 112 exp(-!fx {2m[V(x')-El} 112 dx') 
{2m[ V(x)- E]} 1i 
</p>
<p>(16.2.27) 
</p>
<p>445 
</p>
<p>THE 
</p>
<p>VARIATIONAL 
</p>
<p>ANDWKB 
</p>
<p>METHODS </p>
<p/>
</div>
<div class="page"><p/>
<p>446 
</p>
<p>CHAPTER 16 
</p>
<p>In the classically allowed region II, it will be oscillating. We assume it is a real 
functiont with two free parameters A and B: 
</p>
<p>'l'n (x) = A I/2 cos[_!_ Jx p(x') dx' + B] 
[p(x)] 1i 
</p>
<p>(16.2.28) 
</p>
<p>[The two real parameters A and B replace the one complex parameter 11f(x0) used 
previously.] Unfortunately, neither Eq. (16.2.27) from Eq. (16.2.28) is applicable 
near the turning points. Formally this is because [p(x)ri 12 and {2m[ V(z)- E]} -I/2 
</p>
<p>blow up there. Physically it is because the wavelength tends to infinity there and the 
requirement that V(x) varies little over a wavelength becomes impossible to meet. 
It is therefore impossible to match the approximate llf1 , 'l'n, and 'I'm and thereby 
determine the allowed energies as one does in the simple examples with piecewise 
constant potentials. The problem is surmounted as follows. Near each turning point, 
we define a transition region (shaded bands in the figure) inside which we solve the 
problem by going back to Schrodinger's equation. If V(x) is slowly varying, it may 
be approximated by a linear function in these regions. For instance near XI 
</p>
<p>V(x) ~ V(xi) + V' &middot; (x- xi) 
</p>
<p>=E+ V' &middot; (x-xi) (16.2.29) 
</p>
<p>The exact solutions with this V(x) are then matched to the WKB solutions outside 
the shaded region, that is to say, to the damped exponentials on one side and the 
oscillating cosine on the other. 
</p>
<p>The analysis&sect; near xi would yield the following function in region II: 
</p>
<p>llfn(x) = A I/2 cos[_!_ fx p(x') dx' _!!__] 
[p(x)] 1i XI 4 
</p>
<p>(16.2.30) 
</p>
<p>while the one near x2 would yield 
</p>
<p>llfn(x) = A' IJZ cos[_!_ fx p(x') dx' + !!..] 
[p(x)] 1i ~ 4 
</p>
<p>(16.2.31) 
</p>
<p>For the two solutions to coincide, A and A' must have the same magnitude and 
the difference in phase between the two cosines must be a multiple of 1C : 
</p>
<p>1 IX 1 IX ~ p(x') dx'-~ p(x') dx'-~=nfC, 
Xt ~ 
</p>
<p>t Recall Theorem 16, Sec. 5.6. 
&sect; The details are omitted. 
</p>
<p>n=O, 1, 2, ... </p>
<p/>
</div>
<div class="page"><p/>
<p>or 
</p>
<p>fx, p(x) dx=(n+ ~)1r1i 
X! 
</p>
<p>(16.2.32) 
</p>
<p>or 
</p>
<p>f p(x) dx=(n+ ~)2tr1i (16.2.33) 
where &sect; denotes the integral over a full cycle, from Xt to x2 and back. If n is even, 
A=A', if odd A= -A'. 
</p>
<p>Equation (16.2.32) expresses the quantization of energy since the integral and 
</p>
<p>limits on the left-hand side are functions of energy and the other parameters such 
</p>
<p>as particle mass. 
As an example, consider a particle in a linear potential V(x) =kjxj. The turning 
</p>
<p>points are 
</p>
<p>(16.2.34) 
</p>
<p>and the quantization condition is 
</p>
<p>fE/k fE/k -Elk [2m(E-kjxj)] 112 dx=2 
0 
</p>
<p>[2m(E-kx)] 112 dx=(n+ ~)1i1r (16.2.35) 
</p>
<p>The n, k, m, 1i dependence of E can be found by a scaling argument. Let us define a 
</p>
<p>variable y by 
</p>
<p>x= (Ejk)y (16.2.36) 
</p>
<p>in terms of which we have 
</p>
<p>or 
</p>
<p>(16.2.37) 
</p>
<p>The constant of proportionality may be found by carrying out they integral. The 
</p>
<p>result is 
</p>
<p>[ 3k1itr ( 1 ) ]
213 
</p>
<p>E = n+--
n 4(2m) 112 2 
</p>
<p>(16.2.38) 
</p>
<p>447 
</p>
<p>THE 
</p>
<p>VARIATIONAL 
</p>
<p>ANDWKB 
</p>
<p>METHODS </p>
<p/>
</div>
<div class="page"><p/>
<p>448 
</p>
<p>CHAPTER 16 
</p>
<p>If the method is applied to the potential V(x) = Jcx 4, we would get, by the scaling 
</p>
<p>argument, 
</p>
<p>(16.2.39) 
</p>
<p>where c is a constant that may be found by carrying out a dimensionless integral. If 
the WKB energy levels are compared to the actual ones (obtained either by analytic 
</p>
<p>solution or by accurate numerical integration of Schrodinger's equation) we would 
</p>
<p>find that the agreement is excellent for all but very small n. In the i\.x 4 case, for 
</p>
<p>example, 
</p>
<p>Eo(WKB) 
&middot;-&middot;----=0.716 
&pound; 0( numerical) 
</p>
<p>E,(WKB) &middot;&middot;&middot;&middot;&middot;=0.992 
E1(numerical) 
</p>
<p>The agreement gets even better as we move up in n. Thus the WKB method 
</p>
<p>complements the variational method, which works best for the lowest levels. The 
</p>
<p>improved accuracy with increasing n is reasonable in view of the fact that as we move 
up in energy, the transitional region near the turning points (where the approximation 
</p>
<p>breaks down) plays a decreasingly important role.t 
</p>
<p>What about the WKB wave functions? They too get better at large n, except of 
</p>
<p>course near the turning points, where they blow up due to the [p(x)r 112 factor. If, 
</p>
<p>however, we actually solve the Schrodingcr equation ncar the turning points after 
</p>
<p>approximating the potential by a linear function, the blowup can be avoided and 
</p>
<p>the agreement with the true eigenfunctions greatly enhanced. 
</p>
<p>The WKB wave function has another feature that agrees with the exact answer: 
</p>
<p>the wave function has n nodes (n = 0, I, 2, ... ) in the nth level. We see this analytically 
</p>
<p>from Eq. (16.2.30) for 1j! 11 (x), and Eq. (16.2.32) for the phase integral 
</p>
<p>(1/1i) r p(x') dx'. As X goes from x, to X2, the phase cjJ goes from -n"/4 to 
mr + n)4 and cos(&cent;) vanishes n times. We can in fact understand the quantization 
rule, Eq. ( 16.2.32), as follows. If we assume, in the first approximation, that II' must 
</p>
<p>vanish in the classically forbidden region, it follows that it must undergo an integral 
</p>
<p>number of half-cycles (half-wavelengths) in the interval x 1 ::s;x::s;x2 . This leads to the 
</p>
<p>Bohr Sommerfeld quantization rule. 
</p>
<p>r= p(x) dx= (n +!)fin, 
X] 
</p>
<p>n=O, I, 2, ... ( 16.2.40) 
</p>
<p>But we know that II' doesn't vanish at the turning points and has an exponential tail 
</p>
<p>in the classically forbidden region. Consequently the number of half-cycles completed 
</p>
<p>between x 1 and x2 is somewhat less than n + I. The connection procedure tells us that 
</p>
<p>t There are some exceptional cases, such as the ham;onic oscillator, where the method gives the exact 
energies for all n, </p>
<p/>
</div>
<div class="page"><p/>
<p>it is in fact n+ 1 and hence the usual quantization rule, Eq. (16.2.32). If, however, 
If/ actually vanishes at x 1 and x2 because the potential barrier there is infinite (as in 
</p>
<p>the case of a particle in a box), Eq. (16.2.40) [and not Eq. (16.3.32)] is relevant.t 
</p>
<p>One can also consider an intermediate case where the barrier is infinite at one turning 
</p>
<p>point and not at the other. In this case the quantization rule has an (n+3/4) factor 
</p>
<p>in it. 
The WKB method may also be applied in three dimensions to solve the radial 
</p>
<p>equation in a rotationally invariant problem. In the I= 0 state, there is no centrifugal 
</p>
<p>barrier, and the WKB wave function has the form 
</p>
<p>u (r)- 1 1/2 sin[! f r p(r') dr'], 
[p(r)] 1i o 
</p>
<p>p= {2m[E- V(r)]} 112 (16.2.41) 
</p>
<p>where the lower limit in the phase integral is chosen to be 0, so that U(O) =0. The 
</p>
<p>quantization condition, bearing in mind that the barrier at r = 0 is infinite, is 
</p>
<p>n=O, 1, 2, ... (16.2.42) 
</p>
<p>where rmax is the turning point. This formula is valid only if V(r) is regular at the 
</p>
<p>origin. If it blows up there, the constant we add ton is not 3/4 but something else. 
</p>
<p>Also if /#0, the centrifugal barrier will alter the behavior near r=O and change both 
</p>
<p>the wave function and this constant. 
</p>
<p>Exercise 16.2.5. &bull; In 1974 two new particles called the yt and yt' were discovered, with 
</p>
<p>rest energies 3.1 and 3.7 GeV, respectively (1 GeV = 109 eV). These are believed to be nonrela-
</p>
<p>tivistic bound states of a "charmed" quark of mass m= 1.5 GeV/c 2 (i.e., mc 2 =1.5 GeV) and 
</p>
<p>an antiquark of the same mass, in a linear potential V(r) = V0 + kr. By assuming that these 
</p>
<p>are the n = 0 and n = l bound states of zero orbital angular momentum, calculate V0 using 
the WK.B formula. What do you predict for the rest mass of yt", the n = 2 state? (The measured 
</p>
<p>value is ~4.2 GeV /c 2.) [Hints: (I) Work with GeV instead of eV. (2) There is no need to 
</p>
<p>determine k explicitly.] 
</p>
<p>Exercise 16.2.6. Obtain Eq. (16.2.39) for the A.x4 potential by the scaling trick. 
</p>
<p>Exercise 16.2.7* Find the allowed levels of the harmonic oscillator by the WKB method. 
</p>
<p>Exercise 16.2.8. Consider the /=0 radial equation for the Coulomb problem. Since V(r) 
</p>
<p>is singular at the turning point r=O, we can't use (n+3/4). 
</p>
<p>(l) Will the additive constant be more or less than 3/4? 
</p>
<p>(2) By analyzing the exact equation near r=O, it can be shown that the constant equals 
</p>
<p>l. Using this constant show that the WKB energy levels agree with the exact results. 
</p>
<p>t The assumption that V(x) may be linearized near the turning point breaks down and this invalidates 
Eq. (16.2.29). 
</p>
<p>449 
</p>
<p>THE 
</p>
<p>VARIATIONAL 
</p>
<p>ANDWKB 
</p>
<p>METHODS </p>
<p/>
</div>
<div class="page"><p/>
<p>Time-Independent 
Perturbation Theory 
</p>
<p>17.1. The Formalism 
</p>
<p>17 
</p>
<p>Time-independent perturbation theory is an approximation scheme that applies 
</p>
<p>in the following context: we know the solution to the eigenvalue problem of the 
Hamiltonian H 0 , and we want the solution to H = H 0 +If, where If is small com-
pared to H 0 in a sense to be made precise shortly. For instance, H 0 can be the 
Coulomb Hamiltonian for an electron bound to a proton, and H 1 the addition due 
to an external electric field that is weak compared to the proton's field at the (average) 
location of the electron. One refers to H 0 as the unperturbed Hamiltonian and H 1 as 
the perturbing Hamiltonian or perturbation. 
</p>
<p>We proceed as follows. We assume that to every eigenket IE~)= ln&deg;) of H 0 with 
eigenvalue E~, there is an eigenket In) of H with eigenvalue&pound;,.:!: We then assume 
that the eigenkets and eigenvalues of H may be expanded in a perturbation series&sect;: 
</p>
<p>(17.1.1) 
</p>
<p>(17.1.2) 
</p>
<p>The superscript k on each term gives the power of (the matrix element of) H 1 that 
it is expected to come out proportional to. A term with superscript equal to k is 
called a kth-order term. (Clearly a product like E~lnf) is a term of order k+k.) We 
hope that as the order increases, the terms get systematically smaller; this is when 
we can say that H 1 is smalL When we find explicit formulas for Ink) and&pound;~, these 
</p>
<p>ideas will be sharpened. 
To find the terms in the expansions for In) and En, we start with the eigenvalue 
</p>
<p>equation 
</p>
<p>(17.1.3) 
</p>
<p>t We are assuming that ln&deg;) is nondegenerate. The degenerate case follows. 
&sect;We assume ln&deg;) is normalized (to unity). The nonn of In) will be discussed shortly. 451 </p>
<p/>
</div>
<div class="page"><p/>
<p>452 
</p>
<p>CHAPTER 17 
</p>
<p>or 
</p>
<p>(Ho+HI)[Ino)+lnl)+&middot;. &middot;] 
</p>
<p>= (E~+ E~+ &middot; &middot; &middot; )[ln&deg;) + ln 1) + &middot; &middot; &middot;] (17.1.4) 
</p>
<p>We approach these equations as we did the differential equation in the WKB approxi-
mation. Recall that there we had an equation with terms of order n-2, n- 1, &bull;&bull;&bull; , etc. 
We first ignored all but the n-2 terms and solved for f/J0 &bull; We then fed this into the 
n-1 part to determine f/J 1 &bull; (We could have gone on this way, though we chose to 
stop there.) Likewise, in the present case, we first consider the zeroth-order terms of 
Eq. (17.1.4). We get the equation 
</p>
<p>(17.1.5) 
</p>
<p>Notice that the zeroth-order quantities ln&deg;) and E~ are indeed independent of H 1 
</p>
<p>(or, equivalently, they depend on the zeroth power of H 1). By assumption, this 
equation may be solved and the eigenvectors ln&deg;) and eigenvalues E~ determined. 
So we move on to the first-order terms. We get the equation 
</p>
<p>(17.1.6) 
</p>
<p>(17.1.7) 
</p>
<p>i.e., the first-order change in energy is the expectation value of H 1 in the unperturbed 
state. Notice that E~ is indeed proportional to the first power of H 1&bull; Let us next dot 
both sides ofEq. (17.1.6) with (m0l,m#n, to get 
</p>
<p>or 
</p>
<p>(17.1.8) 
</p>
<p>Since m#n, this equation determines all the components of ln 1) in the eigenbasis of 
H 0, except for the component parallel to ln&deg;), let's call it ln~ 1 ). We determine it by 
the requirement that In) is normalized to this order.t In obvious notation, we have 
</p>
<p>(17.1.9) 
</p>
<p>t Recall that even in eigenvalue problems that can be solved exactly, there is the arbitrariness in the norm 
of the vector. To this order, only 1n:1) alters the length of ln&deg;). [See Eq. (17.1.10).) </p>
<p/>
</div>
<div class="page"><p/>
<p>which leads to 
</p>
<p>or 
</p>
<p>(17.1.10) 
</p>
<p>This means that 
</p>
<p>a real (17.1.11) 
</p>
<p>Using 
</p>
<p>1 + ia = eia (to this order) (17.1.12) 
</p>
<p>we get 
</p>
<p>(17.1.13) 
</p>
<p>where the prime on 2:' means that m#n. Since In) has an arbitrariness in its overall 
phase, even after it is normalized, let us change its phase by the factor e-ia in Eq. 
</p>
<p>(17.1.13). This gets rid of the phase factor multiplying ln&deg;) and does nothing to the 
</p>
<p>first-order piece, to this order. Calling the perturbed eigenket with the new phase 
</p>
<p>also In), we get the result to first order: 
</p>
<p>(17.1.14) 
</p>
<p>Notice that ln1) is orthogonal to ln&deg;) and proportional to the first power of H 1 (as 
</p>
<p>anticipated). We determine E~ from the second-order part ofEq. (17.1.4): 
</p>
<p>(17.1.15) 
</p>
<p>Dotting with (n&deg;1 and using the results from lower order (ln 1 )=1n~)) we obtain 
</p>
<p>E~= (n&deg;IH1In1) 
</p>
<p>(noiHtlmo)(moiHtlno) 
</p>
<p>=2:' Eo-Eo 
m n m 
</p>
<p>We can go on to higher orders, but we choose to stop here. 
</p>
<p>(17.1.16) 
</p>
<p>(17.1.17) 
</p>
<p>453 
</p>
<p>TIME-
</p>
<p>INDEPENDENT 
</p>
<p>PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>454 
</p>
<p>CHAPTER 17 
</p>
<p>Before we turn to examples, let us consider some general features of our results. 
First we note that the energy to a given order is determined by the state vector to 
the next lower order, see Eqs. (17.1.7) and (17.1.16). This is in accord with the 
remarks made in the study of the variational method. The physics behind this phe-
nomenon will become clear when we consider a few examples. Next we ask under 
what conditions the perturbation expansion is good, namely, when the correction 
terms are small compared to the zeroth-order (unperturbed) results. The answer 
follows from Eq. (17.1.14). A necessary condition for ln1) to be small compared to 
ln&deg;) is that 
</p>
<p>(17.1.18) 
</p>
<p>Thus we see that the condition depends on (1) the absolute size of H 1 (i.e., if it is 
due to some external field, the magnitude of the field); (2) the matrix elements of 
H 1 between unperturbed states; and (3) the energy difference between the levels. If 
the unperturbed eigenstate is ln&deg;), the perturbation mixes in orthogonal states lm0); 
this mixing is directly proportional to the matrix element &lt;m0IH1In&deg;) and inversely 
proportional to the energy difference between the two levels, which measures the 
"rigidity" of the system. If for any reason the above inequality is not fulfilled (say 
due to degeneracy, E~ = E~) we must turn to an alternate formalism called degenerate 
perturbation theory to be described later in this chapter. 
</p>
<p>17 .2. Some Examples 
</p>
<p>Consider a particle of charge q and mass m in a harmonic oscillator potential 
V=~mm 2 x 2 &bull; Suppose we apply an external electric field of magnitude f along the 
positive x direction. This corresponds to an electrostatic potential c/J= -fx and a 
potential energy V= -qfx. Thus 
</p>
<p>(17.2.1) 
</p>
<p>We wish to handle H 1 by perturbation theory. Let us first calculate the first-order 
shift in energy, given by 
</p>
<p>( 17.2.2) 
</p>
<p>where ln&deg;) is just the nth state of the unperturbed oscillator. We can see that E 1 
vanishes in many ways. At a formal level, since 
</p>
<p>( 
fz )1,'2 
</p>
<p>X= -- (a+at) 
2mm 
</p>
<p>(17.2.3) </p>
<p/>
</div>
<div class="page"><p/>
<p>it has no diagonal matrix elements. The physics of what is happening is more transpar-
</p>
<p>ent in the coordinate basis, where 
</p>
<p>E~ = -qf f ( !fl~)*xlfl~ dx 
= -qf f llfl~l 2 x dx ( 17.2.4) 
</p>
<p>Now !fl~(x), being the unperturbed eigenfunction, has definite parity ( --1 t. Conse-
quently llf/~1 2 is an even function, while the external potential is an odd function. 
Thus the average interaction with the external field is zero, for the particle is as likely 
to be found in the region of potential &cent;1 as in the region of potential --q,. Notice 
that E~ is the energy of interaction of the unperturbed configuration jn&deg;), with the 
applied field. Consequently this is not the whole story, for the configuration itself 
will get modified by the external field to I n&deg;) +I n1) + &middot; &middot; &middot; , and we should really be 
considering the energy of interaction of the perturbed configurations and the applied 
field. But this is a distinction that is at least a second-order effect, for the change 
8jn) = jn) -jn&deg;) in the configuration is at least a first-order effect and the interaction 
of 81 n) with the applied field involves another order of H 1&bull; So let us calculate the 
perturbed eigenket to first order and then energy levels to second order. From Eq. 
(17.1.14). 
</p>
<p>(17.2.5) 
</p>
<p>Thus to first order, the perturbation mixes the state jn&deg;) with the states immediately 
above and below it. It was stated earlier that E~ --- E?, measures the "rigidity" of the 
system. We find in this example that this quantity is proportional to uJ, which in the 
mass-spring case measures the force constant. How does the wave function of the 
perturbed state look? This is not transparent from the above equation, but we expect 
that it will represent a probability distribution that is no longer centered at and 
symmetric about x = 0, but instead is biased toward positive x (for that is the direction 
of the external field). We will return to confirm this picture quantitatively. 
</p>
<p>The second-order energy shift (which reflects the fact that the configuration of 
the system is not jn&deg;) but jn&deg;)+ln 1)), is 
</p>
<p>(17.2.6) 
</p>
<p>455 
</p>
<p>TIME-
</p>
<p>INDEPENDENT 
</p>
<p>PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>456 
</p>
<p>CHAPTER 17 
</p>
<p>The present problem is a nice testing ground for perturbation theory because it 
</p>
<p>may be exactly solved. This is because H may be written as 
</p>
<p>P 2 1 7 
H=--+&middot;&middot;&middot;&middot;mw&middot;X 2 -qfX 
</p>
<p>2m 2 
</p>
<p>=-+-m())" X __ q_ -- Cf...:__ P
2 1 7 (' f )2 1 2 ( 
</p>
<p>2m 2 mo/, 2 rna/ 
(17.2. 7) 
</p>
<p>This Hamiltonian also describes an oscillator of frequency w, but is different in that 
</p>
<p>(i) the oscillator is centered at x = qfjmw2 ; (ii) each state has a constant energy 
( -lf2/2m()) 2 ) added to it. Thus the eigenfunctions of Hare just the eigenfunctions 
of H 0 shifted ay qf/m()) 2 and the eigenvalues are En= E?, &middot;-l/2 /2m()) 2 . The classical 
picture that goes with Eq. (17.2.7) is clear: the effect of a constant force qf on a 
</p>
<p>mass coupled to a spring of force constant m()) 2 is to shift the equilibrium point to 
</p>
<p>x=qf/mo}. (Imagine a mass hanging from a spring attached to the ceiling and ask 
</p>
<p>what gravity does to its dynamics.) Let us compare these exact results with the 
</p>
<p>perturbation theory. Consider the energy 
</p>
<p>( 17.2.8) 
</p>
<p>Since H 1 is proportional to qf, the power of qf gives the order of the term. According 
to Eq. (17.2.8), there is no first-order shift in energy, and the second-order shift is 
-lf2 j2m()) 2 , which agrees with Eq. (17.2.6). Had we tried to go to higher orders, 
we would have found nothing more. 
</p>
<p>Now consider the state vectors. The exact result is 
</p>
<p>(17.2.9) 
</p>
<p>where T(a) is the operator that translates the system by an amount a. Since we are 
</p>
<p>working to first order in qj; 
</p>
<p>so that 
</p>
<p>7'( r' ') ... ;(q'jmw'~)P J &middot;(&middot; qf )\ p qJ;mor =e J. ~ -z -2 &middot; 
m()) n 
</p>
<p>=l-/-9Lj' (limw)l/2. a~at 
\m())2ii \ 2 1 
</p>
<p>qf limw t 0 
[ 
</p>
<p>' ) ' '1/2 J 
In)= /-(-,2 (--) (a-a) In) 
</p>
<p>m()) fi. , 2 , 
</p>
<p>( 17.2.1 0) 
</p>
<p>(17.2.11) </p>
<p/>
</div>
<div class="page"><p/>
<p>which agrees with Eq. (17.2.5). It is clear that computing In) to higher order in 
perturbation theory will be equivalent to expanding T to higher orders in qf 
</p>
<p>Exercise 17.2.1. * Consider H' = A.x4 for the oscillator problem. 
( 1) Show that 
</p>
<p>, 3n2), . , 2 
En = -,-2 11 + 2n -r2n ] 
</p>
<p>4m'm &middot; 
</p>
<p>(2) Argue that no matter how small ..1. is, the perturbation expansion will break down 
</p>
<p>for some large enough n. What is the physical reason? 
</p>
<p>Exercise 17.2.2. * Consider a spin-1/2 particle with gyromagnetic ratio yin a magnetic 
field B = Bi + B0 k. Treating B as a perturbation, calculate the first- and second-order shifts in 
energy and first-order shift in wave function for the ground state. Then compare the exact 
</p>
<p>answers expanded to the corresponding orders. 
</p>
<p>Exercise 17.2.3. In our study of the H atom, we assumed that the proton is a point 
</p>
<p>charge e. This leads to the familiar Coulomb interaction ( -e2/r) with the electron. (1) Show 
</p>
<p>that if the proton is a uniformly dense charge distribution of radius R, the interaction is 
</p>
<p>r&lt;C,;R 
</p>
<p>r&gt;R 
r 
</p>
<p>(2) Calc,&bull;!dte the first-order shift in the ground-state energy of hydrogen due to this 
</p>
<p>modification. You may assume e -R;ao::: 1. You should find E' = 2e2 R2 /Sa~. 
</p>
<p>Exercise 17.2.4. * ([) Prove the Thomas .. Reiche Kuhn sum rule 
</p>
<p>where In) and In') are eigenstates of H= P 2 /2m+ V(X ). (Hint: Eliminate theE, .... E" factor 
</p>
<p>in favor of H.) 
</p>
<p>(2) Test the sum rule on the nth state of the oscillator. 
</p>
<p>Exercise 17.2.5 (Hard). We have seen that if we neglect the repulsion e2 jr, 2 between the 
</p>
<p>two electrons in the ground state of He, the energy is -8 Ry = -108.8 eV. Treating e2/r 12 as 
a perturbation, show that 
</p>
<p>&lt;JOO, lOOIH'IIOO,IOO)=~ Ry 
</p>
<p>so that E0&deg;+E6=-5.5.Ry=-74.8eV. Recall that the measured value is -78.6eV and the 
</p>
<p>variational estimate is -77.5 eV. [Hint: &lt;H') can be viewed as the interaction between two 
</p>
<p>concentric, spherically symmetric exponentially falling charge distributions. Find the potential 
</p>
<p>&cent; (r) due to one distribution and calculate the interaction energy between this potential and 
the other charge distribution.] 
</p>
<p>457 
TIME-
</p>
<p>INDEPENDENT 
</p>
<p>PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>458 
</p>
<p>CHAPTER 17 
</p>
<p>Selection Rules 
</p>
<p>The labor involved in perturbation theory calculations is greatly reduced by the 
</p>
<p>use of selection rules, which allow us to conclude that certain matrix elements of H 1 
</p>
<p>are zero without explicitly calculating them. They are based on the idea that if 
</p>
<p>then 
</p>
<p>(17.2.12)t 
</p>
<p>Proof 
</p>
<p>Q.E.D. 
</p>
<p>Consider for example H 1 = .:tz, which is invariant under rotations around the z axis. 
Then [L=, H 1] = 0 and 
</p>
<p>(17.2.13) 
</p>
<p>(This result also follows from the Wigner- Eckart theorem.) Or if H 1 is parity invari-
</p>
<p>ant, say H 1 = 1Z2, then its matrix element between states of opposite parity is zero. 
There is a simple way to understand Eq. (17.2.12). To say that [Q, Ji]=O is to 
</p>
<p>say that Ii "carries no Q"; in other words, when it acts on a state it imparts no Q 
to it. We see this as follows. Consider lw 1), which carries a definite amount of the 
variable Q, namely, w1 : 
</p>
<p>(17.2.14) 
</p>
<p>Let us measure Q in the state after Ii acts on it: 
</p>
<p>(17.2.15) 
</p>
<p>We find it is the same as before, namely, m1&bull; The selection rule then merely reflects 
</p>
<p>the orthogonality of eigenstates with different w. 
This discussion paves the way for an extension of the selection rule to a case 
</p>
<p>where H 1 carries a definite amount of n. For instance, if H 1 is a tensor operator 
T~, it carries angular momentum (k, q) and we know from the Wigner-Eckart 
</p>
<p>theorem that 
</p>
<p>(17.2.16) 
</p>
<p>! a stands for other quantum numbers that label the state. </p>
<p/>
</div>
<div class="page"><p/>
<p>i.e., that the matrix element vanishes unless I a2hm2) has the angular momentum 
that obtains when we add to (j1m1) the angular momentum (kq) imparted by the 
</p>
<p>operator. For instance, if H 1 = A.Z- T1&deg;, 
</p>
<p>(17.2.17) 
</p>
<p>(17.2.18) 
</p>
<p>Another example of this type is an operator that is not parity invariant, but parity 
</p>
<p>odd. An example is X, which obeys 
</p>
<p>(17.2.19) 
</p>
<p>You can verify that if X acts on a state of definite parity, it changes the parity of 
</p>
<p>the state. Thus the matrix element of X between eigenstates of parity vanishes unless 
</p>
<p>they have opposite parity. More generally, if 
</p>
<p>(17.2.20) 
</p>
<p>then the matrix element of n between two parity eigenstates vanishes unless they 
have opposite parity. 
</p>
<p>We get more selection rules by combining these selection rules. For instance, 
</p>
<p>we can combine the angular momentum and parity selection rules for the vector 
</p>
<p>operators R to get (in the case of no spin, J = L), 
</p>
<p>(17.2.21) 
</p>
<p>We rule out the possibility 12 = 11 by the parity selection rule, for states of orbital 
</p>
<p>angular momentum I have definite parity ( -1( Equation (17.2.21) is called the 
</p>
<p>dipole selection rule. 
</p>
<p>We now consider an example that illustrates the use of these tricks and a few 
</p>
<p>more. The problem is to determine the response of the hydrogen atom in the ground 
</p>
<p>state to a constant external electric field E=Ck. This is called the Stark effect. Let 
</p>
<p>us first calculate H 1&bull; We do this by determining Yf1, its classical counterpart and 
</p>
<p>then making the operator substitution. If r1 and r2 are the position vectors of the 
</p>
<p>459 
</p>
<p>TIME-
</p>
<p>INDEPENDENT 
</p>
<p>PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>460 
</p>
<p>CHAPTER 17 
</p>
<p>electron and proton, respectively, and cP (r) is the electrostatic potential due to E, 
then 
</p>
<p>.ff1 = -e&cent; (rr) + e&cent; (r2) 
</p>
<p>=e[c/J(r2)- c/J(rt)] 
</p>
<p>=e(r1-r2 )&middot;E (recaliE=-'Vc/J) 
</p>
<p>= er &middot; E (17 .2.22)" 
</p>
<p>where r is the relative coordinate or equivalently the position vector of the electron 
</p>
<p>in the CM frame in the limit m/ M = 0. &pound; 1 is called the dipole interaction, for in 
</p>
<p>terms of 
</p>
<p>(17.2.23) 
</p>
<p>the electric dipole moment of the system, 
</p>
<p>(17.2.24) 
</p>
<p>(This is the electric analog of &pound; = -JL &middot;B.)~ Thus, for the given electric field 
</p>
<p>(17.2.25) 
</p>
<p>Let us now calculate the first-order shift in the energy of the ground state 1100)&sect;: 
</p>
<p>Eloo= &lt;JOOjeZ8JlOO) (17.2.26) 
</p>
<p>We can argue that Eloo = 0 either on the grounds of parity or the Wigner-Eckart 
theorem. More physically, &pound; 1100 vanishes because in the unperturbed state, the electron 
</p>
<p>probability distribution is spherically symmetric and the electron samples c/J (r) and 
c/J(-r)=-c/J(r) equally. Another way to say this is that the unperturbed atom has 
</p>
<p>no mean electric dipole moment &lt;JL&gt; (by parity or the Wigner Eckart theorem) so 
that 
</p>
<p>Eloo= &lt;IOOI- JL &middot; E1100) = -&lt;lOOIJLilOO) &middot; E =0 (17.2.27) 
</p>
<p>But we expect the second-order energy shift to be nonzero, for the external field will 
</p>
<p>shift the electron distribution downward and induce a dipole moment which can 
interact with E. So let us calculate 
</p>
<p>(17.2.28) 
</p>
<p>~ [n the rest of this chapter we will omit the subscript e on Jl,. 
</p>
<p>&sect;When we discuss hydrogen, we will use the symbol[n/m), rather than [{n/m)0 ) to denote the unperturbed 
</p>
<p>state, </p>
<p/>
</div>
<div class="page"><p/>
<p>where 
</p>
<p>0 0 ( 1 ) (1- n2 ) Ewo-Entm=-Ry 1- n2 =Ry T (17.2.29) 
</p>
<p>Unlike in the case of the oscillator, the sum now involves an infinite number of 
terms. Although.~cwe can use dipole selection rules to reduce the sum to 
</p>
<p>(17.2.30) 
</p>
<p>let us keep the form in Eq. (17.2.28) for a while. There are several ways to proceed. 
</p>
<p>Method 1. Since the magnitude of the energy denominator grows with n, we 
have the inequality 
</p>
<p>But since 
</p>
<p>we get 
</p>
<p>2 ezrlfz ' &bull; 2 
IEwol ~- 0 ------ 0 -- I l(n/miZilOO)I 
</p>
<p>l&pound;1- &pound;2l nlm 
</p>
<p>I' I (n/m1Zil00)1 2 
nlm 
</p>
<p>=I' (lOOIZin/m)(nlmiZilOO) 
nlm 
</p>
<p>=I (lOOIZin/m)(nlm\ZilOO)- (l001Zil00)2 
nlm 
</p>
<p>=al-O=a6 (17.2.31) 
</p>
<p>(17.2.32) 
</p>
<p>We can also get a lower bound on IEJ~lol by keeping just the first tem1 in Eq. (17.2.30) 
(since all terms have the same sign): 
</p>
<p>(17.2.33) 
</p>
<p>461 
</p>
<p>TIME-
</p>
<p>INDEPENDENT 
</p>
<p>PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>462 
</p>
<p>CHAPTER 17 
</p>
<p>Now, 
</p>
<p>(17.2.34) 
</p>
<p>so that 
</p>
<p>( 17.2.35) 
</p>
<p>We thus manage to restrict IE,tol to the interval 
</p>
<p>( 17.2.36) 
</p>
<p>Method 2. Consider the general problem of evaluating 
</p>
<p>(17.2.37) 
</p>
<p>If it weren't for the energy denominator, we could use the completeness relation to 
eliminate the sum (after adding and subtracting the m=n term). There exists a way 
to eliminate the energy denominator.t Suppose we can find an operator Q such that 
</p>
<p>( 17 .2.38) 
</p>
<p>then 
</p>
<p>2_ , (n&deg;IH'Im0)(m0IOH0 -H00In&deg;) 
En- I 0 0 
</p>
<p>m En-Em 
</p>
<p>=I' (n&deg;IH'Im0)(m010ln&deg;) 
</p>
<p>(17.2.39) 
</p>
<p>which calls for computing just three matrix elements. But it is not an easy problem 
to find the Q that satisfies Eq. (17.2.38). (There are, however, exceptions, see Exercise 
17.2.7.) A more modest proposal is to find 0 such that 
</p>
<p>( 17.2.40) 
</p>
<p>for a given ln&deg;). You can verify that this is all it takes to derive Eq. (17.2.39) for 
this value of n. In the problem we are interested in, we need to solve 
</p>
<p>(17.2.41) 
</p>
<p>t See A. Dalgarno and J. T. Lewis, Proceedings of the Royal Society, A233, 70 (1955). </p>
<p/>
</div>
<div class="page"><p/>
<p>By writing this equation in the coordinate basis and assuming n is a function of 
coordinates and not momenta, we can show that 
</p>
<p>maoetff ('2 cos (} ) n --- ---+aorcos (} 
coordinate basis fi 2 
</p>
<p>The exact second-order shift is then 
</p>
<p>IETool =I (1001H1QI100) -01 
</p>
<p>=I (1001eZtffQil00)1 
</p>
<p>=~a6tff 2 =~a~tff 2 &bull; (~) 
</p>
<p>= (0.84)~a61ff 2 
</p>
<p>(17.2.42) 
</p>
<p>(17.2.43) 
</p>
<p>which is roughly in the middle of the interval we restricted it to by Method 1. 
</p>
<p>Exercise 17.2.6. Verify Eq. (17.2.34). 
</p>
<p>Exercise 17.2. 7. * For the oscillator, consider H 1 = -qfX. Find an n that satisfies Eq. 
(17.2.38). Feed it into Eq. (1 7.2.39) forE; and compare with the earlier calculation. 
</p>
<p>Exercise 17.2.8. Fill in the steps connecting Eqs. (17.2.41) and (17.2.43). Try to use 
symmetry arguments to reduce the labor involved in evaluating the integrals. 
</p>
<p>We argued earlier that &pound; 1200 represents the interaction of the induced dipole 
moment with the applied field. How big is the induced moment 11? One way to find 
out is to calculate&middot; (Jl) in the perturbed ground state. An easier way to extract it 
from E?oo. Suppose we take a system that has no intrinsic dipole moment and turn 
on an external electric field that starts at 0 and grows to the full value of E. During 
this time the dipole moment grows from 0 to Jl. If you imagme charges &plusmn; q separated 
by a distance x along E, you can see that the work done on the system as x changes 
by dx is 
</p>
<p>dW=-qtff dx 
</p>
<p>=-Iff dj1 
</p>
<p>If we assume that the induced moment is proportional to E: 
</p>
<p>JJ.=aE 
</p>
<p>(where a is called the polarizability), then 
</p>
<p>dW=-atffdtff 
</p>
<p>(17.2.44) 
</p>
<p>(17.2.45) 
</p>
<p>463 
</p>
<p>TIME-
INDEPENDENT 
</p>
<p>PERTURBATION 
THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>464 
</p>
<p>CHAPTER 17 
</p>
<p>or 
</p>
<p>(17.2.46) 
</p>
<p>We identify W with E?oo and determine the polarizability 
</p>
<p>18 , 18 _ A 3 Al a=- a 0 ~- (0.5 ) ~0.56 _-
4 4 
</p>
<p>(17.2.47) 
</p>
<p>If we use a more accurate value a0 = 0.53 A, we get a= 0.67 A3, which is in excellent 
agreement with the measured value of 0.68 A3. For a given E, we can get Jl. from 
Eq. (17.2.45). 
</p>
<p>Finally note that &pound; 1200 is negative. From Eq. ( 17 .1.17) it is dear that the second-
</p>
<p>order shift in the ground-state energy is always negative (unless it vanishes). Since 
</p>
<p>E~ measures the energy shift due to the first-order change in the ground-state state 
</p>
<p>vector, we conclude that the system changes its configuration so as to lower its energy 
</p>
<p>of interaction with the external field. 
</p>
<p>17 .3. Degenerate Perturbation Theory 
</p>
<p>In the face of degeneracy (E~ =&pound;?,,)the condition for the validity of the perturba-
</p>
<p>tion expansion, 
</p>
<p>(17.3.1) 
</p>
<p>is impossible to fulfill. The breakdown of the method may be understood in less 
</p>
<p>formal terms as follows. 
</p>
<p>Let us consider the case when neither H 0 nor H 0 + H 1 is degenerate. For the 
purposes of this argument imagine that H 1 is due to some external field that can be 
</p>
<p>continuously varied from zero to its full value. As the total Hamiltonian grows from 
</p>
<p>H 0 to H 0 + H 1, the corresponding eigenbasis changes continuously from ln&deg;) to In). 
It is this continuous or analytic variation of the eigenbasis with the perturbation 
</p>
<p>that makes it possible to find In) starting with ln&deg;), the way one finds the value of 
</p>
<p>some analytic function at the point x +a starting at the point x and using a Taylor 
</p>
<p>series. Consider now the case when H 0 has a degenerate subspace and H 0 + H 1 is 
nondegenerate in this subspace. (More general cases can be handled the same way.) 
</p>
<p>Imagine starting with the basis In) and slowly turning off the perturbation. We will 
</p>
<p>end up with a basis [n&deg;) of H 0 . If we now turn on the perturbation, we can retrace 
our path back to )n). It is clear that if we start with this basis, lii0), we can evaluate 
</p>
<p>In) perturbatively. But since H 0 is degenerate, we needn't have started with this 
</p>
<p>basis; we could have started with some other basis ln&deg;), chosen randomly. But if we 
start with any basis except [n&deg;), and turn on the external field of infinitesimal size, 
</p>
<p>the change in the basis will not be infinitesimal. It is this nonanalytic behavior that 
is signaled by the divergence in the first-order matrix element. [This can be compared 
</p>
<p>to the divergence of the first derivative in the Taylor series where f(x) is discontinu-</p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 17.1. An example of the degenerate problem from V'(R). In 
the x-y plane, which is the degenerate subspace, we must start not 
with some arbitrarily chosen pair of basis vectors II 0 ) and J2&deg;), but 
with the pair Jl0 ) and JZ0 ) which diagonalizes H 1&bull; 
</p>
<p>13&gt; 
</p>
<p>ous.] So, we must start with the right basis in the degenerate space. We have already 
characterized this basis as one we get if we start with In) and slowly turn off H 1&bull; A 
more useful characterization is the following: it is a basis that diagonalizes H 1 within 
the degenerate space. Why? Because, if we start with this basis, the first-order pertur-
bation coefficient [Eq. (17.1.8)] does not blow up, for the (off-diagonal) matrix 
element in the numerator vanishes along with energy denominator whenever ln&deg;) 
and lm0 ) belong to the degenerate space. Figure 17.1 depicts a simple example from 
W3(R), where the x-y plane is the degenerate space and 11&deg;) and 12&deg;) are randomly 
chosen basis vectors in that subspace. The proper starting point is the pair ll0), 12&deg;), 
which diagonalizes H 1 in the x-y plane. 
</p>
<p>It is worth noting that to find the proper starting point, we need to find the 
basis that diagonalizes H 1 only within the degenerate space and not the full Hilbert 
space. Thus even if we work with infinite-dimensional spaces, the exact diagonaliza-
tion will usually have to be carried out only in some small, finite-dimensional 
subspace. 
</p>
<p>Let us consider, as a concrete example, the Stark effect in the n = 2 level of 
hydrogen. (We ignore spin, which is a spectator variable.) Are we to conclude that 
there is no first-order shift because 
</p>
<p>(2/mlelh"ZI2/m) =0 (17.3.2) 
</p>
<p>by parity invariance, or equivalently, because the atom in these states has no intrinsic 
dipole moment? No, because these states need not provide the correct starting points 
for a perturbative calculation in view of the degeneracy. We must first find the basis 
in then= 2 sector which diagonalizes H 1&bull; Using the selection rules, which tell us that 
only two of the 16 matrix elements are nonzero, we get 
</p>
<p>nlm I 200 210 211 21-1 
200 
</p>
<p>[~ 
A 0 
</p>
<p>] HI-+ 210 0 0 (17.3.3) 211 0 0 
21-1 0 0 
</p>
<p>where 
</p>
<p>A= (200let9'Z1210) = -3e@"a0 (17.3.4) 
</p>
<p>465 
</p>
<p>TIME-
</p>
<p>INDEPENDENT 
PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>466 
</p>
<p>CHAPTER 17 
</p>
<p>Exercise 17.3.1. * Use the dipole selection rules to show that H 1 has the above form and 
carry out the evaluation of L'l.. 
</p>
<p>Since H 1 is just~ times the Pauli matrix ax in the m=O sector, we infer that 
</p>
<p>its eigenvalues are &plusmn; ~ and that its eigenstates are [1200) &plusmn; 1210) ]/2112. In the lml = 
1 sector the old states 12, 1, &plusmn; 1) diagonalize H 1&bull; Our calculation tells us the following. 
</p>
<p>(1) The zeroth-order states stable under the perturbation are 12, 1, &plusmn; 1) and 
</p>
<p>[1200) &plusmn; 1210) ]/i 12. 
(2) The first-order shift &pound; 1 is zero for the first two states and &plusmn;~ for the next 
</p>
<p>two. (Note that ~ is negative.) 
</p>
<p>Notice that the stable eigenstates for which &pound; 1 # 0 are mixtures of I= 0 and I= 
</p>
<p>1. Thus they have indefinite parity and can have a nonzero intrinsic dipole moment 
</p>
<p>which can interact with E and produce a first-order energy shift. From the energy 
</p>
<p>shift, we infer that the size of the dipole moment is 3ea0 &bull; 
</p>
<p>Degenerate perturbation theory is relevant not only when the levels are exactly 
</p>
<p>degenerate but also when they are close, that is to say, when the inequality (17.3.1) 
</p>
<p>is not respected. In that case one must diagonalize H 0 + H 1 exactly in the almost 
degenerate subspace. 
</p>
<p>Exercise 17.3.2. Consider a spin-! particle (with no orbital degrees of freedom). Let 
</p>
<p>H =As;+ B(S;- S};), where S; are 3 x 3 spin matrices, and A&raquo; B. Treating the B term as a 
</p>
<p>perturbation, find the eigenstates of H 0 =AS; that are stable under the perturbation. Calculate 
</p>
<p>the energy shifts to first order in B. How are these related to the exact answers? 
</p>
<p>Fine Structure 
</p>
<p>The Coulomb potential (- e2 jr) does not give the complete interaction between 
</p>
<p>the electron and the proton, though it does provide an excellent first approximation.t 
</p>
<p>There are "fine-structure" corrections to this basic interaction, which produce energy 
</p>
<p>shifts of the order of a 2 times the binding energy due to the Coulomb potential. 
</p>
<p>Since the electron velocity (in a semiclassical picture) is typically f3=vjc-::::.O(a), 
</p>
<p>these are corrections of the order ( v / c f relative to binding energy, which is itself 
proportional to (vjc)2&bull; Thus these are relativistic in origin. There are two parts to 
</p>
<p>this effect. 
The first reflects the fact that to order (vjc)4 the kinetic energy of the electron 
</p>
<p>is not p 2 /2m but 
</p>
<p>2 4 
</p>
<p>T= (c2p2 + m2c4)I;2_ mc2=f!_-~+ O(p6 or v6) 
2m 8m3c 
</p>
<p>We now wish to calculate the effect of this extra term 
</p>
<p>(17.3.5) 
</p>
<p>( 17 .3.6) 
</p>
<p>t We consider here just the fine structure of hydrogen. The analysis may be extended directly to hydrogen-
like atoms. We also ignore the difference between the reduced mass and the electron mass. </p>
<p/>
</div>
<div class="page"><p/>
<p>treating it as a perturbation. Since Hr is rotationally invariant, it is diagonal in the 
lnlm) basis. (In other words, the lnlm) basis is stable under this perturbation.) So 
we can forget about th!C' fact that the levels at each n are degenerate and determine 
E} simply from 
</p>
<p>l 
E}= --3- 2 &lt;nlmiP 41nlm) 
</p>
<p>8m c 
</p>
<p>We evaluate the matrix element by noting that 
</p>
<p>so that 
</p>
<p>E}= - ~ [(E~) 2 + 2E~e 2 I!) + e4 I~) J 
2mc \r nlm \r nlm 
</p>
<p>From the virial theorem [Eq. (13.1.34)] 
</p>
<p>-(~) =2E~ 
nlm 
</p>
<p>while from Exercise ( 17.3 .4) 
</p>
<p>e4 4Eln 
</p>
<p>a6n3(1+1/2) l+l/2 
</p>
<p>so that 
</p>
<p>E}=- (E~)2(-3+~) 
2mc2 I+ 1/2 
</p>
<p>(17.3.7) 
</p>
<p>(17.3.8) 
</p>
<p>(17.3.9) 
</p>
<p>(17.3.10) 
</p>
<p>(17.3.11) 
</p>
<p>(17.3.12) 
</p>
<p>The other relativistic effect is called the spin-orbit interaction. Its origin may be 
understood as follows. The Coulomb interaction (- e2 jr) is the whole story only if 
the electron is at rest. If it moves at a velocity v, there is an extra term which we 
find as follows. In the electron rest frame, the proton will be moving at a velocity 
</p>
<p>467 
</p>
<p>TIME-
INDEPENDENT 
</p>
<p>PERTURBATION 
THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>468 
</p>
<p>CHAPTER 17 
</p>
<p>( - v) and will produce a magnetic field 
</p>
<p>e v x r 
B= --
</p>
<p>c 
{17.3.13) 
</p>
<p>The interaction of the magnetic moment of the electron with this field leads to the 
</p>
<p>spin-orbit energy 
</p>
<p>e f..l'l 
</p>
<p>me r 3 
</p>
<p>So we expect that in the quantum theory there will be a perturbation 
</p>
<p>I ., , \ 
</p>
<p>H =(-&middot;&middot;&middot;~&middot;&middot;&middot;&middot; (-.!_)S&middot;L. s.o. 1 
me/ me r 
</p>
<p>However, the correct answer is half as big: 
</p>
<p>(17.3.14) 
</p>
<p>(17.3.15) 
</p>
<p>(17.3.16) 
</p>
<p>The reason is that the "rest frame of the electron" doesn't have a fixed velocity 
</p>
<p>relative to the CM of the atom since the motion of the electron is not rectilinear. 
</p>
<p>Thus Yf1 s.o deduced in the comoving frame does not directly translate into what must 
</p>
<p>be used in the CM frame. The transformation and the factor of 1/2 were found by 
</p>
<p>Thomas.t In Chapter 20 we will derive Eq. (17.3.16) from the Dirac equation, which 
</p>
<p>has relativistic kinematics built into it. The Thomas factor of I /2 will drop out 
</p>
<p>automatically. 
</p>
<p>Since H, o involves the spin, we must now reinstate it. Since the states at a given 
</p>
<p>n are degenerate, we must start with a basis that diagonalizes H, o .&bull; Since we can 
</p>
<p>rewrite Hs.o as 
</p>
<p>(17.3.17) 
</p>
<p>:j: L H. Thomas Nature 117,574 (1926). </p>
<p/>
</div>
<div class="page"><p/>
<p>the states of total angular momentum suggest themselves. In this basis, 
</p>
<p>(}', m'; l', l/21Hsol}, m; l, 1/2) 
</p>
<p>(17.3.18) 
</p>
<p>(Note that two states with the same totaljm, but built from different l's, are ortho-
gonal because of the orthogonality of the spherical harmonics. Thus, for example, at 
</p>
<p>n = 2, we can build j = 1/2 either from l = 0 or l = 1. The states IJ = 1/2, m; 0, 1 /2) 
and IJ= 1/2, m; 1, l/2) are orthogonal.) Feeding}=!&plusmn; l/2 into Eq. (17.3.18) we get 
</p>
<p>(17.3.19) 
</p>
<p>where the upper and lower values correspond to j = l &plusmn; I /2. Using the result from 
Exercise 17. 3.4 
</p>
<p>(17.3.20) 
</p>
<p>we get 
</p>
<p>(17.3.21) 
</p>
<p>This formula has been derived for l=/0. When 1=0, (ljr3 ) diverges and (I~&middot;S) 
vanishes. But if we set/= 0 in Eq. (17.3.21) we get a finite limit, which in fact happens 
</p>
<p>to give the correct level shift for l = 0 states. This will be demonstrated when we 
study the Dirac equation in Chapter 20. The physical origin behind this shift (which 
</p>
<p>is clearly not the spin-orbit interaction) will be discussed then. Since E~o. and E} 
</p>
<p>are both a 4 effects, we combine them to get the total fine-structure energy shift 
</p>
<p>(17.3.22) 
</p>
<p>for both}=!&plusmn; 1/2. 
</p>
<p>469 
</p>
<p>TIME-
</p>
<p>INDEPENDENT 
</p>
<p>PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>470 
</p>
<p>CHAPTER 17 
</p>
<p>The fine-structure formula can be extended to other atoms as well, provided we 
</p>
<p>make the following change in Eq. (17.3.19): 
</p>
<p>where Vis the potential energy of the electron in question. Consider, for example, 
</p>
<p>the n = 4 states of potassium. We have seen in the pres pin treatment that due to 
</p>
<p>penetration and shielding effects the 4s level lies below the 4p level. If we add spin 
</p>
<p>to this picture, the s state can only become 2S112 while the p state can generate both 
2 P3;2 and 2 P1;2. The last two are split by the fine-structure effectt by an amount 
</p>
<p>(31e j4m2c2)&lt;(1/r)(dVjdr)), where Vis the potential seen by the n=4, I= 1 electron. 
</p>
<p>In the 4p~4s transition, the fine-structure interaction generates two lines in the place 
</p>
<p>of one, with wavelengths 7644.9 A and 7699.0 A. 
</p>
<p>Exercise 17.3.3. Consider the case where H 0 includes the Coulomb plus spin-orbit inter-
</p>
<p>action and H 1 is the effect of a weak magnetic field B = B k. Using the appropriate basis, show 
</p>
<p>that the first-order level shift is related to jz by 
</p>
<p>1 ( eB )( 1 ) . E=- !&plusmn;--; 
2mc 21+ 1 " 
</p>
<p>j=l&plusmn; 1/2 
</p>
<p>Sketch the levels for the n = 2 level assuming that E 1 &laquo; E/., .. 
</p>
<p>Exercise 17.3.4. * We discuss here some tricks for evaluating the expectation values of 
certain operators in the eigenstates of hydrogen. 
</p>
<p>(1) Suppose we want (1/r)nbn&middot; Consider first (l/r). We can interpret (l/r) as the first-
</p>
<p>order correction due to a perturbation ljr. Now this problem can be solved exactly; we just 
</p>
<p>replace e2 by c-)., everywhere. (Why?) So the exact energy, from Eq. (13.1.16) is E(l) = 
-(e2 - l)2m/2n2fi2&bull; The first-order correction is the term linear in l, that is, E 1 =me2l/n2 fi2 = 
</p>
<p>(A/r), from which we get (1/r)= 1/n2a0 , in agreement with Eq. (13.1.36). For later use, let 
</p>
<p>us observe that as E(l)=E0 +E1+&middot; &middot; &middot; =E(l=O)+l(dEjdlh~o+&middot; &middot;&middot;,one way to extract 
</p>
<p>E1 from the exact answer is to calculate A (dE/ dl h ~ o. 
(2) Consider now (l/r2). In this case, an exact solution is possible since the perturbation 
</p>
<p>just modifies the centrifugal term as follows: 
</p>
<p>(17.3.23) 
</p>
<p>where/' is a function of A. Now the dependence of Eon/'(}..,) is, from Eq. (13.1.14), 
</p>
<p>t Actually the splitting at a given I is solely due to the spin-orbit interaction. The kinetic energy correction 
depends only on I and does not contribute to the splitting between the P 312 and P112 levels. </p>
<p/>
</div>
<div class="page"><p/>
<p>Show that 
</p>
<p>Canceling)., on both sides, we get Eq. (17.3.11). 
</p>
<p>(3) Consider finally (//r3). Since there is no such term in the Coulomb Hamiltonian, we 
</p>
<p>resort to another trick. Consider the radial momentum operator, p, = -ili(J /or+ 1 /r), in terms 
</p>
<p>of which we may write the radial part of the Hamiltonian 
</p>
<p>as p;/2m. (Verify this.) Using the fact that ([H,p,])=O in the energy eigenstates, and by 
</p>
<p>explictly evaluating the commutator, show that 
</p>
<p>combining which with the result from part (2) we get Eq. (17.3.20). 
</p>
<p>(4) Find the mean kinetic energy using the trick from part (1 ), this time rescaling the 
</p>
<p>mass. Regain the virial theorem. 
</p>
<p>471 
</p>
<p>TIME-
INDEPENDENT 
</p>
<p>PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>Time-Dependent 
</p>
<p>Perturbation Theory 
</p>
<p>18.1. The Problem 
</p>
<p>18 
</p>
<p>Except for the problem of magnetic resonance, we have avoided studying phe-
</p>
<p>nomena governed by a time-dependent Hamiltonian. Whereas in the time-indepen-
</p>
<p>dent case the problem of solving the equation 
</p>
<p>(18.1.1) 
</p>
<p>reduced to solving the eigenvalue problem of H, in the time-dependent case a frontal 
</p>
<p>attack on the full time-dependent Schrodinger equation becomes inevitable. 
</p>
<p>In this chapter we consider the perturbative solution to a class of phenomena 
</p>
<p>described by 
</p>
<p>(18.1.2) 
</p>
<p>where H 0 is a time-independent piece whose eigenvalue problem has been solved 
</p>
<p>and H 1 is a small time-dependent perturbation. For instance, H 0 could be the hyd-
</p>
<p>rogen atom Hamiltonian and H 1 the addition due to a weak external electromagnetic 
</p>
<p>field. Whereas in the time-independent case one is interested in the eigenvectors and 
</p>
<p>eigenvalues of H, the typical question one asks here is the following. If at t = 0 the 
</p>
<p>system is in the eigenstate li 0 ) of H 0 , what is the amplitude for it to be in the 
eigenstate l/0 ) (f #- i) at a later time t? Our goal is to set up a scheme in which the 
answer may be computed in a perturbation series in powers of H 1&bull; To zeroth order, 
the answer to the question raised is clearly zero, for the only effect of H 0 is to 
multiply li 0 ) by a phase factor exp(-iE?t/1i), which does not alter its orthogonality 
to l/0 ). But as soon as we let H 1 enter the picture, i.e., work to nonzero order, the 
eigenstates of H 0 cease to be stationary and I i 0 ) can evolve into a state with a 
projection along l/0). 473 </p>
<p/>
</div>
<div class="page"><p/>
<p>474 
</p>
<p>CHAPTER 18 
</p>
<p>The next section begins with a simple derivation of the first-order transition 
</p>
<p>amplitude for the process i-&gt;/and is followed by several applications and discussions 
</p>
<p>of special types of perturbations (sudden, adiabatic, periodic, etc.). In Section 3 the 
expressions for the transition amplitude to any order are derived, following a scheme 
</p>
<p>more abstract than the one used in Section 2. Sections 4 and 5 are concerned with 
</p>
<p>electromagnetic interactions. Section 4 contains a brief summary of relevant concepts 
from classical electrodynamics, followed by a general discussion of several fine points 
</p>
<p>of the electromagnetic interaction at the classical and quantum levels. It therefore 
</p>
<p>has little to do with perturbation theory. However, it paves the way for the last 
</p>
<p>section, in which first-order perturbation theory is applied to the study of the inter-
action of atoms with the electromagnetic field. Two illustrative problems are consid-
</p>
<p>ered, one in which the field is treated classically and the other in which it is treated 
</p>
<p>quantum mechanically. 
</p>
<p>18.2. First-Order Perturbation Theory 
</p>
<p>Our problem is to solve Eq. ( 18.1.1) to first order in H 1&bull; Since the eigenkets 
</p>
<p>ln&deg;) of H 0 form a complete basis, we can always expand 
</p>
<p>(18.2.1) 
</p>
<p>To find cn(t) given c,(O) is equivalent to finding 11!'(1)) given ~j~(O)). Now cn(t) 
</p>
<p>changes with time because of H 0 and H 1&bull; Had H 1 been absent, we would know 
</p>
<p>( 18.2.2) 
</p>
<p>Let us use this information and write 
</p>
<p>(18.2.3) 
</p>
<p>If d" changes with time, it is because of H 1&bull; So we expect that the time evolution of 
dn can be written in a nice power series in H 1. The equation of motion for c~(t) is 
</p>
<p>found by operating both sides of Eq. (18.2.3) with (iliojct- H 0 - H 1) to get 
</p>
<p>(18.2.4) 
</p>
<p>and then dotting with (/0 1 exp(iEJt/fl): 
</p>
<p>(18.2.5a) </p>
<p/>
</div>
<div class="page"><p/>
<p>where 
</p>
<p>E~ 
(18.2.5b) 
</p>
<p>Notice that H 0 has been eliminated in Eq. ( 18.2.5), which is exact and fully equivalent 
to Eq. (18.1.1). Let us now consider the case where at t=O, the system is in the state 
</p>
<p>I i 0), i.e., 
</p>
<p>dn(O) = bni (18.2.6) 
</p>
<p>and ask what d1 (t) is. To zeroth order, we ignore the right-hand side ofEq. (18.2.5a) 
</p>
<p>completely, because of the explicit H', and get 
</p>
<p>(18.2.7) 
</p>
<p>in accordance with our expectations. To first order, we use the zeroth-order dn in the 
right-hand side because H 1 is itself of first order. This gives us the first-order equation 
</p>
<p>(18.2.8) 
</p>
<p>the solution to which, with the right initial conditions, is 
</p>
<p>(18.2.9) 
</p>
<p>Since we now know d to first order, we can feed it into the right-hand side of Eq. 
(18.2.5a) to get an equation for dthat is good to second order. Although we can keep 
going to any desired order in this manner, we stop with the first, since a more 
compact scheme for calculating transition amplitudes to any desired order will be 
set up in the next section. At this point we merely note that the first-order calculation 
is reliable if ldr(t)l ~ 1(/#i). If this condition is violated, our calculation becomes 
internally inconsistent, for we can no longer approximate dn (t) by Oni in the right-
hand side of Eq. (18.2.5a). 
</p>
<p>Let us apply our first-order result to a simple problem. Consider a one-dimen-
sional harmonic oscillator in the ground state IO)t of the unperturbed Hamiltonian 
at t = - oo. Let a perturbation 
</p>
<p>H 1(t) = -eCX e--&bull;'i&bull;' (18.2.1 0) 
</p>
<p>~ We shall denote the nth unperturbed state by 1 n) and not 1 n") in this discussion. 
</p>
<p>475 
</p>
<p>TIME-DEPENDENT 
</p>
<p>PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>476 
</p>
<p>CHAPTER 18 
</p>
<p>be applied between t =- oo and + oo. What is the probability that the oscillator is 
in the state In) at t= oo? According to Eq. (18.2.9), for n #0, 
</p>
<p>dn(oo)= ~if"' (-eC)(niXIO) e-''1'' einwt dt 
-x 
</p>
<p>( 18.2.11) 
</p>
<p>Since 
</p>
<p>only d1(oo)#O. We find that it is (using atiO)=Il)) 
</p>
<p>d ( ) - ieC ( 1i )1/2 f oc -t'Jr' iwt d 1 oo -- -- e e t 
1i 2mm 
</p>
<p>-&middot;x 
</p>
<p>I /2 
</p>
<p>ieC ( 1i ) 2 1/2 -w'r' 14 =- -- &middot; (7rT ) e ' 
1i 2mm 
</p>
<p>(18.2.12) 
</p>
<p>Thus the probability of the transition 0--+ l ist 
</p>
<p>2g2 2 
</p>
<p>P -ldlz_e 7fT -w'r'/2 o~1- 1 - e 
2mm1i 
</p>
<p>(18.2.13) 
</p>
<p>This result will be used shortly. 
</p>
<p>Exercise 18.2.1. Show that if H 1(t) = -e6"X/[I + (t/r)2], then, to first order, 
</p>
<p>Exercise 18.2.2.* A hydrogen atom is in the ground state at t=-oo. An electric field 
</p>
<p>E(t) = (k6") e-''1"' is applied until t= oo. Show that the probability that the atom ends up in 
</p>
<p>any of the n = 2 states is, to first order, 
</p>
<p>( )
'( 15 ') P(n=2)= e: 23'~o nr' e-"&gt;'&lt;'12 
</p>
<p>where ro = (E21m- E 100)/1i. Does the answer depend on whether or not we incorporate spin in 
</p>
<p>the picture? 
</p>
<p>We now turn our attention to different types of perturbations. 
</p>
<p>~Since d.(t) and c.(t) differ only by a phase factor, P(n)=lc.l'=ld.l'. </p>
<p/>
</div>
<div class="page"><p/>
<p>The Sudden Perturbation 
</p>
<p>Consider a system whose Hamiltonian changes abruptly over a small time inter-
</p>
<p>val e. What is the change in the state vector as e--+0? We can find the answer without 
resorting to perturbation theory. Assuming that the change occurred around t = 0, 
</p>
<p>we get, upon integrating Schrodinger's equation between t= -e/2 and e/2, 
</p>
<p>I If!( e/2) &gt;-I If/( -e/2) &gt;=I If/after) -I If/before) 
</p>
<p>. fe/2 
= ~~ H(t)llf!(t))dt 
</p>
<p>-e/2 
</p>
<p>(18.2.14) 
</p>
<p>Since the integrand on the right-hand side is finite, the integral is of order e. In the 
limit e--+0, we get 
</p>
<p>I If! after)= I If/before) (18.2.15) 
</p>
<p>An instantaneous change in H produces no instantaneous change in I lfl).t Now the 
limit e--+0 is unphysical. The utility of the above result lies in the fact that it is an 
</p>
<p>excellent approximation if H changes over a time that is very small compared to the 
</p>
<p>natural time scale of the system. The latter may be estimated semiclassically; several 
</p>
<p>examples follow in a moment. For the present, let us consider the case of an oscillator 
</p>
<p>to which is applied the perturbation in Eq. (18.2.10). It is clear that whatever be the 
</p>
<p>time scale of this system, the change in the state vector must vanish as r, the width 
</p>
<p>of the Gaussian pulse, vanishes. This means in particular that the system initially in 
</p>
<p>the ground state must remain there after the pulse, i.e., the 0--+ 1 transition probability 
</p>
<p>must vanish .. This being an exact result, we expect that if the transition probability 
</p>
<p>is calculated perturbatively, it must vanish to any given order. (This is like saying 
</p>
<p>that if an analytic function vanishes identically, then so does every term in its Taylor 
</p>
<p>expansion.) Turning to the first-order probability for 0--+1 in Eq. (18.2.13), we see 
</p>
<p>that indeed it vanishes as r tends to zero. 
</p>
<p>A more realistic problem, where e is fixed, involves a ls electron bound to a 
nucleus of charge Z which undergoes f3 decay by emitting a relativistic electron and 
changing its charge to (Z + 1). The time the emitted electron takes to get out of the 
n = 1 shell is 
</p>
<p>rc::::.a0/Zc (18.2.16) 
</p>
<p>whereas the characteristic time for the ls electron is 
</p>
<p>Tc::::. sizeofstate ~a 0 /zac=~ 
velocity of e- z Z 2ac 
</p>
<p>(18.2.17) 
</p>
<p>so that 
</p>
<p>r/T=Za 
</p>
<p>t We are assuming His finite in the integral ( -&amp;/2, &amp;/2). If it has a delta function spike, it can produce 
a change in I 1/f), see Exercise 18.2.6. 
</p>
<p>477 
</p>
<p>TIME-DEPENDENT 
</p>
<p>PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>478 
</p>
<p>CHAPTER 18 
</p>
<p>For Z small, we may apply the sudden approximation and conclude that the state 
</p>
<p>of the atomic electron is the same just before and just after f3 decay. Of course, this 
state is not an eigenstate of the charge (Z + I) ion, but rather a superposition of 
such states (see Exercise 18.2.4). 
</p>
<p>Exercise 18.2.3. * Consider a particle in the ground state of a box of length L. Argue on 
semiclassical grounds that the natural time period associated with it is Tc::;mL2 jnn. If the box 
</p>
<p>expands symmetrically to double its size in time r ~ T what is the probability of catching the 
</p>
<p>particle in the ground state of the new box? (See Exercise (5.2.1 ).) 
</p>
<p>Exercise 18.2.4. *In the fJ decay H3 (two neutrons+one proton in the nucleus)-&gt;(He 3t 
(two protons+ one neuron in the nucleus), the emitted electron has a kinetic energy of 16 keY. 
</p>
<p>Argue that the sudden approximation may be used to describe the response of an electron 
</p>
<p>that is initially in the Is state of H 3. Show that the amplitude for it to be in the ground state 
</p>
<p>of (He3 )+ is 16(2) 1 1''/27. What is the probability for it to be in the state 
</p>
<p>In= 16, /= 3, m =0) of (He3 ) +'? 
</p>
<p>Exercise 18.2.5. An oscillator is in the ground state of H = H 0 + H 1, where the time-
</p>
<p>independent perturbation H' is the linear potential (-&middot;fx:). If at t=O, H 1 is abruptly turned 
</p>
<p>off, show that the probability that the system is in the nth eigenstate of H 0 is given by the 
</p>
<p>Poisson distribution 
</p>
<p>Hint: Use the formula 
</p>
<p>e--A;_n 
P(n)=-
</p>
<p>1
-, 
</p>
<p>n. 
where 
</p>
<p>exp[A + B] = exp[A] exp[B] exp[- hA, B]] 
</p>
<p>where [A, B] is a c number. 
</p>
<p>Exercise 18.2.6. &bull; Consider a system subject to a perturbation H 1(1) = H 1 8(1). Show that 
</p>
<p>if at t = o- the system is in the state 1 the amplitude to be in a state l/0 ) at t = o+ is, to 
first order, 
</p>
<p>(f'fi) 
</p>
<p>Notice that ( 1) the state of the system does change instantaneously; (2) Even though the 
</p>
<p>perturbation is "infinite" at t = 0, we can still use first-order perturbation theory if the '"area 
</p>
<p>under if" is small enough. 
</p>
<p>The Adiabatic Perturbation 
</p>
<p>We now turn to the other extreme and consider a system whose Hamiltonian 
H(t) changes very slowly from H(O) to !J( r) in a time r. If the system starts out at 
</p>
<p>t = 0 in an eigenstate ln(O)) of H(O), where will it end at time r? The adiabatic 
</p>
<p>theorem asserts that if the rate of change of H is slow enough, the system will end </p>
<p/>
</div>
<div class="page"><p/>
<p>up in the corresponding eigenket In( r)) of H( r) J Rather than derive the theorem 
and the precise definition of "slow enough" we consider a few illustrative examples. 
</p>
<p>Consider a particle in a box of length L(O). If the box expands slowly to a 
length L( r), the theorem tells us that a particle that was in the nth state of the box 
oflength L(O) will now be in the nth state of the box of length L( r). But how slow 
is slow enough? 
</p>
<p>There are two ways to estimate this. The first is a semiclassical method and goes 
as follows. The momentum of the particle is of the order (dropping factors of order 
unity like rr, n, etc.) 
</p>
<p>fi 
p~&shy;
</p>
<p>L 
</p>
<p>and the time it takes to finish one full oscillation is of the order 
</p>
<p>(18.2.18) 
</p>
<p>(18.2.19) 
</p>
<p>We can say the expansion or contraction is slow if the fractional change in the length 
of the box per cycle is much smaller than unity: 
</p>
<p>I~Lipercycle ~ ldL/dtlmL2 /fi = mL I dL 141 
L L n dt 
</p>
<p>This can also be written as 
</p>
<p>v\\'alls 
--41 
Vparticle 
</p>
<p>The second approach is less intuitive&sect; and it estimates T as 
</p>
<p>1 
T~-
</p>
<p>( 18.2.20) 
</p>
<p>(18.2.21) 
</p>
<p>(18.2.22) 
</p>
<p>! This is again a result that is true to any given order in perturbation theory. We shall exploit this fact 
in a moment. 
</p>
<p>&sect;The logic behind this approach and its superiority over the intuitive one will become apparent shortly 
</p>
<p>in an example where we recover the results of time&middot;&middot;independent perturbation theory from the time-
dependent one. 
</p>
<p>479 
TIME-DEPENDENT 
</p>
<p>PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>480 
</p>
<p>CHAPTER 18 
</p>
<p>where COmin is the smallest of the transition frequencies between the initial state i and 
</p>
<p>any accessible final state Jt; it is the smallest of 
</p>
<p>EJ-E7 
COji=--fi- (18.2.23) 
</p>
<p>In the present case, since E~ = (n21i2n 2 12mL2), energy differences are of the order 1i2 I 
mL2 and 
</p>
<p>(18.2.24) 
</p>
<p>which coincides with Eq. (18.2.19). This is not surprising, for we can also write T 
</p>
<p>in Eq. (18.2.19) as 
</p>
<p>(18.2.25) 
</p>
<p>Thus Tin Eq. (18.2.19) is ~1i1E7, while Tineq. (18.2.24) is ~1iiiEJ-E7Imin&middot; Since 
</p>
<p>the energy levels of a quantum system are all of the same order of magnitude (say 
</p>
<p>a Rydberg or fico), energies and energy differences are of the same order of magnitude 
</p>
<p>and the two estimates for Tare equivalent, unless the levels are degenerate or nearly 
</p>
<p>so. In this case, it is T ~ 1 I comin that is to be trusted, for it exposes the instability of 
a degenerate or nearly degenerate system. An explicit example that follows later will 
</p>
<p>illustrate this. 
</p>
<p>Let us consider one more example of the adiabatic theorem, an oscillator subject 
</p>
<p>to the perturbation 
</p>
<p>(18.2.26) 
</p>
<p>between - oo ~ t ~ oo. We expect that if r, which measures the time over which H 1 
</p>
<p>grows from 0 to its peak, tends to infinity, the change in the system will be adiabatic. 
</p>
<p>Thus, if a system starts in the ground state of H(-oo) = H 0 at t = - oo, it will end 
</p>
<p>up in the ground state of H( oo) = H(- oo) = H 0&bull; Our first-order formula, Eq. 
</p>
<p>(18.2.13), for P 0 ~ 1 conforms with this expectation and vanishes exponentially as 
</p>
<p>cor-+oo. Our formula also tells us what larger means: it means 
</p>
<p>corll&gt; l, rll&gt;llco ( 18.2.27) 
</p>
<p>This is what we would expect from the semiclassical estimate or the estimate T ~ 1 / 
</p>
<p>CO min and the condition r ll&gt; T. 
The adiabatic theorem suggests a way of recovering the results of time-indepen-
</p>
<p>dent perturbation theory from time-dependent theory. Consider a Hamiltonian H(t) 
</p>
<p>t This is a state for which &lt;f"IH'ji 0 ) ;or.O. </p>
<p/>
</div>
<div class="page"><p/>
<p>which changes continuously from H 0 at t =- oo to H 0 + H 1 at t = 0: 
</p>
<p>H(t) = H 0 + e'1' H 1, (18.2.28) 
</p>
<p>As r, the rise time of the exponential, goes to infinity, the adiabatic theorem assures 
us that an eigenstate ln&deg;) of H 0 at t= -----w will evolve into the eigenstate In) of H 
at t = 0. If we calculate the state at t = 0 to a given order in time-dependent theory 
</p>
<p>and let r-+oo, we should get the time-independent formula for the state In) to that 
</p>
<p>order. To first order, we know that the projection of the state at t=O along lm0 ) 
(m#n) is 
</p>
<p>dm(O) = ~i r (m0IH'In&deg;) e':r e'"''"'1 dt 
-:x_, 
</p>
<p>( -i/1i)(m0 1 H'ln&deg;) 
</p>
<p>1/r+im,n 
</p>
<p>If we now let r-&gt;oo, we regain the familiar result 
</p>
<p>( 1 8.2.29) 
</p>
<p>(18.2.30) 
</p>
<p>In practice, r-+ oo is replaced by some large r. Equation (18.2.29) tells us what large 
</p>
<p>r means: it is defined by 
</p>
<p>or 
</p>
<p>r ?i&gt; 1/ Wmin (18.2.31) 
</p>
<p>Thus we see that r~ 1/mmin is indeed the reliable measure of the natural time scale 
</p>
<p>of the system. In particular, if the system is degenerate (or nearly so), T-&gt; oo and it 
</p>
<p>becomes impossible, in practice, to change the state of the system adiabatically. 
</p>
<p>Let us wind up the discussion on the adiabatic approximation by observing its 
</p>
<p>similarity to the WKB approximation. The former tells us that if the Hamiltonian 
changes in time from H 0 to H 0 + H 1, the eigenstate ln&deg;) evolves smoothly into its 
counterpart In) in the limit r/T-+oo, where r is the duration over which the Hamil-
tonian changes and Tis the natural time scale for the system. The latter tells us that 
if the potential changes in space from V 0 to V1, a plane wave of momentum p0 = 
[2m(E- V0 )] 112 evolves smoothly into a plane wave of momentum p 1 = 
[2m(E- V1)] 112 in the limit L/A.-&gt;co, where Lis the length over which V changes 
and A.=2trfi/p is natural length scale for the system. 
</p>
<p>We shall return to adiabatic evolutions in Chapter 21. 
</p>
<p>481 
</p>
<p>TIME-DEPENDENT 
PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>482 
</p>
<p>CHAPTER 18 
</p>
<p>The Periodic Perturbation 
</p>
<p>Consider a system that is subject to a periodic perturbation, say an atom placed 
between the plates of a condenser connected to an alternating current (ac) source or 
</p>
<p>in the way of a monochromatic light beam. While in reality these perturbations vary 
</p>
<p>as sines and cosines, we consider here the case 
</p>
<p>(18.2.32) 
</p>
<p>Which is easier to handle mathematically. The sines and cosines can be handled by 
</p>
<p>expressing them in terms of exponentials. 
</p>
<p>Let us say the system comes into contact with this perturbation at t = 0. The 
amplitude for transition from I i 0 ) to l/0 ) in time t (i=l f) is 
</p>
<p>(18.2.33) 
</p>
<p>(18.2.34) 
</p>
<p>The probability for the transition i-+.f is 
</p>
<p>( 18.2.35) 
</p>
<p>Since the function (sin2 x)jx2 is peaked at the origin and has a width L'u:::::n, we 
find that the system likes to go to states f such that 
</p>
<p>or 
</p>
<p>EJt = (E7t + nwt) &plusmn; 2nn 
</p>
<p>or 
</p>
<p>.o ,0 21in: . (' , 2n) E1 - E1 = iiw &plusmn;-= iiw 1 ::r:-
t \ OJ[, 
</p>
<p>(18.2.36) 
</p>
<p>For small t, the system shows no particular preference for the level with 
EJ=E?+nm. Only when rnt"t&gt;2n does it begin to favor EJ=E7+Piw. The reason 
is simple. You and I know the perturbation has a frequency m, say, because we set 
the dial on the ac source or tuned our laser to frequency w. But the system goes by 
what it knows, starting from the time it made contact with the perturbation. In the 
beginning, it will not even know it is dealing with a periodic perturbation; it must 
wait a few cycles to get the message. Thus it can become selective only after a few 
</p>
<p>cycles, i.e., after wt"t&gt;2n. What does it do meanwhile? It Fourier-analyzes the pulse </p>
<p/>
</div>
<div class="page"><p/>
<p>into its frequency components and its transition amplitude to a state with 
EJ = E? + 1iOJ fi is proportional to the Fourier component at OJ = OJfi. The t' integral 
in Eq. (18.2.33) is precisely this Fourier transform.t 
</p>
<p>What happens if we wait a long time? To find out, we consider the case of a 
system exposed to the perturbation from t = - T /2 to T /2 and let T-+ oo. Equation 
(18.2.33) becomes 
</p>
<p>and 
</p>
<p>df= lim -if T/2 H}, ei(rufi-ru)t' dt' 
T~co fl -T/2 
</p>
<p>-2n:i 
=-1i- H},a(OJfi- OJ) 
</p>
<p>We handle the product of 8 functions as follows: 
</p>
<p>(18.2.37) 
</p>
<p>(18.2.38) 
</p>
<p>(18.2.39) 
</p>
<p>(18.2.40) 
</p>
<p>Since the 8 function in front of the integral vanishes unless OJfi =OJ, we may set OJfi = 
OJ in the integral to obtain 
</p>
<p>(18.2.41) 
</p>
<p>Feeding this into Eq. (18.2.39) for P;~f&middot; and dividing by T, we get the average 
transition rate: 
</p>
<p>(18.2.42) 
</p>
<p>This is called Fermi's golden rule and has numerous applications, some of which will 
be discussed later in this chapter and in the next chapter. You may be worried about 
the 8 function in R;~f and in particular whether first-order perturbation theory is to 
be trusted when the rate comes out infinite! As we will see, in all practical applications 
the 8 function will get integrated over for one reason or another. The validity of the 
first-order formula will then depend only on the area under the 8 function. (Recall 
Exercise 18.2.6.) 
</p>
<p>t The inability of a system to assign a definite frequency to an external perturbation until many cycles 
have elapsed is a purely classical effect. The quantum mechanics comes in when we relate frequency to 
energy. 
</p>
<p>483 
</p>
<p>TIME-DEPENDENT 
PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>484 
</p>
<p>CHAPTER 18 
</p>
<p>18.3. Higher Orders in Perturbation Theoryt 
</p>
<p>In Section 18.2 we derived a formula for the transition amplitude from ji 0 ) to 
</p>
<p>l/0 ) to first order in perturbation theory. The procedure for going to higher orders 
was indicated but not pursued. We address that problem here, using a more abstract 
</p>
<p>formalism, desirable for its compactness and the insight it gives us into the anatomy 
</p>
<p>of the perturbation series. 
</p>
<p>The basic idea behind the approach is the same as in Section 18.2: we want to 
</p>
<p>isolate the time evolution generated by H\ for H 0 by itself causes no transitions 
</p>
<p>between its own eigenstates ji0 ) and l/0). To do this, we must get acquainted with 
other equivalent descriptions of quantum dynamics besides the one we have used so 
</p>
<p>far. The description we are familiar with is called the Schrodinger picture. In this 
</p>
<p>picture the state of the particle is described by a vector I 'l's(t)). (We append a 
</p>
<p>subscript S to all quantities that appear in the Schri:idinger picture to distinguish 
</p>
<p>them from their counterparts in other pictures.) The physics is contained in the inner 
</p>
<p>products (cos! ljls(t)) which give the probabilities 
</p>
<p>P(co, t) =I (cos! ljls{t))l 2 (18.3.1) 
</p>
<p>for obtaining the result co when n is measured. Here Ieos) is the normalized eigenket 
of the operator Os(X s, P s) with eigenvalue co. Since X s and P s are time independent 
</p>
<p>so are Os and I cos). Thus the physics is contained in the dot product of the moving 
</p>
<p>ket l'l'sU)) with the stationary kets Ieos). 
</p>
<p>The time evolution of I 'I' sU)) is given in general by 
</p>
<p>and in our problem by 
</p>
<p>d 
ifz -I 'l's(t)) =Hsl lj/s(t)) 
</p>
<p>dt 
</p>
<p>. d 0 I 
di -I lj/s{t)) = [H s+ H s(t)]! ljls(t)) 
</p>
<p>dt 
</p>
<p>The expectation values change according to 
</p>
<p>d 
ifz- (Os) = ([Os, Hs]) 
</p>
<p>dt 
</p>
<p>If we define a propagator Us(t, t0 ) by 
</p>
<p>I 'I' s(t)) = Us(t, to)! 'I' sUo)) 
</p>
<p>t This section may be skimmed through by a reader pressured for time. 
</p>
<p>(18.3.2a) 
</p>
<p>(18.3.2b) 
</p>
<p>(18.3.3) 
</p>
<p>(18.3.4) </p>
<p/>
</div>
<div class="page"><p/>
<p>it follows from Eq. (18.3.2) [because l'l'sCto)) is arbitrary] that 485 
</p>
<p>TIME-DEPENDENT 
PERTURBATION . dUs 
</p>
<p>di~=HsUs 
dt 
</p>
<p>(18.3.5) THEORY 
</p>
<p>Here are some formulas (true for all propagators U) that will be useful in what 
follows (recall Eq. (4.3.16)): 
</p>
<p>The Interaction Picture 
</p>
<p>utu=I 
</p>
<p>U(t3, tz) U(tz, t1) = U(t3, t1) 
</p>
<p>U(t1, t1) =I 
</p>
<p>ut (t1, tz) = U(tz, tJ) 
</p>
<p>(18.3.6) 
</p>
<p>Since Us(t, t0 ) is a unitary operator, which is the generalization of the rotation 
operator to complex spaces, we may describe the time evolution of state vectors as 
"rotations" in Hilbert space.t The rotation is generated by U8 (t, t0 ) or equivalently, 
by H 8 (t)=H~+H1(t). Imagine for a moment that H1 is absent. Then the rotation 
will be generated by U~(t), which obeys 
</p>
<p>&middot;rz dU~ _ Ho uo 
l --- s s 
</p>
<p>dt 
(18.3.7) 
</p>
<p>the formal solution to which is U~(t, t 0 )=e-;H~(t-toJI~. If H1(t) is put back in, both 
H~ and H1(t) jointly produce the rotation U8 &bull; 
</p>
<p>These pictorial arguments suggest a way to freeze out the time evolution gener-
ated by H1. Suppose we switch to a frame that rotates at a rate that U~ (or H~) 
by itself generates. In this frame the state vector moves because H1=f. 0. Let us verify 
this conjecture. To neutralize the rotation induced by U~, i.e., to see things from 
the rotating frame, we multiply I l{ls(t)) by (U~)t to get 
</p>
<p>(18.3.8a) 
</p>
<p>The ket I l{l J( t)) is the state vector in the rotating frame, or in the interaction picture. 
If we set t = t0 in the above equation, we did 
</p>
<p>I 'I' J( to)) = I 'I' sUo)) (18.3.8b) 
</p>
<p>t In this section we use the word "rotation" in this generalized sense, and not in the sense of a spatial 
rotation. </p>
<p/>
</div>
<div class="page"><p/>
<p>486 
</p>
<p>CHAPTER 18 
</p>
<p>i.e., the interaction and Schrodinger kets coincide at t = t0 , which is that instant we 
switch to the moving frame. The time evolution of I tp1(t)) is as followst: 
</p>
<p>Now 
</p>
<p>.. d . dU~t . ot. dlo/s) tn -d I '!fAt))= di-d l'!fs) +Us zn &middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;i&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot; 
t .I (; t 
</p>
<p>=- U1tH~I'!fs) + U~t (H~ + H.~)lo/s) 
</p>
<p>= U~t H.kl'!fs) 
</p>
<p>= u~t H1u1u~tl Vts&gt; 
</p>
<p>= u:tm.v~to/IU)&gt; 
</p>
<p>( U~{Hk(t)U~= H}(t) (18.3.9) 
</p>
<p>is the perturbing Hamiltonian as seen in the rotating frame. So we can write 
</p>
<p>d 
in d.; I tpJ{t)/ =H}(t)i Vft{t)) (18.3.10) 
</p>
<p>So, as we anticipated, the time evolution of the state vector in the interaction picture 
is determined by the perturbing Hamiltonian, Hj. Despite the fact that the state 
vector now rotates at a different rate, the physical predictions are the same as in the 
Schrodinger picture. This is because P(w, t) depends only on the inner product 
between the state vector and the eigenket of n with eigenvalue m, and the inner 
product between two vectors is unaffected by going to a rotating frame. However, 
both the state vector and the eigenket appear different in the interaction picture. 
</p>
<p>Just as 
</p>
<p>l'!fs(t))-+U~t(t, to)lo/s(t))=l'!fr(t)) 
</p>
<p>so does 
</p>
<p>(18.3.11) 
</p>
<p>However, 
</p>
<p>&lt; WsllflsU)) = &lt; w J(l)i lfl t(t)) (18.3.12) 
</p>
<p>The time-dependent ket I w 1(t)) is just the eigenket of the time-dependent operator 
</p>
<p>(18.3.13) 
</p>
<p>which is just n as seen in the rotating frame: 
</p>
<p>(18.3.14) 
</p>
<p>t Whenever the argument of any U is suppressed, it may be assumed to be (I. 10 ). </p>
<p/>
</div>
<div class="page"><p/>
<p>The time dependence of 0 1 may be calculated by combining Eq. (18.3.13), which 
defines it, and Eq. (18.3.7), which gives the time evolution of U~: 
</p>
<p>tli dn1 = i1i dU~t 0 8 U~ + U~t Osi1i dU~ 
dt dt dt 
</p>
<p>= u~t[ns, H~J u~ = [01, H~] (18.3.15) 
</p>
<p>In the interaction picture, the operators evolve in response to the unperturbed Hamil-
tonian H~ .t Whereas in the Schrodinger picture, the entire burden of time evolution 
lies with the state vectors, in this picture it is shared by the state vectors and the 
operators (in such a way that the physics is the same). 
</p>
<p>Let us now address the original problem, of obtaining a perturbation series for 
the transition amplitude. We define a propagator UI(t, t0) in the interaction picture: 
</p>
<p>I 'I' At))= UI(t, to)l 'I'Ato)) ( 18.3.16) 
</p>
<p>which, because of Eq. (18.3.10), obeys 
</p>
<p>(18.3.17) 
</p>
<p>Once we find UI(t), we can always go back to Us(t) by using 
</p>
<p>Us(t, to)= U~(t, to)UI(t, to) (18.3.18) 
</p>
<p>which follows from Eqs. (18.3.8) and (18.3.16). 
Since H} depends on time, the solution to Eq. (18.3.17) is not U1= 
</p>
<p>exp( -iH}(t- t0 )j1i). A formal solution, with the right initial condition, is 
</p>
<p>UI(t, to)=!-~ r HI(t')UI(t', t0 ) dt' 
to 
</p>
<p>(18.3.19) 
</p>
<p>as may be readily verified by feeding it into the differential equation. Since U1 occurs 
on both sides, this is not really a solution, but an integral equation, equivalent to the 
differential equation ( 18.3.17), with the right initial condition built in. So we have 
not got anywhere in terms of the exact solution. But the integral equation provides 
a nice way to carry out the perturbation expansion. Suppose we want U1 to zeroth 
order. We drop anything with an H} in Eq. (18.3.19): 
</p>
<p>UI(t, t0)=I+O(H}) ( 18.3.20) 
</p>
<p>tActually, H~= u~tH~U~=H~ since [H~, U~]=O in this problem. 
</p>
<p>487 
</p>
<p>TIME-DEPENDENT 
PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>488 
</p>
<p>CHAPTER 18 
</p>
<p>This is to be expected, for if we ignore H}, the state vectors do not move in the 
</p>
<p>interaction picture. 
</p>
<p>To first order, we can keep one only power of H}. So we use the zeroth-order 
</p>
<p>value for U1 in the right-hand side of Eq. (18.3.20) to get 
</p>
<p>UJ(t, t0) =I-~ r H}(t') dt' + O(H7) 
to 
</p>
<p>( 18.3.21) 
</p>
<p>Before going to the next order, let us compare this with Eq. (18.2.9) for the transition 
</p>
<p>amplitude df(t), computed to first order. Recall the. definition of df(t): it is the 
</p>
<p>projection along (f~l exp[ iEJ(t- t0) j1i] at time t, of a state that was initially (at t = 
</p>
<p>to) I i~)t: 
</p>
<p>df(t) = (f~leiE~(r-ro)/~Us(t, to)li~) 
</p>
<p>= (f~l U~t (t, to) Us(t, to) I i~) 
</p>
<p>= (f~l UJ(t, to)li~) 
</p>
<p>(18.3.22) 
</p>
<p>(18.3.23) 
</p>
<p>If we feed into this our first-order propagator given in Eq. (18.3.21), we get 
</p>
<p>df(t) = (f~l UJ(t, to)li~) 
</p>
<p>= 8fi-~ I' &lt;J~IH}(t')li~) dt' 
1i to 
</p>
<p>= 8fi-~ r (f~l U~\t', to)H1U~(t', to)li~) dt' 
to 
</p>
<p>= 8fi-~ r (H1)fi eimr.u'-rol dt' 
ro 
</p>
<p>(18.3.24) 
</p>
<p>which agrees with Eq. (18.2.9) if we set t0 =0. 
</p>
<p>Let us now turn to higher orders. By repeatedly feeding into the right-hand side 
</p>
<p>of Eq. (18.3.19) the result for U1 to a known order, we can get Udo higher orders: 
</p>
<p>UJ(t, t0) =I-~ r H}(t') dt' + ( -i/11)2 f r H}(t')H}(t") dt' dt" 
to to to 
</p>
<p>+( -i/1i)3 f r r&middot; H}(t')H}(t'')H}{t111) dt' dt" dt 111 + &middot; &middot; &middot; 
to to to 
</p>
<p>(18.3.25) 
</p>
<p>t We have set t0 =0 in Section 18.2. </p>
<p/>
</div>
<div class="page"><p/>
<p>r + t' + ug + &bull;&bull; &bull; 
Time 
</p>
<p>H~ 
</p>
<p>Figure 18.1. A pictorial representation of the perturbation series. The hatched circle represents the full 
propagator between times t0 and t. The hatched circle is a sum of many terms, each of which corresponds 
to a different number of interactions with the perturbation, H1. Between such interactions, the particle 
evolves in response to just Ht i.e., is propagated by ~. 
</p>
<p>Premultiplying by U~t, t0 ) and expressing H} in terms of H1, we get the Schrodinger 
picture propagator 
</p>
<p>Us(t, to)= U~(t, to)-!_ It U~(t. to)U~t(t', to)H1U~t', t0) dt' 
1i to 
</p>
<p>+(-i/11)2 f r U~t, to)U~t(t', to)H1U~t', to)U~t(t", t0 ) 
~ ~ . 
</p>
<p>x H1U~t", t0 ) dt' dt" + &middot; &middot; &middot; (18.3.26) 
</p>
<p>Us(t, to)= U~t, to)-~ f ifs(t, t')H1U~t', to) dt' 
to 
</p>
<p>+ ( -ij1i)2 f r&middot; ifs(t, t')H1U~t', t")H1U0s(t", t0) dt' dt" + &middot; &middot; &middot; 
to to 
</p>
<p>The above series could be described by the following words. On the left-hand side 
we have the complete Schrodinger picture propagator and on the right-hand side a 
series expansion for it. The first term says the system evolves from t0 to t in response 
to just U~, i.e., in response to H~. The second term, if we read it from right to left 
(imagine it acting on some initial state) says the following: the system evolves from 
t0 to t': in response to ut there it interacts once with the perturbation and thereafter 
responds to U~ alone until time t. The integral overt' sums over the possible times 
at which the single encounter with H1 could have taken place. The meaning of the 
next and higher terms is obvious. These are represented schematically in Fig. 18.1. 
</p>
<p>If we consider specifically the transition from the state li0 ) to l/0 ) (we drop 
the subscriptS everywhere) we get 
</p>
<p>(18.3.27) 
</p>
<p>489 
</p>
<p>TIME-DEPENDENT 
PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>490 
</p>
<p>CHAPTER 18 
</p>
<p>upon introducing a complete set of eigenstates of f/ 0 in the second-order term. The 
</p>
<p>meaning of the first term is obvious. The second (reading right to left) says that 
</p>
<p>between t0 and t' the eigenstate I i 0 ) picks up just a phase (i.e., responds to H~ alone). 
At t' it meets the perturbation, which has an amplitude ( / 0 1 H 11 i 0 ) of converting it 
</p>
<p>to the state l/0 ). Thereafter it evolves as the eigenstate l/0 ) until time t. The total 
amplitude to end up in l/0 ) is found by integrating over the times at which the 
conversion could have taken place. Thus the first-order transition corresponds to a 
one-step process i-+f At the second order, we see a sum over a complete set of states 
</p>
<p>ln&deg;). It means the system can go from I i 0 ) to l/0 ) via any intermediate or virtual 
state I n&deg;) that H 1 can knock I i 0 ) into. Thus the second-order amplitude describes 
a two-step process, i-+n-+f. Higher-order amplitudes have a similar interpretation. 
</p>
<p>The Heisenberg Picture 
</p>
<p>It should be evident that there exist not just two, but an infinite number of 
</p>
<p>pictures, for one can go to frames rotating at various speeds. Not aU these are worthy 
of study, however. We conclude this section with one picture that is very important, 
</p>
<p>namely, the Heisenberg picture. In this picture, one freezes out the complete time 
</p>
<p>dependence of the state vector. The Heisenberg state vector is 
</p>
<p>(18.3.28) 
</p>
<p>The operators in this picture are 
</p>
<p>(18.3.29) 
</p>
<p>and obey 
</p>
<p>(18.3.30) 
</p>
<p>Exercise 18.3.1.* Derive Eq. (18.3.30). 
</p>
<p>Thus in the Heisenberg picture, the state vectors are fixed and the operators 
</p>
<p>carry the full time dependence. (Since the interaction picture lies between this Heisen-
berg picture and the Schri:idinger picture, in that the operators and the state vectors 
</p>
<p>share the time dependence, it is also called the intermediate picture. Another name 
</p>
<p>for it is the Dirac picture.) 
Notice the similarity between Eq. (18.3.30) and the classical equation 
</p>
<p>dm- r . 1 
- 1w,.YfJ 
</p>
<p>dt 
(18.3.31) 
</p>
<p>The Heisenberg picture displays the close formal similarity between quantum and 
classical mechanics: to every classical variable m there is a quantum operator nH, 
which obeys similar equations; all we need to do is make the usual substitution 
</p>
<p>m-+!1, { } -+( -i/1l)[ , ]. The similarity between Eqs. (18.3.30) and (18.3.31) is even </p>
<p/>
</div>
<div class="page"><p/>
<p>more striking if we actually evaluate the commutators and Poisson bracket (PB). 
Consider, for example, the problem of the oscillator for which 
</p>
<p>pj. 1 2 2 
HH=-+-mw XH 
</p>
<p>2m 2 
(18.3.32) 
</p>
<p>Since XH, PH are obtained from X 8 , Ps by a unitary transformation, they satisfy the 
same commutation rules 
</p>
<p>(18.3.33) 
</p>
<p>Note that the time arguments must be equal in XH and PH. Hence Eq. (18.3.33) is 
called the equal-time commutation relation. From Eq. (18.3.30), 
</p>
<p>(18.3.34a) 
</p>
<p>and likewise 
</p>
<p>(18.3.34b) 
</p>
<p>which are identical in form to the classical equations 
</p>
<p>. o.Yt P 
x=-=-
</p>
<p>op m 
(18.3.35) 
</p>
<p>. a.Yt 2 
p=--=-mw x 
</p>
<p>ax 
</p>
<p>This is to be expected, because the recipe for quantizing is such that commutators 
and PB always obey the correspondence [recall Eq. (7.4.40)] 
</p>
<p>(18.3.36) 
</p>
<p>Although the Heisenberg picture is not often used in nonrelativistic quantum mech-
anics, it is greatly favored in relativistic quantum field theory. 
</p>
<p>Exercise 18.3.2. In the paramagnetic resonance problem Exercise 14.4.3 we moved to a 
frame rotating in real space. Show that this is also equivalent to a Hilbert space rotation, but 
that it takes us neither to the interaction nor the Heisenberg picture, except at resonance. 
What picture is it at resonance? (If B = B0k + B cos roti- B sin rotj, associate B0 with H~ and 
B with H1.) 
</p>
<p>491 
</p>
<p>TIME-DEPENDENT 
PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>492 
</p>
<p>CHAPTER 18 
</p>
<p>18.4. A General Discussion of Electromagnetic Interactions 
</p>
<p>This section contains a summary of several concepts from electro-dynamics that 
</p>
<p>are relevant for the next section. It also deals with certain subtle questions of basic 
</p>
<p>interest, not directly linked to the rest of this chapter. 
</p>
<p>Classical Electrodynamics 
</p>
<p>Let us begin with an extremely concise review of this subject.t The response of 
</p>
<p>matter to the electromagnetic field is given by the Lorentz force on a charge q: 
</p>
<p>The response of the fields to the charges is given by Maxwell's equations: 
</p>
<p>V&middot;E=4trp 
</p>
<p>1 aB 
VxE+- -=0 
</p>
<p>c at 
</p>
<p>V&middot;B=O 
</p>
<p>1 aE 4tr 
VxB---=-j 
</p>
<p>c ac c 
</p>
<p>(18.4.1) 
</p>
<p>(18.4.2) 
</p>
<p>(18.4.3) 
</p>
<p>(18.4.4) 
</p>
<p>( 18.4.5) 
</p>
<p>where p and j are the charge and current densities bound by the continuity equation 
</p>
<p>V &bull; ap o &bull;J+-= 
at 
</p>
<p>(18.4.6) 
</p>
<p>Exercise 18.4.1. By taking the divergence of Eq. (18.4.5) show that the continuity equa-
</p>
<p>tion must be obeyed if Maxwell's equations are to be mutually consistent. 
</p>
<p>The potentials A and &cent;1 are now introduced as follows. Equation (18.4.4), com-
</p>
<p>bined with the identity V &middot; V x A= 0, tells us that B can be written as a curl 
</p>
<p>B=VxA (18.4.7) 
</p>
<p>t For any further information see the classic, Classical Electrodynamics by J. D. Jackson, Wiley, New 
York (1975). </p>
<p/>
</div>
<div class="page"><p/>
<p>Feeding this into Eq. (18.4.3), we find that 493 
</p>
<p>TIME-DEPENDENT 
PERTURBATION 
</p>
<p>v x (E+! oA)=o 
c at 
</p>
<p>(18.4.8) THEORY 
</p>
<p>Based on the identity VxVcf&gt;=O, we deduce that E+(1/c)aA;at can be written as 
a gradient, or that 
</p>
<p>1 aA 
E=-- --Vcf&gt; 
</p>
<p>c at 
(18.4.9) 
</p>
<p>If we replace E and B by the potentials in the other two Maxwell equations and use 
the identity v X v X A= v (V. A) - V2 A (true in Cartesian coordinates) we get the 
equations giving the response of A and cf&gt; to the charges and currents: 
</p>
<p>2 1 a 
V cf&gt;+- -(V&middot;A)=-4np 
</p>
<p>c at 
(18.4.10) 
</p>
<p>(18.4.11) 
</p>
<p>Before attacking these equations, let us note that there exists a certain arbitrariness 
in the potentials A and cf&gt;, in that it is possible to change them (in a certain way) 
without changing anything physical. It may be readily verified that A and cf&gt; and 
</p>
<p>A'=A-VA ( 18.4.12) 
</p>
<p>(18.4.13) 
</p>
<p>where A is an arbitrary function, lead to the same fields E and B. 
</p>
<p>Exercise 18.4.2. * Calculate E and B corresponding to (A, l/J) and (A', l/J') using Eqs. 
(18.4.7) and (18.4.9) and verify the above claim. 
</p>
<p>Since the physics, i.e., the force law and Maxwell's equations, is sensitive only 
to E and B, the transformation of the potentials, called a gauge transformation, does 
not affect it. This is known as gauge invariance, A is called the gauge parameter, and 
(A, cf&gt;) and (A', cf&gt;') are called gauge transforms of each other, or said to be gauge 
equivalent. 
</p>
<p>Gauge invariance may be exploited to simplify Eqs. (18.4.10) and (18.4.11). We 
consider the case of the free electromagnetic field (p = j = 0), which will be of interest </p>
<p/>
</div>
<div class="page"><p/>
<p>494 
</p>
<p>CHAPTER 18 
</p>
<p>in the next section. In this case the gauge freedom allows us (see following exercise) 
</p>
<p>to choose A and 4&gt; such that 
</p>
<p>V&middot;A=O (18.4.14) 
</p>
<p>4&gt;=0 (18.4.15) 
</p>
<p>This is called the Coulomb gauge and will be used hereafter. There is no residual gauge 
</p>
<p>freedom if we impose the above Coulomb gauge conditions and the requirement that 
</p>
<p>IAI--+0 at spatial infinity: The potential in the Coulomb gauge is thus unique and 
</p>
<p>"physical" in the sense that for a given E and B there is a unique A. 
</p>
<p>Exercise 18.4.3. * Suppose we are given some A and rp that do not obey the Coulomb 
gauge conditions. Let us see how they can be transformed to the Coulomb gauge. 
</p>
<p>(I) Show that if we choose 
</p>
<p>A(r, t) = -c ["' rp (r, t') dt' 
</p>
<p>and transform to (A', &cent;') then &cent;' = 0. A' is just A- VA, with V &middot;A' not necessarily zero. 
</p>
<p>(2) Show that if we gauge transform once more to (A", &cent;") via 
</p>
<p>A'=-__l__f V&middot;A'{r', t) d 3r' 
4n lr-r'l 
</p>
<p>then V&middot;A"=O. [Hint: Recall V2(1/!r-r'l)=-4n8\r-r').] 
</p>
<p>(3) Verify that&cent;" is also '~ro by using V&middot;E=O. 
</p>
<p>(4) Show that if we want to make any further gauge transformations within the Coulomb 
</p>
<p>gauge, A must be time independent and obey V2 A= 0. If we demand that I Al-+0 at spatial 
</p>
<p>infinity, A becomes unique. 
</p>
<p>In the Coulomb gauge, the equations of motion for the electromagnetic field 
</p>
<p>(away from charges) simplify to 
</p>
<p>(18.4.16a) 
</p>
<p>V&middot;A=O (18.4.16b) 
</p>
<p>V&middot;A=O (18.4.16c) 
</p>
<p>The first equation tells us that electromagnetic waves travel at the speed c. Of special 
</p>
<p>interest to us are solutions to these equations of the formt 
</p>
<p>A= A0 cos(k &middot; r- rot) (18.4.17) 
</p>
<p>t Here k denotes the wave vector and not the unit vector along the z axis. </p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 18.2. The electromagnetic wave at a given time. E, 
B, and k (the wave vector) are mutually perpendicular. 
</p>
<p>Feeding this into the wave equation we find 
</p>
<p>or 
</p>
<p>m = kc 
</p>
<p>The gauge condition tells us that 
</p>
<p>0 = V &middot;A= -(k &middot; A 0) sin(k &middot;r- mt) 
</p>
<p>or 
</p>
<p>(18.4.18) 
</p>
<p>(18.4.19) 
</p>
<p>This means that A must lie in a plane perpendicular to the direction of propagation, 
i.e., that electromagnetic waves are transverse. The electric and magnetic fields corre-
sponding to this solution are 
</p>
<p>1 8A (w) . E=-~at=- -~- A0 sm(k&middot;r--mt) (18.4.20) 
</p>
<p>B=V x A= -(k x A0) sin(k&middot;r-wt) (18.4.21) 
</p>
<p>Thus E and Bare mutually perpendicular and perpendicular to k (i.e., they are also 
transverse)-see Fig. 18.2. They have the same magnitude: 
</p>
<p>IEI=IBI (18.4.22) 
</p>
<p>The energy flow across unit area (placed normal to k) per second is (from any 
standard text) 
</p>
<p>2 
</p>
<p>lSI=_!_ I (Ex B) I =_(J!____ IAol 2 sin2(k &middot;r- wt) 
4n 4nc 
</p>
<p>(18.4.23a) 
</p>
<p>495 
</p>
<p>TIME-DEPENDENT 
PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>496 
</p>
<p>CHAPTER 18 
</p>
<p>The time average over a cycle is 
</p>
<p>(l8.4.23b) 
</p>
<p>The energy per unit volume is 
</p>
<p>( 18.4.24) 
</p>
<p>Notice that lSI equals the energy density times the velocity of wave propagation. 
</p>
<p>The Potentials in Quantum Theory 
</p>
<p>We now ask if quantum mechanics also is invariant under gauge transformations 
</p>
<p>of the potentials. Let us seek the answer to this question in the path integral approach. 
</p>
<p>Recall that 
</p>
<p>U(rt, r'() = N L: exp[iS/Ii] ( 18.4.25) 
paths 
</p>
<p>where N is a normalization factor and the action 
</p>
<p>I", r&middot;~ (I . , 
S=J !t:'dt"= -mlti 2 +1J.v&middot;A&middot;&middot;&middot;&middot;&middot;ql/&gt;)dt" 
</p>
<p>( &bull; ( 2 c 
( 18.4.26) 
</p>
<p>is to be evaluated along each path P that connects (r', t') and (r. 1). Suppose we 
</p>
<p>perform a gauge transformation of the potentials. Then 
</p>
<p>But 
</p>
<p>f'q( aA\) , S-&gt; S = S &middot;&middot;&middot;&middot; .&middot; - v &middot;VA+- dt' 
A C , cJt" 
</p>
<p>I 
</p>
<p>aA dA 
v&middot;VA+--=-
</p>
<p>cJt'' dt" 
</p>
<p>is the total derivative along the trajectory. Consequently 
</p>
<p>SA= S+9. [/\(r', n -1\(r, t)] 
c 
</p>
<p>(18.4.27) 
</p>
<p>(18.4.28) 
</p>
<p>( 18.4.29) 
</p>
<p>It is clear that S and SA imply the same classical dynamics: varying S and varying 
</p>
<p>SA (to find the path of least actions) are equivalent, since sand s~, differ only by </p>
<p/>
</div>
<div class="page"><p/>
<p>(q/c)A at the end points, and the latter are held fixed in the variation. Going on to 
the quantum case, we find from Eqs. (18.4.25) and (18.4.29) that 
</p>
<p>(18.4.30) 
</p>
<p>Since 
</p>
<p>. U(r, t; r', t') = &lt;rl U(t, t')lr') (18.4.31) 
</p>
<p>we see that effect of the gauge transformation is equivalent to a change in the 
coordinate basis: 
</p>
<p>(18.4.32) 
</p>
<p>which of course cannot change the physics. (Recall, however, the discussion in Sec-
tion 7.4.) The change in the wave function under the gauge transformation is 
</p>
<p>(18.4.33) 
</p>
<p>This result may also be obtained within the Schrodinger approach (see the following 
exercise). 
</p>
<p>Exercise 18.4.4 (Proof of Gauge Invariance in the Schrodinger Approach). (1) Write H 
for a particle in the potentials (A, c/J). 
</p>
<p>(2) Write down HA, the Hamiltonian obtained py gauge transforming the potentials. 
(3) Show that if lJI(r, t) is a solution to Schrodinger's equation with the Hamiltonian H, 
</p>
<p>then lJIA(r, t) given in Eq. (18.4.33) is the corresponding solution with H~HA. 
</p>
<p>Although quantum mechanics is similar to classical mechanics in that it is insen-
sitive to gauge transformations of the potentials, it is different in the status it assigns 
to the potentials. This is dramatically illustrated in the Aharonov-Bohm effect, 
depicted schematically in Fig. 18.3.t The experiment is just the double-slit experiment 
with one change: there is a small shaded region (B#O) where magnetic fluxes comes 
out of the paper. (You may imagine a tiny solenoid coming out of the paper, inside 
which are confined the flux lines. These lines must of course return to the other end 
of the solenoid, but this is arranged not to happen in the experimental region.) The 
vector potential (in Coulomb gauge) is shown by closed loops surrounding the coil. 
At a classical level, this variation in the double-slit experiment is expected to make 
no change in the outcome, for there is no magnetic field along the classical paths P 1 
and P2 &bull; There is, of course, an A field along P1 and P2 , but the potential has no 
direct significance in classical physics. Its curl, which is significant, vanishes there. 
</p>
<p>Consider now the quantum case. In the path integral approach, a particle emitted 
by the source has the following amplitude to end up at a point r on the screen, 
before B is turned on: 
</p>
<p>lJI'(r) ~'I' P,(r) + 'If p,(r) (18.4.34) 
</p>
<p>t For the actual experiment see R. G. Chambers, Phys. Rev. Lett., 5, 3 (1960). 
</p>
<p>497 
</p>
<p>TIME-DEPENDENT 
PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>498 
</p>
<p>CHAPTER 18 
</p>
<p>Figure 18.3. An experiment (sche-
</p>
<p>matic) that displays the Aharonov-
</p>
<p>Bohm effect. It is just the double-
</p>
<p>slit experiment but for the small 
</p>
<p>coil coming out of the paper carry-
</p>
<p>ing magnetic flux (indicated by the 
</p>
<p>shaded region marked B#O). 
</p>
<p>where lf/P, (i= 1, 2) is the contribution from the classical path P; and its immediate 
</p>
<p>neighbors. The interference between these two contributions produces the usual inter-
</p>
<p>ference pattern. Let us turn on B. Now each path gets an extra factor 
</p>
<p>(18.4.35) 
</p>
<p>Since V x A= 0 near P1 and P 2 , by Stoke's theorem the integral is the same for P 1 
</p>
<p>and its neighbors and P2 and its neighbors. But the integral on P 1 is not the same 
</p>
<p>as the integral on P 2 , for these paths surround the coil and 
</p>
<p>f A&middot;dr-f A&middot;dr= f A&middot;dr= f (V x A)&middot;ds 
~ ~ s 
</p>
<p>(18.4.36) 
</p>
<p>where sis any surface bounded by the closed loop P 1 + P2 , and &lt;1&gt; is the flux crossing 
</p>
<p>it, i.e., coming out of the paper in Fig. 18.3. Bearing this in mind, we get 
</p>
<p>(. r , (. f ) lfl(r) = exp Lq, 1 A&middot; dr") lfl P,(r) + exp zq A&middot; dr" ljl p,(r) 
Jic &bull; P, ,rn P, . 
</p>
<p>(18.4.37) 
</p>
<p>Pulling out an overall phase factor, which does not affect the interference pattern, 
</p>
<p>we get 
</p>
<p>l1overall ') l- ( iq f \ J ij!(r)=_- lflpJr)+exp- A&middot;drjlf/P,(r) 
tactor ; Ju: 1 
</p>
<p>(overall) = [l!'P,(r) +exp[iq&lt;l&gt;/fic]lftp2(r)] 
factor 1 
</p>
<p>(18.4.38) 
</p>
<p>By varying B (and hence &lt;1&gt;) we change the relative phase between the contribu-
</p>
<p>tions from the two paths and move the interference pattern up and down. Whenever 
</p>
<p>( q&lt;l&gt; /fie)= 2nn:, the pattern will return to its initial form, as if there were no field. </p>
<p/>
</div>
<div class="page"><p/>
<p>In other words, an integral multiple of the flux quantum 
</p>
<p>~o= 2n1ic 
q 
</p>
<p>(18.4.39) 
</p>
<p>will not make any observable difference to the quantum mechanics of the particle. 
This idea is very frequently invoked; we shall do so in Chapter 21. 
</p>
<p>Let us understand how the particle discerns the magnetic field even though the 
dominant paths all lie in the B=O region. Suppose I show you Fig. 18.3 but cover 
the region where the coil is (the shaded region marked B#O); will you know there 
is magnetic flux coming out of the paper? Yes, because the circulating A lines will 
tell you that &sect;A&middot; dr = J B &middot; ds #OJ The classical particle, however, moves along P1 or 
P2 , and can have no knowledge of &sect;A&middot; dr. The best it can do is measure V x A 
locally, and that always equals zero. The quantum particle, on the other hand, "goes 
along P1 and P/' (in the path integral sense) and by piecing together what happens 
along P1 and P2 (i.e., by comparing the relative phase of the contributions from the 
two paths) it can deduce not only the existence of B, but also the total flux. Notice 
that although the particle responds to A and not directly to B, the response is gauge 
invariant. 
</p>
<p>18.5. Interaction of Atoms with Electromagnetic Radiation 
</p>
<p>We will make no attempt to do justice to this enormous field. We will consider 
just two illustrative examples. The first is the photoelectric effect in hydrogen (in 
which the incident radiation knocks the electron out of the atom). The second is the 
spontaneous decay of hydrogen from an excited state to the ground state (decay in 
the absence of external fields), which can be understood only if the electromagnetic 
field is treated as a quantum system. 
</p>
<p>Photoelectric Effect in Hydrogen 
</p>
<p>Consider a hydrogen atom in its ground state 1100) centered at the origin, and 
on which is incident the wave 
</p>
<p>A(r, t)=Aocos(k&middot;r-cot) (18.5.1) 
</p>
<p>For energies fzco sufficiently large, the bound electron can be liberated and will 
come flying out. We would like to calculate the rate for this process using Fermi's 
</p>
<p>t This is like saying that you can infer the existence of a pole in the complex plane and its residue, without 
actually going near it, by evaluating l/2ni &sect;f(z) dz on a path that encloses it. 
</p>
<p>499 
</p>
<p>TIME-DEPENDENT 
PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>500 
</p>
<p>CHAPTER 18 
</p>
<p>golden rule: 
</p>
<p>Two points need to be explained before the application of this rule: 
</p>
<p>(I) For the final state, we must use a positive energy eigenstate of the Coulomb 
</p>
<p>Hamiltonian H 0 = P2 /2m- e2 /r. Now we argue on intuitive grounds that if the ejected 
</p>
<p>electron is very energetic, we must be able to ignore the pull of the proton on it and 
</p>
<p>describe it by a plane wave IPf) in Eq. (18.5.2), with negligible error. While this 
</p>
<p>happens to be the case here, there is a subtle point that is worth noting. If we view 
the Coulomb attraction of the proton as a perturbation relative to the free-particle 
</p>
<p>Hamiltonian r/2m, we can write the eigenstate of H 0 as a perturbation series: 
</p>
<p>l/0 ) = IPf) +higher-order terms 
</p>
<p>We are certainly right in guessing that lp1) dominates the expansion at high energies. 
But we are assuming more: we are assuming that when we evaluate the matrix 
</p>
<p>element in Eq. (18.5.2) the leading term IPf) will continue to dominate the higher-
</p>
<p>order terms. Clearly, the validity of this assumption depends also on the initial state 
</p>
<p>I i 0 ) and the operator H 1&bull; Now it turns out that if the initial state is an s state (as 
in the present case) the higher-order terms are indeed negligible in computing the 
</p>
<p>matrix element, but not otherwise. For instance if the initial state is a p state, the 
</p>
<p>contribution of the first-order term to the matrix element would be comparable to 
the contribution from the leading term lp1). For more details, you must consult a 
book that is devoted to the subjecd 
</p>
<p>(2) The rule applied for potentials of the form H\t)=H 1 e-i'"', whereas here 
</p>
<p>[recall Eq. ( 14.4.11) ],&sect; 
</p>
<p>H 1(t)= (A&middot;P+P&middot;A) 
2mc 
</p>
<p>e A&middot;P (because V&middot;A=O) 
me 
</p>
<p>e 
= - cos(k &middot;r- wt)Ao&middot; P 
</p>
<p>me 
</p>
<p>= _!____ [ ei(k&middot;r .~ wr) + e -i(k-1'- '"'l&lt;\o. p 
2mc 
</p>
<p>(18.5.3) 
</p>
<p>t For example, Section 70 of H. Bethe and E. Salpeter, Quantum Mechanics of One and Two Electron 
Atoms, Plenum, New York (1977). This is also a good place to look for other data on this subj~'Ct. For 
</p>
<p>instance if you want to know what the expectation value of r&middot;&bull; is in the state lnlm) of hydrogen, you 
will find it here. 
</p>
<p>&sect;We do not include in H' the tem1 proportional to IAI 2 , which is of second order. The spin interaction 
-yS&middot; B is of the first order, but negligible in the kinematical region we will focus on. This will be 
</p>
<p>demonstrated shortly. </p>
<p/>
</div>
<div class="page"><p/>
<p>Of the two pieces, only the first has the correct time dependence to induce the 
transition i-+fwith E1&gt; E.-; the second will be killed by the energy-conserving delta 
function. Hereafter we ignore the second term and let 
</p>
<p>HI {t) = _e_ eik&middot;r Ao. p e -irot 
2mc 
</p>
<p>(18.5.4) 
</p>
<p>With these two points out of the way, we can proceed to evaluate the transition 
matrix element in the coordinate basis: 
</p>
<p>( )
</p>
<p>1/2 
</p>
<p>H},=-e- 1 _1_ fe-iprrflieik-rAo&middot;(-i1iV)e-rfaod3r 
2mc (2n:1i)312 n:a6 
</p>
<p>(18.5.5) 
</p>
<p>Consider the factor e;k .... Recall from Chapter 5 that multiplication of a wave function 
by eipo&middot;rfli adds to the state a momentum Po. Thus the factor eik-r represents the fact 
that a momentum 1ik is imparted by the radiation to the atom.t For any transition 
between atomic levels, this momentum transferred is neglible compared to the typical 
momentum p of the electron. We see this as follows. The energy transferred is of 
the order of a Rydberg: 
</p>
<p>so that the photon momentum is 
</p>
<p>1im i 
1ik=-~-
</p>
<p>c aoc 
</p>
<p>(18.5.6) 
</p>
<p>(18.5. 7) 
</p>
<p>On the other hand, the typical momentum of the electron, estimated from the uncer-
tainty principle, is 
</p>
<p>1i 
p~- (18.5.8) 
</p>
<p>ao 
</p>
<p>Thus 
</p>
<p>1ik ez 
(18.5.9) -------p 1ic 137 
</p>
<p>In the present case 1im is a lot higher because we have a liberated, high-energy 
electron. But there is still a wide range of m over which 1ik/p 4, 1. We will work in 
</p>
<p>f You may be worried that there is the (-ifrV) operator between e'""' and the atomic wave function. But 
since V &middot;A= 0, we can also write A&middot; P as P &middot;A, in which case the i"'' will be right next to the atomic 
wave function. 
</p>
<p>501 
</p>
<p>TIME-DEPENDENT 
PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>502 
</p>
<p>CHAPTER 18 
</p>
<p>this domain. In this domain, the ratio of the spin interaction we neglected, to the 
</p>
<p>orbital interaction we are considering, is roughly 
</p>
<p>((e/2mc)S&middot;B) (llCJ&middot;V x A) Ilk 
---'-----~ ~-~ 1 
((ejmc)A &middot; P) (A&middot; P) p 
</p>
<p>(18.5.10) 
</p>
<p>which justifies our neglect. 
The domain we are working in may also be described by 
</p>
<p>kao~ 1 (18.5.11) 
</p>
<p>[Eq. (18.5.9)]. This means that the phase of the wave changes little over the size of 
</p>
<p>the atom. Since the integral in Eq. (18.5.5) is rapidly cut off beyond r~a 0 by the 
wave function e -r!ao, we may appropximate eik&middot;r in the integral as 
</p>
<p>(18.5.12) 
</p>
<p>This is called the electric dipole approximation.t The reason is that in this approxima-
</p>
<p>tion, the atom sees a spatially constant electric field, 
</p>
<p>-1 aA 
E=--
</p>
<p>c at 
</p>
<p>iro A -irot 
=- oe 
</p>
<p>2c 
</p>
<p>and couples to it via its electric dipole moment J.1 = -eR: 
</p>
<p>I iroe -iwt 
H (t) = -wE=-Ao&middot;R e 
</p>
<p>2c 
</p>
<p>This must of course coincide with Eq. (18.5.3) in this approximation: 
</p>
<p>H 1(t) =-e- Ao&middot; P e-iwt 
2mc 
</p>
<p>(18.5.13) 
</p>
<p>(18.5.14) 
</p>
<p>(18.5.15) 
</p>
<p>t By keeping higher powers of k &middot; r in the expansion, one gets terms known as electric quadrupole, magnetic 
dipole, electric octupole, magnetic quadrupole, etc. contributions. 
</p>
<p>&sect;We ignore the "wrong" frequency part of A. </p>
<p/>
</div>
<div class="page"><p/>
<p>The equivalence of Eqs. (18.5.14) and (18.5.15) can be demonstrated in a general 
</p>
<p>situation as follows. Since for any 
</p>
<p>it is true that 
</p>
<p>we find 
</p>
<p>so that 
</p>
<p>0 IPI 2 T 
H =-+ v(R) 
</p>
<p>2m 
</p>
<p>0 ifi 
[R, H ]=&middot;&middot;&middot;&middot;&middot;&middot; P 
</p>
<p>m 
</p>
<p>( 18.5.16a) 
</p>
<p>(18.5.16b) 
</p>
<p>(18.5.17) 
</p>
<p>[by Eq. (18.5.14)] (18.5.18) 
</p>
<p>Consider now the evaluation of the matrix element H}1 in the dipole approximation: 
</p>
<p>where N is a constant given by 
</p>
<p>( 
e \ 1 I )3 .&middot; 2( l ) 1 12 
</p>
<p>N= j~c)~2~n ,;:;~~ 
</p>
<p>If we integrate the V by parts, we get 
</p>
<p>ll l=NA &bull;p &middot;f ,~fpr&bull;!fi ,~r:a 0 i3r fi &middot;0 '! ( (; ( 
</p>
<p>(J 8.5.19) 
</p>
<p>(18.5.20) 
</p>
<p>(18.5.21) 
</p>
<p>503 
</p>
<p>TIME-DEPENDENT 
</p>
<p>PER TUR EIA TlON 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>504 
</p>
<p>CHAPTER 18 
</p>
<p>1 ~elector 
</p>
<p>/4/ 
e /// 
</p>
<p>// 
</p>
<p>Figure 18.4. The photoelectric effect. In any 
</p>
<p>realistic experiment, the resolution in energy and 
</p>
<p>angle are finite. One asks how many electrons 
</p>
<p>come into the cone of solid angle dD. with magni-
</p>
<p>tude of momentum between p and p + dp. 
</p>
<p>(It should now be clear why we prefer the A0 &bull; P form of H 1 to the A0 &bull; R form.) If 
we choose the z axis along p1 , the r integral becomes 
</p>
<p>8n/ao 
-------"----------------------------------
</p>
<p>[(1/ao)l + (PJ/:Ii)2]2 
(18.5.22) 
</p>
<p>Feeding this into Eq. (18.5.5), and the resulting expression into the golden rule, we 
get the transition rate 
</p>
<p>( 18.5.23) 
</p>
<p>Now the time has come to tackle the 8 function. The 8 function gives a singular 
</p>
<p>probability distribution for finding a final electron in a state of mathematically precise 
momentum Pf&middot; This probability is of little interest in practice, where one sets up a 
detector with a finite opening angle dO. and asks how many electrons come into it 
with magnitude of momentum betweenprandpr+dp1 (see Fig. 18.4). The o function 
tells us that electron momenta are concentrated at 
</p>
<p>p}_ = E 0 + fim 
2m ' </p>
<p/>
</div>
<div class="page"><p/>
<p>The contribution from this region is obtained by integrating the 8 function over p1 . 
Using 
</p>
<p>(18.5.24) 
</p>
<p>we get the rate of transition into the detector to be 
</p>
<p>27r I 2 
R,~c~n =---,; I H ftl mpt dfJ. 
</p>
<p>4a~e 2 PJIAo"Ptl 2 dO. 
m7r1i4c2[1 + (ptaof1i)2]4 
</p>
<p>(18.5.25) 
</p>
<p>{In this and all following expressions, p1 = [2m(E7 + 1iro )] 112.} Note that the rate 
depends only on the magnitude of the applied field A0 , the angle between the polariza-
tion A0 and the outgoing momentum, and the magnitude of p1 or equivalently ro, 
the frequency of radiation. The formula above tells us that the electron likes to come 
parallel to A0 , that is, to the electric field which rips it out of the atom. The direction 
of the incident radiation does not appear because we set e'k&middot;r = I. If we keep the ik&middot;r 
factor it will be seen that the electron momentum is also biased toward k, reflecting 
the 1ik momentum input. 
</p>
<p>Exercise 18.5.1. * (1) By going through the derivation, argue that we can take the e'k&middot;r 
factor into account exactly, by replacing Pf by pf- fik in Eq. (18.5.19). 
</p>
<p>(2) Verify the claim made above about the electron momentum distribution. 
</p>
<p>If we integrate R;~c~n over all angles, we get the total rate for ionization. Choos-
ing A0 along the z axis for convenience, we find 
</p>
<p>(18.5.26) 
</p>
<p>Since this is the rate of ionization, and each ionization takes energy 1iro from the 
beam, the energy absorption rate is 
</p>
<p>dEabs 
--= 1iro &middot; R&middot; 11 dt Ha (18.5.27) 
</p>
<p>Now the beam brings in energy at the rate ro21Aol 2 j81rc per unit area. Suppose we 
place, transverse to this beam, a perfectly absorbing disk of area cr. It will absorb 
</p>
<p>505 
</p>
<p>TIME-DEPENDENT 
PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>506 
</p>
<p>CHAPTER 18 
</p>
<p>energy at the rate 
</p>
<p>dEabs aiAol 2m2 
</p>
<p>dt 8nc 
(18.5.28) 
</p>
<p>By comparing Eqs. (18.5.27) and (18.5.28), we see that we can associate with the 
</p>
<p>atom a photoelectric cross section 
</p>
<p>8nc 
0" = --2- 2 &bull; "lim &middot; Ri~an 
</p>
<p>IAol w 
(18.5.29) 
</p>
<p>(18.5.30) 
</p>
<p>in the sense that if an ensemble of N (N large) nonoverlapping (separation;pa0) 
</p>
<p>hydrogen atoms is placed in the way of the beam, the ensemble will absorb energy 
</p>
<p>like a perfectly absorbent disk of area Na. We can also associate a differential cross 
</p>
<p>section da / dn, with the energy flowing into a solid angle dO.: 
</p>
<p>da 8nc 
-=--2- 2 1imRhdn 
dn IAol m 
</p>
<p>32a~e 2 p} cos2 (} 
(18.5.31) 
</p>
<p>In the region where PJGo/"li iP 1, the formula simplifies to 
</p>
<p>da 32e21i5 cos2 (} 
</p>
<p>dO. mcmp}a6 
( 18.5.32) 
</p>
<p>Exercise 18.5.2. * (1) Estimate the photoelectric cross section when the ejected electron 
has a kinetic energy of 10 Ry. Compare it to the atom's geometric cross section ""7rai\. 
</p>
<p>(2) Show that if we consider photoemission from the Is state of a charge Z atom, aocZ 5, 
</p>
<p>in the limit PJGo/ Zli ~ I. 
</p>
<p>Field Quantizationt 
</p>
<p>The general formalism, illustrated by the preceding example, may be applied to 
</p>
<p>a host of other phenomena involving the interaction of atoms with radiation. The 
</p>
<p>results are always in splendid agreement with experiment as long the electromagnetic 
</p>
<p>field is of macroscopic strength. The breakdown of the above formalism for weak 
</p>
<p>fields is most dramatically illustrated by the following example. Consider a hydrogen 
</p>
<p>atom in free space (the extreme case of weak field) in the state 12, l, m). What is the 
</p>
<p>rate of decay to the ground state? Our formalism gives an unambiguous answer of 
</p>
<p>t The treatment of this advanced topic will be somewhat concise. You are urged to work out the missing 
steps if you want to follow it in depth. </p>
<p/>
</div>
<div class="page"><p/>
<p>zero, for free space corresponds to A=O (in the Coulomb gauge), so that JJ 1 =0 
</p>
<p>and the atom should be in the stationary state 12, l, m) forever. But it is found 
experimentally that the atom decays at a rateR~ 109 second- 1, or has a mean lifetime 
r ~ 10-9 second. In fact, all excited atoms are found to decay spontaneously in free 
space to their ground states. This phenomenon cannot be explained within our 
</p>
<p>formalism. 
</p>
<p>So are we to conclude that our description of free space (which should be the 
</p>
<p>simplest thing to describe) is inadequate? Yes! The description of free space by A= 
</p>
<p>A= 0 is classical; it is like saying that the ground state of the oscillator is given by 
x=p=O. Now, we know that if the oscillator is treated quantum mechanically, only 
</p>
<p>the average quantities (OIX I 0) and (01 PI 0) vanish in the ground state, and that 
</p>
<p>there are nonzero fluctuations (Ll.&yen;)2 = (OIX 210) and (&amp;Pf= (OiriO) about these 
mean values. In the same way, if the electromagnetic field is treated quantum mechan-
</p>
<p>ically, it will be found that free space (which is the ground state of the field) is 
described by (A)= (A)= 0 (where A and A are operators)t with non vanishing fluc-
</p>
<p>tuations (.1A)2, (.1A) 2&bull; The free space is dormant only in the average sense; there 
</p>
<p>are always quantum fluctuations of the fields about these mean values. It is these 
</p>
<p>fluctuations that trigger spontaneous decay. 
</p>
<p>As long as we restrict ourselves to macroscopic fields, the quantum and classical 
descriptions of the field become indistinguishable. This is why in going from classical 
to quantum mechanics, i.e., in going from .Jf-' 1 = (e/mc)A &middot;p to H 1 = (e/mc)A &middot;I&gt;, we 
merely promoted p to the operator P, but let A continue to be the classical field. 
</p>
<p>For this reason, this treatment is called the semiclassical treatment. We now turn to 
</p>
<p>the full quantum mechanical treatment in which A will become an operator as well. 
</p>
<p>The basic idea behind quantizing the field is familiar: one finds a complete set 
</p>
<p>of canonical coordinates and momenta to describe the classical field, and promotes 
</p>
<p>them to operators obeying canonical commutation relations. One then takes Jlf, 
</p>
<p>which is just the field energy written in terms of the canonical variables, and obtains 
</p>
<p>JJ by the usual substitution rule. But there are many obstacles, as we shall see. 
</p>
<p>Let us start with the coordinaters of the field. If we decide to describe it in terms 
of the potentials, we have, at each point in spacer, four real coordinates ( 1/J(r), A(r)).&sect; 
</p>
<p>Now, we already know that these coordinates are not entirely physical, in that they 
</p>
<p>can be gauge transformed with no observable consequences. For them to be physical, 
we must constrain them to a point where there is no residual gauge freedom, say by 
</p>
<p>imposing the Coulomb gauge conditions. Although we shall do so eventually, we 
treat them as genuine coordinates for the present. 
</p>
<p>What are the momenta conjugate to these coordinates? To find out, we turn to 
the Lagrangian: 
</p>
<p>f f[l ' 12 J - 1 2 z 3 - 1 I oA . 2 3 ff'-- [I El - I Bl ] d r-- - --- - - V &cent; - IV x AI d r II 8n 8n c at (18.5.33) 
t We depart from our convention here and denote classical and quantum field variables by the same 
</p>
<p>symbols, because this is what everyone does in this case. 
</p>
<p>&sect;Now r is just a label on the field coordinates and not a dynamical variable. 
</p>
<p>II If you are unfamiliar with this!&pound;': Recall that the field energy is f (l/8n)[I~=I"-+-IBI 2 ] d 3r, Eq. (18.4.24)_ 
Write this in the gauge &cent;=0 and change the sign of the term that corresponds to "potential energy." 
</p>
<p>The above result is just the generalization to the gauge with &cent; #0. 
</p>
<p>507 
</p>
<p>TIME-DEPENDENT 
</p>
<p>PERTURBATION 
THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>508 
</p>
<p>CHAPTER 18 
</p>
<p>which, when varied with respect to the potentials, gives Maxwell's equations.t The 
momentum conjugate to each "coordinate" is the derivative of .!F with respect to 
the corresponding "velocity." It follows that the momentum conjugate to rp(r) van-
ishes (at each point r in space), for cfo(r) does not appear in !F. The fact that we are 
dealing with a coordinate whose conjugate momentum vanishes identically tells us 
that we can not follow the canonical route. But fortunately for us, we have the 
freedom to work in a gauge where rp = 0. So hereafter we can forget all about rp and 
its vanishing conjugate momentum. In particular, we can set rp=O in Eq. (18.5.33). 
</p>
<p>Consider now the coordinates A(r). To find ll;(r0), the momentum conjugate 
to A;(r0), we use the relation 
</p>
<p>In differentiating .!F with respect to Air0), we treat the integral in Eq. ( 18.5.33) over 
rasa sum over the continuous index r. The partial derivative picks out just the term 
in the sum carrying the index r = r0 (because the velocities at different points are 
independent variables) and gives&sect; 
</p>
<p>1 . -E;(r0) 
ll;(ro) = ~ A;(ro) = --
</p>
<p>4n-c2 4n-c 
(18.5.34) 
</p>
<p>or in vector form (dropping the subscript 0 on r) 
</p>
<p>I &middot; E 
ll(r)=~A=--
</p>
<p>4n-c2 4n-c 
(18.5.35) 
</p>
<p>Note that ll is essentially the electric field. 
The natural thing to do at this point would be to promote A and II to quantum 
</p>
<p>operators obeying canonical commutation rules, and obtain the quantum Hamil-
tonian H by the substitution rule. But if we did this, we would not be dealing with 
</p>
<p>t See for example, H. Goldstein, Classical Mechanics, Addison-Wesley, Reading, Massachusetts (1965), 
page 366. 
</p>
<p>&sect;A more formal treatment is the following. If, say, I&pound;= L; q~, then we know 
</p>
<p>p&middot;= 82 =L2ti aq,=L2tio =2ti 
1 oq1 ; I aqj i ' lj ' 
</p>
<p>Likewise if 
</p>
<p>f . 3 3 &bull; = ~2A,(r)o"o (r-ro)d r=2AJ{ro) </p>
<p/>
</div>
<div class="page"><p/>
<p>electrodynamics. The reason is classical and is as follows. Consider the Lagrangian 
in Eq. (18.5.33) with ifJ set equal to 0. By varying it with respect to the components 
of A we get the vector equation 
</p>
<p>which is just 
</p>
<p>1 aE 
VxB---=0 
</p>
<p>c at 
</p>
<p>in the gauge with ifJ = 0. Two other equations 
</p>
<p>V&middot;B=O 
</p>
<p>1 as 
VxE+--=0 
</p>
<p>c at 
</p>
<p>(18.5.36) 
</p>
<p>are identically satisfied if we write E and B in terms of A. (Recall how the potentials 
were introduced in the first place.) As for the other Maxwell equation, Gauss's law, 
</p>
<p>V&middot;E=O 
</p>
<p>it does not follow from anything. (We would get this if we varied 2 with respect to 
r/J, but we have eliminated ifJ from the picture.) It must therefore be appended as an 
equation of constraint on the momentum II, which is just E times a constant. (In 
contrast to an equation of motion, which has time derivatives in it, an equation of 
constraint is a relation among the variables at a given time. It signifies that the 
variables are not independent.) The constraint 
</p>
<p>V&middot;II=O (18.5.37) 
</p>
<p>tells us that the components of momenta at nearby points are not independent. 
(Think of the derivatives in V as differences.) We deduce an important feature of 
the constraint if we take the divergence of Eq. (18.5.36): 
</p>
<p>2 1 a2 2 a 
0 = V &middot; V A-- - V &middot;A- V V &middot;A--+- (V &middot;II) = 0 
</p>
<p>c2 at2 at 
(18.5.38) 
</p>
<p>In other words, the theory without the constraint has a conserved quantity V &middot;II, 
and electrodynamics corresponds to the subset of trajectories in which this constant 
of motion is zero. Furthermore, if we limit ourselves to these trajectories, we see 
that V&middot;A is also a constant of motion. [Write Eq. (18.5.37) as V&middot;A=O.] We shall 
choose this constant to be zero, i.e., work in Coulomb gauge. 
</p>
<p>How are we to quantize this theory? One way is to ignore the constraints and 
to quantize the general theory and then try to pick out the subset of solutions (in 
</p>
<p>509 
</p>
<p>TIME-DEPENDENT 
PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>510 
</p>
<p>CHAPTER 18 
</p>
<p>the quantum theory) that correspond to electrodynamics. This can be done but is 
</p>
<p>very hard. Let us therefore tackle the constraints at the classical level. The first 
</p>
<p>problem they pose is that they render the variables A and II noncanonical, and we 
</p>
<p>do not have a recipe for quantizing noncanonical variables. Let us verify that the 
</p>
<p>constraints indeed imply that A and II are noncanonical. Had they been canonical, 
</p>
<p>they would have obeyed the generalizations of 
</p>
<p>(with all other PB zero), namely, 
</p>
<p>(18.5.39) 
</p>
<p>(with all other PB zero.) But if we take the divergence of A with respect tor or II 
</p>
<p>with respect tor', we get zero on the left-hand side but not on the right-hand side. 
</p>
<p>What we would like to do is the following. We would like to trade A and II 
</p>
<p>for a new set of variables that are fewer in number but have the constraints built 
</p>
<p>into them. (This would be like trading the variables x, y, and z constrained by 
</p>
<p>x2 + l + z2 = a2 for the angles () and cfJ on a sphere -Jf radius a.) These variables and 
the corresponding momenta would be canonical and would automatically reproduce 
</p>
<p>electrodynamics if we start with Jf written in terms of these. To quantize, we promote 
</p>
<p>these variables to operators obeying canonical commutation rules. The Hamiltonian 
</p>
<p>and other operators would then be obtained by the substitution rule. 
</p>
<p>Now the problem with the constraints 
</p>
<p>V&middot;A=O, V&middot;II=O (18.5.40) 
</p>
<p>called transversality constraints (for a reason that will follow) is that they are not 
</p>
<p>algebraic, but differential equations. To render them algebraic, we will trade A and II 
</p>
<p>for their Fourier transforms, since differential equations in coordinate space become 
</p>
<p>algebraic when Fourier transformed. It is our hope that the algebraic constraints 
</p>
<p>among the Fourier coefficients will be easier to implement. We will find that this is 
</p>
<p>indeed the case. We will also find a bonus when we are done: the Fourier coefficients 
</p>
<p>are normal coordinates; i.e., when we express the Hamiltonian 
</p>
<p>(18.5.41) 
</p>
<p>(which is obtained from !&pound;' by changing the sign of the potential energy term and 
</p>
<p>eliminating A in favor of II) in terms of these, it becomes a sum over oscillator 
Hamiltonians of decoupled oscillators. This result could have been anticipated for 
</p>
<p>the following reason. If we use the relation IV x Al 2 = -A&middot; V2 A, valid when V &middot;A= 0, 
</p>
<p>we get 
</p>
<p>(18.5.42) </p>
<p/>
</div>
<div class="page"><p/>
<p>which is of the same form as Eq. (7.1.10). [Remember that when we sandwich the 
derivative operator or the identity operator between two elements of function space, 
there will be only one (explicit) sum over the continuous index r, the other one being 
eaten up by the delta functions in the matrix elements.] As the normal modes are 
the eigenvectors of V2 (which we know are plane waves) the passage to the Fourier 
coefficients is the passage to normal coordinates. 
</p>
<p>With all these preliminaries out of the way, let us turn to the Fourier transform 
of the unconstrained A : 
</p>
<p>A(r) = J [a(k) eik-r + a*(k) e -ik&middot;r] d 3k (18.5.43) 
</p>
<p>This expansion deserves a few comments. 
(1) Since we are Fourier transforming a vector A, the Fourier coefficients are 
</p>
<p>vectors a(k). [You may view Eq. (18.5.43) as giving three Fourier expansions, one 
for each component of A.] 
</p>
<p>(2) Since A(r) is a real function, the Fourier coefficient at k and -k must be 
complex conjugates. Our expansion makes this apparent. Stated differently, one real 
vector function A in coordinate space cannot specify one complex vector function 
a(k) ink space: if we multiply both sides with e-iko&middot;r and integrate over r, we find 
that this is indeed the case: 
</p>
<p>(18.5.44) 
</p>
<p>i.e., A(r) is seen to determine only the combination a(k) +a*( -k). We shall exploit 
this point shortly. 
</p>
<p>(3) There is no time argument shown in Eq. (18.5.43) because we view it as 
linear relations between two sets of coordinates, such as the relations 
</p>
<p>Xt+Xn 
Xt=~ 
</p>
<p>X1- Xn 
</p>
<p>x2=~ 
</p>
<p>which are understood to be true at all times. The discrete labels 1, 2, I, and II are 
replaced here by the continuous labels r and k. 
</p>
<p>We similarly expand II (before the transversality constraint is imposed) as 
</p>
<p>(18.5.45) 
</p>
<p>The factor (kj4nic) is pulled out to simplify future manipulations. Note that the same 
function a(k) appears here. There is no conflict, since Il(r) determines a different 
</p>
<p>511 
</p>
<p>TIME-DEPENDENT 
PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>512 
</p>
<p>CHAPTER 18 
</p>
<p>combination: 
</p>
<p>(18.5.46) 
</p>
<p>It is clear that Eqs. (18.5.44) and (18.5.46) may be solved for a(k) in terms of 
</p>
<p>A and ll: the two real vector functions A(r) and ll(r) determine one complex vector 
</p>
<p>function a(k). Consider now the vector a(k) at a given k. We can expand it in terms 
</p>
<p>of ~my three orthonormal vectors. Rather than choose them to be the unit vectors 
</p>
<p>along the x, y, and z directions, let us choose them (with an eye on the constraints) 
</p>
<p>as a function of k, in the following way: 
</p>
<p>E(kl)} 
</p>
<p>E(k2) 
orthonormal vectors in the plane perpendicular to k 
</p>
<p>(18.5.47) 
</p>
<p>E(k3) a unit vector parallel to k 
</p>
<p>If we now expand a(k) (at each k) as 
</p>
<p>3 
</p>
<p>a(k)= I (c2/4n2w) 112a(kA)E(kA) (18.5.48) 
A~ I 
</p>
<p>(where w = kc) and feed this into the expansions for A and ll, we get 
</p>
<p>(18.5.49a) 
</p>
<p>(18.5.49b) 
</p>
<p>These equations relate the old coordinates-three real components of A and 
</p>
<p>three real components of ll at each point in r space-to three complex components 
</p>
<p>of a at each point in k space. Since A and n are canonical variables before we impose 
transversality, their PB are 
</p>
<p>{ A;(r), Aj(r')} = 0 
</p>
<p>{ll;(r), llj(r')} = 0 
</p>
<p>{A;(r), llir')}=5ii5\r-r') 
</p>
<p>From these we may deduce (after some hard work) that 
</p>
<p>{ a(kA), a(k' A')}= 0 = { a*(kA), a*(k' A')} 
</p>
<p>{ a(kA), a*(k' A')}= -i5 u'53(k- k') 
</p>
<p>(18.5.50) 
</p>
<p>(18.5.51) </p>
<p/>
</div>
<div class="page"><p/>
<p>We now address the problem of imposing the constraints, i.e., of regaining 
electrodynamics. The conditions V &middot;A= 0 and V &middot; n = 0 tell us [when we apply them 
to Eqs. (18.5.43) and (18.5.45) and project both sides onto some given k], 
</p>
<p>from which we deduce that 
</p>
<p>k &middot; [a(k) +a*( -k)] =0 
k&middot;[a(k)-a*(-k)]=O 
</p>
<p>k&middot;a(k)=O (18.5.52) 
</p>
<p>The two differential equations of constraint have reduced, as anticipated, to (a 
complex) algebraic constraint. Imposing it on Eq. (18.5.48), we find [using k&middot;E (k, 
lor2)=0], 
</p>
<p>a(k3) =0 (18.5.53) 
</p>
<p>Thus the constraint tells us something very simple: every a(k3) is zero. (Since it 
forces a(k) to lie in a plane transverse to k, we call it the transversality constraint). 
Implementation of the transversality constraint is very simple in momentum space: 
hereafter we let A. take on only the values 1 and 2. Also, setting a(k3) = 0 does not 
change the PB between the remaining a's. Equation (18.5.49) for A and n continues 
to hold, with A. so restricted. However, these fields are now guaranteed to meet the 
transversality conditions. 
</p>
<p>Now for the other nice feature of these conditions. If we express Jf in terms of 
these, we get 
</p>
<p>(18.5.54) 
</p>
<p>Thus a(k.A.) are normal coordinates in the sense that Jf contains no cross terms 
between a's carrying different labels. If we want to get the familiar oscillators, we 
define real variables 
</p>
<p>1 
q(k.A.) =(2m )1/2 [a(kA.) + a*(kA.)] 
</p>
<p>1 ( )
1
</p>
<p>/
2 
</p>
<p>p(k.A.) = t ~ [a(kA.)- a*(k.A.)] 
(18.5.55) 
</p>
<p>which satisfy the canonical PB relations [as you may verify by combining Eqs. 
(18.5.51) and (18.5.55)]. In terms of these variables 
</p>
<p>(18.5.56) 
</p>
<p>513 
</p>
<p>Tl ME-DEPENDENT 
PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>514 
</p>
<p>CHAPTER 18 
</p>
<p>Thus we find that the radiation field is equivalent to a collection of decoupled 
oscillators: there is an oscillator at each k and ll ( = 1 or 2) with frequency w = kc. 
The quantization of the radiation field then reduces to the quantization of the oscilla-
tor, which has already been accomplished in Chapter 7. 
</p>
<p>Since q(kll) and p(k},) are independent canonical coordinates describing the 
</p>
<p>field, we can quantize the field by promoting these to operators Q and P obeying 
canonical commutation rules: 
</p>
<p>[Q(k.A), P(k'IL ')]=in{ q, p} = iM.u&middot;83(k- k') 
</p>
<p>with all other commutators vanishing. As in the case of a single oscillator, it proves 
</p>
<p>useful to work with the combination 
</p>
<p>f \ 1/2 ( , I /2 
</p>
<p>a(kll)=(.&middot;0!) Q+i 1 -) P 
,2fz; 2m rz 
</p>
<p>and its adjoint 
</p>
<p>(18.5.57)t 
</p>
<p>which obey 
</p>
<p>( 18.5.58) 
</p>
<p>and in terms of which A and n,&sect; which are now Hermitian operators, are given by 
</p>
<p>(l8.5.59a) 
</p>
<p>(18.5.59b) 
</p>
<p>To find H, we first symmetrize /It', i.e., a* a -&middot;&bull;ha*a+aa*), make the operator substi-
</p>
<p>tution, and use Eq. (18.5.58), to get 
</p>
<p>(18.5.60) 
</p>
<p>t A small point, in case you are following all the details: a and a' above are the operators corresponding 
to the classical variables a/n 1i 2 and a* /n 112 To see this, invert Eq. (18.5.55). All we need hereafter are 
Eqs. ( 18.5.57)-(18.5.59). 
</p>
<p>&sect;We use the same symbols for the classical and quantum variables in order to follow a widely used 
</p>
<p>convention in this case. It should be dear from the context which is which. </p>
<p/>
</div>
<div class="page"><p/>
<p>Let us now consider the eigenstates of H. In the field ground state 10), all the 
oscillators are in their respective ground states. Thus any lowering operator will 
annihilate 10): 
</p>
<p>a(kA.)IO) =0 for all k, A. 
</p>
<p>The energy of this state, called the vacuum state or simply vacuum, is 
</p>
<p>f fu.o 3 Eo='L -d k 
"' 2 
</p>
<p>(18.5.61) 
</p>
<p>(18.5.62) 
</p>
<p>which is the sum over the zero point energies of the oscillators. This constant energy 
E0 has no physical consequences. 
</p>
<p>We now verify the results claimed earlier. In this ground state 
</p>
<p>(OIAIO) ~(OI(a+at)IO) =0 
</p>
<p>(01010) ~ (OI(a-at)IO) =0 
(18.5.63) 
</p>
<p>In the above equation we have omitted a lot of irrelevant factors; only the central 
idea-that A and U are linear combinations of creation and destruction operators 
and hence have no diagonal matrix elements in 10)-is emphasized. On the other 
hand, 
</p>
<p>(011AI 2!0);.60 
</p>
<p>(O!Illl 210)1'0 
</p>
<p>for the same reason that (X2) ;.60, (/:J2) ;.60 for a single oscillator. 
If we act on jO) with one of the raising operators, we get 
</p>
<p>a\kA.)jO) = jkA.) 
</p>
<p>(18.5.64) 
</p>
<p>(18.5.65) 
</p>
<p>where the labels k and A. tell us that the oscillator bearing that label has gone to its 
first excited level. This state has energy 1im = 1ikc above Eo as may be verified by 
letting H act on it and using Eqs. (18.5.58) and (18.5.61). What about the momentum 
content? Any standard textbook on electrodynamics will tell us that the momentum 
of the field is given, in classical physics, by 
</p>
<p>flll=-1- JcE x B) d3r 
4nc 
</p>
<p>(18.5.66) 
</p>
<p>If we calculate the corresponding quantum operator we will find that it is given by 
</p>
<p>(18.5.67) 
</p>
<p>515 
</p>
<p>TIME-DEPENDENT 
</p>
<p>PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>516 
</p>
<p>CHAPTER 18 
</p>
<p>It is clear on inspection or explicit operation that 
</p>
<p>PikA.)= liklkA) (18.5.68) 
</p>
<p>Thus the state lk/c) has momentum lik. 
</p>
<p>If we apply a\klc) on the vacuum n times, we will create a state with energy 
</p>
<p>nliw and momentum nlik. This allows us to view the action of at(k/c) as the creation 
</p>
<p>of particles of momenta lik and energy liw. These particles, called photons, are 
</p>
<p>massless since 
</p>
<p>( 18.5.69) 
</p>
<p>In terms of photons, we have the correspondence 
</p>
<p>{quantum state} &lt;--&gt; {quantum state of} &lt;--&gt; {number of photons}
 
</p>
<p>of field each oscillator at each k and A 
</p>
<p>For future use, let us obtain the wave function of the photon in the state (k, /c). 
</p>
<p>We begin by deducing the normalization of the states. Combining Eqs. (18.5.65) 
</p>
<p>and (18.5.58) we get 
</p>
<p>(k'/c 'lkA) = (Oia(k'A')at(kA)IO) 
</p>
<p>= (Oiata + o;.;:o3(k- k')IO) 
= Ou&middot;0 3(k- k') (18.5.70) 
</p>
<p>(assuming (010)=1). The o3(k-k') factor and the fact that lik is the momentum 
of the state tell us that the wave function corresponding to lk, /c) is 
</p>
<p>(18.5.71) 
</p>
<p>We use the ~ sign instead of the -+ sign because A has not entered the wave function 
</p>
<p>yet. From the 8;.;: factor and the way A entered the picture in the first place, we 
</p>
<p>conclude that A represents the polarization vector: 
</p>
<p>(18.5.72) 
</p>
<p>You may be unhappy over the fact that unlike the eik&middot;r /(2n)312 factor, which followed 
</p>
<p>from analyzing the momentum content of the state [i.e., from the analysis of Eq. 
</p>
<p>(18.5.68)], the &amp; was pulled out of a hat. It too may be deduced, starting with angular 
</p>
<p>momentum considerations. We do not do so here. 
</p>
<p>Since the wave function of the photon is not a scalar, it has spin. Furthermore, 
</p>
<p>since &amp; is a three-component object, the spin is unity. However, the requirement that 
</p>
<p>k &middot; &amp; = 0 imposes a constraint on the possible orientations of photon spin. Consider, 
</p>
<p>for example, a photon moving along the z axis. The condition k &middot; &amp; = 0 tells us that </p>
<p/>
</div>
<div class="page"><p/>
<p>E cannot have a component along the z axis. What does this mean? The component 
</p>
<p>of E parallel to the z axis is characterized by the fact that it remains invariant under 
</p>
<p>rotations around the z axis, i.e., transforms like an s: = 0 state. So we conclude that 
</p>
<p>the photon can have only S: = &plusmn;fz, but not .1&middot;= = 0. More generally, the spin of the 
photon can only take values &plusmn;li parallel to its momentum. The component of spin 
</p>
<p>parallel to momentum is called helicity. The transversality condition restricts the 
</p>
<p>helicity to be &plusmn;fz-it precludes helicity zero.t 
</p>
<p>We consider one last feature of photons before turning to the problem that 
</p>
<p>started this inquiry, namely, spontaneous decay. Consider a state with one photon 
</p>
<p>in (kA.) and another in (k' A.'): 
</p>
<p>lkA, k' A')= at (kA.)at (k' A ')10) (18.5.73) 
</p>
<p>If we exchange the photon states we get the state 
</p>
<p>(18.5.74) 
</p>
<p>But since [at, a+]= 0, the two state vectors coincide, as they should for identical 
</p>
<p>bosons. 
</p>
<p>Spontaneous Decay 
</p>
<p>Consider the spontaneous decay of the hydrogen atom from 12/m) to 1100). 
</p>
<p>The perturbing Hamiltonian is still given by the substitution rule 
</p>
<p>I e I e 
&pound; =-A&middot;p-&gt;H =-A&middot;P ( 18.5. 75) 
</p>
<p>me me 
</p>
<p>but the A in H 1 is now the operator in Eq. (18.5.59a). 
</p>
<p>The initial state of the system (atom+ field) is 
</p>
<p>(18.5.76) 
</p>
<p>The final state is 
</p>
<p>(18.5. 77) 
</p>
<p>The perturbation H 1 1s time independent (A is the operator in the Schrodinger 
</p>
<p>picture) and 
</p>
<p>EJ- E7 = E10o + fzm - E21m (18.5.78) 
</p>
<p>t The graviton, which is massless and has spin 2, also has only two helicity states, &plusmn;2.1i. This is a general 
feature of massless bosons with spin. 
</p>
<p>517 
</p>
<p>TIME-DEPENDENT 
</p>
<p>PERTURBA'I'ION 
THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>518 
</p>
<p>CHAPTER 18 
</p>
<p>From Fermi's golden rule, we gett 
</p>
<p>( 1 8.5. 79) 
</p>
<p>Consider 
</p>
<p>(18.5.80) 
</p>
<p>Now, A is a sum over a's and at's with different labels. The only relevant one is 
</p>
<p>at(kA.), which raises 10) to 1kA.). Thus, including the factors that accompany a 1(kA), 
</p>
<p>( 18.5.81) 
</p>
<p>so that 
</p>
<p>In the dipole approximation, this becomes, upon using Eq. (18.5.17),&sect; 
</p>
<p>(18.5.82) 
</p>
<p>From parity considerations, it is clear that only l = I is relevant. Writing E' r in 
</p>
<p>the spherical basis (recall Exercise 15.3.2), 
</p>
<p>+I 
</p>
<p>E&middot;r= 2.: (---I)qe'{rlq 
--I 
</p>
<p>= ~c:Jr\ 1 + ~:?r? ~ e\ 1r7 1 ( 18.5.83) 
</p>
<p>where 
</p>
<p>(18.5.84) 
</p>
<p>tIn the photoelectric effect, the field is treated as an external time-dependent perturbation that acts on 
</p>
<p>the atom, and the Piw in the delta function reflects this time dependence. In the present case, the field 
</p>
<p>is part of the system and the !iw stands for the change in its energy. 
</p>
<p>&sect;We are unfortunately forced to use the symbol m for the mass as well as lhe z component of angular 
</p>
<p>momentum. It should be clear from the context what m stands for. </p>
<p/>
</div>
<div class="page"><p/>
<p>and from Eq. (12.5.42), 
</p>
<p>&plusmn;I 47&lt; &plusmn;I ( )
1/2 
</p>
<p>r1 = 3 rY1, (18.5.85) 
</p>
<p>we get 
</p>
<p>f * 3 (47&lt;)1/2 f 2 lflwoE"llfi2Im d r= 3 RwrR21r dr 
X [f yg*(-t:l Yi 1 + t:?Y?- t:l 1 Yi 1)Y;" dQ J 
</p>
<p>(
3)1/2 28 ao I o -I 
</p>
<p>= l 35 31/2 (+t:!Om,+l + GJOm,o+ GJ Om,-d (18.5.86) 
</p>
<p>The evaluation of the integrals (like so many other steps in this high-speed treatment) 
</p>
<p>is left as an exercise. The modulus squared of the above quantity is 
</p>
<p>If we average over the three initial m's (i.e., over an ensemble of such atoms randomly 
</p>
<p>distributed with respect to m), this reduces to 
</p>
<p>(18.5.87) 
</p>
<p>Notice that the result is independent of the direction ofE. This is to be expected since 
</p>
<p>the atom has no sense of direction after the angular (m) averaging. The transition rate 
</p>
<p>IS 
</p>
<p>(18.5.88) 
</p>
<p>where I means the initial state is averaged over all orientations. 
If we sum over all possible photon momenta and two possible polarizations at 
</p>
<p>each momentum, we get, using 
</p>
<p>f 47re o(Ewo + lim - Eum)e dk dO.=--
lie 
</p>
<p>519 
</p>
<p>TIME-DEPENDENT 
</p>
<p>PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>520 
</p>
<p>CHAPTER 18 
</p>
<p>where 
</p>
<p>k=~=Eum-Ewo 
c fie 
</p>
<p>i (1 1)- 3e2 -- -----
2ao1ic 4 8a01ic 
</p>
<p>the total decay rate 
</p>
<p>(18.5.89) 
</p>
<p>Recall that 
</p>
<p>mc2 mc2c 0.5 x 106 eV c 
- =&middot;-------.,---
</p>
<p>1i 1ic - 2000 eVA 
</p>
<p>~0.25 X 103 A -I c 
</p>
<p>Now c = 3 x 1010 em/sec= 3 x 1018 A/sec. So 
</p>
<p>and 
</p>
<p>( )
</p>
<p>5 
</p>
<p>8 1 21 -t R,_an ~ (0.67) - 10 seconds 
137 
</p>
<p>~o.6 x 109 seconds- 1 
</p>
<p>The corresponding mean lifetime is 
</p>
<p>'= I I R ~ 1.6 x 10-9 seconds (18.5.90) 
</p>
<p>in excellent agreement with experiment. 
Even if the fields are macroscopic, we can use the full quantum theory, though 
</p>
<p>the semiclassical treatment will give virtually identical results. The relation of the 
two approaches may be described as follows. Consider a process in which an atom 
goes from the state ia to the state Ia and the field goes from the state with n photons 
in (k, A.) to n + 1 photons in (k, A.).t The result we get in the quantum mechanical 
treatment of this process, which involves the emission of a photon, will agree with 
the semiclassical calculation if we use a classical field A whose energy density&sect; is the 
same as that of (n+ I) photons in (k, A.). The 1 inn+ I is all important at small n, 
and contains the key to spontaneous decay. If we consider a process where a photon 
is absorbed, so that n--'" n - I, the semiclassical method gives the correct answer if we 
</p>
<p>t We do not concern ourselves with other modes, which are spectators. 
&sect; The wavelength and polarization are of course the same as that of the photons. </p>
<p/>
</div>
<div class="page"><p/>
<p>use a classical field A such that the energy density is that of the n photons. The 
</p>
<p>appearance of the (n+ 1) and n factors is easy to understand in the oscillator lan-
</p>
<p>guage. When a photon is created, the amplitude goes as 
</p>
<p>(18.5.91) 
</p>
<p>which gives the factor (n + 1) in the probability, while if it is destroyed, 
</p>
<p>&lt;n-11aln)=n112&lt;n-lln-l) (18.5.92) 
</p>
<p>which gives a factor n in the probability. 
</p>
<p>It is conventional to separate the emission probability proportional to n + 1 
into the probability for induced emission, proportional to n, and the probability for 
</p>
<p>spontaneous emission, proportional to 1. The induced emission is induced by the 
</p>
<p>preexisting photons, and the spontaneous emission is-well, spontaneous. 
</p>
<p>The (n + 1) factor in the emission probability is a feature of bosons in general: 
the probability of a system emitting a boson into a quantum state already occupied 
</p>
<p>by n bosons (of the same kind), is (n + 1) times larger than the probability of emission 
into that state if it is initially unoccupied. This principle is exploited in a laser, which 
</p>
<p>contains a cavity full of atoms in an excited state, ready to emit photons of a fixed 
</p>
<p>frequency but arbitrary directions for k and A. The geometry of the cavity is such 
</p>
<p>that photons of a certain k and A get trapped in it. Consequently, these trapped 
</p>
<p>photons stay back to influence more and more atoms to emit into the mode (kA). 
</p>
<p>This is why we call it light amplification by stimulated emission of radiation. (This 
</p>
<p>genewl principle, in modified form, is exploited in television also: this is the whole 
</p>
<p>idea behind Cll.nned laughter.) 
</p>
<p>521 
</p>
<p>TIME-DEPENDENT 
</p>
<p>PERTURBATION 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>19 
</p>
<p>Scattering Theory 
</p>
<p>19.1. Introduction 
</p>
<p>One of the best ways to understand the structure of particles and the forces 
</p>
<p>between them is to scatter them off each other. This is particularly true at the 
</p>
<p>quantum level where the systems cannot be seen in the literal sense and must be 
</p>
<p>probed by indirect means. The scattering process gives us information about the 
</p>
<p>projectile, the target, and the forces between them. A natural way to proceed (when 
</p>
<p>possible) is to consider cases where two of these are known and learn about the 
</p>
<p>third. Consider, for example, experiments at the Stanford Linear Accelerator Center 
</p>
<p>in which high-energy photons were used to bombard static neutrons. The structure 
</p>
<p>of the photon and its coupling to matter are well understood-the photon is a point 
</p>
<p>particle to an excellent approximation and couples to electric charge in a way we 
</p>
<p>have studied in some detail. It therefore serves as an excellent probe of the neutron. 
</p>
<p>For instance, the very fact that the neutron, which is electrically neutral, interacts 
</p>
<p>with the photon tells us that the neutron is built out of charged constituents (whose 
</p>
<p>total charge add up to zero). These scattering experiments also revealed that the 
</p>
<p>neutron's constituents have spin ~' and fractional charges a e. -~e), a picture that 
had been arrived at from another independent line of reasoning. Furthermore they 
</p>
<p>also indicated that the interaction between these constituents (called quarks) gets very 
</p>
<p>weak as they get close. This information has allowed us to choose, from innumerable 
</p>
<p>possible models of the interquark force, one that is now considered most likely to 
</p>
<p>succeed, and goes by the name of quantum chromodynamics (QCD), a subject that 
</p>
<p>is being vigorously investigated by many particle physicists today. 
</p>
<p>Scattering theory is a very extensive subject and this chapter aims at giving you 
</p>
<p>just the flavor of the basic ideas. For more information, you must consult books 
</p>
<p>devoted to this subjecd 
A general scattering event is of the form 
</p>
<p>a(a)+b(/3)+ ... -. f(y)+g(o)+ ... 
</p>
<p>where {a, b, .. . } are particle names and {a, /3, y, .. . } are the kinematical variables 
</p>
<p>t See. for example, the excellent book by J. R. Taylor, Scattering Theory, Wiley, New York (1971). Any 
details omitted here due to lack of space may be found there. 523 </p>
<p/>
</div>
<div class="page"><p/>
<p>524 
</p>
<p>CHAPTER 19 
</p>
<p>specifying their states, such as momentum, spin, etc. We are concerned only with 
nonrelativistic, elastic scattering of structureless spinless particles. 
</p>
<p>In the next three sections, we deal with a formalism that describes a single 
particle scattering from a potential V(r). As it stands, the formalism describes a 
particle colliding with an immobile target whose only role is to provide the potential. 
{This picture provides a good approximation to processes where a light particle 
collides with a very heavy one, say an a particle colliding with a heavy nucleus.) In 
Section 19.6 we see how, upon proper interpretation, the same formalism describes 
two-body collisions in the CM frame. In that section we will also see how the 
description of the scattering process in the CM frame can be translated to another 
frame, called the lab frame, where the target is initially at rest. It is important to 
know how to pass from one frame to the other, since theoretical calculations are 
most easily done in the CM frame, whereas most experiments are done in the lab 
frame. 
</p>
<p>19.2. Recapitulation of One-Dimensional Scattering and Overview 
</p>
<p>Although we are concerned here with scattering in three dimensions, we begin 
by recalling one-dimensional scattering, for it shares many common features with 
its three-dimensional counterpart. The practical question one asks is the following: 
If a beam of nearly monoenergetic particles with mean momenta (P) = 1ik0 are 
incident from the far left (x-&gt;- oo) on a potential V(x) which tends to zero as 
lxl-&gt;oo, what fraction Twill get transmitted and what fraction R will get reflected?t 
It is not a priori obvious that the above question can be answered, since the mean 
momentum does not specify the quantum states of the incoming particles. But it 
turns out that if the individual momentum space wave functions are sharply peaked 
at 1ik0 , the reflection and transmission probabilities depend only on k0 and not on 
the detailed shapes of the wave functions. Thus it is possible to calculate R(k0 ) and 
T(k0 ) that apply to every particle in the beam. Let us recall some of the details. 
</p>
<p>(1) We start with some wave packet, say a Gaussian, with (P) = 1ik0 and 
(X)-&gt;-oc. 
</p>
<p>(2) We expand this packet in terms of the eigenfunctions IJ!k of H = T+ V with 
coefficients a(k). The functions IJ!k have the following property: 
</p>
<p>'I' k -----+ A e -ikx + B eikx 
x--oo 
</p>
<p>(19.2.1) 
</p>
<p>In other words, the asymptotic form of IJ!k contains an incident wave A eikx and a 
reflected wave Be-ikx as x-&gt;-oo, and just a transmitted wave Ceikx as X-&gt;00. 
Although the most general solution also contains aD e-ikx piece as x-&gt;oo, we set 
</p>
<p>t In general, the particle can come in from the far right as well. Also V(x) need not tend to zero at both 
ends, but to constants V+ and V~ as x---&gt;&plusmn;oo. We assume V+= V~=O for simplicity. We also assume 
lxV(x)l---&gt;0 as lxl---&gt;oo, so that the particle is asymptotically free (1{1-e&plusmn;"'"'). </p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 19.1. A schematic description of scattering. The incident particles, shown by arrows, are really 
</p>
<p>described by wave packets (only one is shown) with mean momentum (P) = (1iko) and mean impact 
</p>
<p>parameter (p) uniformly distributed in the p-plane out to Pmax&raquo;r0 , the range of the potential. The shaded 
</p>
<p>region near the origin stands for the domain where the potential is effective. The detector catches all 
</p>
<p>particles that emerge in the cone of opening angle dn. The beam is assumed to be coming in along the 
</p>
<p>z axis. 
</p>
<p>D = 0 on physical grounds: the incident wave A eikx can only produce a right-going 
</p>
<p>wave as x-H(). 
(3) We propagate the wave packet in time by attaching to the expansion 
</p>
<p>coefficients a(k) the time dependence e-iErfli, where E= 'li2k2 /2p. We examine the 
</p>
<p>resulting solution as t-+oo and identify the reflected and transmitted packets. From 
</p>
<p>the norms of these we get Rand T respectively. 
</p>
<p>( 4) We find at this stage that if the incident packet is sharply peaked in momen-
</p>
<p>tum space at 'lik0 , R and T depend only on k0 and not on the detailed shape of the 
</p>
<p>wave function. Thus the answer to the question raised at the outset is that a fraction 
</p>
<p>R(k0) of the incident particles will get reflected and a fraction T(ko) will get 
</p>
<p>transmitted. 
(5) Having done all this hard work, we find at the end that the same result 
</p>
<p>could have been obtained by considering just one eigenfunction 'l'ko and taking the 
</p>
<p>ratios of the transmitted and reflected current densities to the incident current density. 
</p>
<p>The scattering problem in three dimensions has many similarities with its one-
</p>
<p>dimensional counterpart and also several differences that inevitably accompany the. 
</p>
<p>increase in dimensionality. First of all, the incident particles (coming out of the 
</p>
<p>accelerator) are characterized, not by just the mean momentum (P) = 'lik0 , but also 
</p>
<p>by the fact that they are uniformly distributed in the impact parameter p, which is 
</p>
<p>the coordinate in the plane perpendicular to ko (Fig. 19.1). The distribution is of 
</p>
<p>course not uniform out to p-+oo, but only up to Pmax&raquo;r0 , where r0 , the range of 
</p>
<p>the potential, is the distance scale beyond which the potential is negligible. [For 
</p>
<p>instance, if V(r)=e-'2102 , the range r 0 ~a.] The problem is to calculate the rate at 
</p>
<p>which particles get scattered into a far away detector that subtends a solid angle dO. 
</p>
<p>in the direction (0, c/J) measured relative to the beam direction (Fig. 19.1). To be 
</p>
<p>525 
</p>
<p>SCATTERING 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>526 
</p>
<p>CHAPTER 19 
</p>
<p>precise, one wants the differential cross section da /dO. defined as follows: 
</p>
<p>da(O, &cent;)dO.= number of particles scattered into dO./sec 
</p>
<p>dO. number incident/sec/area in the p plane 
(19.2.2) 
</p>
<p>The calculation of da I dO. proceeds as followsJ 
</p>
<p>(1) One takes some initial wave packet with mean momentum (P) = 1ik0 and 
mean impact parameter (p). The mean coordinate in the beam direction is not 
relevant, as long as it is far away from the origin. 
</p>
<p>(2) One expands the wave packet in terms of the eigenfunctions l/fk of H = 
T+ V which are of the form 
</p>
<p>(19.2.3) 
</p>
<p>where l/finc is the incident wave eik&middot;r and ll'sc is the scattered wave. One takes only 
those solutions in which ll'sc is purely outgoing. We shall have more to say about 
ll'sc in a moment. 
</p>
<p>(3) One propagates the wave packet by attaching the time-dependence factor 
e-iEr;~ (E= 1i2e /2J.L) to each coefficient a(k) in the expansion. 
</p>
<p>(4) One identifies the scattered wave as t--HfJ, and calculates the probability 
current density associated with it. One integrates the total flow of probability into 
the cone dO. at (0, &cent;).This gives the probability that the incident particle goes into 
the detector at((),&cent;). One finds that if the momentum space wave function of the 
incident wave packet is sharply peaked at (P) = 1ik0 , the probability of going into 
dO. depends only on 1ik0 and (p). Call this probability P(p, ko--+dO.). 
</p>
<p>(5) One considers next a beam of particle with 1J(p) particles per second per 
unit area in the p plane. The number scattering into dO. per second is 
</p>
<p>(19.2.4) 
</p>
<p>Since in the experiment 1J(p) = 1], a constant, we have from Eq. (192.2) 
</p>
<p>da 1](d0.) f 2 
dO. d0.=-
</p>
<p>11
-= P(p, k 0 --+d0.) d p (19.2.5) 
</p>
<p>(6) After all this work is done one finds that da/dO. could have been calculated 
from considering just the static solution l/fko and computing in the limit r--+ oo, the 
ratio of the probability flow per second into dn associated with l{fsc, to the incident 
probability current density associated with eiko&middot;r. The reason the time-dependent 
picture reduces to the time-independent picture is the same as in one dimension: as 
we broaden the incident wave packet more and more in coordinate space, the incident 
and scattered waves begin to coexist in a steady-state configuration, l/fko&middot; What about 
</p>
<p>t We do not consider the details here, for they are quite similar to the one-dimensional case. The few 
differences alone are discussed. See Taylor's book for the details. </p>
<p/>
</div>
<div class="page"><p/>
<p>the average over (p)? This is no longer necessary, since the incident packet is now 
</p>
<p>a plane wave iko&middot;&bull; which is already uniform in p.t 
Let us consider some details of extracting du I dQ from lflko. Choosing the z axis 
</p>
<p>parallel to k0 and dropping the subscript 0, we obtain 
</p>
<p>(19.2.6) 
</p>
<p>where ()and 4&gt; are defined in Fig. 19.1. Although the detailed form of !fisc depends 
</p>
<p>on the potential, we know that far from the origin it satisfies the free-particle equation 
</p>
<p>[assuming rV(r)--+0 as r--+oo]. 
</p>
<p>(19.2. 7) 
</p>
<p>and is purely outgoing. 
</p>
<p>Recalling the general solution to the free-particle equation (in a region that 
</p>
<p>excludes the origin) we get 
</p>
<p>!fisc--;:-;: I I (A[j,(kr) + B,n,(kr)) Y/( (), 4&gt;) (19.2.8) 
I m 
</p>
<p>Notice that we do not exclude the Neumann functions because they are perfectly 
</p>
<p>well behaved as r--+ oo. Since 
</p>
<p>j 1(kr)--;:-;: sin(kr-lnl2)1(kr) 
</p>
<p>n1(kr) -;:-;:-cos(kr-lnl2)1(kr) 
(19.2.9) 
</p>
<p>it must be that A1 I B1= -i, so that we get a purely outgoing wave ikr lkr. With this 
condition, the asymptotic form of the scattered wave is 
</p>
<p>(19.2.10) 
</p>
<p>or 
</p>
<p>eikr 
</p>
<p>!fisc----+ -j((), {j&gt;)&sect; 
r-'X) r 
</p>
<p>(19.2.11) 
</p>
<p>and 
</p>
<p>(19.2.12) 
</p>
<p>where f is called the scattering amplitude. 
</p>
<p>t Let us note, as we did in one dimension, that a wave packet does not simply become a plane wave as 
we broaden it, for the former has norm unity and the latter has norm 83(0). So it is assumed that as 
</p>
<p>the packet is broadened, its norm is steadily increased in such a way that we end up with a plane wave. 
</p>
<p>In any case, the overall norm has no significance. 
</p>
<p>&sect;Actually f also depends on k; this dependence is not shown explicitly. 
</p>
<p>527 
</p>
<p>SCATIERING 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>528 
</p>
<p>CHAPTER 19 
</p>
<p>To get the differential cross section, we need the ratio of the probability flowing 
into dO per second to the incident current density. So what are isc and iinc. the 
incident and scattered current densities? Though we have repeatedly spoken of these 
quantities, they are not well defined unless we invoke further physical ideas. This is 
because there is only one current density j associated with lf/k and it is quadratic in 
lf/k. So j is not just a sum of two pieces, one due to etkz and one due to lfl sc ; there 
are cross terms.t We get around this problem as follows. We note that as r-&gt;oo, lf/sc 
is negligible compared to eik= because of the 1/r factor. So we calculate the incident 
current due to etkz to be 
</p>
<p>I . I -IIi ( -tk=v tkz tk=v -tk=)l } . -- e e -e e 
me 2j..li 
</p>
<p>fzk 
</p>
<p>J..l 
(19.2.13) 
</p>
<p>We cannot use this trick to calculate isc into dO. because lf/sc never dominates over 
etkz. So we use another trick. We say that eikz is really an abstraction for a wave 
that is limited in the transverse direction by some Pmax( &raquo; r0 ). Thus in any realistic 
description, only lf/sc will survive as r-&gt;oo for 0#0.&sect; (For a given Pmax. the incident 
wave is present only for 8 0 ~ Pmax/ r. We can make 8 0 arbitrarily small by increasing 
the rat which the detector is located.) With this in mind we calculate (for 0 #0) 
</p>
<p>&bull; - 1i ( *V V *) ]sc - 2------: lfl sc lfl sc - lfl sc lfl sc 
J..ll 
</p>
<p>(19.2.14) 
</p>
<p>Now 
</p>
<p>a a 1 a 
V=e -+eo--+e.p---
</p>
<p>r or r ae r sin (} o4J 
(19.2.15) 
</p>
<p>The last two pieces in V are irrelevant as r-&gt;oo. When the first acts on the asymptotic 
</p>
<p>lf/sc' 
</p>
<p>a eikr eikr ( 1 ) 
-f(0,4J)-=j((),4J)ik-+O 2 
or r r r 
</p>
<p>so that 
</p>
<p>(19.2.16) 
</p>
<p>t We did not have to worry about this in one dimension because j due to A e'kx +Be -lkx is 
(1ik/p)(IAI 2 -IBI 2 )=};nc+J~rwith no cross terms. 
</p>
<p>&sect;In fact, only in this more realistic picture is it sensible to say that the particles entering the detectors at 
0#0 are scattered (and not unscattered incident) particles. At 0=0, there is no way (operationally) to 
separate the incident and scattered particles. To compare theory with experiment, one extracts/( 0 = 0) 
by extrapolating/(0) from 0#0. </p>
<p/>
</div>
<div class="page"><p/>
<p>Probability flows into dO. at the rate 
</p>
<p>Since it arrives at the rate 
</p>
<p>so that finally 
</p>
<p>R(dO.) =isc &middot; e,r2d0. 
</p>
<p>2 1ik 
=i/1 -dO. 
</p>
<p>J.l. 
</p>
<p>(19.2.17) 
</p>
<p>(19.2.18) 
</p>
<p>Thus, in the time-independent picture, the calculation of da I dO. reduces to the 
calculation off((), 4&gt; ) . 
</p>
<p>After this general discussion, we turn to specific calculations. In the next section 
</p>
<p>the calculation of da I dO. is carried out in the time-dependent picture to first order. 
In Section 4, we calculate da I dO. to first order in the time-independent picture. (The 
two results agree, of course.) In Section 5, we go beyond perturbation theory and 
</p>
<p>discuss some general features off for spherically symmetric potentials. Two-particle 
</p>
<p>scattering is discussed in Section 6. 
</p>
<p>19.3. The Born Approximation (Time-Dependent Description) 
</p>
<p>Consider an initial wave packet that is so broad that it can be approximated 
</p>
<p>by a plane wave IP; ). Its fate after scattering is determined by the propagator 
U(tr XJ, t;-+- oo ), that is, by the operator 
</p>
<p>S= lim U(tf&gt; t;) 
lj-+ctJ 
</p>
<p>lj-+- 0C) 
</p>
<p>which is called the S matrix. The probability of the particle entering the detector in 
</p>
<p>the direction ( (), 4&gt;) with opening angle dO. is the probability that the final momentum 
</p>
<p>Pf lies in a cone of opening angle dO. in the direction ( (), 4&gt;): 
</p>
<p>P(p;-+dO.)= L I&lt;Pf1Sip;)l 2 
Ptind(l 
</p>
<p>529 
</p>
<p>SCATTERING 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>530 
</p>
<p>CHAPTER 19 
</p>
<p>If we evaluateS or U to first order, treating Vas a perturbation, the problem reduces 
</p>
<p>to the use of Fermi's G-olden Rule, which tells us that the transition rate is 
</p>
<p>-,[r&bull;cc, (2 2~ l _ ""n , . 2 ~ Pr P r 1 
-&bull;-------- J I\PJivlp,)l a( ;-7 PidPr df"l n &bull; o \ ~J.l ~J.l, &middot;-&middot; 
</p>
<p>(19.3.1) 
</p>
<p>(19.3.2) 
</p>
<p>(Hereafter PJ= p 1 = p = nk is understood.) This transition rate is just the rate of the 
flow of probability into dQ. Since the probability comes in at a rate 
[recall j = pv 1Pr)-&middot;&bull;(2nn) 312 e'Prr:A] 
</p>
<p>(19.3.3) 
</p>
<p>in the direction p;, the differential cross section, which measures the rate at which 
</p>
<p>probability is intercepted (and channeled off to dQ ), is 
</p>
<p>(19.3.4) 
</p>
<p>where 
</p>
<p>1/.q=p;-p, (19.3.5) 
</p>
<p>is the momentum transferred to the particle. For later reference note that 
</p>
<p>( 19.3.6) 
</p>
<p>Thus the dependence of da / dn on the incident energy and the scattering angle is 
</p>
<p>through the combination lql =q=2k sin(B/2). 
By comparing Eqs. (19.3.4) and (19.2.18) we can getf(O), up to a phase factor 
</p>
<p>of unit modulus (relative to the incident wave). We shall see later that this factor is 
</p>
<p>-1. So, 
</p>
<p>(19.3.7) 
</p>
<p>Thus, in this Born approximation, f( (), &cent;) =f( q) is just the Fourier transform of the 
potential with respect to momentum transfer (up to a constant factor). </p>
<p/>
</div>
<div class="page"><p/>
<p>Hereafter we focus on potentials that are spherically symmetric: V(r) = V(r). 
</p>
<p>In this case, we can choose the z' direction parallel to q in the d 3r' integration, so 
</p>
<p>that 
</p>
<p>/( 8, 4J) = 2 ~~ 2 I e-iqr'cosO'V(r') d(cos ()') d4J 'r'2 dr' 
_ - 2J1. I sin qr' V( ') , d , --- --- r r r te q 
=/(8) (19.3.8) 
</p>
<p>That f should be independent of 4J in this case could have been anticipated. The 
incident wave eik= is insensitive to a change in 4J, i.e., to rotations around the z axis. 
</p>
<p>The potential, being spherically symmetric, also knows nothing about 4J. It follows 
</p>
<p>that/ cannot pick up any dependence on 4J. In the language of angular momentum, 
</p>
<p>the incident wave has no I= and this feature is preserved in the scattering. Conse-
</p>
<p>quently the scattered wave must also have no/=, i.e., be independent of 4J. 
</p>
<p>Let us calculate/( 8) for the Yukawa potential 
</p>
<p>(19.3.9) 
</p>
<p>From Eq. (19.3.8), 
</p>
<p>(19.3.10) 
</p>
<p>da 4J1. 2i 
dO. 1i 4 [J1.~+4k 2 sin2(8/2)f 
</p>
<p>(19.3.11) 
</p>
<p>If we now set g=Ze2, J.l.o=O, we get the cross section for Coulomb scattering of a 
</p>
<p>particle of chargee on a potential4J=Ze/r (or V=Ze2/r): 
</p>
<p>da I J1.2(ze2)2 
dQ Coulomb= 4p4 sin4 ( () /2) 
</p>
<p>(Ze2)2 
(19.3.12) 
</p>
<p>where E = p 2 /2J1. is the kinetic energy of the incident particle. This answer happens 
</p>
<p>to be exact quantum mechanically as well as classically. (It was calculated classically 
</p>
<p>by Rutherford and is called the Rutherford cross section.) Although we managed to 
</p>
<p>get the right da /dO. by taking the J.l.o--+0 limit of the Yukawa potential calculation, 
</p>
<p>531 
</p>
<p>SCAITERING 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>532 
</p>
<p>CHAPTER 19 
</p>
<p>there are some fine points to note. First of all, the Coulomb potential cannot be 
handled by the formulation we have developed, since the potential does not vanish 
faster than r- 1&bull; In other words, the asymptotic form 
</p>
<p>is not applicable here since the particle is never free from the influence of the potential. 
(This manifests itself in the fact that the total cross section is infinite: if we try to 
integrate dajdO. over (}, the integral diverges ass d(}j(}3 as e .... o.) It is, however, 
possible to define a scattering amplitude fc( (}) in the following sense. One finds that 
as r-&gt;oo, there are positive energy eigensolutions to the Coulomb Hamiltonian of 
the formt 
</p>
<p>If/--&gt; eik= + f,.( (}) :____ 
_ ( ikr) 
</p>
<p>r--+X f 
(19.3.13) 
</p>
<p>where the tilde tells us that these are not actually plane or spherical waves, but rather 
these objects modified by the long-range Coulomb force. For example 
</p>
<p>eikr ei(kr- ylnkr) 
</p>
<p>( 19.3.14) 
r r 
</p>
<p>(19.3.15) 
</p>
<p>is the distorted spherical wave, familiar to us from Section 12.6. By comparing the 
ratio of flux into dO. to flux coming in (due to these distorted waves) one finds that 
</p>
<p>where 
</p>
<p>fc( 8) = - . r 2 exp(- iy In sin2 (} /2 + const) 
2k(sm 0/2) 
</p>
<p>(19.3.16) 
</p>
<p>and where the constant is purely imaginary. Comparing this to the Yukawa ampli-
tude, Eq. (19.3.10), after setting llo=O, g= Ze2 , we find agreement up to the exponen-
tial phase factor. This difference does not show up in da /dO., but will show up when 
we consider identical-particle scattering later in this chapter. 
</p>
<p>t See A. Messiah, Quantum Mechanics, Wiley, New York (1966}, page 422. </p>
<p/>
</div>
<div class="page"><p/>
<p>Exercise 19.3.1. * Show that 
</p>
<p>2(gJ.lro)2 
O"vukawa = 16:rrro -.;--2 ' 2 
</p>
<p>, I + 4k'ro 
</p>
<p>where r0 = lp, 0 is the range. Compare a to the geometrical cross section associated with this 
</p>
<p>range. 
</p>
<p>Exercise 19.3.2* (1) Show that if V(r) = &middot;&middot;- V,iO(ro &middot;&middot;&middot;&middot; r), 
</p>
<p>(2) Show that as kro--"0, the scattering becomes isotropic and 
</p>
<p>Exercise 19.3.3. * Show that for the Gaussian potential, V(r) = V0 e-''/'0, 
</p>
<p>[Hint: Since l=2e(l-cos 8), d(cos 0)= -d(l)/2e.] 
</p>
<p>Let us end this section by examining some general properties off (e). We see 
from Eq. (19.3.7) that at low energies (k&middot;&middot;&middot;&middot;&gt;O), q=2k sin (0/2)-&gt;0 and 
</p>
<p>/(0)---JL.- ~ V(r')d3r' 
2rdi2 J 
J.1 Vor~ ,......., __________ _ 
</p>
<p>- ti.2 (19.3.17) 
</p>
<p>where Vo is some effective height of V, and r0 is some effective range. At high energies, 
the exponential factor e-iqr'coso&middot; oscillates rapidly. This means that the scattered waves 
</p>
<p>coming from different points r' add with essentially random phases, except in the 
small range where the phase is stationary: 
</p>
<p>qr' cos 0' ::S rr 
</p>
<p>2k sin( 0 /2)ro ::S 1[ (since r' cos ()' ~ ro) 
</p>
<p>k0r0 ;:Srr (sin 0/2':'::.0/2) 
</p>
<p>533 
</p>
<p>SCATTERING 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>534 
</p>
<p>CHAPTER 19 
</p>
<p>Thus the scattering amplitude is appreciable only in a small forward cone of angle 
(dropping constants of order unity) 
</p>
<p>1 
f)&lt; --
</p>
<p>~kro 
(19.3.18) 
</p>
<p>These arguments assume V( r') is regular ncar r' = 0. But in some singular cases 
</p>
<p>[ V oc (r')- 3, say] the r' integral is dominated by small r' and kr' cos f)' is not necessarily 
</p>
<p>a large phase. Both the Yukawa and Gaussian potential (Exercise 19.3.3) are free 
</p>
<p>of such pathologies and exhibit this forward peak at high energies. 
</p>
<p>Exercise 19J4_ Verify the above claim for tht~ Gaussian potentiaL 
</p>
<p>When can we trust the Born approximation? Since we treated the potential as 
</p>
<p>a perturbation, our guess would be that it is reliable at high energies. We shall see 
</p>
<p>in the next section that this is indeed correct, but that the Born approximation can 
</p>
<p>also work at low energies provided a more stringent condition is satisfied. 
</p>
<p>19.4. Born Again (The Time-Independent Description) 
</p>
<p>In this approach, the central problem is to find solutions to the full Schr()dinger 
</p>
<p>equation 
</p>
<p>V l k7) 2jl T/ ( + - 1/fk=-~- ''"k . ,. f( r (19.4.1) 
</p>
<p>of the form 
</p>
<p>( 19.4.2a) 
</p>
<p>where 
</p>
<p>( 19.4.2b) 
</p>
<p>In the above, (}and&cent; arc measured relative to k, chosen along the::: axis (Fig. 19.1 ). 
One approaches the problem as follows. One finds a Green's function G0(r, r') which 
</p>
<p>satisfies 
</p>
<p>(19.4.3) </p>
<p/>
</div>
<div class="page"><p/>
<p>in terms of which theformal general solution to Eq. (19.4.1) is 
</p>
<p>(19.4.4) 
</p>
<p>where &bull;l(r) is an arbitrary free-particle solution of energy f'?e 121-1: 
</p>
<p>(19.4.5) 
</p>
<p>We will soon nail down 1J1&deg; using the boundary conditions. 
</p>
<p>Applying V2 + k 2 to both sides of Eq. (19.4.4) one may easily verify that lJ!k 
indeed is a solution to Eq. (19.4.1 ). The idea here is quite similar to what is employed 
</p>
<p>in solving Poisson's equation for the electrostatic potential in terms of the charge 
</p>
<p>density p: 
</p>
<p>One first finds G, the response to a point charge at r': 
</p>
<p>Exercise 12.6.4 tells us that 
</p>
<p>G(r, r') = G(r----r') =-1-
jr-r'l 
</p>
<p>One then views pas a superposition of point charges and, since Poisson's equation 
</p>
<p>is linear, obtains cp as the sum of cp's produced by these charges: 
</p>
<p>&cent; (r) = f G(r- r')p(r') dV = j p(r? d3r' J jr-r I 
</p>
<p>(By acting on both sides with V2 and using V2G= -4n83 , you may verify that &cent; 
satisfies Poisson's equation.) 
</p>
<p>One can add to this &cent;(r) any &cent;0 that satisfies V2&cent;0 =0. Using the boundary 
condition cp = 0 when p = 0, we get rid of &cent;0 &bull; 
</p>
<p>In the scattering problem we pretend that the right-hand side of Eq. ( 19 .4.1) is 
some given source and write Eq. (19.4.4) for IJ!k in terms of the Green's function. 
</p>
<p>The only catch is that the source for V/k is lJ!k itself. Thus Eq. (19.4.4) is really not 
a solution, but an integral equation for lJ!k. The motivation for converting the 
</p>
<p>differential equation to an integral equation is similar to that in the case of U1( t, t0): 
</p>
<p>to obtain a perturbative expansion for IJ!k in powers of V. To zeroth order in V, Eq. 
(19.4.2a) tells us that lJ!k is eik&middot;r, since there is no scattered wave if Vis neglected; 
</p>
<p>whereas Eq. (19.4.4) tells us that lJ!k = 1J1&deg;, since the integral over r' has an explicit 
power of V in it while 1J1&deg; has no dependence on V [since it is the solution to Eq. 
</p>
<p>(19.4.5)]. We are thus able to nail down the arbitrary function IJI0 in Eq. (19.4.4): 
</p>
<p>(19.4.6) 
</p>
<p>535 
</p>
<p>SCATTERING 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>536 
</p>
<p>CHAPTER 19 
</p>
<p>and conclude that in the present scattering problem 
</p>
<p>I r' 
ik&middot;r + -J.l J r' 0( ') u( ') . ( ') j3 ' V'k = e -~;,-&middot; u r, r r, r lf/k r c r (19.4.7) 
</p>
<p>Upon comparing this to Eq. ( 19.4.2a) we see that we are associating the second piece 
</p>
<p>with the scattered wave. For consistency of interpretation, it must contain purely 
</p>
<p>outgoing waves at spatial infinity. Since G 0(r, r') is the scattered wave produced by 
</p>
<p>a point source at r', it is necessary that G0(r, r') be purely outgoing asymptotically. 
</p>
<p>This is an additional physical constraint on G0 over and above Eq. (19.4.3). As we 
</p>
<p>shall see, this constraint, together with Eq. ( 19.4.3), will determine G0 for us uniquely. 
</p>
<p>Imagine that we have found this G 0 . We are now in a position to obtain a 
</p>
<p>perturbative solution for lJik starting with Eq. ( 19.4. 7). To zeroth order we have seen 
</p>
<p>that lJik = e1k". To go to first order, we feed the zeroth-order V'k into the right-hand 
</p>
<p>side and obtain 
</p>
<p>(19.4.8) 
</p>
<p>If we feed this first-order result back into the right-hand side of Eq. ( 19.4. 7), we get 
</p>
<p>(in symbolic form) the result good to second order: 
</p>
<p>and so on. 
Let us now turn to the determination of G0 , starting with Eq. ( 19.4.3): 
</p>
<p>We note that this equation does not have a unique solution, since, given any solution, 
</p>
<p>we can get another by adding to it a function 1)0 that obeys the homogeneous 
</p>
<p>equation 
</p>
<p>Conversely, any two columns G 0 and G 0' can differ only by some T)u So we will first 
</p>
<p>find the simplest G0 we can, and then add whatever 1)0 it takes to make the sum 
</p>
<p>purely outgoing. 
Since ('\72 + k 2 ) and 8\r &middot;&middot;&middot;&middot; r') are invariant under the overall translation ofr and 
</p>
<p>r', we know the equation admits translationally invariant solutionsi: 
</p>
<p>t Note that if an equation has some symmetry, like rotational invariance. it means only that rotationally 
invariant solutions exist, and not that all solutions are rotationally invariant. For example, the hydrogen 
</p>
<p>atom Hamiltonian is rotationally invariant, but the eigenfunctions are not in generaL But there are 
</p>
<p>some (with l=m=O) which are. </p>
<p/>
</div>
<div class="page"><p/>
<p>Replace r-r' by r for convenience. [Once we find G0(r), we can replacer by r-r'.] 
</p>
<p>So we want to solve 
</p>
<p>(19.4.9) 
</p>
<p>For similar reasons as above, we look for a rotationally invariant solution 
</p>
<p>Writing 
</p>
<p>, 0 U(r) 
G (r) =&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot; 
</p>
<p>r 
</p>
<p>we find that for r i= 0, U(r) satisfies 
</p>
<p>the general solution to which is 
</p>
<p>U(r) =A eikr + B e--ikr 
</p>
<p>or 
</p>
<p>0 A eikr B e&middot;--ikr 
G (r)=--+-- (19.4.10) 
</p>
<p>r r 
</p>
<p>where A and B are arbitrary constants at this point. Since we want G0 to be purely 
outgoing we set B = 0: 
</p>
<p>(19.4.11) 
</p>
<p>(19.4.12) 
</p>
<p>a; or on a function of r alone. 
</p>
<p>537 
</p>
<p>SCATTERING 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>538 
</p>
<p>CHAPTER 19 
</p>
<p>which gives us 
</p>
<p>4Jrr 
( 19.4.13) 
</p>
<p>We cannot add any 17&deg; to this solution, without destroying its purely outgoing 
</p>
<p>nature, since the general form of the free-particle solution, regular in all space, is 
</p>
<p>c I 
</p>
<p>1) 0(r) = I I Clm}l(kr) Y/"(0, &cent;) ( 19.4.14) 
l=O nr= ---1 
</p>
<p>and since, as r-&gt; ::o, the spherical Bessel functions are made up of incoming and 
</p>
<p>outgoing waves of equal amplitude 
</p>
<p>Let us now feed 
</p>
<p>sin(kr -&middot;lJr 2) 
.if (kr) ----.. ------&bull; ----'--------------'-
</p>
<p>kr 
</p>
<p>into Eq. ( 19.4. 7) to obtain 
</p>
<p>-/Jr.&middot; 2) &middot;---- C -i(kr- Ire&middot; 2) 
</p>
<p>2ikr 
</p>
<p>Let us now verify that as r----&gt;-x, VIce has the desired formf(IJ. &cent;) 
</p>
<p>instinct may be to approximate as follows: 
</p>
<p>------------------~----
</p>
<p>lr-r'l r 
</p>
<p>(19.4.15) 
</p>
<p>(19.4.16) 
</p>
<p>(19.4.17) 
</p>
<p>1 r. Our first 
</p>
<p>in the r' integral since r&middot; is confined to I r'l ::;:: r0 (the range), whereas r---+ ct~. That this 
</p>
<p>is wrong is clear from the fact that if we do so, the corresponding f has no 0 or ljJ 
dependence. Let us be more careful. We first approximate </p>
<p/>
</div>
<div class="page"><p/>
<p>= r( 1-2 r~n 112 + o[ (~r} 
</p>
<p>~r(1- r~n (19.4.18) 
</p>
<p>We have thrown away the term quadratic in (r'/r) and used the approximation 
(1 + x)" ~ 1 + nx for small x. So 
</p>
<p>(19.4.19) 
</p>
<p>Whereas replacing lr-r'l- 1 in the integral leads to errors which vanish as r-+oo, this 
is not so for the factor eiklr-r'l. We have 
</p>
<p>klr-r'l =kr( 1- r~n 
</p>
<p>=kr-kf&middot; r' 
</p>
<p>=kr-kr r' (19.4.20) 
</p>
<p>where k1 is the wave vector of the detected particle: it has the same magnitude (k) 
as the incident particle and points in the direction (r) of observation (Fig. 19.2). 
Consequently 
</p>
<p>iklr-r'l ikr 
</p>
<p>e--~~e-ikrr' 
</p>
<p>lr-r'l r 
(19.4.21) 
</p>
<p>and 
</p>
<p>(19.4.22) 
</p>
<p>Thus the solution we have found has the desired form as r-+oo. Equation (19.4.22) 
of course does not determine f( (}, &cent;&gt; ) since 'l'k is present in the r' integration. 
</p>
<p>Figure 19.2. The particle is observed at the point r. The r' integration 
is restricted to the shaded region which symbolizes the range of the 
potential. 
</p>
<p>539 
</p>
<p>SCATTERING 
THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>540 
</p>
<p>CHAPTER 19 
</p>
<p>However, to any desired order this lf/~r. can be replaced by the calculahle lower-order 
approximation. In particular, to first order, 
</p>
<p>f((), &cent;) = ---- 4~~2 f e-ik;-r V(r') e'"krr' dV (19.4.23) 
</p>
<p>where we have added a subscript ito k to remind us that it is the initial or incident 
</p>
<p>wave vector. We recognize _f( 8, &cent;) to be _ju~t the Born approximation calculated in 
</p>
<p>the last section [Eq. (19.3.7)]. The phase factor -1, relative to the incident wave 
</p>
<p>was simply assumed there. The agreement between the time-dependent and time-
</p>
<p>independent calculations off persists to all orders in the perturbation expansion. 
There is another way (involving Cauchy's theorem) to solve 
</p>
<p>(19.4.24) 
</p>
<p>Fourier transforming both sides, we get 
</p>
<p>; 'J,-'2 rt. - I 1 \ 3,'2 
</p>
<p>( 1 ) j ---,w&bull;(~:e 7 2 k2)(-.o ) dJ ( ) - e v + :r (r r= --
2~r I 2Jr 
</p>
<p>(19.4.25) 
</p>
<p>If we let V2 act to the left (remember it is Hermitian) we get 
</p>
<p>' \3.'2 f ' \3!2 
2 2 ' 1 -,q&middot;r 0 - 3 J 
</p>
<p>(k-q)(--) e G(r)dr=(---------) 
\2lr; &bull; 21r. 
</p>
<p>(19.4.26) 
</p>
<p>1 \ 3.-'2 
</p>
<p>2 o. o (1 I ) (k -q~)G (q)= -
21r/ 
</p>
<p>(19.4.27) 
</p>
<p>As always, going to momentum space has reduced the differential equation to an 
</p>
<p>algebraic equation. The solution is 
</p>
<p>(19.4.28) 
</p>
<p>except at the point q = k where G 0 ( q) diverges. The reason for this divergence is the 
following. Equation (19 .4.24) is the coordinate space version of the abstract equation 
</p>
<p>(19.4.29) 
</p>
<p>where 
</p>
<p>( 1 9.4.30) </p>
<p/>
</div>
<div class="page"><p/>
<p>(D, is just the x derivative operator D introduced in Section 1.10, and Dy and Do 
</p>
<p>are y and z derivative operators.) Thus G 0 is the inverse of (D 2 + k 2 ): 
</p>
<p>(19.4.31) 
</p>
<p>Now, we know that we cannot invert an operator that has a vanishing determinant 
</p>
<p>or equivalently (for a Hermitian operator, since it can be diagonalized) a zero eigen-
</p>
<p>value. The operator (D 2 + k2) has a zero eigenvalue since 
</p>
<p>(19.4.32) 
</p>
<p>has nontrivial (plane wave) solutions. We therefore consider a slightly different 
</p>
<p>operator, D 2 + e +is, where s is positive and infinitesimal. This too has a zero 
eigenvalue, but the corresponding eigenfunctions are plane waves of complex wave 
</p>
<p>number. Such functions are not part of the space we are restricting ourselves to, 
</p>
<p>namely, the space of functions normalized to unity or the Dirac delta function.t 
</p>
<p>Thus D 2 + e + ie may be inverted within the physical Hilbert space. Let us call the 
corresponding Green's function G~. At the end of the calculation we will send s to 
zero.&sect; 
</p>
<p>Clearly 
</p>
<p>(19.4.33) 
</p>
<p>The coordinate space function is given by the inverse transform: 
</p>
<p>(19.4.34) 
</p>
<p>We choose the qz axis parallel to r. If () and rjJ are the angles in q space, 
</p>
<p>(19.4.35a) 
</p>
<p>( 19.4.35b) 
</p>
<p>(19.4.35c) 
</p>
<p>t Recall from Section 1.10 that if k is complex, the norm diverges exponentially. 
&sect;This is called the "ie prescription." Throughout the analysis s will be considered only to first order. 
</p>
<p>541 
</p>
<p>SCATTERING 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>542 
</p>
<p>CHAPTER l9 
</p>
<p>[In going from ( 19.4.35a) to (19.4.35b) above, we changed q to - q in the piece.] 
</p>
<p>We proceed to evaluate the above integral by means of Cauchy's residue 
</p>
<p>theorem, which states that for any analytic function f(z) of a complex variable z, 
</p>
<p>( 19.4.36) 
</p>
<p>where ~ denotes integration around a closed contour in the complex z plane and 
</p>
<p>R(z1 ) is the residue of the pole at the point zi lying inside the contour.l 
</p>
<p>Let us view q as a complex variable which happens to be taking only real values 
</p>
<p>(-x to +a::;) in Eq. (19.4.35). 
We are trying to evaluate the integral of the function 
</p>
<p>(19.4.37) 
</p>
<p>along the real axis from - cY..; to + (Y..;. 
This function has poles where 
</p>
<p>k 2 + iB-l = 0 
</p>
<p>or (to first order in 10), 
</p>
<p>(k + q + iry)(k- q + iry) = 0 ( r7 ~ c/2k) (19.4.38) 
</p>
<p>These poles are shown in Fig. 1 Y.3. 
</p>
<p>We are not yet ready to use Cauchy's theorem because we do not have a closed 
</p>
<p>contour. Let us now close the contour via a large semicircle CP whose radius p-+ ex. 
Now we can use Cauchy's theorem, but haven't we changed the quantity we wanted 
</p>
<p>to calculate? No, because CP does not contribute to the integral as p-+ x. To see 
</p>
<p>this, let us write q = p on CP. Then 
</p>
<p>and 
</p>
<p>~ Recall that if 
</p>
<p>then 
</p>
<p>w ( q) ------------&bull; (cons t) 
p-x- q 
</p>
<p>, Ri.~,.l 
</p>
<p>,-'(.:)---::; ----&middot;~-- - -! 
</p>
<p>R(z;) o limf(zJ(z- ::1) 
.---.... :,. 
</p>
<p>(19.4.39) </p>
<p/>
</div>
<div class="page"><p/>
<p>Im q 
</p>
<p>~----------.--------~~~--------~------~---Req 
-k-1"1 
</p>
<p>Figure 19.3. The poles of the function w(q) in the complex q plane. We want the integral along the real 
</p>
<p>axis from -oo to +oo. We add to it the contribution along Cp (which vanishes asp tends to oo) in order 
</p>
<p>to close the contour of integration and to use Cauchy's theorem. 
</p>
<p>f w(q) dq~ f eiqr dq = f" eipr(cosO+isiniJ)i d(} 
Cp Cp q 0 
</p>
<p>(19.4.40) 
</p>
<p>Except for the tiny region near () = 0 (which contributes only an infinitesimal amount) 
</p>
<p>the integral vanishes since e-prsiniJ-+0 as p-+oo. We now use Cauchy's theorem. The 
</p>
<p>only pole enclosed is at q=k+i1J. The residue here is 
</p>
<p>R(k+i17) = lim (q-k- i1J)w(q)=~ ei&lt;k+i~)r 
q-k+&bull;~ 81r r 
</p>
<p>(19.4.41) 
</p>
<p>and 
</p>
<p>eikr 
</p>
<p>G0(r) =lim 2niR = ----
~-o 4nr 
</p>
<p>(19.4.42) 
</p>
<p>Notice that although the i&amp; ( &amp; &gt; 0) prescription happens to give the right answer 
</p>
<p>here, there are other ways to evaluate the integral, which may be appropriate in 
</p>
<p>other contexts. For example if we choose &amp;&lt;0, we get a purely incoming wave, since 
</p>
<p>11 changes sign and the pole near q ~ - k gets into the contour. 
</p>
<p>Validity of the Born Approximation 
</p>
<p>Since in the Born approximation one replaces 'l'k = eit-.' + 'I' sc by just eik-r' in the 
right-hand side of the integral Eq. (19.4.17), it is a good approximation only if 
</p>
<p>I 'l'scl &laquo;leik&middot;r'l in the region lr'l ::Sro. Since we expect 'l'sc to be largest near the origin, 
</p>
<p>543 
</p>
<p>SCATTERING 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>544 
</p>
<p>CHAPTER 19 
</p>
<p>let us perform a comparison there, using Eq. (I9.4.I7) itself to evaluate 'l'sc(O): 
</p>
<p>(I9.4.43) 
</p>
<p>Let us assume V(r) = V(r). In this case a rough criterion for the validity of the Born 
approximation is 
</p>
<p>~~ I f eikr' sin kr' V(r') dr'l &laquo;I (19.4.44) 
Exercise 19.4.1. Derive the inequality (19.4.44). 
</p>
<p>At low energies, kr' -+0, eikr'-+ I, sin kr' -+kr' and we get the condition 
</p>
<p>~If r'V(r') dr'l&laquo; 1 (19.4.45) 
</p>
<p>If V(r) has an effective depth (or height) V0 and rang'~ r0 , the condition becomes 
(dropping constants of order unity) 
</p>
<p>(19.4.46) 
</p>
<p>The low energy condition may be written as 
</p>
<p>Now a particle confined to a well of dimension r0 must have a momentum of order 
11jr0 and a kinetic energy of order 112/ J.Lr6. The above inequality says that if the Born 
approximation is to work at low energies, the potential must be too shallow to bind 
a particle confined to a region of size ro. 
</p>
<p>At high energies when kr0 &raquo; I let us write inside the integral in Eq. (19.4.44) 
</p>
<p>' ' e2ikr' -1 
e'kr sin kr' = ---
</p>
<p>2i 
</p>
<p>and drop the exponential which will be oscillating too rapidly within the range of 
the potential and keep just the -1 part to get the following condition: 
</p>
<p>~k I J V(r') dr'l &laquo;I (19.4.47) </p>
<p/>
</div>
<div class="page"><p/>
<p>which could be rewritten as 
</p>
<p>J1 Vor~ 
--2 - &laquo;kro 
</p>
<p>1i 
(19.4.48) 
</p>
<p>We found that the Born approximation can be good even at low energies if the 
</p>
<p>inequality (19.4.46) is satisfied. In fact, if it is, the Born approximation is good at 
</p>
<p>all energies, i.e., Eq. (19.4.48) is automatically satisfied. 
</p>
<p>19.5. The Partial Wave Expansion 
</p>
<p>We have noted that if V(r)= V(r),f(O, &lt;/&gt;)=f(O). Actually fis also a function 
</p>
<p>of the energy E = 1i2e /211, though this dependence was never displayed explicitly. 
Since any function of e can be expanded in terms of the Legendre polynomials 
</p>
<p>I \ ] /2 
</p>
<p>P1(cos 0)=(~) Y? 
2!+ 1 
</p>
<p>we can expandf(O, k) in terms of P 1(cos 8) with k-dependent coefficients: 
</p>
<p>f((J,k)=I: (2l+l)at(k)Pt(cos8) 
t~o 
</p>
<p>(19.5.1) 
</p>
<p>( 19.5.2) 
</p>
<p>One calls a1(k) the lth partial wave amplitude. It has the following significance. The 
</p>
<p>incident plane wave etkc is composed of states of all angular momenta [from Eq. 
</p>
<p>(12.6.41)]: 
</p>
<p>(f.: 
</p>
<p>eikc=eikrcosB= L i/(2/+ l)}t(kr)P,(cos 8) (19.5.3) 
1~0 
</p>
<p>Since the potential conserves angular momentum, each angular momentum compo-
</p>
<p>nent scatters independently. The amplitude a1 is a measure of the scattering in the 
angular momentum l sector. 
</p>
<p>As it stands, the expansion in Eq. (19.5.2) has not done anything for us: we 
</p>
<p>have traded one function of the two variables (8 and k) for an infinite number of 
</p>
<p>functions a1(k) of one variable k. What makes the expansion useful is that at low 
</p>
<p>energies, only the first few a1(k) are appreciably different from zero. In this case, 
</p>
<p>one manages to describe the scattering in terms of just a few functions a0 , a 1 , &bull;&bull;&bull; of 
one variable. The following heuristic argument (corroborated by explicit calcula-
tions) is usually given to explain why the scattering is restricted to a few low l values 
</p>
<p>at low k. Coming out of the accelerator is a uniform beam of particles moving along 
</p>
<p>the z axis. All particles in a cylinder of radius p and thickness dp (p is the impact 
parameter) have angular momentum 
</p>
<p>(19.5.4) 
</p>
<p>545 
</p>
<p>SCATTERING 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>546 
</p>
<p>CHAPTER 19 
</p>
<p>If the potential has range r0 , particles with p &gt; r0 will ''miss" the target. Thus there 
</p>
<p>will be scattering only up to 
</p>
<p>(19.5.5) 
</p>
<p>[Conversely, by measuring lmax (from the angular dependence of/) we can deduce 
</p>
<p>the range of the potential.] 
</p>
<p>Exercise 19.5.1. * Show that for a I 00-MeV (kinetic energy) neutron incident on a fixed 
nucleus, lma' "'= 2. (Hint: The range of the nuclear force is roughly a Fermi= 10 5 A. Also 
nc?;,200 MeV F is a more useful mnemonic for nuclear physics.) 
</p>
<p>Given a potential V(r), how does one calculate a1(k) in terms of it? In other 
</p>
<p>words, how is a, related to the solution to the Schrodinger equation for angular 
momentum /?We begin by considering a free particle. Using 
</p>
<p>. sin(kr -Ire /2) 
J1(kr) --+ &middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot; 
</p>
<p>r&middot;&middot;&bull;T kr 
(19.5.6) 
</p>
<p>we get, from Eq. (19.5.3), 
</p>
<p>(19.5.7) 
</p>
<p>e 
Pt(COS ()) (19.5.8) 
</p>
<p>upon using i = e1" 2&bull; Thus at each angular momentum we have incoming and outgoing 
</p>
<p>waves of the same amplitude. (Their phases differ by Ire because the repulsive centri-
</p>
<p>fugal barrier potential is present at ficO even for a free particle.) The probability 
currents associated with the two waves are equal and opposite.t This equality is 
</p>
<p>expected since in this steady state there should be no net probability flux flowing 
</p>
<p>into the origin or coming out of it. (This balance should occur separately for each 
</p>
<p>l, since scattering in each I is independent due to angular momentum conservation.) 
</p>
<p>What happens if we turn on a potential? As r--&gt;w, the radial wave functions 
</p>
<p>must reduce to the free-particle wave function, although there can be a phase shift 
</p>
<p>o,(k) due to the potential: 
</p>
<p>lMr) A 1 sin[kr-ln)2+o1(k)] R1(r) =----&gt; &middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;-&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot; (19.5.9) 
r ,._, r 
</p>
<p>:j: Once again, can we speak of the current associated with a given I and also with the incoming and 
</p>
<p>outgoing waves at a given I" Yes. If we calculate the total j (which will have only a radial part as r~YJ) 
</p>
<p>and integrate over all angles, the orthogonality of P/s will eliminate all interference terms between 
</p>
<p>different/'s. There will also be no interference between the incoming and outgoing waves. [See footnote 
</p>
<p>related to Eq. ( 19.2.13).] </p>
<p/>
</div>
<div class="page"><p/>
<p>where At is some constant. So 
</p>
<p>(19.5.10) 
</p>
<p>To find At, we note that since V(r) produces only an outgoing wave, the incoming 
</p>
<p>waves must be the same for lf/k and the plane wave e1k&middot;r = e1k=. Comparing the 
</p>
<p>coefficients of e-tkr/r in Eqs. (19.5.8) and (19.5.10), we get 
</p>
<p>A _ 2/ + 1 i(hr/2+ 8,) t---e 
2ik 
</p>
<p>Feeding this into Eq. (19.5.10) we get 
</p>
<p>. [ oo (e2i8,_ 1) J ikr 
=e&bull;k=+ L (2/+1) --.- Pt(cos 8) -
</p>
<p>t~o 2zk r 
</p>
<p>Comparing this to Eq. (19.5.2) we get 
</p>
<p>(19.5.11) 
</p>
<p>(19.5.12) 
</p>
<p>(19.5.13) 
</p>
<p>(19.5.14) 
</p>
<p>Thus, to calculate at(k), one must calculate the phase shift bt in the asymptotic wave 
</p>
<p>function. 
A comparison ofEqs. (19.5.12) and (19.5.8) tells us that the effect of the poten-
</p>
<p>tial is to attach a phase factor e218' to the outgoing wave. This factor does not change 
the probability current associated with it and the balance between the total incoming 
</p>
<p>and outgoing currents is preserved. This does not mean there is no scattering, since 
</p>
<p>the angular distribution is altered by this phase shift. 
</p>
<p>One calls 
</p>
<p>(19.5.15) 
</p>
<p>the partial wave S matrix element or the S matrix for angular momentum l. Recall 
</p>
<p>that the S matrix is just the t--+oo limit of U(t, -t). It is therefore a function of the 
</p>
<p>Hamiltonian. Since in this problem L is conserved, S (like H) will be diagonal 
</p>
<p>in the common eigenbasis of energy (E= 1i2e /2J1. ), angular momentum (l), and z 
component of angular momentum {m=O). Since Sis unitary (for U is), its eigen-
</p>
<p>values St(k) must be of the form e19 and here 8=28t. If we go to some other basis, 
</p>
<p>say the I p) basis, (p'l SIp) will still be elements of a unitary matrix, but no longer 
</p>
<p>diagonal, for p is not conserved in the scattering process. 
</p>
<p>547 
</p>
<p>SCATTERING 
THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>548 
</p>
<p>CHAPTER 19 
</p>
<p>If we rewrite a1(k) as 
</p>
<p>we get 
</p>
<p>The total cross section 
</p>
<p>is given by 
</p>
<p>upon using the orthogonality relations for the Legendre polynomials 
</p>
<p>JP,(cos (})PI'(cos (}) d(cos (})=-2- 811' 21+ I 
Note that cr is a sum of partial cross sections at each 1: 
</p>
<p>Each cr1 has an upper bound cr/ax, called the unitarity bound 
</p>
<p>The bound is saturated when 81=mcj2, n odd. 
</p>
<p>(19.5.16) 
</p>
<p>(19.5.17) 
</p>
<p>(19.5.18) 
</p>
<p>(19.5.19) 
</p>
<p>(19.5.20) 
</p>
<p>Comparing Eqs. (19.5.17) and (19.5.18) and using P1(cos (}) = l at (} = 0, we get 
</p>
<p>4Jr 
cr=- Im f(O) 
</p>
<p>k 
(19.5.21) 
</p>
<p>This is called the optical theorem. It is not too surprising that there exists a relation 
between the total cross section and the forward amplitude, for the following reason. 
The incident plane wave brings in some current density in the z direction. Some of 
it gets scattered into the various directions. This must reflect itself in the form of a </p>
<p/>
</div>
<div class="page"><p/>
<p>decrease in current density behind the target, i.e., in the () = 0 direction. The decrease 
</p>
<p>can only occur because the incident plane wave and the scattered wave in the forward 
</p>
<p>direction interfere destructively. It is of course not obvious why just the imaginary 
</p>
<p>part of f(O) is relevant or where the factor 4n /k comes from. To find out, you must 
</p>
<p>do Exercise 19.5.6. 
</p>
<p>A Model Calculation of t51: The Hard Sphere 
</p>
<p>Consider a hard sphere, which is represented by 
</p>
<p>V(r)=oo, r&lt;ro 
</p>
<p>=0, r&gt;ro (19.5.22) 
</p>
<p>We now proceed to solve the radial Schrodinger equation, look at the solution 
</p>
<p>as r-+oo, and identify the phase shift. Clearly the (unnormalized) radial function 
</p>
<p>Rt(r) vanishes inside r 5, r0 &bull; Outside, it is given by the free-particle function: 
</p>
<p>(19.5.23) 
</p>
<p>(We keep the nt function since it is regular for r&gt; 0.) The coefficients At and B1 must 
</p>
<p>be chosen such that 
</p>
<p>to ensure the continuity of the wave function at r = r0 &bull; Thus 
</p>
<p>B1 = _j,(kr0) 
</p>
<p>At nJ(kro) 
</p>
<p>From Eq. ( 12.6.32), which gives the asymptotic form of j 1 and nt, 
</p>
<p>_(AT+Bf) 112 [. (k In ~)] - sm r--+u, 
kr 2 
</p>
<p>where 
</p>
<p>~ -t -I(-Bt)-t -I[jJ(kro)J ut- an -- - an --
AI n,(kro) 
</p>
<p>(19.5.24) 
</p>
<p>(19.5.25) 
</p>
<p>(19.5.26) 
</p>
<p>(19.5.27) 
</p>
<p>549 
</p>
<p>SCATTERING 
THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>550 
</p>
<p>CHAPTER 19 
</p>
<p>For instance [from Eq. (12.6.31)] 
</p>
<p>, _ 1 [ sin(kro)/kro J 
uo=tan 
</p>
<p>-cos(kr0 )/kro 
</p>
<p>= -tan- 1 tan(kr0 ) 
</p>
<p>= -kro ( 19.5.28) 
</p>
<p>It is easy to understand the result: the hard sphere has pushed out the wave function, 
forcing it to start its sinusoidal oscillations at r = r0 instead of r = 0. In general, 
repulsive potentials give negative phase shifts (since they slow down the particle and 
reduce the phase shift per unit length) while attractive potentials give positive phase 
shifts (for the opposite reason). This correspondence is of course true only if 8 is 
small, since 8 is defined only modulo tr. For instance, if the phase shift kr0 = n:, a0 
vanishes and s-wave scattering does not expose the hard sphere centered at the origin. 
</p>
<p>Consider the hard sphere phase shift as k-+0. Using 
</p>
<p>we get 
</p>
<p>j1(x)----&gt; x 1/(2l + 1 )! ! 
X-----?0 
</p>
<p>nt(X)----&gt; -x-u+ 1 '(2/- 1 )! ! 
x-o 
</p>
<p>tan 81 ~ 8toc(kro)21 + 1 
k~o 
</p>
<p>(19.5.29) 
</p>
<p>This agrees with the intuitive expectation that at low energies there should be negli-
gible scattering in the high angular momentum states. The above (kr0 ) 21+ 1 dependence 
of 81 at low energies is true for any reasonable potential, with r0 being some length 
scale characterizing the range. [Since there is no hard and fast definition of range, 
we can define the range of any potential to be the ro that appears in Eq. (19.5.29).] 
Notice that although 80 ock 1, the partial cross section does not vanish because 
aoock-2 sin2 8t~k- 2 8l+-&gt;0, as k-+0. 
</p>
<p>Resonances 
</p>
<p>The partial cross section a 1 is generally very small at low energies since 
81oc (k) 21+ 1 as k-+0. But it sometimes happens that 81 rises very rapidly from 0 to n: 
[or more generally, from nn: to (n+ 1)n:] in a very small range of k or E. In this 
region, near k=k0 or E=&pound;0 , we may describe 81 by 
</p>
<p>(19.5.30) </p>
<p/>
</div>
<div class="page"><p/>
<p>where 8 b is some background phase ( ~ mr) that varies very little. The corresponding 
</p>
<p>cross section, neglecting ob, is 
</p>
<p>(19.5.31) 
</p>
<p>a 1 is described by a bell-shaped curve, called the Breit Wigner form, with a maximum 
</p>
<p>height a/ax (the unitarity bound) and a half-width r /2. This phenomenon is called 
a resonance. 
</p>
<p>In Eq. (19.5.31) for a 1, valid only near E0, we have treated rasa constant. Its 
</p>
<p>k dependence may be deduced by noting that as k-?0, we have [from Eq. (19.5.29)], 
</p>
<p>which implies 
</p>
<p>(19.5.32) 
</p>
<p>where y is some constant with dimensions of energy. Thus the expression for a 1 that 
</p>
<p>is valid over a wider range is 
</p>
<p>(19.5.33) 
</p>
<p>For any li=O, a 1 is damped in the entire low-energy region by the net k4
1 factor, 
</p>
<p>except near Eo, where a similar factor in the denominator neutralizes it. Clearly, as 
</p>
<p>l goes up, the resonances get sharper. The situation at l=O (where a 0 starts out 
</p>
<p>nonzero at k = 0) depends on the potential. More on this later. 
We would now like to gain some insight into the dynamics of resonances. We 
</p>
<p>ask what exactly is going on at a resonance, in terms of the underlying Schrodinger 
</p>
<p>equation. We choose to analyze the problem through the S matrix. Near a resonance 
</p>
<p>we have 
</p>
<p>S,( k) = e21151 = e'8' = 1 + it~I~ = E- Eo- ir /2 
e-'8 ' l-itan81 E-E0 +irj2 
</p>
<p>(19.5.34) 
</p>
<p>Although k and E are real in any experiment (and in our analysis so far), let us 
</p>
<p>think of S1(k) as a function of complex E or k. Then we find that the resonance 
</p>
<p>corresponds to a pole in S 1 at a complex point, 
</p>
<p>E=E0 &middot;&middot;- ir /2 (19.5.35) 
</p>
<p>or 
</p>
<p>k=ko-iiJ/2 (19.5.36) 
</p>
<p>551 
</p>
<p>SCATTERING 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>552 
</p>
<p>CHAPTER'l9 
</p>
<p>Figure 19.4. Some of the singularities of S1(k) in the complex 
--1---::e=-------- Re k k plane. The dots on the positive imaginary axis stand for bound 
</p>
<p>R, &bull;R2 state poles and the dots below the real axis stand for resonance 
poles. The physical or experimentally accessible region is along 
the real axis, where S1 has the form c181&bull; 
</p>
<p>where Eo= 1:,lkV2J.l and r = lJ"Fho/ J.l (for small lJ and r). Since r and 17 are small, 
the pole is very close to the real axis, which is why we trust the form of S1 that is 
valid near the point E=&pound;0 on the real axis. 
</p>
<p>What is the implication of the statement that the resonance corresponds to a 
(nearby) pole in S1(k)? To find out, we take a new look at bound states in terms of 
the S matrix. Recall that for k real and positive, if 
</p>
<p>then [from Eqs. (19.5.9) and (19.5.10) or Eq. (19.5.12)], 
</p>
<p>e2;1;,= S,(k) =i= outgoing wave amplitude 
</p>
<p>B incoming wave amplitude 
</p>
<p>(19.5.37) 
</p>
<p>(19.5.38) 
</p>
<p>(up to a constant factor i21). We now define S1(k) for complex k as follows: solve 
the radial equation with k set equal to a complex number, find R(r--+oo), and take 
the ratio A/ B. Consider now the case k = iK( 1C &gt; 0), which corresponds to E real and 
negative. Here we will find 
</p>
<p>(19.5.39) 
</p>
<p>Whereas St(k= iK) is well defined, the corresponding Rkt does not interest us, since 
it is not normalizable. But recall that for some special values of k, Rkt is exponentially 
damped and describes the wave function of a bound state. These bound states corre-
spond to k such that B=O, or S1(k)=oo. Thus poles of S1(k) at k=iK correspond 
to bound states. 
</p>
<p>So a resonance, which is a pole at k = k0 - ilJ must also be some kind of bound 
state. (See Fig. 19.4 for poles of the Smatrix.) We next argue heuristically as follows.t 
Since the bound state at E=E8 (a negative number) has the time dependence 
</p>
<p>e-iEst/fl 
</p>
<p>t This result may be established rigorously. </p>
<p/>
</div>
<div class="page"><p/>
<p>I 
</p>
<p>-Vo ----------_:V(rl 
</p>
<p>Figure 19.5. A typical potential that can sustain resonances. The centrifugal repulsion V, (dot dash line) 
</p>
<p>plus the actual attractive potential (dotted line) gives the effective potential VctT (solid line). The figure 
</p>
<p>shows an example where there would have been a bound state at En but for tunneling. But because of 
tunneling the particle can leak out, and by the same token, a particle can come from outside with 
</p>
<p>positive energy Eo, form a metastable bound state (with a lifetime inversely proportional to the tunneling 
</p>
<p>probability), and then escape. This is called resonance. 
</p>
<p>the resonance must have a time dependence 
</p>
<p>This describes a state of positive energy E0 , but whose norm falls exponentially with 
</p>
<p>a half-life t ~ rz;r. Thus, a resonance, corresponding to a pole at E= Eo- ir /2, 
describes a metastable bound state of energy Eo and lifetime t= rz;r.t 
</p>
<p>So we must next understand how a positive-energy particle manages to form a 
metastable bound state. Consider the case where V(r) is attractive, say a square well 
</p>
<p>of depth V0 and range r0 . The potential appearing in the radial equation is V.rr= 
</p>
<p>V + V,, where V is the centrifugal repulsion (Fig. 19.5). The main point is that Verr 
is attractive at short distances and repulsive at long distances. Consider now a particle 
</p>
<p>with energy Eo&lt; Vmax. such that if tunneling is ignored, the particle can form a 
bound state inside the attractive region, i.e., we can fit in an integral number of half-
</p>
<p>wavelengths. But tunneling is of course present and the particle can escape to infinity 
</p>
<p>as a free particle of energy Eo. Conversely, a free particle of energy Eo shot into the 
potential can penetrate the barrier and form a metastable bound state and leak out 
</p>
<p>again. This is when we say resonance is formed. This picture also explains why the 
resonances get narrower as I increases: as l increases, Vc grows, tunneling is sup-
pressed more, and the lifetime of the metastable state grows. We can also see why 
</p>
<p>l = 0 is different: there is no repulsive barrier due to Vc. If V = Verr is purely attractive, 
only genuine (negative energy) bound states are possible. The closest thing to a 
resonance is the formation of a bound state near zero energy (Exercise 19.5.4). If, 
however, V itself has the form of Vetr in Fig. 19.5, resonances are possible. 
</p>
<p>Exercise 19.5.2. * Derive Eq. ( 19.5.18) and provide the missing steps leading to the optical 
theorem, Eq. (19.5.21). 
</p>
<p>i The energy is not strictly E0 because the uncertainty principle doe' not allow us to define a precise 
energy for a state of finite lifetime. Eo is the mean energy. 
</p>
<p>553 
</p>
<p>SCATTERING 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>554 
</p>
<p>CHAPTER 19 
</p>
<p>Exercise 19.5.3. (I) Show that o&middot;0 __,.4nr5 for a hard sphere as k -+0. 
</p>
<p>(2) Consider the other extreme of kr0 very large. From Eq. ( 19.5.27) and the asymptotic 
</p>
<p>forms of }1 and n1 show that 
</p>
<p>sin2 81 &middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&bull; sin\kr0 -l;r /2) 
kru 
</p>
<p>so that 
</p>
<p>o-= 
/=0 
</p>
<p>if we approximate the sum over l by an integral, 2/+ I by 2!, and the oscillating function 
</p>
<p>sin2 15 by its mean value of 1/2. 
</p>
<p>Exercise 19.5.4. * Show that the s-wave phase shift for a square well of depth Vo and 
range r0 is 
</p>
<p>5o= -kr0 +tan 1 tank' ro) 
</p>
<p>where k' and k are the wave numbers inside and outside the welL For k smalL kr0 is some 
</p>
<p>small number and we ignore it. Let us see what happens to c5o as we vary the depth of the 
well, i.e., change k'. Show that whenever k' ~ k~ = (2n +I )1r /2r0 , 150 takes on the resonant form 
</p>
<p>Eq. (19.5.30) with r;2=t?k,/J1r0 , where k, is the value of k when k'=k~. Starting with a 
well that is too shallow to have any bound state, show k; corresponds to the well developing 
</p>
<p>its first bound state, at zero energy. (See Exercise 12.6.9.) (Note: A zero-energy bound state 
</p>
<p>corresponds to k=O.) As the well is deepened further, this level moves down, and soon. at 
</p>
<p>k2, another zero-energy bound state is formed, and so on. 
</p>
<p>Exercise 19.5.5. Show that even if a potential absorbs particles, we can describe it by 
</p>
<p>where I)(&lt; I), is called the inelasticity factor. 
</p>
<p>(I) By considering probability currents, show that 
</p>
<p>and that once again 
</p>
<p>Jr ' ' 
O';ne! =-:; L ( 2/ + I )[I - l)i] 
</p>
<p>k" ;~o 
</p>
<p>4;r 
O'tot =- lm f(O) 
</p>
<p>k </p>
<p/>
</div>
<div class="page"><p/>
<p>(2) Consider a "black disk" which absorbs everything for r:::; r0 and is ineffective beyond. 
</p>
<p>Idealize it by 11 = 0 for l:::; kro; 11 = 1, 8 = 0 for l&gt; kro. Show that a et = uinel ~ 1rr~. Replace the 
sum by an integral and assume kr0 &raquo; 1. (See Exercise 19.5.3.) Why is uinel always accompanied 
</p>
<p>by O"e!? 
</p>
<p>Exercise 19.5.6. (The Optical Theorem). (I) Show that the radial component of the 
</p>
<p>current density due to interference between the incident and scattered waves is 
</p>
<p>(2) Argue that as long as 8 #0, the average of j;"' over any small solid angle is zero 
because r--&gt;rx). [Assume f( 8) is a smooth function.] 
</p>
<p>(3) Integrate j;"' over a tiny cone in the forward direction and show that (see hint) 
</p>
<p>I j;"' r 2 dQ = - (~~)TIm f(O) 
forward cone \ J.1 1 
</p>
<p>Thus, if we integrate the total current in the region behind the target, we find that the interference 
</p>
<p>terrn (important only in the near-forward direction, behind the target) produces a depletion of 
</p>
<p>particles, casting a "shadow." The total number of particles (per second) missing in the shadow 
</p>
<p>region is given by the above expression for the integrated flux. Equating this loss to the product of 
</p>
<p>the incident flux 1ik I fA. and the cross section a, we regain the optical theorem. (Hint: Since () is 
small, set sin() = 8, cos 8 = I or 1- 8'12 using the judgment. In evaluating the upper limit in the 
8 integration, use the idea introduced in Chapter I, namely, that the limit of a function that 
oscillates as its argument approaches infinity is equal to its average value.) 
</p>
<p>19.6. Two-Particle Scattering 
</p>
<p>In this section we will see how the differential cross section for two-body scatter-
ing may be extracted from the solution of the Schrodinger equation for the relative 
coordinate with a potential V(r = r1 - r2 ). Let us begin by considering the total and 
differential cross sections for two-body scattering. Let a be the total cross section 
for the scattering of the two particles. Imagine a beam of projectiles with density p 1 
and magnitude of velocity v1 colliding head on with the beam of targets with param-
</p>
<p>eters P2 and v2. How many collisions will there be per second? We know that if there 
is only one target and it is at rest, 
</p>
<p>No. of collisions/sec= ax incident projectiles/sec/area 
</p>
<p>(19.6.1) 
</p>
<p>Here we modify this result to take into account that (1) there are p2 targets per unit 
volume (p2 is assumed so small that the targets scatter independently of each other), 
</p>
<p>555 
</p>
<p>SCATTERING 
THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>556 
</p>
<p>CHAPTER 19 
</p>
<p>and (2) the targets are moving toward the projectiles at a relative velocity Vrei = 
vi + v2. Consequently we have 
</p>
<p>No. of collisions/sec/volume of interaction= a Pi (vi + v2)p2 
</p>
<p>( 19.6.2) 
</p>
<p>Note that a is the same for all observers moving along the beam-target axis. 
What about the differential cross section? It will depend on the frame. In the 
</p>
<p>lab frame, where the target is initially at rest, we define, in analogy with Eq. (19.6.2), 
</p>
<p>No. of projectiles scattered into d(cos fh) drpdsecjvol 
</p>
<p>da 
=--dQ.LPiP2Vrei 
</p>
<p>dQ.L 
(19.6.3) 
</p>
<p>Here Vrei is just the projectile velocity and fh and r/JL are angles in the lab frame 
measured relative to the projectile direction. (We can also define ada jdQ.L in terms of 
how many target particles are scattered into dQ.L, but it would not be an independent 
quantity since momentum conservation will fix the fate of the target, given the fate 
of the projectile.) The only other frame we consider is the CM frame, where (daj 
dO.) dQ. is defined as in Eq. (19.6.3).t We relate da jdQ. to da jdQ.L by the following 
argument. Imagine a detector in the lab frame at ((h, rjJL) which subtends an angle 
dQ.L. The number of counts it registers is an absolute, frame-independent quantity, 
although its orientation and acceptance angle dQ. may vary from frame to frame. 
(For example, a particle coming at right angles to the beam axis in the lab frame 
will be tilted forward in a frame moving backward.) So we deduce the following 
equality from Eq. (19.6.2) after noting the frame invariance of p 1p2Vrei: 
</p>
<p>da dQ. = da dQ. 
dQ.L L dQ. 
</p>
<p>(19.6.4) 
</p>
<p>or 
</p>
<p>da da dQ. 
-- ----
</p>
<p>dQ.L dQ. dQ.L 
(19.6.5) 
</p>
<p>We will consider first the calculation of da I dQ., and then dQ.j dQ.L. 
Let us represent the state of the two colliding particles, long before they begin 
</p>
<p>to interact, by the product wave function (in some general frame): 
</p>
<p>(19.6.6) 
</p>
<p>t The CM variables will carry no subscripts. </p>
<p/>
</div>
<div class="page"><p/>
<p>We should remember that these plane waves are idealized forms of broad wave 
packets. Assuming both are moving along the z axis, 
</p>
<p>(19.6.7) 
</p>
<p>Since the potential affects only the relative coordinate, the plane wave describes the 
CM completely; there is no scattering for the CM as a whole. On the other hand, 
lfl[~~(z) will develop a scattered wave and become 
</p>
<p>lf!(Z) = eikz + lf!sc(r) 
</p>
<p>______,. eik= + f( e, 4&gt;) e'k' I r (19.6.8) 
r---+cc, 
</p>
<p>where we have dropped the superscript "rei," since the argument z makes it obvious, 
and set (k1 -k2)/2 equal to k. Thus the static solution for the entire system is 
</p>
<p>lf/system(fl, fz) = lj!CM(ZcM )[eikz + lf!sc(r)] 
</p>
<p>_____,.lf!CM(ZcM)[eikz+f(O, &cent;) eikr/r] 
r~oo 
</p>
<p>(19.6.9) 
</p>
<p>If we go to the CM frame, lflcM(zcM)=e1(k,+k,!&middot;=cM= 1, since k 1 +k2 =0 defines this 
</p>
<p>frame. So we can forget all about the CM coordinate. The scattering in the CM 
</p>
<p>frame is depicted in Fig. 19.6. The classical trajectories are not to be taken literally; 
</p>
<p>Figure 19.6. Two-body scattering in the CM frame. The projectile and target coordinates are r 1 and r2 , 
</p>
<p>respectively. The relative coordinate r=r 1-r2 is slightly displaced in the figure for clarity. Since r 1 and r 
</p>
<p>are always parallel, the probability that the projectile scatters into dO. is the same as the probability that 
</p>
<p>the fictitious particle described by r scatters into df!. To find the latter, we must solve the Schrodinger 
</p>
<p>equation for r. 
</p>
<p>557 
</p>
<p>SCAlTERlNG 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>558 
</p>
<p>CHAPTER 19 
</p>
<p>they merely define the relative coordinate r and the individual coordinates 
ri(projectile) and r2(target). 
</p>
<p>What we want is the rate at which the projectile scatters into dO.. But since r 1 
is parallel to r, this equals the rate at which the fictitious particle described by r 
scatters into solid angle dO.. We find this rate by solving the Schrodinger equation 
for the relative coordinate. Having done so, and having found 
</p>
<p>ikr 
</p>
<p>lf/(r)---+ eik= + j((), &lt;/J) ~ (19.6.10) 
r-'X.J r 
</p>
<p>we recall from Eq. (19.2.17) the rate of scattering into dO.: 
</p>
<p>(19.6.11) 
</p>
<p>Note that is the rate per unit volume of target-beam interaction, since the probability 
density for the CM is unity. To extract da jdO. from R;~do. above we turn to Eq. 
(19.6.3) which defines da jdO. (upon dropping the subscript L). Since the definition 
makes sense only for a flux of wave packets and since we are dealing with plane 
waves here, we replace the number scattered into dO. per second by the probability 
flowing into dO. per second, and the particle densities PI and P2 by probability 
densities of the colliding beams. Since the colliding beams ( eik= = eik&lt;=1 - ='1 = 
eik=1 &bull; e -ik=') are plane waves of unit modulus, PI= p2 = 1. How about Vrei? Remember 
that in the CM frame 
</p>
<p>so 
</p>
<p>So 
</p>
<p>da dO.= Rt~do. 
</p>
<p>dO. PIPz( vi+ v2) 
</p>
<p>or 
</p>
<p>I /l 2(1ik/ 11) dO. 
1ik/f.1. 
</p>
<p>(19.6.12) 
</p>
<p>(19.6.13) </p>
<p/>
</div>
<div class="page"><p/>
<p>Thus the da /dO. we calculated in the previous sections for a single particle scattering 
</p>
<p>off a potential V(r) can also be interpreted as the CM cross section for two bodies 
</p>
<p>interacting via a potential V(r = r 1 - r2). 
</p>
<p>Passage to the Lab Frame 
</p>
<p>We now consider the passage to the lab frame, i.e., the calculation of dO.jdQL&middot; 
</p>
<p>We discuss the equal mass case, leaving the unequal mass case as an exercise. Figure 
</p>
<p>19.7a shows the particles coming in with momentap and -p along the z axis in the 
</p>
<p>CM frame. If p' is the final momentum of the projectile, 
</p>
<p>(19.6.14a) 
</p>
<p>tan cp = p~/p~ (19.6.14b) 
</p>
<p>To go to the lab frame we must move leftward at a speed pjm. In this frame, all 
</p>
<p>momenta get an increment in the z direction (only) equal top. (Thus T, the target, 
</p>
<p>will be at rest before collision.) The scattering angles in the lab frame are given by 
</p>
<p>(19.6.15a) 
</p>
<p>(19.6.15b) 
</p>
<p>Comparing Eqs. (19.6.14) and (19.6.15) we get 
</p>
<p>(19.6.16) 
</p>
<p>p~ p~/p sin() 
tan ()L=--
</p>
<p>p~+p p~jp+ 1 cos()+ 1 
</p>
<p>=tan(()/2) (using IP'I =p) 
</p>
<p>So 
</p>
<p>(19.6.17) 
</p>
<p>One consequence of the result is that ()L~n/2. Given Eqs. (19.6.16) and (19.6.17) 
</p>
<p>it is a simple matter to relate da /dO. to da / dQL. 
</p>
<p>Exercise 19.6.1. * (1) Starting with Eqs. (19.6.16) and (19.6.17), show that the relation 
between dajdQ and dajdQL is 
</p>
<p>da I do-l - =- 4cos90 
dQL 9o dQ 29o 
</p>
<p>(2) Show that (} L :s; rr /2 by using just energy and momentum conservation. 
</p>
<p>559 
</p>
<p>SCATTERING 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>560 
</p>
<p>CHAPTER 19 
</p>
<p>(b) 
p 
</p>
<p>p-::;----- !~ 
2p \ 
</p>
<p>T 
</p>
<p>Figure 19.7. (a) Collision of two equal masses in the CM frame. The labels P and T refer to projectile 
</p>
<p>and target. The angle &lt;J&gt; equals ~ n in the figure. (b) The same collision in the lab frame (where Tis 
initially at rest). 
</p>
<p>(3) For unequal mass scattering, show that 
</p>
<p>sine 
tan 8 L &middot;&middot;&middot;&middot; ------
</p>
<p>COS 8 + (m,jm2) 
</p>
<p>where m2 is the target mass. 
</p>
<p>Scattering of Identical Particles 
</p>
<p>Consider the scattering of two identical spin-zero bosons in their CM frame. 
</p>
<p>We must describe them by a symmetrized wave function. Under the exchange r 1 &lt;-? r2 ; 
</p>
<p>reM= (rt + r2)/2 is invariant while r = r1- r2 changes sign. So 1/fcM(rcM) is auto-
matically symmetric. We must symmetrize 1/f(r) by hand: 
</p>
<p>( 19.6.18) 
</p>
<p>We have used the fact that under r-&gt;-r, ()--.n:-0 and 0-&gt;~,P+n:. The scattering 
</p>
<p>amplitude is thus 
</p>
<p>j;ym({1, 4;) =f( e, 4;) + f(n- e, &cent; + n) (19.6.19) 
</p>
<p>Note that .fsym is consistent with the fact that since the particles are identical, one 
</p>
<p>cannot say which one scattered into ( e, &cent;) and which one into ( n &middot;&middot;&middot;- 8, 4; + n:) 
(Fig. 19. 7). The differential cross section is 
</p>
<p>da 
dO. =lf(O, &cent;)+l(n:-e, &cent;+n)l 2 
</p>
<p>= l.f(O, &cent;)1 2 + I .f(n- 8, &cent;+ n)l 2 +2 Re[.f(8, &cent;)_{*(n- 0, &cent;+ n)] (19.6.20) 
</p>
<p>The first two tenus are what we would get if we had two distinguishable particles 
</p>
<p>and asked for the rate at which one or the other comes into dO.. The third term 
</p>
<p>gives the usual quantum mechanical interference that accompanies identical particles. 
There are two features worth noting about Eq. (19.6.20): 
</p>
<p>(I) To find a, we must integrate over only 2n radians and not 4n radians (if 
not, we will count each distinguishable event twice). </p>
<p/>
</div>
<div class="page"><p/>
<p>{2) Recall that when we obtained the Rutherford cross section by taking the 
</p>
<p>J.lo-+0 limit of the Yukawa cross section, we got the right answer although!( fJ) was 
</p>
<p>not right: it did not contain the exponential phase factor that comes from a careful 
</p>
<p>treatment of the Coulomb potential [see Eq. (19.3.16) and the sentences following 
</p>
<p>it.] When we consider the Coulomb scattering of identical bosons (of chargee, say) 
</p>
<p>the interference terms expose the inadequacy of the }Jo-+0 approach. The correct 
</p>
<p>cross section ist 
</p>
<p>da: e2 1 1 2 cos( r ln tan2 f) /2) ( )2[ J 
dO= 4E sin4 f) /2 + cos4 f) /2 + sin2 f) /2 cos2 f) /2 
</p>
<p>(19.6.21) 
</p>
<p>whereas the J.lo-+0 trick would not have given the cos( r ln tan2 fJ /2) factor. (The 
classical Rutherford treatment would not give the third term at all. Notice, however, 
</p>
<p>that as 1i-+0, it oscillates wildly and averages to zero over any realistic detector.) 
</p>
<p>Consider now the scattering of two identical spin-1/2 fermions, say electrons. 
</p>
<p>Let us assume that the spin variables are spectators, except for their role in the 
</p>
<p>statistics: in the triplet state the spatial function is antisymmetric, while in the singlet 
</p>
<p>it is symmetric. If the electrons are assumed to come in with random values of sz, 
</p>
<p>the triplet is three times as likely as the singlet and the average cross section will be 
</p>
<p>da: 3 
dO =41 f(fJ, f/J)-f(rr- fJ, f/J+ rr)l 2 
</p>
<p>+! lf(fJ, f/J)+f(rr-fJ, f/J+rr)l 2 
4 
</p>
<p>For Coulomb scattering of electrons this becomes 
</p>
<p>da: e2 I 1 cos( r In tan2 fJ /2) ( )2[ J 
dO= 4E sin4 f) /2 + cos4 f) /2 sin2 f) /2 cos2 f) /2 
</p>
<p>Exercise 19.6.2. Derive Eq. (19.6.21) using Eq. (19.3.16) for fc(O). 
</p>
<p>(19.6.22) 
</p>
<p>(19.6.23) 
</p>
<p>Exercise 19.6.3. Assumingf=/(8) show that (da:/dfl)" 12 =0 for fermions in the triplet 
</p>
<p>state. 
</p>
<p>561 
</p>
<p>SCATTERING 
</p>
<p>THEORY </p>
<p/>
</div>
<div class="page"><p/>
<p>20 
</p>
<p>The Dirac Equation 
</p>
<p>Nonrelativistic quantum mechanics, which was developed in the previous chapters, 
</p>
<p>is very successful when applied to problems like the hydrogen atom, where the typical 
</p>
<p>velocity (speaking semiclassically) is small compared to c. (Recall vjc={3=a;;;.lj 
</p>
<p>137 in the ground state.) But even in this case, there are measurable (fine-structure) 
</p>
<p>corrections of the order of (vjc) 4 which have to be put in by hand. If these corrections 
</p>
<p>are to emerge naturally and if relativistic systems (high-Z atoms, for example) are 
</p>
<p>to be described well, it is clear that we need an equation for the electron that has 
</p>
<p>relativity built into it from the start. Such an equation was discovered by Dirac. We 
</p>
<p>study it here with the main goal of seeing the coherent emergence of several concepts 
</p>
<p>that were introduced disjointly at various stages-the spin of the electron, its mag-
</p>
<p>netic moment (g = 2), the spin-orbit, and other fine-structure corrections. 
</p>
<p>In the last section we address some general questions that accompany Dirac's 
</p>
<p>formulation and indicate the need for quantum field theory. 
</p>
<p>20.1. The Free-Particle Dirac Equation 
</p>
<p>Let us consider the simplest case, of a free particle. We start by stating the 
</p>
<p>relation between classical mechanics and the free-particle Schrodinger equation in a 
</p>
<p>way that facilitates generalization. If we start with the nonrelativistic relation 
</p>
<p>and make the substitution 
</p>
<p>p--+P 
</p>
<p>a 
.Yt -+ill-
</p>
<p>ar 
</p>
<p>(20.1.1) 
</p>
<p>(20.1.2) 
</p>
<p>563 </p>
<p/>
</div>
<div class="page"><p/>
<p>564 
</p>
<p>CHAPTER 20 
</p>
<p>and let both sides act on a state vector I If/), we get Schrodinger's equation 
</p>
<p>(20.1.3) 
</p>
<p>A natural starting point for the relativistic equation is the corresponding relation 
due to Einstein 
</p>
<p>(20.1.4) 
</p>
<p>If we make the substitution mentioned above, we get 
</p>
<p>(20.1.5) 
</p>
<p>This equation is undesirable because it treats space and time asymmetrically. To see 
this, we first go to the momentum basis, where P is just p and the square root may 
be expanded in a series: 
</p>
<p>(20.1.6) 
</p>
<p>If we now transform to the coordinate basis, each p2 becomes ( -1i2V2) and the 
asymmetry between space and time is manifest. What we want is an equation that 
is of the same order in both space and time. 
</p>
<p>There are two ways out. One is to replace Eq. (20.1.4) by 
</p>
<p>(20.1.7) 
</p>
<p>and obtain, upon making the operator substitution, 
</p>
<p>(20.1.8a) 
</p>
<p>In the coordinate basis this becomes 
</p>
<p>[ 
7 ( ')2] 1 a- 2 mc-,--v +- lfi=O 
</p>
<p>c ot2 rz 
(20.1.8b) 
</p>
<p>This is called the Klein-Gordon equation and has the desired symmetry between space 
and time. But we move along, since 1f1 here is a scalar and cannot describe the 
electron. It is, however, a good candidate for pions, kaons, etc., which are spinless. 
</p>
<p>The second alternative, due to Dirac, is the following. Let us suppose that the 
quantity in the square root in Eq. (20.1.5) can be written as a perfect square of a 
quantity that is linear in P. We can then take the square root (which will give us </p>
<p/>
</div>
<div class="page"><p/>
<p>our Hamiltonian) and obtain an equation that is of the first order in time and space. 
</p>
<p>So let us write 
</p>
<p>c2P 2 +m2c4 = (caxPx+cayPy+cazPz+ f3mc2f 
</p>
<p>= (ca&middot; P+ f3mc2) 2 
</p>
<p>where a and f3 are to be determined by matching both sides of 
</p>
<p>c2(P?, + P} + P;) + m2c4 
</p>
<p>= [c2(a;P?c+ a;P}+ a;P;) + f3 2m2c4 ] 
</p>
<p>+ [c2PxPy(axay+ ayax) + and cyclic permutations] 
</p>
<p>(20.1.9) 
</p>
<p>+ [mc3 Px(axf3 + f3ax) + x--+y + x--+z] (20.l.IO) 
</p>
<p>(We have assumed that a and f3 are space independent, which is a reasonable assump-
</p>
<p>tion for a free particle.) These equations tell us that 
</p>
<p>(i=x, y, z) 
</p>
<p>(i #j) (20.1.11) 
</p>
<p>It is evident that a and f3 are not c numbers. They are matrices and furthermore 
</p>
<p>Hermitian (so that the Hamiltonian H=ca&middot;P+f3mc2 is Hermitian), traceless, and 
</p>
<p>have eigenvalues &plusmn;1. (Recall the results of Exercise 1.8.8). They must also be even 
</p>
<p>dimensional if the last two properties are to be compatible. They cannot be 2 x 2 
</p>
<p>matrices, since, as we saw in Exercise 14.3.8, the set of three Pauli matrices with 
</p>
<p>these properties cannot be enlarged to include a fourth. So they must be 4 x 4 matri-
</p>
<p>ces. They are not unique (since a --+St aS, f3 --+St f3S preserves the desired properties 
</p>
<p>if Sis unitary.) The following four are frequently used and will be used by us: 
</p>
<p>a=[~ ~l (20.1.12) 
</p>
<p>In the above, CJ and I are 2 x 2 matrices.t We now have the Dirac equation: 
</p>
<p>(20.1.13) 
</p>
<p>with a and f3 known. Hereafter we work exclusively in the coordinate basis. However, 
</p>
<p>we depart from our convention and use the symbol P, reserved for the momentum 
</p>
<p>operator in the abstract, to represent it in the coordinate basis (instead of using 
</p>
<p>- tliV). This is done to simplify the notation in what follows. 
</p>
<p>t For example, f3 is a 4 x 4 diagonal matrix with the first two entries + I and the next two entries - I. 
</p>
<p>565 
</p>
<p>THE DIRAC 
</p>
<p>EQUATION </p>
<p/>
</div>
<div class="page"><p/>
<p>566 
</p>
<p>CHAPTER 20 
</p>
<p>The fact that a and f3 in 
</p>
<p>(20.1.14) 
</p>
<p>are 4 x 4 matrices implies that lfl is a four-component object. It is called a Lorentz 
spinor. Our reaction is mixed. We are happy that relativity, plus the requirement 
th~t the equation be first order in time and space, have led naturally to a multicompo-
nent wave function. But we are distressed that lfl has four components instead of 
two. In the next two sections we will see how, despite this apparent problem, the 
Dirac equation describes electrons. 
</p>
<p>For later use, let us note that since the Hamiltonian is Hermitian, the norm of 
the state is conserved. In the coordinate basis this means 
</p>
<p>(20.1.15) 
</p>
<p>Just as in the nonrelativistic case, this global conservation law has a local version 
also. (See exercise below.) 
</p>
<p>Exercise 20.1.1. * Derive the continuity equation 
</p>
<p>jjp 
-+V&middot;j=O 
iJt 
</p>
<p>20.2. Electromagnetic Interaction of the Dirac Particle 
</p>
<p>In this central section, we see how several properties of the electron emerge 
naturally from the Dirac equation. As a first step, we couple the particle to the 
potential (A, &lt;/J). We then consider the equation to order (vjc)2 and show that the 
particle can be described by a two-component wave function and that it has g = 2. 
Finally we consider the equation to order (vjc) 4 and see the fine-structure emerge. 
</p>
<p>The coupling of the electromagnetic potentials is suggested by the classical 
Hamiltonian for a particle of charge q: 
</p>
<p>(20.2.1) 
</p>
<p>which leads us to 
</p>
<p>a"' 2 ifz-=[ca&middot;(P-qAjc)+f3mc +q&lt;/J]lf/ 
at 
</p>
<p>(20.2.2) </p>
<p/>
</div>
<div class="page"><p/>
<p>The Electron Spin and Magnetic Moment 
</p>
<p>To see just these two features emerge, we can set &cent;=0 and work to order (v/c)2&bull; 
</p>
<p>If we look for energy eigenstates 
</p>
<p>of Eq. (20.2.2), we get 
</p>
<p>(20.2.3) 
</p>
<p>where 
</p>
<p>n=P-qA/c (20.2.4) 
</p>
<p>is the kinetic (mv) momentum operator. We now write If! as 
</p>
<p>(20.2.5) 
</p>
<p>where x and &lt;I&gt; are two-component spinors. Equation (20.2.3), with a and f3 explicitly 
written, becomes 
</p>
<p>[E-mc
2 -cO'&middot;nJ[xJ=[OJ 
</p>
<p>-CO' &middot;n E+ mc2 &lt;I&gt; 0 (20.2.6) 
</p>
<p>which means 
</p>
<p>(20.2.7) 
</p>
<p>and 
</p>
<p>(20.2.8) 
</p>
<p>The second equation tells us that 
</p>
<p>&lt;I&gt;=(~) 
E+mc2 X 
</p>
<p>(20.2.9) 
</p>
<p>Let us examine the term in brackets at low velocities. The denominator is 
</p>
<p>(20.2.10) 
</p>
<p>567 
</p>
<p>THE DIRAC 
EQUATION </p>
<p/>
</div>
<div class="page"><p/>
<p>568 
</p>
<p>CHAPTER 20 
</p>
<p>where Es= E- mc2 is the energy that appears in Schrodinger's equation. At low 
velocities, since Es&laquo;mc2t 
</p>
<p>(20.2.11) 
</p>
<p>The numerator is of the order mvc, where mv is the typical momentum of the state. 
So 
</p>
<p>(20.2.12) 
</p>
<p>For this reason x and &lt;I&gt; are called the large and small components, respectively. The 
terminology is of course appropriate only in the nonrelativistic domain. In this 
domain 
</p>
<p>and Eq. (20.2.7) becomes 
</p>
<p>0''1t 
&lt;I&gt;~-x 
</p>
<p>2mc 
</p>
<p>(0''1t)(0''1t) 
Esx =c0'&middot;1t&lt;l&gt;= X 
</p>
<p>2m 
</p>
<p>This is called the Pauli equation.&sect; If we use the identity 
</p>
<p>and 
</p>
<p>we get 
</p>
<p>0'. AO'. B =A. B + i 0'. A X B 
</p>
<p>iqfz 
1tX1t=-B 
</p>
<p>c 
</p>
<p>[ (P-qA/c)
2 qfz BJ E 
</p>
<p>-0'' x= sX 
2m 2mc 
</p>
<p>(20.2.13) 
</p>
<p>(20.2.14) 
</p>
<p>(20.2.15) 
</p>
<p>(20.2.16) 
</p>
<p>(20.2.17) 
</p>
<p>It is evident that this equation describes a spin-~ particle with g = 2. It is therefore 
appropriate to electrons. {Although g = 2 emerges so naturally from Dirac theory, 
</p>
<p>t Es= T+ V= rr2 + V ~ o(!C)=mv2 , Es 2 ~(':') 2 &laquo;I 
2m (virial m me c 
</p>
<p>theorem) 
</p>
<p>&sect;Actually the Pauli equation is the time-dependent version, with i1ii on the left-hand side. </p>
<p/>
</div>
<div class="page"><p/>
<p>it is incorrect to say that we need relativity to get this result. If we write the free-
</p>
<p>particle Schrodinger equation as 
</p>
<p>(a&middot;Pf 
--x=Esx 
</p>
<p>2m 
</p>
<p>[since (a&middot; P)2 = P 2] and then couple the vector potential A as prescribed by nonrela-
</p>
<p>tivistic mechanics (P--+ P- qA/ c), we get g = 2. Of course spin is introduced artificially 
</p>
<p>here, but g = 2 is not.} 
</p>
<p>Exercise 20.2.1. * Derive Eq. (20.2.16). 
</p>
<p>Exercise 20.2.2. * Solve for the exact levels of the Dirac particle in a uniform magnetic 
field B=B0 k. Assume A={B0 /2)(-yi+xj). Consult Exercise 12.3.8. (Write the equation 
</p>
<p>for X.) 
</p>
<p>Hydrogen Fine Structure 
</p>
<p>We now apply the Dirac equation to the case 
</p>
<p>(20.2.18) 
</p>
<p>that is to say, the electron in the hydrogen atom. (The proton is assumed to be fixed, 
</p>
<p>i.e., infinitely massive.) The small and big components obey the following coupled 
</p>
<p>equations: 
</p>
<p>(20.2.19) 
</p>
<p>(20.2.20) 
</p>
<p>The second one tells us that 
</p>
<p>(20.2.21) 
</p>
<p>(Since P can differentiate V, the order of the factors is important.) If we feed this 
</p>
<p>into the first, we get 
</p>
<p>(E- V-mc2)x=ca&middot;P[ 1 2 ]ca&middot;Px 
E- V+mc 
</p>
<p>(20.2.22) 
</p>
<p>If we approximate E- V + mc2 on the right-hand side as 2mc2, we get 
</p>
<p>Es X = [ ( a;;)2 + V ]x 
</p>
<p>=[:~ + v]x (20.2.23) 
</p>
<p>569 
</p>
<p>THE DIRAC 
EQUATION </p>
<p/>
</div>
<div class="page"><p/>
<p>570 
</p>
<p>CHAPTER 20 
</p>
<p>This is just the nonrelativistic Schrodinger equation we solved in Chapter 13. Notice 
that the Hamiltonian is order (vlc) 2 since it is quadratic in the momentum. To see 
</p>
<p>the fine structure, we must go to order (v/c) 4 . We do this by expanding 
(E- V+mc2)- 1 on the right-hand side to one more order in v2 
</p>
<p>E- V+mc2 
1 ( E,- v')-l 
</p>
<p>2mc2 + Es- V 2mc~ , 1 + 2~&middot;c~ 1 
</p>
<p>(20.2.24) 
</p>
<p>Equation (20.2.22) now becomes 
</p>
<p>(20.2.25) 
</p>
<p>We cannot view this as the time-independent Schrodinger equation (i.e., as Esx = 
</p>
<p>Hx) since Es appears on both sides. By now even our spinal column knows how to 
respond to such a crisis. The right-hand side is a power series in v2 . The first two 
</p>
<p>terms are of the order v2 , and the third is expected to be of order v4 I c4 . Now the 
two a&middot; P factors in the third form use up a factor v2 /c2&bull; So we need E5 - V only to 
order v2 I c2&bull; This we get from the same equation truncated to this order: 
</p>
<p>p2 
(Es- V)x=&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot; X 
</p>
<p>2m 
(20.2.26) 
</p>
<p>We cannot use this result directly in Eq. (20.2.25) since &pound; 5 -- V there does not act 
</p>
<p>on x directly; there is a&middot; P in the way. So we do the following: 
</p>
<p>(Es -- V)a&middot; Px =a&middot; P(Es-- V)x +a&middot; [Es- V P]x 
</p>
<p>p2 
= (cr&middot;P) 2m X +o-&middot;[P, V]x 
</p>
<p>Feeding this into Eq. (20.2.25) we get 
</p>
<p>=Hx 
</p>
<p>(20.2.27) 
</p>
<p>(20.2.28) </p>
<p/>
</div>
<div class="page"><p/>
<p>using once again the identity (20.2.15). We recognize the third term to be just the 
</p>
<p>relativistic correction to the kinetic energy. It is just [recall Eq. (17.3.6)] 
</p>
<p>The fourth term is the spin-orbit interaction, Hs.o. ofEq. (17.3.16): 
</p>
<p>- i 0'. p X [P' V] 
</p>
<p>4m2c2 
</p>
<p>- iO"&middot; P x [- iFiV( -e2 jr)] 
</p>
<p>4m2c2 
</p>
<p>e2 
=~S&middot;L=Hs.o. 
</p>
<p>2m c r 
</p>
<p>Notice that the Thomas factor is built in. 
</p>
<p>{using 
. df} 
</p>
<p>[P, f(x)] = -z11.-
dx 
</p>
<p>(20.2 29) 
</p>
<p>(20.2.30) 
</p>
<p>Consider now the fifth and last term. It upsets the whole interpretation because 
</p>
<p>it is not Hermitian (check this). So if the quantity in brackets in Eq. (20.2.28) is used 
</p>
<p>as a Hamiltonian we will find 
</p>
<p>But this is not surprising, since the conservation law that comes from the Dirac 
</p>
<p>equation is 
</p>
<p>(20.2.31) 
</p>
<p>It follows that x is not a good candidate for the Schrodinger wave function to this 
order in vjc. [It was all right when we worked to order (vjcf.] We find the right 
</p>
<p>one as follows. Note that 
</p>
<p>(20.2.32) 
</p>
<p>! Although P is a differential operator, P x r = -r x P, just as if P and r were c numbers, because the 
cross product never involves the product of a given coordinate and its conjugate momentum. This point 
</p>
<p>was made earlier in the book when it was stated that there was no ordering ambiguity in passing from 
</p>
<p>l=r x p to L. 
</p>
<p>571 
</p>
<p>THE DIRAC 
EQUATION </p>
<p/>
</div>
<div class="page"><p/>
<p>572 
</p>
<p>CHAPTER 20 
</p>
<p>(The neglected terms makes corrections of order v6 I c6 in the end.) Consequently 
</p>
<p>and so, from Eq. (20.2.3I), 
</p>
<p>=const (20.2.33) 
</p>
<p>using (1 + x) =(I+ xi2)(I + xl2) + O(x2) and the Hermiticity of P 2&bull; Consequently, 
the candidate for the Schrodinger wave function is 
</p>
<p>(20.2.34) 
</p>
<p>for it will have a time-independent norm. (To the present accuracy, that is. If we go 
to higher and higher orders in v2 I c2, &lt;I&gt; will creep in more and more.) 
</p>
<p>The equation for x s is obtained by eliminating x in Eq. (20.2.28): 
</p>
<p>Esi+ p 22 xs=Hl+ p 22 Xs ( 2 )-1 ( 2 )-1 
8mc 8mc 
</p>
<p>( p2 ) ( p2 ) Esxs= I +--2 - 2 H 1---2 - 2 Xs 
8m c 8m c 
</p>
<p>(to this order in vic) 
</p>
<p>=Hsxs (20.2.35) 
</p>
<p>In evaluating the commutator, we need consider just the v2lc2 part of H, since P 2l 
8m2c2 is O(v2 lc2) and we are working to order v4lc4 &bull; So 
</p>
<p>is the desired Schrodinger Hamiltonian. The extra piece the above analysis yields 
combines with the non-Hermitian piece in Eq. (20.2.28) to form the Darwin </p>
<p/>
</div>
<div class="page"><p/>
<p>term Hn: 
</p>
<p>1 
Hn=-2- 2 (-2P&middot;[P, V]+[P&middot;P, V]) 
</p>
<p>8m c 
</p>
<p>-1 
=--2 - 2 [P&middot; [P, V]] (using the chain rule for commutators of products) 
</p>
<p>Sm c 
</p>
<p>n2 , 
=--u V"V {using [P, f(x)] =-in df/dx twice} 
</p>
<p>8m c 
</p>
<p>(20.2.37) 
</p>
<p>Thus the Darwin term affects only the s .nates.t [n the ground state, for example, 
</p>
<p>and in general 
</p>
<p>(20.2.38) 
</p>
<p>Recall that in our previous treatment of fine structure we obtained a spin orbit shift 
</p>
<p>valid only for l f:. 0 and then applied it to l = 0 as well, without any real justification. 
The result we got for l = 0 is just what Hn generated above, which was our reason 
</p>
<p>for doing what we did then. Thus Hs.o. (relevant for lt=O) and Hn (relevant only for 
</p>
<p>l = 0) together conspire to produce a fine-structure shift that is smooth in l. The 
physics behind the Darwin term has nothing to do with spin orbit coupling 
</p>
<p>(for there is no such thing for l= 0). Rather, it reflects the fact that in a relativistic 
</p>
<p>theory, the particle cannot be localized to better than its Compton wavelength n/ 
me. Thus the potential that is relevant is not V(r) but some smeared average around 
</p>
<p>the point r: 
</p>
<p>-- av 1 il2 V . 3 
V(r) = V(r) + L-;;-- or;+&middot;&middot;&middot;&middot;&middot;&middot;&middot; L L -~&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot; or;ori+ O(lir) 
</p>
<p>; or; 2! ; j or;ori 
</p>
<p>(20.2.39) 
</p>
<p>where, in the averaging, we have assumed that fluctuations in the various directions 
are uncorrelated and spherically symmetric. If we now feed in 8r ro:::.n/mc, we get the 
right sign and almost the right magnitude for the Darwin term [see Eq. (20.2.37)]. 
</p>
<p>t Recall that only in these states is 1/f nonzero at the origin. 
</p>
<p>573 
</p>
<p>THE DIRAC 
</p>
<p>EQUATION </p>
<p/>
</div>
<div class="page"><p/>
<p>574 
</p>
<p>CHAPTER 20 
</p>
<p>Although we chose to work to order v4 jc4 , the Dirac equation can be solved 
exactly in the Coulomb case, V= -e2 jr. The resulting energy spectrum is 
</p>
<p>(20.2.40) 
</p>
<p>If we expand this in powers of a, we get the rest energy, the Schrodinger energy, 
the fine-structure energy, and so on. Notice that the states of a given n and j are 
degenerate to all orders in a. 
</p>
<p>Whereas the above formula is in fantastic agreement with experimend it is not 
the last word. For example, very precise measurements show that the 2S112 level is 
above the 2P112 level. This phenomenon, called the Lamb shift, can be understood 
only if the electromagnetic field is treated quantum mechanically. 
</p>
<p>20.3. More on Relativistic Quantum Mechanics 
</p>
<p>With the principal goal of this chapter achieved in the last section, we direct 
our attention to certain phenomena that come out of Dirac theory but were not 
apparent in the last few pages. Let us first note that the union of relativity and 
quantum mechanics produces the following problem: relativity allows particle pro-
duction given enough energy, and quantum mechanics allows arbitrarily large energy 
violations over short times. Consequently the degrees of freedom of a relativistic 
system are neither fixed nor finite; a system that initially has one particle can evolve 
into a state with 15 of them. Why doesn't this problem appear in the Dirac theory, 
which seems like a single-particle theory? The answer is that it does appear, but in 
the guise of negative-energy solutions. Let us see what these are and how they lead 
to proliferation of the degrees of freedom. 
</p>
<p>Consider the free-particle Dirac equation (with fz = c = I) 
</p>
<p>Olfl 
i-=(a&middot;P+ f3m)lfl 
</p>
<p>at 
</p>
<p>Let us look for plane wave solutions 
</p>
<p>lfl = w(p) ei(p&middot;r-Er) 
</p>
<p>where w(p) is a spinor that has no space-time dependence. It satisfies 
</p>
<p>Ew= (a&middot;p+ f3m)w 
</p>
<p>t After hyperfine interactions are taken into account. 
</p>
<p>(20.3.1) 
</p>
<p>(20.3.2) 
</p>
<p>(20.3.3) </p>
<p/>
</div>
<div class="page"><p/>
<p>Figure 20.1. In the Dirac theory there are two continuous 
</p>
<p>bands of energy available to the free particle; one goes 
</p>
<p>from +m up to oo and the other goes from -m down to 
</p>
<p>-oo. 
</p>
<p>fE------------------0 
</p>
<p>or in terms of x and &lt;I&gt;, 
</p>
<p>(20.3.4) 
</p>
<p>If p = 0, x and &lt;I&gt; decouple. The equation for x is 
</p>
<p>(E-m)x =0--+E=m (20.3.5) 
</p>
<p>which is fine. It says a particle at rest has energy E=m and is described by an 
</p>
<p>arbitrary two-component spinor which we identify as the spin degree of freedom. 
</p>
<p>The equation for &lt;I&gt; is 
</p>
<p>(E+m)&lt;I&gt;=O-+E=-m (20.3.6) 
</p>
<p>Now even a layperson will tell you that E is supposed to be mc2 not -mc2&bull; The 
</p>
<p>significance of the four components of lfl are evident in the rest frame: there are two 
</p>
<p>possible spin orientations and two signs of the energy. The problem persists for p "# 0 
</p>
<p>as well. Here we find 
</p>
<p>These are consistent only if 
</p>
<p>or 
</p>
<p>or 
</p>
<p>&lt;J'P 
x=~-&lt;1&gt; 
</p>
<p>E-m 
</p>
<p>&lt;J'P 
&lt;I&gt;=~-x 
</p>
<p>E+m 
</p>
<p>(20.3.37) 
</p>
<p>(20.3.8) 
</p>
<p>(20.3.9) 
</p>
<p>The energy levels corresponding to these two options are shown in Fig. 20.1. 
</p>
<p>What do we do with the negative-energy solutions? If there are no interactions, 
</p>
<p>positive-energy electrons will stay where they are and we can postulate that there 
</p>
<p>575 
</p>
<p>THE DIRAC 
EQUATION </p>
<p/>
</div>
<div class="page"><p/>
<p>576 
</p>
<p>CHAPTER 20 
</p>
<p>are no negative-energy electrons. But there are always some perturbations acting on 
all electrons and these can induce all positive-energy electrons to cascade down to 
the negative-energy states. How do we understand the stability of positive-energy 
electrons? 
</p>
<p>There are two ways out, one due to Dirac and one due to Feynman.t Dirac 
postulated that the negative-energy states are all occupied-that what we call the 
vacuum is really the occupied (but unobservable) sea of negative-energy electrons. If 
we accept this, the stability problem is solved by the exclusion principle, which 
prevents the positive-energy electrons from decaying to the occupied negative-energy 
states. This picture has some profound consequences. Suppose we give a negative-
energy electron enough energy (at least 2m) for it to come to a positive-energy state. 
Now we have a positive-energy, charge -e object. But we also have created a hole 
in the "Dirac sea." Since the filled Dirac sea was postulated to be unobservable, the 
hole is observable; it represents an increase in charge by + e (the disappearance of 
-e=appearance of +e), and an increase in energy by lEI, if -lEI was the energy 
of the electron ejected from the sea.&sect; Thus the hole, which has charge + e and positive 
energy, is created along with the electron. It is called a positron. Its mass can easily 
be shown to be m. Positrons were observed a few years after Dirac's theory of holes 
was published. 
</p>
<p>When an electron meets a positron, i.e., a hole in the sea, it jumps in and we 
lose both particles, though some energy (at least equal to 2m) will be liberated in 
the form of photons. (Hereafter we will occasionally refer to these particles as e-, 
e +, and r, respectively.) 
</p>
<p>The trouble with Dirac's solution is that it doesn't apply to spinless particles, 
which don't obey the Pauli principle but which do have the same problem of negative-
energy solutions, as one can see by plugging a plane wave solution into the Klein-
Gordon equation. (In fact this was the reason the Klein-Gordon equation was 
rejected in the early days and Dirac sought a first-order equation.) So let us turn to 
Feynman's resolution, which applies to bosons and fermions. 
</p>
<p>Feynman's idea is the following: negative-energy particles can only travel back-
ward in time. Let us see first how this resolves the problem and then how the statement 
is actually implemented in quantum theory. Consider a negative-energy particle that 
is created at the space-time point c and travels backward to d, where it is destroyed 
(Fig. 20.2a). To us, who move forward in time and see space-time in equal-time 
slices, this is what will seem to be happening: 
</p>
<p>(l) t&lt;td 
</p>
<p>(2) t= td 
</p>
<p>(3) t=tc 
</p>
<p>(4) t&gt; tc 
</p>
<p>Nothing anywhere. 
Negative energy -1 El and charge - e are destroyed, i.e., world energy 
</p>
<p>goes up by I El and charge goes up by e relative to the past. A 
positron is born. 
</p>
<p>Negative energy is created, charge - e is created. This wipes out the 
positron. 
</p>
<p>Nothing anywhere. 
</p>
<p>l In its basic fonn, the idea exploited by Feynman was pointed out by Stueckelberg. 
&sect; Recall the story of the fellow who got so used to the midnight express going past his house that one 
</p>
<p>day when it failed to show up, we woke up screaming "What's that noise?" </p>
<p/>
</div>
<div class="page"><p/>
<p>): A. lc --~ 
'd ------------ d 
</p>
<p>I I 
</p>
<p>X X 
(a) (b) (c) 
</p>
<p>Figure 20.2. (a) A negative-energy particle is created at c, travels back in time to d, where it is destroyed. 
</p>
<p>To us, who move forward in time, it will seem as though an antiparticle of positive energy is created at 
</p>
<p>d and destroyed at c. (b) A normal second order scattering process. (c) A second-order process that 
</p>
<p>involves back-scattering in time. Between times 2 and I we will see a particle-antiparticle pair in addition 
</p>
<p>to the original particle. 
</p>
<p>Thus the process makes perfect sense and represents a positron created at d and 
</p>
<p>destroyed at c. 
How does Feynman ensure that negative-energy states propagate backward? 
</p>
<p>Here is a sketchy description. Recall that the Schrodinger propagator we have used 
</p>
<p>so far is (in the coordinate basis) 
</p>
<p>Us(r, t; r', t') = L: Vtn(r)Vt:(r') e-iE.(t-t'l (20.3.10) 
n 
</p>
<p>where lj/n is an energy eigenfunction labeled by a generic quantum number n. 
</p>
<p>Since every term in the sum satisfies the Schrodinger equation, it is clear that 
</p>
<p>(20.3.11) 
</p>
<p>given this Us and ljl{t') at some initial time, we can get Vt(t) at a later time (t &gt; t'): 
</p>
<p>ljl(t) = Us ljl{t') (schematic) (20.3.12) 
</p>
<p>Now note that although we use Us to propagate ljl forward in time, it can also 
</p>
<p>propagate it backward, since Us#-0 for t&lt;t'. To avoid this possibility explicitly, let 
</p>
<p>us work with 
</p>
<p>Gs(rt, r't') = 8(t- t') Us(rt, r't') (20.3.13) 
</p>
<p>which simply cannot propagate ljl backward. The equation satisfied by Gs is 
</p>
<p>(i ~- H)Gs= [i ~ 8(t- t')] L lj/n(r) ljl: {r') e-iE.U-n at at - n 
= io(t- t')o3(r- r') 
</p>
<p>=io4(x-x') [x={t,r)] (20.3.14) 
</p>
<p>577 
</p>
<p>THE DIRAC 
</p>
<p>EQUATION </p>
<p/>
</div>
<div class="page"><p/>
<p>578 
</p>
<p>CHAPTER 20 
</p>
<p>[We have used the completeness of the eigenfunctions, and B(t-t')=o(t-t').] 
</p>
<p>The propagator in Dirac theory, CiD, obeys a similar equation. Consider the 
</p>
<p>free-particle case. Here 
</p>
<p>/ a \ 
( &middot; &middot; ffO),rO &middot;,;:4( ') 1 0 - &middot; .. ,D=Iu x-x 
\ ct , 
</p>
<p>with 1!0 the free-particle Dirac Hamiltonian. The solution is 
</p>
<p>' 
( 'o( ,. ()( ') ( "" y&middot; JDX,x)= t&middot;-t L+L 
</p>
<p>Vi+ 
</p>
<p>(20.3.15) 
</p>
<p>(20.3.16) 
</p>
<p>where I,&plusmn; denote sums over positive- and negative-energy eigenfunctions, respec&middot;&middot; 
tively. [If we throw away In- we Jose completeness and won't get i84 on the right-
hand side of Eq. (20.3.15).] Although G(b satisfies the requisite equation, it has the 
</p>
<p>negative-energy solutions propagating forward in time. Now here is the trick. c;g is 
not a unique solution to Eq. (20.3.15): we can add or subtract any solution to the 
</p>
<p>free-Dirac equation, provided we subtract it for all times. (If we subtract it only for 
</p>
<p>t&gt;O, say, we are subtracting a 0 function times the solution, which doesn't obey the 
</p>
<p>homogeneous equation.) Let us subtract all negative-energy solutions for all times. 
</p>
<p>This gives us Feynman's propagator 
</p>
<p>Gi(x, x') = eu- t') I- O(t'- t) I (20.3.17) 
n-1-
</p>
<p>Consider now some initial state ~~, (t') which is composed of just positive-energy 
</p>
<p>solutions. Gj will propagate it forward in time, since if!;(t') is orthogonal to every 
term in Ln-. Thus G~if!;(t') = 1j11(t) contains only positive-energy components and 
keeps moving forward. On the other hand if f//;(1') is built out of negative-energy 
</p>
<p>components only, it is orthogonal to every term in In+ and gets propagated back-
wards from t' to t. We will see it as a positron propagating from t to t'. 
</p>
<p>Consider now the electron in some external potential V. The exact propagation 
</p>
<p>of the electron can be described by a perturbation series based on Gj; and in schem-
</p>
<p>atic form, 
</p>
<p>f/lr(t) = Gj(t, t'lVt;(t') +I GJ(t, t")V(t")G~(t", t')f/l;(t') + &middot; &middot; &middot; 
(" 
</p>
<p>We can represent these multiple scattering events by diagrams very much like the 
ones in Section 18.3. There is just one difference. Consider a second-order process. 
</p>
<p>There is of course the usual double scattering in which the electron just gets scattered 
forward in time (Fig. 20.2b). But now there is also the possibility that the potential 
</p>
<p>scatters it backward in time at I and then forward at 2 (Fig. 20.2c). As we move 
forward in time, we first see the electron, then an e + e- pair created at 2, then the 
</p>
<p>annihilation of the e+ with the original e &middot; at 1 and finally the arrival of the created 
</p>
<p>e- at f Since the electron can wiggle and jiggle any number of times (as we go to 
higher orders in the expansion) the intermediate stages can contain any number of </p>
<p/>
</div>
<div class="page"><p/>
<p>e + e- pairs. This is how the degrees of freedom proliferate in a relativistic theory. 
</p>
<p>Even though we started with a one-particle equation, particle production creeps in 
</p>
<p>through the negative-energy solutions-either because the latter imply an infinite sea 
</p>
<p>of sleeping particles which can be awakened or because they allow a single electron 
</p>
<p>to go back and forth in time, thereby becoming many particles at a given time. 
</p>
<p>Although particle production (at least pair production) can be handled in the present 
</p>
<p>formulation, it is time to learn quantum field theory, which provides a natural 
</p>
<p>framework for handling the creation and destruction of particles. We have already 
</p>
<p>seen one example, namely, the quantized electromagnetic field, whose quanta, the 
</p>
<p>photons, can be created and destroyed by operators a+ and a. We need a theory in 
</p>
<p>which particles like electrons and positrons can also be created and destroyed. You 
</p>
<p>are ready for that subjecd 
</p>
<p>t See for example J.D. Bjorken and S.D. Drell, Relativistic Quantum Mechanics and Relativistic Quantum 
Fields, McGraw-Hill, New York (1964), or C. Itzykson and J. B. Zuber, Quantum Field Theory, 
</p>
<p>McGraw-Hill, New York (1980). 
</p>
<p>579 
</p>
<p>THE DIRAC 
</p>
<p>EQUATION </p>
<p/>
</div>
<div class="page"><p/>
<p>21 
</p>
<p>Path Integrals: Part II 
</p>
<p>In this chapter we return to path integrals for a more detailed and advanced treat-
</p>
<p>ment. The tools described here are so widely used in so many branches of physics, 
</p>
<p>that it makes sense to include them in a book such as this. This chapter will be 
</p>
<p>different from the earlier ones in that it will try to introduce you to a variety of new 
</p>
<p>topics without giving all the derivations in the same detail as before. It also has a 
</p>
<p>list of references to help you pursue any topic that attracts you. The list is not 
</p>
<p>exhaustive and consists mostly of pedagogical reviews or books. From the references 
</p>
<p>these references contain, you can pursue any given topic in greater depth. All this 
</p>
<p>will facilitate the transition from course work to research. 
</p>
<p>In Chapter 8 the path integral formula for the propagator was simply postulated 
</p>
<p>and shown to lead to the same results as the operator methods either by direct 
</p>
<p>evaluation of the propagator (in the free particle case) or by showing once and for 
</p>
<p>all that the Schrodinger equation followed from the path integral prescription for 
</p>
<p>computing the time evolution. 
</p>
<p>We begin this chapter by doing the reverse: we start with the operator Hamil-
</p>
<p>tonian H=P 2/2m+ Vand derive the propagator for it as a path integral. We shall 
see that there are many types of path integrals one can derive. We will discuss 
</p>
<p>&bull; The configuration space path integral, discussed in Chapter 8. 
</p>
<p>&bull; The phase space path integral. 
&bull; The coherent state path integral. 
</p>
<p>You will see that the existence of many path integrals is tied to the existence of 
</p>
<p>many resolutions of the identity, i.e., to the existence of many bases. 
</p>
<p>Following this we will discuss two applications: to the Quantum Hall Effect 
</p>
<p>(QHE) and a recent development called the Berry Phase. 
</p>
<p>We then turn to imaginary time quantum mechanics and its relation to statistical 
</p>
<p>mechanics (classical and quantum) as well the calculation of tunneling amplitudes 
</p>
<p>by a semiclassical approximation. You will learn about instantons, the transfer matrix 
</p>
<p>formulation, and so on. 
Finally, we discuss path integrals for two problems with no classical limit: a 
</p>
<p>spin Hamiltonian and a fermionic oscillator. 581 </p>
<p/>
</div>
<div class="page"><p/>
<p>582 
</p>
<p>CHAPTER 21 
</p>
<p>21.1. Derivation of the Path Integral 
</p>
<p>Let us assume that the Hamiltonian is time-independent and has the form 
</p>
<p>The propagator is defined by 
</p>
<p>p2 
H=-+ V(X) 
</p>
<p>2m 
</p>
<p>U(xt; x'O) = U(x, x', t) = (xl exp( -~ Ht}x') 
</p>
<p>(21.1.1) 
</p>
<p>(21.1.2) 
</p>
<p>It was stated in Chapter 8 that U may be written as a sum over paths going from 
(x'O) to (xt). We will now see how this comes about. 
</p>
<p>First, it is evident that we may write 
</p>
<p>(21.1.3) 
</p>
<p>for any N. This merely states that U(t), the propagator for a time t, is the product 
of N propagators U(t/ N). Let us define 
</p>
<p>t 
e=-
</p>
<p>N 
(21.1.4) 
</p>
<p>and consider the limit N--+oo. Now we can write 
</p>
<p>( ie 2 ) ( ie 2) ( ie ) exp --,; (P /2m+ V(X)) ~exp - 2m1i P &middot; exp --,; V(X) (21.1.5) 
</p>
<p>because of the fact that 
</p>
<p>(21.1.6) 
</p>
<p>which allows us to drop the commutator shown (and other higher-order nested 
commutators not shown) on the grounds that they are proportional to higher powers 
of e which is going to 0. While all this is fine if A and B are finite dimensional 
matrices with finite matrix elements, it is clearly more delicate for operators in Hilbert 
space which could have large or even singular matrix elements. We will simply assume 
that in the limit e-+0 the ~ sign in Eq. (21.1.5) will become the equality sign for 
the purpose of computing any reasonable physical quantity. </p>
<p/>
</div>
<div class="page"><p/>
<p>So we have to compute 
</p>
<p>( ie 2) ( ie . ) ( ic 2) ( ic ) , (xi exp - lmfl P &middot; exp ---,; V(X) &middot; exp - lmfi P &middot; exp ---,; V(X) ... ix) 
</p>
<p>Ntimes 
</p>
<p>(2U.7) 
</p>
<p>The next step is to introduce the resolution of the identity: 
</p>
<p>I= r:o dxlx) (xi 
"""CO 
</p>
<p>(21.1.8) 
</p>
<p>between every two adjacent factors of U(t/ N). Let us illustrate the outcome by 
</p>
<p>considering N=3. We find (upon renaming x,x' as x 3 ,x0 for reasons that will be 
</p>
<p>clear soon) 
</p>
<p>f 2 (1 is ') ( is ) U{x3, Xo, t) = TI dx,.(x31 exp --- r exp ---- V(X) lx2) 
n~ J 2mfi fi 1 
</p>
<p>( ) ' ) is . 2 is , x (x2l exp -- P exp(--&middot; J;(X) lx1) 
2mli I li 1 
</p>
<p>( i&amp; ') ( i&amp; ) x (x1 1 exp - p- exp -- V(X) lxo) 
2mfi I fi I 
</p>
<p>(21.1.9) 
</p>
<p>Consider now the evaluation of the matrix element 
</p>
<p>(2U.l0) 
</p>
<p>When the rightmost exponential operates on the ket to its right, the operator X gets 
</p>
<p>replaced by the eigenvalue Xn -I . Thus, 
</p>
<p>(21.1.11) 
</p>
<p>Consider now the remaining matrix element. It is simply the free particle propagator 
</p>
<p>from Xn-I to Xn in time&amp;. We know what it is [say from Eq. (5.1.10)] or the following 
exercise 
</p>
<p>(21.1.12) 
</p>
<p>583 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>584 
</p>
<p>CHAPTER 21 
</p>
<p>Exercise 21.1.1. Derive the above result independently of Eq. (5.LIO) by introducing a 
resolution of the identity in terms of momentum states between the exponential operator and 
the position eigenket in the left-hand side of Eq. (21. Ll2). That is, use 
</p>
<p>Joo dp I= -lp)(pl 
2trli -oo 
</p>
<p>where the plane wave states have a wave function given by 
</p>
<p>which explains the measure for the p integration. 
</p>
<p>Resuming our derivation, we now have 
</p>
<p>[ m ]
112 
</p>
<p>[im(xn-Xn- 1)2] ( is ( ) = -- exp exp --Vi Xn-1) 
2ni1is 21is 1i 
</p>
<p>(2LLI3) 
</p>
<p>(2LL14) 
</p>
<p>(2l.l.l5) 
</p>
<p>Collecting all such factors (there are just three in this case with N = 3), we can readily 
see that for general N 
</p>
<p>U(xN, Xo, t) = ~-- n ~ dxn ( )I/2[JN-I( )1/2 J 
2mflS n~ I 27tlflS 
</p>
<p>(2l.l.l6) 
</p>
<p>If we drop the V terms we see that this is in exact agreement with the free particle 
path integral of Chapter 8. For example, the measure for integration has exactly N 
factors of B- 1 as per Eq. (8.4.8), of which N -I accompany the x-integrals. With 
the V term, the integrand is just the discretized version of exp(iS jli): 
</p>
<p>[
;, im(xn-Xn-d2 is Vi( )] 
</p>
<p>exp ~ Xn-1 
n~ I 2fls 1j 
</p>
<p>i ;, [m(xn-Xn-1 / 
=exp- s ~ 
</p>
<p>1i n~l 2s2 
(21.1.17) </p>
<p/>
</div>
<div class="page"><p/>
<p>We can go back to the continuum notation and write all this as follows: 
</p>
<p>U(x, x', t) = f [.s&amp;x] exp [ ~ f 2"(x, "i) dt l (21.1.18) 
where 
</p>
<p>f ( ) l/2f[iV~~~J( )1/2 l . m ' m [E0x] = hm -.-- n --. - dx,. 
N~oo 2n:z1i&pound; n~l 2n:tfie 
</p>
<p>(21.1.19) 
</p>
<p>The continuum notation is really a schematic for the discretized version that preceded 
it, and we need the latter to define what one means by the path integral. It is easy 
to make many mistakes if one forgets this. In particular, there is no reason to believe 
that replacing differences by derivatives is always legitimate. For example, in this 
problem, in a time &pound;, the variable being integrated over typically changes by 
(9(&amp;112) and not@(&amp;), as explained in the discussion before Eq. (8.5.6). The works 
</p>
<p>in the Bibliography at the end of this chapter discuss some of the subtleties. The 
continuum version is, however, very useful to bear in mind since it exposes some 
aspects of the theory that would not be so transparent otherwise. It is also very 
useful for getting the picture at the semiclassical level and for finding whatever 
connection there is between the macroscopic world of smooth paths and the quantum 
world. We will take up some examples later. 
</p>
<p>The path integral derived above is called the Configuration Space path integral 
or simply th&gt;: ]Jath integral. We now consider another one. Let us go back to 
</p>
<p>Ntimes 
</p>
<p>(21.1.20) 
</p>
<p>Let us now introduce resolutions of the identity between every exponential and 
the next. We need two versions 
</p>
<p>I= f dxix) (xi 
-.:o 
</p>
<p>(21.1.21) 
</p>
<p>f dp I= ~jp)(pi 
2n:ft 
</p>
<p>-- J:.) 
</p>
<p>(21.1.22) 
</p>
<p>where the plane wave states have a wave function given by 
</p>
<p>(xi P) = eipx/li (21. 1.23) 
</p>
<p>585 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>586 
</p>
<p>CHAPTER 21 
</p>
<p>Let us first set N = 3 and insert three resolutions of the identity in terms of 
</p>
<p>p-states and two in terms of x-states with x and p resolutions alternating. This 
gives us 
</p>
<p>(21.1.24) 
</p>
<p>where 
</p>
<p>I I' T&bull; j" X I J' 'f: &bull; x: N dp N ~' I 
l [~'P&pound;h]=j l &middot; &middot; &middot; I f1 ~n f1 dx, 
</p>
<p>40 ____ JC&middot; --,-x_..., -:r) -,-JC, ... __ ::1..! n= 1 2rcfl n= 1 
(21.1.25) 
</p>
<p>2N-l times 
</p>
<p>Evaluating all the matrix elements of the exponential operators is trivial since each 
</p>
<p>operator can act on the eigenstate to its right and get replaced by the eigenvalue. 
</p>
<p>Collecting all the factors (a strongly recommended exercise for you) we obtain 
</p>
<p>(21.1.26) 
</p>
<p>This formula derived for N=3 is obviously true for any N. In the limit N-&gt;rx!, i.e., 
</p>
<p>s-&gt;0, we write schematically in continuous time (upon multiplying and dividing the 
</p>
<p>middle term by s), the following continuum version: 
</p>
<p>,. [~ . ~ l '1 
U(x,x',t)= j [~2pf?x]exp~~.lo [px-Jf"(x,p)]dtJ (21.1.27) 
</p>
<p>where :If= p 2 /2m+ V(x) and (x(t), p(t)) are now written as functions of a continuous 
variable t. This is the Phase Space Path Integral for the propagator. The continuum 
</p>
<p>version is very pretty [with the Lagrangian in the exponent, but expressed in terms 
of (x, p)] but is only a schematic for the discretized version preceding it. 
</p>
<p>In our problem, since p enters the Hamiltonian quadratically, it is possible to 
</p>
<p>integrate out all the N variables p,. Going back to the. discretized form, we isolate </p>
<p/>
</div>
<div class="page"><p/>
<p>the part that depends on just p's and do the integrals: 
</p>
<p>N foo dpn [(-ic 2 i )] n -exp --pn+-pn(Xn-Xn-1) 
1 2rr1i 2m1i 1i 
</p>
<p>-oo 
</p>
<p>=O ~ exp lm(xn-Xn-1) N( )1/2 [. 2] 
1 2rr11ic 21ic 
</p>
<p>(21.1.28) 
</p>
<p>If we now bring in the x-integrals we find that this gives us exactly the configuration 
</p>
<p>space path integral, as it should. 
</p>
<p>Note that if p does not enter the Hamiltonian in a separable quadratic way, it 
</p>
<p>will not be possible to integrate it out and get a path integral over just x, in that we 
</p>
<p>do not know how to do non-Gaussian integrals. In that case we can only write down 
</p>
<p>the phase space path integral. 
</p>
<p>We now turn to two applications that deal with the path integrals just discussed. 
</p>
<p>The Landau Levels 
</p>
<p>We now discuss a problem that is of great theoretical interest in the study of 
</p>
<p>QHE (see Girvin and Prange). We now explore some aspects of it, not all having to 
</p>
<p>do with functional integrals. Consider a particle of mass Jl. and charge q in the x-y 
</p>
<p>plane with a uniform magnetic field B along the z-axis. This is a problem we discussed 
</p>
<p>in Exercise (12.3.8). Using a vector potential 
</p>
<p>we obtained a Hamiltonian 
</p>
<p>B 
A=- (-yi+xj) 
</p>
<p>2 
</p>
<p>H= [Px+qYBj2cf +[Py-qXB/2c]2 
</p>
<p>2p 2p 
</p>
<p>You were asked to verify that 
</p>
<p>Q= (cPx+qYB/2) 
</p>
<p>qB 
P= (Py-qBX/2c) 
</p>
<p>(21.1.29) 
</p>
<p>(21.1.30) 
</p>
<p>(21.1.31) 
</p>
<p>were canonical variables with [Q, P] =ill. It followed that H was given by the formula 
</p>
<p>(21.1.32) 
</p>
<p>and had a harmonic oscillator spectrum with spacing 1lm0 , where 
</p>
<p>mo=qBjpc (21.1.33) 
</p>
<p>587 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>588 
</p>
<p>CHAPTER 21 
</p>
<p>is the cyclotron frequency. In terms of 
</p>
<p>( )
1/2 ( )1/2 JUj)o . 1 
</p>
<p>a=- Q+t -- P 
211 2J1,0Jo1i 
</p>
<p>(21.1.34) 
</p>
<p>and its adjoint, we can write 
</p>
<p>(21.1.35) 
</p>
<p>We seem to have gone from a problem in two dimensions to a one-dimensional 
oscillator problem. How can that be? The point is that there is another canonical 
pair 
</p>
<p>P' = -'-( c_P_x ---=-q_YB_1'-12_;_) 
</p>
<p>qB 
Q' = (Py + qBXj2c) 
</p>
<p>which commutes with Q, P and does not enter H. 
</p>
<p>(21.1.36) 
</p>
<p>Exercise 21.1.2. If you do not recall the details of Exercise (12.3.8), provide all the 
missing steps in the derivation starting at Eq. (21.1.29) and ending with Eq. (21.1.35). Check 
the advertised commutation rules for (Q', P'). 
</p>
<p>The cyclic character of (Q', P') is reflected in the fact that the levels of the oscillator, 
called Landau Levels, are infinitely degenerate. To see this degeneracy consider the 
Lowest Landau Level, abbreviated LLL. The states in this level obey the equation 
</p>
<p>aiO)=O 
</p>
<p>which becomes in the coordinate representation 
</p>
<p>[ o qB J * -+-z lf/o(z, z )=0 
oz* 41ic 
</p>
<p>wherein we have switched to complex coordinates 
</p>
<p>If we make the ansatz 
</p>
<p>z=x+iy z* =x- iy 
</p>
<p>lf/o(z, z*) = exp[- qB zz*]u(z, z*) 
41ic 
</p>
<p>we find the beautiful result 
</p>
<p>0 
- u(z z*) =0 
oz* ' 
</p>
<p>(21.1.37) 
</p>
<p>(21.1.38) 
</p>
<p>(21.1.39) 
</p>
<p>(21.1.40) 
</p>
<p>(21.1.41) </p>
<p/>
</div>
<div class="page"><p/>
<p>as the defining rule for the LLL. Thus u is any analytic function, i.e., function of the 
</p>
<p>combination x + iy. The family of such functions is clearly infinitely large, with the 
monomials [ zm I m = 0, 1, 2, ... ] serving as a linearly independent basis. Thus the 
ground state function lf/o is not a unique function as in the case of the truly one 
</p>
<p>dimensional oscillator but a superposition of functions of the form 
</p>
<p>(21.1.42) 
</p>
<p>I now make the following assertions: 
</p>
<p>&bull; For large m the probability density for the particle is concentrated at some radius 
rm=J2fflr0 where r0 =.jdijqB is called the magnetic length. 
</p>
<p>&bull; If the system is not infinite in size, but is a disc of radius R, the biggest value of 
</p>
<p>m that can fit in, and hence N, the number of LLL states that fit into the disc, is 
</p>
<p>given by 
</p>
<p>(21.1.43) 
</p>
<p>where the numerator is the flux through the sample and the denominator is the flux 
</p>
<p>quantum of Eq. (18.4.39): 
</p>
<p>&lt;Do= 2nFic 
q 
</p>
<p>(21.1.44) 
</p>
<p>Exercise 21.1.3. &bull; (Mandatory if you wish to follow the discussion of the QHE.) Derive 
</p>
<p>the equation for the LLL in the coordinate representation by providing the missing steps in 
</p>
<p>the derivation. Prove the above assertions. Note that N, the number of states in the LLL, is 
</p>
<p>given by the flux through the sample in units of the flux quantum. 
</p>
<p>In the following discussion we will hold N, i.e., the field and sample dimensions, 
</p>
<p>fixed. 
In the study of the QHE one is interested in the problems of an electron gas 
</p>
<p>designed to live in two dimensions. (Since the electron charge is q = -e, our formulas 
</p>
<p>will hold with q = e if the sign of the vector potential and field are reversed at the 
</p>
<p>outset. Henceforth imagine this has been done and that q stands for the magnitude 
</p>
<p>of the electron charge.) The electron spin is frozen along the applied field and has 
</p>
<p>no interesting dynamics. In particular it is the burden of the orbital wave function 
</p>
<p>to ensure antisymmetry. In a real-life problem one is also required to consider the 
</p>
<p>interaction between the electrons as well as interaction between the electrons and 
any external scalar potential V(x, y) due to the background medium. It is assumed 
</p>
<p>that both these interactions have a scale much smaller than the gap FiqB/Jlc between 
Landau levels. Thus at low temperatures, one would like a simplified problem with 
</p>
<p>the Hilbert space restricted to the LLL. What does this problem look like? 
</p>
<p>The path integral can tell us that. We will work out the answer for the case 
</p>
<p>where electron-electron interaction is zero. (Only then do the electrons propagate 
</p>
<p>589 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>590 
</p>
<p>CHAPTER 21 
</p>
<p>independently and we can write out a functional integral for just one electron.) The 
action is 
</p>
<p>(21.1.45) 
</p>
<p>where the terms linear in velocity represent the (qjc)v&middot; A in the Lagrangian in the 
gauge we are using. To get the low-energy physics we must banish the higher Landau 
levels. Since the gap to the higher levels is 1iqB j J.1. c this is readily done by taking the 
limit J.1. --&gt;0. (In this limit the zero point energy of the oscillator, which gives the 
energy of the LLL, diverges. It is assumed this constant is subtracted out of the 
Hamiltonian.) This gives us the low-energy action 
</p>
<p>(21.1.46) 
</p>
<p>where we have done an integration by parts to combine the two terms linear in 
velocity. (The surface term will not affect the equations of motion.) 
</p>
<p>Notice the interesting result that the action is that of a phase space path integral 
withy and oft' joy= (qBjc)x=x as canonically conjugate variables. V(y, x) now plays 
the role of the Hamiltonian for this problem. Since we have just one coordinate and 
one momentum, the problem of the LLL is essentially one-dimensional. 
</p>
<p>In the semiclassical picture, the orbits will obey Hamilton's equations: 
</p>
<p>. av 
y= ax 
</p>
<p>. av 
x=--
</p>
<p>oy 
(21.1.47) 
</p>
<p>and one can try to do Bohr-Sommerfeld quantization. At the quantum level, V can 
become a complicated differential operator since x will turn into the y-derivative. I 
leave the details and applications of the semiclassical picture to the references. 
</p>
<p>Now you might object that if we did not have the operator solution telling us 
that the levels of the problem go as p-I it might not occur to us to consider the limit 
p --&gt;0 in order to isolate the low energy physics. This is not so. We will simply argue 
that in the limit of low energies, i.e., low frequencies, terms in the action with higher 
time derivatives can be neglected compared to those with fewer ones. This would 
allow us to throw out the same kinetic energy term. (Now you can do this even in 
a problem without the magnetic field, but this would leave you with very little 
interesting dynamics. Here we have some linear derivatives left over, i.e., here the 
low-energy physics is the physics of the entire infinitely degenerate LLL.) In problems 
where such nontrivial dynamics is left, one usually finds that variables that used to 
commute become canonically conjugate. 
</p>
<p>Exercise 21.1.4. Study the semiclassical orbits and show that the motion is on contours 
of constant V. (Hint: Consider the gradient of V.) </p>
<p/>
</div>
<div class="page"><p/>
<p>How can X and Y suddenly become noncommuting when by postulate they are 
</p>
<p>commuting? The answer is simply that if two matrices commute in a given space 
</p>
<p>(the full Hilbert space), their truncations to a subspace (here the states of the LLL) 
</p>
<p>need not. What is nice is that the commutator of X and Y, instead of being something 
</p>
<p>ugly is a constant, making the pair canonically conjugate (upon trivial rescaling). 
</p>
<p>Exercise 21.1.5. Consider the commuting 3 x 3 matrices Q and A from Exercise (1.8.10). 
</p>
<p>If you truncate them by dropping the third row and column, show that the 2 x 2 truncations 
</p>
<p>do not commute. 
</p>
<p>Consider a finite system with N electrons, i.e., a system with a fully filled LLL, 
</p>
<p>with one electron per state in the LLL. Ignore all interactions between the electrons 
</p>
<p>or with the medium. What is its ground state? Since their spins are polarized along 
</p>
<p>the field, the spatial wave function must be antisymmetric in the electron spatial 
</p>
<p>coordinates and be analytic. An unnormalized product wave function for the N 
</p>
<p>particles is 
</p>
<p>n 1 o 1 2 N -I ( qB " * ) ( qB " * ) T p = Z1Z2Z3 ... ZN exp -- L..&middot; Z; Z; :=Up exp -- &pound;...,&middot; Z; Z; 
41ic ' 41ic ' 
</p>
<p>When antisymmetrized, this leads to 
</p>
<p>N i-I 
</p>
<p>uA= TI TI (z;-zj) 
i~I j~I 
</p>
<p>(21.1.48) 
</p>
<p>(21.1.49) 
</p>
<p>Exercise 21.1.6. Verify the above equation for the three particle case. Show this also by 
</p>
<p>writing out the (3 x 3) determinant as in Eq. (10.3.36). (In all these manipulations, the expo-
</p>
<p>nential factor in the wave function, which is totally symmetric in the coordinates, plays no 
</p>
<p>part.) 
</p>
<p>This wave function is unique since there is just one way to place N (spin polarized) 
</p>
<p>electrons in N states. So we know the unique ground state for the fully filled LLL 
</p>
<p>in the noninteracting limit. But even if we consider the interactions between electrons, 
</p>
<p>this is the only antisymmetric wave function we can write for this problem where 
</p>
<p>the number of states equals the number of electrons, if we do not want to go above 
the LLL. 
</p>
<p>Now, the really interesting problem is one where in the same field and sample, 
</p>
<p>we have a smaller number vN of electrons where ljv is an odd integer. (This is one 
</p>
<p>of the cases where the experiments show surprising results.) We say the system has 
</p>
<p>a filling factor v meaning it has v times the maximum allowed number of particles 
</p>
<p>in the LLL. The fully filled LLL is thus given by v = 1. Whereas previously we put 
</p>
<p>an electron in each LLL state (and there was just one way to do it and hence one 
</p>
<p>antisymmetric wave function), now there is more than one way and hence many 
</p>
<p>possible superpositions of LLL wave functions that can be candidates for the ground 
</p>
<p>591 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>592 
</p>
<p>CHAPTER 21 
</p>
<p>state of a system of electrons interacting via the Coulomb potential. Laughlin pro-
posed the following wave function: 
</p>
<p>vN i-1 
</p>
<p>Uv= n n (zi-zj)Ijv 
i~I j~I 
</p>
<p>(21.1.50) 
</p>
<p>Let us verify that this wave function fits the description. Pick any one particle 
coordinate, say z1 (since the particles are identical) and observe that the highest 
power that occurs is (for large N) 
</p>
<p>Thus the size of the biggest wave function precisely matches the sample size. Next 
note that the function is antisymmetric under exchange of any two coordinates since 
1/ v is odd. Lastly note that the electrons nicely avoid each other (due to the high-
order zero when any two coordinates approach each other) thereby minimizing their 
repulsive interaction. Not surprisingly, this happens to be an excellent ground state 
wave function at these filling factors, (for small 1/v). 
</p>
<p>The Berry Phase 
</p>
<p>The problem in question has to do with the adiabatic approximation. Recall 
the example of the particle in a box of size L. Let us say it is in the ground state. 
Suppose the box slowly expands with time as some function L(t). The adiabatic 
principle states that if the expansion is slow enough, the particle will be in the ground 
state of the box of size L( T) at time T. Likewise the particle that starts out in the 
state ln(L(O))) will find itself in the instantaneous eigenstate ln(L(t))) at timet. 
</p>
<p>More generally, if the particle Hamiltonian is given by H(R(t)) where R is 
some external coordinate which changes slowly and appears parametrically in H, 
the adiabatic principle tells us that the particle will sit in the nth instantaneous 
eigenket of H(R(t)) at a time t if it started out in the nth eigenstate of H(R(O)). 
</p>
<p>What is the solution to the Schrodinger equation in this approximation? Here 
is a reasonable guess: 
</p>
<p>(21.1.51) 
</p>
<p>where 
</p>
<p>H(t)in(t)) = En(t)ln(t)) (21.1.52) 
</p>
<p>First note that if H does not vary with time, the above answer is clearly correct, 
with the phase factor appropriate to energy En. The above formula recognizes that 
the instantaneous energy varies with time and gives the accumulated phase shift, just 
as the WKB wave function gives the phase as the spatial integral of a position-
dependent momentum for a particle moving in a nonconstant V(x). </p>
<p/>
</div>
<div class="page"><p/>
<p>Over the years, many people, notably Herzberg and Longuet-Higgins and Mead 
</p>
<p>and Truhlar, recognized various problems with this formula and found ways to fix 
</p>
<p>them. The whole problem was brought into sharp focus and synthesized by Berry. 
</p>
<p>You are urged to read his very lucid writings and the collection of related papers 
</p>
<p>(with helpful commentary) edited by Shapere and Wilczek, referred to in the Bibli-
</p>
<p>ography at the end of the chapter. 
</p>
<p>To see what is missing in the above ansatz, let us modify it as follows: 
</p>
<p>(21.1.53) 
</p>
<p>where the extra factor c(t) must be equal to unity if the old ansatz is right. Let us 
</p>
<p>apply the Schrodinger equation to this state: 
</p>
<p>(i1i :t-H(t)) V'(t)) =0 (21.1.54) 
</p>
<p>When the time derivative acts, it generates three terms: one from the derivative of 
</p>
<p>the accumulated phase factor (which neutralizes the action of H on the eigenket), 
</p>
<p>one from the derivative of c(t) and one from the derivative of the instantaneous 
</p>
<p>eigenket. The last two terms lead to the following equation (on dotting both sides 
</p>
<p>with the instantaneous bra): 
</p>
<p>with a solution 
</p>
<p>c(t) = -c(t)(n(t)l !!_ ln(t)) 
dt 
</p>
<p>c(t)=c(O) exp(-r (n{t')l :r, in(t')) dt')=c(O) e;r 
0 
</p>
<p>f t d r=i (n(t')l-ln(t')) dt' 
0 dt' 
</p>
<p>(21.1.55) 
</p>
<p>(21.1.56) 
</p>
<p>(21.1.57) 
</p>
<p>The impressive thing is not to find this extra phase, called the Berry phase or the 
</p>
<p>geometric phase, but to recognize that it can have measurable consequences. After 
</p>
<p>all, we have been learning all along that the phase of a ket makes no difference to 
</p>
<p>any measurable quantity. Since the instantaneous kets themselves are defined only 
</p>
<p>593 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>594 
</p>
<p>CHAPTER 21 
</p>
<p>up to a phase factor, we can choose a new set and modify the extra phase. If we 
choose 
</p>
<p>ln'(t))=eix(r) ln(t)) (21.1.58) 
</p>
<p>then we find 
</p>
<p>d d dx(t) 
i(n'(t)i-ln'(t))=i(n(t)l-ln(t))---
</p>
<p>dt dt dt 
(21.1.59) 
</p>
<p>suggesting that perhaps we could choose x(t) so as to completely neutralize the extra 
phase. It had been generally assumed that such a choice could always be made and 
the extra phase forgotten. 
</p>
<p>Suppose now that the parameter that changes with time and causes the Hamil-
tonian to change returns to its starting value after time T so that: 
</p>
<p>H(T)=H(O) (21.1.60) 
</p>
<p>Now it is no longer obvious that we can get rid of the extra phase. We find 
</p>
<p>i ~ (n'(t)i !!_ ln'(t)) dt = i ~ (n(t)l !!_ in(t)) dt- (X(T)- x(O)) 
J dt J dt (21.1.61) 
</p>
<p>Now the choice of phase factors is quite arbitrary, but it must meet the requirement 
that the assignment is single-valued, at least within the region containing the closed 
loop in question. (A single-valued choice in the entire parameter space will generally 
be impossible. This is a subtle topic, reserved for Exercise (21.1.15).) So let us start 
with such a basis ln(t)) and make a switch to another one ln'(t)) =eix&lt;t&gt; ln(t)). Since 
the new basis is by assumption single-valued, so must be the additional phase factor. 
In other words, (x(T)- x(O)) =2m7r, where m is an integer. This in turn means that 
the prefactor /" =exp[i f (n(t)li (d/dt)in(t)) dt] arising in a closed circuit cannot be 
altered by a choice of basis. Note also that since dt cancels out in any of the integrals, 
we cannot shake this phase by slowing down the rate of change of the parameter. 
The phase factor depends only on the path in parameter space, which explains the 
name "geometric phase." Note that we have not shown that eir #I, but only that 
its value is not affected by redefinition of phases for the state vectors. 
</p>
<p>So let us suppose we have a nonzero y. What exactly does it do? To see this, 
let us consider a problem where the box is not really a box, but the potential of 
some heavy object. For example, let R be the coordinate of some nucleus and r that 
of an electron that is orbiting around it. In this discussion we will deviate from our 
usual notation: capital letters will stand for nuclear coordinates and momenta (class-
ical or quantum) and lowercase letters will represent the electron. We will also 
temporarily ignore the vector nature of these variables. The box here is the Coulomb 
well created by the nucleus. As the nucleus moves, the box moves, rather than change 
size, but the issues are the same. As the nucleus crawls from place to place, the 
nimble electron stays in the instantaneous eigenstate. Even though we have paid no </p>
<p/>
</div>
<div class="page"><p/>
<p>attention to the dynamics of the nucleus, we shall see one is generated by the Berry 
</p>
<p>phase. Let us rewrite the phase factor as follows: 
</p>
<p>exp(-f' (n(t')l ~ ln(t')) dt'J 
0 dt ' 
</p>
<p>( 
. l't l ) 
</p>
<p>= exp !__ iPi J &lt; n( t') I !'_ .. In( t')) dt' 
, Pi O dt' 1 
</p>
<p>(. f' . d dR ' =exp !_ iPi(n(R(t'))l -ln(R(t')))- dt') 
fi 0 . dR dt' 
</p>
<p>= exp !__ A"(R)- dt' ( &middot; ft dR ) 
\ fj 0 dt' ' 
</p>
<p>An(R) = iPi(n(R)I !!__ ln(R)) 
dR 
</p>
<p>where 
</p>
<p>(2l.l.62) 
</p>
<p>(2l.l.63) 
</p>
<p>(21.1.64) 
</p>
<p>(2l. L65) 
</p>
<p>Thus we see that the slow nuclear degree of freedom has a velocity coupling to a 
</p>
<p>vector potential A 11(R), called the Berry potential. The potential depends on which 
</p>
<p>quantum state In) the electronic degree of freedom is in. When the state vectors 
</p>
<p>are redefined by phase transformations, this vector potential undergoes a gauge 
</p>
<p>transformation: 
</p>
<p>ln(R))--&gt;ei;r(Rl ln(R)) (21.1.66) 
</p>
<p>(2l.l.67) 
</p>
<p>Howeuer, its line integral around a closed loop is gauge inuariant and could be nonzero. 
</p>
<p>To ignore this would be to get the wrong dynamics for the nucleus. 
</p>
<p>Now some of you may feel a little unhappy and say: "I know how the vector 
</p>
<p>potential is supposed to enter the Lagrangian or action, but you pulled it out of a 
</p>
<p>phase factor in the wave function of the fast coordinates." This is a fair objection 
</p>
<p>and in answering it in some detail we will learn that there is also a scalar potential 
besides the vector potential. 
</p>
<p>We begin by constructing a path integral for the nuclear degrees of freedom. 
What resolution of the identity should we use? The one appropriate to our problem 
</p>
<p>is this: 
</p>
<p>I' 
</p>
<p>I= J dR ~ IR, n(R)) (n(R), Rl (21.1.68) 
</p>
<p>595 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>596 
</p>
<p>CHAPTER 21 
</p>
<p>where IR, n(R))=IR)&reg;In(R)). In other words, at each R, we pick a basis for the 
</p>
<p>electrons that diagonalizes the instantaneous electronic Hamiltonian H,(R, r,p) 
</p>
<p>He(R, r,p)IR, n(R)) =En(R)IR, n(R)) (2!.1.69) 
</p>
<p>Of course, you can pick a basis for the electrons that has no correlation to the nuclear 
</p>
<p>coordinates. While this is mathematically correct, it is not wise for the adiabatic 
</p>
<p>approximation. For the latter, we now make the approximation that if the electron 
</p>
<p>starts out at some value of n, it stays there and aU other values can be ignored. Thus 
</p>
<p>we write: 
</p>
<p>I~ J dRIR, n(R)) (n(R), Rl (21.1.70) 
where the sum on n has been dropped. The derivation of the configuration space 
path integral in R proceeds as usual. A typical factor in the path-integrand will be 
</p>
<p>it: it: [ l [ J (n(R(t+ t:)), R(t + e)l exp -&middot;~&middot; H(R, P) exp --h- He(R, r, p) ln(R(t)), R(t)) 
(21.1.71) 
</p>
<p>The nuclear part, sandwiched between nuclear coordinate eigenstates, will give the 
</p>
<p>usual factor 
</p>
<p>(R(t + t:)i exp[ _i: H(R, P) }R(t)) 
</p>
<p>= ;-;;;- exp[.0_ [. m" (R(t+ e) R(t))2 - V(R)J&middot;J&middot; 
v~ n 2e&middot; 
</p>
<p>(21.1.72) 
</p>
<p>while the electronic exponential will act on its eigenket to the right and give a factor 
</p>
<p>exp [ -(it:/fi)En(R)] which will change the nuclear potential by E,(R). This is how 
</p>
<p>Born and Oppenheimer analyzed molecules, where there is a clear separation of fast 
</p>
<p>(electronic) and slow (nuclear) degrees of freedom: fix the slow ones, solve for the 
</p>
<p>fast ones at this value, and use the fast eigenenergies as an additional potential for 
</p>
<p>the slow problem which is then solved. 
</p>
<p>But this is not the full story. After the electronic exponential has acted on its 
</p>
<p>eigenket to the right, yielding the exponential phase factor exp[ -(if:jn)En(R)], we 
are still left with the following dot product which multiplies everything: 
</p>
<p>(n(R(t+ D))ln(R(t))) = (n(R')In(R)) (21.1.73) 
</p>
<p>All the results will follow from an analysis of this factor. First, it is true that when 
R=R' this factor equals unity. We are going to perform a Taylor expansion of this 
</p>
<p>product in the difference R- R' = TJ. How far should we go? The answer is clear if 
we recall Chapter 8 where we derived the Schrodinger equation from the path integral </p>
<p/>
</div>
<div class="page"><p/>
<p>by considering the propagator for infinitesimal times, i.e., one time slice of width s. 
I reproduce the relevant formula Eq. (8.5.7) with two changes. I drop all interactions 
</p>
<p>and keep just the free particle propagator but I append the dot product (n(R')In(R)). 
</p>
<p>This yields for the nucleus 
</p>
<p>lf!(R', s) = (~ f2 I oc eimry'flli&pound; (n(R')!n(R' + 17) &gt; lf!(R' + 17, 0) dTI (21.1. 74) 
2rc1izs; _00 
</p>
<p>The exponential allows 17 to fluctuate by (recall Eq. (8.5.6)) 
</p>
<p>' 1C E: 
(
</p>
<p>2 rz ) 1/ 2 
117i~&shy;
</p>
<p>m 
(21.1.75) 
</p>
<p>This means we must go to order 17 2 since we want to go to order s to derive the 
Schrodinger equation. So we expand V' and (n(R')!n(R' + 17)) to this order: 
</p>
<p>2 
</p>
<p>(n(R')!n(R' + 77)) = 1 + 1J(n!on) +!1_ (n!82n) + &middot; &middot; &middot; 
2 
</p>
<p>(21.1.76) 
</p>
<p>(21.1.77) 
</p>
<p>where all derivatives are taken at the point R' and !8n) is the derivative of In) with 
</p>
<p>respect to R' and so on. If we now inject these expansions into Eq. (21.1. 74), and 
</p>
<p>keep just the even powers of 1J as we did in Chapter 8, we find upon doing the 
</p>
<p>Gaussian integrals and dropping the prime on R' 
</p>
<p>Exercise 21.1.7. * Provide the missing steps leading to the above equation. 
</p>
<p>The Hamiltonian can be read off the above: 
</p>
<p>H=-1 (P- A")"+&lt;l&gt;" 
2m 
</p>
<p>A"= in(n! on) 
</p>
<p>(21.1.78) 
</p>
<p>(21.1.79) 
</p>
<p>(21.1.80) 
</p>
<p>( 21.1.81) 
</p>
<p>Exercise 21.1.8. * Providing the missing steps. Use (n! an)=- (anln) which follows from 
a(nln)=O. The potential &lt;I&gt;" arises from adding and subtracting the (A") 2 term which isn't 
there to begin with. 
</p>
<p>597 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>598 
</p>
<p>CHAPTER 21 
</p>
<p>Figure 21.1. The field B2 and electron motion are along the circle. The 
</p>
<p>particle spin is up or down the local magnetic field which is the sum 
</p>
<p>of B, and B2 . The current I produces B,. 
</p>
<p>The ( discretized) action function which will give exactly these results will have 
</p>
<p>the v &middot;A" term (with An evaluated at the midpoint) and the extra scalar potential &lt;D 11 &bull; 
</p>
<p>We will not write that down since we have the Hamiltonian. The following exercise 
</p>
<p>considers this point more carefully. 
</p>
<p>Exercise 21.1. 9. Suppose we do not derive the Hamiltonian as above (by invoking the 
</p>
<p>wave function) but want to determine the correct discretized action function starting with Eq. 
</p>
<p>(21.1.71) and expanding (n(R')In(R)) to quadratic order in R'- R as per Eq. (21.1.77) and 
</p>
<p>exponentiating the result. Do all of the above and show that the argument of the vector 
</p>
<p>potential that arises is not at the midpoint to begin with, as it should to represent the effect 
</p>
<p>correctly [Exercise (8.6.4)]. Fix this with a Taylor series, combine the term quadratic in R'- R 
</p>
<p>that arises, with the one you had to begin with, to obtain (for one time slice) 
</p>
<p>im(R'- i , , "('R+R'J' 
S= . 2Pit: ~- (R .... RJA ,-2-
</p>
<p>(R' R)' 
</p>
<p>2 &lt;ani(! &middot;ln)&lt;n&lt; (2l.I.IQ) 
</p>
<p>Let us now ask what continuum form this describes. Multiplying and dividing by e converts 
</p>
<p>the first term into the kinetic energy and the middle term to the vector potential coupling. 
</p>
<p>The last term needs to be multiplied and divided by t:2 to become the square of the velocity. 
</p>
<p>But this would leave it with an extra t: in the continuum action. Despite this, the term is 
</p>
<p>important since the square of the velocity is very singular. The effect of the term is best revealed 
</p>
<p>by noting that the factor (R'- R) 2 is going to be replaced by it:Pi/m when the functional integral 
</p>
<p>is done, (because of the kinetic energy term in the action that controls the variance of R- R'), 
</p>
<p>make this replacement now, and convert this term to the scalar potential &lt;D ", which we know 
</p>
<p>describes the right Hamiltonian. The role of such terms, naively vanishing in the continuum 
</p>
<p>limit has been discussed by Klauder. Klauder and Skagerstam (1985). 
</p>
<p>It should be clear that the preceding results generalize with R and A replaced 
</p>
<p>by vectors R. A, more fast and slow degrees of freedom, etc. 
We turn to a simple problem where the Berry potential makes a ditference.t 
</p>
<p>Consider the situation in Fig. 21.1. 
A spinless, electrically neutral particle of mass M is restricted to move on a 
</p>
<p>circle of radius a. This motion is going to be the slow degree of freedom in our 
</p>
<p>problem. The orbit is penetrated by a flux due to a field B1k along the z-axis. In 
</p>
<p>addition, a wire carrying some current along the z-axis is introduced at the center. 
</p>
<p>t I thank Ady Stem for suggesting a variant of this example. He is not responsible for any errors in my 
presentation. </p>
<p/>
</div>
<div class="page"><p/>
<p>It produces an azimuthal field of strength B2 &bull; The total field makes an angle 
</p>
<p>fJ =arctan BdB1 
</p>
<p>with respect to the z-axis and has a magnitude B = .j.Bf+m. When the particle 
coordinate is cp, the field B2 is tangent to the circle, i.e., has an azimuthal angle 
cp + rr: /2 in B-space. The Hamiltonian of the particle (not yet coupled to B) is: 
</p>
<p>L2 
H=---
</p>
<p>21 
(21.1.83) 
</p>
<p>where I= M a2 is the moment of inertia, set equal to 1/2 from now on and L = 
-i1i iJjiJcp is the angular momentum operator. The energy eigenvalues are 
</p>
<p>m=O, &plusmn;I, &plusmn;2 ... (21.1.84) 
</p>
<p>We now bring in the fast degree of freedom. Imagine that the particle has 
</p>
<p>spin 1/2. As the particle goes around the circle, the spin will see a varying magnetic 
</p>
<p>field, B, which is the vector sum of the fixed field B1 along the z-axis and the azimuthal 
</p>
<p>field B2. We modify Has follows: 
</p>
<p>(21.1.85) 
</p>
<p>where C and hence the splitting between the two spin states is assumed to be so 
</p>
<p>large (as is the frequency associated with the splitting) that the spin is truly a fast 
</p>
<p>degree of freedom which will not jump between its states as the particle crawls around 
</p>
<p>the loop. 
</p>
<p>What will the allowed energies be? The naive answer is 
</p>
<p>(21.1.86) 
</p>
<p>where B=JBi+ B~ and the two signs correspond to the spin pointing up/down the 
local magnetic field as the particle goes round and round. This is however wrong 
</p>
<p>and one must take into account the Berry potentials A(cp) and &lt;I&gt;. Let us focus on 
</p>
<p>the lower-energy solution in which the spin points up the local field. We choose the 
</p>
<p>spinor to be 
</p>
<p>[ e l cos-.., 10&cent;)= ~ &bull; (} i,P 
ism i e 
</p>
<p>(2l.l.87) 
</p>
<p>(The additional i in the lower component is due to the fact that orbital angle &cent; 
differs from the azimuthal angle of the field by rr: /2 as is clear from Fig. 21.1.) It is 
</p>
<p>599 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>600 
</p>
<p>CHAPTER 21 
</p>
<p>readily found that 
</p>
<p>, . a &middot;2o 
A (ify)=di(0&cent;1, !0&cent;)=-flsm 
</p>
<p>o&cent; 2 
(2!.1.88) 
</p>
<p>which is independent of&cent;, and that the scalar Berry potential is 
</p>
<p>(21.1.89) 
</p>
<p>which is independent of whether the spin is pointing up or down the local field. Since 
</p>
<p>(} is fixed in this problem, &lt;I&gt; can be eliminated by a choice of reference energy, and 
</p>
<p>we no longer consider it. 
</p>
<p>Exercise 21.1.10. Prove the above equations for the vector and scalar potentials. 
</p>
<p>Since the effect of the vector potential is L-+ L .... A', it follows that if we solve 
</p>
<p>(21.1.90) 
</p>
<p>the energy is given by 
</p>
<p>(21.1.91) 
</p>
<p>The orbital eigenfunctions are once again 
</p>
<p>m=O, &plusmn;1, &plusmn;2,. (21.1.92) 
</p>
<p>so that 
</p>
<p>, " 4 + ( . 2 e ')" Jc=mn-,. =\m+sm l n (2!.1.93) 
</p>
<p>and the energy of the spin up state is 
</p>
<p>(21.1.94) 
</p>
<p>It is evident that without the vector potential we would get the wrong answer. For 
</p>
<p>example, without it, there would be a twofold degeneracy under m-+---m. 
</p>
<p>Exercise 21./.ll. Find the potential for the other (spin down) state and the energy 
</p>
<p>eigenvalues. </p>
<p/>
</div>
<div class="page"><p/>
<p>Let us rederive the scalar and vector potentials of Eqs. (21.1.79- 21.1.81) without 
</p>
<p>path integrals, by extracting the effective Hamiltonian that acts on the slow degrees 
</p>
<p>of freedom R. Now the latter need not be in an eigenstate of position, it could be 
</p>
<p>in a superposition ~p(R): 
</p>
<p>l~p)= f lJI(R)IR,n(R))dR (21.1.95) 
</p>
<p>Note that I 1p) is a ket in the direct product space of the slow and fast degrees of 
freedom. Usually the coefficients in such a superposition would depend on both 
</p>
<p>labels. But in our problem the fast degree of freedom is slaved to the slow one, so 
</p>
<p>that the amplitude for the slow one to be in I R) is the same as the amplitude for 
the entire system to be in IR, n(R)). We are going to find the Hamiltonian in the 
</p>
<p>coordinate representation by calculating 
</p>
<p>(HlJI)(R') == (R', n(R')IHI ~p) (21.1.96) 
</p>
<p>= f (R', n(R')IHI R, n(R)) (R, n(R)IlJI) dR (21.1.97) 
</p>
<p>= f (R', n(R')IHIR, n(R))lJI(R) dR (21.1.98) 
Let 
</p>
<p>(21.1.99) 
</p>
<p>It is evident that the fast Hamiltonian H1, acting to the right on its eigenket, 
</p>
<p>will give En(R) and that this will join with V(R) to provide a potential energy term. 
</p>
<p>We focus therefore on just the P 2 /2M since here is where the action is. Let us recall 
</p>
<p>that 
</p>
<p>p2 liz 
(R'I 2MIR)=---- 2Mo"(R'---R) (21.1.100) 
</p>
<p>and insert it into Eq. (21.1.98) to obtain 
</p>
<p>1i2 f (HlJI)(R') = -- (n(R')in(R))8"(R'- R)lJI(R) dR 
2M 
</p>
<p>1i2 
=-- (n(R')I [I o2n(R))lJI(R) + 21on(R))ii~p(R) + ln(R) )il2 lJf(R)JR---R' 
</p>
<p>2M 
</p>
<p>(21.1.101) 
</p>
<p>601 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>602 
</p>
<p>CHAPTER 21 
</p>
<p>where o denotes derivatives respect toR'. It is now straightforward to show that the 
operator on the right-hand side is indeed the one in Eq. (21.1.79). The details are 
left to the following exercise. 
</p>
<p>Exercise 21.1.12. Provide the missing details. Suggestion: Start with Eq. (21.1.79) and 
expand out the (P- A) 2 &bull; Note that when P comes to the left of A, it differentiates both A 
and the wave function lJI that is imagined to be sitting to the right of the Hamiltonian. Now 
go to Eq. (21.I.l01), add and subtract the A2 term and regroup the terms using relations like 
o(nliln) = (ilnliln) + (nlil2n). 
</p>
<p>Now that we accept the reality of the Berry vector potential, let us understand 
it a little better. Normally when we have a vector potential, we take its curl and the 
corresponding magnetic field has as its origin some current. Had there been magnetic 
monopoles, the source could have been a monop&lt;;&gt;le. What is producing the Berry 
potential? Let us first appreciate that the source of the potential does not lie in the 
configuration space of the fast degree of freedom, but in the space of parameters 
that are slowly varying in the fast Hamiltonian Hf. Of course, this slow parameter 
could itself be a real live degree of freedom (as in our ring example) but this is not 
our focus. We simply treat the slow variables as external parameters that define Hf. 
Thus if we consider a spin-1 /2 object with 
</p>
<p>H=-cr&middot;B (21.1.102) 
</p>
<p>then the Berry potential lives in B space. (Since we focus on just the fast variables, 
we drop the subscript on Hf.) To ease our thinking we are going to rename B space 
as R space, but you should not forget this fact. So we write 
</p>
<p>H=-cr&middot;R (2l.l.l03) 
</p>
<p>Every point in R space defines a possible spin Hamiltonian. We have managed to 
define in this space a vector potential. It is derived from the nth quantum state of 
the above Hamiltonian and is given by 
</p>
<p>(21.1.104) 
</p>
<p>What is its curl? To figure this out, we need a little groundwork. Using 
</p>
<p>O=V(njHJm) mi=n (21.1.105) 
</p>
<p>we find on differentiating all three factors and shifting a derivative from bra to ket 
at the cost of sign change (thanks to V(njm) =0), 
</p>
<p>(njVjm) 
(ni(VH)Im&gt; 
</p>
<p>Em-En 
(21.1.106) </p>
<p/>
</div>
<div class="page"><p/>
<p>It is now easy to find a formula for the field tensor FiJ associated with the Berry 
</p>
<p>potential: 
</p>
<p>=iii[ ot(nl o1n)- o1(nl otn)] 
</p>
<p>=iii I: (nl(oH)Im) (ml(oiH)In&gt;- (nl
2
(oF)Im)(ml(oH)In) 
</p>
<p>m&yen;n (&pound;,.,-En) 
</p>
<p>(21.1.107) 
</p>
<p>where m labels a complete set of states we introduce along the way. (Them= n terms 
</p>
<p>drop out due to a cancellation.) This formula is valid in general (for any H) and 
</p>
<p>we now apply it to our problem. 
</p>
<p>In our problem there are many simplifying features: 
</p>
<p>&bull; oH/oR1= -a1 
&bull; There are only two states and hence only one term in the sum over m. The energy 
</p>
<p>denominator squared is 4R2 since 2R is the difference between up and down spin 
states. (Remember R is now the magnitude of the magnetic field!) 
</p>
<p>&bull; So we pull out this denominator, which is independent of m, add a term with rn = 
n (which vanishes by antisymmetry in i andj), use completeness to eliminate the 
</p>
<p>intermediate states, use the commutation relations for the Pauli matrices, and 
</p>
<p>finally the fact that (niO"In) = &plusmn;R (for the states up/down the field). 
</p>
<p>Rather than state the field in terms of the tensor fij, we write in terms of the more 
familiar magnetic field defined by ,ell%= Fij (where the indices i, j, k run cyclically) : 
</p>
<p>(21.l.l08) 
</p>
<p>This is the field of a monopole of strength -fi/2 sitting at the origin, which is the 
point of degeneracy of the Hamiltonian. 
</p>
<p>Exercise 21.1.13. Furnish the missing steps in the above derivation. 
</p>
<p>Note that there are two different magnetic fields in the problem. The tlrst is a 
</p>
<p>real one B which couples to the electron spin and resides in real space. It is produced 
by currents in real space. (There are no known monopole sources for such fields.) 
The second field is the curl of the Berry vector potential that resides in parameter 
space. Its components are denoted by fJH'k which happens, in our problem, to describe 
a monopole in parameter space. We will now see that the Berry monopole will arise 
in any problem where the Hamiltonian (not necessarily containing magnetic fields) 
becomes doubly degenerate. 
</p>
<p>603 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>604 
</p>
<p>CHAPTER 21 
</p>
<p>Assuming the parameter space is three-dimensional, let us focus on just the two 
nearly degenerate levels. Now, any 2 x 2 Hermitian operator can be written as 
</p>
<p>(21.1.109) 
</p>
<p>where rr0 = I is the fourth partner to the Pauli matrices, and/~' are four functions of 
the three independent coordinates of parameter space. The eigenvalues of H are 
clearly 
</p>
<p>E=fo&plusmn;JiT+J;,2 +f}. (21.1.110) 
</p>
<p>The degeneracy occurs at fx = J;, = fo = 0 which we choose to be the origin of coordi-
nates. We also shift the overall zero of energy so that the degenerate eigenvaluef0(0) 
vanishes. Let us now use the three f's themselves as the new coordinates in which 
case / 0 will be some function of these coordinate and vanish at the origin. Thus 
</p>
<p>H=fo(f )I+a&middot;f (21.1.111) 
</p>
<p>in obvious notation. Note thatfo vanishes at the origin but not necessarily elsewhere. 
Let us repeat the same analysis we used in the spin problem, starting with 
</p>
<p>o;H=oJoi+ rr; (21.1.112) 
</p>
<p>If we next evaluate the field tensor as per Eq. (2l.l.l07), we see that the part 
proportional to the identity does not matter (since &lt;min) =0 form ,ton), the problem 
becomes isomorphic to the one in Eq. (21.1.103) and we get just the monopole at 
the origin. 
</p>
<p>Exercise 21.1.14. Take another look at the problem we studied, of a particle moving 
around in a loop with fields in the azimuthal and z-directions. As the particle goes once 
around the circle, the line integral of the vector potential A+ is 
</p>
<p>Let us now look at the same closed orbit in 8-space where it is a loop of fixed radius B2 at 
a fixed height B1 above the Bx- By plane. Thus it defines the co-latitude (at angle() measured 
from the north pole) of a sphere of radius J BT + 113_. In this space we have a monopole of 
strength -Fi/2 at the origin according to Eq. (21.1.108). The flux through this loop is then 
the monopole flux penetrating the area of the cap bounded by this latitude. Using Stoke's 
theorem show that this flux equals -2tr:Fi sin2 () /2 as it should. (Note that the Berry vector 
potential is different in real space and parameter space. Its line integral over a closed loop, 
which measures the accumulated phase change per revolution, is of course the same. Consider 
in general a map from manifold X with points labeled x, to Ywith points labeled y, such that 
each x goes into a unique y. If A(y) is a vector potential in Y, we can import it to X by 
defining a vector potential A(x) such that (suppressing indices) 
</p>
<p>A(x) dx=A(y) dy (21.1.113) </p>
<p/>
</div>
<div class="page"><p/>
<p>By construction, closed loops in X go to closed loops in Y. The line integral of A(x) around 
</p>
<p>a closed loop in X will then equal the line integral of A(y) around the image loop in Y.) 
</p>
<p>Execise 21.1.15. Let us discuss the question of assigning phases to state vectors in param-
</p>
<p>eter space through an example. Let R=(R 0, &cent;)be the coordinate in parameter space. Con-
</p>
<p>sider the Hamiltonian H = -CJ &bull; R. Let us write down the ground state for this problem for all 
points. It is the one where the spin points radially outward everywhere. A choice for the 
</p>
<p>spinor is 
</p>
<p>[ (} 1 
cos-
</p>
<p>\+ e -"\= 2. , &bull; '!' / {! . 
sin- e/(P 
</p>
<p>2 
</p>
<p>This is just the kct we used in the problem of the electron going around in a loop (except for 
</p>
<p>the factor i in the lower component which arose due the rr /2 difference between the azimuthal 
angles in real and parameter space). Since the spinor has no R dependence let us look at it 
</p>
<p>on a unit sphere R = 1. Observe that the lower component does not approach a unique value 
</p>
<p>as we approach the south pole from different directions. (This problem does not exist at the 
</p>
<p>north pole since sine /2 = 0 there.) Thus we really have not defined the spin or globally. If we 
</p>
<p>multiply the whole spinor by the single-valued phase factor e-"1&gt;, we now have a spinor well 
</p>
<p>defined near the south pole, but singular at the north pole. [t follows that we can only define 
</p>
<p>the spinor in patches of parameter space. In our problem two patches will do, one excluding 
</p>
<p>the north pole and one excluding the south. 
</p>
<p>Since we found the Berry potential by taking derivatives of the ket, it follows that the 
</p>
<p>former is also defined only in the patches and not globally. In other words, Eq. (21.1.88) for 
</p>
<p>A+ is to be used away from (} = rr. To describe the south pole, we can use, for example, the 
potential coming from the spinor with good behavior at the south pole, but bad behavior at 
</p>
<p>the north pole. 
</p>
<p>I will now argue that attempts to find a global vector potential in the presence of a 
</p>
<p>monopole are doomed. Say we had a global nonsingular vector potential. Consider its line 
</p>
<p>integral along the direction of increasing 1/J on a latitude near the north pole on a unit sphere 
</p>
<p>surrounding the monopole. By Stokes's theorem this equals the flux through the cap above 
</p>
<p>this latitude. If we enlarge the loop and go past the equator, the line integral will monotonically 
increase. Finally, let us shrink the loop to an infinitesimal one around the south pole. As this 
</p>
<p>loop shrinks, the line integral does not vanish; it equals the full monopole flux. It follows 
</p>
<p>there must be a singularity at the south pole since the integral of a nonsingular potential 
</p>
<p>around an infinitesimal loop must be infinitesimal and vanish with loop size. (It is also possible 
</p>
<p>that the singularity is elsewhere on the sphere, but it has to exist by similar reasoning.) 
</p>
<p>Starting with the gradient in spherical coordinates, show that the vector potential associ-
</p>
<p>ated with\+. e, &cent;&gt;is given by 
</p>
<p>1i (l --cos 8) 
A = - 2 e, R sin e 
</p>
<p>Observe the singularity at the south pole. This is called the Dirac string. Show that its line 
</p>
<p>integral around a tiny loop surrounding the south pole is the full monopole flux. What is 
</p>
<p>happening is this. This vector potential describes not a monopole at the origin, but one where 
</p>
<p>a tiny tube (the Dirac string) comes up the negative z-axis, smuggling in the entire flux to the 
</p>
<p>605 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>606 
</p>
<p>CHAPTER 21 
</p>
<p>origin, from which point it emanates radially. The string flux is the reason the tiny loop 
</p>
<p>around the south pole gives a nonzero answer equal to the total flux. 
</p>
<p>Now there is nothing special about the south pole when we look at the monopole. since 
</p>
<p>it is spherically symmetric. This is reflected in the fact that the Dirac string can be moved 
</p>
<p>around by a gauge transformation. Calculate the vector potential A' with the spinor obtained 
</p>
<p>by multiplying both components of!+. e. t/J) bye ""&middot;Show that it has troubles at the north 
pole and that the two vector potentials are related by the gauge transformation associated 
</p>
<p>with the redefinition i+, e, t/J)-&gt;e-i&cent; i+, e, t/J). 
If we are allowed to use words instead of equations, we can describe the effect of the 
</p>
<p>monopole without any strings: when the charged particle goes around in a loop, it picks up 
</p>
<p>a phase proportional to the solid angle the loop subtends at the origin (where the monopole 
</p>
<p>is). The vector potential is the analytical way to generate the solid angle via Stokes's theorem, 
</p>
<p>but it cannot do it globally. 
</p>
<p>Now Dirac ran into this problem trying to ask how we would describe a real (not Berry) 
</p>
<p>monopole of charge g in real space. It has a radial field that falls off as gi R 2 &bull; No problem 
</p>
<p>there. But quantum mechanics forces us to work with vector potentials. Now any vector potential 
</p>
<p>we can come up with has a string. As usual, Dirac turned a potential disaster into a dazzling 
</p>
<p>prediction by arguing that i/ there is a monopole and we have no choice but to describe it with 
a vector potential, it must be that the string is unobservable. The line integral of the vector 
</p>
<p>potential around the string at the south pole is 4~rg, the total flux of the monopole. For a 
</p>
<p>particle of charge q, this will enter the dynamics via the factor 
</p>
<p>as per Eq. (I R.4.38). (Think of an Aharaonov Bohm experiment in which a particle goes on 
</p>
<p>either side of the string.) If this factor is to be unobservable we require that 
</p>
<p>fine 
q=-
</p>
<p>2g 
</p>
<p>where n is any integer. This remarkable argument tells us that even !(there is a single monopole 
</p>
<p>in the universe, itforces all electric charges to he multiples offic/2g. This explains, for example, 
</p>
<p>why the proton and electron have exactly the same charge. However no monopole has yet 
</p>
<p>been seen. But, the argument is so attractive r for one am sure at least one monopole exists. 
If not, nature would have missed a wonderful opportunity, to paraphrase Einstein. 
</p>
<p>In modern treatments, one uses two patches, say one without the south pole and one 
</p>
<p>without the north pole, with a different vector potential in each. By demanding that where 
</p>
<p>the patches overlap, say the equator, the two potentials differ by a single-valued gauge trans-
</p>
<p>formation, one recovers Dirac's quantization condition. (You may provide the proof yourself 
</p>
<p>if you remember that (I) the difference of the line integrals of the two patch potentials around 
</p>
<p>the equator is the integral over the whole sphere of the outgoing flux; (2) when the wave 
function of a particle of charge q is changed by a phase factor lf&middot; .. +e;xlfl, vector potential 
</p>
<p>changes as per A -+A+ fic/q ex; (3) the change in x around a closed loop must be an integral 
multiple of 2n.) 
</p>
<p>In the Berry phase problem we looked at, the vector potential had q 1 c, the factor multi-
plying A in the Hamiltonian, equal to unity, g = nj2, and hence n = 1. 
</p>
<p>As another application of the Berry phase, let us return to the Hall effect. 
</p>
<p>Laughlin proposed that the excited state (above the ground state), called the quasihole </p>
<p/>
</div>
<div class="page"><p/>
<p>state, be given by 
vN 
</p>
<p>Uqh= f1 (z;-zo)Uv 
i=L 
</p>
<p>(21.1.114) 
</p>
<p>Clearly this describes a situation where the wave function is modified in the vicinity 
</p>
<p>of z0 . We say it describes a quasihole centered at z0 . Note that electrons avoid the 
point z0 due to the extra zeros of the form z- z0 . This means the charge density near 
</p>
<p>this point is below normal. If one integrates the charge deficit due to this modification 
in the wave function (which is the charge of the quasihole) one finds it is vq, where 
</p>
<p>q is the elementary charge e. Thus a theory with elementary charges that are integers 
(electrons) has excitations which have fractional charge! The fractional charge can 
</p>
<p>also be demonstrated as follows. First note that the location zo of the quasihole is 
arbitrary. Assume there is some substrate potential underneath the electron gas 
</p>
<p>whose minimum selects out some preferred location. Suppose we slowly vary the 
</p>
<p>potential and drag the coordinate z0 in Uqh around some closed loop and calculate 
the accumulated Berry phase for this closed orbit. (Since we know the wave function 
</p>
<p>explicitly for any z0 , this is easily done.) This must equal the flux (due to the external 
</p>
<p>magnetic field B that produces the Landau levels) enclosed times q /fie where ij is 
the quasihole charge. The calculation gives a charge v times the elementary charge. 
</p>
<p>Similarly, one may show that the quasiholes are neither bosons nor fermions, but 
anyons (a term coined by Wilczek; see Bibliography): they acquire a phase factor 
eivn under exchange, by taking a state with two quasiholes (located at z0 and z0) 
and adiabatically exchanging them (i.e., their centers) and computing the Berry phase 
</p>
<p>change in the wave function. The adiabatic analysis is valid since the quasihole states 
are separated by a gap from other states. For details, see Shapere and Wilczek (1990). 
</p>
<p>We conclude with some history. 
</p>
<p>Why did Born and Oppenheimer miss the Berry phase? The reason was quite 
</p>
<p>subtle. They were working with a real Hamiltonian whose wave functions could be 
chosen real. They assumed such a choice had been made and that the choice was 
</p>
<p>nonsingular. While this is correct for any open curve in parameter space, there exists 
</p>
<p>the possibility that in closed curves, one could be forced to return to minus the 
starting wave function. Berry considered complex Hamiltonians (isomorphic to the 
</p>
<p>spin example) which allowed a continuum of possible values for the phase (instead 
</p>
<p>of just &plusmn; 1) and made the phenomenon more transparent. 
Finally, although we have discussed the Berry phase in connection with quantum 
</p>
<p>mechanics, it was discovered in optics many decades earlier by Pancharatnam (1958) 
</p>
<p>who considered a polarized beam of light rather than a quantmc1 ~tate going on a 
closed path in parameter space (see Bibliography). For a fascinating review of even 
</p>
<p>earlier precursors, see Berry's article in Physics Today (see Bibliography). 
</p>
<p>Coherent State Path Integral 
</p>
<p>Now we discuss yet another resolution of the identity and the associated path 
integral. These are based on coherent states defined to be eigenstates of the destruction 
operator in the harmonic oscillator problem. 
</p>
<p>Each coherent state carries a complex label z and is given by 
</p>
<p>(21.1.115) 
</p>
<p>607 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
</p>
<p>PARTH </p>
<p/>
</div>
<div class="page"><p/>
<p>608 
</p>
<p>CHAPTER 21 
</p>
<p>where ! 0) is the ground state of the oscillator. ff we recall that 
</p>
<p>(2l.l.ll6) 
</p>
<p>we see that 
</p>
<p>~" 
</p>
<p>lz)=I In) (21.!.117) 
</p>
<p>States labeled by different values of z are not orthonormal. We should have expected 
nonorthogonality since the basis In) labeled by the positive integers n forms a com-
</p>
<p>plete basis and here we have one state for every complex number z! So they couldn't 
</p>
<p>all be orthogonal. It is also possible that despite their large number, they are not a 
</p>
<p>complete set. We shall, however, see that they are an overcomplete basis, i.e., a basis 
</p>
<p>with enough vectors to expand any vector but with more than the smallest number 
</p>
<p>one could have gotten away with. 
</p>
<p>Now we will establish the key property 
</p>
<p>al =zl (21.1.118) 
</p>
<p>as follows: 
</p>
<p>alz) =a I ~ ...... [n) 
u .jn! 
</p>
<p>(21.1.119) 
</p>
<p>(21.1.120) 
</p>
<p>(21.1.121) 
</p>
<p>where, in going to the last line, we have redefined a dummy label n' = n -~ I which 
</p>
<p>runs from 0 to :c. 
Likewise, by taking the adjoint ofEq. (21.1.118), the coherent state bra 
</p>
<p>= (01 exp[z*a] (21.1.122) 
</p>
<p>is seen to obey 
</p>
<p>(zlat = (zlz* (21.1.123) 
</p>
<p>Let us now consider the inner product 
</p>
<p>(21.!.124) </p>
<p/>
</div>
<div class="page"><p/>
<p>If we use the identity 
</p>
<p>(21.1.125) 
</p>
<p>which is valid if [A, B] commutes with A and B, we see 
</p>
<p>(21.1.126) 
</p>
<p>upon noting that when the exponentials are exchanged and expanded out, only the 
first term with no a's acting to the right or at's acting to the left survives. 
</p>
<p>Completeness is shown by proving the following resolution of the identity 
</p>
<p>f
dx dy _ &middot;~ fdz dz* 
</p>
<p>I= --lz) (zl e z &bull; = --. lz)(zl e&middot;--zrz~ 
7r 211:1 
</p>
<p>(21.1.127) 
</p>
<p>where z=x+iy and z*=x-iy. Note that the integral is over the entire x-y plane, 
</p>
<p>and after replacing every z and z* in the integrand by x &plusmn; iy, may be carried out 
using any other coordinates. For example, in Exercise (21.1.16) polar coordinates 
are recommended in verifying the above completeness relation. One can also formally 
go from (x, y) to (z, z*) (after inserting a Jacobian l/2i), but integration over (z, z"') 
is a subtle question we will not get into. We indicate that measure in terms of (z, z"') 
anyway (now and later) so you will know what it means if you ever run into it again. 
</p>
<p>To show Eq. (21.1.127), one uses 
</p>
<p>a:; ,.,..n 
</p>
<p>lz&gt;=I ~In) 
o .yn! 
</p>
<p>(21.1.128) 
</p>
<p>and its adjoint, does the dx dy integral in polar coordinates, and recovers the usual 
sum over ln)(nl. 
</p>
<p>Exercise 2 /_ 1.16. Verify the above resolution of the identity. Consult Appendix A2 for 
</p>
<p>the Gamma function integral. 
</p>
<p>Since the coherent states are right eigenstates of a and left eigenstates of at, 
</p>
<p>(21.1.129) 
</p>
<p>where: H: is any normal ordered expression i.e., an expression with all the destruction 
operators to the right and creation operators to the left. Thus, ata2 is a normal 
ordered expression while a2at is not. Given any expression we can always normal 
order it by pushing the a's to the right, keeping track of commutators. 
</p>
<p>Exercise 21.1.17. Show that a2at =:clat :+2a. (Push one power of a at a time to the right, 
or use [AB, C]=A[B, C]+[A, C]B.) 
</p>
<p>PATH 
INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>610 
</p>
<p>CHAPTER 21 
</p>
<p>We now prove the following remarkable result: if His the oscillator Hamiltonian, 
</p>
<p>(21.1.130) 
</p>
<p>(we drop the constant zero-point energy for this discussion), then 
</p>
<p>U(t)lz) = U(t) exp[atz] U\t) U(t)IO) = exp[at e-iwr z]IO) = lz e -imr) (21.1.131) 
</p>
<p>where we have used the Heisenber,r. equations of motion for at. (In the Heisenberg 
</p>
<p>picture ut (t)QU(t) = Q(t). Here U (t) = U( -t) appears in place of U(t). We use the 
result at(t)=a\0) eiwr and reverse the sign oft.) 
</p>
<p>It is remarkable that under time evolution the coherent state remains a coherent 
</p>
<p>state, but with a new label. This was one of the reasons one got interested in them 
</p>
<p>in the first place. They have far too many interesting properties for us to discuss 
</p>
<p>them all here. Instead you are directed to the reference on this subject. 
</p>
<p>Exercise 21.1.18. Show that the wave function of the coherent state is 
</p>
<p>(21.1.132) 
</p>
<p>Start by using ajz) =ziz&gt; in the coordinate representation. Fix the normalization by demand-
</p>
<p>ing that (z'jz) =e"*z. Read off its mean momentum and position. Show that these evolve with 
</p>
<p>time like classical coordinates given that jz)-+iz e-'"''). Suggestion: Look at Eq. (9.3.7) and 
</p>
<p>parametrize z as z = J(mro j21i)xo + iJ(l j2mroti) Po. 
</p>
<p>It is very easy to find the propagator for the oscillator in this basis: 
</p>
<p>where the subscripts on the end point anticipates the following discussion. 
</p>
<p>Consider the path integral representation for the propagator. Let us first imagine 
</p>
<p>that there are just three intermediate time slices (so that e = t I 4) and three resolutions 
of the identity operator are used, giving us 
</p>
<p>(z41 U4(t/4)1 zo) 
</p>
<p>= f [.Pz!Zlz*] &lt;z41(1-~ H(ata))z3) e-=!=, (z31(1- i: H(ata)}z2 ) e-=~=o (z2 1 
</p>
<p>where 
</p>
<p>N-ld d* N-ld d 
[ *j T1 Z; Z; T1 X; y; !Zlz.@z = ----= ----
</p>
<p>' 21Ci I 1C 
(21.1.134) </p>
<p/>
</div>
<div class="page"><p/>
<p>A typical factor we run into is as follows: 
</p>
<p>(21. U35) 
</p>
<p>(21.1.136) 
</p>
<p>where we have treated e as infinitesimal since eventually it will be, as we let N-+rf:.&gt;. 
If we assemble all the exponential factors together, there will be a piece related to 
the Hamiltonian which clearly gives a factor 
</p>
<p>exp( -~ J: fiwz*(t)z(t) dt) (21.1.137) 
</p>
<p>in the continuum notation, where z" has become z(t=ne). (We also made the 
approximation H(z*(t+ e), z(t)) ~H(z*(t), z(t)).) 
</p>
<p>The other factor in the exponent is 
</p>
<p>(21.1.138) 
</p>
<p>(21. 1.139) 
</p>
<p>which we write in continuum notation as 
</p>
<p>!_ ( -ifi) __:_ z dt' + z*(O)z(O) . [f' d * J 
1i 0 dt' 
</p>
<p>(21.1.140) 
</p>
<p>where z{O) = zo and z*(O) = limc~o z*( e). In other words, in the discretized version 
z0 was defined but not zt . Only in the continuum picture, where we focus on smooth 
trajectories, is this object defined as the above limit. 
</p>
<p>The sum in Eq. (2l.l.l39) can also be rearranged to give 
</p>
<p>(21.1.141) 
</p>
<p>where z(t} is again extraneously introduced as a limit z(t) = lim,~o z(t &middot;&middot;&middot;&middot;&middot;e). 
One usually sees the two schemes averaged to give the following final form of 
</p>
<p>the continuum result: 
</p>
<p>[ z}z1 +z(z; i f'[ifi( *dz dz* ) * J J (ztiU(t)lz;)=exp +- - z -&middot;&middot;--&middot;&middot;&middot;&middot;&middot;&middot;&middot;&middot;z -H(z ,z) dt 
2 1i 0 2 dt dt . 
</p>
<p>(21.1.142) 
</p>
<p>611 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
</p>
<p>PARTU </p>
<p/>
</div>
<div class="page"><p/>
<p>612 
</p>
<p>CHAPTER 21 
</p>
<p>We will use the asymmetric form obtained by doing an integration by parts: 
</p>
<p>(21. 1.143) 
</p>
<p>The warning that this is just a schematic for the previous discretized expression 
</p>
<p>is all the more true here since there is very little in the action to guarantee smooth 
</p>
<p>paths. However, in the limit n-+0, the integral is asymptotically approximated by 
</p>
<p>smooth paths. Let us evaluate this integral in such a limit by finding the stationary 
</p>
<p>point of the action, i.e., the classical solution. H is clear from the action, which has 
</p>
<p>the phase space form (p_-i; - jf) that z and itlz* are canonically conjugate variables. 
</p>
<p>Given this action, if one were asked to quantize, one would promote them to opera-
</p>
<p>tors obeying commutation relations 
</p>
<p>(21.1.144) 
</p>
<p>which we see are just the commutation rules for a and a1&bull; Of course, we are not 
trying to construct the quantum theory from the classical one, but the reverse. The 
</p>
<p>Hamiltonian equation is 
</p>
<p>(21.1.145) 
</p>
<p>which is solved to give 
</p>
<p>z(t) = z(O) e&middot;-iwr (21.1.146) 
</p>
<p>Similarly, we find 
</p>
<p>z*(t) =z*(O) e"vr (21.1.14 7) 
</p>
<p>To evaluate 
</p>
<p>&lt;zrl U(T)Iz;) (21.1.148) 
</p>
<p>in the semiclassical approximation, we need to find a solution that obeys 
</p>
<p>z(O) = Z; (21.1.149) 
</p>
<p>z*(T)=zi (21.1.150) 
</p>
<p>Now we see a problem that we did not have in the configuration space version: since 
the equations here are first order in time, zi detennines z( t) for all times. How can 
we get z*(T) to equal an independently given The answer is that we must regard </p>
<p/>
</div>
<div class="page"><p/>
<p>z and z* as independent and restrict z(t) at t=O and z*(t) at t= T. The solutions 
</p>
<p>then are 
</p>
<p>(21.1.151) 
</p>
<p>z*(t) = zf eiw(t- n (21.1.152) 
</p>
<p>Note that z*(T) is not the complex conjugate of z(T). This means that x and 
y invoked in the definition z = x + iy are not real on this trajectory. However, a 
Gaussian integral is given by its saddle point even if the point is off the original axis 
of integration. This point is explained in Faddeev's lectures (see Bibliography). 
</p>
<p>If we feed this solution into the action we find that the !-integral gives zero due 
to a cancellation between the two terms in the integrand and the only piece that 
</p>
<p>survives is 
</p>
<p>z*(T)z(T) = zf z1 e-twT 
</p>
<p>giving us 
</p>
<p>(zJI U(T)Iz;) =exp(z}z; e twT) (21.1.153) 
</p>
<p>which is the exact answer! 
</p>
<p>Exercise 21. ]_ 19. Evaluate the action for the above path and check the answer given. 
</p>
<p>Exercise 21.1.20. Consider the Gaussian integrals in Eqs. (A.2.4-A.2.5.) Show that if we 
</p>
<p>want just the exponential dependence of the answer, it is given by finding the exponential 
</p>
<p>where the exponent is stationary. This is a general feature of Gaussian integrals. 
</p>
<p>Exercise 21.1.21. A good take-home problem. Rederive the oscillator propagator 
</p>
<p>(x2l U(T)Ix,) given (zJI U(T)Iz;) = exp[zj z, e'"' 1 ]. Introduce two resolutions of the identity 
on either side of U(T) in (x2 1 U(T)Ix1). Use the suitably normalized wave functions (xlz) 
</p>
<p>from Exercise (21.1.18). You will have to do a Gaussian integral over the two pairs of 
</p>
<p>intermediate coherent state variables. Do the integral by saddle point, i.e., find the stationary 
</p>
<p>point of the action and evaluate the integrand there. Focus on just the exponential factor and 
</p>
<p>show that you get the answer to Exercise (8.6.2). 
</p>
<p>21.2. Imaginary Time Formalism 
</p>
<p>Consider the imaginary time propagator 
</p>
<p>U(r) =exp( -~ Hr) (21.2.1) 
</p>
<p>This is obtained by setting 
</p>
<p>t= -ir (21 .2.2) 
</p>
<p>613 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>614 
</p>
<p>CHAPTER 21 
</p>
<p>in the usual propagator. In other words, if the Schrodinger equation had been 
</p>
<p>(21 .2.3) 
</p>
<p>this would have been the propagator. 
</p>
<p>The reasons for looking at this operator will be dear as we go along. But first 
let us note that we can write down the formula for it at once: 
</p>
<p>(21.2.4) 
</p>
<p>where 
</p>
<p>(21.2.5) 
</p>
<p>The main point to note is that even though the time is now imaginary, the eigenvalues 
and eigenfunctions that enter into the formula for U( r) are the usual ones. Conversely, 
</p>
<p>if we knew U( r), we could extract the fanner. 
</p>
<p>Path Integral for the Imaginary Time Propagator 
</p>
<p>Consider the matrix element 
</p>
<p>U(x, x', r) = (xj U( r)jx') (21.2.6) 
</p>
<p>We can write down a path integral for it following exactly the same steps as before. 
The final answer in continuum notation is 
</p>
<p>&lt;xi U( r )jx') = U(x, x', r) = f [fzx] exp [- ~ r .&pound;1} E(x, .\) dr l (21.2.7) 
f , )1/2 N-l , )1/2 [ 9 X] = lim ( _ ___!I!_ n ( ___!1'1___ dx i 
</p>
<p>NHX) 2nnE, o 2nnE 
{21.2.8) 
</p>
<p>m(dx')2 . !.eE=- - + V(x) 
2 dr 
</p>
<p>(21.2.9) 
</p>
<p>where E = r j Nand !f E is called the euclidean Lagrangian. The adjective "euclidean" 
means that &middot;$pace and time now behave alike-the minus signs of Minkowski space 
in the formula for invariants are gone. For example, the invariant x 2 - c2t2 now 
becomes x2 + c2r 2. Notice that !f E is the sum of the euclidean kinetic energy and 
real-time potential energy. Thus the particle obeying the euclidean equations of motion 
will see the potential turned upside down. This will be exploited later. </p>
<p/>
</div>
<div class="page"><p/>
<p>We have emphasized that the continuum form of the path integral is a shorthand 
for the discrete version. It is true here also, but of all the path integrals, this is the 
best behaved. Rapidly varying paths are suppressed by the falling (rather than rapidly 
oscillating) exponential factor. 
</p>
<p>Suppose we want to calculate the euclidean path integral for a free particle. We 
can proceed as we did in Chapter 8 and obtain 
</p>
<p>( m )
112 
</p>
<p>[ m(x-x')2 ] (xi U(r)lx')= - exp ---'----_:____ 
2n~r 2~r 
</p>
<p>(21.2.10) 
</p>
<p>If someone gave us this propagator, we could get the Minkowski space answer by 
setting 
</p>
<p>r=it (21.2.11) 
</p>
<p>This is called analytic continuation. 
A very important feature of euclidean quantum mechanics is that the operator 
</p>
<p>U( r) is not unitary but Hermitian. Thus the norm of the state is not preserved in 
time. In fact what happens is that after a long time every state evolves into the 
ground state 10): 
</p>
<p>lim (xiU(r)lx')= lim I&lt;xln)(nlx')exp(-!Enr) 
r-oo r-oo Fi 
</p>
<p>(21.2.12) 
</p>
<p>(21.2.13) 
</p>
<p>= lf!o(x)lf!~(x') exp( -~ Eor) (21.2.14) 
</p>
<p>Thus all states lead to the ground state as long as the starting point has some overlap 
with it. This is one way to find the ground state in any problem: take any initial 
state and let it evolve for a long time. You should hit the ground state unless you 
had chosen an initial state orthogonal to the ground state. (Sometimes you may do 
this on purpose to find the first excited state. For instance if the problem has parity 
invariance and you choose an initial state odd under parity, you will hit an excited 
state.) 
</p>
<p>For example, the propagator for the oscillator is 
</p>
<p>U(x, x', r) =A( r) exp( ~w [(x2 + x'2) cosh wr- 2xx']) 
2~ smh wr 
</p>
<p>(21.2.15) 
</p>
<p>obtained, say by analytic continuation from real times of the answer in Exercise 
(8.6.2). Note that as r-+oo this becomes proportional to the product of ground state 
wave functions. The prefactor is left to the following exercise. 
</p>
<p>615 
</p>
<p>PATH 
INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>616 
</p>
<p>CHAPTER 21 
</p>
<p>Potential V(x) lor ,..I time Potential V(x) In Imaginary time 
</p>
<p>Figure 21.2. The double-well potential 
</p>
<p>in real and imaginary time. 
</p>
<p>Exercise 21.2.1. Obtain A(t) from Exercise (8.6.3) and continue to imaginary time, and 
verify that in the large r limit, it yields the right prefactor. 
</p>
<p>Tunneling by Path Integrals: Well, well! 
</p>
<p>We now consider one application of the euclidean formalism. We have seen 
</p>
<p>how one can derive the WKB wave function for nonbound states by using path 
</p>
<p>integrals. This procedure does not work for tunneling amplitudes across barriers 
</p>
<p>since we cannot find a classical path that goes over the barrier. On the other hand, 
</p>
<p>in the euclidean dynamics the potential is turned upside down and what is forbidden 
</p>
<p>in Minkowski space is suddenly allowed in the euclidean region! 
</p>
<p>Here is a problem that illustrates this point and many more. Consider a particle 
</p>
<p>in a double-well potential 
</p>
<p>(21.2.16) 
</p>
<p>The classical minima are at 
</p>
<p>(21.2.17) 
</p>
<p>Figure 21.2 shows a graph in Minkowski and euclidean space for the case a= I. 
</p>
<p>Notice that in the euclidean problem the double-well has been inverted into the 
</p>
<p>double-hill. 
What is the ground state of the system? The classical ground state is doubly-
</p>
<p>degenerate: the particle can be sitting at either of the two minima. In the semiclassical 
</p>
<p>approximation, we can broaden these out to Gaussians that are ground states I &plusmn;a) 
in the harmonic oscillatorlike potential around each minimum at x= &plusmn;a. This will 
shift each degenerate ground state by ~ fi(J) where (J) measures the curvature of the 
</p>
<p>potential near the minimum. We can go to higher-order approximations that recog-
</p>
<p>nize that the bottom of the well is not exactly quadratic and shift the ground state 
</p>
<p>energies by higher powers of fl. However, none of this will split the degeneracy of 
</p>
<p>the ground states since whatever we find at the left minimum we will find at the right 
by symmetry under reflection. Lifting of the degeneracy will happen only if we take 
</p>
<p>into account tunneling between the two wells. So we study this problem in the 
</p>
<p>following stripped-down version. First we drop all but the degenerate ground states 
</p>
<p>I &plusmn;a). (The Gaussians centered around the two minima are not quite orthogonal. </p>
<p/>
</div>
<div class="page"><p/>
<p>Assume they have been orthogonalized by a Gram---Schmidt procedure.) The approxi-
</p>
<p>mate Hamiltonian looks like this in this subspace: 
</p>
<p>[ l Eo 0 H= 0 Eo (21.2.18) 
Let us shift our reference energy so that Eo= 0. 
</p>
<p>Note that there are no off-diagonal matrix elements. If this were an exact result, 
</p>
<p>it should mean that if a particle starts out in one well it will never be found at the 
</p>
<p>other. But we know from the wave function approach that if it starts at one side, it 
</p>
<p>can tunnel to the other. This means that there is effectively a nonzero matrix off-
</p>
<p>diagonal matrix element H+-=H-+=&lt;allfl-a) in this basis. The challenge is to 
</p>
<p>find that element in the semiclassical approximation. Once we find it, it is evident 
</p>
<p>that the energy levels will be split into 
</p>
<p>E=&plusmn;H+ (21.2.19) 
</p>
<p>and the eigenstates will be IS I A), the sum and difference of I &plusmn;a). 
Consider 
</p>
<p>(al U( r)l-a) = (al exp(-! llr) I -a) 
' 1i ; 
</p>
<p>(21.2.20) 
</p>
<p>In this discussion of tunneling, U( r) is the propagator from ---- r /2 to r /2 and not from 
0 to r. Note that the term linear in r gives us the off-diagonal matrix element: 
</p>
<p>( 1 ) 1 - 2 (al exp -~Hr 1-a)c::.::O-~ r(aiHI-a)+C!r (21.2.21) 
</p>
<p>We shall calculate (al e-O/~)Hrl-a) by the semiclassical approximation to the eucli-
</p>
<p>dean path integral and extract the approximate matrix element H ~- . Once again, as 
</p>
<p>in the real-time semiclassical approximation, we focus on just the exponential factor 
</p>
<p>and ignore all prefactors. In the semiclassical approximation. 
</p>
<p>(a I exp (-_!_ Hr )1----a) ~exp (' _! Sc~ J 
fi 1 fi I 
</p>
<p>(21.2.22) 
</p>
<p>where Sc1 is the euclidean action for the classical path connecting the left hill to the 
</p>
<p>right. The key point, of course, is that in the double-hill potential of euclidean mechanics 
the classical ground states are not separated by a barrier, so that there will be no 
</p>
<p>problem finding a classical path going from one hill to the other. 
</p>
<p>617 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>618 
</p>
<p>CHAPTER 21 
</p>
<p>The euclidean equations of motion are the same as the real times ones, except 
</p>
<p>for the reversal of the potential. Thus there will be a conserved energy Ee given by 
</p>
<p>m (dx-)2 
Ec=- - - V(x) 
</p>
<p>2 \dT. 
(2!.2.23) 
</p>
<p>Using this we can solve for the trajectory by quadrature: 
</p>
<p>J'' dr 
,, 
</p>
<p>(21.2.24) 
</p>
<p>Now we want the tunneling from the state 1-a &gt; to the state I a&gt;. These are not 
eigenstates of position, but Gaussians centered at x ==Fa. We shall however calculate 
the amplitude to tunnel from the position eigenstate x =-a to the position eigenstate 
</p>
<p>x =a. Except for the overlaps =ala&gt; and (-a I x = -a&gt; this is the same as 
(ai u1-a). These overlaps know nothing about the tunneling barrier. They will 
constitute undetermined prefactors in front of the exponential dependence on the 
</p>
<p>barrier which alone we are after. To extract the extreme low energy physics we must 
</p>
<p>let r-HfJ. To this end, let us consider the trajectory that has E"=O. It is given by 
</p>
<p>doing the above integral with Ee = 0: 
</p>
<p>x( r) =a tanhlJ; ArJ (21.2.25) 
</p>
<p>Notice that in this trajectory the particle starts out at the left maximum (Fig. 21.2) 
</p>
<p>at r-+- :YJ and rolls down the hill and only reaching of the right maximum as r&middot;--&gt; x. 
</p>
<p>If the starting point and ending point are exactly x ==Fa, tunneling takes infinite 
time since only in this limit does the tanh take its limiting value of &plusmn;a. Physically, 
it takes forever since the particle must start from rest at the left end to have zero 
</p>
<p>euclidean energy. On the other hand, for points which are slightly below the maxi-
</p>
<p>mum at each end, the time of travel will be finite since the particle can start with 
</p>
<p>nonzero velocity. Since these points will also have roughly the same overlap with 
</p>
<p>the states l&plusmn;a) we can start with them instead of x= &plusmn;a in which case the tunneling 
will take place in finite time. This will be understood in what follows. 
</p>
<p>The action for the above solution is (using T= Vfor the zero energy solution), 
</p>
<p>~ f 'a f" Sc~ = J (T+ V) dr = 2T dr = j_/(x) dx= -a J2mV(x) dx (21.2.26) 
</p>
<p>and the tunneling amplitude is (ignoring prefactors) 
</p>
<p>(al U]-a)c::::exp(-~ r -.J2mV(x) dx) 
---a 
</p>
<p>(21.2.27) 
</p>
<p>in agreement with tunneling result in the Schrbdinger approach, Eq. (16.2.24) with 
</p>
<p>E=O. </p>
<p/>
</div>
<div class="page"><p/>
<p>Now, onward to extract the matrix element by looking for the term linear in T 
in the answer. But we see no such explicit T dependence in the answer! The resolution 
can be stated in two ways. 
</p>
<p>&bull; The first is tied to the fact that in the limit of large T, the problem becomes 
translationally invariant in time. In other words, if we stare at the classical solution 
above, we see that the tanh is close to &plusmn;a most of the time and jumps rapidly 
from -a to a in a short time centered around T = 0. Pictorially, the particle takes 
a long time to roll off the top, but once it gets going, it rolls down very quickly 
to a point close to the other end point. (For this reason this solution is called an 
instanton, a term coined by 't Hooft: except for the brief "instant" when tunneling 
takes place, the system is essentially in one of its classical ground states.) If we 
draw a new trajectory in which the same tunneling takes place in the same time 
interval, but is centered around a time T = T 0 io 0, this too will be close to being a 
minimum of the action. (It will have exactly the same action as T-+ oo.) In other 
words, the solution we found has many companions, all of nearly the same action, 
but different tunneling instants To. We must sum over all these paths, i.e., integrate 
over the instant of tunneling To. Since they all have nearly the same action, the 
effect is to multiply the answer by T since To is forced to lie within the period 
-T/2&lt; To&lt; T/2. 
</p>
<p>&bull; The second way to argue is that once we find one classical path, we must integrate 
the functional over all fluctuations ox(T)=x(T)-xc~(T). (See Section 8.6.) If we 
expand the action near xc1, there will be no linear term since the action is stationary 
here and we will start with a quadratic expression in ox(T). By diagonalizing this 
quadratic form we can get the answer as a product of Gaussian integrals. Consider 
the one-dimensional example of some function approximated by a Gaussian 
centered at x = 0: 
</p>
<p>fx, I( a)= e-ax' dx 
XJ 
</p>
<p>(21.2.28) 
</p>
<p>If a &gt; 0 we can assume the lim tis can be pushed to infinity and the answer approxi-
mated by 
</p>
<p>(21.2.29) 
</p>
<p>What happens when a -+0? The approximate answer diverges but we know the 
real answer is 
</p>
<p>(21.2.30) 
</p>
<p>This is essentially what happens in the functional integral. Say x( T) is a classical 
solution. Then x(T- To) is also a solution, and 
</p>
<p>8 x( T) = x( T - T 0 ) - x( T) (21.2.31) 
</p>
<p>619 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>620 
</p>
<p>CHAPTER 21 
</p>
<p>is a fluctuation that costs no extra action, i.e., the Gaussian that is supposed to 
</p>
<p>damp out this fluctuation has a --&gt;0. The Gaussian integral is then replaced by the 
range of integration corresponding to this degree of freedom, which is just 
</p>
<p>Jdro==.r. 
</p>
<p>So we have argued for a prefactor of r which came from considering a fluctua-
tion about the classical solution. We were forced to consider it since it reflected an 
</p>
<p>exact symmetry (under time-translation) as result of which it had no a in the Gauss-
</p>
<p>ian to cut it off. We do, however, ignore the Gaussian integrals over the rest of the 
</p>
<p>fluctuations since they cut off by nonzero as. 
</p>
<p>With the prefactor r in front of 
</p>
<p>( -ai U( r)la) = r exp ( ---~ sc~) (21.2.32) 
</p>
<p>we are ready to compare to 
</p>
<p>(
' 1 \ 1 
</p>
<p>(a I exp ----llr )1 -a) =::.0- ---- r(allll-a) + Cir2 
' 1i ' 1i 
</p>
<p>(21.2.33) 
</p>
<p>and read off 
</p>
<p>( 
1 \ 
</p>
<p>11-+ ':::. -exp -~ scl) (21.2.34) 
</p>
<p>where once again we have dropped all prefactors except for the sign which is impor-
</p>
<p>tant. (All euclidean transition amplitudes are positive since the functional is positive. 
The minus sign comes from e-CI/~lnr.J 
</p>
<p>It is now clear that with H-+ negative, the new eigenstates and energies are as 
</p>
<p>follows: 
</p>
<p>IS)= [l+a)+l-a)] (21.2.35) 
</p>
<p>jA)= [I +a) ---1--a)] (21.2.36) 
</p>
<p>Spontaneous Symmetry Breaking 
</p>
<p>Why are we interested in a term that vanishes exponentially fast as 1i --&gt; 0 when 
we ignored all the perturbative corrections to the states I &plusmn;a) which vanished as :finite 
powers of 1i? The reason is that the exponentially small term is the leading term in 
the splitting of the two classically degenerate ground states. 
</p>
<p>But there is another very significant implication of the tunneling calculation. 
This has to do with the phenomenon of spontaneous symmetry breaking which will 
</p>
<p>now be described. </p>
<p/>
</div>
<div class="page"><p/>
<p>Consider a Hamiltonian which has a symmetry, say under parity. If the lowest 
energy state of the problem is itself not invariant under the symmetry, we say symmetry 
</p>
<p>is spontaneously broken. 
Spontaneous symmetry breaking occurs quite readily in classical mechanics. 
</p>
<p>Consider the single-well osciliator. The Hamiltonian is invariant under parity. The 
ground state is a particle sitting at the bottom of the well. This state respects the 
</p>
<p>symmetry: the effect of parity on this state gives back the state. Now consider the 
double-well with minima at x =&plusmn;a. There are two lowest energy configurations 
available to the particle: sitting still at the bottom of either well. No matter which 
</p>
<p>choice it makes, it breaks the symmetry. The breakdown is spontaneous in that there 
was nothing in the Hamiltonian that tilted the scales. Once the particle has made a 
</p>
<p>choice {based on accidents of initial conditions) the other option does not enter its 
dynamics. Let us note the twin signatures of symmetry breaking: there is more than 
one ground state, and these states are not invariant under the symmetry (some 
</p>
<p>observable, not invariant under the symmetry has a nonzero value), but instead get 
mapped into each other by the symmetry operation. 
</p>
<p>Now consider the quantum case of the double well, but with an infinite barrier 
</p>
<p>between the wells. (I mean a barrier across which tunneling is impossible either in 
the path integrals or wave function approach. So a delta function spike is not such a 
barrier.) Once again the particle has two choices, these being Gaussian-like functions 
</p>
<p>centered at the two troughs: I &plusmn;a). They show the twin features of symmetry break-
ing: they are degenerate and noninvariant under parity ((X) #0). But here is a twist. 
In quantum theory a particle can be in two places at the same time. In particular, 
</p>
<p>we can form the combinations of these degenerate eigenvectors 
</p>
<p>IS/ A)= [I +a)&plusmn;~ -a)] 
.J2 
</p>
<p>lliS/A)=&plusmn;iS/A) 
</p>
<p>which are eigenstates of parity. Indeed, in quantum theory the relation 
</p>
<p>[H, H] =0 
</p>
<p>(21.2.37) 
</p>
<p>(21.2.38) 
</p>
<p>(21.2.39) 
</p>
<p>guarantees that such parity eigenstates can be formed. But should they be formed? 
The answer is negative in this problem due to the infinite barrier. The reason is this. 
</p>
<p>Suppose the particle in question is sighted in one side during a measurement. Then 
there is no way for its wave function to develop any support in the other side. (One 
says the motion is not ergodic.) Even in quantum theory, where energy can be 
nonconserved over small times, barrier penetration is forbidden if the barrier is 
infinite. This means in particular that the symmetric and antisymmetric functions will 
never be realized by any particle that has ever been seen on either side. The correct 
thing to do then is to build a Hilbert space of functions with support on just one side. 
That every state so built has a degenerate partner in the inaccessible well across the 
barrier, is academic. The particle will not even know a parallel universe just like its 
</p>
<p>621 
</p>
<p>PATH 
INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>622 
</p>
<p>CHAPTER 21 
</p>
<p>own exists. Real life will not be symmetric in such a problem and the symmetric 
</p>
<p>and antisymmetric wave functions (with zero &lt;X)) represent unrealizable situations. 
</p>
<p>Symmetry is spontaneously broken. 
</p>
<p>Now for the more typical problem with a finite barrier. In this case, a particle 
</p>
<p>once seen in the left side can later be seen in the right side and vice versa. Symmetric 
</p>
<p>and antisymmetric wave functions are physically sensible and we can choose energy 
</p>
<p>eigenstates which are also parity eigenstates. These states will no longer be degener-
</p>
<p>ate. In normal problems, the symmetric state, or more generally the state with eigen-
</p>
<p>value unity for the symmetry operation, the one invariant under the symmetry 
</p>
<p>operation, will be the unique ground state. Recall that in the oscillator problem the 
</p>
<p>ground state not only had definite parity, it was invariant under parity. Likewise, in 
</p>
<p>the hydrogen atom, the ground state not only had definite angular momentum, the 
</p>
<p>angular momentum was zero and was invariant under rotations. However, in both 
</p>
<p>these problems there was no multiplicity of classical ground states and no real chance 
</p>
<p>of symmetry breakdown. (The oscillator had just one classical ground state at the 
</p>
<p>bottom of the well, and the hydrogen atom had one infinitely deep within the 
</p>
<p>Coulomb well.) What the instanton calculation tells us is that the double well, despite 
</p>
<p>having two classical ground states that break symmetry, has, in the quantum theory, 
</p>
<p>a unique, symmetric, ground state. 
</p>
<p>Thus, even though the tunneling calculation was very crude and approximate, 
</p>
<p>it led to a very profound conclusion: the symmetry of the Hamiltonian is the sym-
</p>
<p>metry of the ground state, symmetry breaking does not take place in the double-
</p>
<p>well problem. 
This concept of symmetry restoration by tunneling (which in turn is tied to the 
</p>
<p>existence of classical euclidean solutions with finite action going from one putative 
</p>
<p>degenerate ground state to another) is very deep and plays a big role in many 
</p>
<p>problems. There have been problems (quantum chromodynamics) where one did not 
</p>
<p>even realize that the minimum one had assumed was unique for years was one of 
</p>
<p>an infinite family of degenerate minima, till an instanton (of finite action) connecting 
</p>
<p>the two classical minima was found and interpreted. We discuss a simpler example 
</p>
<p>to illustrate the generality of the notion: a particle in a periodic potential V(x) = 
</p>
<p>1- cos 2rrx. The minima are at x = n, where n is any integer. The symmetry of the 
</p>
<p>problem is the discrete translation x-&gt; x + l. The approximate states, In), which are 
Gaussians centered around the classical minima, break the symmetry and are con-
</p>
<p>verted to each other by T, the operator that translates x-&gt; x + I 
</p>
<p>Tln)=ln+l) (21.2.40) 
</p>
<p>However, adjacent classical minima are connected by a nonzero tunneling amplitude 
</p>
<p>of the type we just calculated and H has off-diagonal amplitudes between In) and 
</p>
<p>In&plusmn; I). (There are also solutions describing tunneling to next-nearest-neighbor min-
</p>
<p>ima, but these have roughly double the action as the nearest-neighbor tunneling 
</p>
<p>process and lead to an off-diagonal matrix element that is roughly the square of the 
</p>
<p>one due to nearest-neighbor tunneling.) Suppose the one-dimensional world 
</p>
<p>were finite and formed a closed ring of size N, so that there were N degenerate 
</p>
<p>classical minima. These would evolve into N nondegenerate levels (the analogs of 
</p>
<p>IS/ A)) due to the mixing due to tunneling. The ground state would be a symmetric </p>
<p/>
</div>
<div class="page"><p/>
<p>combination: 
</p>
<p>1 N 
IS)=-Lin) 
</p>
<p>.JN] 
</p>
<p>The details are left to the following exercise. 
</p>
<p>Exercise 21.2.2. (Very important) 
Assume that 
</p>
<p>N 
</p>
<p>H=L.. Eoln)(nl-t(ln)(n+ 11 +In+ 1)(nl) 
</p>
<p>(21.2.41) 
</p>
<p>(21.2.42) 
</p>
<p>describes the low-energy Hamiltonian of a particle in a periodic potential with minima at 
integers n. The integers n go from I to N since it is assumed the world is a ring of length N 
so that the N + 1 th point is the first. Thus the problem has symmetry under translation by 
one site despite the finite length of the world. The first term in H represents the energy of the 
Gaussian state centered at x = n. The second represents the tunneling to adjacent minima with 
tunneling amplitude t. Consider the state 
</p>
<p>(21.2.43) 
</p>
<p>Show that it is an eigenstate of T. Find the eigenvalue. Use the condition TN= I to restrict 
the allowed values of () and make sure that we still have just N states. Show that I () &gt; is an 
eigenstate of H with eigenvalue E( 9) =Eo- 2t cos (). Consider N = 2 and regain the double-
well result. (You might have some trouble with a factor of 2 in front of the cos () term. 
Remember that in a ring with just two sites, each site is both ahead and behind the other and 
H couples them twice.) 
</p>
<p>Will the ground state always be invariant under the symmetry operation that 
commutes with H? The answer is yes, as long as the barrier height is finite, or more 
precisely, as long as there is a finite action solution to the euclidean equations of 
motion linking classical minima. This is usually the case for quantum mechanics of 
finite number of degrees of freedom with finite parameters in the Hamiltonian. On 
the other hand, if V0 -+ oo in the periodic potential, there really will be N degenerate 
minima with particles living in any one minimum trapped there forever. In quantum 
field theory, where there are infinitely many degrees of freedom, even if the par-
ameters are finite, the barrier is often infinitely high if all degrees of freedom try to 
jump over a barrier. In other words, symmetry breaking can take place. 
</p>
<p>For a more complete discussion of the tunneling question, you must consult the 
Bibliography, especially the works by Coleman and Rajaraman. These references 
will also answer other questions you might have such as: What about solutions 
where the particle rattles back and forth between the two hilltops in the inverted 
double-well potential? (These give contributions where the prefactors go as higher 
powers of r.) Is there a way to read off the splitting between IS I A) directly from 
(al U(r)l-a) without picking off the term linear in r? (Yes, by summing over an 
infinite amount of rattling back and forth.) You will find many interesting points to 
</p>
<p>623 
</p>
<p>PATH 
INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>624 
</p>
<p>CHAPTER 21 
</p>
<p>ponder, but our result will prove to be correct to leading order in the exponentially 
small quantity e-o /~Js". 
</p>
<p>Imaginary Time Path Integrals and Quantum Statistical Mechanics 
</p>
<p>We now discuss two other reasons for studying imaginary time path integrals. 
</p>
<p>The first concerns quantum statistical mechanics and the second classical statistical 
</p>
<p>mechanics. 
</p>
<p>Consider the partition function for a quantum system: 
</p>
<p>(21.2.44) 
f1 
</p>
<p>where the temperature T and Boltzmann's constant k appear in the combination 
</p>
<p>f3 = 1/kT and where E, is the energy of the nth eigenstate of the Hamiltonian H. We 
can rewrite this as 
</p>
<p>Z=Tr e-PH (21.2.45) 
</p>
<p>where the trace is taken in the eigenbJ.sis of H. Now we exploit the fact that the 
</p>
<p>trace is invariant under a unitary change of basis and switch to the x-basis to obtain 
</p>
<p>Z= f x (xl e-PH lx) dx 
-ex_ 
</p>
<p>(21.2.46) 
</p>
<p>The integrand is of course familiar to us now: 
</p>
<p>&lt;xl exp(-{JH)Ix&gt;=&lt;xl exp ( -~ f3nH)x&gt;= U(x, x, f3n) (21.2.47) 
</p>
<p>In other words, Z is the sum over amplitudes to go from the point x back to the 
</p>
<p>point x in imaginary time r = f3n, in other words, over closed paths. 
</p>
<p>Exercise 21.2.3. Starting with U(x, x, r) for the oscillator (see Eq. (21.2.15) and Exercise 
</p>
<p>(21.2.1)) do the integral over x to obtain Z. Compare this to the sum 
</p>
<p>X 
</p>
<p>Z=L e---fJiiw(n+ 1;2) (21.2.48) 
0 
</p>
<p>This connection between quantum statistical mechanics and imaginary time 
</p>
<p>quantum mechanics is the starting point for a whole industry. Some applications are 
</p>
<p>discussed in the book by Feynman and Hibbs. It would take us too far astray to get 
</p>
<p>into any of these in depth. I will merely show how we take the classical limit of this 
</p>
<p>formula. Consider a single particle of mass min a potential V(x). Then 
</p>
<p>Z(/3) = f dx r [~x] exp [ -~ f:~ [ ~ (~: )2 + V(x( r)) J dr J (21.2.49) </p>
<p/>
</div>
<div class="page"><p/>
<p>where the limits on the functional integral remind us to consider paths starting and 
ending at the same point x, which is then integrated over, via the ordinary integral. 
Consider the limit fJfl-+ 0 either due to high temperatures or vanishing fl (the classical 
limit). Look at any one value of x. We need to sum over paths that start at x, go 
somewhere and come back to x in a very short time fJfl. If the particle wanders off 
a distance Ax, the typical kinetic energy is m(Ax/ fJfl)2 and the suppression factor is 
</p>
<p>(21.2.50) 
</p>
<p>from which it follows that 
</p>
<p>(21.2.51) 
</p>
<p>If the potential does not vary over such a length scale [called the thermal wavelength, 
see Exercise (21.2.4)] we can approximate it by a constant equal to its value at the 
starting point x and write 
</p>
<p>(21.2.52) 
</p>
<p>where in the last step we have used the fact that with V(x) pulled out, the functional 
integral is just the amplitude for a free particle to go from x to x in time fJfl. How 
does this compare with classical statistical mechanics? There the sum over states is 
replaced by an integral over phase space: 
</p>
<p>(21.2.53) 
</p>
<p>where the arbitrary prefactor A reflects one's freedom to multiply Z by a constant 
without changing anything physical since Z is a sum over relative probabilities and 
any prefactor will drop out in any averaging process. Equivalently it corresponds to 
the fact that the number of classical states in a region dx dp of phase space is not 
uniquely defined. If we do the p integral and compare to the classical limit of the 
path integral we see that quantum theory fixes 
</p>
<p>I 
A=-
</p>
<p>2trfl 
(21.2.54) 
</p>
<p>in accordance with the uncertainty principle which associates an area of order 
AX AP ~ fl in phase space with each quantum state. 
</p>
<p>625 
</p>
<p>PATH 
INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>626 
</p>
<p>CHAPTER 21 
</p>
<p>Exercise 21.2.4. Consider a particle at temperature T, with mean energy of order kT. 
</p>
<p>Assuming all the energy is kinetic, estimate its momentum and convert to the de Broglie 
</p>
<p>wavelength. Show that this gives us a number of the order of the thermal wavelength. This 
</p>
<p>is the minimum size over which the particle can be localized. 
</p>
<p>Relation to Classical Statistical Mechanics 
</p>
<p>So far we have discussed the relation of the imaginary time path integral to 
</p>
<p>quantum statistical mechanics. Now we consider its relation to classical statistical 
</p>
<p>mechanics. Consider a classical system with N + 1 sites and a degree of freedom Xn 
at each site. The variables at the end of the chain, called x0 and xN, are fixed. Then 
</p>
<p>(21.2.55) 
</p>
<p>where E is the energy function and we have written f3 in terms of the more familiar 
temperature variable as f3 = (1/kT). Let E have the form 
</p>
<p>N-1 
</p>
<p>E= I [Kt(Xn-Xn-1) 2 +Kzx~] (21.2.56) 
</p>
<p>where the first term represents the springlike coupling between nearest neighbors 
</p>
<p>that forces them to maintain a fixed separation and the second one provides a 
</p>
<p>quadratic potential that discourages each x from wandering off its neutral position 
</p>
<p>x= 0. If we compare this to the discretized imaginary time Feynman path integral 
</p>
<p>for the quantum oscillator 
</p>
<p>foo N-l [ 1 N-l (m (xn-Xn-t) 2 mm 2 2)] U(xo, XN, r) = n dx; exp -- I &amp; - 2 +~- Xn 
-cxc I fi 1 2 &amp; 2 
</p>
<p>(21.2.57) 
</p>
<p>we see the following correspondence: 
</p>
<p>&bull; The Feynman path integral from x0 to xN is identical in form to a classical partition 
</p>
<p>function of a system of N + 1 coordinates Xn with the boundary condition that the 
first and last be fixed at x0 and xN. The variables Xn are interpreted as intermediate 
</p>
<p>state labels of the quantum problem (in the repeated resolution of the identity) 
</p>
<p>and as the classical variables summed over in the partition function. 
</p>
<p>&bull; The role of the action in the Feynman integral is played by the energy in the 
</p>
<p>partition function. 
&bull; The role of 1i is played by T. In particular, as either variable goes to zero, the 
</p>
<p>sum over configurations is dominated by the minimum of action or energy and 
</p>
<p>fluctuations are suppressed. 
</p>
<p>&bull; The parameters in the classical and quantum problems can be mapped into each 
</p>
<p>other. For example, f3K1 =m/2fis and f3K2 =mm2 sj2fi. 
</p>
<p>&bull; Since s ...... 0 in the quantum problem, the parameters of the classical problem must 
</p>
<p>take some limiting values (K1 -+ oo and K2 -+ 0 in a special way) to really be in 
</p>
<p>correspondence with the quantum problem with H = P 2 /2m+ mm2x 2 /2. </p>
<p/>
</div>
<div class="page"><p/>
<p>&bull; The single quantum degree of freedom is traded for a one dimensional array of 
classical degrees of freedom. This is a general feature: the dimensionality goes up 
by 1 as we go from the quantum to the classical problem. For example, a one-
dimensional array of quantum oscillators would map on to the partition function 
of a two-dimensional array of classical variables. The latter array would be labeled 
by the time slice n as well as the quantum oscillator whose intermediate state label 
it stands for. 
</p>
<p>Our emphasis has been on the notion that the quantum oscillator problem can be 
written as a path integral which we now see is also a classical partition function. It 
is just as interesting to take a classical problem and translate it back to the operator 
version. In the classical problem we are interested in the free energy and thermal 
averages over the Boltzmann distribution, i.e., correlation functions like 
</p>
<p>Soc nN-1 dx-x X e-{3E(xo .... ,XN) _ 00 1 1 12 78 
&lt;X X )-~~~=-.----=~--~-12 78- Joo nN 1dX&middot;e-{3E(xo, ... ,xN) 
</p>
<p>-oo 1 r 
</p>
<p>(21.2.58) 
</p>
<p>where we use wedgy brackets to represent thermal averages as we did quantum 
averages, hoping you will be able to keep track of what is meant from the context. 
In the quantum theory we are interested in eigenstates of H, especially the ground 
state, Heisenberg operators, etc. We now develop the dictionary between the two 
approaches. Rather than use the oscillator, we turn to a problem with a simpler 
Hilbert space: that of a spin-1/2 problem. 
</p>
<p>For this purpose consider the Ising model in one dimension. The lattice now is 
an array of N + 1 dots numbered 0 to N. At each point lies an Ising spin which can 
take only two values, s = &plusmn; 1. The partition function is 
</p>
<p>(21.2.59) 
</p>
<p>where K contains the factor -{3. For the case we are interested in, K&gt;O, the Boltz-
mann weight is large when s;=s;+1 and small when s;= -s;+ 1. Thus the nearest-
</p>
<p>. neighbor coupling represents the ferromagnetic tendency of the spins to be aligned 
with their neighbors. The additional, spin independent energy of minus -K per site 
is a shift in energy made for convenience. Given this formula for Z, we can answer 
all thermodynamic questions. This is our classical problem. We will first solve for 
the free energy and correlation function viewing the problem classically. Then we will 
map this into a quantum problem and rederive the same results and our dictionary. 
</p>
<p>Let us first keep s0 fixed at one value and define a relative variable: 
</p>
<p>(21.2.60) 
</p>
<p>627 
</p>
<p>PATH 
INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>628 
</p>
<p>CHAPTER 21 
</p>
<p>It is clear that given so and !;, we can reconstruct the state of the system. Thus, we 
</p>
<p>can write 
</p>
<p>(21.2.61) 
</p>
<p>Since the exponential factorizes into a product over i, we can do the sums over each 
</p>
<p>t; and obtain (after appending a factor of 2 for the two possible choices of s0 ) 
</p>
<p>(21.2.62) 
</p>
<p>One is generally interested in the free energy per site in the thermodynamic limit 
</p>
<p>N--+oo: 
</p>
<p>f(K) = lim I_ ln Z 
N-'XCN 
</p>
<p>(21.2.63) 
</p>
<p>(This definition off differs by a factor - {3 from the more traditional one. I use the 
</p>
<p>present one to reduce the clutter.) We see that 
</p>
<p>f(K)=1n(1+e- 2K) (21.2.64) 
</p>
<p>where we have dropped ln 2/ N in the thermodynamic limit. Had we chosen to fix s0 
</p>
<p>at one of the two values, the factor 2 would have been missing in Eq. (21.2.62) but 
</p>
<p>there would have been no difference in Eq. (21.2.64) for the free energy per site. 
</p>
<p>Boundary conditions are unimportant in the thermodynamic limit in this sense. 
</p>
<p>Consider next the correlation function (which measure the likelihood that spins 
</p>
<p>S; and Sj are parallel): 
</p>
<p>Lsk sfsi exp [ Lk K(sksk+ 1 -1) J 
(sjs;)= Z &middot;&middot; (21.2.65) 
</p>
<p>for j&gt; i. Using the fact that s7 = 1, we can write 
(21.2.66) 
</p>
<p>Thus 
</p>
<p>(21.2.67) 
</p>
<p>where the answer factorizes over i since the Boltzmann weight factorizes over i when 
</p>
<p>written in terms oft;. The average for any one tis easy 
</p>
<p>1 eO&middot;K-1 e-ZK 
(t) = O&middot;K -zK =tanh K 
</p>
<p>e +e 
(21.2.68) </p>
<p/>
</div>
<div class="page"><p/>
<p>so that finally 
</p>
<p>&lt;sA)= (tanh K) 1-; = exp [(j- i) In tanh K] (21.2.69) 
</p>
<p>Note that the result depends on just the difference in coordinates. This is not a generic 
result but a peculiarity of this model. The reason is that the problem of N + 1 points 
(for any finite N) is not translationally invariant. Correlations between two spins 
could, and generally do, depend on where the two points are in relation to the ends. 
On the other hand, in all models we expect that as N-. oo, we will see translational 
invariance far from the ends and deep in the interior. To have translational in variance 
in a finite system, we must use periodic boundary conditions: now the world has the 
shape of a ring and every point is equivalent to every other. Correlation functions 
will now depend only on the difference between the two coordinates but they will 
not decay monotonically with separation! This is because as one point starts moving 
away from the other, it eventually starts approaching the first point from the other 
side! Thus the correlation function will be a sum of two terms, one of which grows 
as j- i increases to values of order N. However, if we promise never to consider 
separations comparable toN, this complication can be ignored [see Exercise (21.2.9)]. 
(Our calculation of correlations in terms oft; must be amended in the face of periodic 
boundary conditions to ensure that the sum over t; is restricted to configurations for 
which the product of t;'s over the ring equals unity.) 
</p>
<p>The correlation length ~ is defined by the formula 
</p>
<p>lim &lt;s;s1)-. e -U-iJ/~ (21.2.70) 
j-i-oo 
</p>
<p>Thus in our problem 
</p>
<p>~-I= -In tanh K (21.2.71) 
</p>
<p>(We have assumedj&gt;i in our analysis. In generalj-i is to be replaced by IJ-il in 
these definitions. Also the model in question shows the exponential behavior for all 
separations and not just in the limit IJ- il -. oo. This too is peculiar to our model 
and stems from the fact that the model is in one spatial dimension and the Ising 
spin can take only two values.) 
</p>
<p>We will now rederive these results in the quantum version. If Z stands for a 
path integral, the Ising variables must be the intermediate state labels that occur in 
the resolution of the identity for a quantum problem. Clearly the quantum problem 
is that of a spin-1 /2. 
</p>
<p>To proceed, let us take another look at 
</p>
<p>Z=I f1 eK(sfS,+t-Il 
s, i 
</p>
<p>(21.2.72) 
</p>
<p>Each exponential factor is labeled by two discrete indices which can take two values 
each. Furthermore, the second label for any factor is the first label for the next. 
</p>
<p>629 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>630 
</p>
<p>CHAPTER 21 
</p>
<p>Finally, these labels are being summed over. It is clear that we are seeing here a 
</p>
<p>matrix product. (We are simply undoing the resolution of the identity.) So we write 
</p>
<p>Z=I (21.2.73) 
</p>
<p>where we have introduced a 2 x 2 matrix T whose rows and columns are labeled by 
</p>
<p>a pair of spins and whose element Tss' equals the Boltzmann weight associated with 
</p>
<p>a pair of neighboring spins in the states, s'. Thus 
</p>
<p>T ~ + = T __ = l, T ~ _ = T_ + = exp( ... 2K) 
</p>
<p>Thus this matrix, called the Transfer Matrix, is given by 
</p>
<p>{21.2.74) 
</p>
<p>and 
</p>
<p>Z= {21.2.75) 
</p>
<p>for the case of fixed boundary conditions (which we will focus on) where the first 
</p>
<p>spin is fixed at s0 and the last at sN. If we sum over the end spins (free boundary 
</p>
<p>conditions) 
</p>
<p>Z= \ 
L. 
</p>
<p>(21.2.76) 
</p>
<p>If we consider periodic boundary conditions where s0 = sN and one sums over these, 
</p>
<p>N Z=Tr T (21.2.77) 
</p>
<p>We will now show the insensitivity of the free energy per site to boundary conditions 
</p>
<p>in the thermodynamic limit. Suppose we used fixed boundary conditions. Then if we 
</p>
<p>write 
</p>
<p>T= lloiO) &lt;Oi +lld I) &lt;I I (21.2.78) 
</p>
<p>where ji), lli [i=O, 1] are the eigenvectors (assumed orthonormal) and eigenvalues 
</p>
<p>ofT, then 
</p>
<p>(21 .2.79) 
</p>
<p>Assuming ), 0 is the bigger of the two eigenvalues, 
</p>
<p>(21.2.80) </p>
<p/>
</div>
<div class="page"><p/>
<p>and 
</p>
<p>(21.2.81) 
</p>
<p>and the free energy per site in the infinite volume limit, 
</p>
<p>(21.2.82, 
</p>
<p>is clearly independent of the boundary spins as long as (Oiso) and (sNIO) do not 
vanish. 
</p>
<p>Exercise 21.2.5. Check this claim for periodic boundary conditions starting with Eq. 
(21.2.75). 
</p>
<p>Let us rewrite T as follows. Consider the identity 
</p>
<p>(21.2.83) 
</p>
<p>=cosh K*(J+tanh K*a 1 ) (21.2.84) 
</p>
<p>where K* Is presently unrelated to K; in particular, it is not the conjugate! If we 
choose 
</p>
<p>tanh K* = e - 2K (21.2.85) 
</p>
<p>we see from Eq. (21.2.74) that up to a prefactor cosh K*, 
</p>
<p>(21.2.86) 
</p>
<p>We will temporarily drop this prefactor but remember to subtract ln cosh K* from 
the free energy per site. It does not, however, affect the correlation function which 
will be seen to depend only on the ratios of eigenvalues of T. Note that K*, called 
the dual of K, is large when K is small and vice versa. 
</p>
<p>For later reference, let us note that in the present case, the eigenvalues of Tare 
e&plusmn;K &bull; and the corresponding eigenvectors are 
</p>
<p>Suppose we write 
</p>
<p>T -H =e 
</p>
<p>(21.2.87) 
</p>
<p>(21.2.88) 
</p>
<p>Then T can be interpreted as the time evolution operator for one time step in the 
imaginary time direction. The spatial site index i of the classical problem has become 
</p>
<p>631 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>632 
</p>
<p>CHAPTER 21 
</p>
<p>the discrete imaginary time index for the quantum problem. The free energy is simply 
</p>
<p>related to Eo, the ground state energy of H: 
</p>
<p>(21.2.89) 
</p>
<p>f=-Eo=K* (21.2.90) 
</p>
<p>Exercise 21.2.6. Show thatfabove agrees with Eq. (21.2.64) upon remembering to sub-
</p>
<p>tract In cosh K"' and using the definition of K"'. 
</p>
<p>Consider next the correlation function (sis;) for}&gt; i. I claim that if the boundary 
</p>
<p>spins are fixed at s0 and sN, 
</p>
<p>(21.2.91) 
</p>
<p>To see the correctness of this, look at the numerator. Retrace our derivation by 
</p>
<p>introducing a complete set of cr3 eigenstates between every factor ofT. Reading from 
</p>
<p>right to left, we get just the Boltzmann weights till we get to site i. There the &lt;13 
</p>
<p>acting on its eigenstate, gives s;, the value of the spin there. Then we proceed as 
</p>
<p>usual to }, repeat this and go to the Nth site. (The dependence of &lt;sis;) on the 
</p>
<p>boundary conditions will be seen to disappear in the thermodynamic limit.) Let us 
</p>
<p>rewrite Eq. (21.2.89) another way. Define Heisenberg operators 
</p>
<p>(21.2.92) 
</p>
<p>In terms of these 
</p>
<p>(21.2.93) 
</p>
<p>Consider now the limit as N-+ oo, i and j fixed at values far from the end points 
</p>
<p>labeled 0 and N so that N- j and i are large, and we may approximate 
</p>
<p>r::::IO)(OIA.g a=N, N-j, i (21.2.94) 
</p>
<p>In this limit, we have from Eq. (21.2.91) 
</p>
<p>(21.2.95) 
</p>
<p>and the dependence on the boundary has dropped out. For the case i&gt; }, we will get 
</p>
<p>the operators in the other order. In general then, 
</p>
<p>(21.2.96) </p>
<p/>
</div>
<div class="page"><p/>
<p>where the time-ordering symbol ff will order the operators with time increasing from 
</p>
<p>the right to left : 
</p>
<p>We will pursue the evaluation of this correlation function using the eigenvectors 
</p>
<p>of T. But 'lrst let us replace a 3(j) by the unit operator in the above derivation to 
obtain the mean magnetization as 
</p>
<p>(21.2.98) 
</p>
<p>In our example, 10) is the eigenket of O't so that there is no mean magnetization. 
The only exception is at zero temperature or zero K*: now the eigenvalues are equal 
</p>
<p>and we can form linear combinations corresponding to either of the fully ordered 
</p>
<p>(up or down) a 3 eigenstates. 
Let us compare symmetry breaking and its restoration in the Ising problem to 
</p>
<p>what happened in the double well. 
</p>
<p>&bull; In the limit 1i ~ 0, the particle in the double-well seeks the minimum of the 
</p>
<p>euclidean action: 
</p>
<p>(21.2.99) 
</p>
<p>which is given by (dx/dr)=O, x=&plusmn;a, the minima of the double-well potential. 
</p>
<p>There is degeneracy and symmetry breaking in the ground state. A particle that 
</p>
<p>starts out in one well will not ever go to the other in the course of time. Even 
</p>
<p>though n commutes with H, we do not form parity eigenstates, instead we form 
eigenstates of position (or more accurately, well index, left or right). In the Ising 
</p>
<p>problem, in the limit of zero temperature, the partition function is dominated by 
</p>
<p>the state of minimum energy, with all spins up or all spins down on all sites (which 
</p>
<p>can be viewed as discrete points in imaginary time of the spin-1/2 problem). In 
</p>
<p>the operator language, T and H commute with a 1 in general and eigenstates of 
Hare chosen to be eigenstates of a 1 as well. But at zero K*, the two eigenstates 
become degenerate and we form combinations which arc chosen to be eigenstates of 
</p>
<p>0"3. This is because a state starting out up/down with respect to al will stay that 
way forever. (In classical statistical mechanics terms, if the spin at one of the chain 
</p>
<p>is up/down, all will be up/down at zero temperature.) 
</p>
<p>&bull; For nonzero fi, there is tunneling between the wells, degeneracy is lifted and 
</p>
<p>symmetry is restored in the ground state. This is thanks to an instanton configura-
</p>
<p>tion that has finite action and connects the two classical ground states. In the 
Ising problem, for nonzero K*, i.e., nonzero temperature, there exist instantonlike 
configurations in which the spin starts out up at one end of the chain (i.e., the 
distant past in the imaginary time interpretation) and at some point flips down 
and vice versa. This has finite energy (only one pair of nearest-neighbor spins is 
antiparallcl and the additional energy cost is 2K}. The eigenstates of the transfer 
matrix (or the spin Hamiltonian) are now the symmetric and antisymmetric 
</p>
<p>633 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
</p>
<p>PARTH </p>
<p/>
</div>
<div class="page"><p/>
<p>634 
</p>
<p>CHAPTER 21 
</p>
<p>combinations of the up and down states, i.e., eigenstates of a 1 &bull; The ground state 
is unique and symmetric. 
</p>
<p>Exercise 21.2.7. Consider the Hamiltonian of the spin-! /2 problem that arises in the 
</p>
<p>transfer matrix treatment of the Ising chain 
</p>
<p>ll= ---K*u, (21.2.100) 
</p>
<p>The off-diagonal matrix element (after pulling out the sign), i.e., K*. must represent the 
</p>
<p>tunneling amplitude (for going from up to down ground state) per unit time in the low-
</p>
<p>temperature limit (which you recall is like the 11----&gt; 0 limit). The preceding discussion tells us 
</p>
<p>it is just e- 'K where 2K is the energy cost of the interface of the up and down ground states. 
Verify that these two results agree for low temperatures by going back to the definition 
</p>
<p>of K* 
</p>
<p>Let us return to Eq. (21.2.96). Even though it appears that everything depends 
</p>
<p>on just the ground state, a knowledge of all states is required even in the infinite 
</p>
<p>volume limit to evaluate the correlat.ion. Going to Eq. (21.2.96) for the casej&gt;i, let 
</p>
<p>us insert the complete set of (two) eigenvectors of T between the Pauli matrices. 
</p>
<p>When we insert 10) (01 we get (s)2, the square of the magnetization which happens 
</p>
<p>to vanish here. Moving it to the left-hand side, we get the connected correlation 
</p>
<p>function 
</p>
<p>1 A.1 
= ( &middot;~&middot;&middot;&middot;&middot; 
</p>
<p>\Ao 
</p>
<p>(21.2.102) 
</p>
<p>(2!.2.103) 
</p>
<p>Let us note that 
</p>
<p>&bull; The correlation depends only on ratios of the eigenvalues of T and falls exponen-
</p>
<p>tially with distance with a coefficient 2K*. Now 2K* is just the gap to the first 
</p>
<p>excited state of the Hamiltonian H defined by T= e -H which in our example is 
</p>
<p>-K*a 1 &bull; The result 
</p>
<p>~&middot;&middot;&middot;J =E1- Eo=m (2!.2.104) 
</p>
<p>is also very general. The reason one uses the symbol 1n for the gap (called the 
mass gap) is that in a field theory the lowest energy state above the vacuum is a 
</p>
<p>single particle at rest and this has energy m (in units wht~re c = 1 ). 
&bull; The connected correlation function is determined by the matrix clement of the 
</p>
<p>operator in question ( a 3 ) between the ground state and the next excited state. 
</p>
<p>This is also a general feature. If this matrix element vanishes. we must go up in 
</p>
<p>the levels till we find a state that is connected to the ground state by the action 
of the operator. (In this problem we know I(Oia311)1 2 = l since a3 is the spin-flip 
</p>
<p>operator for the eigenstates of a,.) </p>
<p/>
</div>
<div class="page"><p/>
<p>This simple example has revealed most of the general features of the problem. The 
only difference is that for a bigger transfer matrix, the sum over states will have 
more than two terms. Thus the correlation function will be a sum of decaying 
exponentials and a unique correlation length will emerge only asymptotically when 
the smallest mass gap dominates. Also in the more complex problems (in higher 
dimensions) there may not be any finite action instantons connecting the multiple 
classical minima and there can be many ground states of H with broken symmetry. 
Assuming this happens, as it does in the two-dimensional Ising model (below some 
temperature Tc), you can ask: how does the ground state choose between spin up 
and spin down since there is no bias in the Boltzmann weight to make the choice? 
The answer is that indeed, if we do not set any bias, the system will always pick a 
mean magnetization of zero. How then do we know that the system is ready to 
magnetize? We use a principle called clustering. It states that as i and j separate, 
(sjs;) .... (sj) (s, ). The idea is that if i lies in our galaxy and j lies in another they 
become statistically independent. Consider now the two-dimensional Ising model 
below Tc. In zero field we will find that (sjs;) does not approach (s;) (sj) (which 
is zero since we gave the system no reason to choose one value of magnetization 
over its opposite) but that instead (s;sj) approaches the square of the magnetization 
the system will have if you would only give it the slightest reason for choosing one 
sign over the other. At this point, having seen the breakdown of clustering for the 
spin variable, you are to modify the partition function to restore clustering in two 
equivalent ways. One is to limit the sum over states to those with a net positive (or 
negative) magnetization. Then (s) #0 any more and you will find that 
(s,) (sj) .... (s/. The other option is to apply a small field, calculate the magnetiza-
tion, and let the field go to zero. (This too essentially kills half the states in the sum. 
Both recipes reflect the fact that a magnet below its Tc will not be able to dynamically 
evolve from pointing up to pointing down. Recall the particle trapped on one side 
of the infinite barrier between the two wells. Thus summing over things the system 
cannot do is a mistake.) Now, the magnetization is the derivative of the free energy 
with respect to the applied field h. It is easy to show that it is an even function of 
h.&middot; [See Exercise 21.2.8).] If the system does not want to magnetize, you will find 
that f ~ h2, so that df/ dh -&gt; 0 as h -&gt; 0. On the other hand if it wants to magnetize 
you will find f ~I hi and df/ dh ~sign h. 
</p>
<p>Exercise 21.2.8. Consider the Ising model in a magnetic field by adding a term hIs, to 
the exponent in Eq. (21.2.59). Show that Z(h) = Z( -h). Show that the transfer matrix T= 
eK*a, eh"'= TKTh reproduces the Boltzmann weight. Note that Tis not Hermitian. By splitting 
the coupling to h into two factors, show that T~ 12 T KT~ 12 is just as good and also Hermitian. 
Find its eigenvalues and eigenvectors and show that there is degeneracy only for h = K* = 0. 
Find the magnetization as a function of h by evaluating (s) = (01 cr310). Starting with the 
partition function, show that 
</p>
<p>&lt;s&gt;=_!_ ainz=aj 
N fJh fJh 
</p>
<p>Evaluate /from the largest eigenvalue ofT and regain the answer for (s) found from (s) = 
(01 cr3IO). 
</p>
<p>Exercise 21.2.9. Consider the correlation function for the problem with periodic bound-
ary conditions and write it as a ratio of two traces. Saturate the denominator with the largest 
</p>
<p>635 
</p>
<p>PATH 
INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>636 
</p>
<p>CHAPTER 21 
</p>
<p>eigenket, but keep both eigenvectors in the numerator and show that the answer is invariant 
</p>
<p>underj-i+-+N-(j-i). Using the fact that a 3 exchanges 10) and II) should speed things up. 
</p>
<p>Provide the interpretation. Argue that as long asj-i is much smaller than tV, only one term 
</p>
<p>is needed. 
</p>
<p>Exercise 21.2.10. Recall the remarkable fact that the correlation function (sis,) in the 
</p>
<p>Ising model was translationaily invariant in the finite open chain vvith one end 11xed at s0 . 
</p>
<p>Derive this result using the transfer matrix formalism as follows. 
</p>
<p>Explicitly evaluate a.li J by evaluating T-1rr3T1 in terms of a3 and a,. Show that 
rr3(j )0"3(i) is a function only ofj- i by using some identities for hyperbolic functions. Keep 
</p>
<p>going till you explicitly have the correlation function. It might help to use 
</p>
<p>L,,, ls.v) =(I+ a, )I so). 
</p>
<p>21.3. Spin and Fermion Path Integrals 
</p>
<p>Now we turn to path integrals for two systems with no classical limit: a spinS 
</p>
<p>system and a fermionic oscillator, to be described later. The fermion problem will 
</p>
<p>be somewhat abstract at this stage, but it is in here because you are likely to see it 
</p>
<p>in many different branches of physics. 
</p>
<p>Spin Coherent States and Path Integral 
</p>
<p>Consider a spin 5' degree of freedom. The Hilbert space is 2S+ I dimensional. 
</p>
<p>Choosing Sc eigenstates as our basis we can write the propagator U(t) as a sum over 
</p>
<p>configurations by using the resolution 
</p>
<p>s 
</p>
<p>I= 'i IS=&gt;&lt;S=I (21.3.1) 
... s 
</p>
<p>The intermediate states will have discrete labels (as in the Ising model). 
We consider here an alternate scheme in which an overcomplete basis is used. 
</p>
<p>Consider the spin coherent state 
</p>
<p>10)::=10, o/)= U(R(Q))ISS) (21 .:1.2) 
</p>
<p>where 1 Q) denotes the state obtained by rotating the normalized, fully polarized 
state, 1 SS) by an angle () around the x-axis and then by cp around the z-axis using 
the unitary rotation operator U(R(Q) ). 
</p>
<p>Given that 
</p>
<p>(SSISISS)=kS (21.3.3) </p>
<p/>
</div>
<div class="page"><p/>
<p>it is clear (say by considering utsu) that 
</p>
<p>(01 Sl 0) = S(i sin () cos &cent; + j sin () sin &cent; + k cos ()) (21.3.4) 
</p>
<p>Note that our spin operators are not defined with an ll. Thus for spin-1, the eigen-
values of Sz are 0, &plusmn;1. 
</p>
<p>Exercise 21.3.1. Show the above result by invoking Eq. (12.4.13). 
</p>
<p>The coherent state is one in which the spin operator has a nice expectation value: 
equal to a classical spin of length S pointing along the direction of 0. It is not an 
eigenvector of the spin operator (not expected anyway since the three components of 
spin do not commute) and higher powers of the spin operators do not have expecta-
tion values equal to the corresponding powers of the classical spin. For example, 
(01 S~l 0) i= S 2 sin2 () cos2 &cent;. However, the difference between this wrong answer and 
the right one is of order S. Generally the nth power of the spin operator will have 
an expectation value equal to the nth power of the expectation value of that operator 
plus corrections that are of order sn-t. If S is large, they may be ignored. This is 
so when one usually uses the present formalism. 
</p>
<p>Let us now examine the equation 
</p>
<p>(21.3.5) 
</p>
<p>The result is obviously true for S= 1/2, given that the up spinor along the direction 
()&cent;is 
</p>
<p>() ~ () 
10) =I 6&cent;) =cos -11/2, 1/2) + e' sin -11/2, -1/2) 
</p>
<p>2 2 
(21.3.6) 
</p>
<p>As for higher spin, imagine 2S spin-1 /2 particles joining to form a spin S state. 
There is only one direct product state with Sz = S: where all the spin-1 /2's are 
pointing up. Thus the normalized fully polarized state is 
</p>
<p>ISS)=11/2, 1/2)&reg;11/2, 1/2)&reg; &middot; &middot; &middot;11/2, 1/2) (21.3.7) 
</p>
<p>If we now rotate this state, it becomes a tensor product of rotated states and when 
we form the inner product in the left-hand side of Eq. (21.3.5), we obtain the right-
hand side. 
</p>
<p>The resolution of the identity in terms of these states is 
</p>
<p>1= 28+ 1 JdniO)(OI 
4n-
</p>
<p>(21.3.8) 
</p>
<p>where dO= d cos () d&cent;. The proof can be found in the references. You are urged to 
do the following exercise that deals with S= 1/2. 
</p>
<p>637 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>638 
</p>
<p>CHAPTER 21 
</p>
<p>Exercise 21.3.2. Prove the completeness relation for S= 1/2 by carrying out the integral 
over Q using Eq. (21.3.6). 
</p>
<p>When we work out the path integral we will get a product of factors like the 
</p>
<p>following: 
</p>
<p>ir 
&middot; &middot; &middot; (Q(I + c)II -tf H(S )IQ(t))&middot; &middot; &middot; (21.3.9) 
</p>
<p>We work to order E. Since H already has a factor of c in front of it, we set 
</p>
<p>is ic 
(D.(t + c)l ----'- H(S )IQ(t)) ~- &middot;- (D.(t)l H(S )IQ(t)) = -ic:.:!f (D.) (21.3. I 0) 
</p>
<p>11 . 11 
</p>
<p>If the Hamiltonian is linear in S, we simply replace the quantum spin operator 
</p>
<p>by the classical vector pointing along 8, &cent; and if not, we can replace the operator 
</p>
<p>by the suitable expectation value in the state IQ(t)). This is what we called /1.:/f(Q) 
</p>
<p>in the preceding equation. 
</p>
<p>Next we turn to lhe product 
</p>
<p>(Q(t+ E)IQ(t)) ~ 1- iES( I-eos 0)&lt;/J--: (21.3.11) 
</p>
<p>where we have expanded Eq. (21.3.5) to first order in tJ.O and tJ.&cent;. This gives us the 
following representation of the propagator in the continuum limit: 
</p>
<p>&lt;D.rl U(f)IQ; &gt; = j" f?Q expli II [S cos e&cent;- .Yf(Q )] drj 
~It 
</p>
<p>(21.3.12) 
</p>
<p>where a total derivative in &cent; has been dropped and J f?Q is the measure with all 
factors of 7r in it. 
</p>
<p>Even by the standards of continuum functional integrals we have hit a new 
</p>
<p>low, when we replaced differences by derivatives as if the paths are smooth. In the 
</p>
<p>configuration path integral, we saw that between one time and the next the fluctuation 
</p>
<p>in x was of the order c 1 2 which is why we had to expand (n(R')In(R)) to order 
</p>
<p>(R'- R) 2 in the Berry calculation of the effective interaction. The factor that provided 
</p>
<p>any kind of damping on the variation in the coordinate was the kinetic energy terrn 
</p>
<p>exp[im(x'- xf/21ic:]. In the present problem there is no such term. There is no reason 
</p>
<p>why the difference in Q from one time to another should be treated as a small 
</p>
<p>quantity. Thus although the discretized functional integral is never wrong (since all 
we use is the resolution of the identity) any further assumptions about the smallness 
of the change in Q from one time to the next are suspect. There is one exception. 
</p>
<p>Suppose S-&bull;&lt;t::. Then we see from Eq. (21.3.5) that the overlap is unity if the two 
states are equal and falls rapidly if they are different. (It is easier to consider the 
</p>
<p>case &cent; 2 = &cent; 1 .) This is usually the limit ( S-. in which one uses this formalism. 
We now consider two simple applications. First let 
</p>
<p>(21.3.13) </p>
<p/>
</div>
<div class="page"><p/>
<p>We know the allowed eigenvalues are 1i(-S, -S+ 1, ... , S). Let us derive this from 
</p>
<p>the continuum path integral. 
Given (OIHiil) = ns cos 0, it follows that Yf = s cos 0, and that the functional 
</p>
<p>integral is 
</p>
<p>[f !'}cos ()!'}lj&gt; J exp[is f (cos ()~-cos()) dt] (21.3.14) 
We note that 
</p>
<p>&bull; This is a phase space path integral with cos() as the momentum conjugate to 4&gt;! 
&bull; Phase space is compact here (the unit sphere), as compared to the problem of a 
</p>
<p>particle moving on a sphere for which configuration space is compact but all 
</p>
<p>momenta are allowed and phase space is infinite in extent. 
</p>
<p>&bull; The spinS plays the role of 1/1i. 
&bull; The Hamiltonian for the dynamics is cos () since we pulled out the S to the front. 
</p>
<p>In particular, this means that cos () is a constant of motion, i.e., the orbits will be 
</p>
<p>along fixed latitude. 
</p>
<p>Recall the WKB quantization rule 
</p>
<p>f p dq=27rn1i (21.3.15) 
for a problem with no turning points. In our problem, p =cos () is just the conserved 
</p>
<p>energy E. Of all the classical orbits along constant latitude lines, the ones chosen by 
</p>
<p>WKB obey 
</p>
<p>(21.3.16) 
</p>
<p>since S -I plays the role of n. The allowed energies are 
</p>
<p>E ='!_ 
n S 
</p>
<p>(21.3.17) 
</p>
<p>Note that there is exactly enough room in this compact phase space for 2S + 1 orbits 
and that the allowed values of E translate into the allowed values of H when we 
</p>
<p>reinstate the factor of ns that was pulled out along the way. 
So we got lucky with this problem. In general, if H is more complicated we 
</p>
<p>cannot hope for much luck unless Sis large. Now you may ask why we bother with 
</p>
<p>this formalism given that spins of real systems are very small. Here is at least one 
</p>
<p>reason, based on a problem I am familiar with. In nuclear physics one introduces a 
</p>
<p>pseudospin formalism in which a proton is called spin up and the neutron is called 
</p>
<p>spin down. A big nucleus can have a large pseudospin, say 25. The Hamiltonian for 
</p>
<p>the problem can be written in terms of the pseudospin operators and they can be 
</p>
<p>639 
</p>
<p>PATH 
INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>640 
</p>
<p>CHAPTER 21 
</p>
<p>50 x 50 matrices. Finding the energy levels analytically is hopeless. But we can turn 
the large S in our favor by doing a WKB quantization using the appropriate H. 
</p>
<p>Coherent states are also very useful in the study of interacting quantum spins. 
For example, in the one-dimensional Heisenberg model, the Hamiltonian is a sum 
of dot products of nearest neighbor spin operators on a line of points. Since each 
spin operator appears linearly, the Hamiltonian in the action is just the quantum 
one with S replaced by a classical vector of length S. Even though the spin is never 
very large in these problems, one studies the large S limit to get a feeling for the 
subject and to make controlled approximations in 1/S. 
</p>
<p>Fermion Oscillator and Coherent States 
</p>
<p>Let us recall that in the case of the harmonic oscillator the fact that the energy 
levels were uniformly spaced 
</p>
<p>E= nfzm (21.3.18) 
</p>
<p>(dropping zero point motion) allowed one to introduce the notion of quanta. Rather 
than saying the oscillator was in the nth state we could say there was one quantum 
level of energy fzro and there were n quanta in it. This is how phonons, photons, 
etc., are viewed, and it is a very seminal idea. 
</p>
<p>That the level could be occupied by any number of quanta meant they were 
bosons. Indeed our perception of a classical electric or magnetic field is thanks to 
this feature. 
</p>
<p>Consider now a variant of the problem wherein the quanta are fermions. Thus 
the level can contain one or no quanta. There can be no macroscopic field associated 
this state, which is why the fermion problem is unfamiliar to us at first. We now 
develop the theory of a fermionic oscillator. 
</p>
<p>We start by writing down the Hamiltonian: 
</p>
<p>(21.3.19) 
</p>
<p>What distinguishes this problem from the bosonic one are the anticommutation 
relations: 
</p>
<p>{'Pt, 'P} = 'Pt'P + 'P'Pt = 1 
</p>
<p>{'I', 'P} = {'Pt, 'Pt} = 0 
</p>
<p>Note that the last equation tells us 
</p>
<p>(21.3.20) 
</p>
<p>(21.3.21) 
</p>
<p>(21.3.22) 
</p>
<p>This equation will be used all the time without explicit warning. We shall see that it 
represents the Pauli principle forbidding double occupancy. The number operator 
</p>
<p>(21.3.23) </p>
<p/>
</div>
<div class="page"><p/>
<p>obeys 
</p>
<p>(21.3.24) 
</p>
<p>Thus the eigenvalues of N can only be 0 or 1. The corresponding normalized eigen-
</p>
<p>states obey 
</p>
<p>MO)=OIO) (21.3.25) 
</p>
<p>Nil)= Ill) (21.3.26) 
</p>
<p>We will now prove that 
</p>
<p>(21.3.27) 
</p>
<p>I.JIIl)=IO) (21.3.28) 
</p>
<p>As for the first, 
</p>
<p>(21.3.29) 
</p>
<p>which shows that I.JitiO) has N= 1. Its norm is unity: 
</p>
<p>(2l.3.30) 
</p>
<p>It can be similarly shown that I.JIIl) = 10) after first verifying that 'Pil) is not a null 
vector, that it has unit norm. 
</p>
<p>There are no ot~.er vectors in the Hilbert space: any attempts to produce more 
</p>
<p>states are thwarted by I.J12 = I.Jit2 = 0. In other words, the Pauli principle rules out 
more vectors: the state is either empty or singly occupied. 
</p>
<p>Thus the Fermi oscillator Hamiltonian 
</p>
<p>(21.3.31) 
</p>
<p>has eigenvalues 0 and no. 
We will work not with H 0 but with 
</p>
<p>H=Ho&middot;&middot;--pN (21.3.32) 
</p>
<p>where J--1 is called the chemical potential. For the oscillator, since 
</p>
<p>(21.3.33) 
</p>
<p>this merely amounts to measuring all energies relative to the chemical potential. The 
</p>
<p>role of the chemical potential will be apparent soon. 
</p>
<p>641 
</p>
<p>PATH 
INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>642 
</p>
<p>CHAPTER 21 
</p>
<p>Let us now turn to thermodynamics. The central object here is the grand partition 
function, defined to be 
</p>
<p>(21.3.34) 
</p>
<p>where the trace is over any complete set of eigenstates, f3 is the inverse temperature 
1/kT, and A is the free energy (different from the traditional one by a factor -f3). 
The term grand partition function signifies that we are summing over states with a 
different number of particles or quanta. For this reason the free energy is denoted 
by A and not f Just as 8 controls the amounts of energy the system takes from the 
reservoir, p controls the number of particles. (This description is also possible for 
the bosonic oscillator. Instead of saying that we have just one oscillator which can 
be in any state labeled by n, and viewing the sum over states as the partition function 
of one oscillator, we can focus on the quanta and say that we are summing over 
states with variable number of quanta and interpret the usual sum over states as a 
grand partition function.) 
</p>
<p>If we use the N basis, this sum is trivial: 
</p>
<p>(21.3.35) 
</p>
<p>All thermodynamic quantities can be deduced from this function. For example, it is 
clear from Eq. (21.3.34) that the mean occupation number is 
</p>
<p>(21.3.36) 
</p>
<p>Exercise 21.3.3. Prove the formula for (N) in general, starting with Eq. (21.3.34). (Write 
out the trace in a basis common to H and N, as a sum over energy levels at any one N, 
followed by a sum over N.) 
</p>
<p>At zero temperature we find from Eq. (21.3.36) 
</p>
<p>(N) = 8(p-!1o) (21.3.37) 
</p>
<p>i.e., the fermion is present if its energy is below chemical potential and absent if it 
is not. At finite temperatures the mean number varies more smoothly with p. 
</p>
<p>We will now develop a path integral formula for the partition function. 
We proceed in analogy with the bosonic oscillator by trying to find a fermion 
</p>
<p>coherent state I 'I') which is an eigenstate of the destruction operator 
</p>
<p>(21.3.38) 
</p>
<p>The eigenvalue 'I' is a peculiar beast because if we act once more with '&yen; we find 
</p>
<p>(21.3.39) 
</p>
<p>since '&yen;2 = 0. Any ordinary variable whose square is zero is itself zero. But this 'I' is 
no ordinary variable, it is a Grassmann variable. These variables anticommute with </p>
<p/>
</div>
<div class="page"><p/>
<p>each other and with all fermionic creation and destruction operators. (They will 
</p>
<p>therefore commute with a string containing an even number of such operators.) 
</p>
<p>That is how they are defined. The variable ljf is rather abstract and defined by its 
anticommuting nature. There are no big or small Grassmann variables. You will get 
</p>
<p>used to them and even learn to love them just as you did the complex numbers. 
</p>
<p>(Surely when you first heard it, you did not readily embrace the notion that 4i was 
an honest solution to the question "What number times itself gives -16?" You 
</p>
<p>probably felt that it may be the right answer, but it sure wasn't a number.) 
We now write down the coherent state. It is 
</p>
<p>(21.3.40) 
</p>
<p>where ljf is a Grassmann number. This state obeys: 
</p>
<p>(21.3.41) 
</p>
<p>(21.3.42) 
</p>
<p>(21.3.43) 
</p>
<p>(21.3.44) 
</p>
<p>(21.3.45) 
</p>
<p>where we have appealed to the fact that IJI anticommutes with '&yen; and that ljl 2 = 0. If 
we act on both sides of Eq. (21.3.45) with'&yen;, the left vanishes due to '&yen;2 = 0 and the 
</p>
<p>right due to 1J1 2 = 0. 
It may be similarly verified that 
</p>
<p>(21.3.46) 
</p>
<p>where 
</p>
<p>(Vii =(01-(li.Y=(OI+.Y(ll (21.3.47) 
</p>
<p>Please note two points. First, the coherent state vectors are not the usual vectors 
</p>
<p>from a complex vector space since they are linear combinations with Grassmann 
</p>
<p>coefficients. Second, Vi is not in any sense the complex conjugate of ljf and (Vii is 
not the adjoint of IIJI). You should therefore be prepared to see a change of 
</p>
<p>Grassmann variables in which 1/f and Vi undergo totally unrelated transformations. 
The inner product of two coherent states is 
</p>
<p>&lt;.YIIfl&gt; = C (01- &lt;IIl/1)(10) -1/fll)) (21.3.48) 
</p>
<p>= (010) +&lt;II Vi Ifill&gt; (21.3.49) 
</p>
<p>=l+lj/IJI (21.3.50) 
</p>
<p>(21.3.51) 
</p>
<p>643 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
</p>
<p>PART ri </p>
<p/>
</div>
<div class="page"><p/>
<p>644 
</p>
<p>CHAPTER 21 
</p>
<p>Any function of a Grassmann variable can be expanded as follows: 
</p>
<p>(21.3.52) 
</p>
<p>there being no higher powers possible. 
We will now define integrals over Grassmann numbers. (Don't throw up your 
</p>
<p>hands: it will be over in no time.) These have no geometric significance (as areas or 
volumes) and are formally defined. We just have to know how to integrate 1 and 1/f 
since that takes care of all possible functions. Here is the list of integrals: 
</p>
<p>(21.3.53) 
</p>
<p>(21.3.54) 
</p>
<p>That's it! As you can see, a table of Grassmann integrals is not going to be a best-
seller. (For those of you have trouble remembering all these integrals, here is a useful 
mnemonic: the integral of any function is the same as the derivative! Verify this.) 
There are no limits on these integrals. Integration is assumed to be a linear operation. 
The differential dl/f is also a Grassmann number. Thus J dl/fl/f = -1. The integrals 
for t7i or any other Grassmann variable are identical. These integrals are simply 
assigned these values. They are very important since we see for the first-time ordinary 
numbers on the right-hand side. Anything numerical we calculate in this theory goes 
back to these integrals. 
</p>
<p>A result we will use often is this: 
</p>
<p>(21.3.55) 
</p>
<p>Note that if the differentials or variables come in any other order there can be a 
change of sign. For example, we will also invoke the result 
</p>
<p>(21.3.56) 
</p>
<p>Let us now consider some Gaussian integrals. You are urged to show the following: 
</p>
<p>(21.3.57) 
</p>
<p>(21.3.58) </p>
<p/>
</div>
<div class="page"><p/>
<p>where in the second formula M is a 2-by-2 matrix, lfl is a column vector with entries 
</p>
<p>lf/1 and lf/2, 1{1 a column vector with entries lft1 and lft2 and [dl{l dlfl] = 
dl{l1 dlf/1 dl{l2 dlf/2&middot; This result is true for matrices of any size. To prove these simply 
</p>
<p>expand the exponential and do the integrals. 
</p>
<p>Exercise 21.3.4. Prove the above two equations. 
</p>
<p>Consider next the "averages" over the Gaussian measure: 
</p>
<p>(21.3.59) 
</p>
<p>The proof is straightforward and left as an exercise. 
</p>
<p>Exercise 21.3.5. Provide the missing details in the evaluation of the above integral. 
</p>
<p>Exercise 21.3.6. Jacobians for Grassmann change of variables are the inverses of what 
</p>
<p>you expect. Start with J acp dcp =a. Define x = acp, write dcp = J( cp I x) dx and show that 
J(c/J/x)=a and not l/a. (Treat the Jacobian as a constant that can be pulled out of the 
integral.) Evaluate Eq. (21.3.57) by introducing x =a'lf. Remember there is no need to 
change tji. 
</p>
<p>Consider now two sets of Grassmann variables (labeled I and 2). It is readily shown 
</p>
<p>that 
</p>
<p>Exercise 21.3.7. Prove the above result. 
</p>
<p>Exercise 21.3.8. Show that 
</p>
<p>=lit/ lJ jk- liik lJ jl 
</p>
<p>a; a1 ai a1 
</p>
<p>= &lt;Tl&gt; &lt;]k)- (ik) &lt;Ji&gt; 
</p>
<p>(21.3.60) 
</p>
<p>(21.3.61) 
</p>
<p>(21.3.62) 
</p>
<p>(21.3.63) 
</p>
<p>(21.3.64) 
</p>
<p>This is called Wick's theorem and is very useful in field theory and many-body theory. 
</p>
<p>645 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>646 
</p>
<p>CHAPTER 21 
</p>
<p>We need two more results before we can write down the path integral. The first is 
</p>
<p>the resolution of the identity: 
</p>
<p>(21.3.65) 
</p>
<p>In the following proof of this result we will use all the previously described properties 
</p>
<p>and drop terms that are not going to survive integration. (Recall that only ~~If= 
</p>
<p>-'iilf! has a nonzero integral.) 
</p>
<p>fllf)(lf!l e-"'"' dlf!dlf= Jllf)(ilfl(l-l{llf)dlf!dlf 
</p>
<p>= Jno&gt;-lfll&gt;J(&lt;OI-&lt;IIlf!)(l-lf!lf)dlfidlf 
</p>
<p>= J (IO) &lt;OI + lfll) &lt;IIlf!)(l-lf!v') dvr dv' 
</p>
<p>= !O) (01 f (-lf!'ii) dlf! d'if+ 11 &gt;&lt;II f 'iilf! dlf! d'if 
=I (21.3.66) 
</p>
<p>The final result we need is that for any bosonic operator (an operator made of an 
</p>
<p>even number of Fermi operators) 
</p>
<p>I" 
</p>
<p>Tr 0= J 1)/IOilJI) e &bull;N dlf! d'if (21.3.67) 
</p>
<p>The proof is very much like the one just given and is left as an exercise. 
</p>
<p>Exercise 21.3.9. Prove the above formula for the trace. 
</p>
<p>The Fermionic Path Integral 
</p>
<p>Consider the partition function for a single oscillator: 
</p>
<p>(21.3.68) 
</p>
<p>(21.3.69) 
</p>
<p>You cannot simply replace q1t and 'I' by -l[! and lfl, respectively,. in the exponential. 
</p>
<p>This is because when we expand out the exponential not all the 'l's will be acting to 
</p>
<p>the right on their eigenstates and neither will all 'l't s be acting to the left on their 
</p>
<p>eigenstates. (Remember that we are now dealing with operators, not Grassmann </p>
<p/>
</div>
<div class="page"><p/>
<p>numbers. The exponential will have an infinite number of terms in its expansion.) 
We need to convert the exponential to its normal ordered form in which all the 
creation operators stand to the left and all the destruction operators to the right. 
Luckily we can write down the answer by inspection: 
</p>
<p>(21.3.70) 
</p>
<p>whose correctness we can verify by considering the two possible values of \{'t\{1. 
(Alternatively, you can expand the exponential and use the fact that Nk = N for any 
nonzero k.) Now we may write 
</p>
<p>(21.3.71) 
</p>
<p>(21.3.72) 
</p>
<p>(21.3.73) 
</p>
<p>(21.3.74) 
</p>
<p>as expected. While this is the right answer, this is not the path integral approach. It 
does, however, confirm the correctness of all the Grassmannian integration and 
minus signs. As for the path integral approach the procedure is the usual one. 
Consider 
</p>
<p>Z=Tr e-PH (21.3.75) 
</p>
<p>where His a normal ordered operator H(\{1\ \{1 ). We write the exponential as follows: 
</p>
<p>(21.3.76) 
</p>
<p>= (1 - sH) ... ( 1 - sH) s= /3/N (21.3.77) 
Ntimes 
</p>
<p>take the trace as per Eq. (21.3.67) by integrating over ifiof/lo, and introduce the 
resolution of the identity N- I times: 
</p>
<p>N-1 
</p>
<p>X (t/iN-21 ... I f//I) e-'iil'l'l (t/iii(l- sH)I f//o) e-Yio'l'o n difiidf/1; 
i=O 
</p>
<p>(21.3.78) 
</p>
<p>647 
</p>
<p>PATH 
INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>648 
</p>
<p>CHAPTER 21 
</p>
<p>Now we may legitimately make the replacement 
</p>
<p>(21.3.79) 
</p>
<p>where in the last step we are anticipating the limit of infinitesimal c:. Let us now 
</p>
<p>define an additional pair of variables (not to be integrated over) 
</p>
<p>(21.3.80) 
</p>
<p>(21.3.81) 
</p>
<p>The first of these equations allows us to replace the leftmost bra in Eq. (21.3.7:-;), 
</p>
<p>( -ljlul, by ( lj)N 1. The reason for introducing 'I'~&middot; will follow soon. 
Putting together all the factors (including the overlap of coherent states) we end 
</p>
<p>up with 
</p>
<p>(21.3.82) 
</p>
<p>= f)t' ex P l [ ( &middot;(&middot;&middot;&middot;&middot;&middot;&middot; .......... &amp; ................... ) If/; - H( ljJ, + , , 111;)) Jc: l d Q!; d vr; (21.3.83) 
'="' j' exp (ftJ ljl( r) ( _ii - f:lo + .u \}lfl( r) dr)' [ii'lj/ &pound;tlfl] 
</p>
<p>\ 0 \ cr ' 
(21.3.84) 
</p>
<p>where the last step needs some explanation. With all the factors of &pound; in place we do 
</p>
<p>seem to get the continuum expression in the last formula. However, the notion of 
</p>
<p>replacing differences by derivatives is purely symbolic for Grassmann variables. There 
</p>
<p>is no sense in which lj/;, 1 .... lj/; is small. in fact the objects have no numerical values. 
</p>
<p>What this really means here is the following. In a while we will trade lfl( r) for If/( OJ) 
</p>
<p>related by Fourier transformation. At that stage we will replace -8/ ar by iw while 
the exact answer is - l. If we do not make this replacement, the Grassmann 
</p>
<p>integral, when evaluated in terms of ordinary numbers, will give exact results for 
</p>
<p>anything one wants to calculate, say the free energy. With this approximation, only 
</p>
<p>quantities insensitive to high frequencies will be given correctly. The free energy will 
</p>
<p>come out wrong but the correlation functions will be correctly reproduced. ('T'his is 
</p>
<p>because the latter are given by derivatives of the free energy and these derivatives 
</p>
<p>make the integrals sufficiently insensitive to high frequencies.) Notice also that we 
</p>
<p>are replacing H(lj/1+ 1, 1J11)=If(ljr('r+ c:), lft(r)) by H(ljr(r), IJI(r)) in the same spirit. 
</p>
<p>Now turn to the Fourier expansions alluded to above. Let us write 
</p>
<p>lj/(r)=I ljl(w) 
n j3 
</p>
<p>IJI(r)=I---Ifl(w) 
n j3 
</p>
<p>(21.3.85) 
</p>
<p>(21.3.86) </p>
<p/>
</div>
<div class="page"><p/>
<p>where the allowed frequencies, called Matsubara frequencies, are chosen to satisfy 
</p>
<p>the antisymmetric boundary conditions in Eqs. (21.3.80 21.3.81). Thus 
</p>
<p>(2n+ l);r 
OJ =&middot;&middot;&middot; 
</p>
<p>n /3 (21.3.87) 
</p>
<p>where n is an integer. Note that we have chosen the Fourier expansions as if ljl and 
</p>
<p>Iii were complex conjugates, which they are not This choice, however, makes the 
calculations easy. 
</p>
<p>For future reference note that if /3-+w, it follows from Eq. (21.3.87) that when 
n increases by unity, OJn changes by dOJ =2;r/f3. Thus 
</p>
<p>_!_ L -+ j' dOJ 
/3 n 2;r 
</p>
<p>The inverse transformations are 
</p>
<p>~p 
</p>
<p>!j!( OJ)= J !j!( r) e1m,r dr 
0 
</p>
<p>~p 
</p>
<p>!p( OJ)= J !p( r) e -lm,r dr 
0 
</p>
<p>where we use the orthogonality property 
</p>
<p>I'{J 
</p>
<p>J eim"re-i"'m'dr 
0 
</p>
<p>ei(m,&middot;&middot;&middot; mm)/3- 1 
----- = /3bmn 
</p>
<p>i(OJn-OJm) 
</p>
<p>(21.3.88) 
</p>
<p>(21.3.89) 
</p>
<p>(21.3.90) 
</p>
<p>(21.3.91) 
</p>
<p>Performing the Fourier transforms in the action and changing the functional integra-
tion variables to !j!( OJ) and !p( OJ) (the Jacobian is unity) and going to the limit 
</p>
<p>/3-+CXJ, which converts sums over discrete frequencies to integrals over a continuous 
ro, as per Eq. (21.3.88), we end up with 
</p>
<p>f [f cx: dro l Z= exp 2;r !p(OJ)(iw -n0 + p)!j!(w) [0lif.i(w) .SI!\ft(w)] 
-x -
</p>
<p>(21.3.92) 
</p>
<p>Although f3 has disappeared from the picture it will appear as 2n8(0), which we 
know stands for the total imaginary time /3. (Recall Fermi's golden rule calculations.) 
An example will follow shortly. 
</p>
<p>649 
</p>
<p>PATH 
</p>
<p>INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>650 
</p>
<p>CHAPTER 21 
</p>
<p>Let us first note that the frequency space correlation function is related to the 
</p>
<p>integral over just a single pair of variables [Eq. (21.3.59)] and is given by: 
</p>
<p>( !f( m 1) lfl( m2)) 
</p>
<p>f !f(mJ)lf/(m2)explJ', ~; lji'(m)(iw-Q0 +,U)1f!{m)J9!f(m).01!1f(m)] 
~ If% dm J J expl -ry 2; t/f(w)(iw-!"!0 +,U)!1f(w) [q!f(m).P!If(m)] 
</p>
<p>2no(wl- m2) 
(21.3.93) 
</p>
<p>im~-no+ ,u 
</p>
<p>In particular, 
</p>
<p>.. 2no(O) f3 
(!lf(W)!If(m)\=--~- -=--~-
</p>
<p>&bull; 
1 im-f20 +J.i im-f20 +,u 
</p>
<p>(21.3.94) 
</p>
<p>Exercise 21.3.10. Try to demonstrate the above two equations. Note first of all that 
</p>
<p>unless m 1 = m2, we get zero since only a 'iilf pair has a chance of having a nonzero integral. 
This explains the i'i-function. As for the 2H, go back to the stage where we had a sum over 
</p>
<p>frequencies and not an integral, i.e., go against the arrow in Eq. (21.3.88) and use it in the 
exponent of Eq. (21.3.93 ). 
</p>
<p>Let us now calculate the mean occupation number (N): 
</p>
<p>1 az 
(N)= 
. f3Z Dp 
</p>
<p>as in the operator approach. 
</p>
<p>(21.3.95) 
</p>
<p>(21.3.96) 
</p>
<p>(21.3.97) 
</p>
<p>(21.3.98) 
</p>
<p>Notice that we had to introduce the factor e""" into the w integral. We under-
</p>
<p>stand this as follows. If we had done the calculation using timer instead of frequency 
m, we would have calculated the average of 'Pt'P. This would automatically have 
</p>
<p>turned into !f(r+ 6)1/f(r) when introduced into the path integral since the coherent 
state bra to the left of the operator would have come from the next time slice 
compared to the ket at the right. [Remember how H("P 1, 'P) turned into 
</p>
<p>H( !f(i + 1 ), llf(i) ).] Notice that the integral over m was not convergent, varying as 
dm/m. It was therefore sensitive to the high frequencies and we had to intervene </p>
<p/>
</div>
<div class="page"><p/>
<p>with the factor eiwoT. This factor allows us to close the contour in the upper half-
plane. If f.l &gt;00 , the pole of the integrand lies in that half-plane and makes a contribu-
tion. If not we get zero. In correlation functions that involve integrals that have two 
or more powers of w in the denominator and are hence convergent, we will not 
introduce this factor. 
</p>
<p>Exercise 21.3.11. Advanced 
In field theory and many-body physics one is interested in the Green's function: 
</p>
<p>G( r) = (ff('P( r)'P\0))) (21.3.99) 
</p>
<p>where ( ) denotes the average with respect to Z, 
</p>
<p>is the Heisenberg operator, and .9"" the time-ordering symbol for fermionic operators: 
</p>
<p>(21.3.100) 
</p>
<p>Note the minus sign when the order of operators is reversed. Show that 'P( r) = '&yen; e -&lt;o.- Jllr 
for our problem of the single oscillator. 
</p>
<p>Show, using the operator formalism that in our problem 
</p>
<p>and that in the zero-temperature limit this reduces to 
</p>
<p>Let us define the pair of transforms: 
</p>
<p>G(w)= f_~ G(r) e1"" dr 
</p>
<p>Show that 
</p>
<p>f oe . dw G(r)= G(w) e-&bull;wr-
2n 
</p>
<p>-oc 
</p>
<p>1 
G(w)=---
</p>
<p>no- p-iw 
</p>
<p>independent of which of no or Jl is greater. 
</p>
<p>(21.3.101) 
</p>
<p>(21.3.102) 
</p>
<p>(21.3.103) 
</p>
<p>(21.3.104) 
</p>
<p>(21.3.105) 
</p>
<p>(21.3.106) 
</p>
<p>We saw in the study of the Ising model that the two-point correlation function in the 
functional integral translates into ground state average of the time-ordered product (for infin-
itely long system in the imaginary time direction) and vice versa. (If the parenthetical condition 
</p>
<p>651 
</p>
<p>PATH 
INTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>652 
</p>
<p>CHAPTER 21 
</p>
<p>is not met, there will not be enough time for the system to relax into the ground state before 
</p>
<p>we stick in the operators being averaged.) 
It is likewise true here that 
</p>
<p>&lt;.9"'('1'( r )'P\0))) = &lt; 1/)("r) 'lf(O)) (21.3.107) 
</p>
<p>where the average on the right-hand side is done by the Grassmann functional integral. 
</p>
<p>Working at zero temperature, verify this for the frequency transform of both sides. (In the 
</p>
<p>right-hand side write 'I'( r) in terms of 'l'(ro ), etc., using the zero temperature version of Eqs. 
</p>
<p>(21.3.85 21.3.86) and Eq. (21.3.93).) 
</p>
<p>This brings us to the end of the discussion of fermionic path integrals. Clearly 
</p>
<p>this is just the beginning and our discussion has been just an introduction. 
</p>
<p>21.4. Summary 
</p>
<p>Let us survey what has been done in this chapter. We started by learning how 
</p>
<p>to use different resolutions of the identity to derive different path integrals. We 
</p>
<p>looked at the configuration space, phase space, and coherent state path integrals. 
</p>
<p>We realized that, while the introduction of the resolution of the identity is not an 
</p>
<p>approximation, any assumption that changes in the coordinates being integrated 
</p>
<p>over were small between time slices was to be carefully examined. In configuration 
</p>
<p>space integrals the kinetic energy term provided a damping of fluctuations to some-
</p>
<p>thing of order s 112. In other integrals there was no such assurance. In particular, the 
</p>
<p>continuum forms of the action were purely formal objects and only the discrete 
</p>
<p>version defined the path integral, assuming the limit of infinite number of integrals 
</p>
<p>existed. Despite this, the path integrals were very useful for seeing the theory as a 
</p>
<p>whole before us, as a constructive solution to the quantum evolution problem. In 
</p>
<p>particular, in the classical limit the smallness of 1i allowed us to think in terms of 
</p>
<p>smooth paths. The study of the LLL (in connection with the QHE) and the Berry 
</p>
<p>phase analysis illustrated some correct uses of the path integral. 
</p>
<p>We then turned to imaginary time quantum mechanics. We showed that from 
</p>
<p>it one could extract the real-time energies and wave functions. In addition, imaginary 
</p>
<p>time path integrals directly defined quantum statistical mechanics and were formally 
</p>
<p>similar to classical statistical mechanics. The transfer matrix played the role of the 
</p>
<p>discrete imaginary time evolution operator. Symmetry breaking was analyzed from 
</p>
<p>many angles. 
</p>
<p>Finally, we studied two systems with no classical limit: the quantum spins and 
</p>
<p>fermion oscillators. Although we studied just one fermionic oscillator, the generaliza-
</p>
<p>tion to many is direct and you should have no trouble following that topic when 
</p>
<p>you get to it. Grassmann integrals are undoubtedly the most abstract notion in this 
</p>
<p>book. But there is no doubt that as you use them (comparing them to the operator 
</p>
<p>solution as a check) you will soon learn to think directly in terms of them. But 
</p>
<p>remember this: there is no real notion of a semiclassical analysis here since the action 
</p>
<p>is not a number-valued object and cannot be said to be stationary at any point. Note 
</p>
<p>also that every Grassmann integral you write is eventually equal to an ordinary 
</p>
<p>number though the integrand and integration measure are not. These numbers </p>
<p/>
</div>
<div class="page"><p/>
<p>correspond to physical entities like the ground energy or correlation function of a 
</p>
<p>fermion system. 
</p>
<p>The only functional integral we evaluated was the Gaussian integral. This is 
</p>
<p>essentially all we know how to do. What if the action is not quadratic but has quartic 
</p>
<p>terms? Then we do perturbation theory. We bring down the quartic term from the 
exponential (in the form of an infinite series) and evaluate term by term since we 
</p>
<p>know how to integrate xn times a Gaussian. Recall Appendix A.2 as well as the 
Wick's theorem for fermions in Exercise (21.3.64). But that's another story. 
</p>
<p>Bibliography 
</p>
<p>M. Berry, Proc. R. Soc. Lond., Ser. A392 45, 1984. A fascinating account of the history of the subject 
</p>
<p>may be found in M. Berry, Physics Today, 43, 12 1990. 
</p>
<p>S. Coleman, Aspects of Symmetry (Cambridge University Press, New York, 1985). Included here for the 
</p>
<p>article on instantons. 
</p>
<p>L. D. Faddeev, in Methods in Field Theory, Les Houches Lectures, 1975 (R. Balian and J. Zinn-Justin, 
</p>
<p>eds.) (North-Holland/World Scientific, Singapore, 1981). Look in particular at the discussion of 
</p>
<p>holomorphic form of the functional integral. i.e., the coherent state integral. 
</p>
<p>M. Gutzwiller, Chaos in Classical and Quantum Mechanics (Springer Verlag, New York, 1990). For a 
</p>
<p>solid introduction to many aspects of path integrals, especially the semiclasslcal limit. 
</p>
<p>1. R. Klauder and B. Skagerstam, eds., Coherent States (World Scientific, Singapore, 1985). Everything 
</p>
<p>you ever wanted to know about coherent states. 
</p>
<p>S. Pancharatnam, Ind. Acad. Sci., 44(5), Sec. A, 1958. Reprinted in Shapere and Wilczek (1989) (see 
</p>
<p>below). Discusses the geometric phase in the context of optics. 
</p>
<p>R. E. Prange and S. M. Girvin, eds., The Quantum Hall Efj'ect (Springer, New York, 1987). Has many 
</p>
<p>contributed papers by leaders in the field as well as helpful commentaries. 
</p>
<p>R. Rajaraman, Instantons and Solitons (North Holland, New York, 1982). Extremely clear discussion of 
</p>
<p>the subject, usually starting with a warmup toy example from elementary quantum mechanics. Very 
</p>
<p>few details are "left to the reader." 
</p>
<p>L. S. Schulman, Techniques and Applications of Path Integrals (Wiley Interscience, New York, 1981). A 
</p>
<p>very readable and clear discussion of functional integrals, and pitfalls and fine points (such as the 
</p>
<p>midpoint prescription for vector potential coupling). 
</p>
<p>A. Shapere and F. Wilczek, eds., Geometric Phases in Physics (World Scientific, Singapore, 1989). Collec-
</p>
<p>tion of all key papers and some very good introductions to each subtopic. Saves countless trips to 
</p>
<p>the library. 
</p>
<p>M. Stone. ed., The Quantum Hall Effect (World Scientific, Singapore, 1992). A nice collection of reprints 
</p>
<p>with commentary. 
</p>
<p>'t Hooft, Phys. Rev. Lett., 37, 8 (1976). 
</p>
<p>F. Wilczek, ed., Fractional Statistics and Anyon Superconductivity (World Scientific, Singapore, 1990). 
</p>
<p>Referenced here for its applications to the Quantum Hall Effect. However, other topics like fractional 
</p>
<p>statistics discussed there should be within your reach after this book. 
</p>
<p>653 
</p>
<p>PATH 
</p>
<p>fNTEGRALS: 
</p>
<p>PART II </p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix 
</p>
<p>A.L Matrix Inversion 
</p>
<p>This brief section is included only to help you understand Eq. (1.8.5) in the 
</p>
<p>main text and is by no means comprehensive. 
Consider the inversion of a 3 x 3 matrix 
</p>
<p>(A.l.l) 
</p>
<p>The elements of Af have been named in this way rather than as lvfu, for in the 
following discussion we will treat the rows as components of the vectors A, B, and 
C, i.e., in the notation of vector analysis (which we will follow in this section). 
</p>
<p>Consider next a triplet of vectors 
</p>
<p>AR=BxC 
</p>
<p>BR=CX A 
</p>
<p>CR=A X B 
</p>
<p>(A.l.2) 
</p>
<p>655 </p>
<p/>
</div>
<div class="page"><p/>
<p>656 
</p>
<p>APPENDIX 
</p>
<p>which are said to be reciprocal to A, B, and C. In general, 
</p>
<p>and cyclic permutations (A.l.3) 
</p>
<p>If we construct now a matrix M (called the cofactor transpose of M) whose columns 
are the reciprocal vectors, 
</p>
<p>then 
</p>
<p>(bR)l 
</p>
<p>(bRh 
</p>
<p>(bRh 
</p>
<p>Now all three diagonal elements are equal: 
</p>
<p>A&middot;AR=A&middot;(B X C) =B&middot;(C X A)=B&middot;BR=C&middot;(A X B)=C&middot;CR 
</p>
<p>= detM 
</p>
<p>(A.l.4) 
</p>
<p>(A.l.S) 
</p>
<p>where the last equality follows from the fact that the cross product may be written 
</p>
<p>as a determinant: 
</p>
<p>(A.l.6) 
</p>
<p>(We shall follow the convention of using two vertical lines to denote a determinant.) 
</p>
<p>Hence the inverse of the matrix M is given by 
</p>
<p>lvr' __ fv!_ 
det i\1 
</p>
<p>(A.l.7) 
</p>
<p>When does det l.J vanish? If one of the vectors, say C, is a linear combination 
</p>
<p>of the other two; for if 
</p>
<p>C=aA+/1:8 
</p>
<p>then 
</p>
<p>A&middot;(B x C) =A&middot;(B x aA) + A&middot;(B x j3B)=B&middot;(aA x A)=O </p>
<p/>
</div>
<div class="page"><p/>
<p>Thus the determinant vanishes if the rows of the matrix are not linearly independent 
(LI) and vice versa. If the matrix is used to represent three simultaneous equations, 
it means not all three equations are independent. The method can be generalized for 
inverting n x n matrices, with real or complex elements. One defines a cross product 
of n- 1 vectors as 
</p>
<p>k 
</p>
<p>(A.1.8) 
</p>
<p>The resulting vector is orthogonal to the ones in the product, changes sign when we 
interchange any two of the adjacent ones, and so on, just like its three-dimensional 
counterpart. If we have a matrix M, whose n rows may be identified with n vectors, 
At, A2 , &bull;&bull;&bull; , An, then the cofactor transpose has as its columns the reciprocal vectors 
AtR, ... , AnR, where 
</p>
<p>(A.l.9) 
</p>
<p>One tricky point: the cross product is defined to be orthogonal to the vectors 
in the product with respect to an inner product 
</p>
<p>and not 
</p>
<p>A&middot;B=" A*B L... I I 
</p>
<p>even when the components of A are complex. There is no contradiction here, for 
the vectors At, ... , An are fictitious objects that enter a mnemonic and not the 
elements of the space '~.r( C) on which the operator acts. 
</p>
<p>Exercise A.l.l. Using the method described above, show that 
</p>
<p>~]-! =[ ~ =~ !] 
1 -1 3 -2 
</p>
<p>and 
</p>
<p>[ ~0 ~]-!=_!_[-: -~ -821] 
-1 2 12 4 -2 
</p>
<p>657 
</p>
<p>APPENDIX </p>
<p/>
</div>
<div class="page"><p/>
<p>658 
</p>
<p>APPENDIX 
</p>
<p>Theorem A.l.l. If fll V) = 10) implies IV)= 10) then g-t exists. 
</p>
<p>Proof Let I Vt), ... , I Vn) be aLI basis in &middot;~r. Then another LI basis is gener-
</p>
<p>ated by the action of n, i.e., fll Vt ), ... , fll Vn) is also a LI basis. To see this, let us 
</p>
<p>assume the contrary, that there exists a relation of the form 
</p>
<p>with not all a;=O. Upon pulling out n, because it is linear, we get 
</p>
<p>which, when combined with the assumed property of n, implies that 
</p>
<p>with not all a;= 0, which is not true. So we can conclude that every vector IV') in 
</p>
<p>~;n may be written as a unique linear combination in the new basis generated by n 
</p>
<p>as 
</p>
<p>I V')=l: a;fll V;) 
</p>
<p>In terms of IV)= 2: a;l V;), we see that every IV') in ''1.-,;n may be written as 
</p>
<p>I V')=fll V) 
</p>
<p>where IV) is unique. In other words, we can think of every IV') in 'l,;n as arising 
</p>
<p>from a unique source I V) in ~;n under the action of n. Define an operator A whose 
</p>
<p>action on any vector IV') in ~;n is to take it back to its unique source IV). (If the 
</p>
<p>source of IV') were not unique-say, because there are two vectors I Vt) and I V2 ) 
</p>
<p>that are mapped into IV') by n-then we could not define A, for acting on IV'), it 
</p>
<p>would not know whether to give I Vt) or I V2).) The action of A is then 
</p>
<p>AI V') =IV), where IV')= fll V) 
</p>
<p>We may identify A as the inverse of n, 
</p>
<p>A=n- 1 or An=/ 
</p>
<p>since for any IV') in V' 
</p>
<p>AI V') = Afll V) =I V) Q.E.D. </p>
<p/>
</div>
<div class="page"><p/>
<p>A.2. Gaussian Integrals 
</p>
<p>We discuss here all the Gaussian integrals that we will need. Consider 
</p>
<p>( f -ax2 d lo a)= e x, 
-x 
</p>
<p>a&gt;O (A.2.1) 
</p>
<p>This integral cannot be evaluated by conventional methods. The trick is to consider 
</p>
<p>I6(a)= foo e-ax'dx foc e&middot;-ay'dy= fx' IX e-a(x'+y')dxdy 
- u:::: -en ---- cc ~ ---&middot; c:;c 
</p>
<p>Switching to polar coordinates in the xy plane, 
</p>
<p>Therefore 
</p>
<p>l~(a)=f&middot;.-c 12" eap'pdpd&cent; 
o Jo 
</p>
<p>= rr/a 
</p>
<p>lo( a)= (rr I a ) 112 
</p>
<p>By differentiating with respect to a we can get all the integrals of the form 
</p>
<p>l2n(a) = f . x 2" dx 
-ey~ 
</p>
<p>For example, 
</p>
<p>dx 
</p>
<p>(A.2.2) 
</p>
<p>(A.2.3) 
</p>
<p>The integrals l2n+l(a) vanish because these are integrals of odd functions over an 
even interval -oo to +oo. Equations (A.2.2) and (A.2.3) are valid even if a is purely 
imaginary. 
</p>
<p>Consider next 
</p>
<p>fo(a, /3) = f' e-ax'+fJx dx 
--- oc .. 
</p>
<p>(A.2.4) 
</p>
<p>659 
</p>
<p>APPENDIX </p>
<p/>
</div>
<div class="page"><p/>
<p>660 
</p>
<p>APPENDIX 
</p>
<p>By completing the square on the exponent, we get 
</p>
<p>lo(a, /3) = 
,.. --:~_ , '-I 
</p>
<p>4a J a(Y &middot;&middot;{3 'a)' j f3'&bull;4a (TC) e - &middot; - c.x = e &middot; ----
----:r \a/ 
</p>
<p>(A.2.5) 
</p>
<p>These results are valid even if a and f3 arc complex, provided Re a&gt; 0. Finally. by 
applying to both sides of the equation 
</p>
<p>Jr x &middot;&middot;m d I e r= 
o a 
</p>
<p>the operator (-d/da)". we obtain 
</p>
<p>Consider this integral with a = 1 and n replaced by z- 1, where z is an arbitrary 
</p>
<p>complex number. This defines the gamma function l(z) 
</p>
<p>f'(z)= I, r &middot; 1 e' dr 
0 
</p>
<p>For real, positive and integral ::, 
</p>
<p>l(z) = (::: ---- 1 )! 
</p>
<p>A.3. Complex Numbers 
</p>
<p>A complex variable z can be written in terms of two real variables x andy, and 
</p>
<p>i=(-1)12, as 
</p>
<p>z=x+(v (A.3.l) 
</p>
<p>Its complex conjugate z* is defined to be 
</p>
<p>:::* =x ----(v (A.3.2) 
</p>
<p>One may invert these two equations to express the real and imaginary parts, x and 
</p>
<p>y, as 
</p>
<p>x= ~(z+ z*), _y=(z-z*)/2i (A.3.3) </p>
<p/>
</div>
<div class="page"><p/>
<p>The modulus squared of z, defined to be zz*, equals 
</p>
<p>zz* = izl 2 = (x+ iy)(x- iy) = x2 + l (A.3.4) 
</p>
<p>You may verify that z = z' implies that x = x' and y = y' by considering the modulus 
of z-z'. 
</p>
<p>From the power-series expansions 
</p>
<p>one can deduce that 
</p>
<p>sin x=x- x3 /3! + x5 /5!- &middot; &middot; &middot; 
</p>
<p>cos x= 1- x2/2! + x4 /4!- &middot; &middot; &middot; 
</p>
<p>eix = cos x + i sin x 
</p>
<p>It is clear that eix has unit modulus (x is real). 
</p>
<p>The expression z = x + iy gives z in Cartesian form. The polar form is 
</p>
<p>z=x+iy=(x2+l)1!2l&middot; , x 2 112&middot;-1--i~~. {nJ 
(x&middot;&middot;+y)~- (x+y) 1 " 
</p>
<p>= p(cos 0 + i sin 0) 
</p>
<p>=pew 
</p>
<p>where 
</p>
<p>Clearly 
</p>
<p>lzl =p 
</p>
<p>(A.3.5) 
</p>
<p>(A.3.6) 
</p>
<p>(A.3. 7) 
</p>
<p>Each complex number z = x+ iy may be visualized as a point (x, y) in the x-y plane. 
This plane is also called the complex z plane. 
</p>
<p>A.4. The is Prescription 
</p>
<p>We will now derive and interpret the formula 
</p>
<p>l 1 
--=?f'-&plusmn;in8(x) 
x"fic: x 
</p>
<p>(A.4.1) 
</p>
<p>661 
</p>
<p>APPENDIX </p>
<p/>
</div>
<div class="page"><p/>
<p>662 
</p>
<p>APPENDIX 
</p>
<p>where s--&gt;0 is a positive infinitesimally small quantity. Consider an integral of the 
</p>
<p>form 
</p>
<p>/=lim fx.f(x) ~x. 
&middot;&middot;~o x-ts 
</p>
<p>-oc 
</p>
<p>(A.4.2) 
</p>
<p>Viewing this as the integral on the real axis of the complex z = x + iy plane, we see 
that the integrand has an explicit pole at z = is in addition to any singularities f 
might have. We assume/has no singularities on or infinitesimally close to the real 
</p>
<p>axis. As long as s is fixed, there is no problem with the integral. For example, iff 
</p>
<p>has some poles in the upper half-plane and vanishes fast enough to permit our closing 
</p>
<p>the contour in the upper half-plane, the integral equals 2;ri times the sum of the 
</p>
<p>residues of the poles off and the pole at z =is. Likewise, if we change the sign of 
</p>
<p>the s term, we simply drop the contribution from the explicit pole, which is now in 
the lower half-plane. 
</p>
<p>What if S--&gt;0? Now the pole is going to ram (from above) into our contour which 
</p>
<p>runs along the x-axis. So we prepare for this as follows. Since the only singularity near 
</p>
<p>the real axis is the explicit pole as z =is, we make the following deformation of the 
</p>
<p>contour without changing the value of I: the contour runs along the real axis from 
</p>
<p>-oo to -s', (s' is another positive infinitesimal) goes around counterclockwise, below 
</p>
<p>the origin in a semicircle of radius s', and resumes along the real axis from x = s' to 
</p>
<p>oo. The nice thing is that we can now set e = 0, which brings the pole to the origin. 
</p>
<p>The three parts of the integration contour contribute as follows: 
</p>
<p>I= lim [f -&bull;j(x) dx +I y_ j(x) dx + i;rf(O)J 
s&middot;-o -x X 8 &bull; X 
</p>
<p>=f/J f.x f(x) dx +i;rf(O). 
</p>
<p>-x X 
</p>
<p>(A.4.3) 
</p>
<p>The sum of the two integrals in the limit s' --&gt;0 is defined as the principal value integral 
</p>
<p>denoted by the symbol f/J. In the last term, which is restricted to the infinitesimal 
</p>
<p>neighbourhood of the origin, we have set the argument of the smooth function f to 
zero and done the integral of dzjz counterclockwise around the semicircle to get i;r. 
</p>
<p>Eq. (A.4.1) is a compact way to say all this. It is understood that Eq. (A.4.1) 
</p>
<p>is to be used inside an integral only and that inside an integral the factor lj(x-is) 
</p>
<p>leads to two terms: the first, f!J(ljx), leads to the principal value integral, and the 
</p>
<p>second, i;ro(x), leads to i;rf(O). 
It is clear that if we reverse the sign of the s term, we change the sign of the delta 
</p>
<p>function since the semicircle now goes around the pole in the clockwise direction. The 
</p>
<p>principal part is not sensitive to this change of direction and is unaffected. 
</p>
<p>It is clear that if we replace x by x- a the pole moves from the origin to x =a 
</p>
<p>andf(O) gets replaced by f(a) so that we may write 
</p>
<p>1 
f/J--&plusmn;i;ro(x-a) 
</p>
<p>(x-a)'fie (x-a) 
</p>
<p>It is clear that the limits on x need not be &plusmn;oo for the formula to work. 
</p>
<p>(A.4.4) </p>
<p/>
</div>
<div class="page"><p/>
<p>Finally, note that according to Eq. (A.4.4) the difference between the integrals 
</p>
<p>with two signs of s is just 2Jrif(a). This too agrees with the present analysis in 
terms of the integral I in Eq. (A.4.2) since in the difference of the two integrals the 
contribution along the real axis cancels due to opposite directions of travel except 
for the part near the pole where the difference of the two semicircles (one going 
above and going below the pole) is a circle around the pole. 
</p>
<p>663 
</p>
<p>APPENDIX </p>
<p/>
</div>
<div class="page"><p/>
<p>Answers to Selected Exercises 
</p>
<p>Chapter 1 
</p>
<p>1.8.1. (l) tw~,&gt;~[H 
</p>
<p>(2) No, no. 
</p>
<p>1.8.2. (1) Yes 
</p>
<p>(2) tw~o&gt;~[H I w = 1 &gt; -+-h [~], 2 . 
1 
</p>
<p>I w =&middot;&middot;-I)-+-~[- ~1&middot;&middot; 
i''" 
</p>
<p>1 
</p>
<p>1.8.10. w=0,0,2;}c=2,3,-l. 
</p>
<p>Chapter 4 
</p>
<p>4.2.1. (I) I, 0, -1 
</p>
<p>(2) (Lx)=O,(L~)=1/2,t1Lx=lj2 112 
</p>
<p>[
1/2 ] 
</p>
<p>(3) !Lx= 1)--&gt; 1/2 112 , 
</p>
<p>1/2 
</p>
<p>[ 
l/2 ] 
</p>
<p>ILx=-1)-+ -1/2112 
</p>
<p>1/2 665 </p>
<p/>
</div>
<div class="page"><p/>
<p>666 
</p>
<p>ANSWERS TO 
</p>
<p>SELECTED 
</p>
<p>EXERCISES 
</p>
<p>(4) P(Lx=l)=J/4, P(Lx= 0) = 1 P(Lx=-1)= 1/4 
</p>
<p>(5) Ill') [ 
1 /~ ] =projection of I !Jf) on the L; = 1 eigen-
</p>
<p>(l/4+ l/2) 1/21/2 __ 
</p>
<p>space. P(L;=l)=3/4. If L= is measured P(Lz=l)=l/3, P(Lz=-1)= 
2/3. Yes, the state changes. 
</p>
<p>(6) No. To see this right away note that if 8 1 =82 =83 =0, llJ!)= I!Lx=1) 
and if 81 = 83 = 0 and 82 = n. I !Jf) = !L, = -1 ). [See answer to part (3).] 
The vectors IV') and ewl !Jf) are physically equivalent only in the sense 
that they generate the same probability distribution for any observable. 
This does not mean that when the vector Ill') appears as a part of a 
linear combination it can be multiplied by an arbitrary phase factor. In 
our example one can only say, for instance, that 
</p>
<p>Chapter 5 
</p>
<p>5.4.2. (a) 
</p>
<p>(b) 
</p>
<p>Chapter 7 
</p>
<p>is physically equivalent to I !Jf). Although ltv)' has different coefficients 
from I !Jf) in the linear expansion, it has the same "direction" as I !Jf). In 
summary, then, the relative phases 82 -8 1 and 83 -8 1 are physically 
relevant but the overall phase is not, as you will have seen in the calcula-
tion of P(L, = 0). 
</p>
<p>R = (ma 11;))2 /(1i4e + m2a2 V~); T= 1- R 
T= ( cosh2 2Ka + a 2 sinh2 2Ka) ---I where iK is the complex wave number 
for !xi sa and a={V0 ----2E)/[4E(V0 --- E)f 12 . 
</p>
<p>7.4.2. 0, 0, (n+l/2)n/mw, (n+l/2)mwn, (n+l/2)n. Note that the rec1pe 
mw--&gt;(mw) 1 is at work here. 
</p>
<p>7.4.5. (1) 
</p>
<p>(2) 
</p>
<p>(3) 
</p>
<p>(l/21/2)(IO) e-1''";2+ ll) e-3rwti2) 
</p>
<p>(X(t)) = (n/2mw) 112 cos wt, (P(I)) = -(mwft/2) 112 sin wt 
</p>
<p>(X(t)) = (in)- 1([X, H]) = (P(t)) /m, (P(t)) = -mw2(X(t) ). By elimin-
ating (P) we can get an equation for (X(t)) and vice versa and solve it 
using the initial values (X(O)) and (P(O) ), e.g., (X(t)) = 
(X(O)) cos wt+ [(P(O))/mw] sin wt. </p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 10 
</p>
<p>10.3.2. r 112[1334) + 1343) + 1433)] 
</p>
<p>Chapter 12 
</p>
<p>Chapter 13 
</p>
<p>13.3.1. Roughly 200 MeV. 
</p>
<p>13.3.2. Roughly 1 A. 
</p>
<p>Chapter 14 
</p>
<p>(a+ 8) (!3 + r) &middot;(/3- r) (a- 8) 14.3.5. M= - 2- I+ - 2- CTx+l -s- CTy+ - 2- CT= 
14.3.7. (1) 2114(cos n/8+i(sin n/8)ax). 
</p>
<p>(2) 2/31-1/3CTx. 
</p>
<p>(3) CTx 
</p>
<p>14.4.4. Roughly 2 X 10-9 second. 
</p>
<p>14.4.6. (eli/2mc) tanh(eliB/2mckT)k 
</p>
<p>14.5.2. (1) Roughly one part in a million. 
</p>
<p>(2) 1010 G. 
</p>
<p>14.5.3. 1/2, 1/4, 0. 
</p>
<p>( 1 +c2
os 0)2 
</p>
<p>14.5.4. 
</p>
<p>Chapter 15 
</p>
<p>15.2.2. (1) (11, 1/2(-1/2)13/21/2)=(1/3)112 
</p>
<p>&lt;I 0, 1/21/213/21/2)={2/3)112 
</p>
<p>(11, 1/2(-1/2)11/21/2)={2/3)1/ 2 
</p>
<p>(1 0, 1/2 1/211/2 1/2) = -{1/3) 1/ 2 
</p>
<p>667 
</p>
<p>ANSWERS TO 
SELECTED 
</p>
<p>EXERCISES </p>
<p/>
</div>
<div class="page"><p/>
<p>668 
</p>
<p>ANSWERS TO 
</p>
<p>SELECTED 
</p>
<p>EXERCISES 
</p>
<p>15.2.6. 
</p>
<p>(2) IJm&gt; = 12, 1 &gt; = T 112 lm, = 1, lnz = 0) + T 112 lm, = 0, mz = 1 &gt; 
12, 0) = 6- 11211, -1) + ( ~) 112 10, 0) + &lt;k) 112l-l, 1 &gt; 
11, l)=T112Il,O)-T11210, 1) 
</p>
<p>11, 0)=2 11211, -1)-2 1121-1, 1) 
</p>
<p>10, o&gt; = r' 1211, -1 &gt; ---- r' 12IO, o&gt; + T 112 l-1, 1 &gt; 
The others are either zero, obvious, or follow from Eq. ( 15.2.11 ). 
</p>
<p>l- (2L&middot;S )/Fi2 
[p- = -------
</p>
<p>21+ 1 
</p>
<p>Chapter 16 
</p>
<p>16.1.2. E(a0 ) = IOE0/n 2 
</p>
<p>16.1.3. -ma5Vb/n1f 
</p>
<p>16.2.4. Roughly 1.5 x 1017 seconds or 1010 years. </p>
<p/>
</div>
<div class="page"><p/>
<p>Table of Constants 
</p>
<p>fzc= 1973.3 eVA 
</p>
<p>a =e2 jfzc= 1/137.04 
</p>
<p>mc2 = 0.511 MeV (m is the electron mass) 
</p>
<p>Mc2 = 938.28 MeV (21-f is the proton mass) 
</p>
<p>a0 =fz2/me2 =0.511 A 
</p>
<p>efz/2mc = 0.58 X w--s eV /G (Bohr magneton) 
k=8.62x 10-5 eV/K 
</p>
<p>kT~ 1/40 eV at T= 300 K (room temperature) 
</p>
<p>1 eV=l.6x I0- 12 erg 
</p>
<p>Mnemonics for Hydrogen 
</p>
<p>In the ground state, 
</p>
<p>vjc=.=f3=a 
</p>
<p>&pound;1 = -T= -imv2 = -imc2a2 
</p>
<p>mvao= 1i 
</p>
<p>669 </p>
<p/>
</div>
<div class="page"><p/>
<p>Index 
</p>
<p>Absorption spectrum, 368 
</p>
<p>Accidental degeneracy 
</p>
<p>free-particle case, 426 
</p>
<p>harmonic oscillator case, 352, 423 
</p>
<p>hydrogen atom case, 359, 422 
</p>
<p>Actinides, 3 71 
</p>
<p>Active transformations, 29, 280 
</p>
<p>Adjoint, 13, 25, 26 
</p>
<p>Aharonov-Bohm effect, 497 
</p>
<p>Angular momentum 
</p>
<p>addition of 
</p>
<p>J +J, 408 
</p>
<p>L+S, 414 
</p>
<p>S+S. 403 
</p>
<p>commutation rules, 319 
</p>
<p>eigenfunctions, 324, 333 
</p>
<p>eigenvalue problem of, 321 
</p>
<p>spin. 373 
</p>
<p>in three dimensions, 318 
</p>
<p>in two dimensions, 308 
</p>
<p>Anticommutation relations, 640 
</p>
<p>Anti-Hermitian operators, 27 
</p>
<p>Antisymmetric states. 261 
</p>
<p>Anyons, 607 
</p>
<p>Balmer series, 36 7 
</p>
<p>Basis, 6 
</p>
<p>Berry phase, 592 
</p>
<p>Berry potential, 603 
Bohr magneton, 389 
</p>
<p>Bohr model, 364 
</p>
<p>Bohr radius, 244, 357 
</p>
<p>Bohr-Sommerfeld quantization rule, 448 
</p>
<p>Born approximation 
</p>
<p>time-dependent, 529 
</p>
<p>time-independent, 534 
</p>
<p>validity of. 543 
</p>
<p>Bose-Einstein statistics, 271 
</p>
<p>Bosons, 263 
</p>
<p>Bound states, 160, 445 
</p>
<p>energy quantization in, 160 
</p>
<p>Bra, 11 
</p>
<p>Breit-Wigner form, 551 
</p>
<p>de Broglie waves, 112, 366 
</p>
<p>Double well. 616 
</p>
<p>tunneling in. 6 I 6 
</p>
<p>Canonical commutation rule. 131 
</p>
<p>Canonically conjugate operators, 69 
</p>
<p>Canonical momentum, 80 
</p>
<p>electromagnetic case, 84 
</p>
<p>Canonical transformations 
</p>
<p>active, 97 
</p>
<p>introduction to, 92 
</p>
<p>point transformations, 94 
</p>
<p>regular, 97 
</p>
<p>Center of mass ( CM), 85 
</p>
<p>Centrifugal barrier, 340 
</p>
<p>Characteristic equation, 33 
</p>
<p>Characteristic polynomial, 33 
</p>
<p>Chemical potential. 641 
</p>
<p>Classical limit, 179 
</p>
<p>Classical radius of electron, 364 
</p>
<p>Clebsch--Gordan coefficients. 412 
</p>
<p>Cofactor matrix. 656 
</p>
<p>Coherent states 
fermionic, 642 
</p>
<p>oscillator, 607 
</p>
<p>spin. 636 
</p>
<p>Collapse of state vector, 122, 139 
</p>
<p>Commutator, 20 
</p>
<p>Compatible variables, 129 
</p>
<p>Completeness relation, 23, 59 
</p>
<p>Complete set of observables, 133 671 </p>
<p/>
</div>
<div class="page"><p/>
<p>672 
</p>
<p>INDEX 
</p>
<p>Complex numbers, 660 
</p>
<p>Compton scattering, 123 
</p>
<p>Compton wavelength, 246 
</p>
<p>electronic, 363 
</p>
<p>Condon-Shortley convention, 410 
</p>
<p>Configuration space, 76 
</p>
<p>Consistency test 
</p>
<p>for three-dimensional rotations, 318 
</p>
<p>for translations, 306, 312 
</p>
<p>for translations and rotations, 310 
</p>
<p>Coordinates 
canonical, 94 
</p>
<p>center-of-mass, 85 
</p>
<p>cyclic, 81 
</p>
<p>relative, 85 
</p>
<p>Correlation function, 628 
</p>
<p>connected, 634 
</p>
<p>Correlation length, 629 
</p>
<p>Correspondence principle, 197 
</p>
<p>Coulomb scattering, 531 
</p>
<p>Coupled mass problem, 46 
</p>
<p>Creation operator, 205 
</p>
<p>Cross section 
in CM frame, 557 
</p>
<p>differential, 526, 529 
</p>
<p>for Gaussian potential, 533 
</p>
<p>for hard sphere, 549 
in lab frame, 559 
</p>
<p>partial, 548 
</p>
<p>photoelectric, 506 
</p>
<p>Rutherford. 531 
</p>
<p>for Yukawa potential, 531 
</p>
<p>Cyclotron frequency, 588 
</p>
<p>Dalgarno and Lewis method, 462 
</p>
<p>Darwin term, 572 
</p>
<p>Degeneracy, 38, 44, 120 
</p>
<p>Density matrix, 133 
</p>
<p>Derivative operator, 63 
eigenvalue problem for, 66 
</p>
<p>matrix elements of, 64 
</p>
<p>Destruction operator, 205 
Determinant, 29 
</p>
<p>Diagonalization 
of Hermitian operator, 40 
</p>
<p>simultaneous, 43 
</p>
<p>Differential cross section, 526, 529 
Dipole approximation, 502 
Dipole moment, 463 
Dipole selection rule, 465 
Dirac delta function, 60 
</p>
<p>definition of. 60 
derivatives of, 61 
Gaussian approximation for, 61 
integral representation of, 63 
three-dimensional, 342 
</p>
<p>Dirac equation 
</p>
<p>electromagnetic, 566 
</p>
<p>free particle, 565 
</p>
<p>Dirac monopole, 605 
Dirac notation, 3 
</p>
<p>Dirac string, 605 
</p>
<p>Direct product 
</p>
<p>of operators, 250 
</p>
<p>spaces, 249 
</p>
<p>Double-slit experiment, 108 
</p>
<p>quantum explanation of, 175 
</p>
<p>Dual spaces. II 
</p>
<p>Ehrenfest's theorem, 180 
</p>
<p>Eigenket, 30 
</p>
<p>Eigenspace, 37 
</p>
<p>Eigenvalue problem, 30 
</p>
<p>Eigenvector, 30 
</p>
<p>Einstein temperature, 220 
</p>
<p>Electromagnetic field 
</p>
<p>interactions with matter, 83, 90, 499 
</p>
<p>quantization of, 506 
</p>
<p>review of, 492 
</p>
<p>Ensemble 
</p>
<p>classical, 125 
</p>
<p>mixed, 133 
</p>
<p>quantum, 125 
</p>
<p>Euclidean Lagrangian, 614 
</p>
<p>Euler angles, 333 
</p>
<p>Euler-Lagrange equations, 79 
</p>
<p>Exchange operator, 278 
</p>
<p>Exclusion principle, 264 
</p>
<p>Expectation value, 127 
</p>
<p>Fermi-Dirac statistics, 270 
</p>
<p>Fermionic oscillator, 640 
</p>
<p>thermodynamics of, 642 
</p>
<p>Fermi's golden rule, 483 
</p>
<p>Fermions, 263 
</p>
<p>Field, 2 
</p>
<p>Filling factor, 591 
</p>
<p>Fine-structure constant, 362 
</p>
<p>Fine-structure correction, 367. 466 
</p>
<p>Fourier transform, 62 
Free-particle problem 
</p>
<p>cartesian coordinates, 151 
</p>
<p>spherical coordinates, 426 
Functional, 77 
Functions of operators, 54 
</p>
<p>Gauge 
</p>
<p>Coulomb, 494 
invariance, 493, 496 
</p>
<p>transformations, 493, 496 
Gaussian integrals, 659 </p>
<p/>
</div>
<div class="page"><p/>
<p>Gaussian potential, 533 
</p>
<p>Generalized force, 80 
</p>
<p>Generalized potential, 84 
</p>
<p>Geometric phase, 593 
</p>
<p>Gram-Schmidt theorem, 14 
</p>
<p>Grassmann numbers, 642 
</p>
<p>Green's function, 534 
</p>
<p>Gyromagnetic ratio, 386 
</p>
<p>Hamiltonian formulation, 86 
</p>
<p>Hamilton's equations, 88 
</p>
<p>Harmonic oscillator 
</p>
<p>classical, 83 
</p>
<p>fermionic, 640 
</p>
<p>isotropic, 260, 351 
</p>
<p>quantum, 185 
</p>
<p>in the coordinate basis, 189 
</p>
<p>in the energy basis, 202 
</p>
<p>energy levels of, 194 
</p>
<p>propagator for, 196 
</p>
<p>wave functions of, 195, 202 
</p>
<p>thermodynamics of, 219 
</p>
<p>three-dimensional, 260, 351 
</p>
<p>two-dimensional, 316 
</p>
<p>Heisenberg picture, 147, 490 
</p>
<p>Hermite polynomials, 490 
</p>
<p>Hermitian operators, 27 
</p>
<p>diagonalization of, 40 
</p>
<p>simultaneous, 4 3 
eigenbases of, 3b 
</p>
<p>eigenvalues of, 35 
</p>
<p>eigenvectors of, 36 
</p>
<p>infinite-dimensional, 65 
</p>
<p>Hilbert space, 6 7 
</p>
<p>bosonic, 265 
</p>
<p>fermionic, 265 
</p>
<p>normal mode problem in, 70 
</p>
<p>for two particles, 265 
</p>
<p>'t Hooft, 619 
</p>
<p>Hydrogen atom 
</p>
<p>degeneracy of, 359 
</p>
<p>energy levels of, 356 
</p>
<p>21-cm line, 408 
</p>
<p>wave functions of, 356, 357 
</p>
<p>Hyperfine interaction, 407 
</p>
<p>Ideal measurement, 122 
</p>
<p>Identical particles 
</p>
<p>bosons. 263 
</p>
<p>definition of, 260 
</p>
<p>fermions, 263 
</p>
<p>Identity operator, 19 
</p>
<p>Impact parameter, 523 
</p>
<p>Improper vectors, 67 
</p>
<p>Incompatible variables, 128 
</p>
<p>Induced emission, 521 
</p>
<p>Inelasticity, 554 
</p>
<p>Infinite-dimensional spaces, 57 
</p>
<p>Inner product, 8 
</p>
<p>Inner product space, 7 
</p>
<p>Inverse of operator, 20, 655 
</p>
<p>Ionic bond, 370 
</p>
<p>Irreducible space, 330 
</p>
<p>Irreducible tensor operator, 418 
</p>
<p>Ising model, 627 
</p>
<p>Ket, 3 
Klein~ Gordon equation, 564 
</p>
<p>Kronecker's delta, 10 
</p>
<p>Lagrangian, 76 
</p>
<p>for electromagnetic interactions, 83 
</p>
<p>Laguerre polynomial, 356 
</p>
<p>Lamb shift. 574 
</p>
<p>Landau Level, 587, 588 
</p>
<p>Laughlin wave function, 592 
</p>
<p>Laughlin quasihole, 607 
</p>
<p>Least action principle, 77 
</p>
<p>Legendre transform, 87 
</p>
<p>Linear independence, 4 
</p>
<p>Linear operators, 18 
</p>
<p>Lorentz spinor, 566 
</p>
<p>Lowering operator 
</p>
<p>angular momentum, 322 
</p>
<p>for harmonic oscillator, 205 
</p>
<p>see also Destruction operator 
</p>
<p>Lowest Landau LeveL 588 
</p>
<p>Lyman series, 367 
</p>
<p>Magnetic moment, 385 
</p>
<p>Magnetic quantum number, 314 
</p>
<p>Matrix elements, 20 
</p>
<p>Matrix inversion, 655 
</p>
<p>Mendeleev, 370 
</p>
<p>Metastable states. 553 
</p>
<p>Minimum uncertainty packet, 241 
</p>
<p>Multielectron atoms, 369 
</p>
<p>Negative absolute temperature, 394 
</p>
<p>Norm, 9 
</p>
<p>Normal modes, 52 
</p>
<p>Number operator, 207 
</p>
<p>Numerical estimates, 361 
</p>
<p>Operators, 18 
</p>
<p>adjoint of, 25 
</p>
<p>anti-Hermitian, 27 
</p>
<p>conjugate, 69 
</p>
<p>derivatives of, 55 
</p>
<p>functions of. 54 
</p>
<p>673 
</p>
<p>INDEX </p>
<p/>
</div>
<div class="page"><p/>
<p>674 
</p>
<p>INDEX 
</p>
<p>Hermitian. 27 
identity, 22 
</p>
<p>infinite-dimensional, 63 
inverse of, 20 
</p>
<p>linear, 18 
</p>
<p>matrix elements of. 21 
product of, 20 
</p>
<p>projection, 22 
</p>
<p>unitary, 28 
</p>
<p>Optical theorem, 548, 555 
Orthogonality, 9 
</p>
<p>Orthogonal matrix, 28 
Orthonormality, 9 
</p>
<p>Outer product, 23 
</p>
<p>Paramagnetic resonance, 392 
Parity invariance, 297 
Partial wave 
</p>
<p>amplitude. 545 
</p>
<p>expansion, 545 
Particle in a box, !57, 259 
</p>
<p>Paschen series. 367 
</p>
<p>Passive transformation, 29. 280 
Path integral 
</p>
<p>coherent state. 607, 610 
configuration space, 582 
definition. 223 
</p>
<p>fermionic, 646 
free particle, 225, 582 
</p>
<p>imaginary time, 614 
</p>
<p>phase space, 586 
</p>
<p>recipe. 223 
</p>
<p>and Schriidinger's equation, 229 
</p>
<p>statistical mechanics, 624 
Pauli equation, 568 
</p>
<p>Pauli exclusion principle, 264 
Pauli matrices, 381 
</p>
<p>Periodic table, 370 
Perturbations 
</p>
<p>adiabatic. 478 
</p>
<p>periodic. 482 
</p>
<p>sudden. 477 
</p>
<p>time-independent, 451 
Phase shift, 546 
</p>
<p>Phase space. 88 
</p>
<p>Phonons, 198 
</p>
<p>Photoelectric effect, Ill, 499 
Photons. II 0. 198 
</p>
<p>quantum theory of, 516 
Physical Hilbert space, 67 
Pictures 
</p>
<p>Heisenberg, 147, 490 
interaction (Dirac), 485 
Schriidinger, 147, 484 
</p>
<p>Planck's constant, Ill 
Poisson brackets, 92 
</p>
<p>invariance of, 96 
</p>
<p>Polarizability, 464 
</p>
<p>P operator, 116 
</p>
<p>Population inversion, 395 
Postulates. 115, 211 
</p>
<p>Probability amplitude, Ill. 121 
</p>
<p>Probability current density. 166 
Probability density. 121 
Product basis, 403 
</p>
<p>Projection operator. 23 
Propagator 
</p>
<p>for coupled masses, 51 
Feynman's. 578 
</p>
<p>for free particle, !53 
</p>
<p>for Gaussian packet, !54 
</p>
<p>for harmonic oscillator. 615 
for (classical) string, 72 
</p>
<p>Proper vectors. 6 7 
</p>
<p>Pseudospin, 639 
</p>
<p>Quadrupole tensor, 425 
Quanta, 197 
</p>
<p>Quantization of energy, 160 
Quantum Hall Effect (QHE), 589 
</p>
<p>Radial equation 
</p>
<p>in three dimensions, 339 
in two dimensions, 316 
</p>
<p>Radial part of wave function 
in three dimensions, 339 
</p>
<p>in two dimensions, 316 
Raising operator 
</p>
<p>for angular momentum, 222 
</p>
<p>for harmonic oscillator, 205 
Range of potential, 525 
Rare earth elements, 3 71 
Ray, 118 
</p>
<p>Recursion relation, 193 
Reduced mass, 86 
</p>
<p>Reduced matrix element, 420 
Reflection coefficient, 168 
Resonances, 550 
</p>
<p>Rotations 
generators of (classical), 100 
generators of (quantum), 308 
in variance under (classical), I 00 
invariance under (quantum), 310 
</p>
<p>Runge-Lenz vector, 360, 422 
Rutherford cross section, 531 
Rydberg, 355 
</p>
<p>Scattering 
general theory, 523 
of identical particles, 560 
from step potential, 167 
of two particles, 555 
</p>
<p>Scattering amplitude, 527 </p>
<p/>
</div>
<div class="page"><p/>
<p>Schrodinger equation 
</p>
<p>equivalence to path integral, 229 
</p>
<p>time-dependent, 116, 143 
time-independent, 145 
</p>
<p>Schrodinger picture, 14 7, 484 
Schwartz inequality, 16 
Selection rule 
</p>
<p>angular momentum, 458, 459 
</p>
<p>dipole, 459 
</p>
<p>general, 458 
</p>
<p>Shell, 370 
</p>
<p>Singlet, 405 
S matrix 
</p>
<p>definition of, 529 
</p>
<p>partial wave, 54 7 
</p>
<p>Spectroscopic notation, 350 
</p>
<p>modified, 415 
</p>
<p>Spherical Bessel functions, 348 
Spherical Hankel functions, 348 
Spherical harmonics, 335, 336 
</p>
<p>Spherical Neumann functions, 348 
Spin, 325, 373 
</p>
<p>Spinor, 375 
</p>
<p>Spin-orbit interaction, 468 
Spin statistics theorem, 264 
Spontaneous decay, 517 
Spontaneous emission, 521 
Square-well potential, 164 
Stark effect, 459, 465 
Stationary states, 146 
Statistics, 264 
</p>
<p>determination of, 269 
Stem-Gerlach experiment, 399 
Subspaces, 17 
</p>
<p>Superposition principle, 117 
Symmetric states, 263 
Symmetries 
</p>
<p>classical, 98 
</p>
<p>quantum, 279 
</p>
<p>spontaneous breakdown of, 620 
</p>
<p>Tensor 
</p>
<p>antisymmetric ( ~&gt;;;k), 319 
cartesian, 41 7 
</p>
<p>irreducible, 418 
</p>
<p>operator, 417 
</p>
<p>quadrupole, 425 
</p>
<p>second rank, 418 
</p>
<p>spherical, 41 7 
Thermal wavelength, 625 
Thomas factor, 468, 571 
Thomas-Reiche-Kuhn rule, 457 
Time-ordered integral, 148 
Time-ordering symbol, 633, 651 
Time-reversal symmetry, 301 
Time translation invariance, 294 
Top state, 410 
</p>
<p>Total S basis, 405 
</p>
<p>Trace, 30 
</p>
<p>Transformations, 29 
active, 29, 97, 280 
</p>
<p>canonical, 92 
generator of, 99, 283 
</p>
<p>identity, 98 
</p>
<p>passive, 29, 280, 284 
</p>
<p>point, 94 
</p>
<p>regular, 97 
</p>
<p>unitary, 27 
</p>
<p>Translated state, 280 
</p>
<p>Translation 
finite, 289 
</p>
<p>generator of, 100, 283 
operator, 280 
</p>
<p>Translational invariance 
implications of, 98, 292 
in quantum theory, 279 
</p>
<p>Transmission coefficient, 168 
Transverse relaxation time, 395 
Triangle inequality, 116, 412 
Triplets, 405 
</p>
<p>Tunneling, 175, 616 
</p>
<p>Two-particle Hilbert space, 247 
</p>
<p>Uncertainty, 128 
</p>
<p>Uncertainty principle 
</p>
<p>applications of, 198 
</p>
<p>derivation of, 237 
energy-time, 245 
</p>
<p>physical basis of, 140 
Uncertainty relation, 138 
Unitarily bound, 548 
Unitary operator, 27 
</p>
<p>eigenvalues of, 39 
</p>
<p>eigenvectors of, 39 
</p>
<p>Variational method, 429 
Vector addition coefficients, 412 
Vectors 
</p>
<p>components of, 6 
improper, 67 
</p>
<p>inner product of, 8 
</p>
<p>norm of, 9 
</p>
<p>orthogonality of, 9 
</p>
<p>outer product of, 25 
</p>
<p>proper, 67 
</p>
<p>Vector operator, 313 
Vector space 
</p>
<p>axioms for, 2 
basis for, 6 
dimensionality of, 5 
field of, 2 
</p>
<p>of Hilbert, 67 
infinite dimensional, 57 
subspace of, 17 
</p>
<p>675 
</p>
<p>INDEX </p>
<p/>
</div>
<div class="page"><p/>
<p>676 
</p>
<p>INDEX 
</p>
<p>Virial theorem, 212 
for hydrogen, 359, 471 
</p>
<p>Wave functions, 121 
Wave-particle duality, 113 
</p>
<p>Waves 
interference of, I 08 
</p>
<p>matter, 112 
</p>
<p>plane, 108 
Wick's theorem, 645 
</p>
<p>Wigner-Eckart theorem, 420 
WKB approximation 
</p>
<p>and bound states, 445 
</p>
<p>introduction to, 435 
and path integrals, 438 
</p>
<p>three-dimensional, 449 
and tunneling, 444 
</p>
<p>X operator, 68 
</p>
<p>matrix elements of, 68 
</p>
<p>Yukawa potential, 531 
</p>
<p>Zeeman effect, 398 
Zero point energy, 198 </p>
<p/>
</div>
<ul>	<li>Preface to the Second Edition</li>
	<li>Preface to the First Edition</li>
<ul>	<li>For Whom Is this Book Intended?</li>
	<li>Acknowledgments</li>
</ul>
	<li>Prelude</li>
<ul>	<li>Note to the Student</li>
</ul>
	<li>Contents</li>
	<li>1 Mathematical Introduction</li>
<ul>	<li>1.1. Linear Vector Spaces: Basics</li>
	<li>1.2. Inner Product Spaces</li>
	<li>1.3. Dual Spaces and the Dirac Notation</li>
<ul>	<li>1.3.1. Expansion of Vectors in an Orthonormal Basis</li>
	<li>1.3.2. Adjoint Operation</li>
<ul>	<li>Gram-Schmidt Theorem</li>
	<li>Schwarz and Triangle Inequalities</li>
</ul>
</ul>
	<li>1.4. Subspaces</li>
	<li>1.5. Linear Operators</li>
	<li>1.6. Matrix Elements of Linear Operators</li>
<ul>	<li>Matrices Corresponding to Products of Operators</li>
	<li>The Adjoint of an Operator</li>
	<li>Hermitian, Anti-Hermitian, and Unitary Operators</li>
</ul>
	<li>1. 7. Active and Passive Transformations</li>
	<li>1.8. The Eigenvalue Problem</li>
<ul>	<li>The Characteristic Equation and the Solution to the Eigenvalue Problem</li>
	<li>Degeneracy</li>
	<li>Diagonalization of Hermitian Matrices</li>
	<li>Simultaneous Diagonalization of Two Hermitian Operators</li>
	<li>The Normal Modes</li>
</ul>
	<li>1.9. Functions of Operators and Related Concepts</li>
<ul>	<li>Derivatives of Operators with Respect to Parameters</li>
</ul>
	<li>1.10. Generalization to Infinite Dimensions</li>
<ul>	<li>Operators in Infinite Dimensions</li>
</ul>
</ul>
	<li>2 Review of Classical Mechanics</li>
<ul>	<li>2.1. &middot;The Principle of Least Action and Lagrangian Mechanics</li>
	<li>2.2. The Electromagnetic Lagrangian</li>
	<li>2.3. The Two-Body Problem</li>
	<li>2.4. How Smart Is a Particle?</li>
	<li>2.5. The Hamiltonian Formalism</li>
	<li>2.6. The Electromagnetic Force in the Hamiltonian Scheme</li>
	<li>2.7. Cyclic Coordinates, Poisson Brackets, and Canonical Transformations</li>
<ul>	<li>Canonical Transformations</li>
	<li>Active Transformations</li>
</ul>
	<li>2.8. Symmetries and Their Consequences</li>
<ul>	<li>A Useful Relation Between SandE</li>
</ul>
</ul>
	<li>3 All Is Not Well with&#13;Classical Mechanics</li>
<ul>	<li>3.1. Particles and Waves in Classical Physics</li>
	<li>3.2. An Experiment with Waves and Particles (Classical)</li>
	<li>3.3. The Double-Slit Experiment with Light</li>
	<li>3.4. Matter Waves (de Broglie Waves)</li>
	<li>3.5. Conclusions</li>
</ul>
	<li>4 The Postulates-a&#13;General Discussion</li>
<ul>	<li>4.1. The Postulatest</li>
	<li>4.2. Discussion of Postulates I-III</li>
<ul>	<li>CoUapse of the State Vector</li>
	<li>How to Test Quantum Theory</li>
	<li>Expectation Value</li>
	<li>The Uncertainty</li>
	<li>Compatible and Incompatible Variables</li>
	<li>The Density Matrix-a Digressiont</li>
	<li>Generalization to More Degrees of Freedom</li>
</ul>
	<li>4.3. The Schrodinger Equation (Dotting Your i 's and Crossing Your h's)</li>
<ul>	<li>Setting Up the Schr&ouml;dinger Equation</li>
	<li>General Approach to the Solution</li>
	<li>Choosing a Basis for Solving Schrodinger's Equation</li>
</ul>
</ul>
	<li>5 Simple Problems in&#13;One Dimension</li>
<ul>	<li>5.1. The Free Particle</li>
<ul>	<li>Time Evolution of the Gaussian Packet</li>
	<li>Some General Features of Energy Eigenfunctions</li>
</ul>
	<li>5.2. The Particle in a Box</li>
	<li>5.3. The Continuity Equation for Probability</li>
<ul>	<li>Ensemble Interpretation of j</li>
</ul>
	<li>5.4. The Single-Step Potential: A Problem in Scattering</li>
	<li>5.5. The Double-Slit Experiment</li>
	<li>5.6. Some Theorems</li>
</ul>
	<li>6 The Classical Limit</li>
	<li>7 The Harmonic Oscillator</li>
<ul>	<li>7.1. Why Study the Harmonic Oscillator</li>
	<li>7 .2. Review of the Classical OscUla tor</li>
	<li>7.3. Quantization of the Oscillator (Coordinate Basis)</li>
	<li>7 .4. The OsciUator in the Energy Basis</li>
	<li>7 .5. Passage from the Energy Basis to the X Basis</li>
</ul>
	<li>8 The Path Integral Formulation&#13;of Quantum Theory</li>
<ul>	<li>8.1. The Path Integral Recipe</li>
	<li>8.2. Analysis of the Recipe</li>
	<li>8.3. An Approximation to U(t) for a Free Particle</li>
	<li>8.4. Path Integral Evaluation of the Free-Particle Propagator</li>
	<li>8.5. Equivalence to the Schrodinger Equation</li>
	<li>8.6. Potentials of the Form V =a + bx + cx2 + di +exx</li>
</ul>
	<li>9 The Heisenberg&#13;Uncertainty Relations</li>
<ul>	<li>9.1. Introduction</li>
	<li>9.2. Derivation of the Uncertainty Relations</li>
	<li>9.3. The Minimum Uncertainty Packet</li>
	<li>9.4. Applications of the Uncertainty Principle</li>
	<li>9.5. The Energy-Time Uncertainty Relation</li>
</ul>
	<li>10 Systems with N Degrees&#13;of Freedom</li>
<ul>	<li>10.1. N Particles in One Dimension</li>
<ul>	<li>The Two-Particle Hilbert Space</li>
	<li>V 102 As a Direct Product Space</li>
	<li>The Direct Product Revisited</li>
	<li>Evolution of the Two-Particle State Vector</li>
	<li>N Particles in One Dimension</li>
</ul>
	<li>10.2. More Particles in More Dimensions</li>
	<li>10.3. Identical Particles</li>
<ul>	<li>The Classical Case</li>
	<li>Two-Particle Systems-Symmetric and Antisymmetric States</li>
	<li>Bosons and Fermions</li>
	<li>Bosonic and Fermionic Hilbert Spaces</li>
	<li>Determination of Particle Statistics</li>
	<li>Systems of N Identical Particles</li>
	<li>When Can We Ignore Symmetrization and Antisymmetrization?</li>
</ul>
</ul>
	<li>11 Symmetries and&#13;Their Consequences</li>
<ul>	<li>11.1. Overview</li>
	<li>11.2. Translational Invariance in Quantum Theory</li>
<ul>	<li>Translation in Terms of Passive Transformations</li>
	<li>A Digression on the Analogy with Classical Mechanicst</li>
	<li>Finite Translations</li>
	<li>A Digression on Finite Canonical and Unitary Transformations</li>
	<li>System of Particles</li>
	<li>Implications of Translational Invariances</li>
</ul>
	<li>11.3. Time Translational Invariance</li>
	<li>11.4. Parity Invariance</li>
	<li>11.5. Time-Reversal Symmetry</li>
</ul>
	<li>12 Rotational Invariance&#13;and Angular Momentum</li>
<ul>	<li>12.1. Translations in Two Dimensions</li>
	<li>12.2. Rotations in Two Dimensions</li>
<ul>	<li>Explicit Construction of U[R]</li>
</ul>
	<li>12.3. The Eigenvalue Problem of Lz</li>
<ul>	<li>Solutions to Rotationally Invariant Problems</li>
</ul>
	<li>12.4. Angular Momentum in Three Dimensions</li>
	<li>12.5. The Eigenvalue Problem of L 2 and Lz</li>
<ul>	<li>Finite Rotations</li>
	<li>Angular Momentum Eigenfunctions in the Coordinate Basis</li>
</ul>
	<li>12.6. Solution of Rotationally Invariant Problems</li>
<ul>	<li>General Properties of U Et</li>
	<li>The Free Particle in Spherical Coordinates</li>
	<li>Connection with the Solution in Cartesian Coordinates</li>
	<li>The Isotropic Oscillator</li>
</ul>
</ul>
	<li>13 The Hydrogen Atom</li>
<ul>	<li>13.1. The Eigenvalue Problem</li>
<ul>	<li>The Energy Levels</li>
	<li>The Wave Functions</li>
</ul>
	<li>13.2. The Degeneracy of the Hydrogen Spectrum</li>
	<li>13.3. Numerical Estimates and Comparison with Experiment</li>
<ul>	<li>Numerical Estimates</li>
	<li>Comparison with Experiment</li>
</ul>
	<li>13.4. Multielectron Atoms and the Periodic Table</li>
</ul>
	<li>14 Spin</li>
<ul>	<li>14.1. Introduction</li>
	<li>14.2. What is the Nature of Spin?</li>
	<li>14.3. Kinematics of Spin</li>
<ul>	<li>Explicit Forms of Rotation Operators</li>
</ul>
	<li>14.4. Spin Dynamics</li>
<ul>	<li>Orbital Magnetic Moment in Quantum Theory</li>
	<li>Spin Magnetic Moment</li>
	<li>Paramagnetic Resonance</li>
	<li>Negative Absolute Temperature (Optional Digression)</li>
</ul>
	<li>14.5. Return of Orbital Degrees of Freedom</li>
	<li>The Stern-Gerlach (SG) Experiment</li>
</ul>
	<li>15 Addition of Angular Momenta</li>
<ul>	<li>15.1. A Simple Example</li>
	<li>15.2. The General Problem</li>
<ul>	<li>Clebsch-Gordan (CG) Coefficients</li>
	<li>Addition of L and S</li>
	<li>The Modified Spectroscopic Notation</li>
</ul>
	<li>15.3. Irreducible Tensor Operators</li>
<ul>	<li>Tensor Operators</li>
</ul>
	<li>15.4. Explanation of Some "Accidental" Degeneracies</li>
<ul>	<li>Hydrogen</li>
	<li>The Oscillator</li>
	<li>The Free-Particle Solutions</li>
</ul>
</ul>
	<li>16 The Variational and WKB Methods</li>
<ul>	<li>16.1. The Variational Method</li>
	<li>16.2. The Wentzel-Kramers--Brillouin Method</li>
<ul>	<li>Connection with the Path Integral Formalism</li>
	<li>Tunneling Amplitudes</li>
	<li>Bound States</li>
</ul>
</ul>
	<li>17 Time-Independent Perturbation Theory</li>
<ul>	<li>17.1. The Formalism</li>
	<li>17.2. Some Examples</li>
<ul>	<li>Selection Rules</li>
</ul>
	<li>17.3. Degenerate Perturbation Theory</li>
<ul>	<li>Fine Structure</li>
</ul>
</ul>
	<li>18 Time-Dependent Perturbation Theory</li>
<ul>	<li>18.1. The Problem</li>
	<li>18.2. First-Order Perturbation Theory</li>
<ul>	<li>The Sudden Perturbation</li>
	<li>The Adiabatic Perturbation</li>
	<li>The Periodic Perturbation</li>
</ul>
	<li>18.3. Higher Orders in Perturbation Theory</li>
<ul>	<li>The Interaction Picture</li>
	<li>The Heisenberg Picture</li>
</ul>
	<li>18.4. A General Discussion of Electromagnetic Interactions</li>
<ul>	<li>Classical Electrodynamics</li>
	<li>The Potentials in Quantum Theory</li>
</ul>
	<li>18.5. Interaction of Atoms with Electromagnetic Radiation</li>
<ul>	<li>Photoelectric Effect in Hydrogen</li>
	<li>Field Quantization</li>
	<li>Spontaneous Decay</li>
</ul>
</ul>
	<li>19 Scattering Theory</li>
<ul>	<li>19.1. Introduction</li>
	<li>19.2. Recapitulation of One-Dimensional Scattering and Overview</li>
	<li>19.3. The Born Approximation (Time-Dependent Description)</li>
	<li>19.4. Born Again (The Time-Independent Description)</li>
<ul>	<li>Validity of the Born Approximation</li>
</ul>
	<li>19.5. The Partial Wave Expansion</li>
<ul>	<li>A Model Calculation of 1: The Hard Sphere</li>
	<li>Resonances</li>
</ul>
	<li>19.6. Two-Particle Scattering</li>
<ul>	<li>Passage to the Lab Frame</li>
	<li>Scattering of Identical Particles</li>
</ul>
</ul>
	<li>20 The Dirac Equation</li>
<ul>	<li>20.1. The Free-Particle Dirac Equation</li>
	<li>20.2. Electromagnetic Interaction of the Dirac Particle</li>
<ul>	<li>The Electron Spin and Magnetic Moment</li>
	<li>Hydrogen Fine Structure</li>
</ul>
	<li>20.3. More on Relativistic Quantum Mechanics</li>
</ul>
	<li>21 Path Integrals: Part II</li>
<ul>	<li>21.1. Derivation of the Path Integral</li>
<ul>	<li>The Landau Levels</li>
	<li>The Berry Phase</li>
	<li>Coherent State Path Integral</li>
</ul>
	<li>21.2. Imaginary Time Formalism</li>
<ul>	<li>Path Integral for the Imaginary Time Propagator</li>
	<li>Tunneling by Path Integrals: Well, well!</li>
	<li>Spontaneous Symmetry Breaking</li>
	<li>Imaginary Time Path Integrals and Quantum Statistical Mechanics</li>
	<li>Relation to Classical Statistical Mechanics</li>
</ul>
	<li>21.3. Spin and Fermion Path Integrals</li>
<ul>	<li>Spin Coherent States and Path Integral</li>
	<li>Fermion Oscillator and Coherent States</li>
	<li>The Fermionic Path Integral</li>
</ul>
	<li>21.4. Summary</li>
	<li>Bibliography</li>
</ul>
	<li>Appendix</li>
<ul>	<li>A.1. Matrix Inversion</li>
	<li>A.2. Gaussian Integrals</li>
	<li>A.3. Complex Numbers</li>
	<li>A.4. The i Prescription</li>
</ul>
	<li>Answers to Selected Exercises</li>
	<li>Table of Constants</li>
	<li>Index</li>
</ul>
</body></html>