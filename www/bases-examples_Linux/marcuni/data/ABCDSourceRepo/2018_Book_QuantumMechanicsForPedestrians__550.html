<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>299525_Print.indd</title>
</head>
<body><div class="page"><p/>
<p>Undergraduate Lecture Notes in Physics
</p>
<p>Jochen&nbsp;Pade
</p>
<p>Quantum 
Mechanics for 
Pedestrians 1
Fundamentals
</p>
<p> Second Edition </p>
<p/>
</div>
<div class="page"><p/>
<p>Undergraduate Lecture Notes in Physics
</p>
<p>Series editors
</p>
<p>Neil Ashby, University of Colorado, Boulder, CO, USA
</p>
<p>William Brantley, Department of Physics, Furman University, Greenville, SC, USA
</p>
<p>Matthew Deady, Physics Program, Bard College, Annandale-on-Hudson, NY, USA
</p>
<p>Michael Fowler, Department of Physics, University of Virginia, Charlottesville,
</p>
<p>VA, USA
</p>
<p>Morten Hjorth-Jensen, Department of Physics, University of Oslo, Oslo, Norway</p>
<p/>
</div>
<div class="page"><p/>
<p>Undergraduate Lecture Notes in Physics (ULNP) publishes authoritative texts covering
</p>
<p>topics throughout pure and applied physics. Each title in the series is suitable as a basis for
</p>
<p>undergraduate instruction, typically containing practice problems, worked examples, chapter
</p>
<p>summaries, and suggestions for further reading.
</p>
<p>ULNP titles must provide at least one of the following:
</p>
<p>&bull; An exceptionally clear and concise treatment of a standard undergraduate subject.
</p>
<p>&bull; A solid undergraduate-level introduction to a graduate, advanced, or non-standard subject.
</p>
<p>&bull; A novel perspective or an unusual approach to teaching a subject.
</p>
<p>ULNP especially encourages new, original, and idiosyncratic approaches to physics teaching
</p>
<p>at the undergraduate level.
</p>
<p>The purpose of ULNP is to provide intriguing, absorbing books that will continue to be the
</p>
<p>reader&rsquo;s preferred reference throughout their academic career.
</p>
<p>More information about this series at http://www.springer.com/series/8917</p>
<p/>
<div class="annotation"><a href="http://www.springer.com/series/8917">http://www.springer.com/series/8917</a></div>
</div>
<div class="page"><p/>
<p>Jochen Pade
</p>
<p>Quantum Mechanics
for Pedestrians 1
</p>
<p>Fundamentals
</p>
<p>Second Edition
</p>
<p>123</p>
<p/>
</div>
<div class="page"><p/>
<p>Jochen Pade
Institut f&uuml;r Physik
Universit&auml;t Oldenburg
Oldenburg, Germany
</p>
<p>ISSN 2192-4791 ISSN 2192-4805 (electronic)
Undergraduate Lecture Notes in Physics
ISBN 978-3-030-00463-7 ISBN 978-3-030-00464-4 (eBook)
https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>Library of Congress Control Number: 2018954852
</p>
<p>Originally published with the title: Original Quantum Mechanics for Pedestrians 1: Fundamentals
1st edition: &copy; Springer International Publishing Switzerland 2014
2nd edition: &copy; Springer Nature Switzerland AG 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission
</p>
<p>or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a specific statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
</p>
<p>book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional affiliations.
</p>
<p>This Springer imprint is published by the registered company Springer Nature Switzerland AG
</p>
<p>The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>Preface to the Second Edition, Volume 1
</p>
<p>The first edition of &lsquo;Physics for Pedestrians&rsquo; was very well received. Repeatedly, I
</p>
<p>was asked to extend the considerations to relativistic phenomena. This has now
</p>
<p>been done in this second edition. Volume 1 contains elements of relativistic
</p>
<p>quantum mechanics, and Volume 2 contains elements of quantum field theory.
</p>
<p>These extensions are placed in the Appendix. They are not comprehensive and
</p>
<p>complete presentations of the topics, but rather concise accounts of some essential
</p>
<p>ideas of relativistic quantum physics.
</p>
<p>Furthermore, for the sake of completeness and to guarantee a consistent notation,
</p>
<p>there are outlines of relevant topics such as special relativity, classical field theory,
</p>
<p>and electrodynamics.
</p>
<p>In addition, a few minor bugs have been fixed and some information has been
</p>
<p>updated.
</p>
<p>I gratefully thank Svend-Age Biehs, Heinz Helmers, Stefanie Hoppe, Friedhelm
</p>
<p>Kuypers and Lutz Polley who have helped me in one way or another to prepare this
</p>
<p>second edition.
</p>
<p>Oldenburg, Germany Jochen Pade
</p>
<p>February 2018
</p>
<p>v</p>
<p/>
</div>
<div class="page"><p/>
<p>Preface to the First Edition, Volume 1
</p>
<p>There are so many textbooks on quantum mechanics&mdash;do we really need another
</p>
<p>one?
</p>
<p>Certainly, there may be different answers to this question. After all, quantum
</p>
<p>mechanics is such a broad field that a single textbook cannot cover all the relevant
</p>
<p>topics. A selection or prioritization of subjects is necessary per se, and moreover,
</p>
<p>the physical and mathematical foreknowledge of the readers has to be taken into
</p>
<p>account in an adequate manner. Hence, there is undoubtedly not only a certain
</p>
<p>leeway, but also a definite need for a wide variety of presentations.
</p>
<p>Quantum Mechanics for Pedestrians has a thematic blend that distinguishes it
</p>
<p>from other introductions to quantum mechanics (at least those of which I am
</p>
<p>aware). It is not just about the conceptual and formal foundations of quantum
</p>
<p>mechanics, but from the beginning and in some detail it also discusses both current
</p>
<p>topics as well as advanced applications and basic problems as well as epistemo-
</p>
<p>logical questions. Thus, this book is aimed especially at those who want to learn not
</p>
<p>only the appropriate formalism in a suitable manner, but also those other aspects of
</p>
<p>quantum mechanics addressed here. This is particularly interesting for students who
</p>
<p>want to teach quantum mechanics themselves, whether at the school level or
</p>
<p>elsewhere. The current topics and epistemological issues are especially suited to
</p>
<p>generate interest and motivation among students.
</p>
<p>Like many introductions to quantum mechanics, this book consists of lecture
</p>
<p>notes which have been extended and complemented. The course which I have given
</p>
<p>for several years is aimed at teacher candidates and graduate students in the mas-
</p>
<p>ter&rsquo;s program, but is also attended by students from other degree programs. The
</p>
<p>course includes lectures (two sessions/four hours per week) and problem sessions
</p>
<p>(two hours per week). It runs for 14 weeks, which is reflected in the 28 chapters
</p>
<p>of the lecture notes.
</p>
<p>Due to the usual interruptions such as public holidays, it will not always be
</p>
<p>possible to treat all 28 chapters in 14 weeks. On the other hand, the later chapters in
</p>
<p>particular are essentially independent of each other. Therefore, one can make a
</p>
<p>selection based on personal taste without losing coherence. Since the book consists
</p>
<p>of extended lecture notes, most of the chapters naturally offer more material than
</p>
<p>vii</p>
<p/>
</div>
<div class="page"><p/>
<p>will fit into a two-hour lecture. But the &lsquo;main material&rsquo; can readily be presented
</p>
<p>within this time; in addition, some further topics may be treated using the exercises.
</p>
<p>Before attending the quantum mechanics course, the students have had among
</p>
<p>others an introduction to atomic physics: Relevant phenomena, experiments, and
</p>
<p>simple calculations should therefore be familiar to them. Nevertheless, experience
</p>
<p>has shown that at the start of the lectures, some students do not have enough
</p>
<p>substantial and available knowledge at their disposal. This applies less to physical
</p>
<p>and more to the necessary mathematical knowledge, and there are certainly several
</p>
<p>reasons for this. One of them may be that for teacher training; not only the
</p>
<p>quasi-traditional combination physics/mathematics is allowed, but also others such
</p>
<p>as physics/sports, where it is obviously more difficult to acquire the necessary
</p>
<p>mathematical background and, especially, to actively practice its use.
</p>
<p>To allow for this, I have included some chapters with basic mathematical
</p>
<p>knowledge in the Appendix, so that students can use them to overcome any
</p>
<p>remaining individual knowledge gaps. Moreover, the mathematical level is quite
</p>
<p>simple, especially in the early chapters; this course is not just about practicing
</p>
<p>specifically elaborated formal methods, but rather we aim at a compact and easily
</p>
<p>accessible introduction to key aspects of quantum mechanics.
</p>
<p>As remarked above, there are a number of excellent textbooks on quantum
</p>
<p>mechanics, not to mention many useful Internet sites. It goes without saying that in
</p>
<p>writing the lecture notes, I have consulted some of these, have been inspired by
</p>
<p>them and have adopted appropriate ideas, exercises, etc., without citing them in
</p>
<p>detail. These books and Internet sites are all listed in the bibliography and some are
</p>
<p>referred to directly in the text.
</p>
<p>A note on the title Quantum Mechanics for Pedestrians: It does not mean
</p>
<p>&lsquo;quantum mechanics light&rsquo; in the sense of a painless transmission of knowledge &agrave; la
</p>
<p>Nuremberg funnel. Instead, &lsquo;for pedestrians&rsquo; is meant here in the sense of auton-
</p>
<p>omous and active movement&mdash;step by step, not necessarily fast, from time to time
</p>
<p>(i.e., along the more difficult stretches) somewhat strenuous, depending on the level
</p>
<p>of understanding of each walker&mdash;which will, by the way, become steadily better
</p>
<p>while walking on.
</p>
<p>Speaking metaphorically, it is about discovering on foot the landscape of
</p>
<p>quantum mechanics; it is about improving one&rsquo;s knowledge of each locale (if
</p>
<p>necessary, by taking detours); and it is perhaps even about finding your own way.
</p>
<p>By the way, it is always amazing not only how far one can walk with some
</p>
<p>perseverance, but also how fast it goes&mdash;and how sustainable it is. &lsquo;Only where you
</p>
<p>have visited on foot, have you really been.&rsquo; (Johann Wolfgang von Goethe).
</p>
<p>Klaus Schlupmann, Heinz Helmers, Edith Bakenhus, Regina Richter, and my
</p>
<p>sons, Jan Philipp and Jonas have critically read several chapters. Sabrina Milke
</p>
<p>assisted me in making the index. I enjoyed enlightening discussions with Lutz
</p>
<p>Polley, while Martin Holthaus provided helpful support and William Brewer made
</p>
<p>useful suggestions. I gratefully thank them and all the others who have helped me in
</p>
<p>some way or other in the realization of this book.
</p>
<p>viii Preface to the First Edition, Volume 1</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents
</p>
<p>Part I Fundamentals
</p>
<p>1 Towards the Schr&ouml;dinger Equation . . . . . . . . . . . . . . . . . . . . . . . . 3
</p>
<p>1.1 How to Find a New Theory . . . . . . . . . . . . . . . . . . . . . . . . . . 3
</p>
<p>1.2 The Classical Wave Equation and the Schr&ouml;dinger
</p>
<p>Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
</p>
<p>1.2.1 From the Wave Equation to the Dispersion
</p>
<p>Relation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
</p>
<p>1.2.2 From the Dispersion Relation to the Schr&ouml;dinger
</p>
<p>Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
</p>
<p>1.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
</p>
<p>2 Polarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
</p>
<p>2.1 Light as Waves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
</p>
<p>2.1.1 The Typical Shape of an Electromagnetic Wave . . . . . 16
</p>
<p>2.1.2 Linear and Circular Polarization . . . . . . . . . . . . . . . . . 17
</p>
<p>2.1.3 From Polarization to the Space of States . . . . . . . . . . 19
</p>
<p>2.2 Light as Photons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
</p>
<p>2.2.1 Single Photons and Polarization . . . . . . . . . . . . . . . . . 23
</p>
<p>2.2.2 Measuring the Polarization of Single Photons . . . . . . . 25
</p>
<p>2.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
</p>
<p>3 More on the Schr&ouml;dinger Equation . . . . . . . . . . . . . . . . . . . . . . . . 29
</p>
<p>3.1 Properties of the Schr&ouml;dinger Equation . . . . . . . . . . . . . . . . . . 29
</p>
<p>3.2 The Time-Independent Schr&ouml;dinger Equation . . . . . . . . . . . . . 31
</p>
<p>3.3 Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
</p>
<p>3.3.1 Classical Numbers and Quantum-Mechanical
</p>
<p>Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
</p>
<p>3.3.2 Commutation of Operators; Commutators . . . . . . . . . . 36
</p>
<p>3.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
</p>
<p>ix</p>
<p/>
</div>
<div class="page"><p/>
<p>4 Complex Vector Spaces and Quantum Mechanics . . . . . . . . . . . . . 41
</p>
<p>4.1 Norm, Bra-Ket Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
</p>
<p>4.2 Orthogonality, Orthonormality . . . . . . . . . . . . . . . . . . . . . . . . 44
</p>
<p>4.3 Completeness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
</p>
<p>4.4 Projection Operators, Measurement . . . . . . . . . . . . . . . . . . . . . 47
</p>
<p>4.4.1 Projection Operators . . . . . . . . . . . . . . . . . . . . . . . . . 47
</p>
<p>4.4.2 Measurement and Eigenvalues . . . . . . . . . . . . . . . . . . 51
</p>
<p>4.4.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
</p>
<p>4.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
</p>
<p>5 Two Simple Solutions of the Schr&ouml;dinger Equation . . . . . . . . . . . . 55
</p>
<p>5.1 The Infinite Potential Well . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
</p>
<p>5.1.1 Solution of the Schr&ouml;dinger Equation, Energy
</p>
<p>Quantization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
</p>
<p>5.1.2 Solution of the Time-Dependent Schr&ouml;dinger
</p>
<p>Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
</p>
<p>5.1.3 Properties of the Eigenfunctions and Their
</p>
<p>Consequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
</p>
<p>5.1.4 Determination of the Coefficients cn . . . . . . . . . . . . . . 62
</p>
<p>5.2 Free Motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
</p>
<p>5.2.1 General Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
</p>
<p>5.2.2 Example: Gaussian Distribution . . . . . . . . . . . . . . . . . 65
</p>
<p>5.3 General Potentials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
</p>
<p>5.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
</p>
<p>6 Interaction-Free Measurement . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
</p>
<p>6.1 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
</p>
<p>6.1.1 Classical Light Rays and Particles
</p>
<p>in the Mach&ndash;Zehnder Interferometer . . . . . . . . . . . . . . 73
</p>
<p>6.1.2 Photons in the Mach&ndash;Zehnder Interferometer . . . . . . . 75
</p>
<p>6.2 Formal Description, Unitary Operators . . . . . . . . . . . . . . . . . . 78
</p>
<p>6.2.1 First Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
</p>
<p>6.2.2 Second Approach (Operators) . . . . . . . . . . . . . . . . . . 80
</p>
<p>6.3 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
</p>
<p>6.3.1 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
</p>
<p>6.3.2 Quantum Zeno Effect . . . . . . . . . . . . . . . . . . . . . . . . 82
</p>
<p>6.3.3 Delayed-Choice Experiments . . . . . . . . . . . . . . . . . . . 83
</p>
<p>6.3.4 The Hadamard Transformation . . . . . . . . . . . . . . . . . . 83
</p>
<p>6.3.5 From the MZI to the Quantum Computer . . . . . . . . . . 84
</p>
<p>6.3.6 Hardy&rsquo;s Experiment . . . . . . . . . . . . . . . . . . . . . . . . . 84
</p>
<p>6.3.7 How Interaction-Free is the &lsquo;Interaction-Free&rsquo;
</p>
<p>Quantum Measurement? . . . . . . . . . . . . . . . . . . . . . . 84
</p>
<p>6.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
</p>
<p>x Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>7 Position Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
</p>
<p>7.1 Position Probability and Measurements . . . . . . . . . . . . . . . . . . 88
</p>
<p>7.1.1 Example: Infinite Potential Wall . . . . . . . . . . . . . . . . . 88
</p>
<p>7.1.2 Bound Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
</p>
<p>7.1.3 Free Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
</p>
<p>7.2 Real Potentials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
</p>
<p>7.3 Probability Current Density . . . . . . . . . . . . . . . . . . . . . . . . . . 95
</p>
<p>7.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
</p>
<p>8 Neutrino Oscillations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
</p>
<p>8.1 The Neutrino Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
</p>
<p>8.2 Modelling the Neutrino Oscillations . . . . . . . . . . . . . . . . . . . . 100
</p>
<p>8.2.1 States . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
</p>
<p>8.2.2 Time Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
</p>
<p>8.2.3 Numerical Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
</p>
<p>8.2.4 Three-Dimensional Neutrino Oscillations . . . . . . . . . . 103
</p>
<p>8.3 Generalizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
</p>
<p>8.3.1 Hermitian Operators . . . . . . . . . . . . . . . . . . . . . . . . . 105
</p>
<p>8.3.2 Time Evolution and Measurement . . . . . . . . . . . . . . . 106
</p>
<p>8.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
</p>
<p>9 Expectation Values, Mean Values, and Measured Values . . . . . . . 109
</p>
<p>9.1 Mean Values and Expectation Values . . . . . . . . . . . . . . . . . . . 109
</p>
<p>9.1.1 Mean Values of Classical Measurements . . . . . . . . . . 109
</p>
<p>9.1.2 Expectation Value of the Position in Quantum
</p>
<p>Mechanics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
</p>
<p>9.1.3 Expectation Value of the Momentum in Quantum
</p>
<p>Mechanics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
</p>
<p>9.1.4 General Definition of the Expectation Value . . . . . . . . 113
</p>
<p>9.1.5 Variance, Standard Deviation . . . . . . . . . . . . . . . . . . . 115
</p>
<p>9.2 Hermitian Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
</p>
<p>9.2.1 Hermitian Operators Have Real Eigenvalues . . . . . . . . 117
</p>
<p>9.2.2 Eigenfunctions of Different Eigenvalues Are
</p>
<p>Orthogonal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
</p>
<p>9.3 Time Behavior, Conserved Quantities . . . . . . . . . . . . . . . . . . . 119
</p>
<p>9.3.1 Time Behavior of Expectation Values . . . . . . . . . . . . 119
</p>
<p>9.3.2 Conserved Quantities . . . . . . . . . . . . . . . . . . . . . . . . . 120
</p>
<p>9.3.3 Ehrenfest&rsquo;s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . 121
</p>
<p>9.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
</p>
<p>Contents xi</p>
<p/>
</div>
<div class="page"><p/>
<p>10 Stopover; Then on to Quantum Cryptography . . . . . . . . . . . . . . . 125
</p>
<p>10.1 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
</p>
<p>10.2 Summary and Open Questions . . . . . . . . . . . . . . . . . . . . . . . . 125
</p>
<p>10.2.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
</p>
<p>10.2.2 Open Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
</p>
<p>10.3 Quantum Cryptography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
</p>
<p>10.3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
</p>
<p>10.3.2 One-Time Pad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
</p>
<p>10.3.3 BB84 Protocol Without Eve . . . . . . . . . . . . . . . . . . . 133
</p>
<p>10.3.4 BB84 Protocol with Eve . . . . . . . . . . . . . . . . . . . . . . 135
</p>
<p>11 Abstract Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
</p>
<p>11.1 Hilbert Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
</p>
<p>11.1.1 Wavefunctions and Coordinate Vectors . . . . . . . . . . . 139
</p>
<p>11.1.2 The Scalar Product . . . . . . . . . . . . . . . . . . . . . . . . . . 141
</p>
<p>11.1.3 Hilbert Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
</p>
<p>11.2 Matrix Mechanics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
</p>
<p>11.3 Abstract Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
</p>
<p>11.4 Concrete: Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
</p>
<p>11.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
</p>
<p>12 Continuous Spectra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
</p>
<p>12.1 Improper Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
</p>
<p>12.2 Position Representation and Momentum Representation . . . . . . 157
</p>
<p>12.3 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
</p>
<p>12.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
</p>
<p>13 Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
</p>
<p>13.1 Hermitian Operators, Observables . . . . . . . . . . . . . . . . . . . . . . 166
</p>
<p>13.1.1 Three Important Properties of Hermitian
</p>
<p>Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
</p>
<p>13.1.2 Uncertainty Relations . . . . . . . . . . . . . . . . . . . . . . . . 170
</p>
<p>13.1.3 Degenerate Spectra . . . . . . . . . . . . . . . . . . . . . . . . . . 173
</p>
<p>13.2 Unitary Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
</p>
<p>13.2.1 Unitary Transformations . . . . . . . . . . . . . . . . . . . . . . 174
</p>
<p>13.2.2 Functions of Operators, the Time-Evolution
</p>
<p>Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
</p>
<p>13.3 Projection Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
</p>
<p>13.3.1 Spectral Representation . . . . . . . . . . . . . . . . . . . . . . . 178
</p>
<p>13.3.2 Projection and Properties . . . . . . . . . . . . . . . . . . . . . . 179
</p>
<p>13.3.3 Measurements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
</p>
<p>13.4 Systematics of the Operators . . . . . . . . . . . . . . . . . . . . . . . . . 181
</p>
<p>13.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
</p>
<p>xii Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>14 Postulates of Quantum Mechanics . . . . . . . . . . . . . . . . . . . . . . . . . 187
</p>
<p>14.1 Postulates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
</p>
<p>14.1.1 States, State Space (Question 1) . . . . . . . . . . . . . . . . . 188
</p>
<p>14.1.2 Probability Amplitudes, Probability (Question 2) . . . . . 190
</p>
<p>14.1.3 Physical Quantities and Hermitian Operators
</p>
<p>(Question 2) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
</p>
<p>14.1.4 Measurement and State Reduction (Question 2) . . . . . 191
</p>
<p>14.1.5 Time Evolution (Question 3) . . . . . . . . . . . . . . . . . . . 192
</p>
<p>14.2 Some Open Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
</p>
<p>14.3 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
</p>
<p>14.3.1 Postulates of Quantum Mechanics as a Framework . . . 199
</p>
<p>14.3.2 Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
</p>
<p>14.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
</p>
<p>Appendix A: Abbreviations and Notations . . . . . . . . . . . . . . . . . . . . . . . . 203
</p>
<p>Appendix B: Units and Constants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
</p>
<p>Appendix C: Complex Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
</p>
<p>Appendix D: Calculus I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
</p>
<p>Appendix E: Calculus II . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
</p>
<p>Appendix F: Linear Algebra I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
</p>
<p>Appendix G: Linear Algebra II . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
</p>
<p>Appendix H: Fourier Transforms and the Delta Function. . . . . . . . . . . . 273
</p>
<p>Appendix I: Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
</p>
<p>Appendix J: From Quantum Hopping to the Schr&ouml;dinger Equation . . . 311
</p>
<p>Appendix K: The Phase Shift at a Beam Splitter . . . . . . . . . . . . . . . . . . . 317
</p>
<p>Appendix L: The Quantum Zeno Effect . . . . . . . . . . . . . . . . . . . . . . . . . . 319
</p>
<p>Appendix M: Delayed Choice and the Quantum Eraser . . . . . . . . . . . . . 327
</p>
<p>Appendix N: The Equation of Continuity . . . . . . . . . . . . . . . . . . . . . . . . . 333
</p>
<p>Appendix O: Variance, Expectation Values . . . . . . . . . . . . . . . . . . . . . . . 335
</p>
<p>Appendix P: On Quantum Cryptography . . . . . . . . . . . . . . . . . . . . . . . . . 339
</p>
<p>Appendix Q: Schr&ouml;dinger Picture, Heisenberg Picture, Interaction
</p>
<p>Picture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
</p>
<p>Appendix R: The Postulates of Quantum Mechanics . . . . . . . . . . . . . . . . 351
</p>
<p>Appendix S: System and Measurement: Some Concepts . . . . . . . . . . . . . 367
</p>
<p>Contents xiii</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix T: Recaps and Outlines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
</p>
<p>T.1 Discrete - Continuous . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
</p>
<p>T.2 Special Relativity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
</p>
<p>T.3 Classical Field Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388
</p>
<p>T.4 Electrodynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
</p>
<p>Appendix U: Elements of Relativistic Quantum Mechanics . . . . . . . . . . . 405
</p>
<p>U.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405
</p>
<p>U.2 Constructing Relativistic Equations . . . . . . . . . . . . . . . . . . . . . . 406
</p>
<p>U.3 Plane Wave Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413
</p>
<p>U.4 Covariant Formulation of the Dirac Equation . . . . . . . . . . . . . . 418
</p>
<p>U.5 Dirac Equation and the Hydrogen Atom . . . . . . . . . . . . . . . . . . 427
</p>
<p>U.6 Discussion of the Dirac Equation . . . . . . . . . . . . . . . . . . . . . . . . 428
</p>
<p>U.7 Exercises and Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433
</p>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 . . . . . . . . . . . . . . . 441
</p>
<p>Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513
</p>
<p>Index of Volume 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515
</p>
<p>Index of Volume 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 519
</p>
<p>xiv Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents of Volume 2
</p>
<p>Part II Applications and Extensions
</p>
<p>15 One-Dimensional Piecewise-Constant Potentials . . . . . . . . . . . . . . . 3
</p>
<p>16 Angular Momentum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
</p>
<p>17 The Hydrogen Atom . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
</p>
<p>18 The Harmonic Oscillator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
</p>
<p>19 Perturbation Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
</p>
<p>20 Entanglement, EPR, Bell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
</p>
<p>21 Symmetries and Conservation Laws . . . . . . . . . . . . . . . . . . . . . . . . 99
</p>
<p>22 The Density Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
</p>
<p>23 Identical Particles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
</p>
<p>24 Decoherence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
</p>
<p>25 Scattering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
</p>
<p>26 Quantum Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
</p>
<p>27 Is Quantum Mechanics Complete? . . . . . . . . . . . . . . . . . . . . . . . . . 203
</p>
<p>28 Interpretations of Quantum Mechanics . . . . . . . . . . . . . . . . . . . . . . 219
</p>
<p>Appendix A: Abbreviations and Notations . . . . . . . . . . . . . . . . . . . . . . . . 235
</p>
<p>Appendix B: Special Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
</p>
<p>Appendix C: Tensor Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
</p>
<p>Appendix D: Wave Packets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
</p>
<p>Appendix E: Laboratory System, Center-of-Mass System . . . . . . . . . . . . 263
</p>
<p>Appendix F: Analytic Treatment of the Hydrogen Atom . . . . . . . . . . . . . 267
</p>
<p>xv</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix G: The Lenz Vector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
</p>
<p>Appendix H: Perturbative Calculation of the Hydrogen Atom . . . . . . . . 293
</p>
<p>Appendix I: The Production of Entangled Photons . . . . . . . . . . . . . . . . . 297
</p>
<p>Appendix J: The Hardy Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
</p>
<p>Appendix K: Set-Theoretical Derivation of the Bell Inequality . . . . . . . . 309
</p>
<p>Appendix L: The Special Galilei Transformation . . . . . . . . . . . . . . . . . . . 311
</p>
<p>Appendix M: Kramers&rsquo; Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
</p>
<p>Appendix N: Coulomb Energy and Exchange Energy
</p>
<p>in the Helium Atom . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
</p>
<p>Appendix O: The Scattering of Identical Particles . . . . . . . . . . . . . . . . . . 329
</p>
<p>Appendix P: The Hadamard Transformation . . . . . . . . . . . . . . . . . . . . . . 333
</p>
<p>Appendix Q: From the Interferometer to the Computer . . . . . . . . . . . . . 339
</p>
<p>Appendix R: The Grover Algorithm, Algebraically . . . . . . . . . . . . . . . . . 345
</p>
<p>Appendix S: Shor Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
</p>
<p>Appendix T: The Gleason Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
</p>
<p>Appendix U: What is Real? Some Quotations . . . . . . . . . . . . . . . . . . . . . 369
</p>
<p>Appendix V: Remarks on Some Interpretations of Quantum
</p>
<p>Mechanics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
</p>
<p>Appendix W: Elements of Quantum Field Theory . . . . . . . . . . . . . . . . . . 387
</p>
<p>Appendix X: Exercises and Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485
</p>
<p>Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 577
</p>
<p>Index of Volume 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579
</p>
<p>Index of Volume 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 583
</p>
<p>xvi Contents of Volume 2</p>
<p/>
</div>
<div class="page"><p/>
<p>Introduction
</p>
<p>Quantum mechanics is probably the most accurately verified physical theory
</p>
<p>existing today. To date, there has been no contradiction from any experiments; the
</p>
<p>applications of quantum mechanics have changed our world right up to aspects of
</p>
<p>our everyday life. There is no doubt that quantum mechanics &lsquo;functions&rsquo;&mdash;it is
</p>
<p>indeed extremely successful. On a formal level, it is clearly unambiguous and
</p>
<p>consistent and (certainly not unimportant)&mdash;as a theory&mdash;it is both aesthetically
</p>
<p>satisfying and convincing.
</p>
<p>The question in dispute is the &lsquo;real&rsquo; meaning of quantum mechanics. What does
</p>
<p>the wavefunction stand for, and what is the role of chance? Do we actually have to
</p>
<p>throw overboard our classical and familiar conceptions of reality? Despite the
</p>
<p>nearly century-long history of quantum mechanics, fundamental questions of this
</p>
<p>kind are still unresolved and are currently being discussed in a lively and contro-
</p>
<p>versial manner. There are two contrasting positions (along with many intermediate
</p>
<p>views): Some see quantum mechanics simply as the precursor stage of the &lsquo;true&rsquo;
</p>
<p>theory (although eminently functional); others see it as a valid, fundamental theory
</p>
<p>itself.
</p>
<p>This book aims to introduce its readers to both sides of quantum mechanics, the
</p>
<p>established side and the side that is still under discussion. We develop here both the
</p>
<p>conceptual and formal foundations of quantum mechanics, and we discuss some of
</p>
<p>its &lsquo;problem areas.&rsquo; In addition, this book includes applications&mdash;oriented funda-
</p>
<p>mental topics, some &lsquo;modern&rsquo; ones&mdash;for example, issues in quantum information&mdash;
</p>
<p>and &lsquo;traditional&rsquo; ones such as the hydrogen and the helium atoms. We restrict
</p>
<p>ourselves to the field of nonrelativistic physics, although many of the ideas can be
</p>
<p>extended to the relativistic case.1 Moreover, we consider only time-independent
</p>
<p>interactions.
</p>
<p>In introductory courses on quantum mechanics, the practice of formal skills often
</p>
<p>takes priority (this is subsumed under the slogan &lsquo;shut up and calculate&rsquo;). In
</p>
<p>accordance with our objectives here, we will also give appropriate space to the
</p>
<p>discussion of fundamental questions. This special blend of basic discussion and
</p>
<p>1In the second edition, some essentials of relativistic quantummechanics are added; see theAppendix.
</p>
<p>xvii</p>
<p/>
</div>
<div class="page"><p/>
<p>modern practice is in itself very well suited to evoke interest and motivation in
</p>
<p>students. This is, in addition, enhanced by the fact that some important fundamental
</p>
<p>ideas can be discussed using very simple model systems as examples. It is not
</p>
<p>coincidental that some of the topics and phenomena addressed here are treated in
</p>
<p>various simplified forms in high-school textbooks.
</p>
<p>In mathematical terms, there are two main approaches used in introductions to
</p>
<p>quantum mechanics. The first one relies on differential equations (i.e., analysis) and
</p>
<p>the other one on vector spaces (i.e., linear algebra); of course, the &lsquo;finished&rsquo;
</p>
<p>quantum mechanics is independent of the route of access chosen. Each approach
</p>
<p>(they also may be called the Schrodinger and the Heisenberg routes) has its own
</p>
<p>advantages and disadvantages; the two are used in this book on an equal footing.
</p>
<p>The roadmap of the book is as follows:
</p>
<p>The foundations and structure of quantum mechanics are worked out step by step
</p>
<p>in the first part (Volume 1, Chaps. 1&ndash;14), alternatively from an analytical approach
</p>
<p>(odd chapters) and from an algebraic approach (even chapters). In this way, we
</p>
<p>avoid limiting ourselves to only one of the two formulations. In addition, the two
</p>
<p>approaches reinforce each other in the development of important concepts. The
</p>
<p>merging of the two threads starts in Chap. 12. In Chap. 14, the conclusions thus far
</p>
<p>reached are summarized in the form of quite general postulates for quantum
</p>
<p>mechanics.
</p>
<p>Especially in the algebraic chapters, we take up current problems early on
</p>
<p>(interaction-free quantum measurements, the neutrino problem, quantum cryptog-
</p>
<p>raphy). This is possible since these topics can be treated using very simple math-
</p>
<p>ematics. Thus, this type of access is also of great interest for high-school level
</p>
<p>courses. In the analytical approach, we use as elementary physical model systems
</p>
<p>the infinite potential well and free particle motion.
</p>
<p>In the second part (Volume 2, Chaps. 15&ndash;28), applications and extensions of the
</p>
<p>formalism are considered. The discussion of the conceptual difficulties (measure-
</p>
<p>ment problem, locality and reality, etc.) again constitutes a central theme, as in the
</p>
<p>first volume. In addition to some more traditionally oriented topics (angular
</p>
<p>momentum, simple potentials, perturbation theory, symmetries, identical particles,
</p>
<p>scattering), we begin in Chap. 20 with the consideration of whether quantum
</p>
<p>mechanics is a local realistic theory. In Chap. 22, we introduce the density operator
</p>
<p>in order to consider in Chap. 24 the phenomenon of decoherence and its relevance
</p>
<p>to the measurement process. In Chap. 27, we continue the realism debate and
</p>
<p>explore the question as to what extent quantum mechanics can be regarded as a
</p>
<p>complete theory. Modern applications in the field of quantum information can be
</p>
<p>found in Chap. 26.
</p>
<p>Finally, we outline in Chap. 28 the most common interpretations of quantum
</p>
<p>mechanics. Apart from this chapter, a general statement applies: While it is still a
</p>
<p>controversial issue as to which (if indeed any) of the current interpretations is the
</p>
<p>&lsquo;correct&rsquo; one, an introduction to quantum mechanics must take a concrete position
</p>
<p>and has to present the material in a coherent form. In this book, we choose the
</p>
<p>version commonly known as the &lsquo;standard interpretation.&rsquo;
</p>
<p>xviii Introduction</p>
<p/>
</div>
<div class="page"><p/>
<p>A few words about the role of mathematics:
</p>
<p>In describing objects that&mdash;due to their small size&mdash;are beyond our everyday
</p>
<p>experience, quantum mechanics cannot be formulated completely in terms of
</p>
<p>everyday life and must therefore remain to some extent abstract. A deeper under-
</p>
<p>standing of quantum mechanics cannot be achieved on a purely linguistic level; we
</p>
<p>definitely need mathematical descriptions.2 Of course, one can use analogies and
</p>
<p>simplified models, but that works only to a certain degree and also makes sense
</p>
<p>only if one is aware of the underlying mathematical apparatus, at least in broad
</p>
<p>terms.3
</p>
<p>It is due to this interaction of the need for mathematical formulations and the
</p>
<p>lack of intuitive access that quantum mechanics is often regarded as &lsquo;difficult.&rsquo; But
</p>
<p>that is only part of the truth; to be sure, there are highly formalized and demanding
</p>
<p>aspects. Many wider and interesting issues, however, are characterized by very
</p>
<p>simple principles that can be described using only a basic formalism.
</p>
<p>Nevertheless, beginners in particular perceive the role of mathematics in quan-
</p>
<p>tum mechanics as discouraging. Three steps serve to counter this impression or, in
</p>
<p>the optimum case, to avoid it altogether:
</p>
<p>First, we keep the mathematical level as simple as possible and share the usual
</p>
<p>quite nonchalant attitude of physicists toward mathematics. In particular, the first
</p>
<p>chapters go step by step, so that the initially diverse mathematics skills of the
</p>
<p>readers are gradually brought up to a common level.
</p>
<p>In addition, we use very simple models, toy models so to speak, especially in the
</p>
<p>first part of the book, in order to treat the main physical ideas without becoming
</p>
<p>involved in complicated mathematical questions. Of course, these models are only
</p>
<p>rough descriptions of actual physical situations. But they manage with relatively
</p>
<p>simple mathematics, do not require approximation methods or numerics, and yet
</p>
<p>still permit essential insights into the fundamentals of quantum mechanics.4 Only in
</p>
<p>Volume 2, more realistic models are applied, and this is reflected occasionally in a
</p>
<p>somewhat more demanding formal effort.
</p>
<p>The third measure involves exercises and some support from the Appendix. At
</p>
<p>the end of almost every chapter, there is a variety of exercises, some of them
</p>
<p>dealing with advanced topics. They invite the reader to work with the material in
</p>
<p>2This applies at least to physicists; for as Einstein remarked: &lsquo;But there is another reason for the
</p>
<p>high repute of mathematics: it is mathematics that offers the exact natural sciences a certain
</p>
<p>measure of security which, without mathematics, they could not attain.&rsquo; To give a layman without
</p>
<p>mathematical training an understanding of quantum mechanics, one will (or must) rely instead on
</p>
<p>math-free approaches.
3Without appropriate formal considerations, it is impossible to understand, for example, how to
</p>
<p>motivate the replacement of a physical measurement variable by a Hermitian operator.
4We could instead also make use of the large reservoir of historically important experiments. But
</p>
<p>their mathematical formulation is in general more complex, and since in the frame of our
</p>
<p>considerations they do not lead to further-reaching conclusions than our &lsquo;toy models,&rsquo; we restrict
</p>
<p>ourselves to the latter for clarity and brevity.
</p>
<p>Introduction xix</p>
<p/>
</div>
<div class="page"><p/>
<p>order to better assimilate and more clearly grasp it, as well as of course to train the
</p>
<p>necessary formal skills.5
</p>
<p>The learning aids in the Appendix include chapters with some basic mathe-
</p>
<p>matical and physical background information; this allows the reader to refresh
</p>
<p>&lsquo;passive&rsquo; knowledge without the need to refer to other sources or to become
</p>
<p>involved with new notations.
</p>
<p>Moreover, the no doubt unusually extensive Appendix contains the solutions to
</p>
<p>many of the exercises and, in addition, some chapters in which further-reaching
</p>
<p>questions and issues are discussed; although these are very interesting in them-
</p>
<p>selves, their treatment would far exceed the framework of a lecture course.
</p>
<p>The footnotes with a more associative character can be skipped on a first reading.
</p>
<p>A note on the term &lsquo;particle&rsquo;: Its meaning is rather vague in physics. On the one
</p>
<p>hand, it denotes &lsquo;something solid, not wavelike&rsquo;; on the other hand &lsquo;something
</p>
<p>small&rsquo;, ranging from the elementary particles as structureless building blocks of
</p>
<p>matter, to objects which themselves are composed of constituent &lsquo;particles&rsquo; like the
</p>
<p>a particle and other atomic nuclei or even macroscopic particles like sand grains. In
</p>
<p>quantum mechanics, where indeed it is often not even clear whether a particular
</p>
<p>object has mainly particle or mainly wave character, the careless use of the term
</p>
<p>may cause confusion and communication problems.
</p>
<p>Accordingly, several terms which go beyond &lsquo;wave&rsquo; or &lsquo;particle&rsquo; have been
</p>
<p>suggested, such as quantal particle, wavical, wavicle, quantum object, quanton.
</p>
<p>Throughout this book, we will use the term &lsquo;quantum object,&rsquo; unless there are
</p>
<p>traditionally established terms such as &lsquo;identical particles&rsquo; or &lsquo;elementary particles.&rsquo;
</p>
<p>The consistent use of &lsquo;quantum object&rsquo; instead of &lsquo;particle&rsquo; may perhaps seem
</p>
<p>somewhat pedantic, but we hope that it will help to ensure that fewer false images
</p>
<p>stick in the minds of readers; it is for this reason that this term is also found in many
</p>
<p>high-school textbooks.
</p>
<p>Quantum mechanics is a fundamental theory of physics, which has given rise to
</p>
<p>countless applications. But it also extends deep into areas such as philosophy and
</p>
<p>epistemology and leads to thinking about &lsquo;what holds the world together at its core&rsquo;;
</p>
<p>in short, it is also an intellectual adventure. The fascinating thing is that the more
</p>
<p>one becomes acquainted with quantum mechanics, the more one realizes how
</p>
<p>simple many of its central ideas really are.6 It would be pleasing if Quantum
</p>
<p>Mechanics for Pedestrians could help to reveal this truth.
</p>
<p>5
&lsquo;It is a great support to studying, at least for me, to grasp everything that one reads so clearly that
</p>
<p>one can apply it oneself, or even make additions to it. One is then inclined to believe in the end that
</p>
<p>one could have invented everything himself, and that is encouraging.&rsquo; Georg Christoph
</p>
<p>Lichtenberg, Scrap Books, Vol. J (1855).
6
&lsquo;The less we know about something, the more complicated it is, and the more we know about it,
</p>
<p>the easier it is. This is the simple truth about all the complexities.&rsquo; Egon Friedell, in
</p>
<p>Kulturgeschichte der Neuzeit; Kulturgeschichte Agyptens und des alten Orients (Cultural history
</p>
<p>of modern times; the cultural history of Egypt and the ancient Near East).
</p>
<p>xx Introduction</p>
<p/>
</div>
<div class="page"><p/>
<p>Let us close with a remark by Richard Feynman which holds true not only for
</p>
<p>physics in general, but even more for quantum mechanics: &lsquo;Physics is like sex:
</p>
<p>Sure, it may give some practical results, but that&rsquo;s not why we do it.&rsquo;
</p>
<p>Introduction xxi</p>
<p/>
</div>
<div class="page"><p/>
<p>Overview of Volume 1
</p>
<p>In the following 14 chapters, we want to work out the fundamental structure of
</p>
<p>quantum mechanics, videlicet on the basis of a few simple models. The use of these
</p>
<p>&lsquo;toy systems&rsquo; has two advantages.
</p>
<p>First, their simplicity allows us to identify the essential mechanisms of quantum
</p>
<p>mechanics without getting lost in complex mathematical considerations. These
</p>
<p>mechanisms, which we summarize in Chap. 14 in the form of postulates, can
</p>
<p>nevertheless be formulated in a rather general manner.
</p>
<p>Second, we can emphasize the essential ideas very quickly in this manner, so
</p>
<p>that we can treat and understand current topics quite soon along the trail.
</p>
<p>xxiii</p>
<p/>
</div>
<div class="page"><p/>
<p>Part I
</p>
<p>Fundamentals</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 1
</p>
<p>Towards the Schr&ouml;dinger Equation
</p>
<p>We construct an equation that is valid for matter in the nonrelativistic domain, but also
</p>
<p>allows for wave-like solutions. This is the Schr&ouml;dinger equation; it describes the dynamics
</p>
<p>of a quantum system by means of the time evolution of the wavefunction.
</p>
<p>Many different paths lead to the goal of this chapter, the Schr&ouml;dinger equation (SEq).
</p>
<p>We choose a traditional one, in which wave properties and the relationship between
</p>
<p>energy and momentum are the defining elements. Another approach (quantum hop-
</p>
<p>ping) can be found in Appendix J, Vol. 1. Certainly that approach is more uncon-
</p>
<p>ventional, but on the other hand, it makes the basic physical principles more clearly
</p>
<p>manifest. Of course, the two approaches both lead to the same result.
</p>
<p>After a few words about the construction of new theories, we consider solutions
</p>
<p>of the classical wave equation. It will turn out that the wave equation is not suitable
</p>
<p>for describing quantum-mechanical phenomena. But we learn in this way how to
</p>
<p>construct the &lsquo;right&rsquo; equation, i.e. the Schr&ouml;dinger equation. We restrict our consid-
</p>
<p>erations to sufficiently low velocities so that we can ignore relativistic effects.1
</p>
<p>1.1 How to Find a New Theory
</p>
<p>Classical mechanics cannot explain a goodly number of experimental results, such as
</p>
<p>the interference of particles (two-slit experiment with electrons), or the quantization
</p>
<p>of angular momentum, energy etc. (Stern-Gerlach experiment, atomic energy levels).
</p>
<p>A new theory is needed&mdash;but how to construct it, how do we find the adequate new
</p>
<p>physical concepts and the appropriate mathematical formalism?
</p>
<p>1Relativistic effects are explicitly considered inAppendixU,Vol. 1 (relativistic quantummechanics)
</p>
<p>and Appendix W, Vol. 2 (quantum field theory).
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4_1
</p>
<p>3</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_1&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_1&amp;domain=pdf</a></div>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4_1">https://doi.org/10.1007/978-3-030-00464-4_1</a></div>
</div>
<div class="page"><p/>
<p>4 1 Towards the Schr&ouml;dinger Equation
</p>
<p>The answer is: There is no clearly-prescribed recipe, no deductive or inductive
</p>
<p>&lsquo;royal road&rsquo;. To formulate a new theory requires creativity or, in simpler terms,
</p>
<p>something like &lsquo;intelligent guessing&rsquo;.2 Of course, there are experimental and
</p>
<p>theoretical frameworks that limit the arbitrariness of guessing and identify certain
</p>
<p>directions. Despite this, however, it is always necessary to think of something new
</p>
<p>which does not exist in the old system&mdash;or rather, cannot and must not exist in it.
</p>
<p>The transition from Newtonian to relativistic dynamics requires as a new element
</p>
<p>the hypothesis that the speed of light must have the same value in all inertial frames.
</p>
<p>This element does not exist in the old theory&mdash;on the contrary, it contradicts it and
</p>
<p>hence cannot be inferred from it.
</p>
<p>In the case of quantum mechanics (QM), there is the aggravating circumstance
</p>
<p>that we have no sensory experience of the microscopic world3 which is the actual
</p>
<p>regime of quantum mechanics. More than in other areas of physics, which are closer
</p>
<p>to everyday life and thus more intuitive,4 we need to rely on physical or formal
</p>
<p>analogies,5 we have to trust the models and mathematical considerations, as long as
</p>
<p>they correctly describe the outcome of experiments, even if they are not in accord
</p>
<p>with our everyday experience. This is often neither easy nor familiar6&mdash;in quantum
</p>
<p>mechanics particularly, because the meaning of some terms is not entirely clear. In
</p>
<p>fact, quantum mechanics leads us to the roots of our knowledge and understanding
</p>
<p>of the world, and this is why in relation to certain questions it is sometimes called
</p>
<p>&lsquo;experimental philosophy&rsquo;.
</p>
<p>In short, we cannot derive quantum mechanics strictly from classical mechanics
</p>
<p>or any other classical theory7; new formulations must be found and stand the test
</p>
<p>of experiments. With all these caveats and preliminary remarks, we will now start
</p>
<p>along our path to quantum mechanics.8
</p>
<p>2How difficult this can be is shown e.g. by the discussion about quantum gravity. For dozens of
</p>
<p>years, there have been attempts to merge the two basic realms of quantum theory and general
</p>
<p>relativity theory&mdash;so far (2018) without tangible results.
3Evolution has made us (more or less) fit for the demands of our everyday life&mdash;and microscopic
</p>
<p>phenomena simply do not belong to that everyday world. This fact, among others, complicates the
</p>
<p>teaching of quantum mechanics considerably.
4Insofar as e.g. electrodynamics or thermodynamics are intuitive...
5In the case of quantum mechanics, a physical analogy would be for example the transition from
</p>
<p>geometrical optics (= classical mechanics) to wave optics (= quantum mechanics). If one prefers
</p>
<p>to proceed abstractly, one can for instance replace the Poisson brackets of classical mechanics
</p>
<p>by commutators of corresponding operators&mdash;whereby at this point it naturally remains unclear
</p>
<p>without further information why one should entertain such an idea.
6This also applies e.g. to the special theory of relativity with its &lsquo;paradoxes&rsquo;, which contradict our
</p>
<p>everyday experience.
7This holds true in a similar way for all fundamental theories. For instance, Newtonian mechanics
</p>
<p>cannot be inferred strictly from an older theory; say, Aristotle&rsquo;s theory of motion. In the frame
</p>
<p>of classical mechanics, Newton&rsquo;s axioms are principles which are not derivable, but rather are
</p>
<p>postulated without proof.
8&ldquo;A journey of a thousand miles begins with a single step&rdquo;. (Lao Tzu).</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 The Classical Wave Equation and the Schr&ouml;dinger Equation 5
</p>
<p>1.2 The Classical Wave Equation and the Schr&ouml;dinger
</p>
<p>Equation
</p>
<p>This approach to the Schr&ouml;dinger equation is based on analogies, where the math-
</p>
<p>ematical formulation in terms of differential equations9 plays a central role. In par-
</p>
<p>ticular, we take as a basis the physical principle of linearity and in addition the
</p>
<p>non-relativistic relation between energy and momentum, i.e.
</p>
<p>E =
p2
</p>
<p>2m
. (1.1)
</p>
<p>By means of the de Broglie relations10
</p>
<p>E =  and p = k, (1.2)
</p>
<p>Equation (1.1) may be rewritten and yields the dispersion relation11:
</p>
<p> =

2
</p>
<p>2m
k2. (1.3)
</p>
<p>In the following, we will examine special solutions of differential equations, namely
</p>
<p>plane waves, and check whether their wavenumber k and frequency  satisfy the
</p>
<p>dispersion relation (1.3).
</p>
<p>A remark on the constants k and : They are related to the wavelength  and the
</p>
<p>frequency  by k = 2/ and  = 2. In quantum mechanics (and in other areas
</p>
<p>of physics), one hardly ever has to deal with  and , but almost exclusively with k
</p>
<p>and . This may be the reason that in physics,  is usually called &lsquo;the frequency&rsquo;
</p>
<p>(and not the angular frequency).
</p>
<p>1.2.1 From the Wave Equation to the Dispersion Relation
</p>
<p>As a result of interference phenomena, the double-slit experiment and other experi-
</p>
<p>ments suggest that the electron, to put it rather vaguely, is &lsquo;somehow a kind of wave&rsquo;.
</p>
<p>9Some basic facts about differential equations can be found in AppendixE, Vol. 1.
10The symbol h was introduced by Planck in 1900 as an auxiliary variable (&lsquo;Hilfsvariable&rsquo; in
</p>
<p>German, hence the letter h) in the context of his work on the black body spectrum. The abbreviation
</p>
<p> for h
2
</p>
<p>was probably used for the first time in 1926 by P.A.M. Dirac. In terms of frequency /
</p>
<p>wavelength , the de Broglie relations are E = h and p = h

. In general, the symmetrical form
</p>
<p>(1.2) is preferred.
11The term &lsquo;dispersion relation&rsquo; means in general the relationship between  and k or between
</p>
<p>E and p (it is therefore also called the energy-momentum relation). Dispersion denotes the depen-
</p>
<p>dence of the velocity of propagation of a wave on its wavelength or frequency, which generally
</p>
<p>leads to the fact that a wave packet made up of different wavelengths diverges (disperses) over time.</p>
<p/>
</div>
<div class="page"><p/>
<p>6 1 Towards the Schr&ouml;dinger Equation
</p>
<p>Now we have learned in mechanics and electrodynamics that the classical wave
</p>
<p>equation
</p>
<p>&part;2 (r, t)
</p>
<p>&part;t2
= c2
</p>
<p>(
</p>
<p>&part;2 (r, t)
</p>
<p>&part;x2
+
</p>
<p>&part;2 (r, t)
</p>
<p>&part;y2
+
</p>
<p>&part;2 (r, t)
</p>
<p>&part;z2
</p>
<p>)
</p>
<p>= c2&nabla;2 (r, t) (1.4)
</p>
<p>describes many kinds of waves (acoustic, elastic, light waves, etc.).  contains the
</p>
<p>amplitude and phase of the wave; c is its velocity of propagation, assumed to be
</p>
<p>constant.12 It seems obvious to first check this equation to see if it can explain
</p>
<p>phenomena such as particle interference and so on. To keep the argument as simple
</p>
<p>as possible, we start from the one-dimensional equation:
</p>
<p>&part;2 (x, t)
</p>
<p>&part;t2
= c2
</p>
<p>&part;2
</p>
<p>&part;x2
 (x, t) . (1.5)
</p>
<p>The results thus obtained can be readily generalized to three dimensions.
</p>
<p>In the following, wewill examine the question ofwhether or not thewave equation
</p>
<p>can describe the behavior of electrons. Though the answer will be &lsquo;no&rsquo;, we will
</p>
<p>describe the path to this answer in a quite detailed way because it shows, despite
</p>
<p>the negative result, how one can guess or construct the &lsquo;right&rsquo; equation, namely, the
</p>
<p>Schr&ouml;dinger equation.
</p>
<p>But first, we would like to point out an important property of the wave equation:
</p>
<p>it is linear&mdash;the unknown function  occurs only to the first power and not with
</p>
<p>other exponents such as 2 or 1/2. From this, it follows that when we know two
</p>
<p>solutions,1 and2, any arbitrary linear combination 1+2 is also a solution.
</p>
<p>In other words: The superposition principle holds.
</p>
<p>1.2.1.1 Separation of Variables
</p>
<p>Equation (1.5) has a solution, for example as the function
</p>
<p> (x, t) = 0e
i(kx&minus;t) (1.6)
</p>
<p>with the wave number k and frequency . How can we find such solutions? An
</p>
<p>important constructive approach is the so-called separation of variables which can
</p>
<p>be used for all linear partial differential equations. This ansatz, for obvious reasons
</p>
<p>also called product ansatz, reads
</p>
<p> (x, t) = f (t) &middot; g (x) (1.7)
</p>
<p>with yet-to-be-determined functions f (t) and g (x). Substitution into (1.5) leads,
</p>
<p>with the usual shorthand notation f &equiv;
d f
</p>
<p>dt
and g&prime; &equiv; dg
</p>
<p>dx
, to
</p>
<p>12The Laplacian &part;
2
</p>
<p>&part;x2
+ &part;
</p>
<p>2
</p>
<p>&part;y2
+ &part;
</p>
<p>2
</p>
<p>&part;z2
is written as &nabla;2, since it is the divergence (&nabla;&middot;) of the gradient,
</p>
<p>i.e. &nabla; (&nabla; f ) = &nabla;2 f (see AppendixD, Vol. 1).</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 The Classical Wave Equation and the Schr&ouml;dinger Equation 7
</p>
<p>f (t) &middot; g (x) = c2 f (t) &middot; g&prime;&prime; (x) ; (1.8)
</p>
<p>or, after division by f (t) &middot; g (x), to
</p>
<p>f (t)
</p>
<p>f (t)
= c2
</p>
<p>g&prime;&prime; (x)
</p>
<p>g (x)
. (1.9)
</p>
<p>At this point we can argue as follows: x and t each appear on only one side of the
</p>
<p>equation, respectively (i.e. they are separated). Since they are independent variables
</p>
<p>we can, for example, fix x and vary t independently of x . Then the equality in (1.9)
</p>
<p>can be satisfied for all x and t only if both sides are constant. To save extra typing,
</p>
<p>we call this constant 2 instead of simply . It follows that:
</p>
<p>f (t)
</p>
<p>f (t)
= 2;
</p>
<p>g&prime;&prime; (x)
</p>
<p>g (x)
=
</p>
<p>1
</p>
<p>c2
2;  &isin; C. (1.10)
</p>
<p>Solutions of these differential equations are the exponential functions
</p>
<p>f (t) &sim; e&plusmn;t ; g (x) &sim; e&plusmn;
1
c
x . (1.11)
</p>
<p>The range of values of the yet undetermined constant can be limited by the require-
</p>
<p>ment that physically meaningful solutions must remain bounded for all values of the
</p>
<p>variables.13 It follows that  cannot be real, because then we would have unlim-
</p>
<p>ited solutions for t or x &rarr; +&infin; or &minus;&infin;. Exactly the same is true if  is a complex
</p>
<p>number14 with a non-vanishing real part. In other words:must be purely imaginary,
</p>
<p> &isin; I &rarr;  = i;  &isin; R. (1.12)
</p>
<p>Since the term 
c
occurs in (1.11), we introduce the following abbreviation:
</p>
<p>k =

</p>
<p>c
. (1.13)
</p>
<p>Thus we obtain for the functions f and g
</p>
<p>f (t) &sim; e&plusmn;it ; g (x) &sim; e&plusmn;ikx ;  &isin; R (1.14)
</p>
<p>where, unless noted otherwise, we assume without loss of generality that k &gt; 0,
</p>
<p> &gt; 0 (in general, it follows that 2 = c2k2 from (1.9)). All combinations of the
</p>
<p>functions f and g, such as eit e&minus;ikx , e&minus;it eikx , etc., each multiplied by an arbitrary
</p>
<p>constant, are also solutions of the wave equation.
</p>
<p>13This is one of the advantages of physics as compared tomathematics: under certain circumstances,
</p>
<p>we can excludemathematically correct solutions due to physical requirements (see alsoAppendixE,
</p>
<p>Vol. 1).
14Some remarks on the subject of complex numbers are to be found in Appendix C, Vol. 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>8 1 Towards the Schr&ouml;dinger Equation
</p>
<p>1.2.1.2 Solutions of the Wave Equation; Dispersion Relation
</p>
<p>To summarize: the separation ansatz has provided us with solutions of the wave
</p>
<p>equation. Typically, they read for k &gt; 0,  &gt; 0:
</p>
<p>1 (x, t) = 01e
it eikx ; 2 (x, t) = 02e
</p>
<p>&minus;it eikx
</p>
<p>3 (x, t) = 03e
it e&minus;ikx ; 4 (x, t) = 04e
</p>
<p>&minus;it e&minus;ikx . (1.15)
</p>
<p>The constants 0i are arbitrary, since due to the linearity of the wave equation, a
</p>
<p>multiple of a solution is also a solution.
</p>
<p>Which physical situations are described by these solutions? Take, for example:
</p>
<p>2 (x, t) = 02e
&minus;it eikx = 02e
</p>
<p>i(kx&minus;t). (1.16)
</p>
<p>Due to k &gt; 0,  &gt; 0, this is a plane wave moving to the right, just as are &lowast;2 ,
</p>
<p>3 and 
&lowast;
3 (
</p>
<p>&lowast; means the complex conjugate). By contrast, 1 and 4 and their
</p>
<p>complex conjugates are plane waves moving to the left.15 For a clear and intuitive
</p>
<p>argumentation, see the exercises at the end of this chapter.
</p>
<p>Although a plane wave is quite a common construct in physics,16 the waves found
</p>
<p>here cannot describe the behavior of electrons. To see this, we use the de Broglie
</p>
<p>relations
</p>
<p>E =  and p = k. (1.17)
</p>
<p>From (1.13), it follows that:
</p>
<p> = kc, (1.18)
</p>
<p>and this gives with (1.17):
</p>
<p>E
</p>
<p>
= c
</p>
<p>p
</p>
<p>
or E = p &middot; c. (1.19)
</p>
<p>This relationship between energy and momentum cannot apply to our electron.
</p>
<p>We have restricted ourselves to the nonrelativistic domain, where according to
</p>
<p>15To determine whether a plane wave moves to the left or to the right, one can set the exponent
</p>
<p>equal to zero. For k &gt; 0 and  &gt; 0, one obtains for example for 1 or 4:
</p>
<p>v =
x
</p>
<p>t
= &minus;
</p>
<p>
</p>
<p>k
&lt; 0.
</p>
<p>Due to v &lt; 0, this is a left-moving plane wave. In contrast, for k &lt; 0 and  &gt; 0, we have
</p>
<p>right-moving plane waves.
16Actually it is &lsquo;unphysical&rsquo;, because it extends to infinity and on the average is equal everywhere,
</p>
<p>and therefore it is localized neither in time nor in space. But since the wave equation is linear, one
</p>
<p>can superimpose plane waves (partial solutions), e.g. in the form
&int;
</p>
<p>c (k) ei(kx&minus;t)dk. The resulting
</p>
<p>wave packets can be quite well localized, as will be seen in Chap.15, Vol. 2.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 The Classical Wave Equation and the Schr&ouml;dinger Equation 9
</p>
<p>E = p2/2m, a doubling of the momentum increases the energy by a factor of 4,
</p>
<p>while according to (1.19), only a factor of 2 is found. Apart from that, it is not clear
</p>
<p>what the value of the constant propagation velocity c of the waves should be.17 In
</p>
<p>short, with (1.18), we have deduced the wrong dispersion relation, namely  = kc
</p>
<p>and not the non-relativistic relation  = 
2
</p>
<p>2m
k2 which we formulated in (1.3). This
</p>
<p>means that the classical wave equation is not suitable for describing electrons&mdash;we
</p>
<p>must look for a different approach.
</p>
<p>A remark about the three-dimensional wave equation (1.4): Its solutions are plane
</p>
<p>waves of the form
</p>
<p> (r, t) = 0e
i(kr&minus;t); k =
</p>
<p>(
</p>
<p>kx , ky, kz
)
</p>
<p>, ki &isin; R (1.20)
</p>
<p>with kr = kx x + ky y + kzz and 
2 = c2k2 = c2 |k|2 = c2k2. The wave vector k
</p>
<p>indicates the direction of wave propagation. In contrast to the one-dimensional wave,
</p>
<p>the components of k usually have arbitrary signs, so that the double sign&plusmn; in (1.14)
</p>
<p>does not appear here.
</p>
<p>1.2.2 From the Dispersion Relation to the Schr&ouml;dinger
</p>
<p>Equation
</p>
<p>We now take the opposite approach: We start with the desired dispersion relation
</p>
<p>and deduce from it a differential equation under the assumptions that plane waves
</p>
<p>are indeed solutions and that the differential equation is linear, i.e. schematically:
</p>
<p>Wave equation =&rArr;
plane waves, linear
</p>
<p>&lsquo;wrong&rsquo; relation E = cp
</p>
<p>Schrodinger equation &lArr;=
plane waves, linear
</p>
<p>&lsquo;right&rsquo; relation E =
p2
</p>
<p>2m
.
</p>
<p>The energy of a classical force-free particle is given by
</p>
<p>E =
p2
</p>
<p>2m
. (1.21)
</p>
<p>With the de Broglie relations, we obtain the dispersion relation
</p>
<p> =
k2
</p>
<p>2m
. (1.22)
</p>
<p>Now we look for an equation whose solutions are plane waves, e.g. of the
</p>
<p>form  = 0e
i(kx&minus;t), with the dispersion relation (1.22). To achieve this, we
</p>
<p>17Likewise, (1.19) does not apply to an electron in the relativistic domain, since E &sim; p holds only
</p>
<p>for objects with zero rest mass.</p>
<p/>
</div>
<div class="page"><p/>
<p>10 1 Towards the Schr&ouml;dinger Equation
</p>
<p>differentiate the plane wave once with respect to t and twice with respect to x (we
</p>
<p>use the abbreviations &part;x :=
&part;
&part;x
, &part;xx = &part;
</p>
<p>2
x =
</p>
<p>&part;2
</p>
<p>&part;x2
, etc.):
</p>
<p>&part;t = &minus;i0e
i(kx&minus;t)
</p>
<p>&part;2x = &minus;k
20e
</p>
<p>i(kx&minus;t).
(1.23)
</p>
<p>We insert these terms into (1.22) and find:
</p>
<p> = 1
&minus;i
</p>
<p>&part;t =

</p>
<p>2m
k2 = 
</p>
<p>2m
</p>
<p>(
</p>
<p>&minus; 1

&part;2x
</p>
<p>)
</p>
<p>&rarr; i&part;t = &minus;

</p>
<p>2m
&part;2x.
</p>
<p>(1.24)
</p>
<p>Conventionally, one multiplies by  to finally obtain:
</p>
<p>i&part;t = &minus;

2
</p>
<p>2m
&part;2x, (1.25)
</p>
<p>or, in the three-dimensional case,
</p>
<p>i
&part;
</p>
<p>&part;t
 = &minus;
</p>
<p>
2
</p>
<p>2m
&nabla;2. (1.26)
</p>
<p>This is the free time-dependent Schr&ouml;dinger equation. As the name suggests, it
</p>
<p>applies to an interaction-free quantum object.18 For motions in a field with potential
</p>
<p>energy V , we have (in analogy to the classical energy E =
p2
</p>
<p>2m
+ V ) the (general)
</p>
<p>time-dependent Schr&ouml;dinger equation
</p>
<p>i
&part;
</p>
<p>&part;t
 = &minus;
</p>
<p>
2
</p>
<p>2m
&nabla;2 + V. (1.27)
</p>
<p>Written out in full detail, it reads:
</p>
<p>i
&part;
</p>
<p>&part;t
 (r, t) = &minus;
</p>
<p>
2
</p>
<p>2m
&nabla;2 (r, t)+ V (r, t) (r, t) . (1.28)
</p>
<p>It is far from self-evident that the potential19 V should be introduced into the
</p>
<p>equation in this manner and not in some other way. It is rather, like the whole
</p>
<p>&lsquo;derivation&rsquo; of (1.28), a reasonable attempt or a bold step which still has to prove
</p>
<p>itself, as described above.
</p>
<p>18We repeat a remark from the Introduction: For the sake of greater clarity we will use in quantum
</p>
<p>mechanics the term &lsquo;quantum object&rsquo; instead of &lsquo;particle&rsquo;, unless there are traditionally preferred
</p>
<p>terms such as &lsquo;identical particles&rsquo;.
19Although it is the potential energy V , this term is commonly referred to as the potential. One
</p>
<p>should note that the two concepts differ by a factor (e.g. in electrostatics, by the electric charge).</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 The Classical Wave Equation and the Schr&ouml;dinger Equation 11
</p>
<p>We note that the SEq is linear in : From two solutions, 1 and 2, any linear
</p>
<p>combination 11 + 22 with i &isin; C is also a solution (see exercises). This is a
</p>
<p>crucial property for quantum mechanics, as we shall later see again and again.
</p>
<p>Two remarks concerning  (r, t), which is called the wavefunction,20 state func-
</p>
<p>tion or, especially in older texts, the psi function ( function), are in order: The
</p>
<p>first is rather technical and almost self-evident. In general, only r and t are given
</p>
<p>as arguments of the wave function. But since these two variables have the physical
</p>
<p>units meter and second (we use the International System of Units, SI), they do not
</p>
<p>occur alone in the wave function, but always in combination with quantities having
</p>
<p>the inverse units. In the solutions, we always use kr and t , where k has the unit
</p>
<p>m&minus;1 and  the unit s&minus;1.
</p>
<p>The second point is more substantial and far less self-evident. While the solution
</p>
<p>of the classical wave equation (1.5) has a direct and very clear physical meaning,
</p>
<p>namely the description of the properties of the observed wave (amplitude, phase,
</p>
<p>etc.), this is not the case for the wavefunction. Its magnitude | (r, t)| specifies an
</p>
<p>amplitude&mdash;but an amplitude of what? What is it that here makes up the &lsquo;waves&rsquo;
</p>
<p>(remember that we are discussing electrons)? This was never referred to concretely
</p>
<p>in the derivation&mdash;it was never necessary to do so. It is indeed the case that the
</p>
<p>wavefunction has no direct physical meaning (at least not in everyday terms).21
</p>
<p>Perhaps it can best be understood as a complex-valued field of possibilities. In fact,
</p>
<p>one can extract from the wavefunction the relevant physical data, with an often
</p>
<p>impressive accuracy, without the need of a clear idea of what it specifically means.
</p>
<p>This situation (one operates with something, not really knowing what it is) produces
</p>
<p>unpleasant doubts, uncertainties and sometimes learning difficulties, particularly on
</p>
<p>first contact with quantum mechanics. But it is the state of our knowledge&mdash;the
</p>
<p>wavefunction as a key component of quantum mechanics has no direct physical
</p>
<p>meaning&mdash;that is how things are.22
</p>
<p>20Despite its name, the wavefunction is a solution of the Schr&ouml;dinger equation and not of the wave
</p>
<p>equation.
21This is one of the major problems in the teaching of quantum mechanics e.g. in high schools.
22Notwithstanding its somewhat enigmatic character (or perhaps because of it?), the wavefunction
</p>
<p>appears even in thrillers. An example: Harry smiled. &ldquo;Good. In classical physics, an electron can be
</p>
<p>said to have a certain position. But in quantum mechanics, no. The wavefunction defines an area,
</p>
<p>say, of probability. An analogy might be that if a highly contagious disease turns up in a segment
</p>
<p>of the population, the disease control center gets right on it and tries to work out the probability of
</p>
<p>its recurrence in certain areas. The wavefunction isn&rsquo;t an entity, it&rsquo;s nothing in itself, it describes
</p>
<p>probability.&rdquo; Harry leaned closer as if he were divulging a sexy secret and went on: &ldquo;So what we&rsquo;ve
</p>
<p>got, then, is the probability of an electron&rsquo;s being in a certain place at a certain moment. Only when
</p>
<p>we&rsquo;re measuring it can we know not only where it is but if it is. So the cat...&rdquo; Martha Grimes, in
</p>
<p>The Old Wine Shades.</p>
<p/>
</div>
<div class="page"><p/>
<p>12 1 Towards the Schr&ouml;dinger Equation
</p>
<p>1.3 Exercises
</p>
<p>1. Consider the relativistic energy-momentum relation
</p>
<p>E2 = m20c
4 + p2c2. (1.29)
</p>
<p>Show that in the nonrelativistic limit v  c, it gives approximately (up to an
</p>
<p>additive positive constant)
</p>
<p>E =
p2
</p>
<p>2m0
. (1.30)
</p>
<p>2. Show that the relation E = p &middot; c (c is the speed of light) holds only for objects
</p>
<p>with zero rest mass.
</p>
<p>3. A (relativistic) object has zero rest mass. Show that in this case the dispersion
</p>
<p>relation reads 2 = c2k2.
</p>
<p>4. Let k &lt; 0,  &gt; 0. Is ei(kx&minus;t) a right- or left-moving plane wave?
</p>
<p>5. Solve the three-dimensional wave equation
</p>
<p>&part;2 (r, t)
</p>
<p>&part;t2
= c2&nabla;2 (r, t) (1.31)
</p>
<p>explicitly by using the separation of variables.
</p>
<p>6. Given the three-dimensional wave equation for a vector field A (r, t),
</p>
<p>&part;2A (r, t)
</p>
<p>&part;t2
= c2&nabla;2A (r, t) . (1.32)
</p>
<p>(a) What is a solution in the form of a plane wave?
</p>
<p>(b) Which condition must A0 satisfy if A is (a) a longitudinal, (b) a transverse
</p>
<p>wave?
</p>
<p>7. Given are the SEq
</p>
<p>i
&part;
</p>
<p>&part;t
 (r, t) = &minus;
</p>
<p>
2
</p>
<p>2m
&nabla;2 (r, t)+ V (r, t) (r, t) (1.33)
</p>
<p>and two solutions 1 (r, t) and 2 (r, t). Show explicitly that any linear combi-
</p>
<p>nation of these solutions is also a solution.
</p>
<p>8. The wavefunction of a quantum object of mass m is given by
</p>
<p> (x, t) = 0 exp
</p>
<p>(
</p>
<p>&minus;
x2
</p>
<p>2b2
&minus; i
</p>
<p>
</p>
<p>2mb2
t
</p>
<p>)
</p>
<p>, (1.34)
</p>
<p>where b is a fixed length. Determine the potential energy V (x) of the quantum
</p>
<p>object.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.3 Exercises 13
</p>
<p>9. Given the plane waves
</p>
<p>1 (x, t) = 01e
&plusmn;i(kx&minus;t); 2 (x, t) = 02e
</p>
<p>&plusmn;i(kx+t); k, &gt; 0; 0i &isin; R.
</p>
<p>(1.35)
</p>
<p>Explain in a visual way that 1 (x, t) is a right- and 2 (x, t) is a left-moving
</p>
<p>plane wave.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 2
</p>
<p>Polarization
</p>
<p>In this chapter, we make the transition from classical mechanics to quantum mechanics
</p>
<p>by considering light polarization. This leads us directly to two key concepts of quantum
</p>
<p>mechanics, vector space and probability. For the first time, we encounter the problem of
</p>
<p>measurement in quantum mechanics.
</p>
<p>The approach to quantum mechanics in the preceding chapter is based on the descrip-
</p>
<p>tion of the time evolution of a state by means of a differential equation. In this chapter,
</p>
<p>we choose a different approach. We consider (for now) not the Schr&ouml;dinger equation
</p>
<p>or another description of the space-time behavior, but instead the emphasis is now
</p>
<p>on how we can define states (for the moment, time-independent states).
</p>
<p>Again we start from classical formulations which we &lsquo;pep up&rsquo; quantum mechan-
</p>
<p>ically. For this purpose we first show that under certain circumstances, we can treat
</p>
<p>electromagnetic waves as if they propagate in a two-dimensional complex vector
</p>
<p>space.1 As is known from optics, we can express the intensities of light waves as
</p>
<p>the absolute squares of their amplitudes. After reviewing classical formulations, we
</p>
<p>extend these ideas to the quantum-mechanical case by means of a reinterpretation
</p>
<p>which is, while not mandatory, very plausible. In this interpretation, the amplitudes
</p>
<p>do not lead to intensities, but instead to probabilities. At this point we will see for
</p>
<p>the first time that the concept of measurement in quantum mechanics is not as trivial
</p>
<p>as it is in classical mechanics.
</p>
<p>1If this term is not familiar (or forgotten): The basic concepts are summarized in Appendix G,
</p>
<p>Vol. 1. In addition, we will return to this topic in Chap. 4. For the moment, it is enough to know that
</p>
<p>e.g. the set of all vectors
</p>
<p>(
</p>
<p>a1
a2
</p>
<p>)
</p>
<p>with ai &isin; C forms a two-dimensional complex vector space. An
</p>
<p>important property is that any linear combination of two vectors is itself a valid vector in this space.
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4_2
</p>
<p>15</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_2&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_2&amp;domain=pdf</a></div>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4_2">https://doi.org/10.1007/978-3-030-00464-4_2</a></div>
</div>
<div class="page"><p/>
<p>16 2 Polarization
</p>
<p>We will base our discussion on the polarization of light, which should be familiar
</p>
<p>from lectures and lab courses.2
</p>
<p>2.1 Light as Waves
</p>
<p>We first derive the &lsquo;minimal description&rsquo; of a classical electromagnetic wave. Using
</p>
<p>the common definition of linear and circular polarization, we see that we can describe
</p>
<p>these &lsquo;typical waves&rsquo; in a two-dimensional complex vector space.
</p>
<p>2.1.1 The Typical Shape of an Electromagnetic Wave
</p>
<p>We start with the description of an electromagnetic plane wave as3
</p>
<p>E (r, t) = E0ei(kr&minus;t); B =
k &times; E
</p>
<p>c
(2.1)
</p>
<p>with k &middot; E0 = 0 (a transverse wave, as follows from the first Maxwell equation. In a
charge-free region of space, it states that &nabla;E = 0); 2 = c2k2 (dispersion relation
for zero rest mass), with E0 &isin; C3. In the following, we restrict our considerations to
the electric field4 E; the magnetic field can be calculated from E by using (2.1).
</p>
<p>It holds quite generally that the description of a plane wave can be made consid-
</p>
<p>erably simpler and more transparent by means of a suitable choice of the coordinate
</p>
<p>system, without losing any of its physical significance. We choose the new z axis
</p>
<p>to point in the direction of propagation of the wave, i.e. the k direction&mdash;in other
</p>
<p>words, k = (0, 0, k)&mdash;and obtain
</p>
<p>E (r, t) =
(
</p>
<p>E0x , E0y, 0
)
</p>
<p>ei(kz&minus;t). (2.2)
</p>
<p>The z component disappears due to the transverse nature of the wave (see exercises
</p>
<p>at the end of this chapter).
</p>
<p>2From the theory of electromagnetism, we know that light is a transverse wave, i.e. that its elec-
</p>
<p>tric field oscillates perpendicular to its direction of propagation. The polarization describes the
</p>
<p>orientation of this oscillation.
</p>
<p>Polarization is often regarded as an esoteric and specialized topic, possibly because we cannot
</p>
<p>see directly whether light is polarized. However, it is a ubiquitous phenomenon in our environment&mdash;
</p>
<p>natural light is almost always polarized, at least partially. Many animals, such as bees or other insects,
</p>
<p>take advantage of this; they can detect and analyze light polarization. In our daily life, polarization
</p>
<p>is used e.g. in polarizing filters for cameras or some sunglasses. Moreover, the fundamentals of the
</p>
<p>formal treatment of polarization are also very simple, as we shall see below.
3We note that a real light wave is only approximately described by a plane wave, since that would
</p>
<p>have the intensity at all points and all times. However, this approximate description is common for
</p>
<p>several reasons, and suffices for our purposes here.
4In this connection also called the light vector.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.1 Light as Waves 17
</p>
<p>The amplitude can be written quite generally as
</p>
<p>E0x = ei |E0x | ; E0y = ei

</p>
<p>E0y

</p>
<p> ; , &isin; R. (2.3)
</p>
<p>It follows that
</p>
<p>E (r, t) =
(
</p>
<p>|E0x | ,

</p>
<p>E0y

</p>
<p> ei(&minus;), 0
)
</p>
<p>ei(kz&minus;t+). (2.4)
</p>
<p>We can now put  = 0 without loss of generality, since the last equation shows that
any value of  can be compensated by a suitable choice of the zero of time. In order
</p>
<p>to avoid confusion, we rename  as . Then the typical form of an electromagnetic
</p>
<p>wave is given by5:
</p>
<p>E (r, t) =
(
</p>
<p>|E0x | ,

</p>
<p>E0y

</p>
<p> ei, 0
)
</p>
<p>ei(kz&minus;t). (2.5)
</p>
<p>2.1.2 Linear and Circular Polarization
</p>
<p>For purposes of illustration (we will include some figures in the following), we
</p>
<p>consider in this subsection only the real part of the wave function (the imaginary part
</p>
<p>alone would be just as suitable):
</p>
<p>Ex (r, t) = |E0x | cos (kz &minus; t) ; Ey (r, t) =

</p>
<p>E0y

</p>
<p> cos (kz &minus; t + ) . (2.6)
</p>
<p> &isin; R can assume all possible values. One can, however, single out two basic
cases, namely  = 0 (linear polarization) and  = &plusmn;/2 (elliptical or circular
polarization).
</p>
<p>2.1.2.1 Linear Polarization
</p>
<p>With the choice  = 0, we have
</p>
<p>Ex (r, t) = |E0x | cos (kz &minus; t) ; Ey (r, t) =

</p>
<p>E0y

</p>
<p> cos (kz &minus; t) . (2.7)
</p>
<p>It follows immediately that
</p>
<p>Ey =

</p>
<p>E0y

</p>
<p>
</p>
<p>|E0x |
Ex . (2.8)
</p>
<p>This is a straight line on which the light vector oscillates back and forth&mdash;hence the
</p>
<p>name linear polarization, see Fig. 2.1.
</p>
<p>Basic types of this polarization are obtained by setting one component equal to
</p>
<p>zero:
</p>
<p>5The relative phase could of course be associated with the y component instead of the x component.</p>
<p/>
</div>
<div class="page"><p/>
<p>18 2 Polarization
</p>
<p>Fig. 2.1 Linear polarization.
</p>
<p>The z-axis points out of the
</p>
<p>image plane
</p>
<p>horizontally polarized:
</p>
<p>vertically polarized:
</p>
<p>Ex (r, t) = |E0x | cos (kz &minus; t) ; Ey =

</p>
<p>E0y

</p>
<p> = 0
Ex = |E0x | = 0; Ey (r, t) =
</p>
<p>
</p>
<p>E0y

</p>
<p> cos (kz &minus; t) .
(2.9)
</p>
<p>The names are self-explanatory. Due to the vector character of the electric field, it
</p>
<p>follows readily that any linearly-polarized wave can be written as a superposition of
</p>
<p>horizontally- and vertically-polarized waves.
</p>
<p>2.1.2.2 Elliptical and Circular Polarization
</p>
<p>In this case, we choose  = &plusmn;/2 and this means that
</p>
<p>Ex (r, t) = |E0x | cos (kz &minus; t)
Ey (r, t) =
</p>
<p>
</p>
<p>E0y

</p>
<p> cos (kz &minus; t &plusmn; /2) = 

</p>
<p>E0y

</p>
<p> sin (kz &minus; t) .
(2.10)
</p>
<p>It follows from this that
</p>
<p>(
</p>
<p>Ex
</p>
<p>|E0x |
</p>
<p>)2
</p>
<p>+
</p>
<p>(
</p>
<p>Ey

</p>
<p>E0y

</p>
<p>
</p>
<p>)2
</p>
<p>= 1. (2.11)
</p>
<p>The arrowhead of the light vector thus moves on an ellipse with semiaxes |E0x | and

</p>
<p>E0y

</p>
<p>&mdash;hence the name elliptical polarization; see Fig. 2.2. The direction of rotation
</p>
<p>can be determined by using
</p>
<p>tan  =
Ex
</p>
<p>Ey
= 
</p>
<p>
</p>
<p>E0y

</p>
<p>
</p>
<p>|E0x |
tan (kz &minus; t) =
</p>
<p>
</p>
<p>E0y

</p>
<p>
</p>
<p>|E0x |
tan (&plusmn;t  kz) (2.12)
</p>
<p>(most easily seen for fixed z).</p>
<p/>
</div>
<div class="page"><p/>
<p>2.1 Light as Waves 19
</p>
<p>Fig. 2.2 Elliptical
</p>
<p>polarization. The z-axis is
</p>
<p>directed out of the image
</p>
<p>plane
</p>
<p>In particular, for |E0x | =

</p>
<p>E0y

</p>
<p>, the ellipse becomes a circle and we have
</p>
<p>circularly-polarized light, i.e. right circularly-polarized light with the upper sign,
</p>
<p>left circularly-polarized light with the lower sign.6
</p>
<p>2.1.3 From Polarization to the Space of States
</p>
<p>In summary, we have in the complex representation (remember e&plusmn;i/2 = &plusmn;i) for the
linearly (horizontal h/vertical v) and circularly (right r /left l) polarized waves:
</p>
<p>Eh = (|A0x | , 0, 0) ei(kz&minus;t)
</p>
<p>Ev = (0, |B0x | , 0) ei(kz&minus;t)
</p>
<p>Er = (|C0x | , i |C0x | , 0) ei(kz&minus;t)
</p>
<p>El = (|C0x | ,&minus;i |C0x | , 0) ei(kz&minus;t). (2.13)
</p>
<p>So far, we have just repeated material that should be known from previous
</p>
<p>semesters. Now we turn to something that is (quite possibly) new. We begin by
</p>
<p>noting that the representation (2.13) is redundant and we can simplify it further.
</p>
<p>2.1.3.1 Simplifying the Notation
</p>
<p>To achieve this simplification, we have to restrict our world: it will consist exclu-
</p>
<p>sively of the waves given by (2.13). In particular, there is e.g. no other direction of
</p>
<p>propagation and no other wave number k. Then we can simplify as follows:
</p>
<p>1. The factor ei(kz&minus;t) occurs everywhere, so we can omit it.
</p>
<p>2. Since the third component is always zero, we suppress it. In other words, our little
</p>
<p>world is two dimensional.
</p>
<p>6In physical optics, right and left circular polarization is usually defined the other way around (optics
</p>
<p>convention).</p>
<p/>
</div>
<div class="page"><p/>
<p>20 2 Polarization
</p>
<p>3. The notation as a row vector was chosen for typographical convenience; the
</p>
<p>correct notation is as a column vector.7
</p>
<p>4. We fix the undetermined quantities |A0x | etc. in such a way that the respective
vector has length 1, and thus represents a unit vector. We can then build up a
</p>
<p>general vector by taking appropriate linear combinations of these unit vectors.
</p>
<p>In summary:
</p>
<p>(|A0x | , 0, 0) ei(kz&minus;t)
1.&rarr; (|A0x | , 0, 0)
</p>
<p>2.&rarr; (|A0x | , 0)
3.&rarr;
</p>
<p>(
</p>
<p>|A0x |
0
</p>
<p>)
</p>
<p>4.&rarr;
(
</p>
<p>1
</p>
<p>0
</p>
<p>) (2.14)
</p>
<p>and8
</p>
<p>(|C0x | , i |C0x | , 0) ei(kz&minus;t)
1.&rarr; (|C0x | , i |C0x | , 0)
</p>
<p>2.&rarr; (|C0x | , i |C0x |)
3.&rarr;
</p>
<p>(
</p>
<p>|C0x |
i |C0x |
</p>
<p>)
</p>
<p>4.&rarr; 1&radic;
2
</p>
<p>(
</p>
<p>1
</p>
<p>i
</p>
<p>)
</p>
<p>.
(2.15)
</p>
<p>In short, we go from a three-dimensional to a two-dimensional complex vector
</p>
<p>space which we call the state space. In this space, the states associated with the vectors
</p>
<p>( 2.13) are written as two-component vectors which depend neither on position nor on
</p>
<p>time. For a convenient shorthand notation comparable to Eh in (2.13), we introduce
</p>
<p>the notation |h, |v, |r and |l for light in the linear horizontal, linear vertical, right
circular and left circular polarized states, respectively. For these states we have the
</p>
<p>representation (denoted by the symbol &sim;=)
</p>
<p>|h &sim;=
(
</p>
<p>1
</p>
<p>0
</p>
<p>)
</p>
<p>; |v &sim;=
(
</p>
<p>0
</p>
<p>1
</p>
<p>)
</p>
<p>|r &sim;= 1&radic;
2
</p>
<p>(
</p>
<p>1
</p>
<p>i
</p>
<p>)
</p>
<p>; |l &sim;= 1&radic;
2
</p>
<p>(
</p>
<p>1
</p>
<p>&minus;i
</p>
<p>)
</p>
<p>.
</p>
<p>(2.16)
</p>
<p>We emphasize that in our &lsquo;small world&rsquo; this representation is completely equivalent
</p>
<p>to the representation in (2.13).
</p>
<p>A note concerning the symbol &sim;=: actually, the representation (2.16) is just one
of infinitely many possible ones, based on the fact that in (2.13), we identify the
</p>
<p>horizontal direction with the x axis. Of course, this is not to be taken for granted,
</p>
<p>since the y axis might just as well play the same role, given a corresponding ori-
</p>
<p>entation. Generally, one can start from any representation |h &sim;= 1&radic;
a2+b2
</p>
<p>(
</p>
<p>a
</p>
<p>b
</p>
<p>)
</p>
<p>and
</p>
<p>7In the following, we want to multiply vectors by matrices. In the usual notation, a matrix acts on
</p>
<p>a vector from the the left, which therefore&mdash;according to the usual rules of matrix multiplication&mdash;
</p>
<p>must be a column vector. See also Appendix F, Vol. 1, on linear algebra.
</p>
<p>8The length of the vector
</p>
<p>(
</p>
<p>1
</p>
<p>i
</p>
<p>)
</p>
<p>is given by
&radic;
</p>
<p>2; we explain the reasoning for this in Chap. 4.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.1 Light as Waves 21
</p>
<p>|v &sim;= 1&radic;
c2+d2
</p>
<p>(
</p>
<p>c
</p>
<p>d
</p>
<p>)
</p>
<p>with a&lowast;c + b&lowast;d = 0. We therefore identify specific representa-
</p>
<p>tions by the special symbol &sim;= and do not simply use the equals sign.9
The question may arise as to whether the representation (2.16) is not oversimpli-
</p>
<p>fied. In this context, we recall the following: In physics, the objective of the formal
</p>
<p>description is not to describe &lsquo;nature&rsquo; (whatever is meant by this term) directly but
</p>
<p>rather to find a model for a part of nature and to describe this model as accurately as
</p>
<p>possible. This is also reflected in the much-quoted &lsquo;accuracy&rsquo; of the natural sciences.
</p>
<p>It is not the description of nature which is exact, but at most the formal treatment of
</p>
<p>the model (if at all).10 Kepler&rsquo;s laws, for example, do not describe the conditions in
</p>
<p>the solar system exactly, as is well known: the planets influence each other, they are
</p>
<p>not point masses, there are moons and the solar wind, etc. Kepler&rsquo;s laws, however,
</p>
<p>are exact within the framework of the model &lsquo;point mass earth moves around point
</p>
<p>mass sun&rsquo;, and this model is correct and sufficiently precise for many applications.11
</p>
<p>In this sense, the description by models is not unique, but depends on the particular
</p>
<p>question being considered. The general rule is: as easy as possible, as elaborate as
</p>
<p>necessary.12 This is easily said, but of course it is not clear from the outset in all cases
</p>
<p>what it means in detail. In fact, it is the art in science to carve out meaningful, workable
</p>
<p>models from the &lsquo;jumble of reality&rsquo;, neither oversimplified nor overcomplicated.
</p>
<p>For the following considerations, our quite modest, simplistic representation
</p>
<p>(2.16) will be sufficient: We need no direction of propagation, no plane waves, no
</p>
<p>explicit time behavior and so on.
</p>
<p>2.1.3.2 Two Basic Systems
</p>
<p>With |h, |v and |r , |l we have two pairs of linearly-independent vectors and
therefore two basis systems for our two-dimensional vector space. They can be
</p>
<p>transformed into each other by
</p>
<p>9Different symbols are in use to denote representations; Flie&szlig;bach writes :=, for example. Apart
from that, many authors denote representations not by a special symbol, but by simply writing =.
10Also, the general mathematical modelling uses concepts that are implemented only approximately
</p>
<p>in reality. A time-honored example is Euclidean geometry with its points and lines, which strictly
</p>
<p>speaking do not exist anywhere in our real world. Yet no one doubts that Euclidean geometry is
</p>
<p>extremely useful for practical calculations. &ldquo;Although this may be seen as a paradox, all exact
</p>
<p>science is dominated by the idea of approximation&rdquo; (Bertrand Russell).
11Most theoretical results are based on approximations or numerical calculations and are in this
</p>
<p>sense not strictly precise. This naturally applies a fortiori to experimental results. Even though there
</p>
<p>are high-precision measurements with small relative errors of less than a part per billion, it has to
</p>
<p>be noted that each measurement is inaccurate. Nevertheless, one can estimate this inaccuracy in
</p>
<p>general quite precisely; keyword &lsquo;theory of errors&rsquo;.
12If several theories describe the same facts, one should prefer the simplest of them (this is the
</p>
<p>principle of parsimony in science, also called Occam&rsquo;s razor: &ldquo;entia non sunt multiplicanda praeter
</p>
<p>necessitatem&rdquo;).</p>
<p/>
</div>
<div class="page"><p/>
<p>22 2 Polarization
</p>
<p>|r = |h+i |v&radic;
2
</p>
<p>|l = |h&minus;i |v&radic;
2
</p>
<p>(2.17)
</p>
<p>and
</p>
<p>|h = |r+|l&radic;
2
</p>
<p>|v = |r&minus;|l
i
&radic;
</p>
<p>2
</p>
<p>(2.18)
</p>
<p>These relations hold independently of the representation, and this is why we write =
here, and not &sim;=. Mathematically, (2.17) and (2.18) are basis transformations; physi-
cally, these equations mean that we can consider linearly-polarized light as a super-
</p>
<p>position of right and left circularly-polarized light&mdash;and of course vice versa.
</p>
<p>2.1.3.3 Intensity and the Absolute Square Amplitude
</p>
<p>If we send right circularly-polarized light through an analyzer (linearly horizon-
</p>
<p>tal/vertical), then the relative intensity of horizontally and vertically polarized light
</p>
<p>is 1/2, respectively. Where can we find this factor 1/2 in the expression |r = |h+i |v&radic;
2
</p>
<p>?
</p>
<p>Clearly, we obtain the intensities (as usual) by calculating the squared sum of the
</p>
<p>coefficients (amplitudes), i.e.
</p>
<p>1
</p>
<p>2
=
</p>
<p>(
</p>
<p>1
&radic;
</p>
<p>2
</p>
<p>)2
</p>
<p>=

</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>i
&radic;
</p>
<p>2
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>2
</p>
<p>. (2.19)
</p>
<p>Next, we consider light whose polarization plane is rotated by . The rotation
</p>
<p>matrix, which is known to be given by
</p>
<p>(
</p>
<p>cos  &minus; sin 
sin  cos 
</p>
<p>)
</p>
<p>, transforms the state |h &sim;=
(
</p>
<p>1
</p>
<p>0
</p>
<p>)
</p>
<p>into the rotated state | &sim;=
(
</p>
<p>cos 
</p>
<p>sin 
</p>
<p>)
</p>
<p>.13 Hence we have | = cos  |h +
</p>
<p>sin  |v. The absolute square of the coefficient of |h is cos2 . This is known as the
Law of Malus14 and gives the relative intensity of the horizontally-polarized light.
</p>
<p>Thus we have recovered the known relationship between intensity and absolute
</p>
<p>squared amplitude: For |A = c1 |h + c2 |v, the (relative) intensity of e.g. |h is
given by |c1|2, whereby |c1|2 + |c2|2 = 1 has to hold. In other words: the state |A
must be normalized.
</p>
<p>13The active rotation (rotation of the vector by  counterclockwise) is given by
</p>
<p>(
</p>
<p>cos  &minus; sin 
sin  cos 
</p>
<p>)
</p>
<p>;
</p>
<p>the passive rotation (rotation of the coordinate system) by
</p>
<p>(
</p>
<p>cos  sin 
</p>
<p>&minus; sin  cos 
</p>
<p>)
</p>
<p>.
</p>
<p>14Perhaps familiar from school or undergraduate laboratory courses?</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Light as Photons 23
</p>
<p>2.2 Light as Photons
</p>
<p>The above considerations are independent of the intensity of the light&mdash;they apply
</p>
<p>to an intense laser beam as well as to the dimmest glow. But if we can turn down
</p>
<p>the intensity of a light source sufficiently far, we eventually encounter a situation
</p>
<p>where the light consists of a stream of single photons.15 Even then&mdash;and that is the
</p>
<p>crucial point&mdash;we assume that the above formulation remains valid. This is the above-
</p>
<p>mentioned jump from classical physics to quantum mechanics, which is not strictly
</p>
<p>derivable logically, but requires additional assumptions; there are in fact two of
</p>
<p>them. First, the existence of photons is assumed, which we take as an experimentally
</p>
<p>assured fact. Secondly, there is the assumption that expressions such as (2.17) and
</p>
<p>(2.18) retain their validity even for single photons.
</p>
<p>Though it is not absolutely mandatory, as mentioned before, the second assump-
</p>
<p>tion is without any apparent alternatives&mdash;provided that light consists of a stream of
</p>
<p>photons&mdash;because we cannot draw the conclusion from the above considerations that
</p>
<p>these equations apply only above a certain number of photons. An additional degree
</p>
<p>of plausibility can be found in the fact that the wave character of light, for exam-
</p>
<p>ple in (2.17) and (2.18), never enters the arguments explicitly. And, finally, such an
</p>
<p>assumption&mdash;independently of plausibility&mdash;has to be proven by experiment, which
</p>
<p>of course has long since been done.
</p>
<p>2.2.1 Single Photons and Polarization
</p>
<p>We see that polarization is a property of individual photons. This fact is new and is
</p>
<p>by no means self-evident; thus, for individual photons we have e.g.
</p>
<p>|r = |h+i |v&radic;
2
</p>
<p>|l = |h&minus;i |v&radic;
2
</p>
<p>.
(2.20)
</p>
<p>However, the interpretation must be different from the case of &lsquo;classical&rsquo; light, since
</p>
<p>a photon in state |r whose linear polarization is measured (i.e. with respect to |h
or |v) cannot split up into two linearly-polarized photons (how would the energy
</p>
<p>15Single-photon experiments are standard technology these days. In 1952, Schr&ouml;dinger declared:
</p>
<p>&ldquo;We never experiment with just one electron or atom or (small) molecule. In thought-experiments
</p>
<p>we sometimes assume that we do; this invariably entails ridiculous consequences.&rdquo; Times have
</p>
<p>changed: Precision experiments using a single photon or a single atom are the basis of e.g. today&rsquo;s
</p>
<p>time standard, and modern quantum-mechanical developments such as the quantum computer rest
</p>
<p>on those &lsquo;ridiculous consequences&rsquo;. We recall that photons (as far as we know) have immeasurably
</p>
<p>small dimensions and are in this sense referred to as point objects (or point particles). Although
</p>
<p>they represent light of a specific wavelength, they do not have a spatial extension on the order of
</p>
<p>the wavelength of the light.</p>
<p/>
</div>
<div class="page"><p/>
<p>24 2 Polarization
</p>
<p>E =  be divided up in that case?).16 We must assume that we can infer the
probabilities P of finding a photon, initially in the state |r, in a state |h or |v after
its passage through e.g. a linear polarizing filter, from (2.20); namely
</p>
<p>P(h) =

</p>
<p>
</p>
<p>
1/
&radic;
</p>
<p>2
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>2
</p>
<p>=
1
</p>
<p>2
and P(v) =
</p>
<p>
</p>
<p>
</p>
<p>
i/
&radic;
</p>
<p>2
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>2
</p>
<p>=
1
</p>
<p>2
. (2.21)
</p>
<p>Therefore, one must beware of interpreting the expressions (2.20) incorrectly&mdash;it
</p>
<p>is not at all the case that an |r photon consists of half a horizontally- and half a
vertically-polarized object. Rather, (2.20) tells us that an |r photon contains two
possibilities to present itself in a measurement as either |h or |v&mdash;but only one of
these is realized in any given measurement. Before the measurement, however, the
</p>
<p>photon is in a superposition of the two states. This is a very common trait of quantum
</p>
<p>mechanical systems: states can be superposed.
</p>
<p>This superposition principle is valid for all states or objects described by quantum
</p>
<p>mechanics, whether we attribute to them more wave- or more particle-like character.
</p>
<p>In the macroscopic domain, the superposition of states would lead to very unusual
</p>
<p>effects&mdash;for example, in a system with the two states |cow in barn and |cow in field,
or in the famous example of Schr&ouml;dinger&rsquo;s cat, namely |dead cat and |live cat.
Our direct daily experience does not include such superposed states, and so certain
</p>
<p>quantum-mechanical phenomena are in conflict with &lsquo;common sense&rsquo; (whatever
</p>
<p>that may be, exactly). But as mentioned above, our sensory apparatus was trained
</p>
<p>by evolution under macroscopic conditions17 and our understanding of the world is
</p>
<p>based on corresponding model concepts. No one will seriously argue that therefore,
</p>
<p>the whole of nature should operate according to these &lsquo;daily life rules&rsquo; which literally
</p>
<p>permeate our flesh and blood.
</p>
<p>So when speaking of the paradoxes of quantum mechanics, we should recog-
</p>
<p>nize that the real paradox is simply that the rules of quantum mechanics (which we
</p>
<p>can indeed recognize, identify and formulate) proceed according to a different pat-
</p>
<p>tern from our familiar daily-life rules (i.e. &lsquo;common sense&rsquo;). But quantum mechan-
</p>
<p>ics works, and indeed it works verifiably, consistently, reproducibly, and with an
</p>
<p>amazingly high degree of accuracy&mdash;in short, according to all scientific standards, it
</p>
<p>is a successful theory. Quantum mechanics is one of the best if not the best-validated
</p>
<p>basic theory in physics. Of course, in spite of this the question remains as to why
</p>
<p>there are only microscopic superposition states and apparently no macroscopic ones.
</p>
<p>This is a central problem of quantum mechanics, which we will address several times
</p>
<p>in various chapters in the following.
</p>
<p>16In a vacuum, photons are indivisible, and that holds also for most interactions with matter. One
</p>
<p>has to work hard to &lsquo;cut&rsquo; photons. This can be achieved for example in the interaction with certain
</p>
<p>nonlinear crystals, where a single photon breaks up into two photons of lower energy (parametric
</p>
<p>fluorescence, see Appendix I, Vol. 2). Devices for polarization measurement are of course manu-
</p>
<p>factured in such a way that they leave the photons unsplit.
17Furthermore, in &lsquo;slow&rsquo; conditions&mdash;the effects of the theory of relativity are beyond our daily life
</p>
<p>experience, as well.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Light as Photons 25
</p>
<p>Fig. 2.3 Birefringence (left) and polarization beam splitter (right)
</p>
<p>2.2.2 Measuring the Polarization of Single Photons
</p>
<p>Back to our single photons: We produce one of them with a certain polarization
</p>
<p>and send it through the usual polarization filter (analyzer), which absorbs photons
</p>
<p>with the &lsquo;wrong&rsquo; polarization,18 or through an analyzer with two outputs, such as a
</p>
<p>birefringent crystal or a polarizing beam splitter (PBS); see Fig. 2.3. We assume an
</p>
<p>angle  between the polarization direction and the analyzer axis. Whether a photon
</p>
<p>passes the analyzer or not, or alternatively where it leaves the PBS, can be predicted
</p>
<p>with certainty only if  = 0 (the photon passes, or exits to the right); or  = /2
(the photon is absorbed, or exits downwards).19 In all other cases, we can specify
</p>
<p>only the probability P() that the photon passes the absorbing analyzer or the PBS
</p>
<p>with the same polarization; it is given by P() = cos2 .
We can interpret these facts as follows: Before a measurement, one cannot objec-
</p>
<p>tively determine whether the photon will pass through the analyzer or not. This is
</p>
<p>revealed only by the process of measurement. Which one of the two possibilities will
</p>
<p>be realized cannot be said before the measurement, but one can specify the respec-
</p>
<p>tive probabilities of observing each of the two outcomes. We can summarize the
</p>
<p>general result: For a state such as |z = c |x + d |y, the term P = |c|2 represents
the probability of measuring the state |x (assuming a normalized state |z, i.e. with
|c|2 + |d|2 = 1). In symbolic shorthand notation,
</p>
<p>measuring probability = |coefficient|2. (2.22)
</p>
<p>This is a similar finding as in classical physics&mdash;with the very significant differ-
</p>
<p>ence that the statement holds there for intensities , but here for probabilities.
</p>
<p>We note that while it is possible to measure with sufficient accuracy the polar-
</p>
<p>ization which a classical wave had before the measurement, this is in principle
</p>
<p>impossible for a single photon of unknown polarization. Quantum objects do not
</p>
<p>always possess well-defined values of all physical quantities&mdash;a linearly-polarized
</p>
<p>photon &lsquo;has&rsquo;, for example, no well-defined circular polarization. If we send a
</p>
<p>18We point out that this is not an exotic quantum-mechanical procedure&mdash;the eyes of every bee, or
</p>
<p>suitable sunglasses, perform precisely this kind of &lsquo;measurement process&rsquo;.
19These cases can be produced by inserting a further analyzer whose orientation is  + 0 or
 + /2, for example.</p>
<p/>
</div>
<div class="page"><p/>
<p>26 2 Polarization
</p>
<p>horizontally linear-polarized photon through a linear analyzer, rotated through the
</p>
<p>angle , we find horizontally- and vertically-polarized light with the probabilities
</p>
<p>cos2  and sin2 , respectively&mdash;in principle, with no ifs, ors, or buts.
</p>
<p>2.2.2.1 The Ensemble
</p>
<p>How can we verify experimentally that the calculated probabilities are correct? Obvi-
</p>
<p>ously not in a single experiment. Because if we send e.g. a circularly-polarized photon
</p>
<p>through a PBS, it emerges on the other side as either horizontally or vertically polar-
</p>
<p>ized, and we have no information about the probabilities. So we need to repeat the
</p>
<p>measurement more than once. Now the term ensemble comes into play. In quantum
</p>
<p>mechanics, this term refers to a set of (strictly speaking) infinitely many identically-
</p>
<p>prepared copies of a system.20 It is a fictitious set which has no counterpart in physical
</p>
<p>reality, but serves only for conceptual clarification. The &lsquo;strictly speaking&rsquo; in brack-
</p>
<p>ets refers to the fact that often (and for practical reasons), N identical copies of an
</p>
<p>system are called an ensemble, if N is sufficiently large (even though not infinite).
</p>
<p>The concept &lsquo;ensemble&rsquo; allows us to calculate the probabilities for the occurrence
</p>
<p>of certain measurement values, and thus to predict them&mdash;they are given simply by
</p>
<p>the fraction of subsets of the ensemble which are characterized by the presence of
</p>
<p>those values. In the example given above, this is P() = cos2  for passing the
analyzer or sin2  for not passing through it.
</p>
<p>We emphasize that the use of the word ensemble does not imply that the relevant
</p>
<p>physical quantities (here the polarization) have well-defined values which are dis-
</p>
<p>tributed in some unknown way among the members of the ensemble. In the example,
</p>
<p>the ensemble consists of horizontally-polarized photons whose polarization proper-
</p>
<p>ties are not defined with respect to an analyzer that is rotated by .
</p>
<p>In practice, one can of course measure only finitely many systems; often one
</p>
<p>has to make do even with a single system (in the example, a single photon). But
</p>
<p>the predictions arising from the concept of ensembles are generally valid and apply
</p>
<p>(in terms of probabilities) to the particular case considered.21
</p>
<p>So we can imagine that we prepare N systems in an identical manner and always
</p>
<p>measure the same variable, in our example the rate of observation of vertically- or
</p>
<p>horizontally-polarized photons behind the PBS.22 For N &rarr; &infin;, the relative frequen-
cies of occurrence of the different measurement results become the probabilities of
</p>
<p>the ensemble; in the above example cos2  and sin2 .
</p>
<p>20The systems need not be in the same state, but the preparation process must be the same.
21Just as the interference pattern in the double slit experiment builds up gradually from scattered
</p>
<p>spots over time.
22Another example of an ensemble are electrons which are prepared by a Stern&ndash;Gerlach apparatus
</p>
<p>and a velocity filter so that their spins are pointing upwards and their speeds are confined to a
</p>
<p>particular interval (v &minus;v, v +v). A further example is a set of hydrogen atoms in a particular
excited state, whereby here the preparation refers to the energy, but not to the angular momentum
</p>
<p>of the state.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Light as Photons 27
</p>
<p>2.2.2.2 Ensemble or Single Object?
</p>
<p>As we can see, the experimental verifiability of the theory is not guaranteed for a
</p>
<p>single quantum object, but rather requires an ensemble. Hence, one can argue that
</p>
<p>the formalism developed above (regardless of our derivation) is essentially a mathe-
</p>
<p>matical rule which applies only to an ensemble (so-called ensemble interpretation).
</p>
<p>Another position asserts however that the formalism applies also to an individual
</p>
<p>quantum object, as we have assumed to be the case. Both interpretations lead to the
</p>
<p>same results, but they are based on different concepts of &lsquo;reality&rsquo;.
</p>
<p>We encounter here for the first time a situation typical of quantum mechanics:
</p>
<p>The formalism and the verification of its predictions by measurements are uncon-
</p>
<p>troversial (if we accept certain basic assumptions); the controversial issue concerns
</p>
<p>what quantum mechanics &lsquo;really&rsquo; means. This debate is as old as quantum mechanics
</p>
<p>itself, and is still very much alive; there are a dozen or more different explanations
</p>
<p>(interpretations). We will discuss these questions often and will give an overview of
</p>
<p>current interpretations in Chap. 28, Vol. 2.
</p>
<p>2.2.2.3 Do We Really Need Probabilities?
</p>
<p>Finally, a remark about the concept of probability. In classical physics, probabilities
</p>
<p>reflect the fact that we do not know (or do not wish to know) enough about some of
</p>
<p>the properties of a system in order to calculate them explicitly. For instance, in the
</p>
<p>kinetic theory of gases, one is not interested in the behavior of a single molecule;
</p>
<p>a well-known example from a quite different field are the opinion polls before an
</p>
<p>election, where the behavior of individual voters is not of interest. Analogously, one
</p>
<p>could assume here that the occurrence of probabilities indicates that below the level of
</p>
<p>our discussion, there are some hidden variables, and if we were able to know them,
</p>
<p>we could formulate the whole process exactly without resorting to probabilities.
</p>
<p>This is an obvious idea which was brought up very soon after the emergence of
</p>
<p>quantum mechanics. It took nearly 40 years until a criterion was found to resolve this
</p>
<p>question in principle, and a few more years to disprove the idea of hidden variables
</p>
<p>experimentally based on that criterion&mdash;at least this holds for the major classes of
</p>
<p>hidden variables. More on this topic in Chaps. 20 and 27, Vol. 2.
</p>
<p>According to our current knowledge, we cannot avoid the term &lsquo;probability&rsquo; in
</p>
<p>quantum mechanics. It is, so to speak, a structural element of quantum mechanics,
</p>
<p>the sign that quantum mechanics deals first of all with possibilities, one of which is
</p>
<p>realized by a measurement, with a certain probability for different outcomes.</p>
<p/>
</div>
<div class="page"><p/>
<p>28 2 Polarization
</p>
<p>2.3 Exercises
</p>
<p>1. Given an electromagnetic wave E (r, t) = E0ei(kr&minus;t) in a charge-free region
of space (we consider the electric field only); show that the wave is transverse,
</p>
<p>i.e. that k &middot; E0 = 0 holds (Hint: cf. the Maxwell equation &nabla;E = 0). Specialize to
k = (0, 0, k).
</p>
<p>2. Linear combinations
</p>
<p>(a) Express |r as a linear combination of |h and |v. Do the same for |l.
(b) Express |h as a linear combination of |r and |l. Do the same for |v.
</p>
<p>3. A phase shift of 90 is described by ei/2 = i . What follows for a phase shift of
180?
</p>
<p>4. Elliptical polarization: Given the state |z =  |h +  |v, with ||2 + ||2 = 1;
express |z as a superposition of |r and |l.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 3
</p>
<p>More on the Schr&ouml;dinger Equation
</p>
<p>We first examine some general properties of the Schr&ouml;dinger equation. Among other topics,
</p>
<p>the concept of vector space emerges&mdash;the solutions of the Schr&ouml;dinger equation form such
</p>
<p>a space. In the Schr&ouml;dinger equation, operators occur. We see that the order of the operators
</p>
<p>plays a role, provided that they do not commute.
</p>
<p>In Chap. 1, we introduced the Schr&ouml;dinger equation (SEq)
</p>
<p>i
&part;
</p>
<p>&part;t
 (r, t) = &minus;
</p>
<p>
2
</p>
<p>2m
&nabla;2 (r, t)+ V (r, t) (r, t) . (3.1)
</p>
<p>For our considerations it is the basic differential equation of quantum mechanics. In
</p>
<p>view of of its central role, we want to examine in the following which properties the
</p>
<p>SEq has and which consequences follow from those properties. By separating out
</p>
<p>the time, one can obtain from (3.1) the stationary Schr&ouml;dinger equation (also known
</p>
<p>as the time-independent Schr&ouml;dinger equation), which for us is the workhorse of
</p>
<p>quantum mechanics. Finally, we make a few preliminary comments on operators,
</p>
<p>which in quantum mechanics are identified with measurable quantities.
</p>
<p>3.1 Properties of the Schr&ouml;dinger Equation
</p>
<p>The SEq has several immediately recognizable features which model important phys-
</p>
<p>ical properties and imply certain consequences. For example, one sees immediately
</p>
<p>that  (r, t) must be complex if the potential V is real (we will restrict ourselves to
</p>
<p>this case; the reason will be discussed later). Certain features are treated here only
</p>
<p>provisionally, while more detailed treatments follow in subsequent chapters. Some
</p>
<p>basic facts about differential equations are summarized in Appendix E, Vol. 1.
</p>
<p>1. The SEq is linear in . If one has found two solutions 1 and 2, then any
</p>
<p>linear combination c1 + d2 is also a solution (with c, d &isin; C). This means
</p>
<p>that one can superimpose the solutions - the superposition principle holds. This
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4_3
</p>
<p>29</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_3&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_3&amp;domain=pdf</a></div>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4_3">https://doi.org/10.1007/978-3-030-00464-4_3</a></div>
</div>
<div class="page"><p/>
<p>30 3 More on the Schr&ouml;dinger Equation
</p>
<p>Fig. 3.1 Schematic
</p>
<p>representation of three
</p>
<p>physically-equivalent wave
</p>
<p>functions (x), 1
2
(x),
</p>
<p>&minus;(x)
</p>
<p>principle, known e.g. from the description of classical waves, has far-reaching
</p>
<p>consequences in quantum mechanics. Properties of the microscopic world which
</p>
<p>for our everyday understanding are very &lsquo;bizarre&rsquo; are largely due to the seemingly
</p>
<p>trivial fact of linearity. Incidentally, due to this linearity, the total wave functions
</p>
<p> and c are physically equivalent, or to be more precise, theymust be physically
</p>
<p>equivalent; see Fig. 3.1.
</p>
<p>Being a linear equation, the SEq always has the solution  &equiv; 0, the so-called
</p>
<p>trivial solution. This solution does not describe a physical state, as one can for
</p>
<p>example add arbitrary multiples of it to any other state without changing anything
</p>
<p>physically. In other words, if it turns out that the state of a physical system is
</p>
<p>described by the trivial solution, then we know that this state does not exist.
</p>
<p>2. The SEq is a differential equation of first order in time. This means that for
</p>
<p>a given initial condition  (r, t = 0), the wave function  (r, t) is determined
</p>
<p>for all times (greater and less than zero). In other words, in the time evolution of
</p>
<p> (r, t), there are no stochastic or random elements &mdash; by specifying (r, t = 0),
</p>
<p>one uniquely defines the wave function for all past and future times.
</p>
<p>3. The SEq is a differential equation of second order in space. To describe a specific
</p>
<p>given physical situation, the solution must satisfy certain boundary conditions.
</p>
<p>4. The SEq determines, as we shall see below, which results are generally possible,
</p>
<p>but not which result will be realized in an actual measurement. This information
</p>
<p>must therefore come from somewhere else.1
</p>
<p>The ability to form superpositions is a fundamental property of all elements of a
</p>
<p>vector space V . In fact, it can easily be shown that the solutions of the SEq span a
</p>
<p>vector space over the complex numbers&mdash;see the definition in Appendix G, Vol. 1.
</p>
<p>Thus, we have a similar situation as for the polarization: The states of a system are
</p>
<p>described by elements of a vector space, in which the superposition principle applies.
</p>
<p>The dimensions of the spaces may be different&mdash;it is 2 in the case of polarization,
</p>
<p>while the dimension of the solution space of the SEq is unknown to us yet. But at
</p>
<p>least we have found with V a structure which is common to both the approaches of
</p>
<p>Chaps. 1 and 2.
</p>
<p>1One can summarize the difference between classical mechanics and quantum mechanics in a bold
</p>
<p>and simple way as follows: Classical mechanics describes the time evolution of the factual, quantum
</p>
<p>mechanics (i.e. the SEq) describes the time evolution of the possible.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.1 Properties of the Schr&ouml;dinger Equation 31
</p>
<p>Things are different for the pair of concepts &lsquo;determinism&mdash;probability&rsquo;. In this
</p>
<p>regard, our two approaches to quantum mechanics (still) do not match. Probabilities
</p>
<p>which we had to introduce in the algebraic approach to the transition from classical
</p>
<p>mechanics&rarr; quantum mechanics do not appear in the SEq. On the contrary, the SEq
</p>
<p>is a deterministic equation whose solutions are uniquely determined for all times,
</p>
<p>given the initial conditions. Hence, the apparent randomness of quantum mechanics
</p>
<p>(e.g. in radioactive decay) is not hidden in the SEq.
</p>
<p>As we will see in the next chapters, chance comes into play through the wavefunc-
</p>
<p>tion. We emphasize once again that the wavefunction as a solution of the SEq has no
</p>
<p>direct, intuitive meaning in the &lsquo;everyday world&rsquo;. In this respect, the question of what
</p>
<p> &lsquo;actually is&rsquo; cannot be answered in everyday terms. Perhaps the idea mentioned
</p>
<p>in Chap. 1, of a complex-valued field of possibilities, is the most appropriate.
</p>
<p>3.2 The Time-Independent Schr&ouml;dinger Equation
</p>
<p>In Chap. 1, we found that the solutions of the free Schr&ouml;dinger equation ((3.1) with
</p>
<p>V &equiv; 0) are plane waves with the dispersion relation  = 2k2/2m. But what are
</p>
<p>the solutions for a non-vanishing potential V ? The answer is: There are virtually
</p>
<p>no closed or analytical solutions in this case. Apart from just a handful of special
</p>
<p>potentials, one always has to deal with approximations or numerical results.
</p>
<p>Nevertheless, the approach to the Schr&ouml;dinger equation may be facilitated by sep-
</p>
<p>arating out the variable t . This leads to the so-called stationary or time-independent
</p>
<p>Schr&ouml;dinger equation which depends on space variables only. The prerequisite is,
</p>
<p>however, that the potential V must not depend on time:
</p>
<p>V (r, t) = V (r) . (3.2)
</p>
<p>Of course there are also physically reasonable potentials which do depend on time,
</p>
<p>but we will restrict ourselves to time-independent potentials in the following.
</p>
<p>The method of choice is again the separation of variables. We insert the ansatz
</p>
<p> (r, t) = f (t) &middot;  (r) (3.3)
</p>
<p>into the Schr&ouml;dinger Equation (3.1) and obtain
</p>
<p>i
f
</p>
<p>f
= &minus;
</p>
<p>
2
</p>
<p>2m
</p>
<p>1
</p>
<p>
&nabla;2+ V . (3.4)
</p>
<p>The right- and the left-hand sides must be constant (because the independent variables
</p>
<p>&lsquo;space&rsquo; and &lsquo;time&rsquo; are separated):
</p>
<p>i
f
</p>
<p>f
= const. = E = . (3.5)</p>
<p/>
</div>
<div class="page"><p/>
<p>32 3 More on the Schr&ouml;dinger Equation
</p>
<p>E and  are a yet undetermined energy (this follows from its physical units) and
</p>
<p>frequency. The sign (i.e. E and not &minus;E) is chosen in such a way that it agrees with
</p>
<p>the usual definition of the energy. A solution of this last equation is
</p>
<p>f (t) = e&minus;i Et/ = e&minus;it (3.6)
</p>
<p>or
</p>
<p> (r, t) = e&minus;it (r) . (3.7)
</p>
<p>E must be a real number, because otherwise the solutions would be unphysical, since
</p>
<p>they would tend for t &rarr; &infin; or t &rarr; &minus;&infin; towards infinity and would not be bounded.
</p>
<p>Inserting the wavefunction (3.7) into the time-dependent Schr&ouml;dinger equation,
</p>
<p>i
&part;
</p>
<p>&part;t
 (r, t) = &minus;
</p>
<p>
2
</p>
<p>2m
&nabla;2 (r, t)+ V (r) (r, t) (3.8)
</p>
<p>leads to the time-independent (= stationary) Schr&ouml;dinger equation:
</p>
<p>E (r) = &minus;

</p>
<p>2
</p>
<p>2m
&nabla;2 (r)+ V (r) (r) . (3.9)
</p>
<p>At this point, the possible values of E are not explicitly defined. We take up this
</p>
<p>issue again in Chap. 5.
</p>
<p>In the last two equations, the expression &minus; 
2
</p>
<p>2m
&nabla;2 + V (r) occurs. It is called the
</p>
<p>Hamiltonian operator H (or simply Hamiltonian for short) and is a central term in
</p>
<p>quantum mechanics:
</p>
<p>H = &minus;

</p>
<p>2
</p>
<p>2m
&nabla;2 + V (r) . (3.10)
</p>
<p>With this, the time-dependent Schr&ouml;dinger equation is written as
</p>
<p>i
&part;
</p>
<p>&part;t
 = H. (3.11)
</p>
<p>Note that the expression (3.10) is just one possible form of the Hamiltonian,
</p>
<p>and indeed a particularly simple one. Other formulations (which are considered
</p>
<p>in the Appendix) contain vector potentials or describe relativistic situations.
</p>
<p>The properties of the Schr&ouml;dinger equation which we listed above hold true for
</p>
<p>all SEq (3.11), independently of the special form of the Hamiltonian H . This applies
</p>
<p>also to the method used to derive the time-independent SEq from the time-dependent
</p>
<p>SEq, as long as the otherwise arbitrary operator H does not depend on time. In that
</p>
<p>case, the separation ansatz  (r, t) = e&minus;it (r) always leads to the stationary SEq
</p>
<p>H = E. (3.12)
</p>
<p>In a certain sense, this is quantum mechanics in short form.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Operators 33
</p>
<p>3.3 Operators
</p>
<p>Mathematically, the stationary SEq (3.12) is none other than an eigenvalue problem.
</p>
<p>You perhaps remember such problems from school days in the following form: Given
</p>
<p>a matrix A and a vector x , for which numbers  = 0 do there exist solutions x of
</p>
<p>the equation Ax = x? The answer is that the allowed values of  are given by the
</p>
<p>solutions of the secular equation det (A &minus; ) = 0.
</p>
<p>In the SEq (3.12), the Hamiltonian operator H appears on the left side instead
</p>
<p>of the matrix A. The concept of an operator2 plays an essential role in quantum
</p>
<p>mechanics. While in the following chapters we will return repeatedly to this topic,
</p>
<p>here we give just a brief heuristic consideration or motivation. The term &lsquo;operator&rsquo;
</p>
<p>can be best illustrated by &lsquo;manipulation&rsquo; or &lsquo;tool&rsquo;. To apply an operator A to a function
</p>
<p>means to manipulate this function in a prescribed manner.
</p>
<p>For example, the operator A = &part;
&part;x
</p>
<p>differentiates a function partially with respect
</p>
<p>to x . The operator B = &part;
&part;x
x multiplies a function by x and then differentiates the
</p>
<p>product. Products of operators are performed from right to left; AB f means that we
</p>
<p>first apply B to f and then A to B f . In the following, we always take for granted
</p>
<p>that the functions have the properties which are required for the application of the
</p>
<p>operator under consideration. For instance, the functions on which A = &part;
&part;x
</p>
<p>acts must
</p>
<p>be differentiable with respect to x .
</p>
<p>The eigenvalue problem can be formulated in a general way. Consider a general
</p>
<p>operator A (which can be a matrix or a differential operator, for example). If the
</p>
<p>equation
</p>
<p>A f =  f (3.13)
</p>
<p>can be solved for certain numbers  &isin; C (which means that there are solutions f ),
</p>
<p>then  is called an eigenvalue of the operator A and f is called the associated
</p>
<p>eigenfunction. If one wants to emphasize that the function f is an element of a
</p>
<p>vector space, then f is called an eigenvector instead of an eigenfunction. The
</p>
<p>set of all eigenvalues is termed the spectrum; the spectrum can contain finitely or
</p>
<p>infinitely many elements. The eigenvalues may be countable (discrete spectrum) or
</p>
<p>uncountable (continuous spectrum); spectra can contain both discrete and continuous
</p>
<p>components.
</p>
<p>If there are two or more (e.g. n) linearly-independent eigenfunctions correspond-
</p>
<p>ing to the same eigenvalue, one speaks of degeneracy. The eigenvalue is called n-fold
</p>
<p>degenerate, where n is the degree of degeneracy. Degeneracy is the consequence of
</p>
<p>a symmetry which is intrinsic to the problem; it can in principle be avoided by an
</p>
<p>arbitrary small, suitable &lsquo;perturbation operator&rsquo;.
</p>
<p>2A mapping between two vector spaces (whose elements can be functions, for example) is usu-
</p>
<p>ally called an operator; a mapping from one vector space to its scalar field a functional. Integral
</p>
<p>transforms such as the Fourier or the Laplace transform can be viewed as integral operators.
</p>
<p>In the interest of a unique terminology, we fix the difference between operator and function as
</p>
<p>follows: The domain of definition and the range of operators are vector spaces, while for functions,
</p>
<p>they are sets of numbers.</p>
<p/>
</div>
<div class="page"><p/>
<p>34 3 More on the Schr&ouml;dinger Equation
</p>
<p>Here are two simple examples of eigenvalue problems:
</p>
<p>1. Given the operator &part;
&part;x
</p>
<p>, the eigenvalue problem reads
</p>
<p>&part;
</p>
<p>&part;x
f (x) =  f (x);  &isin; C. (3.14)
</p>
<p>Obviously, we can solve this equation for all . The solution is
</p>
<p>f (x) = f0e
x . (3.15)
</p>
<p>The spectrum is continuous and not degenerate.
</p>
<p>2. Given the operator &part;
2
</p>
<p>&part;x2
, the eigenvalue problem
</p>
<p>&part;2
</p>
<p>&part;x2
f = 2 f ;  &isin; C. (3.16)
</p>
<p>is clearly invariant under the exchange x &rarr; &minus;x , and its solutions are
</p>
<p>f = f0+e
+x and f = f0&minus;e
</p>
<p>&minus;x . (3.17)
</p>
<p>The spectrum is continuous and doubly degenerate (for one value of 2 there exist
</p>
<p>the two linearly-independent eigenfunctions e+x and e&minus;x ).
</p>
<p>The limitation of the range of allowed functions in these two examples (for
</p>
<p>instance due to boundary conditions) can lead to a discrete spectrum; examples are
</p>
<p>found in the exercises. A (classical) example is the vibration of a violin string. The
</p>
<p>fundamental vibrational mode has the wavelength  = 2L , where L is the length of
</p>
<p>the string (i.e. the position variable x along the string is bounded, 0 &le; x &le; L). Other
</p>
<p>allowed solutions are harmonics of the fundamental mode, i.e. their frequencies are
</p>
<p>whole-number multiples of the fundamental frequency. The (countable) eigenvalues
</p>
<p>are these integer multiples, giving a discrete spectrum.
</p>
<p>3.3.1 Classical Numbers and Quantum-Mechanical
</p>
<p>Operators
</p>
<p>The SEq (3.1) formally resembles the expression for the classical energy
</p>
<p>E =
p2
</p>
<p>2m
+ V . (3.18)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Operators 35
</p>
<p>Indeed, one can transform the numerical (3.18 ) into an operator equation and vice
</p>
<p>versa, if one identifies3:
</p>
<p>x &harr; x or r &harr; r
</p>
<p>px &harr;

</p>
<p>i
&part;
&part;x
</p>
<p>or p &harr; 
i
&nabla;.
</p>
<p>E &harr; i &part;
&part;t
</p>
<p>(3.19)
</p>
<p>In this way, the expression (3.18) leads to the SEq (3.1) in its representation as an
</p>
<p>operator equation:
</p>
<p>i
&part;
</p>
<p>&part;t
= &minus;
</p>
<p>
2
</p>
<p>2m
&nabla;2 + V (r, t) = H. (3.20)
</p>
<p>We can motivate these &lsquo;translations&rsquo; from classical to quantum-mechanical quan-
</p>
<p>tities as follows: We differentiate a plane wave
</p>
<p>f = ei(kx&minus;t) (3.21)
</p>
<p>with respect to x :
&part; f
</p>
<p>&part;x
= ikei(kx&minus;t) = ik f. (3.22)
</p>
<p>In order to find the momentum, we multiply both sides with / i and obtain with
</p>
<p>p = k
</p>
<p>
</p>
<p>i
</p>
<p>&part; f
</p>
<p>&part;x
= k f = p f or
</p>
<p>
</p>
<p>i
</p>
<p>&part;
</p>
<p>&part;x
f = p f or
</p>
<p>(
</p>
<p>
</p>
<p>i
</p>
<p>&part;
</p>
<p>&part;x
&minus; p
</p>
<p>)
</p>
<p>f = 0. (3.23)
</p>
<p>The bracket in the last equation does not depend on the particular wave number k.
</p>
<p>Because of its linearity, this equation applies to all functions which we can generate
</p>
<p>by a superposition of plane waves (i.e. all &lsquo;sufficiently reasonable&rsquo; functions), if we
</p>
<p>understand p to represent not the momentum of a single wave, but that of the whole
</p>
<p>new function. It is quite natural to define an operator p (momentum operator, usually
</p>
<p>just called p, sometimes also pop or p):
</p>
<p>p =

</p>
<p>i
</p>
<p>&part;
</p>
<p>&part;x
. (3.24)
</p>
<p>In this context, x is also called the position operator. This formulation may appear
</p>
<p>unnecessarily complicated at this point, since the application of the position operator
</p>
<p>simply means multiplication by x . But later on we will encounter other contexts
</p>
<p>where this is no longer the case. Here, we can at least motivate the terminology by
</p>
<p>the following parallel
</p>
<p>3This small table is sometimes (rather jokingly) referred to as the &lsquo;dictionary of quantum mechanics.&rsquo;</p>
<p/>
</div>
<div class="page"><p/>
<p>36 3 More on the Schr&ouml;dinger Equation
</p>
<p>application of the
momentum
</p>
<p>position
operator to ei(kx&minus;t) yields
</p>
<p>pei(kx&minus;t)
</p>
<p>xei(kx&minus;t).
(3.25)
</p>
<p>The crucial point of the translations table (3.19), which in more sophisticated
</p>
<p>language is referred to as the correspondence principle,4 is that it allows the trans-
</p>
<p>lation of classical expressions into those of quantum mechanics. Some examples:
</p>
<p>The classical expression E =
p2x
2m
</p>
<p>becomes in quantum mechanics i &part;
&part;t
</p>
<p>= &minus; 
2
</p>
<p>2m
&part;2
</p>
<p>&part;x2
,
</p>
<p>and E = p
2
</p>
<p>2m
becomes i &part;
</p>
<p>&part;t
= &minus; 
</p>
<p>2
</p>
<p>2m
&nabla;2. The classical angular momentum l = r &times; p
</p>
<p>leads to the quantum-mechanical angular momentum operator l = 
i
r &times; &nabla;, and
</p>
<p>from the relativistic energy-momentum relation E2 = m20c
4 + p2c2, we obtain
</p>
<p>&minus;2 &part;
2
</p>
<p>&part;t2
= m20c
</p>
<p>4 &minus; c22&nabla;2. This last expression is the so-called Klein-Gordon equa-
</p>
<p>tion which describes free relativistic quantum objects with zero spin.
</p>
<p>3.3.2 Commutation of Operators; Commutators
</p>
<p>In this process of translation, however, problems can arise if we translate products of
</p>
<p>two or more variables. These are due to the fact that numbers commute, but in general
</p>
<p>operators do not.5 As an illustrative example, we consider the classical expression xpx
which obviously equals px x . But this no longer applies to its quantum-mechanical
</p>
<p>replacement by operators
</p>
<p>xpx = x

</p>
<p>i
</p>
<p>&part;
</p>
<p>&part;x
= px x =
</p>
<p>
</p>
<p>i
</p>
<p>&part;
</p>
<p>&part;x
x =
</p>
<p>
</p>
<p>i
</p>
<p>(
</p>
<p>1 + x
&part;
</p>
<p>&part;x
</p>
<p>)
</p>
<p>. (3.26)
</p>
<p>Anyone who is not sure about such considerations should transform the operator
</p>
<p>equations into &lsquo;usual&rsquo; equations by applying the operators to a function (the function
</p>
<p>need not be specified in detail here, but must of course meet the necessary technical
</p>
<p>requirements). Then, for example, we have for the operator &part;
&part;x
x due to the product
</p>
<p>rule
&part;
</p>
<p>&part;x
x f = f + x
</p>
<p>&part; f
</p>
<p>&part;x
=
</p>
<p>&part;
</p>
<p>&part;x
x f =
</p>
<p>(
</p>
<p>1 + x
&part;
</p>
<p>&part;x
</p>
<p>)
</p>
<p>f (3.27)
</p>
<p>or briefly, in operator notation,
</p>
<p>&part;
</p>
<p>&part;x
x = 1 + x
</p>
<p>&part;
</p>
<p>&part;x
. (3.28)
</p>
<p>4In the old quantum theory, the (Bohr) correspondence principle denoted an approximate agree-
</p>
<p>ment of quantum-mechanical and classical calculations for large quantum numbers. In modern
</p>
<p>quantum mechanics, correspondence refers to the assignment of classical observables to corre-
</p>
<p>sponding operators. This assignment, however, has mainly a heuristic value and must always be
</p>
<p>verified or confirmed experimentally. A more consistent procedure is for example the introduction
</p>
<p>of position and momentum operators by means of symmetry transformations (see Chap. 21 Vol. 2).
5It is known for example that for two square matrices A and B (= operators acting on vectors), in
</p>
<p>general AB = BA holds.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Operators 37
</p>
<p>The importance of the topic of &lsquo;operators&rsquo; in quantum mechanics is based, among
</p>
<p>other things, on the fact that measurable variables (such as the momentum px ) are
</p>
<p>represented by operators (such as &minus;i&part;x ). If, as in (3.26), the order of the operators
</p>
<p>matters because of x 
i
</p>
<p>&part;
&part;x
</p>
<p>= 
i
</p>
<p>&part;
&part;x
x , then this holds true also for the corresponding
</p>
<p>measurement variables. In other words, it makes a difference in quantum mechanics
</p>
<p>whether we measure first the position x and then the momentum px , or vice versa.
</p>
<p>For the corresponding operators, the equality
</p>
<p>(xpx &minus; px x) f =

</p>
<p>i
</p>
<p>(
</p>
<p>x
&part; f
</p>
<p>&part;x
&minus; x
</p>
<p>&part; f
</p>
<p>&part;x
&minus; f
</p>
<p>)
</p>
<p>= i f (3.29)
</p>
<p>holds, or
</p>
<p>xpx &minus; px x = i. (3.30)
</p>
<p>Because differences of this kind play a key role in quantum mechanics, there is a
</p>
<p>special notation, namely a square bracket, called the commutator:
</p>
<p>[x, px ] = xpx &minus; px x = i. (3.31)
</p>
<p>For two operators A and B, the commutator6 is defined as
</p>
<p>[A, B] = AB &minus; BA. (3.32)
</p>
<p>If it is equal to zero, A and B are called commuting operators.7
</p>
<p>We repeat our remark that the order is crucial (of operators as well as of
</p>
<p>measurements). Of course there are commuting operators, such as for instance px
and y or px and z, and so on. Position and momentum commute if and only if they
</p>
<p>do not refer to the same coordinate.
</p>
<p>6The anticommutator is defined as
</p>
<p>{A, B} = AB + BA
</p>
<p>(despite the use of the same curly brackets, it is of course quite different from the Poisson brackets
</p>
<p>of classical mechanics).
7There is an interesting connection with classical mechanics which we have already mentioned
</p>
<p>briefly in a footnote in Chap. 1: In classical mechanics, the Poisson bracket for two variables U and
</p>
<p>V is defined as
</p>
<p>{U, V }Poisson =
&sum;
</p>
<p>i
</p>
<p>(
</p>
<p>&part;U
</p>
<p>&part;qi
</p>
<p>&part;V
</p>
<p>&part; pi
&minus;
</p>
<p>&part;U
</p>
<p>&part; pi
</p>
<p>&part;V
</p>
<p>&part;qi
</p>
<p>)
</p>
<p>,
</p>
<p>where qi and pi are the positions and (generalized) momenta of n particles, i = 1, 2, . . . , 3n. In
</p>
<p>order to avoid confusion with the anticommutator, we have added the (otherwise uncommon) index
</p>
<p>Poisson. If U and V are defined as quantum-mechanical operators, their commutator is obtained
</p>
<p>by setting [U, V ] = i {U, V }Poisson. Example: In classical mechanics, we choose U = q1 &equiv; x
</p>
<p>and V = p1 &equiv; px . Then it follows that {q1, p1}Poisson = 1, and we find the quantum-mechanical
</p>
<p>result [q1, p1] = [x, px ] = i. This method, called &lsquo;canonical quantization&rsquo;, is considered in more
</p>
<p>detail in the relativistic sections in the Appendix.</p>
<p/>
</div>
<div class="page"><p/>
<p>38 3 More on the Schr&ouml;dinger Equation
</p>
<p>A short remark concerning the problem of translation of &lsquo;ambiguous&rsquo; terms such
</p>
<p>as xpx : The problem can be resolved by symmetrization. The reason will be discussed
</p>
<p>in Chap. 13; here, it suffices to say that in this way one gets the correct quantum-
</p>
<p>mechanical expression. With the two possibilities xpx and px x , we construct the
</p>
<p>symmetrized expression
</p>
<p>AQM =
xpx + px x
</p>
<p>2
=
</p>
<p>
</p>
<p>2i
</p>
<p>(
</p>
<p>x
&part;
</p>
<p>&part;x
+
</p>
<p>&part;
</p>
<p>&part;x
x
</p>
<p>)
</p>
<p>=

</p>
<p>2i
</p>
<p>(
</p>
<p>1 + 2x
&part;
</p>
<p>&part;x
</p>
<p>)
</p>
<p>. (3.33)
</p>
<p>However, this trick is hereafter hardly ever needed &mdash; quantum mechanics is very
</p>
<p>good-natured in a certain sense.8 Consider, for example, the angular momentum
</p>
<p>l = r &times; p. Must it be symmetrized, i.e. l = r&times;p
2
</p>
<p>&minus; p&times;r
2
</p>
<p>, for the translation into
</p>
<p>quantum mechanics? The answer is &lsquo;no&rsquo;, because for it we have
</p>
<p>lx = (r &times; p)x = ypz &minus; zpy =

</p>
<p>i
</p>
<p>(
</p>
<p>y
&part;
</p>
<p>&part;z
&minus; z
</p>
<p>&part;
</p>
<p>&part;y
</p>
<p>)
</p>
<p>, (3.34)
</p>
<p>and we see that we need not symmetrize, since position and momentum commute if
</p>
<p>they belong to different coordinates:
</p>
<p>&part;z y f (y, z) = y&part;z f (y, z) (3.35)
</p>
<p>or
</p>
<p>[x, px ] = i;
[
</p>
<p>x, py
]
</p>
<p>=
[
</p>
<p>x, pz
]
</p>
<p>= 0; analogously for y, z. (3.36)
</p>
<p>Actually, for the &lsquo;standard&rsquo; operators, one can do without symmetrization.
</p>
<p>One of the few counterexamples is the radial momentum pr/r which occurs e.g.
</p>
<p>in the formulation of the kinetic energy in spherical coordinates (see exercises).
</p>
<p>Another example is the Lenz vector . If a particle with mass m moves in a potential
</p>
<p>U = &minus;
r
</p>
<p>, then the vector , defined by
</p>
<p> =
1
</p>
<p>m
(l &times; p)+
</p>
<p>r
</p>
<p>r
, (3.37)
</p>
<p>is a conserved quantity. For the translation into quantum mechanics, the term l &times; p
</p>
<p>must be symmetrized. For more on the Lenz vector, see Appendix G, Vol. 2.
</p>
<p>8Actually that is good news, because this symmetrization is not without problems. Take
</p>
<p>for example x2 p&mdash;is the symmetrized expression xpx , 1
2
</p>
<p>(
</p>
<p>x2 p + px2
)
</p>
<p>, 1
3
</p>
<p>(
</p>
<p>x2 p + xpx + px2
)
</p>
<p>,
1
4
</p>
<p>(
</p>
<p>x2 p + 2xpx + px2
)
</p>
<p>or a completely different term? Or does everything lead to the same
</p>
<p>quantum-mechanical expression (as is indeed the case in this example)?</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Exercises 39
</p>
<p>3.4 Exercises
</p>
<p>1. Show explicitly that the solutions of the Schr&ouml;dinger (3.1) span a vector space.
</p>
<p>2. Calculate
[
</p>
<p>x, &part;
2
</p>
<p>&part;x2
</p>
<p>]
</p>
<p>.
</p>
<p>3. Given the relativistic energy-momentum relation E2 = m20c
4 + c2 p2; from this
</p>
<p>dispersion relation, deduce a differential equation.
</p>
<p>4. Separation: Deduce the time-independent Schr&ouml;dinger equation from the time-
</p>
<p>dependent Schr&ouml;dinger equation by means of the separation of variables.
</p>
<p>5. Given the eigenvalue problem
</p>
<p>&part;
</p>
<p>&part;x
f (x) =  f (x);  &isin; C (3.38)
</p>
<p>with f (x) satisfying the boundary conditions f (0) = 1 and f (1) = 2, calculate
</p>
<p>the eigenfunction and eigenvalue.
</p>
<p>6. Given the eigenvalue problem
</p>
<p>&part;2
</p>
<p>&part;x2
f = 2 f ;  &isin; C (3.39)
</p>
<p>with f (x) satisfying the boundary conditions f (0) = f (L) = 0 and L = 0,
</p>
<p> = 0, calculate eigenfunctions and eigenvalues.
</p>
<p>7. Given the nonlinear differential equation
</p>
<p>y&prime;(x) =
dy(x)
</p>
<p>dx
= y2(x). (3.40)
</p>
<p>y1 (x) and y2 (x) are two different nontrivial solutions of (3.40), i.e. y1 = const &middot;
</p>
<p>y2 and y1y2 = 0.
</p>
<p>(a) Show that a multiple of a solution, i.e. f (x) = cy1 (x) with c = 0, c = 1,
</p>
<p>is not a solution of (3.40).
</p>
<p>(b) Show that a linear combination of two solutions, i.e. g(x) = ay1 (x) +
</p>
<p>by2 (x) with ab = 0, but otherwise arbitrary, is not a solution of (3.40).
</p>
<p>(c) Find the general solution of (3.40).
</p>
<p>8. Radial momentum
</p>
<p>(a) Show that the classical momentum p obeys
</p>
<p>p2 =
(
</p>
<p>pr
)2
</p>
<p>+
(
</p>
<p>p&times;r
)2
. (3.41)
</p>
<p>(b) Deduce the quantum-mechanical expression pr for the classical radial
</p>
<p>momentum rp
(
</p>
<p>= pr
)
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>40 3 More on the Schr&ouml;dinger Equation
</p>
<p>9. Show explicitly that the classical expression l = r&times;p need not be symmetrized
</p>
<p>for the translation into quantum mechanics.
</p>
<p>10. Given the operators A = x d
dx
</p>
<p>, B = d
dx
x and C = d
</p>
<p>dx
:
</p>
<p>(a) Calculate A fi (x) for the functions f1(x) = x
2, f2(x) = e
</p>
<p>ikx and f3(x) =
</p>
<p>ln x .
</p>
<p>(b) Determine A2 f (x) for arbitrary f (x).
</p>
<p>(c) Calculate the commutators [A, B] and [B,C].
</p>
<p>(d) Compute eiC x2 &minus; (x + i)2. Prove the equation eiCeikx = e&minus;keikx .</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 4
</p>
<p>Complex Vector Spaces and Quantum
</p>
<p>Mechanics
</p>
<p>In our complex vector space, we can define a scalar product. The properties of orthogonality
</p>
<p>and completeness lead to the important concept of a complete orthonormal system. The
</p>
<p>measurement process can be formulated by means of suitable projection operators.
</p>
<p>Up to now, we have occasionally used the terms &lsquo;vector space&rsquo; or &lsquo;state space&rsquo;. In
</p>
<p>this chapter, we will address this concept in more detail. For reasons of simplicity, we
</p>
<p>will rely heavily on the example of polarization, where the basic formulations are of
</p>
<p>course independent of the specific realization and are valid for all two-dimensional
</p>
<p>state spaces (such as polarization states, electron spin states, a double-well potential,
</p>
<p>the ammonia molecule, etc.). Moreover, the concepts introduced here retain their
</p>
<p>meaning in higher-dimensional state spaces, as well. Therefore, we can introduce
</p>
<p>and discuss many topics by using the example of the simple two-dimensional state
</p>
<p>space. From the technical point of view, this chapter is about the discussion of some
</p>
<p>of the elementary facts of complex vector spaces. The basic definitions are given in
</p>
<p>Appendix G, Vol. 1.1
</p>
<p>In Chap. 2, we introduced the polarization states which also apply to single
</p>
<p>photons2
</p>
<p>|h &sim;=
(
</p>
<p>1
</p>
<p>0
</p>
<p>)
</p>
<p>; |v &sim;=
(
</p>
<p>0
</p>
<p>1
</p>
<p>)
</p>
<p>|r &sim;= 1&radic;
2
</p>
<p>(
</p>
<p>1
</p>
<p>i
</p>
<p>)
</p>
<p>; |l &sim;= 1&radic;
2
</p>
<p>(
</p>
<p>1
</p>
<p>&minus;i
</p>
<p>) . (4.1)
</p>
<p>These vectors are obviously elements of a two-dimensional complex vector
</p>
<p>space V . In fact, one can convince oneself that all the axioms which apply to a
</p>
<p>vector space are satisfied; see Appendix G, Vol. 1. To put it simply, these axioms
</p>
<p>state in the end that one can perform all operations as usual&mdash;one can add vectors
</p>
<p>1Of course, we treat these technical aspects not as an end in themselves, but because they are
</p>
<p>of fundamental importance for the physical description of natural phenomena in the context of
</p>
<p>quantum mechanics.
2For the notation &sim;=, see Chap. 2.
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4_4
</p>
<p>41</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_4&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_4&amp;domain=pdf</a></div>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4_4">https://doi.org/10.1007/978-3-030-00464-4_4</a></div>
</div>
<div class="page"><p/>
<p>42 4 Complex Vector Spaces and Quantum Mechanics
</p>
<p>and multiply them by a number, subject to the familiar rules such as the distributive
</p>
<p>law, etc. We note in this context that products of numbers and vectors commute, so
</p>
<p>that c &middot; |z = |z &middot; c holds. Although the notation |z &middot; c is perhaps unfamiliar, it is
nevertheless absolutely correct.
</p>
<p>Especially important is the fact that the elements of a vector space can be super-
</p>
<p>posed&mdash;if |x and |y are elements of the vector space, then so is  |x + &micro; |y with
,&micro; &isin; C. In our example of polarization, this means that each vector (except the
zero vector) represents a viable physical state.3 This superposition principle4 is any-
</p>
<p>thing but self-evident&mdash;just think for example of the state space which consists of
</p>
<p>all positions that are reachable in a chess game beginning from the starting position.
</p>
<p>Obviously, here the superposition principle does not hold, since the multiplication of
</p>
<p>such a state with a number or the addition or linear combination of states is simply
</p>
<p>not meaningful. Another example is the phase space of classical mechanics, in which
</p>
<p>the states are denoted by points&mdash;the addition of these points or states is not defined.
</p>
<p>We will repeatedly come across the central importance of the superposition prin-
</p>
<p>ciple in quantum mechanics in the following sections and chapters.
</p>
<p>4.1 Norm, Bra-Ket Notation
</p>
<p>The familiar visual space R3 has the pleasant property that one can calculate the
</p>
<p>length of a vector and the angle between two vectors, namely by means of the scalar
</p>
<p>product. We want to implement these concepts also in the complex vector space, at
</p>
<p>least to some extent.
</p>
<p>Following the familiar formula, the length L of the vector
</p>
<p>(
</p>
<p>a
</p>
<p>b
</p>
<p>)
</p>
<p>would be
</p>
<p>L2 = a2 + b2. But this is wrong, since the vector space is complex. Accordingly,
</p>
<p>due to 1 + i2 = 0, the vector
(
</p>
<p>1
</p>
<p>i
</p>
<p>)
</p>
<p>would have zero length, which evidently makes
</p>
<p>no sense.5 Instead, the correct formula reads
</p>
<p>L = |a|2 + |b|2 = aa&lowast; + bb&lowast;. (4.2)
</p>
<p>Making use of the usual rules of matrix multiplication, we can write this as the
</p>
<p>product of a row vector with a column vector6:
</p>
<p>3Later on, we will meet vector spaces where this is no longer the case; keyword &lsquo;identical particles&rsquo;
</p>
<p>or &lsquo;superselection rules&rsquo;.
4We note that the superposition principle contains three pieces of information: (1) The multiplication
</p>
<p>of a state by a scalar is meaningful. (2) The addition of two states is meaningful. (3) Every linear
</p>
<p>combination of two states is again an element in the vector space.
5As we know, only the zero vector has length zero.
6We recall that &lowast; means complex conjugation.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.1 Norm, Bra-Ket Notation 43
</p>
<p>L2 =
(
</p>
<p>a&lowast; b&lowast;
)
</p>
<p>(
</p>
<p>a
</p>
<p>b
</p>
<p>)
</p>
<p>. (4.3)
</p>
<p>The space of the row vectors is called the dual space to V . One obtains the vector
(
</p>
<p>a&lowast; b&lowast;
)
</p>
<p>from the corresponding column vector by complex conjugation and invert-
</p>
<p>ing the roles of column and row (= transposing, symbol T ). By this process, we obtain
the adjoint,7 which is denoted by a kind of superscripted cross
</p>
<p>(
</p>
<p>a&lowast; b&lowast;
)
</p>
<p>=
(
</p>
<p>a
</p>
<p>b
</p>
<p>)&lowast;T
=
</p>
<p>(
</p>
<p>a
</p>
<p>b
</p>
<p>)&dagger;
</p>
<p>. (4.4)
</p>
<p>The operation is analogously defined for general n &times; m-matrices: the adjoint is
always obtained by complex conjugation and transposition. We note that the adjoint
</p>
<p>is a very important term in quantum mechanics.
</p>
<p>We have denoted the elements of the vector space using the short-hand notation
</p>
<p>| . Analogously, we choose for the elements of the dual space the notation  |. The
symbols are defined as follows:
</p>
<p>|  is called a ket (4.5)
 | is called a bra.
</p>
<p>This is the so-called bra-ket notation (from bracket= bra-(c)-ket), or Dirac notation,
named after P.A.M. Dirac who first introduced it.8 We have for example
</p>
<p>|h&dagger; = h| or
(
</p>
<p>1
</p>
<p>0
</p>
<p>)&dagger;
</p>
<p>=
(
</p>
<p>1 0
)
</p>
<p>r |&dagger; = |r or 1&radic;
2
</p>
<p>(
</p>
<p>1 &minus;i
)&dagger; = 1&radic;
</p>
<p>2
</p>
<p>(
</p>
<p>1
</p>
<p>i
</p>
<p>)
</p>
<p>.
</p>
<p>(4.6)
</p>
<p>With these concepts we can now define the length L of a vector |z as L2 = z| z
(actually, one would expect to write z| |z, but the double bar is omitted). Instead
of length, the term norm is generally used. The designations are   or equivalently
| |. For example, we have
</p>
<p>|h2 = h| h =
(
</p>
<p>1 0
)
</p>
<p>(
</p>
<p>1
</p>
<p>0
</p>
<p>)
</p>
<p>= 1 &middot; 1 + 0 &middot; 0 = 1 (4.7)
</p>
<p>and correspondingly for |r
</p>
<p>7Strictly speaking, there are two adjoints. The one considered here is called Hermitian adjoint; it
</p>
<p>applies so to say in non-relativistic considerations. In the relativistic case, there is another kind,
</p>
<p>called Dirac adjoint which is defined differently. The bulk of the book is devoted to non-relativistic
</p>
<p>considerations; here adjoint means always Hermitian adjoint.
8In the bra-ket notation, one cannot identify the dimension of the corresponding vector space (the
</p>
<p>same holds true for the familiar vector notations v or &minus;&rarr;v , by the way). If necessary, this information
must be given separately.</p>
<p/>
</div>
<div class="page"><p/>
<p>44 4 Complex Vector Spaces and Quantum Mechanics
</p>
<p>|r2 = r | r =
1
</p>
<p>2
</p>
<p>(
</p>
<p>1 &minus;i
)
</p>
<p>(
</p>
<p>1
</p>
<p>i
</p>
<p>)
</p>
<p>=
1
</p>
<p>2
(1 &middot; 1 &minus; i &middot; i) = 1. (4.8)
</p>
<p>Both vectors have the length 1. Such vectors are called unit vectors; they are
</p>
<p>normalized. The term z| z is a scalar product (also called inner product or dot
product); more about this topic is to be found in Chap. 11 and in Appendix G, Vol. 1.
</p>
<p>We remark that we can use an equals sign in (4.7) and (4.8) instead of &sim;=, since scalar
products are independent of the representation.
</p>
<p>A comment on the nomenclature: A complex vector space in which a scalar
</p>
<p>product is defined is called a unitary space.
</p>
<p>4.2 Orthogonality, Orthonormality
</p>
<p>Now that we know how to calculate the length of a vector, the question of the angle
</p>
<p>between two vectors still remains open. First, we note that we can also form inner
</p>
<p>products of different vectors, for example
</p>
<p>v| r =
1
&radic;
</p>
<p>2
</p>
<p>(
</p>
<p>0 1
)
</p>
<p>(
</p>
<p>1
</p>
<p>i
</p>
<p>)
</p>
<p>=
i
&radic;
</p>
<p>2
(4.9)
</p>
<p>Note: As with any scalar product, a| b is a (generally complex) number. For the
adjoint of an inner product, for example, we have
</p>
<p>(v| r)&dagger; = r | v =
1
&radic;
</p>
<p>2
</p>
<p>(
</p>
<p>1 &minus;i
)
</p>
<p>(
</p>
<p>0
</p>
<p>1
</p>
<p>)
</p>
<p>= &minus;
i
&radic;
</p>
<p>2
. (4.10)
</p>
<p>So to form the adjoint of an expression we follow the procedure: (1) A number is
</p>
<p>replaced by its complex conjugate, c&dagger; = c&lowast;; (2) A ket is replaced by the corresponding
bra and vice versa; (3) The order of terms is reversed, for example a| b&dagger; = a| b&lowast; =
b| a.
</p>
<p>Regarding the question of the angle, in the following only one particular angle
</p>
<p>plays a role (apart from the angle zero), namely the right angle. One says that two
</p>
<p>vectors are orthogonal if their scalar product vanishes (this convention of terminol-
</p>
<p>ogy is valid also for non-intuitive higher-dimensional complex vector spaces). An
</p>
<p>example:
</p>
<p>v| h =
(
</p>
<p>0 1
)
</p>
<p>(
</p>
<p>1
</p>
<p>0
</p>
<p>)
</p>
<p>= 0. (4.11)
</p>
<p>More generally and in short form: a| b = 0 &harr; |a &perp; |b.
Note that the zero vector is orthogonal to itself and to all other vectors. Just as in
</p>
<p>the trivial solution of the SEq, it does not describe a physical state and is therefore
</p>
<p>in general not taken into account in considerations concerning orthogonality, etc.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Orthogonality, Orthonormality 45
</p>
<p>Systems of vectors, all of which are normalized and pairwise orthogonal, play
</p>
<p>a special role.9 Such a system of vectors is called an orthonormal system (ONS).
</p>
<p>Two-dimensional examples are the systems {|h, |v} and {|r, |l}; an example from
three-dimensional visual space R3 are the three unit vectors lying on the coordinate
</p>
<p>axes. The general formulation reads: {|n, n = 1, 2, . . .} is an ONS if and only if
</p>
<p>i |  j
&rang;
</p>
<p>= i j (4.12)
</p>
<p>where the Kronecker delta (Kronecker symbol) is defined as usual by
</p>
<p>i j =
{
</p>
<p>1
</p>
<p>0
for
</p>
<p>i = j
i = j . (4.13)
</p>
<p>4.3 Completeness
</p>
<p>We can write any vector |z from our two-dimensional complex vector space as
</p>
<p>|z = a |h + b |v (4.14)
</p>
<p>where |h and |v are orthonormal. Due to h| z = a h| h+b h| v = a &middot;1+b &middot;0
= a (analogously for v| z), this property leads to
</p>
<p>h| z = a and v| z = b. (4.15)
</p>
<p>We insert this and find10
</p>
<p>|z = h| z |h + v| z |v
= |h h| z + |v v| z (4.16)
= {|h h| + |v v|} |z ,
</p>
<p>or in other words (by comparing the left and right sides)11:
</p>
<p>9In the two-dimensional vector space that we are currently addressing, such a system consists of
</p>
<p>course of two vectors; as stated above, the zero vector is excluded a priori from consideration.
10We repeat the remark that for products of numbers and vectors, it holds that c &middot; |z = |z &middot; c.
Because h| z is a number, we can therefore write h| z |h as |h h| z.
11In equations such as (4.17), the 1 on the right side is not necessarily the number 1, but is generally
</p>
<p>something that works like a multiplication by 1, i.e. a unit operator. For instance, this is the unit
</p>
<p>matrix when working with vectors. The notation 1 for the unit operator (which implies writing
</p>
<p>simply 1 instead of
</p>
<p>(
</p>
<p>1 0
</p>
<p>0 1
</p>
<p>)
</p>
<p>, for instance) is of course quite lax. On the other hand, as said before,
</p>
<p>the effect of multiplication by the unit operator and by 1 is identical, so that the small inaccuracy
</p>
<p>is generally accepted in view of the economy of notation. If necessary, &lsquo;one knows&rsquo; that 1 means
</p>
<p>the unit operator. But there are also special notations for it, such as E, In (where n indicates the</p>
<p/>
</div>
<div class="page"><p/>
<p>46 4 Complex Vector Spaces and Quantum Mechanics
</p>
<p>|h h| + |v v| = 1. (4.17)
</p>
<p>A term like |x y| is called a dyadic product. To get an idea of the meaning of
such products, we choose the representation of row and column vectors. With
</p>
<p>|h &sim;=
(
</p>
<p>1
</p>
<p>0
</p>
<p>)
</p>
<p>; h| &sim;=
(
</p>
<p>1 0
)
</p>
<p>(4.18)
</p>
<p>we have, according to the rules of matrix multiplication,
</p>
<p>|h h| &sim;=
(
</p>
<p>1
</p>
<p>0
</p>
<p>)
</p>
<p>(
</p>
<p>1 0
)
</p>
<p>=
(
</p>
<p>1 0
</p>
<p>0 0
</p>
<p>)
</p>
<p>. (4.19)
</p>
<p>As can be seen, dyadic products are matrices or, more generally, operators which
</p>
<p>can be applied to states and usually change them.
</p>
<p>With
</p>
<p>|v v| &sim;=
(
</p>
<p>0
</p>
<p>1
</p>
<p>)
</p>
<p>(
</p>
<p>0 1
)
</p>
<p>=
(
</p>
<p>0 0
</p>
<p>0 1
</p>
<p>)
</p>
<p>(4.20)
</p>
<p>it follows that
</p>
<p>|h h| + |v v| &sim;=
(
</p>
<p>1 0
</p>
<p>0 1
</p>
<p>)
</p>
<p>(unit matrix). (4.21)
</p>
<p>This equation, or (4.17), indicates that the ONS {|h, |v} is complete, i.e. it
spans the whole space. Consequently, {|h, |v} is a complete orthonormal system
(CONS). Another one is, for example, {|r, |l} (see exercises). The terminology
is transferred readily to n-dimensional vector spaces: A CONS consists of states
</p>
<p>{|n, n = 1, 2, . . .} which are normalized and pairwise orthogonal (orthonormal-
ity), and which span the whole space (completeness)12:
</p>
<p>n| m = nm (orthonormality)
&sum;
</p>
<p>n |n n| = 1 (completeness)
. (4.22)
</p>
<p>With the methods developed so far, we can easily calculate the fractions of
</p>
<p>vertically&mdash;and horizontally-polarized light which are found e.g. in right circularly-
</p>
<p>polarized light. Of course, the example is simple enough to read off the answer
</p>
<p>directly from (4.1). But here, we are concerned with setting up a procedure that
</p>
<p>dimension) and others. An analogous remark applies to the zero operator. By the way, we recall
</p>
<p>that in the case of vectors we write quite naturally
&minus;&rarr;
a = 0 and not &minus;&rarr;a = &minus;&rarr;0 .
</p>
<p>12For the summation we use almost exclusively the abbreviation
&sum;
</p>
<p>n (instead of
&sum;&infin;
</p>
<p>n=1 or
&sum;N
</p>
<p>n=1
etc.). In the shorthand notation, the range of values of n must follow from the context of the problem
</p>
<p>at hand, if necessary.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.3 Completeness 47
</p>
<p>works in any space. Basically, it is a multiplication by 1&mdash;but with 1 in a special
</p>
<p>notation. We have:
</p>
<p>|r = 1 &middot; |r 4.17= (|h h| + |v v|) &middot; |r
</p>
<p>= |h h| r + |v v| r =
1
&radic;
</p>
<p>2
|h +
</p>
<p>i
&radic;
</p>
<p>2
|v, (4.23)
</p>
<p>where we have used h| r = 1&radic;
2
</p>
<p>and v| r = i&radic;
2
</p>
<p>in the final step.
</p>
<p>With (4.23), we have formulated the state |r in the basis {|h, |v}. This being
the case, the coefficients 1/
</p>
<p>&radic;
2 and i/
</p>
<p>&radic;
2 are none other than the coordinates of |r
</p>
<p>with respect to |h and |v. However, the term coordinate is used quite rarely in
quantum mechanics; instead, one speaks of projection,13 which is perhaps an even
</p>
<p>more descriptive term.
</p>
<p>For higher dimensions, the following applies: Given a vector space V and a CONS
</p>
<p>{|n, n = 1, 2, . . .} &isin; V , any vector | &isin; V can be represented as
</p>
<p>| = 1 &middot; | =
&sum;
</p>
<p>n
</p>
<p>|n n | =
&sum;
</p>
<p>n
</p>
<p>cn |n ; cn = n | &isin; C. (4.24)
</p>
<p>The coefficients (coordinates) cn are the projections of | onto the basis vectors
|n.
</p>
<p>4.4 Projection Operators, Measurement
</p>
<p>4.4.1 Projection Operators
</p>
<p>As mentioned above, expressions like |h h| or |n n| act on states and are there-
fore operators. They are different from those that we met up with in the analytical
</p>
<p>approach of Chap. 3 (e.g. the derivative &part;
&part;x
</p>
<p>), but this is actually not surprising, since
</p>
<p>the states defined in the algebraic and the analytical approaches are quite different.
</p>
<p>We note, however, that there is a structure common to both approaches: in each case
</p>
<p>the states are elements of a vector space, and changes of these states are produced
</p>
<p>by operators.
</p>
<p>The term |h h| is a particularly simple example of a projection operator (or
projector). If P is a projection operator, we have14
</p>
<p>P2 = P. (4.25)
</p>
<p>13For the connection between inner product and projection, see Appendix F, Vol. 1.
14As we shall see in Chap. 13, a projection operator in quantum mechanics must meet a further
</p>
<p>condition (self-adjointness).</p>
<p/>
</div>
<div class="page"><p/>
<p>48 4 Complex Vector Spaces and Quantum Mechanics
</p>
<p>In fact, in the specific example P = |h h|, we have, due to the normalization
h |h = 1, the equality
</p>
<p>P2 = |h h |h h| = |h h| = P. (4.26)
</p>
<p>A further example of a projection operator is |h h| + |v v|, namely the projection
onto the total space (because of |h h| + |v v| = 1).
</p>
<p>The property P2 = P is actually very intuitive: if one filters out (= projects) a
component of a total state by means of P , then a second projection does not change
</p>
<p>this component. In the matrix representation (4.19) with P &sim;=
(
</p>
<p>1 0
</p>
<p>0 0
</p>
<p>)
</p>
<p>, this reads
</p>
<p>as follows:
</p>
<p>(
</p>
<p>1 0
</p>
<p>0 0
</p>
<p>)(
</p>
<p>a
</p>
<p>b
</p>
<p>)
</p>
<p>=
(
</p>
<p>a
</p>
<p>0
</p>
<p>)
</p>
<p>;
(
</p>
<p>1 0
</p>
<p>0 0
</p>
<p>)(
</p>
<p>1 0
</p>
<p>0 0
</p>
<p>)(
</p>
<p>a
</p>
<p>b
</p>
<p>)
</p>
<p>=
(
</p>
<p>a
</p>
<p>0
</p>
<p>)
</p>
<p>. (4.27)
</p>
<p>4.4.1.1 Projection Operators and Measurement
</p>
<p>Projection operators gain special importance from the fact that they can be used for the
</p>
<p>modelling of the measurement process. To see this, we start with a simple example,
</p>
<p>namely a right circularly-polarized state. Using (4.1), we write it as a superposition
</p>
<p>of linearly-polarized states:
</p>
<p>|r =
|h + i |v
</p>
<p>&radic;
2
</p>
<p>. (4.28)
</p>
<p>We send this state |r through an analyzer which can detect linearly-polarized states,
e.g. a polarizing beam splitter (PBS). Before the measurement, we cannot say with
</p>
<p>certainty which one of the two linearly-polarized states we will measure. According
</p>
<p>to the considerations of Chap. 2, we can specify only the probabilities of measuring
</p>
<p>one of the states&mdash;in our example they are
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>1&radic;
2
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>2
</p>
<p>= 1
2
</p>
<p>and
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>i&radic;
2
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>2
</p>
<p>= 1
2
. We extend this
</p>
<p>idea to the more general state
</p>
<p>|z = a |h + b |v; |a|2 + |b|2 = 1, (4.29)
</p>
<p>for which the probabilities of obtaining the vertically or horizontally polarized state
</p>
<p>are given by |b|2 or |a|2.
After the measurement, we have a different state from before the measurement,
</p>
<p>namely either |h or |v.15 Since states can be changed only by the action of oper-
ators, we have to model this transition by an operator. This modelling should be
</p>
<p>as simple and universal as possible, in order to be independent of the specific
</p>
<p>experimental details. Let us assume that we have the state |h after the measurement.
</p>
<p>15In other words, due to the process of measuring, a superposition such as |z = a |h + b |v
&lsquo;collapses&rsquo; e.g. into the state |h.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Projection Operators, Measurement 49
</p>
<p>Then we can describe this process by applying |h h| to |z, i.e. the projection of |z
onto |h, which leads to |h h |z = a |h (with an analogous formulation for |v).
As a result of this &lsquo;operation&rsquo;, we obtain the desired state |h, but multiplied by a
factor a, the absolute square of which gives the probability of obtaining that state in
</p>
<p>a measurement.
</p>
<p>Hence, we can model the measurement process |z &rarr; |h as follows:
</p>
<p>|z
before measurement
</p>
<p>= a |h + b |v projection&rarr; |h h| (a |h + b |v)
</p>
<p>= a |h normalization&rarr;
|a|
a
</p>
<p>|h
after measurement
</p>
<p>, (4.30)
</p>
<p>where we obtain the final result with probability |a|2. Occasionally, it is assumed
that one can set the normalization factor equal to 1 after the measurement, which
</p>
<p>formally means |a|
a
= 1. As we said above, an analogous formulation applies to the
</p>
<p>measurement result |v.
</p>
<p>4.4.1.2 Extension to Higher Dimensions
</p>
<p>The generalization to dimensions N &gt; 2 is straightforward. Before the measure-
</p>
<p>ment, the state is a superposition of different states, i.e. | =
&sum;
</p>
<p>cn |n, where
{|n, n = 1, . . .} is a CONS. After the measurement, we have just one of the
states, e.g. |i . The measurement process is modelled by the projection operator
Pi = |i  i |. With a slightly different notation, we have
</p>
<p>|before =
&sum;
</p>
<p>n
</p>
<p>cn |n &rarr; |i  i |before = |i  i |
&sum;
</p>
<p>n
</p>
<p>cn |n = ci |i  .
</p>
<p>(4.31)
</p>
<p>The probability of measuring this state is thus given by |ci |2 = |i | |2. After the
measurement, we have again a normalized state, namely
</p>
<p>|after, normalized =
Pi |
|Pi ||
</p>
<p>=
ci |i 
|ci |
</p>
<p>. (4.32)
</p>
<p>We emphasize that the measurement process itself is not modelled, but only the
</p>
<p>situation immediately before and after the measurement. As an example, the situation
</p>
<p>is sketched in Fig. 4.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>50 4 Complex Vector Spaces and Quantum Mechanics
</p>
<p>Fig. 4.1 Example sketch of
</p>
<p>the coefficients cn in (4.31)
</p>
<p>and (4.32) before the
</p>
<p>measurement (blue) and after
</p>
<p>the measurement (red)
</p>
<p>4.4.1.3 The Measurement Problem
</p>
<p>Of course one may ask at this point, which mechanism picks out precisely the state
</p>
<p>|i  from the superposition
&sum;
</p>
<p>n cn |n, and not some other state. There is still no
satisfactory answer to this question in spite of the advanced age of quantum mechan-
</p>
<p>ics. In fact, it is still an open problem, called the measurement problem. It is perhaps
</p>
<p>the conceptual problem of quantum mechanics. We shall meet it repeatedly in the
</p>
<p>following chapters. The different interpretations of quantum mechanics, at which we
</p>
<p>look closer in the last chapter of Vol. 2, are in some sense simply different ways of
</p>
<p>dealing with the measurement problem.
</p>
<p>We note that the measurement problem has nothing to do with the extension to
</p>
<p>arbitrary dimensions, but applies even to the simplest systems. An example already
</p>
<p>treated in Chap. 2 and above is the right circularly-polarized photon, which we exam-
</p>
<p>ine with respect to its possible linear polarization states. If we send
</p>
<p>|r =
|h + i |v
</p>
<p>&radic;
2
</p>
<p>(4.33)
</p>
<p>through e.g. a PBS, we find either a horizontal or a vertical linearly-polarized photon,
</p>
<p>with probabilities 1
2
</p>
<p>in each case. Before the measurement, we cannot say which
</p>
<p>polarization we will obtain.
</p>
<p>The key question is whether there is in principle such a selection mechanism.
</p>
<p>We have two alternatives. The first one: Yes, there is such a mechanism, although
</p>
<p>we do not know either the process (at present?) or the variables which it acts upon,
</p>
<p>the so-called hidden variables. If we knew these, we could describe the selection
</p>
<p>process that occurs during the measurement without any use of probabilities. The
</p>
<p>other alternative: No, there is no such mechanism. The selection of states in the
</p>
<p>course of the measurement is purely random&mdash;one speaks of objective chance.
</p>
<p>The choice of the alternative in question must be decided experimentally. We have
</p>
<p>already noted in Chap. 2 that, all in all, relevant experiments do not support the exis-
</p>
<p>tence of hidden variables. Therefore, we hereafter assume the existence of objective
</p>
<p>chance, but we will take up the measurement problem again in later chapters.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Projection Operators, Measurement 51
</p>
<p>Fig. 4.2 Measurement of
</p>
<p>the linear polarization of a
</p>
<p>photon
</p>
<p>4.4.2 Measurement and Eigenvalues
</p>
<p>To arrive at a more compact description of the measurement, we imagine that, after
</p>
<p>the PBS, we have a detector which is connected to a display. For vertical polarization,
</p>
<p>a pointer shows &lsquo;&minus;1&rsquo;, for horizontal polarization &lsquo;+1&rsquo;; see Fig. 4.2. This means that
after the measurement on the state |z = a |h + b |v, the value &lsquo;&minus;1&rsquo; or &lsquo;+1&rsquo; is
displayed with the probabilities |b|2 or |a|2.
</p>
<p>We now want to describe the measured physical quantity &lsquo;horizontal/vertical
</p>
<p>polarization&rsquo;, encoded by&plusmn;1. To this end, we choose a linear combination of the pro-
jection operators |h h| and |v v|. The simplest non-trivial combination is clearly
the polarization operator PL
</p>
<p>PL = |h h| &minus; |v v| &sim;=
(
</p>
<p>1 0
</p>
<p>0 &minus;1
</p>
<p>)
</p>
<p>= z, (4.34)
</p>
<p>where z is one of the three Pauli matrices (more on the Pauli matrices in the exer-
</p>
<p>cises). We note that the Pauli matrices, and thus PL , are not projection operators;
</p>
<p>here the P stands for &lsquo;polarization&rsquo;.
</p>
<p>The properties which are relevant for the measurement follow now by consid-
</p>
<p>ering the eigenvalue problem PL |z = &micro; |z (where we have now introduced the
eigenvalue problem, treated in the analytical approach already in Chap. 3, into the
</p>
<p>algebraic approach considered here). As is easily verified (see exercises), PL has
</p>
<p>the eigenvalues&micro; = +1 and&micro; = &minus;1 and the eigenvectors |z1 = |h and |z&minus;1 = |v.
This means that the eigenvectors describe the possible states and the eigenvalues the
</p>
<p>possible pointer positions (measurement results) after the measurement&mdash;the pointer
</p>
<p>position +1 tells us, for example, that after the measurement we have the state |h.
Similarly, we can imagine a measuring apparatus for circular polarization, in
</p>
<p>which the physical quantity &lsquo;right/left circular polarization&rsquo; is encoded by &plusmn;1. We
describe this by
</p>
<p>PC = |r r | &minus; |l l| &sim;=
(
</p>
<p>0 &minus;i
i 0
</p>
<p>)
</p>
<p>= y . (4.35)</p>
<p/>
</div>
<div class="page"><p/>
<p>52 4 Complex Vector Spaces and Quantum Mechanics
</p>
<p>The eigenvalues of PC are also &plusmn;1, where the eigenvalue +1 belongs to the
eigenvector |r and the eigenvalue &minus;1 to the eigenvector |l.
</p>
<p>Finally, we treat a linear polarization state rotated by 45. For the rotated state,
</p>
<p>we have (see exercises):
</p>
<p>
</p>
<p>h&prime;
&rang;
</p>
<p>=
|h + |v
</p>
<p>&radic;
2
</p>
<p>;

</p>
<p>v&prime;
&rang;
</p>
<p>=
&minus; |h + |v
</p>
<p>&radic;
2
</p>
<p>. (4.36)
</p>
<p>We describe the corresponding measuring apparatus by the operator
</p>
<p>PL &prime; =

</p>
<p>h&prime;
&rang; &lang;
</p>
<p>h&prime;

</p>
<p>&minus;

</p>
<p>v&prime;
&rang; &lang;
</p>
<p>v&prime;

</p>
<p> &sim;=
(
</p>
<p>0 1
</p>
<p>1 0
</p>
<p>)
</p>
<p>= x . (4.37)
</p>
<p>This operator has the eigenvalues &plusmn;1, also. (For the determination of the eigenvalues
and eigenvectors of the three Pauli matrices, see the exercises.)
</p>
<p>We learn from these three examples that the information about possible mea-
</p>
<p>surement results lies in the eigenvalues of certain operators. For this purpose we
</p>
<p>have constructed three examples that yield information about certain polarization
</p>
<p>states. The question of how to extend these findings and how to represent general
</p>
<p>physically-measurable variables will be treated in the following chapters.
</p>
<p>4.4.3 Summary
</p>
<p>We summarize with the help of the example |z = a |h+ b |v, with |a|2 +|b|2 = 1
and ab = 0. Before the measurement, we can say only that: (1) The pointer will
display position &lsquo;+1&rsquo; with probability |a|2 = |h |z|2, and position &lsquo;&minus;1&rsquo; with prob-
ability |b|2; and (2) Just one of the eigenvalues of PL = |h h| &minus; |v v| will be
realized, with the corresponding probability. After the measurement, one of the two
</p>
<p>eigenvalues is realized (the pointer displays one of the two possible values), and the
</p>
<p>photon is in the corresponding state (the associated normalized eigenstate of PL ),
</p>
<p>e.g. h |z|h |z| |h. We cannot discern which mechanism leads to this choice, but can only
specify the probabilities for the possible results. The process is irreversible&mdash;the
</p>
<p>initial superposition no longer exists, and it cannot be reconstructed from the mea-
</p>
<p>surement results from a single photon.16 This is possible at most by measuring an
</p>
<p>ensemble of photons in the state a |h+b |v many times. From the relative frequen-
cies of occurrence of the pointer values &plusmn;1, we can infer the quantities |a|2 and |b|2.
</p>
<p>16In order to make it clear once more: If, for example, we measure an arbitrarily-polarized state
</p>
<p>|z = a |h + b |v with |a|2 + |b|2 = 1 and ab = 0, we find with probability |a|2 a horizontal
linearly-polarized photon. This does not permit the conclusion that the photon was in that state
</p>
<p>before the measurement. It simply makes no sense in this case to speak of a of a well-defined value
</p>
<p>of the linear polarization (+1 or &minus;1) before the measurement.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Exercises 53
</p>
<p>4.5 Exercises
</p>
<p>1. Find examples for state spaces which
</p>
<p>(a) have the structure of a vector space,
</p>
<p>(b) do not have the structure of a vector space.
</p>
<p>2. Polarization: Determine the length of the vector 1&radic;
2
</p>
<p>(
</p>
<p>1
</p>
<p>i
</p>
<p>)
</p>
<p>.
</p>
<p>3. Given y| = i
(
</p>
<p>1 &minus;2
)
</p>
<p>and z| =
(
</p>
<p>2 i
)
</p>
<p>, determine y| z.
4. The Pauli matrices are
</p>
<p>x =
(
</p>
<p>0 1
</p>
<p>1 0
</p>
<p>)
</p>
<p>; y =
(
</p>
<p>0 &minus;i
i 0
</p>
<p>)
</p>
<p>; z =
(
</p>
<p>1 0
</p>
<p>0 &minus;1
</p>
<p>)
</p>
<p>(4.38)
</p>
<p>In addition to x ,y,z , the notation 1,2,3 is also common.
</p>
<p>(a) Show that 2i = 1, i = x, y, z.
</p>
<p>(b) Determine the commutators
[
</p>
<p>i , j
]
</p>
<p>= i j &minus;  ji and the anticommuta-
tors
</p>
<p>{
</p>
<p>i , j
}
</p>
<p>= i j +  j (i = j).
</p>
<p>(c) Calculate the eigenvalues and eigenvectors for each Pauli matrix.
</p>
<p>5. Determine the eigenvalues and eigenvectors of the matrix
</p>
<p>M =
(
</p>
<p>1 4
</p>
<p>2 &minus;1
</p>
<p>)
</p>
<p>. (4.39)
</p>
<p>Normalize the eigenvectors. Are they orthogonal?
</p>
<p>6. Given the CONS {|a1, |a2}, determine the eigenvalues and eigenvectors of the
operator
</p>
<p>M = |a1 a1| &minus; |a2 a2| . (4.40)
</p>
<p>7. Given a CONS {|n} and a state | =
&sum;
</p>
<p>n
</p>
<p>cn |n, cn &isin; C, calculate the
</p>
<p>coefficients cn .
</p>
<p>8. Show in bra-ket notation: The system {|r, |l} is a CONS. Use the fact that
{|h, |v} is a CONS.
</p>
<p>9. Given the operator |h r |:
</p>
<p>(a) Is it a projection operator?
</p>
<p>(b) How does the operator appear in the representation (4.1)?
</p>
<p>(c) Given the state |z with the representation |z &sim;=
(
</p>
<p>z1
z2
</p>
<p>)
</p>
<p>, apply the operator
</p>
<p>|h r | to this state (calculation making use of the representation).</p>
<p/>
</div>
<div class="page"><p/>
<p>54 4 Complex Vector Spaces and Quantum Mechanics
</p>
<p>(d) Use the concrete representation to prove the equality
</p>
<p>(|h r | z)&dagger; = z| r h| . (4.41)
</p>
<p>10. We choose the following representation for the states |h and |v:
</p>
<p>|h &sim;=
1
&radic;
</p>
<p>2
</p>
<p>(
</p>
<p>i
</p>
<p>1
</p>
<p>)
</p>
<p>; |v &sim;=
a
</p>
<p>&radic;
2 |a|
</p>
<p>(
</p>
<p>1
</p>
<p>i
</p>
<p>)
</p>
<p>. (4.42)
</p>
<p>(a) Show that the representing vectors form a CONS.
</p>
<p>(b) Determine |r and |l in this representation. Specialize to the cases of a = 1,
&minus;1, i,&minus;i .
</p>
<p>11. Show that the three vectors
</p>
<p>a =
1
&radic;
</p>
<p>2
</p>
<p>
</p>
<p>
</p>
<p>1
</p>
<p>i
</p>
<p>0
</p>
<p>
</p>
<p>; b =
</p>
<p>
</p>
<p>
</p>
<p>0
</p>
<p>0
</p>
<p>1
</p>
<p>
</p>
<p>; c = &minus;
1
&radic;
</p>
<p>2
</p>
<p>
</p>
<p>
</p>
<p>1
</p>
<p>&minus;i
0
</p>
<p>
</p>
<p> (4.43)
</p>
<p>form a CONS. Do the same for
</p>
<p>a =
1
&radic;
</p>
<p>2
</p>
<p>
</p>
<p>
</p>
<p>1
</p>
<p>0
</p>
<p>&minus;1
</p>
<p>
</p>
<p>; b =
1
</p>
<p>2
</p>
<p>
</p>
<p>
</p>
<p>1&radic;
2
</p>
<p>1
</p>
<p>
</p>
<p>; c =
1
</p>
<p>2
</p>
<p>
</p>
<p>
</p>
<p>1
</p>
<p>&minus;
&radic;
</p>
<p>2
</p>
<p>1
</p>
<p>
</p>
<p>. (4.44)
</p>
<p>12. A three-dimensional problem: Given the CONS {|u, |v, |w} and the operator17
</p>
<p>L = |v u| + (|u + |w) v| + |v w|. (4.45)
</p>
<p>(a) Determine the eigenvalues and eigenvectors of L .
</p>
<p>(b) Show that the three eigenvectors form a CONS.
</p>
<p>17Essentially, this operator is the x component of the orbital angular momentum operator for the
</p>
<p>angular momentum 1; see Chap. 16 Vol. 2.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 5
</p>
<p>Two Simple Solutions of the Schr&ouml;dinger
</p>
<p>Equation
</p>
<p>The infinite potential well is the simplest model case for a discrete energy spectrum. We see
</p>
<p>that the eigenfunctions form a complete orthonormal system. Free motion is the simplest
</p>
<p>model case for a continuous spectrum. In both cases, we solve the initial-value problem. We
</p>
<p>make our first contact (within the analytical approach) with the interpretation of probability
</p>
<p>and measurements.
</p>
<p>This chapter deals with the solutions of the SEq for two simple but important one-
</p>
<p>dimensional systems. First, we consider the infinite potential well as a simple model
</p>
<p>of a bounded system, then force-free unlimited motion as a simple model of an
</p>
<p>unbounded system. Here, &lsquo;bounded motion&rsquo; means basically that the system is con-
</p>
<p>fined to a finite region, in contrast to unlimited motion.
</p>
<p>The two examples in this chapter are of interest not only in view of our current
</p>
<p>state of knowledge in this course, but also because they provide further information.
</p>
<p>At the same time, they are mathematically so simple that they are treated as specific
</p>
<p>cases even at the school level. Among other things, we will see below that the
</p>
<p>striking differences between the two solutions can be attributed to &lsquo;just&rsquo; their different
</p>
<p>boundary conditions.1
</p>
<p>5.1 The Infinite Potential Well
</p>
<p>We imagine a ping-pong ball which bounces back and forth between two fixed,
</p>
<p>infinitely rigid walls, whereby friction and gravity are switched off. We can represent
</p>
<p>the two walls by infinitely high potential barriers at x = 0 and x = a; for 0 &lt; x &lt; a,
the potential energy is zero. Classically, the ping-pong ball can have any speed or
</p>
<p>kinetic energy (it has no potential energy). This means that in Fig. 5.1, the ball can
</p>
<p>fly at any height (the height in the figure corresponds to the ball&rsquo;s kinetic energy, not
</p>
<p>to its position!).
</p>
<p>1See also the exercises for Chap. 3.
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4_5
</p>
<p>55</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_5&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_5&amp;domain=pdf</a></div>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4_5">https://doi.org/10.1007/978-3-030-00464-4_5</a></div>
</div>
<div class="page"><p/>
<p>56 5 Two Simple Solutions of the Schr&ouml;dinger Equation
</p>
<p>Fig. 5.1 Infinite potential
</p>
<p>well. In classical mechanics
</p>
<p>(left), all energies are
</p>
<p>allowed. In quantum
</p>
<p>mechanics, only discrete
</p>
<p>energy levels are allowed
</p>
<p>In contrast, the quantum-mechanical ping-pong ball can occupy only certain
</p>
<p>&lsquo;energy levels&rsquo;, as we will see below.2 In other words, its energy is quantized. This
</p>
<p>system, which represents the prototype of a bounded problem in quantum mechanics,
</p>
<p>is called the infinite potential well:
</p>
<p>V =
{
</p>
<p>0 for 0 &lt; x &lt; a
</p>
<p>&infin; otherwise . (5.1)
</p>
<p>5.1.1 Solution of the Schr&ouml;dinger Equation, Energy
</p>
<p>Quantization
</p>
<p>The stationary SEq is given for 0 &lt; x &lt; a by:
</p>
<p>E (x) = &minus;

</p>
<p>2
</p>
<p>2m
&prime;&prime; (x) (5.2)
</p>
<p>Outside the infinite potential well and at its edges (walls), the wavefunction vanishes
</p>
<p>identically
</p>
<p>(x) &equiv; 0 for x &le; 0 and a &le; x . (5.3)
</p>
<p>Hence, the problem is described by (5.2) with the boundary conditions3
</p>
<p>2 Indeed, the quantum-mechanical ping-pong ball is quite a peculiar ball, namely an object described
</p>
<p>by a standing wave.
3A conclusive argument for these boundary conditions is given in Chap. 15, Vol. 2. For now, one
</p>
<p>might think (in an intuitive analogy to the wavefunction) of a rope which is clamped at both ends
</p>
<p>(although the question remains open as to what a rope has to do with this quantum-mechanical
</p>
<p>situation). Alternatively, one might consider a continuity requirement for the wavefunction at the
</p>
<p>walls to be plausible.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 The Infinite Potential Well 57
</p>
<p>(0) = 0; (a) = 0. (5.4)
</p>
<p>We write (5.2) in the form
</p>
<p>&prime;&prime; = &minus;
2m E
</p>
<p>2
. (5.5)
</p>
<p>In order to arrive at a more compact form, we make use of the de Broglie relation
</p>
<p>p = k and obtain
</p>
<p>E =
p2
</p>
<p>2m
=
</p>
<p>
2
</p>
<p>2m
k2; (5.6)
</p>
<p>it then follows that
</p>
<p>&prime;&prime; = &minus;k2. (5.7)
</p>
<p>This is the familiar differential equation for the classical harmonic oscillator, with
</p>
<p>the solutions
</p>
<p> = Aeikx + Be&minus;ikx ; 0 &lt; x &lt; a; (A, B) = (0, 0) , (5.8)
</p>
<p>where we assume without loss of generality that k &gt; 0.4 At this point, the energy E
</p>
<p>(and hence k) is not yet determined; we will find them in the next step.
</p>
<p>The solution (5.8) contains the three free variables A, B and k, two of which can
</p>
<p>be fixed by the boundary conditions:
</p>
<p>0 =  (0) = A + B
0 =  (a) = Aeika + Be&minus;ika . (5.9)
</p>
<p>This is a homogeneous system of equations for A and B. It follows that:
</p>
<p>A = &minus;B
0 = Aeika &minus; Ae&minus;ika . (5.10)
</p>
<p>This yields for A = 05:
eika &minus; e&minus;ika = 0, (5.11)
</p>
<p>or, equivalently6:
</p>
<p>sin ka = 0. (5.12)
</p>
<p>Only when this condition is met does the system (5.9) have a nontrivial (i.e. physical)
</p>
<p>solution. Equation (5.12) can be satisfied only for certain values of k, namely ka =
n, n &isin; N. Thus, there exist only discrete values for k:
</p>
<p>4We have k = 0, since for k = 0, only the trivial solution is obtained.
5For A = 0, we would obtain the trivial solution.
6We recall that sin x = ei x&minus;e&minus;i x
</p>
<p>2i
.</p>
<p/>
</div>
<div class="page"><p/>
<p>58 5 Two Simple Solutions of the Schr&ouml;dinger Equation
</p>
<p>k =
{
</p>
<p>
</p>
<p>a
,
</p>
<p>2
</p>
<p>a
,
</p>
<p>3
</p>
<p>a
,
</p>
<p>4
</p>
<p>a
. . .
</p>
<p>}
</p>
<p>= {kn} ; kn =
n
</p>
<p>a
; n &isin; N. (5.13)
</p>
<p>Accordingly, there are countably infinitely many solutions (= eigenfunctions) of the
SEq, namely
</p>
<p>n (x) = 2i A sin kn x . (5.14)
</p>
<p>Due to the linearity of the SEq, one can choose the amplitude freely. With the choice7
</p>
<p>2i A =
&radic;
</p>
<p>2
</p>
<p>a
, (5.15)
</p>
<p>we arrive at
</p>
<p>n (x) =
&radic;
</p>
<p>2
</p>
<p>a
sin kn x . (5.16)
</p>
<p>Because of the relation E = 2k2
2m
</p>
<p>, the energy can also assume only discrete values.
</p>
<p>These energy eigenvalues are given by:
</p>
<p>En =

</p>
<p>2
</p>
<p>2m
k2n =
</p>
<p>
2
</p>
<p>2m
</p>
<p>2
</p>
<p>a2
n2. (5.17)
</p>
<p>Since the SEq (5.2) has solutions only for certain eigenfunctions n or energy levels
</p>
<p>En , one often writes the eigenvalue problem from the outset as
</p>
<p>Enn (x) = &minus;

</p>
<p>2
</p>
<p>2m
&prime;&prime;n (x) . (5.18)
</p>
<p>Thus, we have a discrete energy spectrum, which occurs whenever the quantum
</p>
<p>object is bounded or localized.
</p>
<p>As is well known, the quantization of energy means the following: If we look at the
</p>
<p>energy of a quantum object in the infinite potential well, we always detect one of these
</p>
<p>eigenvalues, but never any intermediate values. In other words, the possible measured
</p>
<p>(energy) values are the eigenvalues of the (energy) operator, i.e. the Hamiltonian. We
</p>
<p>have already encountered the same situation in the algebraic approach in Chap. 4,
</p>
<p>where we saw that the possible measured polarization values are determined by the
</p>
<p>eigenvalues of the corresponding polarization operators. In fact, this is a general
</p>
<p>aspect of quantum mechanics: Physical quantities are represented by operators, and
</p>
<p>the eigenvalues of those operators are the experimentally measurable quantities.
</p>
<p>Two more comments on the eigenfunctions:
</p>
<p>1. The amplitude in (5.16) is chosen to give the greatest simplicity of the result. In
</p>
<p>principle, also the form
</p>
<p>7This special choice will be justified below.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 The Infinite Potential Well 59
</p>
<p>n (x) =
&radic;
</p>
<p>2
</p>
<p>a
ein sin kn x (5.19)
</p>
<p>is possible, where n &isin; R is a phase shift. In order to avoid unnecessary restric-
tions, we will use the eigenfunctions in their complex form (5.19) in the following
</p>
<p>exemplary considerations, wherever appropriate.
</p>
<p>2. If we take into account the time dependence (see below), a state of definite energy
</p>
<p>En is given by n (x) e
&minus;in t &sim; sin kn x &middot; e&minus;in t , i.e. it is a standing wave.
</p>
<p>5.1.2 Solution of the Time-Dependent Schr&ouml;dinger Equation
</p>
<p>How would a total solution for the wavefunction  look? In Chap. 3, we started with
</p>
<p>the separation ansatz:
</p>
<p>(x, t) = (x)e&minus;it with E = . (5.20)
</p>
<p>The eigenfunctions n (x) are solutions of the stationary SEq with the eigenvalues
</p>
<p>En or n . Therefore, each of the functions n (x) e
&minus;in t is a particular solution of the
</p>
<p>time-dependent SEq. Due to the linearity of the SEq, we obtain the general solution
</p>
<p>by superposition of all the particular solutions. It follows that8:
</p>
<p> (x, t) =
&sum;
</p>
<p>n
</p>
<p>cnn (x) e
&minus;in t (5.21)
</p>
<p>with
</p>
<p>cn &isin; C ; n (x) =
&radic;
</p>
<p>2
</p>
<p>a
ein sin kn x; n =
</p>
<p>En
</p>
<p>
=
</p>
<p>k2n
</p>
<p>2m
. (5.22)
</p>
<p>Thus, we have integrated the SEq in closed form. We note that this is one of the few
</p>
<p>examples where this is possible.9
</p>
<p>The coefficients cn in (5.22) are determined by the particular choice of the sys-
</p>
<p>tem. If all the cn vanish except for one, the system is in a definite energy state;
</p>
<p>otherwise, it is in a superposition of several states. With the last equations, the prob-
</p>
<p>lem &lsquo;infinite potential well&rsquo; is completely determined&mdash;we know, in closed form, all
</p>
<p>the eigenvalues, the corresponding eigenfunctions and thus the general form of the
</p>
<p>time-dependent solution. From (5.21), we see explicitly that the solutions  (x, t),
</p>
<p>as discussed in Chap. 3, are elements of a vector space V . It holds for example that
</p>
<p>with
</p>
<p>8We recall that we are using a shorthand notation for the summation
&sum;
</p>
<p>n . The range of values of n
</p>
<p>must be clear from the context. Here, it would be n = 1, . . . .&infin; or
&sum;&infin;
</p>
<p>n=1.
9The form (5.21) for the general solution applies just as well to other potentials besides the infinite
</p>
<p>potential well considered here, although of course the eigenfunctions are then not the same as those
</p>
<p>in (5.22).</p>
<p/>
</div>
<div class="page"><p/>
<p>60 5 Two Simple Solutions of the Schr&ouml;dinger Equation
</p>
<p> (x, t) =
&sum;
</p>
<p>n
</p>
<p>cnn (x) e
&minus;in t and (x, t) =
</p>
<p>&sum;
</p>
<p>n
</p>
<p>dnn (x) e
&minus;in t , (5.23)
</p>
<p>every linear combination  =  +  may be written as
</p>
<p>(x, t) =
&sum;
</p>
<p>n
</p>
<p>(cn + dn)n (x) e&minus;in t =
&sum;
</p>
<p>n
</p>
<p>bnn (x) e
&minus;in t , (5.24)
</p>
<p>and thus is also a solution.
</p>
<p>However, one can still learn a lot more from this example. This is due to special
</p>
<p>properties of the eigenfunctions, whereby&mdash;and this is the salient point&mdash;these rela-
</p>
<p>tionships are valid in general and not only for the infinite potential well. Thanks to
</p>
<p>these properties, the inclusion of the initial-value problem (and thus the proof that
</p>
<p>the solution of the SEq is determinate) is relatively easy, as we shall see in a moment.
</p>
<p>5.1.3 Properties of the Eigenfunctions and Their
</p>
<p>Consequences
</p>
<p>An essential property of the eigenfunctions (5.19) is their so-called orthonormality.
</p>
<p>As one can show,10 the functions are normalized:
</p>
<p>a
&int;
</p>
<p>0
</p>
<p>&lowast;n (x)n (x) dx = 1 (5.25)
</p>
<p>and orthogonal:
a
</p>
<p>&int;
</p>
<p>0
</p>
<p>&lowast;m (x)n (x) dx = 0; m = n. (5.26)
</p>
<p>Written compactly, they are orthonormal11:
</p>
<p>a
&int;
</p>
<p>0
</p>
<p>&lowast;m (x)n (x) dx = nm . (5.27)
</p>
<p>Here, we integrate the product &lowast;mn and not mn , so that the expression is indepen-
dent of the phase which occurs in (5.19). Because the wavefunctions vanish outside
</p>
<p>of the interval [0, a], the integration can extend from &minus;&infin; to &infin;. We thus obtain the
general formulation:
</p>
<p>10See the exercises for this chapter.
11This explains also the choice which we made in (5.15) or (5.19).</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 The Infinite Potential Well 61
</p>
<p>Fig. 5.2 Example sketch of
</p>
<p>two functions f (x) and g(x)
</p>
<p>which are orthogonal in the
</p>
<p>sense of (5.28)
</p>
<p>&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p>&lowast;m (x)n (x) dx = nm . (5.28)
</p>
<p>In fact, the eigenfunctions of all the Hamiltonians we consider possess this impor-
</p>
<p>tant property, provided the corresponding eigenvalues are discrete. Here, it is of
</p>
<p>course assumed that the integrals exist, which means that the functions are square-
</p>
<p>integrable.12
</p>
<p>So far, we have used the term orthonormal in connection with &lsquo;usual&rsquo; vectors, such
</p>
<p>as column or row vectors or kets and bras, e.g. in Chap. 4 in the form i

</p>
<p> j
&rang;
</p>
<p>= i j .
That now also functions such as (5.21) are deemed orthonormal may seem surprising
</p>
<p>at first. It is due to the fact that, as mentioned above, these functions are also elements
</p>
<p>of the vector space V of the solutions of the SEq,13 and as such (i.e. as vectors), they
</p>
<p>can be orthogonal to each other. Indeed, the form on the left side of (5.28) is a scalar
</p>
<p>product, as is shown explicitly in Chap. 11. Hence, one has to distinguish between two
</p>
<p>aspects: On one hand, n (x) is a function of x ; on the other hand and simultaneously,
</p>
<p>it is an element of the vector space V and in this sense a vector.14 The orthogonality
</p>
<p>of two functions to each other does not mean that the graphs of these two functions
</p>
<p>intersect only at right angles or something similar; but rather that they, as members
</p>
<p>of V , behave as described in (5.28). This may appear as shown in Fig. 5.2.
</p>
<p>By the way, an even function is always orthogonal to an odd one (with symmetric
</p>
<p>limits of integration).
</p>
<p>In addition to their orthonormality, the eigenfunctions (5.19) have the property
</p>
<p>of completeness. Intuitively, this means that it is possible to formulate any solution
</p>
<p>of the SEq for the infinite potential well as a superposition of these eigenfunctions,
</p>
<p>as we have already noted in (5.21). Concerning their orthonormality, we use very
</p>
<p>12Square-integrable (or quadratically integrable) over the interval [a, b] are those functions f (x)
</p>
<p>for which
&int; b
</p>
<p>a
| f (x)|2 dx &lt; &infin; holds. The short notation reads f (x) &isin; L2 [a, b]. For a = &minus;&infin; and
</p>
<p>b = &infin;, the notation L2 [R] is common.
13Hence the often undifferentiated use of the terms eigenfunction and eigenvector.
14This use of the term vector has of course nothing to do with arrows or with the properties of
</p>
<p>transformation behavior (polar and axial vectors).</p>
<p/>
</div>
<div class="page"><p/>
<p>62 5 Two Simple Solutions of the Schr&ouml;dinger Equation
</p>
<p>similar formulations in both the algebraic and the analytic approaches, with
</p>
<p>n |m = nm or
&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p>&lowast;m (x)n (x) dx = nm . (5.29)
</p>
<p>The question of an analogous comparison for the completeness, which reads
&sum;
</p>
<p>n |n
n| = 1 in the algebraic approach, will be taken up again only in Chap. 11. But here,
we can already state that the eigenfunctions of the infinitely-deep potential well form
</p>
<p>a complete orthonormal system, a CONS.
</p>
<p>5.1.4 Determination of the Coefficients cn
</p>
<p>Back to the example of the infinite potential well: For the general solution of the
</p>
<p>time-dependent SEq (i.e. the total wavefunction), we found the expression
</p>
<p> (x, t) =
&sum;
</p>
<p>n
cnn (x) e
</p>
<p>&minus;in t . (5.30)
</p>
<p>The eigenfunctions and eigenvalues are defined by the physical problem (i.e. the
</p>
<p>shape of the potential), while the actual behavior in time is determined by the choice
</p>
<p>of the coefficients cn . If we know all the coefficients, we have uniquely determined the
</p>
<p>time dependence of  (x, t). On the other hand, the SEq is a differential equation of
</p>
<p>first order in time, which means that the specification of the initial condition (x, 0)
</p>
<p>determines the temporal behavior. In other words, knowledge of the initial condition
</p>
<p>(x, 0) gives the same information as knowledge of all the coefficients cn .
15 So it
</p>
<p>must be possible to calculate: (i) (x, 0) from knowledge of all the cn&rsquo;s, and (ii) all
</p>
<p>the coefficients cn from knowledge of (x, 0).
</p>
<p>This is trivial in the first case, since we have  (x, 0) =
&sum;
</p>
<p>n cnn (x). For the
</p>
<p>other direction, we use the orthonormality of the eigenfunctions (5.27). An additional
</p>
<p>technical note: We always assume that the functions considered here are sufficiently
</p>
<p>well-behaved, that all series converge, and that we can interchange any limiting
</p>
<p>processes such as derivatives, integrals, and infinite sums. Of course, this must be
</p>
<p>shown explicitly for particular cases, but we will save ourselves some trouble and
</p>
<p>leave this job to others, and accept their results. Some remarks on this are given in
</p>
<p>Appendix D, Vol. 1.16
</p>
<p>15At first sight, it may seem strange that one can compute infinitely many complex numbers cn from
</p>
<p>one initial condition (x, 0). But in fact, with (x, 0) we have uncountably many values.
16&ldquo;Physicists usually have a nonchalant attitude when the number of dimensions is extended to
</p>
<p>infinity. Optimism is the rule, and every infinite sequence is presumed to be convergent, unless
</p>
<p>proven guilty.&rdquo; A. Peres, Quantum Theory, p. 79.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 The Infinite Potential Well 63
</p>
<p>We begin with
</p>
<p> (x, 0) =
&sum;
</p>
<p>n
cnn(x). (5.31)
</p>
<p>Multiplying this equation from the left by &lowast;m(x) and integrating yields:
</p>
<p>a
&int;
</p>
<p>0
</p>
<p>&lowast;m(x) (x, 0) dx =
a
</p>
<p>&int;
</p>
<p>0
</p>
<p>&sum;
</p>
<p>n
</p>
<p>cn
&lowast;
m(x)n (x) dx . (5.32)
</p>
<p>Interchanging the integration and the summation and using the orthonormality (5.27)
</p>
<p>of the eigenfunctions leads to:
</p>
<p>&sum;
</p>
<p>n
</p>
<p>cn
</p>
<p>a
&int;
</p>
<p>0
</p>
<p>&lowast;m(x)n (x) dx =
&sum;
</p>
<p>n
</p>
<p>cnn,m = cm (5.33)
</p>
<p>or, compactly,
</p>
<p>cm =
a
</p>
<p>&int;
</p>
<p>0
</p>
<p>&lowast;m(x) (x, 0) dx . (5.34)
</p>
<p>Thus, the specification of the initial condition allows us to calculate uniquely all of
</p>
<p>the coefficients. It follows that
</p>
<p> (x, t) =
&sum;
</p>
<p>n
</p>
<p>
</p>
<p>
</p>
<p>a
&int;
</p>
<p>0
</p>
<p>&lowast;n(x
&prime;)
</p>
<p>(
</p>
<p>x &prime;, 0
)
</p>
<p>dx &prime;
</p>
<p>
</p>
<p>n (x) e
&minus;in t (5.35)
</p>
<p>gives an expression for the solution of the time-dependent SEq. We can read off from
</p>
<p>this equation directly that specifying the initial condition uniquely determines the
</p>
<p>time behavior of  (x, t) for all times.
</p>
<p>5.2 Free Motion
</p>
<p>As a second simple model system, we consider force-free unbounded motion. It is
</p>
<p>also described by the SEq17
</p>
<p>i (x, t) = &minus;

</p>
<p>2
</p>
<p>2m
 &prime;&prime;; (5.36)
</p>
<p>but here we assume that there are no limits on the motion. The quantum object is not
</p>
<p>localized and can move throughout all space.
</p>
<p>17This equation is very similar to the heat equation
.
</p>
<p>f = &nabla;2 f &mdash;apart from i in the SEq. As is well
known, this &lsquo;small difference&rsquo; is the mother of all worlds.</p>
<p/>
</div>
<div class="page"><p/>
<p>64 5 Two Simple Solutions of the Schr&ouml;dinger Equation
</p>
<p>5.2.1 General Solution
</p>
<p>As we know, special (particular) solutions of the problem are plane waves of the
</p>
<p>form
</p>
<p>part(x, t) = ei(kx&minus;t). (5.37)
</p>
<p>Since each k &isin; R is allowed, and thus also any energy E = 2k2
2m
</p>
<p>, we have a continuous
</p>
<p>energy spectrum. This case always occurs if the quantum object is not localized (i.e. is
</p>
<p>unbounded).
</p>
<p>The general solution is the superposition of particular solutions, that is18
</p>
<p>(x, t) =
&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p>c(k)ei(kx&minus;t)dk. (5.38)
</p>
<p>At t = 0, we obtain
</p>
<p>(x, 0) =
&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p>c(k)eikx dk. (5.39)
</p>
<p>The specification of this initial condition determines the time evolution here, also,
</p>
<p>since the SEq is a differential equation of first order in time. Consequently, it must be
</p>
<p>possible to compute the coefficients c(k) uniquely from (x, 0). This indeed works;
</p>
<p>by means of Fourier transformation,19 we obtain immediately
</p>
<p>c (k) =
1
</p>
<p>2
</p>
<p>&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p>(x, 0)e&minus;ikx dx, (5.40)
</p>
<p>so that we can, in principle, determine the solution for any given initial distribution.
</p>
<p>Thus, we have again integrated the SEq in closed form. In a compact notation, the
</p>
<p>solution reads:
</p>
<p>(x, t) =
1
</p>
<p>2
</p>
<p>&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p>
</p>
<p>
</p>
<p>&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p>(x &prime;, 0)e&minus;ikx
&prime;
dx &prime;
</p>
<p>
</p>
<p> ei(kx&minus;t)dk with  =
k2
</p>
<p>2m
. (5.41)
</p>
<p>Again in this case, we see immediately that the wavefunction is determinate.
</p>
<p>18Integral and not sum, because k is a continuous &lsquo;index&rsquo;. The integration variable k may of course
</p>
<p>also assume negative values here.
19Some basics on Fourier transformation can be found in Appendix H, Vol. 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 Free Motion 65
</p>
<p>5.2.2 Example: Gaussian Distribution
</p>
<p>A concrete standard example is based on the initial condition
</p>
<p> (x, 0) =
1
</p>
<p>(
</p>
<p>b20
)
</p>
<p>1
4
</p>
<p>exp
</p>
<p>(
</p>
<p>&minus;
x2
</p>
<p>2b20
</p>
<p>)
</p>
<p>ei K x , (5.42)
</p>
<p>where we assume K &gt; 0 without loss of generality.20 Without the factor ei K x ,
</p>
<p>the center of the distribution would be stationary, i.e. it would remain at the same
</p>
<p>position. For the following discussion, we concentrate on the absolute square of the
</p>
<p>wavefunction. Initially, it is given by
</p>
<p>(x, 0) = | (x, 0)|2 =
1
</p>
<p>&radic;
b0
</p>
<p>exp
</p>
<p>(
</p>
<p>&minus;
x2
</p>
<p>b20
</p>
<p>)
</p>
<p>. (5.43)
</p>
<p>This function has the form of a Gaussian bell curve with its maximum max =
(
&radic;
</p>
<p>b0)
&minus;1 at x = 0. The width of the curve is given by 2b0; it is measured between
</p>
<p>the points where the curve has the value  = max/e.21
In this example, one can determine  (x, t) exactly, but the calculation is tedious
</p>
<p>and will be omitted here.22 One arrives eventually at
</p>
<p>(x, t) = | (x, t)|2 =
1
</p>
<p>&radic;
b (t)
</p>
<p>exp
</p>
<p>(
</p>
<p>&minus;
(
</p>
<p>x &minus; K
m
</p>
<p>t
)2
</p>
<p>b2 (t)
</p>
<p>)
</p>
<p>, (5.44)
</p>
<p>where b (t) is given by
</p>
<p>b (t) =
</p>
<p>&radic;
</p>
<p>b20 +
(
</p>
<p>t
</p>
<p>b0m
</p>
<p>)2
</p>
<p>(5.45)
</p>
<p>with b (0) = b0. Obviously, the function b(t) increases monotonically with t and
tends towards t
</p>
<p>b0m
for t &rarr; &infin;.
</p>
<p>Equation (5.44) again represents a Gaussian curve, with its maximum at x =
K
m
</p>
<p>t and width 2b(t). This means that the curve becomes wider and wider with
</p>
<p>increasing t , while its maximum moves with constant velocity v = K
m
</p>
<p>to the right.
</p>
<p>Its height is given by max(x, t) = (
&radic;
</p>
<p>b(t))&minus;1, i.e. it decreases continuously due to
the monotonic form of b(t). In short, the distribution (x, t) becomes steadily wider
</p>
<p>and flatter&mdash;it &lsquo;goes fuzzy&rsquo; or becomes &lsquo;smeared out&rsquo;; see Fig. 5.3.
</p>
<p>This concludes our mathematical findings. However, the question remains as to
</p>
<p>what this &lsquo;smearing out&rsquo; means physically. One thing is clear: It cannot mean that the
</p>
<p>20The quite specific form of the coefficients is due to the normalization.
21Occasionally, this width is referred to as the halfwidth, although the function has dropped not to
</p>
<p>1/2, but to 1/e of its maximum value.
22A slightly more detailed analysis can be found in Appendix D, Vol. 2 (wave packets).</p>
<p/>
</div>
<div class="page"><p/>
<p>66 5 Two Simple Solutions of the Schr&ouml;dinger Equation
</p>
<p>Fig. 5.3 Spreading of the
</p>
<p>density distribution (5.44).
</p>
<p>Arbitrary units; maximum at
</p>
<p>t = 0, normalized to 1
</p>
<p>object (electron, etc.) is itself smeared out&mdash;an electron is, within the framework of
</p>
<p>our considerations, always an (indivisible) point object. We will discuss this point in
</p>
<p>more detail in Chap. 7. Here, we simply mention in anticipation that the question boils
</p>
<p>down to the interpretation of (x, t) as a probability density. It allows us by means
</p>
<p>of
&int; b
</p>
<p>a
(x, t) to calculate the probability of finding the quantum object in the interval
</p>
<p>[a, b]. Describing the spreading of the Gaussian curve means that the wavefunction
</p>
<p>from which the probability is calculated spreads out (and not the quantum object
</p>
<p>itself). In other words, the uncertainty with which we can determine the location of a
</p>
<p>quantum object, x &asymp; b(t), increases over time. With this interpretation of (x, t),
we have introduced the term &lsquo;probability&rsquo; also into the analytical approach.
</p>
<p>However, this concept makes sense only if the effects are noticeable (i) very
</p>
<p>strongly for microscopic objects, and (ii) nearly not at all for macroscopic objects.
</p>
<p>Everyday things around us do not have a spreading probability of being found at
</p>
<p>a particular location, in contrast to objects in the microscopic world. In order to
</p>
<p>arrive at a numerical estimate, we compute the time t2b0 after which the width of a
</p>
<p>bell-shaped curve, initially b0, has doubled, that is b
(
</p>
<p>t2b0
)
</p>
<p>= 2b0. It follows that:
&radic;
</p>
<p>b20 +
(
</p>
<p>t2b0
</p>
<p>b0m
</p>
<p>)2
</p>
<p>= 2b0 and thus t2b0 =
&radic;
</p>
<p>3
m
</p>
<p>
b20. (5.46)
</p>
<p>We calculate this doubling time t2b0 for two examples ( &asymp; 10&minus;34 kg m2/s):
</p>
<p>1. A &lsquo;Grain of sand&rsquo;: m = 1 g, b0 = 1 mm:
</p>
<p>t2b0,grain = 1.7 &middot;
10&minus;3
</p>
<p>10&minus;34
10&minus;6s = 1.7 &middot; 1025s &asymp; 5.4 &middot; 1017years; (5.47)
</p>
<p>2. An &lsquo;Electron&rsquo;, m = 10&minus;30 kg, b0 = 10&minus;10 m:
</p>
<p>t2b0,electron = 1.7 &middot;
10&minus;30
</p>
<p>10&minus;34
10&minus;20s &asymp; 1.7 &middot; 10&minus;16s. (5.48)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 Free Motion 67
</p>
<p>Fig. 5.4 Characterization of
</p>
<p>the energy spectrum for an
</p>
<p>arbitrary potential,
</p>
<p>depending on the
</p>
<p>localizability of the quantum
</p>
<p>object
</p>
<p>We see clearly the difference between a macroscopic and a microscopic object.
</p>
<p>We note that this calculation is only about orders of magnitude, not &lsquo;exact&rsquo; values,
</p>
<p>and that the results apply only if the objects are completely isolated during the time
</p>
<p>t2b0 (i.e. they do not interact with anything else in the universe). And of course we
</p>
<p>know that the &lsquo;grain of sand&rsquo; with m = 1 g is a many-particle system with internal
interactions.
</p>
<p>5.3 General Potentials
</p>
<p>A few words about the nature of the energy spectrum are in order. We have found that
</p>
<p>the energy spectrum of the infinite potential well is discrete, whereas it is continuous
</p>
<p>for unlimited motions. That does not mean that every system has either a discrete
</p>
<p>or a continuous energy spectrum. Consider, for example, the hydrogen atom, i.e.,
</p>
<p>proton plus electron. When the electron is in a bound state, we have discrete energies.
</p>
<p>If we ionize the atom, thus separating the electron from the nucleus so that it can
</p>
<p>move freely and without limit, then it can move with any kinetic energy&mdash;we have
</p>
<p>a continuous energy spectrum in this range. The situation is shown schematically in
</p>
<p>Fig. 5.4. In short, there are many systems whose energy spectrum has both a discrete
</p>
<p>and a continuous part.23
</p>
<p>We will take up the formal treatment of this question later, where we will also
</p>
<p>see that continuous systems are mathematically more difficult than discrete ones. To
</p>
<p>circumvent these problems, one can resort to a &lsquo;trick&rsquo;, which helps to ensure that the
</p>
<p>entire spectrum is discrete.
</p>
<p>We outline the basic idea: For this we start from an arbitrary (sufficiently well-
</p>
<p>behaved) potential V (x) that vanishes at infinity. Now let us imagine that we put
</p>
<p>the system under consideration in addition into a potential well with infinitely high
</p>
<p>potential walls on all sides, see Fig. 5.5. The walls should be so far away that we
</p>
<p>can assume that their existence has no measurable influence on the &lsquo;local&rsquo; physics
</p>
<p>23In fact, it may also be the case that discrete and continuous spectra overlap, or that discrete levels
</p>
<p>are embedded in the continuum, as we shall see using the example of the helium atom in Chap. 23,
</p>
<p>Vol. 2.</p>
<p/>
</div>
<div class="page"><p/>
<p>68 5 Two Simple Solutions of the Schr&ouml;dinger Equation
</p>
<p>Fig. 5.5 At sufficiently high
</p>
<p>resolution, the apparent
</p>
<p>continuum of energy
</p>
<p>eigenvalues proves to consist
</p>
<p>of closely-spaced individual
</p>
<p>levels
</p>
<p>gy
</p>
<p>(i.e. in our lab). In particular, the potential V is negligible (zero) at the location of
</p>
<p>the walls. The stationary SEq E (x) = &minus; 2
2m
</p>
<p>&prime;&prime;(x) + V (x)(x) is a second-order
differential equation with respect to x and accordingly has two linearly-independent
</p>
<p>fundamental solutions, 1(kx) and 2(kx), with k
2 = 2m E/2. For the following
</p>
<p>argument, it does not matter exactly what form these functions take, it is sufficient
</p>
<p>that they exist. Each solution of the stationary SEq for the energy E can be expressed
</p>
<p>as a linear combination (x) = A1(kx)+ B2(kx). If we now imagine infinitely
high potential walls at x = &plusmn;L , then  (x) must vanish there. It follows that
</p>
<p>A1(&minus;kL)+ B2(&minus;kL) = 0 (5.49)
A1(kL)+ B2(kL) = 0.
</p>
<p>This is a homogeneous system of equations for the quantities A and B. This system
</p>
<p>is solvable24 if
</p>
<p>1(&minus;kL)2(kL)&minus; 2(&minus;kL)1(kL) = 0 (5.50)
</p>
<p>applies. This equation can be satisfied only for certain values of kL . For a given
</p>
<p>L , this is therefore a determining equation for k, with countably infinitely many
</p>
<p>solutions kn . Hence the energy is discrete.
</p>
<p>24Moreover, it follows from (5.49) for instance that
</p>
<p>B = &minus;
1(kL)
</p>
<p>2(kL)
A
</p>
<p>and so
</p>
<p>(x) = A
[
</p>
<p>1(kx)&minus;
1(kL)
</p>
<p>2(kL)
2(kx)
</p>
<p>]
</p>
<p>,
</p>
<p>leaving only one remaining free constant (and one must remain because of the linearity of the SEq).</p>
<p/>
</div>
<div class="page"><p/>
<p>5.3 General Potentials 69
</p>
<p>The larger L is, the closer the energy levels lie together. We can visualize this
</p>
<p>by the fact that for sufficiently large n, the influence of the potential V (x) is small
</p>
<p>(i.e. the main influence arises from the infinite potential well) and the energy levels
</p>
<p>are given approximately by
</p>
<p>En &asymp;

</p>
<p>2k2n
</p>
<p>2m
=
</p>
<p>
22
</p>
<p>2m
</p>
<p>n2
</p>
<p>L2
. (5.51)
</p>
<p>The difference between these energy levels is
</p>
<p>En &minus; En&minus;1 &asymp;

</p>
<p>22
</p>
<p>2m
</p>
<p>2n &minus; 1
L2
</p>
<p>. (5.52)
</p>
<p>For sufficiently large L , one can reduce this difference to below any measurable
</p>
<p>value. In other words, we have in this case discrete energy eigenvalues, but they are
</p>
<p>so dense that they look to us like a (quasi-)continuum; cf. Fig. 5.5.
</p>
<p>A numerical example: If the potential walls were a light year apart, then the
</p>
<p>differences between two neighboring energy levels for an electron are of the order
</p>
<p>of 10&minus;50 eV (see exercises).
Finally, we note that another &lsquo;trick&rsquo; for the discretization of the spectrum is the
</p>
<p>introduction of periodic boundary conditions of the form  (x + L) =  (x). In this
way, one can model solids, or also motions on a cylinder or a torus. Two examples
</p>
<p>can be found in the exercises.
</p>
<p>5.4 Exercises
</p>
<p>1. Given the free stationary SEq
</p>
<p>E(x) = &minus;

</p>
<p>2
</p>
<p>2m

</p>
<p>&prime;&prime;
(x), (5.53)
</p>
<p>formulate the corresponding equation for the Fourier transform of .
</p>
<p>2. Given the stationary SEq
</p>
<p>E(x) = &minus;

</p>
<p>2
</p>
<p>2m

</p>
<p>&prime;&prime;
(x)+ V (x)(x), (5.54)
</p>
<p>formulate the corresponding equation for the Fourier transform of .
</p>
<p>3. The Hamiltonian has discrete nondegenerate eigenvalues En , n = 1, 2, . . .. What
is the general solution of the time-dependent SEq?
</p>
<p>4. Infinite potential well: Show that the eigenfunctions in the form n(x) =
&radic;
</p>
<p>2
a
</p>
<p>ein sin(kn x) constitute an orthonormal system of functions (
&int; a
</p>
<p>0
&lowast;m(x)n(x)
</p>
<p>= mn). Hint: The integrals can be calculated for example by means of</p>
<p/>
</div>
<div class="page"><p/>
<p>70 5 Two Simple Solutions of the Schr&ouml;dinger Equation
</p>
<p>sin x sin y = cos(x&minus;y)&minus;cos(x+y)
2
</p>
<p>or the exponential representation of the sine func-
</p>
<p>tions.
</p>
<p>5. Infinite potential well: Formulate the general solution of the time-dependent SEq
</p>
<p>and verify that specification of the initial condition determines the wave function.
</p>
<p>Concretize the considerations to the special cases (C &isin; C is an arbitrary complex
constant):
</p>
<p>(a) (x, t = 0) = C(x &minus; a
2
);
</p>
<p>(b) (x, t = 0) = C ;
(c) (x, t = 0) = Cei K x .
</p>
<p>6. Given the three-dimensional SEq E(r) = &minus; 2
2m
</p>
<p>&nabla;2(r), which energy eigen-
values are allowed if one imposes the following periodic boundary conditions:
</p>
<p>(x, y, z) = (x + L x , y, z) = (x, y + L y, z) = (x, y, z + L z)?
7. An electron is located between the two walls of an infinite potential well, which
</p>
<p>are one light year apart. Calculate roughly the magnitude of the difference
</p>
<p>between two adjacent energy levels.
</p>
<p>8. Find examples for functions which
</p>
<p>(a) are integrable, but not square-integrable;
</p>
<p>(b) are square-integrable, but not integrable.
</p>
<p>9. Given the stationary SEq
</p>
<p>E (x) = &minus;

</p>
<p>2
</p>
<p>2m
&prime;&prime;(x)+ V (x)(x), (5.55)
</p>
<p>rewrite this equation for a dimensionless independent variable.
</p>
<p>10. A short outlook into string theory (compactified or rolled-up dimensions): String
</p>
<p>theory assumes that the elementary building blocks of nature are not point
</p>
<p>objects, but rather one-dimensional objects (strings) with a certain energy&mdash;
</p>
<p>comparable to an object in a one-dimensional potential well. Strings have a
</p>
<p>spatial extension of order of the Planck length and live in higher-dimensional
</p>
<p>spaces (e.g. dim = 10 or dim = 26), where only four dimensions are not rolled
up (compactified)&mdash;quite similar to our following simple example.25
</p>
<p>For the formal treatment, we take the two-dimensional SEq
</p>
<p>25When a writer like Terry Pratchett couples the idea of rolled-up dimensions with other physical
</p>
<p>paradigms, it reads like this: &ldquo;..and people stopped patiently building their little houses of rational
</p>
<p>sticks in the chaos of the universe and started getting interested in the chaos itself&mdash;partly because
</p>
<p>it was a lot easier to be an expert on chaos, but mostly because it made really good patterns that
</p>
<p>you could put on a T-shirt.
</p>
<p>And instead of getting on with proper science, scientists suddenly went around saying how
</p>
<p>impossible it was to know anything, and that there wasn&rsquo;t really anything you could call reality to
</p>
<p>know anything about, and how all this was tremendously exciting, and incidentally did you know
</p>
<p>there were possibly all these little universes all over the place but no-one can see them because they
</p>
<p>are all curved in on themselves? Incidentally, don&rsquo;t you think this is a rather good T-shirt?&rdquo; Terry
</p>
<p>Pratchett, in Witches Abroad, A Discworld Novel.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 Exercises 71
</p>
<p>Fig. 5.6 The &lsquo;cylinder
</p>
<p>world&rsquo; of our toy string
</p>
<p>&minus;

</p>
<p>2
</p>
<p>2m
</p>
<p>(
</p>
<p>&part;2
</p>
<p>&part;x2
+
</p>
<p>&part;2
</p>
<p>&part;y2
</p>
<p>)
</p>
<p>= E (5.56)
</p>
<p>as starting point. In the x direction, we have an infinite potential well
</p>
<p>V =
{
</p>
<p>0 for 0 &lt; x &lt; a
</p>
<p>&infin; otherwise (5.57)
</p>
<p>and for the y coordinate we postulate
</p>
<p> (x, y) =  (x, y + 2R) . (5.58)
</p>
<p>So we have a combination of two different boundary conditions: In the x direc-
</p>
<p>tion,  (0, y) =  (a, y) = 0 applies, while in the y direction the periodic
boundary condition  (x, y) =  (x, y + 2R) is valid. In other words, the
quantum object &lsquo;lives&rsquo; on the surface of a cylinder of length a and of radius R,
</p>
<p>see Fig. 5.6. The problem is now to calculate the possible energy levels. Discuss
</p>
<p>in particular the situation when R  a.
11. Given the free one-dimensional SEq (5.36) and the function (x), show that
</p>
<p> (x, t) = A
1
&radic;
</p>
<p>t
</p>
<p>&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p>e
im
2
</p>
<p>(x&minus;y)2
t (y) dy (5.59)
</p>
<p>is a solution (A is a normalization constant).</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 6
</p>
<p>Interaction-Free Measurement
</p>
<p>We discuss an experiment which provides an example of the unusual effects that may result
</p>
<p>from the superposition of states, and of the peculiarities that may be associated with the
</p>
<p>quantum-mechanicalmeasurement process. In addition,wemake the acquaintance of unitary
</p>
<p>operators.
</p>
<p>Self-interference, i.e. the interference of a quantum object with itself, is a fascinat-
</p>
<p>ing phenomenon of quantum mechanics, which we discuss below in terms of the
</p>
<p>interaction-free quantum measurement. The experiment is based on the principle of
</p>
<p>the Mach&ndash;Zehnder interferometer (MZI). It shows the existence of quantum super-
</p>
<p>positions as clearly as the famous double-slit experiment, but it is by comparison
</p>
<p>formally and experimentally much &lsquo;handier&rsquo;, so that it is increasingly finding its way
</p>
<p>into textbooks. At the same time, it also allows for the treatment of further-reaching
</p>
<p>questions. That is why wemeet theMZI not only in manymodern basic experiments,
</p>
<p>but also for example in the field of quantum information, where we can realize basic
</p>
<p>functions of the quantum computer by means of the MZI and its components (see
</p>
<p>the closing remarks to this chapter).
</p>
<p>6.1 Experimental Results
</p>
<p>6.1.1 Classical Light Rays and Particles in the
</p>
<p>Mach&ndash;Zehnder Interferometer
</p>
<p>6.1.1.1 Light Rays
</p>
<p>The experimental setup consists of a Mach&ndash;Zehnder interferometer and two pho-
</p>
<p>todetectors, which respond to incident light; see Fig. 6.1. Coherent light enters the
</p>
<p>apparatus at the lower left and is split by a beam splitter (or half-silvered mirror)
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4_6
</p>
<p>73</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_6&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_6&amp;domain=pdf</a></div>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4_6">https://doi.org/10.1007/978-3-030-00464-4_6</a></div>
</div>
<div class="page"><p/>
<p>74 6 Interaction-Free Measurement
</p>
<p>Fig. 6.1 Schematic of a MZI. BS= beam splitter, M=mirror, D= detector. Left: At the first beam
splitter, the light is split into two sub-beams (blue and red do not signify the colors of the light beams,
</p>
<p>but serve only for better visualization), and these two sub-beams are split again at the second beam
</p>
<p>splitter, resulting in four sub-beams. The figure at the right is a compact description of these facts,
</p>
<p>which we will use in the following
</p>
<p>into two beams.1 These beams impinge, after reflection by a mirror, on a second
</p>
<p>beam splitter, so as to produce a total of four sub-beams, two each of which meet
</p>
<p>at one of the two detectors. The experimental finding is now that the upper detector
</p>
<p>D2 never responds and the lower detector D1 always responds. In other words, the
</p>
<p>relative intensity I on D1 is given by I1 = 1, and on D2 by I2 = 0. Here, we assume
on the whole ideal conditions: the optical paths &lsquo;above&rsquo; and &lsquo;below&rsquo; have exactly the
</p>
<p>same length, there is no absorption by the mirrors, the efficiency of the detectors is
</p>
<p>100%, and so on.
</p>
<p>This different behavior of the two detectors may perhaps be surprising, since the
</p>
<p>experimental setup appears to be completely symmetrical at first glance. But in fact,
</p>
<p>its symmetry is broken, as long as the light enters the first beam splitter only in the
</p>
<p>horizontal and not also in the vertical direction (and with the same intensity).
</p>
<p>The following consideration shows why the two detectors react differently: That
</p>
<p>part of the lower beamwhich after the second beam splitter entersD2orD1undergoes
</p>
<p>a reflection (1 &times; mirror) or two reflections (1 &times; mirror, 1 &times; beam splitter), while
the part of the upper beam which after the second beam splitter enters D2 or D1
</p>
<p>undergoes three reflections (1 &times; mirror, 2 &times; beam splitter) or two reflections (1 &times;
mirror, 1&times; beam splitter). In other words, the detector D1 sees two light beams with
the same history (i.e. the same phase), which consequently interfere constructively.
</p>
<p>In contrast, the detector D2 sees two beams with different histories. We will show
</p>
<p>immediately that this indeed gives destructive interference.
</p>
<p>In a variant of the experimental setup, we use a blocker that absorbs the upper
</p>
<p>beam or scatters it out of the MZI; see Fig. 6.2. Obviously, the upper and lower
</p>
<p>sub-beams now cannot interfere and the intensities at the detectors are given by
</p>
<p>I1 = I2 = 1/4.
</p>
<p>1The two beams can in principle be separated quite far apart. In this way, the non-classical effects of
</p>
<p>certain quantum-mechanical setups can be demonstrated more impressively than in the double-slit
</p>
<p>experiment.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 Experimental Results 75
</p>
<p>Fig. 6.2 MZI with a blocker
</p>
<p>in the upper beam
</p>
<p>6.1.1.2 Particles in the MZI
</p>
<p>What happens if we send particles (m = 0) instead of light waves through the
apparatus? Of course we have to replace the beam splitters by devices which let the
</p>
<p>particles pass or reflect themwith probabilities of 1/2, but otherwise the experimental
</p>
<p>setup remains the same. If we now interpret the number of particles per detector as
</p>
<p>intensities, it follows directly that for the case without a blocker, I1 = I2 = 1/2
holds, and for the case with a blocker, I1 = I2 = 1/4.
</p>
<p>6.1.1.3 Comparison: Light&ndash;Particles
</p>
<p>It follows that with a blocker, the intensities are given by I1 = I2 = 1/4, regardless
of whether we use waves or particles. For the case without a blocker, however, there
</p>
<p>is a distinctive difference, since for waves we have I1 = 1 and I2 = 0, while for
particles, I1 = I2 = 1/2. So we can conclude that if we perform an experiment (in
the sense of a black-box setup) without the blocker and measure I2 = 0, then we
know that a wave and not a particle has passed through the apparatus. These results
</p>
<p>are summarized in the Table6.1.
</p>
<p>6.1.2 Photons in the Mach&ndash;Zehnder Interferometer
</p>
<p>6.1.2.1 Single-Photon Experiments (MZI Without Blocker)
</p>
<p>We let light enter the MZI and reduce its intensity (similar to the polarization exper-
</p>
<p>iments of Chap.2). Since our previous considerations do not rely on the intensity of
</p>
<p>the incident light, they should also apply to the limit of vanishing light intensity. This
</p>
<p>Table 6.1 Intensities at the
</p>
<p>two detectors
Without blocker With blocker
</p>
<p>Wave I1 = 1; I2 = 0 I1 = 14 ; I2 =
1
4
</p>
<p>Particle I1 = 12 ; I2 =
1
2
</p>
<p>I1 = 14 ; I2 =
1
4</p>
<p/>
</div>
<div class="page"><p/>
<p>76 6 Interaction-Free Measurement
</p>
<p>means that eventually there is only one photon in the MZI at a given time. In fact,
</p>
<p>the experimental findings are: even if we operate with single photons, only detector
</p>
<p>D1 responds, while D2 remains silent, or I1 = 1 and I2 = 0.
So we must conclude that a single photon is a wave and not a particle. On the
</p>
<p>other hand, a photon is a point object as far as we know. Our everyday understanding
</p>
<p>perceives the situation as contradictory: An object can be both point-like and wave-
</p>
<p>like. But our cognitive abilities are, as we have mentioned before, formed and trained
</p>
<p>by evolution in our macro-physical environment and not under quantum-mechanical
</p>
<p>conditions.
</p>
<p>In addition, we have to conclude that due to the interference effect, the photon
</p>
<p>&lsquo;somehow&rsquo; interacts with itself. It is not intuitively obvious how this takes place.
</p>
<p>Certainly, it is not the case that the photon splits into two smaller fragments. We have
</p>
<p>here the same problem as in the double slit experiment&mdash;if there are two possibilities
</p>
<p>which can be realized by a quantum-mechanical system, then certain interference
</p>
<p>phenomena will appear which have no classical analogues (self-interference).
</p>
<p>As we said previously, it is perhaps best to imagine a quantum-mechanical
</p>
<p>possibility landscape, in which the quantum object (photon, electron, . . .) moves.
</p>
<p>A superposition of possibilities yields a new landscape with new features, in which
</p>
<p>the object moves differently than in the landscape of only one possibility.
</p>
<p>6.1.2.2 Interaction-Free Measurement (MZI With Blocker)
</p>
<p>With a beam blocker, we have I1 = I2 = 1/4, and that means that in 25% of all
cases, detector 2 responds. This in turn implies that we know in these cases that
</p>
<p>there is a blocker in the apparatus without the photon having interacted directly with
</p>
<p>the blocker (otherwise it would have disappeared from the apparatus and could not
</p>
<p>be detected in either detector).2 This situation is called an interaction-free quantum
</p>
<p>measurement. Below, we make some critical remarks about this terminology.
</p>
<p>The whole issue can be formulated more sensationally3 by choosing a bomb4 as
</p>
<p>the blocker. The bomb is so sensitive that a single photon is enough to cause it to
</p>
<p>detonate5&mdash;so to speak, just seeing the bomb means that it explodes.6 We can use
</p>
<p>2Thus, there are apparently physical effects influenced by potential but unrealized events, that is,
</p>
<p>events that could have happened, but did not actually occur. Such events are called counterfactual
</p>
<p>(not corresponding to the facts).
3A.C.Elitzur andL.Vaidman, &ldquo;QuantumMechanical Interaction-FreeMeasurements&rdquo;, Foundations
</p>
<p>of Physics 23, 987 (1993).
4In order to avoid the militaristic note, some textbooks use &lsquo;cracker test&rsquo; instead of &lsquo;bomb test&rsquo;, but
</p>
<p>this sounds a bit whimsical.
5&ldquo;A physical experiment which makes a bang is always worth more than a quiet one. Therefore a
</p>
<p>man cannot strongly enough ask of Heaven: If it wants to let him discover something, may it be
</p>
<p>something that makes a bang. It will resound into eternity.&rdquo; Georg Christoph Lichtenberg, Scrap
</p>
<p>Books, Vol. F (1147).
6This remark seems a bit exaggerated, but in fact the rods of the human eye can apparently react
</p>
<p>to even a single photon. The cones, responsible for color vision, need about 100 times stronger</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 Experimental Results 77
</p>
<p>this fact in the following setup: Assume that we have a black-box MZI, and we do
</p>
<p>not know whether it contains a bomb or not. The task now is to clarify this issue.
</p>
<p>It cannot be solved by means of classical physics. Quantum mechanics helps us&mdash;
</p>
<p>at least, we know in a quarter of the cases that a bomb is hidden in the apparatus
</p>
<p>without its blowing up in our faces. In fact, one can increase the &lsquo;efficiency&rsquo; in a
</p>
<p>somewhat modified apparatus to virtually 100% by exploiting the so-called quantum
</p>
<p>Zeno effect. More about this in AppendixL, Vol. 1.
</p>
<p>We have here again - as a purely quantum-mechanical effect - the superposition of
</p>
<p>possibilities (self-interference) that makes possible this surprising result. Of course,
</p>
<p>it is again not the case that the photon &lsquo;splits&rsquo; up and, quasi by way of trial and error,
</p>
<p>passes at the same time through both arms of the MZI. The superposition of the pos-
</p>
<p>sibilities provides precisely the different landscape mentioned above, in which the
</p>
<p>photon propagates in a different way. We can most easily describe this propagation
</p>
<p>by means of probabilities&mdash;if we let a photon start through the apparatus, it will end
</p>
<p>upwith a probability of 1/4 in detector 2, and thenwe know that a bomb is in the beam
</p>
<p>path. But if we (in whatever way) know which arm the photon has passed through
</p>
<p>(which-way information), the landscape of possibilities or probabilities changes dra-
</p>
<p>matically: in 50% of the cases the bomb explodes, in the other 50% nothing exciting
</p>
<p>happens. Formulated as a &lsquo;standard rule&rsquo;: If the path is known/unknown, then the
</p>
<p>probabilities/amplitudes are added:
</p>
<p>path is
known
</p>
<p>unknown
&rarr; add probabilities
</p>
<p>amplitudes
. (6.1)
</p>
<p>Further comments on which-way experiments (or delayed-choice experiments) are
</p>
<p>found in AppendixM, Vol. 1.
</p>
<p>In this context, the term wave-particle duality occasionally crops up. What is
</p>
<p>meant is this: Depending on the experimental situation, a quantum system shows
</p>
<p>either particle-like or wave-like features. We take as an example electrons in the
</p>
<p>double slit experiment. If we allow for interference, then the electrons show their
</p>
<p>&lsquo;wave nature&rsquo;; if wewant to see them as particles, e.g. by following their path through
</p>
<p>the slits, they show their &lsquo;particle nature&rsquo;. Dualism in this context means that these
</p>
<p>properties are complementary&mdash;either particle or wave, but we can never measure
</p>
<p>both at the same time. We can state briefly and quite generally that asking for a
</p>
<p>particular property of an object leads to an answer that puts that property in the
</p>
<p>foreground and suppresses the other (complementary) property.
</p>
<p>On closer inspection, the term wave-particle duality seems, however, to be redun-
</p>
<p>dant, or to favor misunderstandings, since it supports the widespread but erroneous
</p>
<p>notion that, before a measurement, a quantum object is actually a particle or a wave.
</p>
<p>That is a misinterpretation which can cloud the mind in the process of learning quan-
</p>
<p>tum mechanics. Indeed, before a measurement, quantum objects in general do not
</p>
<p>have well-defined properties. It is therefore understandable that one is often advised
</p>
<p>excitation. See e.g. Davide Castelvecchi, People can sense single photons, Nature, https://doi.org/
</p>
<p>10.1038/nature.2016.20282 (Jul 2016).</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1038/nature.2016.20282">https://doi.org/10.1038/nature.2016.20282</a></div>
<div class="annotation"><a href="https://doi.org/10.1038/nature.2016.20282">https://doi.org/10.1038/nature.2016.20282</a></div>
</div>
<div class="page"><p/>
<p>78 6 Interaction-Free Measurement
</p>
<p>to omit completely the terms &lsquo;wave-particle duality&rsquo; or &lsquo;complementarity&rsquo;; indeed,
</p>
<p>doing so does not cause a noticeable loss in understanding.7
</p>
<p>A quantum object is simply something for which we have no detailed everyday
</p>
<p>terms, and depending on how we look at it, it seems to be more like a particle or
</p>
<p>more like a wave (but in fact it is neither)&mdash;it is simply a quantum object.8 We could
</p>
<p>call it informally a &lsquo;quob,&rsquo; but would it then be more familiar or intuitive?
</p>
<p>6.2 Formal Description, Unitary Operators
</p>
<p>To arrive at a simple, clear-cut description of states, we choose as the only distinctive
</p>
<p>criterion their direction of motion&mdash;either horizontal or vertical. Thus, we neglect
</p>
<p>polarization, beam profile, explicit time behavior and so on. We describe the con-
</p>
<p>ditions with and without a blocker under the assumption that we have two identical
</p>
<p>beam splitters.
</p>
<p>6.2.1 First Approach
</p>
<p>We divide the setup into four regions, as shown in Fig. 6.3. We denote the state in the
</p>
<p>region i by |zi . With regard to the simplest possible description as just mentioned,
we represent |zi  as a superposition of horizontal |H and vertical |V  propagation
directions,9 where these states constitute a CONS in a two-dimensional vector space.
</p>
<p>One can see that the propagation is horizontal in region 1 and both vertical and
</p>
<p>horizontal in region 2. Accordingly, we can write |z1 = |H and |z2 = c1 |H +
c2 |V . To determine the numbers c1 and c2, we take into account that (i) the relative
phase shift is 90 = 
</p>
<p>2
(see AppendixK, Vol. 1), which corresponds to ei/2 = i ,
</p>
<p>and (ii) that the intensity in a half-silvered mirror10 is equal for &lsquo;horizontal&rsquo; and
</p>
<p>7The Feynman Lectures on Physics, 5th Edition, 1970, Vol II, p. 37-1: &ldquo;Newton thought that light
</p>
<p>was made up of particles, but then it was discovered that it behaves like a wave. Later, however
</p>
<p>(in the beginning of the twentieth century), it was found that light did indeed sometimes behaves
</p>
<p>like a particle. Historically, the electron, for example, was thought to behave like a particle, and
</p>
<p>then it was found that in many respects it behaved like a wave. So it really behaves like neither.
</p>
<p>Now we have given up. We say: &lsquo;It is like neither.&rdquo;&rsquo; Richard P. Feynman, S. Tomonaga and J.
</p>
<p>Schwinger were awarded the Nobel Prize in Physics 1965 for their fundamental work in quantum
</p>
<p>electrodynamics.
8We note at this point, more generally, that the practice of declaring all things perceived to simply
</p>
<p>&lsquo;exist&rsquo; may be inadequate. Instead, one should first look at perception itself and examine its pre-
</p>
<p>dictability. Therefore, in quantum mechanics we need advanced methods, because we cannot come
</p>
<p>to grips with the &lsquo;perceptions&rsquo; (observations, measurements) by simply using intuitive, classical
</p>
<p>instruments. To obtain the information relevant to quantum mechanics, we have to think and act in
</p>
<p>a largely formal manner.
9Not to be confused with the polarization states |h and |v.
10For asymmetrical beam splitters (reflectance = transmittance), see the exercises.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Formal Description, Unitary Operators 79
</p>
<p>Fig. 6.3 Division of the
</p>
<p>MZI into four regions
</p>
<p>&lsquo;vertical&rsquo;, i.e. |c1|2 = |c2|2. Thus, it follows that |z2 = c [|H + i |V ].We postpone
the determination of the constant c and summarize:
</p>
<p>|H &rarr;
beam splitter
</p>
<p>c [|H + i |V ] (6.2)
</p>
<p>and analogously
</p>
<p>|V  &rarr;
beam splitter
</p>
<p>c [|V  + i |H]. (6.3)
</p>
<p>At a mirror, we have a phase shift of 180 = or ei = &minus;1 and therefore
</p>
<p>|H &rarr;
mirror
</p>
<p>&minus; |V ; |V  &rarr;
mirror
</p>
<p>&minus; |H. (6.4)
</p>
<p>All in all, we have
</p>
<p>|z1 = |H
|z2 = c [|H + i |V ]
|z3 = &minus; c [|V  + i |H] (6.5)
|z4 = &minus; c2 [|V  + i |H]&minus; ic2 [|H + i |V ] = &minus;2ic2 |H.
</p>
<p>It follows immediately that only detector 1 responds, while detector 2 remains dark,
</p>
<p>as is indeed observed experimentally.
</p>
<p>We can define the constant c as follows:We assume that the setup operates without
</p>
<p>losses&mdash;what goes in, comes out. This manifests itself in the fact that the norms are
</p>
<p>equal, or more precisely must be equal, zi | zi  =
&lang;
</p>
<p>z j

</p>
<p> z j
&rang;
</p>
<p>. The simplest choice is
</p>
<p>&minus;2ic2 = 1 or c = &plusmn;ei/4/
&radic;
2. We choose the upper sign and find c = 1+i
</p>
<p>2
.
</p>
<p>For the case with a blocker, we have analogously
</p>
<p>|z1 = |H
|z2 = c [|H + i |V ]</p>
<p/>
</div>
<div class="page"><p/>
<p>80 6 Interaction-Free Measurement
</p>
<p>|z3 = &minus;c |V  (6.6)
</p>
<p>|z4 = &minus;c2 [|V  + i |H] =
1
</p>
<p>2i
[|V  + i |H] .
</p>
<p>We see that also in this case, the intensities measured by the two detectors are
</p>
<p>displayed correctly. We note that the transition |z2 &rarr; |z3 does not conserve the
norm: z2| z2 = 2 |c|2 = z3| z3 = |c|2. It is the absorbing effect of the blocker
which leads to this inequality.
</p>
<p>6.2.2 Second Approach (Operators)
</p>
<p>We have just described the experiment with &lsquo;states and arrows&rsquo;. A more compact
</p>
<p>approach is permitted by using operators.We can describe the effect of a beam splitter
</p>
<p>by an operator T , and the effect of a mirror without or with a blocker by S and S&prime;.
Without the blocker, this leads to:
</p>
<p>|z1 = initial state
|z2 = T |z1
|z3 = S |z2 = ST |z1 (6.7)
|z4 = T |z3 = T S |z2 = T ST |z1 = final state
</p>
<p>and with the blocker, to:
</p>
<p>|z1 = initial state; |z4 = T S&prime;T |z1 = final state. (6.8)
</p>
<p>Theoperators are applied in sequence fromright to left:T ST |z1= T (S (T |z1)).
To obtain an explicit formulation for T , we consider the effect of this operator on
</p>
<p>the basis vectors. According to (6.2) and (6.3), we have
</p>
<p>T |H = 1+ i
2
</p>
<p>[|H + i |V ] ; T |V  = 1+ i
2
</p>
<p>[i |H + |V ] . (6.9)
</p>
<p>Using the completeness relation |H H | + |V  V | = 1 leads to
</p>
<p>T |H H | + T |V  V | = T = 1+ i
2
</p>
<p>[|H + i |V ] H | + 1+ i
2
</p>
<p>[i |H + |V ] V | ,
(6.10)
</p>
<p>or compactly,
</p>
<p>T = 1+ i
2
</p>
<p>[1+ i |H V | + i |V  H |] . (6.11)
</p>
<p>Analogously, the &lsquo;mirror-operator&rsquo; without a blocker is given by:</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Formal Description, Unitary Operators 81
</p>
<p>S = &minus; |H V | &minus; |V  H | (6.12)
</p>
<p>and with the blocker by
</p>
<p>S&prime; = &minus; |V  H | . (6.13)
</p>
<p>We learn from this that operators can generally be represented as linear combina-
</p>
<p>tions of dyadic products of the basis vectors.
</p>
<p>It is easily verified that with (6.11)&ndash;(6.13), we have
</p>
<p>T ST = 1 (6.14)
</p>
<p>and
</p>
<p>T S&prime;T = 1
2
[1+ i |H V | &minus; i |V  H |] , (6.15)
</p>
<p>so that we obtain again from the initial state |z1 = |H the final states |z4 = |H
and |z4 = 12 [|H &minus; i |V ] for the case with and without the blocker, respectively.
For the explicit representation of the operators and their products as matrices, see
</p>
<p>the exercises.
</p>
<p>The adjoint of the operator T is
</p>
<p>T &dagger; = 1&minus; i
2
</p>
<p>[1&minus; i |H V | &minus; i |V  H |] (6.16)
</p>
<p>and it follows that
</p>
<p>T &dagger;T = T T &dagger; = 1. (6.17)
</p>
<p>Analogously, the same holds true for S, but not for S&prime;, since here an (irreversible)
absorption is included:
</p>
<p>SS&dagger; = S&dagger;S = 1; S&prime;S&prime;&dagger; = |V  V | ; S&prime;&dagger;S&prime; = |H H |. (6.18)
</p>
<p>In fact, the operators T and S share an important property&mdash;they are unitary. As
</p>
<p>a generalization of (6.17), an operator (or matrix) U is unitary if
</p>
<p>U &dagger;U = UU &dagger; = 1 or U &dagger; = U&minus;1. (6.19)
</p>
<p>The name &lsquo;unitary&rsquo; stems from the fact that certain expressions are left unchanged
</p>
<p>under the transformation performed by the operator&mdash;in a way, it acts similarly to
</p>
<p>multiplication by 1. For example, the scalar product and thus also the norm are invari-
</p>
<p>ant. To show this, we start with two states | and |, and the unitary transformed
states
</p>
<p>
</p>
<p>&prime;
&rang;
</p>
<p>= U | and

</p>
<p>&prime;
&rang;
</p>
<p>= U |. Remember that a product of operators is
reversed11 in the adjoint, (AB)&dagger; = B&dagger;A&dagger;. This means that
</p>
<p>11This is well known from linear algebra, e.g. when transposing or inverting matrices.</p>
<p/>
</div>
<div class="page"><p/>
<p>82 6 Interaction-Free Measurement
</p>
<p>(
</p>
<p>&prime;
&rang;)&dagger; = (U |)&dagger; &rarr;
</p>
<p>&lang;
</p>
<p>&prime;

</p>
<p> = |U &dagger;. (6.20)
</p>
<p>It follows that
&lang;
</p>
<p>&prime;

</p>
<p> &prime;
&rang;
</p>
<p>= |U &dagger;U | = | , (6.21)
</p>
<p>i.e. the scalar product is conserved.Unitary transformations can always be understood
</p>
<p>in the end as a coordinate or basis transformation, even if the corresponding space
</p>
<p>is more elaborate than our two-dimensional space. These transformations conserve
</p>
<p>in particular scalar products, hence also lengths and angles, and they are reversible
</p>
<p>(becauseU&minus;1 = U &dagger; exists). Irreversible processes (e.g. measurements) can therefore
not be represented by unitary transformations.
</p>
<p>6.3 Concluding Remarks
</p>
<p>As mentioned earlier in this chapter, the MZI is an essential tool for many mod-
</p>
<p>ern fundamental experiments, both theoretically and experimentally. Due to lack of
</p>
<p>space, we can only outline some of them here, but a more detailed discussion is found
</p>
<p>in the Appendices (L and M, Vol.1; J, P and Q, Vol. 2). At the end of this chapter,
</p>
<p>we will take a closer look at the term &lsquo;interaction-free&rsquo;.
</p>
<p>6.3.1 Extensions
</p>
<p>Out of a great number of applications of the MZI, we have selected those which
</p>
<p>are understandable with our present knowledge and which do not require additional
</p>
<p>concepts such as the Aharonov&ndash;Bohm effect.
</p>
<p>6.3.2 Quantum Zeno Effect
</p>
<p>There is an extension of the &lsquo;bomb test&rsquo; which uses the quantum Zeno effect. This
</p>
<p>effect essentially implies that one can prevent the change of a system under appro-
</p>
<p>priate circumstances by frequently-repeated measurements (&lsquo;a watched pot never
</p>
<p>boils&rsquo;). The experiment uses a modified MZI setup and is based on the observation
</p>
<p>of the polarization state of photons. In principle, one can achieve an efficiency of up
</p>
<p>to 100% (see AppendixL, Vol. 1).</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Concluding Remarks 83
</p>
<p>Fig. 6.4 Delayed-choice
</p>
<p>experiment. The second
</p>
<p>beam splitter can be removed
</p>
<p>or inserted
</p>
<p>6.3.3 Delayed-Choice Experiments
</p>
<p>Here,we use the familiarMZI setup, but the second beam splitter BS2 can be removed
</p>
<p>or inserted after the photon has passed the first beam splitter (hence the name &lsquo;delayed
</p>
<p>decision or choice&rsquo;); see Fig. 6.4. The operation can be executed so quickly that a
</p>
<p>&lsquo;notification&rsquo; of the photon would have to be superluminal.
</p>
<p>Thus, the photon has to &lsquo;decide&rsquo; whether it passes through the MZI as a coherent
</p>
<p>superposition (BS2 is inserted and only D1 responds) or whether it passes through
</p>
<p>only one of the two arms (BS2 is removed, the respective detector responds). The
</p>
<p>salient point is that the photon must take the decision after it has passed the first beam
</p>
<p>splitter (and possibly the mirror) but before we decide whether BS2 will be left in the
</p>
<p>path or not. That would mean (at least in a classical argument) that the photon must
</p>
<p>know before entering BS1 whether BS2 will be left or removed. In other words, the
</p>
<p>photon had to know about our future decision. Does that mean that delayed-choice
</p>
<p>experiments prove that certain events may have a retroactive action with respect to
</p>
<p>time?12
</p>
<p>With a similar setup, one can produce a quantum eraser, with which one can
</p>
<p>subsequently delete (&lsquo;erase&rsquo;) which-way information in certain experiments and thus
</p>
<p>restore their interference effects (see AppendixM, Vol. 1).
</p>
<p>6.3.4 The Hadamard Transformation
</p>
<p>The Hadamard transformation plays an important role in quantum information. It
</p>
<p>can be carried out by means of the MZI. Another method that can be experimentally
</p>
<p>realized uses the combination of a beam splitter and a phase shifter. Written as a
</p>
<p>12Experiments are not confined to small distances. See e.g. F. Vedovato et al., Extending Wheeler&rsquo;s
</p>
<p>delayed-choice experiment to space,Science AdvancesVol. 3, no.10, https://doi.org/10.1126/sciadv.
</p>
<p>1701180 (Oct 2017), where a delayed-choice experiment is reported with a propagation distance
</p>
<p>of up to 3500km.</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1126/sciadv.1701180">https://doi.org/10.1126/sciadv.1701180</a></div>
<div class="annotation"><a href="https://doi.org/10.1126/sciadv.1701180">https://doi.org/10.1126/sciadv.1701180</a></div>
</div>
<div class="page"><p/>
<p>84 6 Interaction-Free Measurement
</p>
<p>2&times; 2 matrix, the Hadamard transformation H is (see Appendix P, Vol. 2 for the case
n &times; n):
</p>
<p>H = 1&radic;
2
</p>
<p>(
</p>
<p>1 1
</p>
<p>1 &minus;1
</p>
<p>)
</p>
<p>. (6.22)
</p>
<p>6.3.5 From the MZI to the Quantum Computer
</p>
<p>The MZI, with additional phase shifts, can be described as a network consisting of
</p>
<p>three simple quantum logic gates, namely as a combination of two Hadamard gates
</p>
<p>and a phase shifter. On this basis, other building blocks of quantum information such
</p>
<p>as the CNOT gate can be constructed (see Chap.26, Vol. 2, and Appendix Q, Vol. 2).
</p>
<p>6.3.6 Hardy&rsquo;s Experiment
</p>
<p>This experiment combines an interaction-free measurement with quantum entangle-
</p>
<p>ment. This concept (which we will learn about in Chap.20, Vol. 2) is another key
</p>
<p>aspect of quantum mechanics which has no classical counterpart. The experiment
</p>
<p>consists essentially of two superimposed MZI&rsquo;s (see Appendix J, Vol. 2).
</p>
<p>6.3.7 How Interaction-Free is the &lsquo;Interaction-Free&rsquo;
</p>
<p>Quantum Measurement?
</p>
<p>Finally, a few words about the adjective &lsquo;interaction-free&rsquo;. Indeed, we should always
</p>
<p>put it in quotationmarks. This is due to the fact that, strictly speaking, this experiment
</p>
<p>can never be completely &lsquo;interaction-free&rsquo;; there is an operator that describes the
</p>
<p>behavior of the photon in the interferometer, and this operator takes on a different
</p>
<p>form depending on whether a bomb is placed in the light path or not. Taking this
</p>
<p>into account, the term &lsquo;measurement with minimal interaction&rsquo; is more correct and
</p>
<p>insofar preferable.
</p>
<p>This is because there is a fundamental limit to the attainable sensitivity of the
</p>
<p>detonator of the bomb, and the measurement can be called interaction-free at most
</p>
<p>within the limits of this sensitivity. The reason for this limitation is the uncertainty
</p>
<p>principle xp &ge; /2. It is the basis for the following argument: If the bomb
(the detonator) is locatedwith an uncertaintyx , then a givenmomentumuncertainty
</p>
<p>p results (forx &rarr; 0, we would havep = &infin;). To prevent the bomb from going
off &lsquo;by itself&rsquo;, the detonator must not respond to momentum transfers smaller than
</p>
<p>p. In other words, the uncertainty principle necessarily requires that the bomb have
</p>
<p>an &lsquo;ignition threshold&rsquo;. Under such circumstances one cannot speak of &lsquo;interaction-</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 Concluding Remarks 85
</p>
<p>free&rsquo;; amore appropriate term ismeasurement with minimal interaction. Alongwith a
</p>
<p>momentum transfer, there is also a possible energy transfer. The fact that this transfer
</p>
<p>can be very small in macroscopic objects (&sim;1/M) and vanishes in the limit M &rarr; &infin;
does not fundamentally alter the situation.
</p>
<p>Conclusion: There is no &lsquo;interaction-free&rsquo; quantum measurement, i.e. a measure-
</p>
<p>ment without interaction, but at most a measurement with minimal interaction. It is
</p>
<p>perhaps surprising that the term &lsquo;interaction-free&rsquo; has established itself in the physics
</p>
<p>community (almost) without difficulty. On the other hand, one must admit that this
</p>
<p>term is very striking and much more effective in catching public attention than the
</p>
<p>more correct expressions (just as the term &lsquo;ozone hole&rsquo; is in use rather than the more
</p>
<p>correct &lsquo;stratospheric region of low ozone concentration&rsquo;). Thus, the interaction-free
</p>
<p>quantum measurement is another example of the fact that physics operates not only
</p>
<p>as pure science, but also through its perceptions by the larger society.
</p>
<p>6.4 Exercises
</p>
<p>1. Show that for all |zi  in (6.5), |zi 2 = 1 holds.
2. Given a MZI with symmetrical beam splitters, calculate the final state with and
</p>
<p>without a blocker if the initial state is given by  |H +  |V .
3. Given an operator A with
</p>
<p>A |H = a |H; A |V  = b |V , (6.23)
</p>
<p>determine the explicit form of A.
</p>
<p>4. Which eigenvalues can a unitary operator have?
</p>
<p>5. Circularly- and linearly-polarized states are connected by |r = 1&radic;
2
|h + i&radic;
</p>
<p>2
|v
</p>
<p>and |l = 1&radic;
2
|h &minus; i&radic;
</p>
<p>2
|v. Show that this basis transformation is unitary (or that
</p>
<p>the transformation matrix is unitary).
</p>
<p>6. Give the matrix representations of the operators T , S and S&prime; from (6.11)&ndash;(6.13)
and their combinations T ST and T S&prime;T .
</p>
<p>7. Given the operator
</p>
<p>U = a |H H | + b |H V | + c |V  H | + d |V  V | &sim;=
(
</p>
<p>a b
</p>
<p>c d
</p>
<p>)
</p>
<p>; (6.24)
</p>
<p>for which values of the coefficients is U a unitary operator? In other words: How
</p>
<p>is the general two-dimensional unitary transformation formulated?
</p>
<p>8. Given a MZI without a blocker and with asymmetrical beam splitters (transmit-
</p>
<p>tance = reflectance), determine the properties required of the beam splitters in
order that a beam entering horizontally activates only detector 1, while detector
</p>
<p>2 remains dark.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 7
</p>
<p>Position Probability
</p>
<p>We establish the concept of probability within the analytical approach to quantum mechanics
</p>
<p>in the form of the position probability density and its associated probability current density
</p>
<p>In the algebraic approach to quantum mechanics, we introduced early on the notion of
</p>
<p>probability. Now we want to develop this concept in the analytical approach, as well,
</p>
<p>and furthermore we aim at merging the two approaches gradually. The problem is as
</p>
<p>follows: In the algebraic approach, probabilities appear rather naturally (due to the
</p>
<p>plausible redefinition of intensity&rarr; probability). The SEq, however, is deterministic.
An initial state fixes the time evolution of the wavefunction for all times, and clearly
</p>
<p>this leaves no room for chance.
</p>
<p>Therefore, probabilities cannot come into play from the SEq itself, but only
</p>
<p>through the wavefunction  (x, t). As we already briefly mentioned in Chap. 5,
</p>
<p>the absolute square of  (x, t) can be regarded as the position probability density,1
</p>
<p>usually denoted by the letter :
</p>
<p> (x, t) = &lowast; (x, t) (x, t) = | (x, t)|2 (7.1)
</p>
<p>The interpretation of  as a probability density is not at all obvious, and in the
</p>
<p>early days of quantum mechanics it took some time until Max Born arrived at this
</p>
<p>concept. At that point (and still for us at present), it was a hypothesis or conjecture
</p>
<p>which had to prove itself by leading to consistent results and conclusions (which of
</p>
<p>course it did).2
</p>
<p>We will develop this concept in the following and will discuss its consequences.
</p>
<p>1The probability w (probability of finding the quantum object in a given region of space) is obtained
</p>
<p>by integrating the probability density , as in w =
&int;
</p>
<p>dV , over the spatial region of interest.
</p>
<p>Analogously, the mass m is given as an integral over the mass density  as m =
&int;
</p>
<p>dV .
2Especially when one is speaking to lay people about probabilities in quantum mechanics, one
</p>
<p>should always keep in mind that this is a conceptually difficult notion. On the one hand, there is
</p>
<p>the wavefunction with its abstractness, not understandable in everyday terms. On the other hand, it
</p>
<p>is just this wavefunction which allows us to determine concrete values of probabilities. The How
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4_7
</p>
<p>87</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_7&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_7&amp;domain=pdf</a></div>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4_7">https://doi.org/10.1007/978-3-030-00464-4_7</a></div>
</div>
<div class="page"><p/>
<p>88 7 Position Probability
</p>
<p>7.1 Position Probability and Measurements
</p>
<p>7.1.1 Example: Infinite Potential Wall
</p>
<p>This section is intended to serve primarily as a brief motivation.
</p>
<p>We want to calculate the probability of finding an object with well-defined energy
</p>
<p>in the infinite potential well within the interval 0 &lt; x1 &lt; x2 &lt; a. Classically this is
</p>
<p>quite simple3; the probability is evidently given by
</p>
<p>wclx1,x2 =
x2 &minus; x1
</p>
<p>a
. (7.2)
</p>
<p>For the quantum-mechanical analysis, we assume a state with the given energy
</p>
<p>En (see Chap. 5):
</p>
<p> (x, t) = e&minus;i En t/
&radic;
</p>
<p>2
</p>
<p>a
sin
</p>
<p>n
</p>
<p>a
x; En =
</p>
<p>
2
</p>
<p>2m
</p>
<p>(n
</p>
<p>a
</p>
<p>)2
</p>
<p>; n = 1, 2, . . . (7.3)
</p>
<p>We consider the expression
</p>
<p>wqmx1,x2 =
x2
&int;
</p>
<p>x1
</p>
<p>&lowast; (x, t) (x, t) dx . (7.4)
</p>
<p>As outlined in Chap. 5, we choose as the first factor under the integral not (x, t),
</p>
<p>but rather the complex conjugate wavefunction &lowast; (x, t). This guarantees that we
always obtain positive expressions for the probability, since &lowast; (x, t) (x, t) &ge; 0;
this is as required of a probability. A simple calculation leads us to
</p>
<p>wqmx1,x2 =
x2 &minus; x1
</p>
<p>a
&minus;
</p>
<p>sin
(
</p>
<p>n x2&minus;x1
a
</p>
<p>)
</p>
<p>cos
(
</p>
<p>n x2+x1
a
</p>
<p>)
</p>
<p>n
. (7.5)
</p>
<p>The comparison of (7.2) and (7.5) suggests the interpretation of w
qm
x1,x2 as the
</p>
<p>probability of finding the object in the interval [x1, x2]. This has the consequence that
</p>
<p>we can interpret &lowast; (x, t) (x, t) = ||2 as a probability density. We see (compare
also Fig. 7.1) that the quantum-mechanical probability becomes increasingly similar
</p>
<p>to the classical one with increasing n, i.e. with increasing energy. This behavior is
</p>
<p>typical of many quantum-mechanical phenomena: The quantum character becomes
</p>
<p>all the more clearer, the lower the energies (low with respect to the energy scale of
</p>
<p>the system under consideration), and vice versa.
</p>
<p>and Why are certainly not intuitively obvious and cannot be formulated convincingly with the aid
</p>
<p>of familiar everyday ideas.
3The velocity is constant between the turning points.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 Position Probability and Measurements 89
</p>
<p>Fig. 7.1 Position probability
</p>
<p>(7.5) as a function of
</p>
<p>z = x2&minus;x1
a
</p>
<p>for x2 = a 1+z2
and x1 = a 1&minus;z2 . The
situation is shown for n = 1
(red), n = 10 (green) and
n = 1000 (blue). The latter
case is graphically
</p>
<p>indistinguishable from the
</p>
<p>classical straight line wclx1,x2
given in (7.2)
</p>
<p>7.1.2 Bound Systems
</p>
<p>We start with the time-dependent SEq:
</p>
<p>i (x, t) = H(x, t). (7.6)
</p>
<p>Using the separation ansatz
</p>
<p> (x, t) = e&minus;i
Et
 (x), (7.7)
</p>
<p>we obtain the time-independent SEq:
</p>
<p>H (x) = E(x). (7.8)
</p>
<p>In this paragraph we assume that there are only discrete and no continuous eigen-
</p>
<p>values, as discussed in Sect. 5.3. The eigenvalues and the eigenfunctions are given
</p>
<p>by En = n and n (x), and the total solution reads
</p>
<p> (x, t) =
&sum;
</p>
<p>n
</p>
<p>cnn (x) e
&minus;i En t
</p>
<p> ; cn &isin; C, (7.9)
</p>
<p>with the initial state
</p>
<p> (x, 0) =
&sum;
</p>
<p>n
</p>
<p>cnn(x). (7.10)
</p>
<p>The orthonormality of the eigenfunctions, already mentioned in Chap. 5, is impor-
</p>
<p>tant for the following discussion (as we will show later on, this is a common feature
</p>
<p>of the eigenfunctions of all the Hamiltonians we will deal with in this book):</p>
<p/>
</div>
<div class="page"><p/>
<p>90 7 Position Probability
</p>
<p>&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p>&lowast;n (x)l (x) dx = nl . (7.11)
</p>
<p>Since the total wavefunction  is a solution of a linear differential equation,
</p>
<p>multiples of it are also solutions. We choose a multiple so that  is normalized, i.e.
</p>
<p>&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p>|(x, t)|2 dx =
&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p>(x, t)dx = 1. (7.12)
</p>
<p>In short, we can always assume  to be normalized.
</p>
<p>We now interpret |(x, t)|2 as a (position) probability density. Thus, the last equa-
tion implies that the quantum object is located with probability 1 (i.e. with certainty)
</p>
<p>somewhere in space, as it must be. The probability that the object is localized in a
</p>
<p>particular region, say a &le; x &le; b at time t , is given by (as in (7.4)):
</p>
<p>w(a &le; x &le; b, t) =
b
</p>
<p>&int;
</p>
<p>a
</p>
<p>|(x, t)|2 dx . (7.13)
</p>
<p>Clearly, this probability is always positive definite, and the total probability
&int;&infin;
&minus;&infin; |(x, t)|
</p>
<p>2 dx equals one.4 Thus we have found for the wavefunction not an
</p>
<p>immediate, but at least an indirect physical significance,5 in that its absolute square
</p>
<p>can be viewed as the position probability density.6
</p>
<p>The extension of these considerations to three dimensions causes no problems.
</p>
<p>7.1.2.1 Conclusions
</p>
<p>What are the conclusions one can draw? We insert the total wavefunction (7.9) into
</p>
<p>(7.12) and obtain in the first step
</p>
<p>1
!=
</p>
<p>&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p>&sum;
</p>
<p>n
</p>
<p>c&lowast;n
&lowast;
n (x) e
</p>
<p>in t
&sum;
</p>
<p>l
</p>
<p>cll (x) e
&minus;il t dx . (7.14)
</p>
<p>4The fact that we can actually interpret this as a probability is shown by the general definition.
</p>
<p>A probability measure &micro; on R is a mapping &micro; from the set of intervals (which are given here by
</p>
<p>the integration intervals) into the unit interval [0, 1] which meets the following requirements: (i)
</p>
<p>&micro;(I ) = 1 &ge; 0 for all intervals I (positive definite), (ii) &micro;(R) = 1 (normalized), (iii) &micro; (I1 &cup; I2) =
&micro;(I1)+ &micro;(I2) for all pairwise disjoint intervals I1 and I2 (additivity property or  additivity).
5The wavefunction itself is non-intuitive&mdash;it is just a complex-valued field of possibilities, as men-
</p>
<p>tioned above.
6Since the concept is unique, one often omits the term &lsquo;position&rsquo; and uses the more compact
</p>
<p>&lsquo;probability density&rsquo;. We will do so also, for the most part.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 Position Probability and Measurements 91
</p>
<p>Under the usual assumption that we can interchange the sum and the integral, and
</p>
<p>with the notation of the two sums as a double sum, we obtain
</p>
<p>1 =
&sum;
</p>
<p>n,l
</p>
<p>c&lowast;ne
in t cle
</p>
<p>&minus;il t
&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p>&lowast;n (x)l (x) dx
</p>
<p>=
&sum;
</p>
<p>n,l
</p>
<p>c&lowast;ncle
i(n&minus;l )tn,l =
</p>
<p>&sum;
</p>
<p>n
</p>
<p>c&lowast;ncn =
&sum;
</p>
<p>n
</p>
<p>|cn|2. (7.15)
</p>
<p>In other words: the fact that  is normalized is equivalent to
</p>
<p>&sum;
</p>
<p>n
</p>
<p>|cn|2 = 1. (7.16)
</p>
<p>This equation is valid independently of time, so we can limit ourselves in our further
</p>
<p>considerations to t = 0.
With (7.16), we have found the same relation as in the algebraic approach: The
</p>
<p>absolute squares of the coefficients give the probabilities of finding the corresponding
</p>
<p>states or quantum numbers. We illustrate this point by means of two concepts: mean
</p>
<p>value and collapse.
</p>
<p>Mean value. We consider an ensemble of identically-prepared systems (7.10),
</p>
<p>where the measured quantity is the energy.7 If we measure N members of the
</p>
<p>ensemble, we observe the state n (x) (or the energy En) rn times, where of course
</p>
<p>N =
&sum;
</p>
<p>n rn . As usual, the mean value of the energy is found to be
</p>
<p>Emean value =
&sum;
</p>
<p>n
hn En (7.17)
</p>
<p>with the relative frequencies of occurrence hn = rn/N . For N &rarr; &infin;, the relative
frequencies hn become the probabilities |cn|2 of measuring the state n (x) or the
energy En , and we obtain the expectation value
</p>
<p>Eexpectation value =
&sum;
</p>
<p>n
|cn|2 En. (7.18)
</p>
<p>These concepts and the question of how they can be extended to continuous variables
</p>
<p>will be discussed further in Chap. 9.
</p>
<p>Collapse. We can apply the concept of probability to individual systems, with which
</p>
<p>one mainly deals in practice. Before a single measurement, we can say that by mea-
</p>
<p>suring (7.10) we will obtain one of the states n , say  j (x), with the probability
</p>
<p>w j =

</p>
<p>c j

</p>
<p>
</p>
<p>2
. After the measurement, the system is in a well-defined state, let us say
</p>
<p>l (x). Thus we know immediately after the measurement the state of the system
</p>
<p>with certainty:
</p>
<p>7Those who wish may consider the infinite potential well as a concrete example.</p>
<p/>
</div>
<div class="page"><p/>
<p>92 7 Position Probability
</p>
<p>cn = 0 for n 	= l directly after measurement (7.19)
</p>
<p>or, formulated explicitly,8
</p>
<p>before (x, t) =
&sum;
</p>
<p>n
</p>
<p>cnn (x) e
&minus;i En t
</p>
<p> &rarr;
measurement
</p>
<p>after (x, t) =
cl
</p>
<p>|cl |
l (x) e
</p>
<p>&minus;i El t
 .
</p>
<p>(7.20)
</p>
<p>We see that the measurement has forced the system into a unique state. We have
</p>
<p>already met up with this process of state reduction in the algebraic approach. That it
</p>
<p>now also occurs here is not due to the SEq. There is an additional element, namely,
</p>
<p>our interpretation of the wavefunction as a probability amplitude or a complex-valued
</p>
<p>field of possibilities.
</p>
<p>The following picture emerges: The SEq describes the unperturbed time evolution
</p>
<p>of a quantum system. This evolution is interrupted by the measurement process,
</p>
<p>which changes the wavefunction. One also speaks of the collapse of the wavefunction.
</p>
<p>After the measurement, the system is again subject to the time evolution described
</p>
<p>by the SEq.9
</p>
<p>As in the algebraic approach, there are open questions concerning the measure-
</p>
<p>ment. For example, if the measurement process is not included in the SEq, does this
</p>
<p>mean that measurement is not a quantum-mechanical process? Or is our description
</p>
<p>by means of the SEq plus measurement process insufficient? Or is it simply the best
</p>
<p>we can ever achieve, because nature is in reality not as simple as described by our
</p>
<p>theories? In short, what does &lsquo;to measure&rsquo; actually mean in quantum mechanics?
</p>
<p>7.1.3 Free Systems
</p>
<p>In the case of free, unbounded systems, we have seen that an initial situation of the
</p>
<p>form (we limit ourselves to one dimension)
</p>
<p>(x, 0) = | (x, 0)|2 =
1
</p>
<p>&radic;
b0
</p>
<p>exp
</p>
<p>(
</p>
<p>&minus;
x2
</p>
<p>b20
</p>
<p>)
</p>
<p>(7.21)
</p>
<p>evolves in the course of time into
</p>
<p>(x, t) = | (x, t)|2 =
1
</p>
<p>&radic;
b (t)
</p>
<p>exp
</p>
<p>(
</p>
<p>&minus;
(
</p>
<p>x &minus; K
m
</p>
<p>t
)2
</p>
<p>b2 (t)
</p>
<p>)
</p>
<p>, (7.22)
</p>
<p>with b (t) given by
</p>
<p>8The state must be normalized after the measurement; this is expressed by the factor cl|cl | .
9We note again that the exact process of measurement itself is not described.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 Position Probability and Measurements 93
</p>
<p>b (t) =
</p>
<p>&radic;
</p>
<p>b20 +
(
</p>
<p>t
</p>
<p>b0m
</p>
<p>)2
</p>
<p>. (7.23)
</p>
<p>We repeat the following remark (cf. Sect. 5.2.2): We model here the quantum-like
</p>
<p>behavior of material bodies (m 	= 0) such as electrons. The spreading of (x, t)
does not mean that the electron itself is &lsquo;smeared out&rsquo; in space (in that case, the
</p>
<p>smearing would also apply to the electron&rsquo;s properties such as its mass and charge),
</p>
<p>like a mound of honey which flattens and spreads. It is the wavefunction, which
</p>
<p>determines the position probability, that disperses, rather than the object itself. In
</p>
<p>other words, the uncertainty with which we can determine the location of a quantum
</p>
<p>object increases in the course of time: x &asymp; b(t).
Again the question arises: What happens when we perform a measurement? Let
</p>
<p>us assume that we have arrayed detectors along the entire x axis, each of length a.
</p>
<p>Now we release a free quantum object; the detectors are still switched off. We wait
</p>
<p>sufficiently long to be sure that x &asymp; b(t)  a holds. Then we measure the position
of the object by activating the detectors &mdash; one of them will respond. At that moment,
</p>
<p>the spatial uncertainty, which had grown steadily before our measurement, shrinks
</p>
<p>abruptly to a, and the wavefunction is correspondingly modified.10 This means that
</p>
<p>we again observe the connection between the measurement process and the collapse
</p>
<p>of the wavefunction (or state reduction).
</p>
<p>The considerations about the mean value which were outlined above for discrete
</p>
<p>eigenvalues cannot readily be applied to continuous measurements of quantities such
</p>
<p>as the position or the momentum. We will address this issue again in Chap. 9, and will
</p>
<p>formulate it so generally that the nature of the eigenvalue spectrum will no longer
</p>
<p>be relevant.
</p>
<p>7.2 Real Potentials
</p>
<p>The probability density  is positive definite, which follows directly from the defini-
</p>
<p>tion (7.1). The probability of localizing the quantum object at time t in the interval
</p>
<p>[x1, x2] is given by W (x1 &lt; x &lt; x2; t) =
&int; x2
</p>
<p>x1
 (x, t) dx . In order to indeed interpret
</p>
<p> as a probability density, the equation11
</p>
<p>&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p> (x, t) dx
!= 1 &forall; t (7.24)
</p>
<p>must hold. In words: The quantum object must be located somewhere in space, and
</p>
<p>this must be true at all times. Therefore, two requirements must be met:
</p>
<p>10We can regard this state as the initial condition for a new cycle of free propagation, in which case
</p>
<p>one refers to the measurement process as a (state-)preparation.
11We assume that there are neither creation nor annihilation processes.</p>
<p/>
</div>
<div class="page"><p/>
<p>94 7 Position Probability
</p>
<p>1. The integral
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
| (x, t)|2 dx has to exist, at least at a certain time t . If it does,
</p>
<p>we can normalize the wave function so that at this time t ,
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
| (x, t)|2 dx = 1
</p>
<p>holds.
</p>
<p>2. In addition, we must show that the normalization constant does not change, so that
&int; &infin;
</p>
<p>&minus;&infin;
| (x, t)|2 dx = 1 is valid for all times.
</p>
<p>Can we always satisfy these two requirements?
</p>
<p>The first requirement means that  (x, t) must be square integrable in view of
</p>
<p>the interpretation of |(x, t)|2 as a probability density. This is certainly the case
over a finite interval of space when the wavefunction is sufficiently smooth or &lsquo;well-
</p>
<p>behaved&rsquo; (i.e. does not have singularities, etc.), which we always assume in the
</p>
<p>following. In order that the integral from &minus;&infin; to &infin; exists, the condition
</p>
<p> &sim;
|x |&rarr;&infin;
</p>
<p>|x | ; &lt; &minus;
1
</p>
<p>2
(7.25)
</p>
<p>must be fulfilled in addition, at least at some time t . One often describes this condition
</p>
<p>by saying that the wavefunction must approach zero rapidly enough at infinity.12 We
</p>
<p>note in this context that there may be correct mathematical solutions of differential
</p>
<p>equations that must still be excluded for physical reasons. More about this issue is
</p>
<p>included in some of the following chapters and in Appendix E, Vol. 1.
</p>
<p>As to the second requirement13: We have to show that
&int;&infin;
&minus;&infin;  (x, t) dx = 1 holds
</p>
<p>at all times. This means that
</p>
<p>d
</p>
<p>dt
</p>
<p>&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p> (x, t) dx
!= 0, (7.26)
</p>
<p>and it follows14 that
</p>
<p>0
!=
</p>
<p>&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p>&part;
</p>
<p>&part;t
&lowast;dx =
</p>
<p>&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p>(
</p>
<p>&lowast; +&lowast;
)
</p>
<p>dx . (7.27)
</p>
<p>We replace the time derivatives making use of the SEq
</p>
<p>12In three dimensions, the condition is slightly different. Because of
&int;
</p>
<p>dV =
&int;
</p>
<p>r2dr sin dd,
</p>
<p>the wavefunction has to go to zero as r with  &lt; &minus; 3
2
</p>
<p>.
13With the conceptual framework derived in later chapters, the proof may be formulated in a
</p>
<p>considerably shorter way.
14As always, we assume the commutability of differentiation and integration. See Appendix D,
</p>
<p>Vol. 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 Real Potentials 95
</p>
<p>i = &minus;

</p>
<p>2
</p>
<p>2m
 &prime;&prime; + V (7.28)
</p>
<p>and find, assuming a real potential V &isin; R,15
</p>
<p>0
!=
</p>
<p>&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p>[(
</p>
<p>
</p>
<p>2mi
&lowast;&prime;&prime; &minus;
</p>
<p>V
</p>
<p>i
&lowast;
</p>
<p>)
</p>
<p> +&lowast;
(
</p>
<p>&minus;

</p>
<p>2mi
 &prime;&prime; +
</p>
<p>V
</p>
<p>i

</p>
<p>)]
</p>
<p>dx
</p>
<p>=

</p>
<p>2mi
</p>
<p>&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p>(
</p>
<p>&lowast;
&prime;&prime;
 &minus;&lowast; &prime;&prime;
</p>
<p>)
</p>
<p>dx . (7.29)
</p>
<p>We transform the second derivatives w.r.t. spatial coordinates by partial integra-
</p>
<p>tion. It follows that
</p>
<p>0
!=
</p>
<p>
</p>
<p>2mi
</p>
<p>
</p>
<p>
</p>
<p>(
</p>
<p>&lowast;&prime;
)&infin;
&minus;&infin; &minus;
</p>
<p>&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p>&lowast;&prime; &prime;dx
</p>
<p>
</p>
<p>&minus;

</p>
<p>2mi
</p>
<p>
</p>
<p>
</p>
<p>(
</p>
<p>&lowast; &prime;
)&infin;
&minus;&infin; &minus;
</p>
<p>&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p>&lowast;&prime; &prime;dx
</p>
<p>
</p>
<p> .
</p>
<p>(7.30)
</p>
<p>The integrals cancel each other and we finally obtain
</p>
<p>(
</p>
<p>&lowast;&prime; &minus;&lowast; &prime;
)

</p>
<p>
</p>
<p>&infin;
&minus;&infin;
</p>
<p>!= 0. (7.31)
</p>
<p>This condition is fulfilled due to (7.25), since for  &lt; &minus; 1
2
, we have  &prime; &sim;
</p>
<p>|x |&rarr;&infin;
|x |2&minus;1 &rarr; 0.
</p>
<p>We see that the probability concept is inherently consistent, if the wavefunction
</p>
<p>vanishes at infinity rapidly enough and if the potential is real. These are very important
</p>
<p>properties, which we assume to be always fulfilled from now on.16
</p>
<p>7.3 Probability Current Density
</p>
<p>In the following, an expression for the (position) probability current density is
</p>
<p>derived. We rely on the continuity equation17
</p>
<p>&part;
</p>
<p>&part;t
+&nabla;j = 0. (7.32)
</p>
<p>15Here, the potential may depend on the time t .
16Complex potentials are required when one wants to describe e.g. absorption processes. These
</p>
<p>potentials are also called optical potentials (referring to the complex optical refractive index whose
</p>
<p>imaginary part describes absorption). An example is found in the exercises.
17The derivation of the continuity equation is given in Appendix N, Vol. 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>96 7 Position Probability
</p>
<p>This equation is a differential formulation of a global conservation law. It is valid
</p>
<p>not only for the mass density, but in fact applies to all densities (e.g. the charge density)
</p>
<p>for which integral conservation laws hold (e.g. global conservation of charge).
</p>
<p>In particular, we assume the validity of the continuity equation for the probability
</p>
<p>density of quantum mechanics. Thus, we can calculate the probability current den-
</p>
<p>sity j. For the sake of simplicity, we consider only the one-dimensional problem and
</p>
<p>extend the result at the end to three dimensions.
</p>
<p>In one dimension, the continuity equation reads
</p>
<p>(x, t)+
&part;
</p>
<p>&part;x
j (x, t) = 0. (7.33)
</p>
<p>To derive the relationship between j and , we insert  = ||2. With  =
&lowast; +&lowast; and the SEq
</p>
<p> = &minus;

</p>
<p>2mi
 &prime;&prime; +
</p>
<p>V
</p>
<p>i
, (7.34)
</p>
<p>we can rewrite the continuity equation as
</p>
<p>(
</p>
<p>
</p>
<p>2mi
&lowast;&prime;&prime; &minus;
</p>
<p>V &lowast;&lowast;
</p>
<p>i
</p>
<p>)
</p>
<p> +&lowast;
(
</p>
<p>&minus;

</p>
<p>2mi
 &prime;&prime; +
</p>
<p>V
</p>
<p>i
</p>
<p>)
</p>
<p>+
&part;
</p>
<p>&part;x
j = 0. (7.35)
</p>
<p>Since we assume V &isin; R, the potential terms cancel. It follows that
</p>
<p>&part;
</p>
<p>&part;x
j =
</p>
<p>
</p>
<p>2mi
</p>
<p>(
</p>
<p>&lowast; &prime;&prime; &minus;&lowast;&prime;&prime;
)
</p>
<p>=

</p>
<p>2mi
&lowast; &prime;&prime; &minus;&lowast;&prime;&prime; +&lowast;&prime; &prime; &minus;&lowast;&prime; &prime;)
</p>
<p>=

</p>
<p>2mi
</p>
<p>(
</p>
<p>&part;
</p>
<p>&part;x
&lowast; &prime; &minus;
</p>
<p>&part;
</p>
<p>&part;x
&lowast;&prime;
</p>
<p>)
</p>
<p>. (7.36)
</p>
<p>Integration gives18:
</p>
<p>j (x, t) =

</p>
<p>2mi
(&lowast; &prime; &minus;&lowast;&prime;). (7.37)
</p>
<p>We have thus found an expression for the probability current density. We already
</p>
<p>know that it vanishes at infinity; see (7.31).
</p>
<p>The extension of the probability current density to three dimensions yields in a
</p>
<p>straightforward manner:
</p>
<p>j (r, t) =

</p>
<p>2mi
(&lowast;&nabla; &minus;&nabla;&lowast;). (7.38)
</p>
<p>18Actually, there could still be a constant of integration on the right-hand side, but it is set equal to
</p>
<p>zero due to the requirement j = 0 for  = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Probability Current Density 97
</p>
<p>As an (unphysical, but familiar19) example, we consider a plane wave
</p>
<p> (r, t) = Aei(kr&minus;t). (7.39)
</p>
<p>With
</p>
<p>&nabla; (r, t) = Aikei(kr&minus;t), (7.40)
</p>
<p>it follows that
</p>
<p>j(r, t) =

</p>
<p>2mi
</p>
<p>(
</p>
<p>ikAA&lowast; + ikAA&lowast;
)
</p>
<p>=
k
</p>
<p>m
|A|2. (7.41)
</p>
<p>Because of  = &lowast; = |A|2, we obtain the well-known relationship
</p>
<p>j =
k
</p>
<p>m
 =
</p>
<p>p
</p>
<p>m
 := v (7.42)
</p>
<p>where v is the velocity of e.g. a maximum of the wave.20
</p>
<p>We make some general remarks on the one-dimensional probability current den-
</p>
<p>sity j = 
2im
</p>
<p>(
</p>
<p>&lowast;&prime; &minus; &lowast;&prime;
)
</p>
<p>:
</p>
<p>1. For (x) = Aex ( &isin; R, A&isin; C), we have
</p>
<p>j =

</p>
<p>2im
</p>
<p>(
</p>
<p> |A|2 e2x &minus;  |A|2 e2x
)
</p>
<p>= 0. (7.43)
</p>
<p>With real exponents, j disappears. To put it graphically, this does not mean that
</p>
<p>nothing flows into the region or out of it, but rather that whatever flows in must also
</p>
<p>flow out again.
</p>
<p>2. For (x) = Aeix ( &isin; R, A&isin; C), we have
</p>
<p>j =

</p>
<p>2im
</p>
<p>(
</p>
<p>i |A|2 + i |A|2
)
</p>
<p>=

</p>
<p>m
|A|2. (7.44)
</p>
<p>So there is a &lsquo;net flow&rsquo;, that is, something is actually transported.
</p>
<p>19Unphysical, because the infinitely-extended plane wave, whose magnitude is one everywhere,
</p>
<p>does not represent a physical object. The fact that we can still make use of plane waves in quantum
</p>
<p>mechanics is due to the linearity of quantum mechanics, which allows us to construct wave packets
</p>
<p>with physically reasonable behavior by superposition of plane waves.
20We note that the &lsquo;velocity&rsquo; of a quantum object is a seldom-used notion in quantum mechan-
</p>
<p>ics. The momentum is the central quantity. &lsquo;Velocity&rsquo; will appear only in the context of Galilean
</p>
<p>transformations (relative motion of inertial frames, Chap. 21, Vol. 2) and in the Bohmian interpre-
</p>
<p>tation of quantum mechanics (see Chap. 28, Vol. 2), which is based on classical mechanics.</p>
<p/>
</div>
<div class="page"><p/>
<p>98 7 Position Probability
</p>
<p>7.4 Exercises
</p>
<p>1. Show for  = | (x, t)|2 that:
</p>
<p>&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p> (x, t) dx = 1 &forall; t. (7.45)
</p>
<p>Here, we assume that (i) the potential is real, and (ii)  &sim;
x&rarr;&infin;
</p>
<p>xa , with a &lt; &minus; 1
2
.
</p>
<p>2. Infinite potential well: Given the wave functions
</p>
<p>(a)  (x, t) = e&minus;in t
&radic;
</p>
<p>2
a
</p>
<p>sin n
a
</p>
<p>x
</p>
<p>(b)  (x, t) = cne&minus;in t
&radic;
</p>
<p>2
a
</p>
<p>sin n
a
</p>
<p>x + cme&minus;im t
&radic;
</p>
<p>2
a
</p>
<p>sin m
a
</p>
<p>x ,
</p>
<p>Calculate for both cases the probability of finding the quantum object in the
</p>
<p>interval (x1, x2)
</p>
<p>wqmx1,x2 =
x2
&int;
</p>
<p>x1
</p>
<p>&lowast; (x, t) (x, t) dx . (7.46)
</p>
<p>3. Given the SEq i = H with a real potential, derive from the continuity
equation constructively (i.e. not just proving by insertion) that j is given by
</p>
<p>j =

</p>
<p>2mi
(&lowast;&nabla; &minus; &nabla;&lowast;). (7.47)
</p>
<p>4. Calculate j (one-dimensional) for  = Aex and  = Aeix , with  &isin; R and
A &isin; C.
</p>
<p>5. Calculate j (r, t) for  (r, t) = Aei(kr&minus;t).
6. Given a modification of the infinite potential well, namely the potential
</p>
<p>V (x) =
{
</p>
<p>iW for 0 &lt; x &lt; a
</p>
<p>&infin; otherwise ; W &isin; R, (7.48)
</p>
<p>calculate the energy spectrum and show that the norm of the (time-dependent)
</p>
<p>total wavefunction is independent of time only for W = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 8
</p>
<p>Neutrino Oscillations
</p>
<p>Thus far, in the algebraic approach we have not considered the question of the time evolution
</p>
<p>of a system. We now want to tackle this topic on the basis of a problem of current interest.
</p>
<p>In addition, we meet up with Hermitian operators, and we address once again the problem
</p>
<p>of measurement.
</p>
<p>8.1 The Neutrino Problem
</p>
<p>As is well known, the neutrino  was originally postulated by Wolfgang Pauli in
</p>
<p>order to &lsquo;save&rsquo; the conservation of energy in beta decay. As it turned out later after
</p>
<p>careful examination. Each of the three elementary particles, the electron e, the muon
</p>
<p>&micro; and the tauon  has its &lsquo;own&rsquo; neutrino, i.e. e, &micro;, and  .
1The rest mass of all three
</p>
<p>neutrinos seemed to be vanishingly small, and it was generally assumed to be zero.
</p>
<p>Change of scene:We consider now the sun and the particleswhich it emits. Among
</p>
<p>them are the three neutrino species, and those in a certain ratio, which can be deter-
</p>
<p>mined reasonably reliably on the basis of current solar models. But measurements
</p>
<p>on earth yielded a rather different value for this ratio. The question was: Are the solar
</p>
<p>models incorrect, or is something wrong with our description of neutrinos?
</p>
<p>1Wolfgang Pauli in 1930 initially chose the name &lsquo;neutron.&rsquo; The term &lsquo;neutrino&rsquo;was introduced later
</p>
<p>by Enrico Fermi. In 1956, the electron neutrino was detected experimentally for the first time, and
</p>
<p>in 1962, the muon neutrino. The tauon was observed in 1975, but the corresponding neutrino only
</p>
<p>in 2000. There may be still other types of neutrinos. These (as yet hypothetical) sterile neutrinos
</p>
<p>interact only via gravity and not&mdash;like the other neutrinos&mdash;through the weak interaction (hence
</p>
<p>the adjective &lsquo;sterile&rsquo;). See e.g. D. Castelvecchi, Icy telescope throws cold water on sterile neutrino
</p>
<p>theory, Nature, https://doi.org/10.1038/nature.2016.20382 (Aug 2016), and literature referenced
</p>
<p>there.
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4_8
</p>
<p>99</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_8&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_8&amp;domain=pdf</a></div>
<div class="annotation"><a href="https://doi.org/10.1038/nature.2016.20382">https://doi.org/10.1038/nature.2016.20382</a></div>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4_8">https://doi.org/10.1007/978-3-030-00464-4_8</a></div>
</div>
<div class="page"><p/>
<p>100 8 Neutrino Oscillations
</p>
<p>There were good arguments to regard the solar models as correct. So something
</p>
<p>had to be changed in the description of the neutrinos. And it was this: If one assumes
</p>
<p>that the rest masses of the neutrinos are not exactly zero, the three neutrino species
</p>
<p>can change into each other over the course of time (neutrino oscillations); that is, on
</p>
<p>the way from the sun to the earth. In this way it could be explained that on earth, we
</p>
<p>measure a different relative abundance of the three neutrinos than is predicted by the
</p>
<p>solar models.
</p>
<p>8.2 Modelling the Neutrino Oscillations2
</p>
<p>We will now describe the process of neutrino oscillations, as simply as possible.
</p>
<p>In order to make clear the principle, we confine ourselves to a simpler model with
</p>
<p>only two neutrinos, since the computations for three neutrinos are more complicated.
</p>
<p>A fewwords about the three-dimensional case can be found at the end of this chapter.
</p>
<p>In this chapter, wewill for once visit the field of relativistic phenomena.We can do
</p>
<p>so because we need only the statement that there is a Hamiltonian (and in particular
</p>
<p>its energy eigenvalues) for the physical problem, without having to worry about its
</p>
<p>specific form or any details of the interaction.
</p>
<p>8.2.1 States
</p>
<p>We start with the production of neutrinos (e.g. in the sun or in an accelerator) as a
</p>
<p>superposition of two states |1 and |2 with well-defined, different rest masses m01
and m02, called mass (eigen-)states. The momenta are equal, but the total ener-
</p>
<p>gies E1 and E2 are therefore different.
3 Without loss of generality, we can set
</p>
<p>m := m01 &minus; m02 &gt; 0 and hence E := E1 &minus; E2 &gt; 0 or  = 1 &minus; 2 (with
 = E/). The states form a CONS; i
</p>
<p>
</p>
<p> j
&rang;
</p>
<p>= i j and |1 1| + |2 2| = 1.4
For certain reasons, one cannot measure the states |1 and |2 directly, but only
</p>
<p>superpositions of these states, which we call (referring to the actual situation) the
</p>
<p>electron neutrino and the muon neutrino, |e and

</p>
<p>&micro;
&rang;
</p>
<p>(also termed flavor states).
</p>
<p>We have
</p>
<p>|e = cos |1 + sin  |2

</p>
<p>&micro;
&rang;
</p>
<p>= &minus; sin  |1 + cos |2. (8.1)
</p>
<p>2The importance of the issue can be seen e.g. from the fact that the Nobel Prize in Physics 2015 was
</p>
<p>awarded jointly to Takaaki Kajita (born 1959, Japanese physicist) and Arthur B. McDonald (born
</p>
<p>1943, Canadian physicist) &ldquo;for the discovery of neutrino oscillations, which shows that neutrinos
</p>
<p>have mass&rdquo;.
3We remind the reader: E2 = m20c4 + p2c2.
4Call the Hamiltonian for free neutrino motion H . We have H |1 = E1 |1 and H |2 = E2 |2
with E = E1 &minus; E2 &gt; 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Modelling the Neutrino Oscillations 101
</p>
<p>Here,  is an (abstract) angle, called the mixing angle. The states |e and

</p>
<p>&micro;
&rang;
</p>
<p>also
</p>
<p>form a CONS. Therefore, we can represent |1 and |2 as a superposition of |e
and
</p>
<p>
</p>
<p>&micro;
&rang;
</p>
<p>. We then have
</p>
<p>|1 = cos |e &minus; sin 

</p>
<p>&micro;
&rang;
</p>
<p>|2 = sin  |e + cos |&micro;. (8.2)
</p>
<p>In fact, these transformations are nothing more than rotations by the angle&plusmn;within
a two-dimensional space, or equivalently, a change of basis, which is described by
</p>
<p>the well-known transformation
</p>
<p>(
</p>
<p>cos &plusmn; sin 
 sin  cos
</p>
<p>)
</p>
<p>. (8.3)
</p>
<p>It represents a particularly simple example of a unitary matrix.
</p>
<p>8.2.2 Time Evolution
</p>
<p>Next, we want to investigate the time evolution of the states |e and

</p>
<p>&micro;
&rang;
</p>
<p>. To this end
</p>
<p>we use use the fact, found from the analytical approach, that the time evolution of
</p>
<p>a state with well-defined energy E is described by the factor e&minus;i Et/. Although this
requirement suggests itself, it is not self-evident that it must be satisfied here. If we
</p>
<p>accept that it holds true (or regard it as an axiom for the moment), we find: If at time
</p>
<p>zero an initial state |z(t = 0) = |z(0) exists with the well-defined energy E = ,
its time evolution is described by
</p>
<p>|z(t) = |z(0) e&minus;i Et/. (8.4)
</p>
<p>This is a very important and universally valid fact in quantum mechanics. It follows
</p>
<p>that
</p>
<p>i
d
</p>
<p>dt
|z(t) = E | z(t). (8.5)
</p>
<p>Ifwe assume that E is an eigenvalue of an operator H , we have essentially &lsquo;recovered&rsquo;
</p>
<p>the free SEq.5
</p>
<p>We see that the time evolution (8.4) is a unitary process that conserves the norm:
</p>
<p>z(t)| z(t) = z(0)| eit e&minus;it | z(0) = z(0)| z(0). (8.6)
</p>
<p>We now take a muon-neutrino as the initial state |(0), i.e. |(0) =

</p>
<p>&micro;
&rang;
</p>
<p>. Then
</p>
<p>it follows with (8.1) for the time evolution:
</p>
<p>5Here, H denotes a (still) unknown operator and not the well-known operator&minus; 2
2m
</p>
<p>&nabla;
2+V . Double
</p>
<p>meanings of this type are quite common in quantum mechanics. We will learn the reason for this
</p>
<p>in later chapters.</p>
<p/>
</div>
<div class="page"><p/>
<p>102 8 Neutrino Oscillations
</p>
<p>|(t) = &minus; sin  |1 e&minus;i1t + cos |2 e&minus;i2t . (8.7)
</p>
<p>Evidently, we have |1 ||2 =

</p>
<p>&minus; sin e&minus;i1t

</p>
<p>
</p>
<p>2 = sin2 &mdash;this would be the prob-
ability of obtaining |1 in a measurement. But since we can measure only the states
|e and
</p>
<p>
</p>
<p>&micro;
&rang;
</p>
<p>, we have to project the corresponding portions out of |(t), by means
of the projection operators |e e| and
</p>
<p>
</p>
<p>&micro;
&rang; &lang;
</p>
<p>&micro;

</p>
<p>. With (8.2) we find, for example,
</p>
<p>e| 1 = cos and e| 2 = sin . It follows for the electron neutrino that
</p>
<p>|e e |(T ) =
[
</p>
<p>&minus; sin  cose&minus;i1T + cos sin e&minus;i2T
]
</p>
<p>|e. (8.8)
</p>
<p>It is seen that this term includes both frequencies 1 and 2 and thus displays a very
</p>
<p>different behavior from the mass states. We obtain the probability of measuring |e
by the usual application of the absolute square of the prefactor (see the exercises):
</p>
<p>pe (T ) =

</p>
<p>&minus; sin  cose&minus;i1T + cos sin e&minus;i2T

</p>
<p>
</p>
<p>2 = sin2 2 &middot; sin2
(
</p>
<p>
</p>
<p>2
T
</p>
<p>)
</p>
<p>.
</p>
<p>(8.9)
</p>
<p>8.2.3 Numerical Data
</p>
<p>Equation (8.9) shows that the probability to find the neutrino in the state |e depends
periodically on time, where the period is  = 2
</p>
<p>
. The neutrino oscillates between
</p>
<p>the states |e and

</p>
<p>&micro;
&rang;
</p>
<p>; see Fig. 8.1. This is quite similar to two coupled pendulums
</p>
<p>which show beats, in which the energy flows periodically from one pendulum to the
</p>
<p>other.
</p>
<p>To get a feeling for the order of magnitudes, we perform a rough calculation.
</p>
<p>We can assume in good approximation that the neutrinos, due to their low mass,
</p>
<p>are moving with nearly the speed of light. In space, we have a period of length
</p>
<p>L = c = c 2

</p>
<p>. We approximate the difference  by (see the exercises)
</p>
<p> = c
4
</p>
<p>2pc
</p>
<p>(
</p>
<p>m21 &minus; m22
)
</p>
<p>:= c
4m2
</p>
<p>2pc
. (8.10)
</p>
<p>It follows
</p>
<p>L = c 2

</p>
<p>= 4
c2
</p>
<p>p
</p>
<p>m2
. (8.11)
</p>
<p>This term is most easily evaluated in the theoretical unit system in which
</p>
<p> = c = 1 and energies and masses are measured in eV, see AppendixB, vol. 1.6 The
</p>
<p>6Numerical examples: the electron in this system of units has a rest mass of about 0.5MeV. The
</p>
<p>accelerator LHC operates with protons of energies of up to 7TeV.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Modelling the Neutrino Oscillations 103
</p>
<p>Fig. 8.1 pe(T ) of (8.9) for  = /6
</p>
<p>mass difference between neutrinos7 is about m2 &asymp; 10&minus;3 (eV)2, the momentum is
10GeV=1010 eV. We then find
</p>
<p>L = 4 10
10
</p>
<p>10&minus;3 eV
= 4 10
</p>
<p>19
</p>
<p>MeV
, (8.12)
</p>
<p>and with the conversion of units of length 1
MeV
</p>
<p>= 0.1973&times; 10&minus;12m it follows finally
</p>
<p>L = 4 &times; 1019 &times; 0.1973&times; 10&minus;12 m &asymp; 25000 km. (8.13)
</p>
<p>Of course we should not take the numerical value too seriously&mdash;we have considered
</p>
<p>only two instead of three neutrinos, and just the uncertainty regarding the mass
</p>
<p>difference leaves a wide margin for error. What is instead important is that we can at
</p>
<p>least qualitatively describe an effect such as neutrino oscillations, and this with only
</p>
<p>the simplest of formal means.
</p>
<p>8.2.4 Three-Dimensional Neutrino Oscillations
</p>
<p>The neutrino question remains an issue of ongoing research, since there are still some
</p>
<p>unresolved problems.8 We will not go into this more deeply, but give only a very
</p>
<p>7Of course, this is a key parameter&mdash;if it is 10&minus;6 eV instead of 10&minus;3 eV, then the length increases
correspondingly by a factor of 1000.
8A recent review which also contains the values cited in Table8.1 is given by G.L. Fogli et al.,
</p>
<p>&lsquo;Global analysis of neutrino masses, mixings and phases: entering the era of leptonic CP violation
</p>
<p>searches&rsquo;, http://arXiv.org/abs/1205.5254v3 (2012).</p>
<p/>
<div class="annotation"><a href="http://arXiv.org/abs/1205.5254v3">http://arXiv.org/abs/1205.5254v3</a></div>
</div>
<div class="page"><p/>
<p>104 8 Neutrino Oscillations
</p>
<p>Table 8.1 Values of the
</p>
<p>mixing angles
s212 = 0.307 or 12 &asymp; 34
</p>
<p>s223 = 0.5 or 23 &asymp; 45
</p>
<p>s213 = 0.021 or 13 &asymp; 8
</p>
<p>brief comment on the three-dimensional problem. One assumes three flavor states
</p>
<p>and three mass states:
</p>
<p>
</p>
<p>
</p>
<p>e
&micro;

</p>
<p>
</p>
<p> = U &middot;
</p>
<p>
</p>
<p>
</p>
<p>1
2
3
</p>
<p>
</p>
<p>. (8.14)
</p>
<p>With the abbreviations si j = sin i j and ci j = cos i j , the transformation matrix U is
written as
</p>
<p>U =
</p>
<p>
</p>
<p>
</p>
<p>1 0 0
</p>
<p>0 c23 s23
0 &minus;s23 c23
</p>
<p>
</p>
<p> &middot;
</p>
<p>
</p>
<p>
</p>
<p>c13 0 s13e
&minus;i
</p>
<p>0 1 0
</p>
<p>&minus;s13ei 0 c13
</p>
<p>
</p>
<p> &middot;
</p>
<p>&times;
</p>
<p>
</p>
<p>
</p>
<p>c12 s12 0
</p>
<p>&minus;s12 c12 0
0 0 1
</p>
<p>
</p>
<p> &middot;
</p>
<p>
</p>
<p>
</p>
<p>ei1/2 0 0
</p>
<p>0 ei2/2 0
</p>
<p>0 0 1
</p>
<p>
</p>
<p>.
</p>
<p>(8.15)
</p>
<p>The first three of these four unitary matrices describe (from left to right) the changes
</p>
<p>&micro; &harr;  , e &harr;  , and e &harr; &micro;. The phases  (Dirac phase) and i (Majorana
phase) are introduced as a result of further considerations.9 As a product of unitary
</p>
<p>matrices, the matrix U is itself again unitary (see the exercises).
</p>
<p>For the angles, current values are given in Table8.1.
</p>
<p>For the mass differences, one finds m2 = m221 = m22 &minus; m21 = 7.5&times; 10&minus;5 eV2
</p>
<p>and m2 = m223 = m23 &minus;
m21+m22
</p>
<p>2
= 2.5 &times; 10&minus;3 eV2. The unit eV is defined in
</p>
<p>AppendixB, Vol. 1.10
</p>
<p>For several reasons, new discoveries of the neutrino&rsquo;s properties are expected to
</p>
<p>change our understanding of the universe. Thus, neutrinos are a topic of ongoing
</p>
<p>research, see e.g. E. Gibney, Morphing neutrinos provide clue to antimatter mystery,
</p>
<p>Nature https://doi.org/10.1038/nature.2016.20405 (Aug 2016). A most important
</p>
<p>9The first three matrices are (except for the phase shift ) the rotation matrices
</p>
<p>Dx (23) Dy (13) Dz (12). The first matrix describes e.g. a rotation by the angle 23 around the x
</p>
<p>axis.
10The values for the mixing angles and the mass differences are from Neutrino Mixing - Particle
</p>
<p>Data Group, pdg.lbl.gov/2017/listings/rpp2017-list-neutrino-mixing.pdf (30. 5. 2017). The precise
</p>
<p>determination of these angles is a current topic; see for instance Eugenie S. Reich, &lsquo;Neutrino
</p>
<p>oscillations measured with record precision&rsquo;, Nature 08March 2012, where the measurement of the
</p>
<p>angle 13 is discussed, or P. Adamson et al. (NOvA Collaboration), Measurement of the Neutrino
</p>
<p>Mixing Angle 23 in NOvA, Phys. Rev. Lett 118, 151802 (10. 4. 2017).</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1038/nature.2016.20405">https://doi.org/10.1038/nature.2016.20405</a></div>
</div>
<div class="page"><p/>
<p>8.2 Modelling the Neutrino Oscillations 105
</p>
<p>open issue is the absolute mass scale of neutrinos. This question will be investi-
</p>
<p>gated e.g. by the KATRIN experiment, launched in June 2018 (KArlsruhe TRItium
</p>
<p>Neutrino, Karlsruhe, Germany).
</p>
<p>8.3 Generalizations
</p>
<p>8.3.1 Hermitian Operators
</p>
<p>In this section, wewant to generalize the findings obtained on the basis of the neutrino
</p>
<p>problem. First, we extend the formulation (8.5) to a &lsquo;proper&rsquo; SEq
</p>
<p>i
d
</p>
<p>dt
| (t) = H | (t) . (8.16)
</p>
<p>Apart from the mere analogy to the SEq in the analytical approach, the motivation
</p>
<p>for this step is that we want to find a linear differential equation of first order in time
</p>
<p>also for the algebraic approach. It is clear that we have at this point no information
</p>
<p>about the operator H which appears in (8.16)&mdash;neither how it is constructed internally
</p>
<p>(spatial derivatives as in theLaplace operator cannot occur here), nor about its relation
</p>
<p>to the Hamiltonian used in the analytical approach. These points will be discussed
</p>
<p>in later chapters.
</p>
<p>Here we want to clarify which properties H must have in order that the evolution
</p>
<p>of | (t) be unitary, which means that the scalar product  (t) | (t) must be
constant for all times. With this in mind, we write (8.16) and the adjoint equation in
</p>
<p>compact form
</p>
<p>i

</p>
<p>(t)
&rang;
</p>
<p>= H |(t); &minus;i
&lang;
</p>
<p>(t)

</p>
<p> = (t)| H &dagger;. (8.17)
</p>
<p>If  (t) | (t) does not depend on time, it follows that
</p>
<p>i
d
</p>
<p>dt
 (t) | (t) = i
</p>
<p>&lang;
</p>
<p> (t) | (t) + i  (t)

</p>
<p> (t)
&rang;
</p>
<p>= 0. (8.18)
</p>
<p>We insert (8.17) and obtain
</p>
<p>&minus; (t)| H &dagger; | (t) +  (t)| H |(t) =  (t)| H &minus; H &dagger; |(t) = 0. (8.19)
</p>
<p>Since this equation holds for every |(t), it follows that H &dagger; = H .
In general, an operator A is called self-adjoint or Hermitian if A = A&dagger;. The
</p>
<p>importance of such operators in quantummechanics lies in the fact that all physically-
</p>
<p>measurable quantities are represented by self-adjoint operators. Indeed, Hermitian</p>
<p/>
</div>
<div class="page"><p/>
<p>106 8 Neutrino Oscillations
</p>
<p>operators have real eigenvalues11 as we want to show now. Let A be a Hermitian
</p>
<p>operator, A = A&dagger;. Then the eigenvalue problem and its adjoint version read:
</p>
<p>A |an = n |an and an| A&dagger; = &lowast;n an|. (8.20)
</p>
<p>Multiplication of the first equation from the left by an| and of the second equation
from the right by |an leads, due to A = A&dagger;, to
</p>
<p>an| A |an = n an |an and an| A&dagger; |an = an| A |an = &lowast;n an | an. (8.21)
</p>
<p>The comparison showsn = &lowast;n , i.e.n &isin; R. Other properties of Hermitian operators
are discussed in the following chapters.
</p>
<p>In Chap.4, we made the acquaintance of projection operators, and in Chap.6 of
</p>
<p>unitary operators, and now Hermitian operators join in.12 The good news is that
</p>
<p>the zoo of operators13 of quantum mechanics is complete&mdash;we will be concerned
</p>
<p>only (to be exact, with one exception) with these three types of operators (or the
</p>
<p>corresponding matrices or other representations):
</p>
<p>A = A&dagger; Hermitian operator
AA&dagger; = A&dagger;A = 1 unitary operator
A2 = A projection operator.
</p>
<p>(8.22)
</p>
<p>The names are used also for the corresponding matrices and representations. We
</p>
<p>outline in brief form the applications of these operators:We can represent physically-
</p>
<p>measurable quantities by Hermitian operators; the unperturbed time evolution of a
</p>
<p>system is described by a unitary operator; and the measurement process can be
</p>
<p>modelled with the help of projection operators.
</p>
<p>8.3.2 Time Evolution and Measurement
</p>
<p>We denote the eigenvalues and eigenvectors of H in (8.16) by En and |n. The
general solution as a generalization of (8.7) is a superposition of the eigenvectors:
</p>
<p>| (t) =
&sum;
</p>
<p>n
</p>
<p>cn |n e&minus;i En t/ (8.23)
</p>
<p>where the integration constants ci are determined by the initial conditions (see the
</p>
<p>exercises).
</p>
<p>11Since measured values are real, we can interpret them as eigenvalues of Hermitian operators.
12These properties are not mutually exclusive: A unitary operator or a projection operator can also
</p>
<p>be e.g. Hermitian.
13Since these operators exhibit only a few species and are fairly well-behaved, one could also speak
</p>
<p>of a &lsquo;pet zoo&rsquo;.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Generalizations 107
</p>
<p>A measurement interrupts the time evolution of | (t) as described in (8.23). If
we want to measure e.g. the state |, then we can describe it by projecting onto |;
that is by the term |  | corresponding to (8.8). Here, | ||2 is the probability
that we actually obtain | from a measurement:
</p>
<p>| ||2 =
&sum;
</p>
<p>n
</p>
<p>cn  |n e&minus;i En t/
&sum;
</p>
<p>m
</p>
<p>c&lowast;m m | ei Em t/
</p>
<p>=
&sum;
</p>
<p>n,m
</p>
<p>cnc
&lowast;
m  |n m | e&minus;i(En&minus;Em )t/. (8.24)
</p>
<p>After or due to the measurement, we have the state | instead of |.
We remark that all these considerations hold for systems of arbitrary dimensions.
</p>
<p>8.4 Exercises
</p>
<p>1. Given that |1 1| + |2 2| = 1, show: |e e| +

</p>
<p>&micro;
&rang; &lang;
</p>
<p>&micro;

</p>
<p> = 1.
</p>
<p>2. Show that the matrices
</p>
<p>
</p>
<p>
</p>
<p>c 0 se&minus;i
</p>
<p>0 1 0
</p>
<p>&minus;sei 0 c
</p>
<p>
</p>
<p> and
</p>
<p>
</p>
<p>
</p>
<p>1 0 0
</p>
<p>0 c s
</p>
<p>0 &minus;s c
</p>
<p>
</p>
<p> with  &isin; R are
</p>
<p>unitary. The abbreviations s and c stand for sin and cos.
</p>
<p>3. Show that the product of two unitary matrices is also unitary.
</p>
<p>4. Is the beam splitter operator T from Chap.6,
</p>
<p>T = 1+ i
2
</p>
<p>[1+ i |H V | + i |V  H |], (8.25)
</p>
<p>a Hermitian, a unitary or a projection operator? {|H, |V } is a CONS.
5. Given A =
</p>
<p>(
</p>
<p>1 i
</p>
<p>&minus;i 1
</p>
<p>)
</p>
<p>:
</p>
<p>(a) Show that A is Hermitian, but not unitary.
</p>
<p>(b) Calculate ecA.
</p>
<p>6. Given the operators14
</p>
<p>L1 =
|v (u| + w|)+ (|u + |w) v|&radic;
</p>
<p>2
</p>
<p>L2 =
&minus; |v (u| &minus; w|)+ (|u &minus; |w) v|
</p>
<p>i
&radic;
2
</p>
<p>L3 = |u u| &minus; |w w|. (8.26)
</p>
<p>14These are essentially the three components of the orbital angular momentum operator for angular
</p>
<p>momentum 1; see Chap.16, Vol. 2.</p>
<p/>
</div>
<div class="page"><p/>
<p>108 8 Neutrino Oscillations
</p>
<p>(a) Are these Hermitian, unitary or projection operators?
</p>
<p>(b) Calculate [L1, L2].
</p>
<p>7. Show that the time evolution
</p>
<p>|(t) = &minus; sin  |1 e&minus;i1t + cos |2 e&minus;i2t (8.27)
</p>
<p>is unitary.
</p>
<p>8. Determine explicitly e |(t) in (8.8), and
&lang;
</p>
<p>&micro; |(t).
9. Determine explicitly pe in (8.9), and p&micro;.
</p>
<p>10. Prove (8.10); find an approximation forE in the case of very small rest masses.
</p>
<p>11. Given the state
</p>
<p>| (t) =
&sum;
</p>
<p>n
</p>
<p>cn |n e&minus;i En t/ (8.28)
</p>
<p>with the initial condition | (0). {|n} is a CONS. How are the constants cn
related to the initial conditions?
</p>
<p>12. Given two CONS {|i } and {|i }. A quantum system is in the superposition
|z =
</p>
<p>&sum;
</p>
<p>i di |i .
(a) Calculate the probability of measuring the quantum system in the state |k.
(b) Show that
</p>
<p>&sum;
</p>
<p>k pk = 1.
13. Given the model system
</p>
<p>i
d
</p>
<p>dt
| (t) = H | (t) with H = 1+ Ay; A &gt; 0, (8.29)
</p>
<p>where y is the y-Pauli matrix:
</p>
<p>(a) Determine the eigenvalues and eigenvectors of H ;
</p>
<p>(b) How does the general expression | (t) read for a time-dependent state?
(c) How is | (t) expressed for the initial state | (t = 0) =
</p>
<p>(
</p>
<p>1
</p>
<p>0
</p>
<p>)
</p>
<p>?
</p>
<p>(d) Assume that we measure | (t) from part c. With which probability will
we find the state | =
</p>
<p>(
</p>
<p>1
</p>
<p>0
</p>
<p>)
</p>
<p>(i.e. the initial state)?</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 9
</p>
<p>Expectation Values, Mean Values,
</p>
<p>and Measured Values
</p>
<p>The probability concept is further expanded. In addition, we look at Hermitian operators
</p>
<p>in more detail. The time behavior of mean values leads to the notion of conserved quantities.
</p>
<p>We continue the discussion started in Chap.7 on the calculation of the mean value
</p>
<p>of measured quantities, and generalize the formalism so that it is also applicable to
</p>
<p>the continuous case. As in the algebraic approach, the formulations lead us again
</p>
<p>to Hermitian operators, which are of particular importance in quantum mechanics.
</p>
<p>Furthermore, we address conserved quantities and establish a connection to classical
</p>
<p>mechanics.
</p>
<p>9.1 Mean Values and Expectation Values
</p>
<p>9.1.1 Mean Values of Classical Measurements
</p>
<p>In classical physics, it is assumed that there is a &lsquo;true&rsquo; value of each physical quantity
</p>
<p>which can be measured. Measuring this quantity several times, e.g. the location x ,
</p>
<p>one will in general obtain different values xi , where each value occurs with the
</p>
<p>frequency of occurrence ni . The cause of the discrepancies between different values
</p>
<p>are inadequacies in the measuring apparatus (apart from the possibly varying skill
</p>
<p>of the experimenter). For l different readings, the total number of measurements
</p>
<p>amounts to N =
&sum;l
</p>
<p>i = 1 ni . The mean value x (average) is defined as
1
</p>
<p>x =
</p>
<p>&sum;l
i = 1 ni xi
</p>
<p>&sum;l
i = 1 ni
</p>
<p>=
</p>
<p>&sum;l
i = 1 ni xi
</p>
<p>N
=
</p>
<p>l
&sum;
</p>
<p>i = 1
</p>
<p>ni xi with
</p>
<p>l
&sum;
</p>
<p>i = 1
</p>
<p>ni = 1, (9.1)
</p>
<p>1Instead of x, the notation x is also common.
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4_9
</p>
<p>109</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_9&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_9&amp;domain=pdf</a></div>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4_9">https://doi.org/10.1007/978-3-030-00464-4_9</a></div>
</div>
<div class="page"><p/>
<p>110 9 Expectation Values, Mean Values, and Measured Values
</p>
<p>where the ni = ni/N are the relative frequencies which in the limit l &rarr; &infin; become
</p>
<p>the probabilities wi (Law of Large Numbers). In this limit, the average becomes the
</p>
<p>expected value or &lsquo;true&rsquo; value:
</p>
<p>x =
&sum;
</p>
<p>i
</p>
<p>pi xi with
&sum;
</p>
<p>i
</p>
<p>pi = 1. (9.2)
</p>
<p>This averaging concept is also applicable to sets of continuous data. We perform
</p>
<p>the familiar transition, known from school,2 of going from a sum
&sum;
</p>
<p>to an integral
&int;
</p>
<p>, and obtain with the probability density3  (x)
</p>
<p>x =
</p>
<p>&int;
</p>
<p> (x) xdx with
</p>
<p>&int;
</p>
<p> (x) dx = 1. (9.3)
</p>
<p>We generalize the last equation to three dimensions:
</p>
<p>x =
</p>
<p>&int;
</p>
<p> (x) xdV with
</p>
<p>&int;
</p>
<p> (x) dV = 1. (9.4)
</p>
<p>9.1.2 Expectation Value of the Position in Quantum
</p>
<p>Mechanics
</p>
<p>We wish to transfer these ideas to quantum mechanics. (r, t) is the solution of the
</p>
<p>time-dependent SEq. With the probability density
</p>
<p> = | (r, t)|2 (9.5)
</p>
<p>(recall that must be normalized), we can, as described above, determine the prob-
</p>
<p>ability w of finding the quantum object in a spatial region G as w(G) =
&int;
</p>
<p>G
dV .
</p>
<p>If we now ask for its mean position in x-direction, we can formulate in analogy to
</p>
<p>(9.4)
</p>
<p>x =
</p>
<p>&int;
</p>
<p>&lowast; (r, t) (r, t) x dV . (9.6)
</p>
<p>For a discussion of this situation, we imagine an ensemble of N identically-prepared
</p>
<p>quantum objects, all of which we launch at time t = 0 from x = 0 (one dimensional,
</p>
<p>moving to the right). After a time T , we find the ensemblemember i at the position xi .
</p>
<p>Then the mean value of x is given by x =
&sum;
</p>
<p>i xi , and this value agrees increasingly
</p>
<p>2See also the chapter &lsquo;Discrete and continuous&rsquo; in AppendixT, Vol. 1.
3For the special choice  (x) =
</p>
<p>&sum;
</p>
<p>i pi  (x &minus; xi ), we obtain the expression (9.2) from (9.3). For the
</p>
<p>delta function  (x), see AppendixH. Vol. 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Mean Values and Expectation Values 111
</p>
<p>better with the value given by (9.6) as N increases. In the limit N &rarr; &infin;, (9.6) is
</p>
<p>obtained exactly.4
</p>
<p>In three dimensions, it follows that
</p>
<p>r =
</p>
<p>&int;
</p>
<p>&lowast; (r, t) (r, t) r dV . (9.7)
</p>
<p>9.1.3 Expectation Value of the Momentum in Quantum
</p>
<p>Mechanics
</p>
<p>We now examine the momentum of the quantum object (the following calculation
</p>
<p>is one dimensional; the three-dimensional case is given below). We assume that the
</p>
<p>expectation value p obeys the equation
</p>
<p>d
</p>
<p>dt
x =
</p>
<p>1
</p>
<p>m
p . (9.8)
</p>
<p>This is an assumption at this point,5 which has to prove itself in the following (i.e.
</p>
<p>above all, it must lead to self-consistent results). It follows that:
</p>
<p>p = m
d
</p>
<p>dt
x = m
</p>
<p>d
</p>
<p>dt
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&lowast;xdx
</p>
<p>= m
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>(
</p>
<p>&lowast; +&lowast;
)
</p>
<p>xdx = m
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&lowast;xdx + c.c. (9.9)
</p>
<p>c.c.means the complex conjugate of the preceding term. We replace the time deriva-
</p>
<p>tives by space derivatives by means of the SEq i = &minus; 
2
</p>
<p>2m
 &prime;&prime; + V. We obtain
</p>
<p>(note: V &isin; R):
</p>
<p>p =

</p>
<p>2i
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&lowast;&prime;&prime;xdx + c.c. (9.10)
</p>
<p>Partial integration yields
</p>
<p>p =

</p>
<p>2i
</p>
<p>[
</p>
<p>(
</p>
<p>&lowast;&prime;x
)&infin;
</p>
<p>&minus;&infin;
&minus;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&lowast;&prime;
(
</p>
<p> &prime;x +
)
</p>
<p>dx
</p>
<p>]
</p>
<p>+ c.c. (9.11)
</p>
<p>4In fact, we cannot measure a point-like position xi , but instead only an intervalxi which contains
</p>
<p>the quantum object. However, with the idea that we can make the interval arbitrarily small, we can
</p>
<p>accept the above argument as a limiting case. More will be said on this issue in Chap.12.
5
&lang;
</p>
<p>d
dt
</p>
<p>x
&rang;
</p>
<p>= 1
m
p would be better.</p>
<p/>
</div>
<div class="page"><p/>
<p>112 9 Expectation Values, Mean Values, and Measured Values
</p>
<p>Since the wavefunction vanishes rapidly enough at infinity,6 the integrated part van-
</p>
<p>ishes. What remains is:
</p>
<p>p =
</p>
<p>{
</p>
<p>&minus;

</p>
<p>2i
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&lowast;&prime; &prime;xdx &minus;

</p>
<p>2i
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&lowast;&prime;dx
</p>
<p>}
</p>
<p>+ c.c. (9.12)
</p>
<p>The first term cancels with its c.c. It follows that
</p>
<p>p = &minus;

</p>
<p>2i
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&lowast;&prime;dx + c.c.
</p>
<p>= &minus;

</p>
<p>2i
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&lowast;&prime;dx +

</p>
<p>2i
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&lowast; &prime;dx . (9.13)
</p>
<p>A further integration by parts of &minus; 
2i
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
&lowast;&prime;dx leads to
</p>
<p>p =

</p>
<p>2i
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&lowast; &prime;dx &minus;

</p>
<p>2i
</p>
<p>{
</p>
<p>&lowast;

</p>
<p>
</p>
<p>&infin;
</p>
<p>&minus;&infin;
&minus;
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&lowast; &prime;dx
</p>
<p>}
</p>
<p>. (9.14)
</p>
<p>Again, the integrated term &lowast;|&infin;&minus;&infin; vanishes and we obtain the result
</p>
<p>p =

</p>
<p>i
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&lowast; &prime;dx, (9.15)
</p>
<p>or, if the other term in (9.13) is integrated by parts:
</p>
<p>p = &minus;

</p>
<p>i
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&lowast;&prime;dx . (9.16)
</p>
<p>As one can easily see, these terms can be written by using the momentum operator.
</p>
<p>With p = 
i
</p>
<p>d
dx
, we obtain7
</p>
<p>p =
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&lowast;
(
</p>
<p>
</p>
<p>i
</p>
<p>d
</p>
<p>dx

</p>
<p>)
</p>
<p>dx =
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>(
</p>
<p>
</p>
<p>i
</p>
<p>d
</p>
<p>dx

</p>
<p>)&lowast;
</p>
<p>dx
</p>
<p>=
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>&lowast; (p) dx =
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
</p>
<p>(p)&lowast; dx . (9.17)
</p>
<p>Two comments:
</p>
<p>1. The equality
&int; &infin;
</p>
<p>&minus;&infin;
&lowast; (p) dx =
</p>
<p>&int; &infin;
</p>
<p>&minus;&infin;
(p)&lowast; dx plays an important role in
</p>
<p>quantum mechanics in a slightly different notation: it applies not only to the
</p>
<p>6Recall that this behavior is necessary for the interpretation of ||2 as a probability density. See
</p>
<p>Chap.7.
7As usual we use the same symbol, p, for the physical quantity &lsquo;momentum&rsquo; and the corresponding
</p>
<p>operator. What is meant in each case should be clear from context.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Mean Values and Expectation Values 113
</p>
<p>momentum, but equally to all measurable physical quantities. We will return to
</p>
<p>this point later.
</p>
<p>2. The equality holds only if the wavefunction vanishes sufficiently rapidly at
</p>
<p>infinity.
</p>
<p>In the three-dimensional case, we find accordingly8:
</p>
<p>p =
</p>
<p>&int;
</p>
<p>&lowast;
(
</p>
<p>
</p>
<p>i
&nabla;
</p>
<p>)
</p>
<p>dV =
</p>
<p>&int; (
</p>
<p>
</p>
<p>i
&nabla;
</p>
<p>)&lowast;
</p>
<p>dV
</p>
<p>=
</p>
<p>&int;
</p>
<p>&lowast; (p) dV =
</p>
<p>&int;
</p>
<p>(p)&lowast; dV . (9.18)
</p>
<p>Similarly, one can derive the expectation value of the energy. It follows that
</p>
<p>E = H =
</p>
<p>&int;
</p>
<p>&lowast;HdV =
</p>
<p>&int;
</p>
<p>(H)&lowast; dV . (9.19)
</p>
<p>9.1.4 General Definition of the Expectation Value
</p>
<p>We summarize the results obtained so far:
</p>
<p>r =
</p>
<p>&int;
</p>
<p>&lowast;r  dV =
</p>
<p>&int;
</p>
<p>(r)&lowast; dV
</p>
<p>p =
</p>
<p>&int;
</p>
<p>&lowast;pdV =
</p>
<p>&int;
</p>
<p>(p)&lowast; dV
</p>
<p>H =
</p>
<p>&int;
</p>
<p>&lowast;HdV =
</p>
<p>&int;
</p>
<p>(H)&lowast; dV . (9.20)
</p>
<p>We generalize to an arbitrary operator A representing a measurable variable 9 and
</p>
<p>define its expectation value A by
</p>
<p>A =
</p>
<p>&int;
</p>
<p>&lowast;AdV . (9.21)
</p>
<p>We do not require equality with
&int;
</p>
<p>(A)&lowast;dV , as we did in (9.20). Actually, this
</p>
<p>does not apply to any operator, but only to a certain class of operators (Hermitian
</p>
<p>8As mentioned in Chap. 4, we use the shorthand notation of summation
&sum;
</p>
<p>n
instead of
</p>
<p>&sum;&infin;
</p>
<p>n=1
.
</p>
<p>Similarly, we also abbreviate integrals:
</p>
<p>&int;
</p>
<p>dV is not an indefinite, but a definite integral, which is
</p>
<p>carried out over the whole domain of definition of , namely
</p>
<p>&int;
</p>
<p>dV &equiv;
</p>
<p>&int;
</p>
<p>domain of definition
</p>
<p>dV .
</p>
<p>The range of integration is explicitly specified only in exceptional cases.
9An example is the angular momentum, l = r &times; p.</p>
<p/>
</div>
<div class="page"><p/>
<p>114 9 Expectation Values, Mean Values, and Measured Values
</p>
<p>operators); we come back to this point in a moment. We note that we have a tool with
</p>
<p>(9.21)10 to connect quite generally operators of quantummechanics with measurable
</p>
<p>quantities.11
</p>
<p>We first want to recover the expressions found in Chap.7. For this purpose we
</p>
<p>start from the eigenvalue problem
</p>
<p>Hn (x) = Enn (x);
</p>
<p>&int;
</p>
<p>&lowast;n (x)m (x) dx = nm . (9.22)
</p>
<p>The total state is
</p>
<p> (x, t) =
&sum;
</p>
<p>n
cnn (x) e
</p>
<p>&minus;i En t/. (9.23)
</p>
<p>Then it follows that
</p>
<p>H =
</p>
<p>&int;
</p>
<p>&lowast;Hdx =
</p>
<p>&int;
</p>
<p>&sum;
</p>
<p>n,m
</p>
<p>c&lowast;n
&lowast;
n (x) e
</p>
<p>i En t/ Hcmm (x) e
&minus;i Em t/dx
</p>
<p>=
</p>
<p>&int;
</p>
<p>&sum;
</p>
<p>n,m
</p>
<p>c&lowast;n
&lowast;
n (x) e
</p>
<p>i En t/ Emcmm (x) e
&minus;i Em t/dx
</p>
<p>=
&sum;
</p>
<p>n,m
</p>
<p>c&lowast;n Emcme
i(En&minus;Em )t/
</p>
<p>&int;
</p>
<p>&lowast;n (x)m (x) dx
</p>
<p>=
&sum;
</p>
<p>n,m
</p>
<p>c&lowast;n Emcme
i(En&minus;Em )t/nm =
</p>
<p>&sum;
</p>
<p>n
</p>
<p>|cn|
2 En. (9.24)
</p>
<p>So we have found again the familiar expression for the expectation value. As men-
</p>
<p>tioned above, the definition (9.21) has the advantage that it is readily applicable to
</p>
<p>continuous variables such as the position (see exercises).
</p>
<p>Some remarks are in order:
</p>
<p>1. The expectation value depends on the state. If necessary, one can include this by
</p>
<p>using e.g. the notation A =
&int;
</p>
<p>&lowast; AdV .
</p>
<p>2. In general, the expectation value is time dependent, but this is often not explicitly
</p>
<p>stated. For pure energy states (proportional to e&minus;it ), however, the time depen-
</p>
<p>dence cancels out when averaging over e.g. x . In such cases, the expectation
</p>
<p>values are independent of time (see exercises).
</p>
<p>3. A remark just to clarify our concepts: Strictly speaking, the mean value refers
</p>
<p>to a data set from the past, i.e. to a previously performed measurement, and it is
</p>
<p>formulated in terms of relative frequencies of occurrence. By contrast, the expec-
</p>
<p>tation value, as a conjecture about the future, is formulated with probabilities and
</p>
<p>is the theoretically-predicted mean. However, in quantum mechanics the notions
</p>
<p>10Another one we have already discussed above (e.g. in Chap. 4), namely that only the eigenvalues
</p>
<p>of operators can occur as measured values.
11One can show that this type of averaging must follow under very general conditions (Gleason&rsquo;s
</p>
<p>theorem, see AppendixT, Vol. 2).</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Mean Values and Expectation Values 115
</p>
<p>of expectation value and mean value are often used interchangeably due to a
</p>
<p>certain nonchalance of the physicists, and for a finite number of measurements,
</p>
<p>the term probabilitywi is applied instead of relative frequency (as indeed &lsquo;ensem-
</p>
<p>ble&rsquo; is used also for a finite set of identically prepared systems). A brief example
</p>
<p>in AppendixO, Vol. 1, illustrates the difference between the mean value and the
</p>
<p>expectation value.
</p>
<p>4. As stated at the beginning of this chapter, a repeated measurement of a classical
</p>
<p>quantity yields a different value each time. With continued repetition, the mean
</p>
<p>value of all these measured values shows an increasingly better agreement with
</p>
<p>the true value. If the measuring instruments were ideal, we would obtain the same
</p>
<p>value every time.
</p>
<p>In contrast, in quantum mechanics, successive measurements of an identical
</p>
<p>ensemble can in general give different values (corresponding to different eigen-
</p>
<p>values of the measured physical quantity) even with an ideal measurement appa-
</p>
<p>ratus.12 We have already mentioned that in a single experiment we can obtain
</p>
<p>only one eigenvalue of the operator which corresponds to the measured physical
</p>
<p>quantity. Which one of the eigenvalues this will be cannot be predicted before the
</p>
<p>experiment (if the state is given by a superposition). In other words: Quantum-
</p>
<p>mechanical variables generally have no &lsquo;true&rsquo; value.
</p>
<p>When we speak of the expectation value of a physical quantity A, this therefore
</p>
<p>does not imply that A has necessarily this value in the sense that classical quan-
</p>
<p>tities have &lsquo;true&rsquo; values. Instead of the expectation value, it would therefore be
</p>
<p>more cautious and unbiased to speak of the expected measured value or the like.
</p>
<p>However, such a terminology is not often used.
</p>
<p>9.1.5 Variance, Standard Deviation
</p>
<p>A convenient measure of the deviation from the mean value of a classical variable
</p>
<p>A (no doubt familiar from introductory laboratory courses) is the mean square devi-
</p>
<p>ation or variance (A)2:
</p>
<p>(A)2 =
&lang;
</p>
<p>(A &minus; A)2
&rang;
</p>
<p>=
&lang;
</p>
<p>A2
&rang;
</p>
<p>&minus; A2 . (9.25)
</p>
<p>To obtain the same physical unit as for A, one takes the root and obtains with A
</p>
<p>the standard deviation, also called dispersion or uncertainty. A brief motivation for
</p>
<p>the form of this expression is found in AppendixO, Vol. 1.
</p>
<p>We adopt this concept also in quantum mechanics. Since the uncertainty A
</p>
<p>depends in general on the state  of the system, there is also the notation  A or
</p>
<p>12If we send e.g. a horizontally linearly-polarized photon through a linear analyzer, rotated by the
</p>
<p>angle , we obtain, as discussed in Chap.2, different measurement results (horizontal or vertical
</p>
<p>polarization) with the probabilities cos2  and sin2 . This is true in principle and is not due to
</p>
<p>shortcomings of the measurement apparatus.</p>
<p/>
</div>
<div class="page"><p/>
<p>116 9 Expectation Values, Mean Values, and Measured Values
</p>
<p>the like. An example is given in the exercises, treating the uncertainty of position
</p>
<p>and momentum in the infinite potential well.
</p>
<p>We repeat a note on the significance of the standard deviation in quantummechan-
</p>
<p>ics (cf. remark 4, above): In classical physics, the standard deviationA is a measure
</p>
<p>of the dispersion of the measured values which arises due to instrumental imper-
</p>
<p>fections. In quantum mechanics, the meaning is quite different; A is not due to
</p>
<p>instrumental errors, but is an unavoidable genuine quantum effect. Successive mea-
</p>
<p>surements can yield different values even for ideal measuring equipment.13 If and
</p>
<p>only if  A = 0 is the quantum object in an eigenstate of the measured operator
</p>
<p>A and all members of an ensemble have the same value of the physical quantity A. An
</p>
<p>exercise illustrates that statement by considering the energy in the infinite potential
</p>
<p>well.
</p>
<p>In this sense, the quantum-mechanical dispersion may be seen as a measure of the
</p>
<p>extent to which a system &lsquo;has&rsquo; a value for A (A = 0) or &lsquo;has not&rsquo; (A &gt; 0). Thus,
</p>
<p>if in the infinite potential well (of width a), the energy eigenstate n has the position
</p>
<p>uncertaintyx = a
2
</p>
<p>&radic;
</p>
<p>n22&minus;6
3n22
</p>
<p>&sim; 0.3a (see the exercises), this does not mean that each
</p>
<p>(single) position measurement always has an error of this magnitude, but rather that
</p>
<p>the quantum object simply does not have a position in the classical sense. In other
</p>
<p>words, the concept of &lsquo;exact location&rsquo; is not appropriate to this quantum-mechanical
</p>
<p>problem. More on this issue will be given in later chapters.
</p>
<p>9.2 Hermitian Operators
</p>
<p>An essential property of measurement results is that they are real. If the expectation
</p>
<p>values represent measurable quantities, they must also be real. Therefore, it must
</p>
<p>hold that
</p>
<p>A = A&lowast; (9.26)
</p>
<p>or
&int;
</p>
<p>&lowast; (A) dV =
</p>
<p>&int;
</p>
<p>(A)&lowast; dV . (9.27)
</p>
<p>All operators in the small table (9.20) share this property. One can furthermore show
</p>
<p>for these operators that for two arbitrary functions14 1 and 2, the equation
</p>
<p>&int;
</p>
<p>&lowast;1 A2dV =
</p>
<p>&int;
</p>
<p>(A1)
&lowast; 2dV (9.28)
</p>
<p>13In view of this, some other expression than &lsquo;standard deviation&rsquo; would be more appropriate in
</p>
<p>quantum mechanics to indicate the spread of measurement results, but the mathematical simplicity
</p>
<p>of this expression has led to its widespread use.
14Arbitrary only insofar as the two functions have to satisfy the necessary technical requirements
</p>
<p>and the integrals have to exist.We note that theHermiticity of operatorsmay depend on the functions
</p>
<p>on which they act. This point is addressed explicitly in the exercises for this chapter.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Hermitian Operators 117
</p>
<p>holds. Generally, an operator A satisfying (9.28) is calledHermitian.We have already
</p>
<p>met up with Hermitian operators (and their representation as Hermitian matrices) in
</p>
<p>the algebraic approach, but these operators do not seem to have much to do with
</p>
<p>(9.28). But, contrary to that appearance, they in fact amount to the same thing, as we
</p>
<p>will see in more detail in Chap.11 under the topic &lsquo;matrix mechanics&rsquo;.
</p>
<p>As just pointed out, the expectation values of Hermitian operators are real. This
</p>
<p>makes sense, since in quantum mechanics all measurable quantities are represented
</p>
<p>by Hermitian operators. In addition, Hermitian operators in general have other very
</p>
<p>practical features: they have only real eigenvalues (which represent the possible
</p>
<p>individual measured values), and, in the case of a nondegenerate spectrum, their
</p>
<p>eigenfunctions are pairwise orthogonal to each other (as we have already seen in the
</p>
<p>example of the infinite potential well). We now want to prove these two properties.
</p>
<p>9.2.1 Hermitian Operators Have Real Eigenvalues
</p>
<p>We consider the eigenvalue equation
</p>
<p>A fn = an fn; n = 1, 2, . . . (9.29)
</p>
<p>where the operator A is Hermitian:
</p>
<p>&int;
</p>
<p>f &lowast;m A fndV =
</p>
<p>&int;
</p>
<p>(A fm)
&lowast; fndV . (9.30)
</p>
<p>We want to show now that its eigenvalues are real, i.e. an = a
&lowast;
n .
</p>
<p>To this end, we write the two equations:
</p>
<p>A fn = an fn; (A fn)
&lowast; = a&lowast;n f
</p>
<p>&lowast;
n . (9.31)
</p>
<p>We multiply the left equation by f &lowast;n and the right one by fn:
</p>
<p>f &lowast;n A fn = f
&lowast;
n an fn; (A fn)
</p>
<p>&lowast; fn = a
&lowast;
n f
</p>
<p>&lowast;
n fn. (9.32)
</p>
<p>Integration over all space yields
</p>
<p>&int;
</p>
<p>f &lowast;n A fndV = an
</p>
<p>&int;
</p>
<p>f &lowast;n fndV ;
</p>
<p>&int;
</p>
<p>(A fn)
&lowast; fndV = a
</p>
<p>&lowast;
n
</p>
<p>&int;
</p>
<p>f &lowast;n fndV . (9.33)
</p>
<p>Because of the Hermitian property of A, the two left-hand sides of these equations
</p>
<p>are the same. Therefore also the right sides have to be equal:
</p>
<p>an
</p>
<p>&int;
</p>
<p>f &lowast;n fndV = a
&lowast;
n
</p>
<p>&int;
</p>
<p>f &lowast;n fndV &harr;
(
</p>
<p>an &minus; a
&lowast;
n
</p>
<p>)
</p>
<p>&int;
</p>
<p>f &lowast;n fndV = 0. (9.34)</p>
<p/>
</div>
<div class="page"><p/>
<p>118 9 Expectation Values, Mean Values, and Measured Values
</p>
<p>Due to
&int;
</p>
<p>f &lowast;n fndV = 1 &#13;= 0, it follows that
</p>
<p>an = a
&lowast;
n &harr; an &isin; R. (9.35)
</p>
<p>We see that the eigenvalues of a Hermitian operator are real. This also holds, as
</p>
<p>said above, for the expectation values. We note again that the result of measuring a
</p>
<p>physical quantity can only be one of the eigenvalues of the corresponding operator.
</p>
<p>9.2.2 Eigenfunctions of Different Eigenvalues Are
</p>
<p>Orthogonal
</p>
<p>Given a Hermitian operator A and the eigenvalue equation
</p>
<p>A fn = an fn; (9.36)
</p>
<p>the spectrum is assumed to be nondegenerate. Then we have
</p>
<p>&int;
</p>
<p>f &lowast;m fndV = 0 for n &#13;= m. (9.37)
</p>
<p>In order to show this, we begin with
</p>
<p>A fn = an fn; (A fm)
&lowast; = am f
</p>
<p>&lowast;
m . (9.38)
</p>
<p>am is real, as we have just shown. We extend the equations
</p>
<p>f &lowast;m A fn = an f
&lowast;
m fn; (A fm)
</p>
<p>&lowast; fn = am f
&lowast;
m fn (9.39)
</p>
<p>and integrate:
</p>
<p>&int;
</p>
<p>f &lowast;m A fndV = an
</p>
<p>&int;
</p>
<p>f &lowast;m fndV ;
</p>
<p>&int;
</p>
<p>(A fm)
&lowast; fndV = am
</p>
<p>&int;
</p>
<p>f &lowast;m fndV . (9.40)
</p>
<p>Since A is Hermitian, the left sides are equal. It follows that:
</p>
<p>(an &minus; am)
</p>
<p>&int;
</p>
<p>f &lowast;m fndV = 0. (9.41)
</p>
<p>Since n &#13;= m (and because there is no degeneracy), we have an &#13;= am . Then we
</p>
<p>conclude:
&int;
</p>
<p>f &lowast;m fndV = 0 for n &#13;= m. (9.42)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Hermitian Operators 119
</p>
<p>We can generalize this equation by including the case m = n. Since we always
</p>
<p>normalize the eigenfunctions, the result reads:
</p>
<p>&int;
</p>
<p>f &lowast;m fndV = nm . (9.43)
</p>
<p>In other words, Hermitian operators always have real eigenvalues and their eigen-
</p>
<p>functions constitute an O N system.
</p>
<p>9.3 Time Behavior, Conserved Quantities
</p>
<p>Examination of the time behavior of expectation values leads to the concept of con-
</p>
<p>served quantities. In addition, we can establish a connection to classical mechanics.
</p>
<p>9.3.1 Time Behavior of Expectation Values
</p>
<p>Since the wavefunction depends upon time, the expectation value of a physical
</p>
<p>quantity
</p>
<p>A =
</p>
<p>&int;
</p>
<p> (r, t)&lowast; A(r, t)dV (9.44)
</p>
<p>will in general also be time dependent.
</p>
<p>We consider the first time derivative of A and express the derivatives of the
</p>
<p>wavefunction using the SEq i = H, while assuming that the potential V in
</p>
<p>H = &minus; 
2
</p>
<p>2m
&nabla;2 + V is real. It follows that:
</p>
<p>i
d
</p>
<p>dt
A = i
</p>
<p>&int;
</p>
<p>&lowast;AdV + i
</p>
<p>&int;
</p>
<p>&lowast; AdV + i
</p>
<p>&int;
</p>
<p>&lowast; AdV
</p>
<p>= &minus;
</p>
<p>&int;
</p>
<p>(H)&lowast; AdV + i
</p>
<p>&int;
</p>
<p>&lowast; AdV +
</p>
<p>&int;
</p>
<p>&lowast; AHdV
</p>
<p>=
H Hermitian
</p>
<p>&minus;
</p>
<p>&int;
</p>
<p>&lowast;H AdV + i
</p>
<p>&int;
</p>
<p>&lowast; AdV +
</p>
<p>&int;
</p>
<p>&lowast; AHdV
</p>
<p>=
</p>
<p>&int;
</p>
<p>&lowast; (AH &minus; H A)dV + i
</p>
<p>&lang;
</p>
<p>&part;
</p>
<p>&part;t
A
</p>
<p>&rang;
</p>
<p>. (9.45)
</p>
<p>Here, we have used the Hermiticity of the Hamiltonian:
</p>
<p>&int;
</p>
<p>&lowast;1 H2dV =
&int;
</p>
<p>(H1)
&lowast; 2dV or
</p>
<p>&int;
</p>
<p>&lowast;H AdV =
&int;
</p>
<p>(H)&lowast; AdV .
(9.46)</p>
<p/>
</div>
<div class="page"><p/>
<p>120 9 Expectation Values, Mean Values, and Measured Values
</p>
<p>(AH &minus; H A) is evidently the commutator of A and H . Then
</p>
<p>i
d
</p>
<p>dt
A =
</p>
<p>&int;
</p>
<p>&lowast; [A, H ]dV + i
</p>
<p>&lang;
</p>
<p>&part;
</p>
<p>&part;t
A
</p>
<p>&rang;
</p>
<p>, (9.47)
</p>
<p>or in compact form,
</p>
<p>i
d
</p>
<p>dt
A = [A, H ] + i
</p>
<p>&lang;
</p>
<p>&part;
</p>
<p>&part;t
A
</p>
<p>&rang;
</p>
<p>. (9.48)
</p>
<p>Practically all of the operators which we consider below do not depend explicitly
</p>
<p>on time.15 In that case, &part;
&part;t
</p>
<p>A is zero, and it thus follows that:
</p>
<p>i
d
</p>
<p>dt
A = [A, H ], if A is not explicitly time dependent. (9.49)
</p>
<p>Although we will deal hereafter only with time-independent Hamiltonians, we
</p>
<p>nevertheless remark that the reasoning leading to (9.48) and (9.49) applies to both
</p>
<p>time-dependent and time-independentHamiltonians. The key feature is theHermitic-
</p>
<p>ity of H .
</p>
<p>9.3.2 Conserved Quantities
</p>
<p>Let us assume that we have an operator A which (i) is not explicitly time-dependent,
</p>
<p>i.e. &part; A
&part;t
</p>
<p>= 0, and which (ii) commutes with H , i.e. [A, H ] = 0. Then it follows from
</p>
<p>(9.49) that:
</p>
<p>i
d
</p>
<p>dt
A = 0 , if
</p>
<p>&part;
</p>
<p>&part;t
A = 0 and [A, H ] = 0. (9.50)
</p>
<p>In other words, the expectation value A (and the associated physical quantity)
</p>
<p>remains constant over time, in which case one speaks of a conserved quantity or a
</p>
<p>constant of the motion.16 As is well known, conserved quantities play a special role
</p>
<p>in physics: They (or the underlying symmetries) allow for a simpler description of a
</p>
<p>system.17 For time-independent operators, the statements &lsquo;A commutes with H &rsquo; and
</p>
<p>&lsquo;A is a conserved quantity&rsquo; are equivalent. Thus, we have an effective instrument at
</p>
<p>our disposal for determining whether or not a given operator represents a conserved
</p>
<p>quantity.
</p>
<p>15Examples are the momentum operator p = 
i
&nabla; or the Hamiltonian, if the potential is time-
</p>
<p>independent.
16Or also of a &lsquo;good quantum number&rsquo;, if required.
17We will take a closer look at this question in Chap.21, Vol. 2, &lsquo;Symmetries&rsquo;.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 Time Behavior, Conserved Quantities 121
</p>
<p>9.3.3 Ehrenfest&rsquo;s Theorem
</p>
<p>The question of whether position and momentum are conserved quantities leads to
</p>
<p>a connection with classical mechanics. It also provides a retrospective confirmation
</p>
<p>(in the sense of a self-consistent approach) of (9.8).
</p>
<p>The physical problem is three-dimensional. We begin with the x component of
</p>
<p>the momentum. With H = H0 + V =
p2
</p>
<p>2m
+ V , we have
</p>
<p>[px , H ] = px H &minus; H px = px H0 + px V &minus; H0 px &minus; V px
</p>
<p>=

</p>
<p>i
</p>
<p>&part;
</p>
<p>&part;x
V &minus; V
</p>
<p>
</p>
<p>i
</p>
<p>&part;
</p>
<p>&part;x
=
</p>
<p>
</p>
<p>i
</p>
<p>&part;V
</p>
<p>&part;x
+
</p>
<p>
</p>
<p>i
V
</p>
<p>&part;
</p>
<p>&part;x
&minus;
</p>
<p>
</p>
<p>i
V
</p>
<p>&part;
</p>
<p>&part;x
=
</p>
<p>
</p>
<p>i
</p>
<p>&part;V
</p>
<p>&part;x
. (9.51)
</p>
<p>For the time behavior of px , it follows that:
</p>
<p>d
</p>
<p>dt
px  = &minus;
</p>
<p>&lang;
</p>
<p>&part;V
</p>
<p>&part;x
</p>
<p>&rang;
</p>
<p>or
d
</p>
<p>dt
p = &minus; &nabla;V  . (9.52)
</p>
<p>Next, we consider the position x . We have:
</p>
<p>[x, H ] = x H &minus; H x = x H0 + xV &minus; H0x &minus; V x
</p>
<p>= x
p2
</p>
<p>2m
&minus;
</p>
<p>p2
</p>
<p>2m
x = x
</p>
<p>p2x
</p>
<p>2m
&minus;
</p>
<p>p2x
</p>
<p>2m
x = &minus;
</p>
<p>
2
</p>
<p>2m
</p>
<p>(
</p>
<p>x
&part;2
</p>
<p>&part;x2
&minus;
</p>
<p>&part;2
</p>
<p>&part;x2
x
</p>
<p>)
</p>
<p>(9.53)
</p>
<p>and it follows that
</p>
<p>[x, H ] =

2
</p>
<p>m
</p>
<p>&part;
</p>
<p>&part;x
or [r, H ] =
</p>
<p>
2
</p>
<p>m
&nabla;. (9.54)
</p>
<p>With p = 
i
&nabla;, this yields for the time behavior:
</p>
<p>d
</p>
<p>dt
r =
</p>
<p>1
</p>
<p>m
p . (9.55)
</p>
<p>Hence, we have recovered our starting point, (9.8), which means that we have
</p>
<p>obtained a confirmation of our ansatz in the sense of a self-consistent description.
</p>
<p>We summarize the results of this section. For the expectation values of position
</p>
<p>and momentum, we have18:
</p>
<p>d
</p>
<p>dt
r =
</p>
<p>1
</p>
<p>m
p and
</p>
<p>d
</p>
<p>dt
p = &minus; &nabla;V  . (9.56)
</p>
<p>The form of the equations is reminiscent of the classical Hamilton equations for a
</p>
<p>particle,
</p>
<p>18So we find m d
2
</p>
<p>dt2
r = &minus; &nabla;V  = F(r). In principle, one must still show that F(r) = F(r)
</p>
<p>(or one defines the force accordingly).</p>
<p/>
</div>
<div class="page"><p/>
<p>122 9 Expectation Values, Mean Values, and Measured Values
</p>
<p>d
</p>
<p>dt
r =
</p>
<p>1
</p>
<p>m
p and
</p>
<p>d
</p>
<p>dt
p = &minus;&nabla;V, (9.57)
</p>
<p>which can be written in this simple case as a Newtonian equation of motion:
</p>
<p>dp
</p>
<p>dt
= m
</p>
<p>d2r
</p>
<p>dt2
= &minus;&nabla;V = F. (9.58)
</p>
<p>In short: The quantum-mechanical expectation values obey the corresponding
</p>
<p>classical equations. This (and therefore also the (9.56)) is called Ehrenfest&rsquo;s theo-
</p>
<p>rem.19
</p>
<p>9.4 Exercises
</p>
<p>1. Given a Hermitian operator A and the eigenvalue problem An = ann , n =
</p>
<p>1, 2, . . ., show that:
</p>
<p>(a) The eigenvalues are real.
</p>
<p>(b) The eigenfunctions are pairwise orthogonal. Here, it is assumed that the
</p>
<p>eigenvalues are nondegenerate.
</p>
<p>2. Show that the expectation value of a Hermitian operator is real.
</p>
<p>3. Show that
&int;
</p>
<p>&lowast;1 A2dV =
</p>
<p>&int;
</p>
<p>(A1)
&lowast; 2dV (9.59)
</p>
<p>holds for the operators r, p, H . Restrict the discussion to the one-dimensional
</p>
<p>case. What conditions must the wavefunctions satisfy?
</p>
<p>4. Show that for the infinite potential well (between 0 and a), x = a
2
.
</p>
<p>5. Given the infinite potential well with walls at x = 0 and x = a; we consider the
</p>
<p>state
</p>
<p> (x, t) =
</p>
<p>&radic;
</p>
<p>2
</p>
<p>a
sin
</p>
<p>(n
</p>
<p>a
x
)
</p>
<p>e&minus;in t . (9.60)
</p>
<p>(a) Determine the position uncertainty x .
</p>
<p>(b) Determine the momentum uncertainty p.
</p>
<p>6. In the infinite potential well, a normalized state is given by
</p>
<p> (x, t) = cnn(x)e
&minus;in t + cmm(x)e
</p>
<p>&minus;im t ; cn, cm &isin; C; n &#13;= m. (9.61)
</p>
<p>Calculate x.
</p>
<p>19We note that also the general law (9.48) for the time dependence of mean values is sometimes
</p>
<p>called Ehrenfest&rsquo;s theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.4 Exercises 123
</p>
<p>7. Consider an infinite square well with potential limits at x = 0 and x = a. The
</p>
<p>initial value of the wavefunction is  (x, 0) =  &isin; R for b &minus;  &le; x &le; b + 
</p>
<p>and  (x, 0) = 0 otherwise (of course, 0 &le; b &minus;  and b +  &le; a). Remember
</p>
<p>that the eigenfunctions n (x) =
&radic;
</p>
<p>2
a
sin kn x with kn =
</p>
<p>n
a
form a CONS.
</p>
<p>(a) Normalize the initial state.
</p>
<p>(b) Calculate  (x, t).
</p>
<p>(c) Find the probability of measuring the system in the state n.
</p>
<p>8. Show that for the expectation value of a physical quantity A,
</p>
<p>i
d
</p>
<p>dt
A = [A, H ] + i
</p>
<p>&lang;
</p>
<p>&part;
</p>
<p>&part;t
A
</p>
<p>&rang;
</p>
<p>(9.62)
</p>
<p>holds. Show that for time-independent operators, the expectation value of the
</p>
<p>corresponding physical quantity is conserved, if A commutes with H .
</p>
<p>9. Show that
</p>
<p>d
</p>
<p>dt
r =
</p>
<p>1
</p>
<p>m
p and
</p>
<p>d
</p>
<p>dt
p = &minus;&nabla;V . (9.63)
</p>
<p>10. Under which conditions is the orbital angular momentum l = r&times;p a conserved
</p>
<p>quantity?
</p>
<p>11. Given the Hamiltonian H with a discrete and nondegenerate spectrum En and
</p>
<p>eigenstates n (r), show that the energy uncertainty H vanishes, iff the quan-
</p>
<p>tum object is in an eigenstate of the energy.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 10
</p>
<p>Stopover; Then on to Quantum
</p>
<p>Cryptography
</p>
<p>We first compare formulations of the analytic and algebraic approaches to quantum mechan-
</p>
<p>ics. In the second section, we see that the properties of the measurement process in quantum
</p>
<p>mechanics permit an encryption method which is in principle absolutely secure.
</p>
<p>10.1 Outline
</p>
<p>This chapter is exceptional insofar as the formalism is not developed further. Rather, it
</p>
<p>serves to collect our previously acquired knowledge, to compare and to check where
</p>
<p>there are open questions of form or content. In the second part of the chapter, we
</p>
<p>take up quantum cryptography. We will see that even allegedly abstract or theoretical
</p>
<p>peculiarities of quantum mechanics, such as those of the measurement process, can
</p>
<p>have immediate practical applications.
</p>
<p>10.2 Summary and Open Questions
</p>
<p>First, we collect the essential concepts and structures of quantum mechanics which
</p>
<p>we have developed up to now in the preceding chapters. Comparison of the analytical
</p>
<p>and algebraic approaches (ana and ala) shows, on the one hand, that there are many
</p>
<p>significant parallels, but also that a few components are missing in each case. To keep
</p>
<p>the text readable, we dispense with detailed remarks about which chapter introduced
</p>
<p>or treated the particular subject matter.
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4_10
</p>
<p>125</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_10&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_10&amp;domain=pdf</a></div>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4_10">https://doi.org/10.1007/978-3-030-00464-4_10</a></div>
</div>
<div class="page"><p/>
<p>126 10 Stopover; Then on to Quantum Cryptography
</p>
<p>10.2.1 Summary
</p>
<p>10.2.1.1 States
</p>
<p>We started with states, which we wrote in the analytical approach in terms of a
</p>
<p>position- and time-dependent wavefunction  (r, t), and in the algebraic approach
</p>
<p>as a ket | (t) or its representation as a column vector. It should be pointed out again
that | (t) does not depend on position, but only on time. The states are in both
cases elements of vector spaces.
</p>
<p>10.2.1.2 Time-Dependent SEq, Hamiltonian
</p>
<p>The time behavior of both approaches is described by the time-dependent SEq. It
</p>
<p>reads
</p>
<p>i
&part;
</p>
<p>&part;t
 (r, t) = H (r, t)
</p>
<p>i
d
</p>
<p>dt
| (t) = H | (t) .
</p>
<p>(10.1)
</p>
<p>In the analytical approach, H is the Hamiltonian &minus; 2
2m
</p>
<p>&nabla;2 +V , while in the algebraic
approach it is an abstract operator, about which we know almost nothing so far&mdash;
</p>
<p>except that it can be represented as a matrix.1 In any case, H is a Hermitian operator,
</p>
<p>where this property is defined in the ana by integrals, in the ala by scalar products:
</p>
<p>&int;
</p>
<p>&lowast;HdV =
&int;
</p>
<p>(H)&lowast; dV
</p>
<p>| H | = | H &dagger; | .
(10.2)
</p>
<p>10.2.1.3 Mean Value and Expectation Value
</p>
<p>If the system is in the state  (r, t), we can obtain the expectation value A of an
operator A (that is, the expectation value of the corresponding physical quantity) as
</p>
<p>A =
&int;
</p>
<p>&lowast;HdV . (10.3)
</p>
<p>A corresponding formulation in the algebraic approach is still pending.
</p>
<p>We note that the expectation value of a time-independent operator A in general
</p>
<p>depends on time because of  =  (r, t). It is a conserved quantity (i.e. is independent
of time) if the operator A commutes with H , i.e. [A, H ] = 0.
</p>
<p>1A comment on the notation: although the Hamiltonians of the two approaches in (10.1) are com-
</p>
<p>pletely different mathematical objects, it is customary to denote them with the same symbol H . The
</p>
<p>same holds for the eigenfunctions and vectors.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 Summary and Open Questions 127
</p>
<p>10.2.1.4 Time-Independent SEq, Eigenvalues and Eigenvectors
</p>
<p>The time evolution of the states can be developed by means of the eigenvalues and
</p>
<p>eigenvectors of H . We assume discrete, nondegenerate spectra with eigenvalues En .
</p>
<p>We denote the analytical eigenvectors (eigenfunctions) by n (r), the algebraic ones
</p>
<p>by |n. They are the solutions of the eigenvalue problems (stationary SEq):
</p>
<p>Hn (r) = Enn (r)
H |n = En |n .
</p>
<p>(10.4)
</p>
<p>We note that the range of values of n can be finite or infinite.
</p>
<p>Since H is a Hermitian operator in both of these approaches, its eigenfunctions
</p>
<p>{n (r)} or {|n} form an orthonormal system (ONS):
&int;
</p>
<p>&lowast;m (r) n (r) dV = nm
m |n = nm .
</p>
<p>(10.5)
</p>
<p>In the ala, we described the completeness of an ONS by
&sum;
</p>
<p>n
|n n| = 1. An
</p>
<p>analogous formulation in the ana is still pending.
</p>
<p>10.2.1.5 Time-Dependent Solution
</p>
<p>Using eigenvalues and eigenvectors, the time-dependent solution of the SEq can be
</p>
<p>written as
</p>
<p> (r, t) =
&sum;
</p>
<p>n
</p>
<p>cnn (r) e
&minus;i En t/
</p>
<p>| (t) =
&sum;
</p>
<p>n
</p>
<p>cn |n e&minus;i En t/.
(10.6)
</p>
<p>Being solutions of the deterministic SEq (10.1), these states are defined uniquely and
</p>
<p>for all times by specifying an initial condition  (r, 0) or | (0). We can see this by
using (10.5) to obtain
</p>
<p>&int;
</p>
<p>&lowast;m (r)  (r, 0) dV =
&sum;
</p>
<p>n
</p>
<p>cn
&int;
</p>
<p>&lowast;m (r) n (r) dV =
&sum;
</p>
<p>n
</p>
<p>nmcn = cm
</p>
<p>m | (0) =
&sum;
</p>
<p>n
</p>
<p>cn m |n =
&sum;
</p>
<p>n
</p>
<p>nmcn = cm .
(10.7)
</p>
<p>or, more compactly,
</p>
<p>cn =
&int;
</p>
<p>&lowast;n (r)  (r, 0) dV
</p>
<p>cn = n | (0) .
(10.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>128 10 Stopover; Then on to Quantum Cryptography
</p>
<p>Up to this point, the formalisms developed in the ana and ala are very simi-
</p>
<p>lar, in spite of some differences (definition of a Hermitian operator, state, SEq
</p>
<p>as a differential equation or as a matrix equation). We conclude that there obvi-
</p>
<p>ously must be a close connection. For example, the (10.8) suggest that the integral
&int;
</p>
<p>&lowast;n (r)  (r, 0) dV of the ana corresponds to a scalar product in the ala. We take up
this issue again in the next chapter.
</p>
<p>10.2.1.6 Measurement, Probability
</p>
<p>The formalism of quantum mechanics just outlined is strictly deterministic. A random
</p>
<p>element occurs only if we want to obtain information about the system by means of
</p>
<p>a measurement. We have seen in previous chapters that the coefficients of the form
</p>
<p>|cn|2 which appear in (10.6) give the probabilities of finding the system in the state
n (r) or |n. With quantized values (such as the energy or the state of a neutrino,
i.e. muon neutrino or electron neutrino, etc.), one can always measure only one of
</p>
<p>the values of the spectrum. Other results are not possible.
</p>
<p>In the ala, we formulated the measurement process by using projection operators.
</p>
<p>If we want to measure e.g. the state |, we model this by applying the projection
operator P = | | to the state |:
</p>
<p>|  | = c | . (10.9)
</p>
<p>Here, the term |c|2 = | ||2 denotes the probability of in fact obtaining the state
| by a measurement on |. In the ana, we have not yet introduced projection
operators. The parallelism of the descriptions in the ana and ala suggests, however,
</p>
<p>that there must be an equivalent in the ana.
</p>
<p>10.2.1.7 Measurement, Collapse
</p>
<p>Through the measurement, the system is transferred from the state | into |
(provided c =  | = 0). In the formulation of the ala, this can be written as
</p>
<p>| &rarr;
with probability |c|2
</p>
<p>P |

</p>
<p>P |

</p>
<p>
</p>
<p>=  || || |. (10.10)
</p>
<p>On measurement, a superposition of states generally breaks down2 and the result
</p>
<p>is one single state. We have described this behavior in terms of state reduction or
</p>
<p>collapse of the state. After the measurement, we again have a normalized state, where
</p>
<p>a possibly remaining global phase is irrelevant,3 since states are physically the same
</p>
<p>if they differ only by a phase (we will discuss this point later, in Chap. 14). The state
</p>
<p>2In other words, if the initial and final states are not the same.
3Quantum mechanics is very well-behaved in this sense.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 Summary and Open Questions 129
</p>
<p>after the measurement can be interpreted as a new initial state4 (at time T , we start
</p>
<p>our clock again), which evolves in a unitary manner until the next measurement.
</p>
<p>We remark again that the actual measurement process itself is not modelled, but
</p>
<p>rather only the situation before and after the measurement.
</p>
<p>10.2.2 Open Questions
</p>
<p>The descriptions in the ana and ala outlined above leave open some questions which
</p>
<p>we now summarize briefly. These questions are in part of a more formal nature,
</p>
<p>and in part concern content (although this division is not necessarily clearcut). The
</p>
<p>answers to the open questions will be provided in the following chapters.
</p>
<p>10.2.2.1 Formal Questions
</p>
<p>As mentioned above, the great similarity of the expressions (10.1) and (10.8) suggests
</p>
<p>that there is a direct connection between the two approaches and the corresponding
</p>
<p>formulations. So it must be clarified, for example, which relationship exists between
</p>
<p>the description of states as kets and as wavefunctions. As a result, we will find among
</p>
<p>other things a representation of the projection operator in the ana, thus far defined
</p>
<p>only in the ala. In addition, this connection must explain the different formulations, as
</p>
<p>in (10.8); this is also true for the definitions of the Hamiltonians in the two approaches
</p>
<p>(as &minus; 2
2m
</p>
<p>&nabla;2 + V and as a matrix) which at first glance seem quite distinct.
Another topic still to be treated is that of degenerate as well as continuous spectra.
</p>
<p>This will be done in Chap. 12.
</p>
<p>10.2.2.2 Questions of Content
</p>
<p>A measurement, as described in (10.10), is generally (i.e. for |c|2 = 1) not reversible,
so it is not a unitary process. Assuming the validity of the projection principle for
</p>
<p>determining the measurement probabilities, we must explain how this state reduction
</p>
<p>comes about, i.e. the transition from a superposition such as |(t) to a single state
such as |e. Meanwhile, it is accepted that this collapse of the state is a non-local,
i.e. superluminal effect.5 Some of following considerations answer part of the open
</p>
<p>questions, but another part is still poorly understood and still under discussion. We
</p>
<p>will come back to these topics in several chapters in volume 2.
</p>
<p>4In this case one speaks of &lsquo;state preparation&rsquo;.
5This makes it perhaps understandable that Einstein dismissed it as &lsquo;spooky action at a distance&rsquo;. It
</p>
<p>can be shown that the effect is not suitable for the superluminal transmission of information&mdash;the
</p>
<p>validity of the theory of relativity thus remains unquestioned.</p>
<p/>
</div>
<div class="page"><p/>
<p>130 10 Stopover; Then on to Quantum Cryptography
</p>
<p>To avoid misunderstandings: Here, we have a problem at the level of the
</p>
<p>interpretation of quantum mechanics; that is, of its comprehension. On a formal
</p>
<p>level&mdash;technically, so to speak&mdash;quantum mechanics works extremely well; it is
</p>
<p>simply fapp (after a proverbial expression due to John S. Bell: &lsquo;Ordinary quantum
</p>
<p>mechanics is just f ine for all practical purposes&rsquo;).6
</p>
<p>We will resume the discussion of the issues of content in Chap. 14. In the rest
</p>
<p>of this chapter, we will examine a practical application of quantum mechanics&mdash;to
</p>
<p>some extent a case of fapp.
</p>
<p>10.3 Quantum Cryptography
</p>
<p>There are some popular misconceptions about quantum mechanics. The &lsquo;quantum
</p>
<p>jump&rsquo; is symptomatic&mdash;what in quantum mechanics means the &lsquo;smallest possible
</p>
<p>change&rsquo; has become in everyday language a metaphor for a giant leap, a radical
</p>
<p>change.7
</p>
<p>Two other misconceptions are that quantum mechanics always requires an
</p>
<p>enormous mathematical apparatus,8 and that the abstract peculiarities of quantum
</p>
<p>mechanics such as the measurement problem are at most of theoretical interest. That
</p>
<p>both assertions are wrong is shown by quantum cryptography.9 In fact, it is based on
</p>
<p>a peculiarity of the quantum-mechanical measurement process and can, in its sim-
</p>
<p>plest formulation, be described without any formula at all,10 as we will see shortly.
</p>
<p>Of course, one can describe the whole situation more formally, but here we have one
</p>
<p>of the admittedly very few examples where this is not necessarily required.
</p>
<p>The procedure is based on the quantum-mechanical principles that (i) there are
</p>
<p>superpositions of several states, and that (ii) before a measurement of such a super-
</p>
<p>position, we can specify only the probability of obtaining one of these states as a
</p>
<p>result. These principles are what make quantum cryptography possible, not only in
</p>
<p>theory, but also as a practical method.
</p>
<p>6In an extension of Bell&rsquo;s one-liner, those theories which, on the one hand, one cannot really (or
</p>
<p>does not want to) justify, but which, on the other hand, agree well with experimental results and are
</p>
<p>very useful for all practical purposes, are called fapp theories. Quantum mechanics may be such a
</p>
<p>theory, if one regards it only as a tool (or judges it primarily by its usefulness) and is not willing (or
</p>
<p>able) to reflect upon its meaning.
7However, the movie title &lsquo;Quantum of Solace&rsquo; promises not a &lsquo;quantum jump&rsquo;, but rather a mini-
</p>
<p>mum in terms of comfort for James Bond&mdash;quantum solace, so to speak.
8We have already seen that this is not always true, e.g. in the algebraic approach, where the basic
</p>
<p>ideas can be formulated using simple vector algebra.
9This term is short and to the point, but also a bit misleading. As we shall see shortly, quantum
</p>
<p>mechanics does not help to encrypt a message, but rather ensures that the key cannot be discovered
</p>
<p>by a spy.
10For this reason, the topic is also very well suited for discussion at the school level.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Quantum Cryptography 131
</p>
<p>10.3.1 Introduction
</p>
<p>Cipher texts and encryptions were already common in pre-Christian cultures. One of
</p>
<p>the most famous old encryption methods is attributed to Caesar, and is still called the
</p>
<p>Caesar cipher. Here, the text is encoded by replacing each letter with for example the
</p>
<p>third letter which follows it in the alphabet. Thus, &lsquo;cold&rsquo; becomes &lsquo;frog&rsquo; and &lsquo;bade&rsquo;
</p>
<p>becomes &lsquo;edgh&rsquo;.
</p>
<p>Of course, nowadays it is a no-brainer to crack this ciphering method&mdash;it suffices
</p>
<p>that one knows very precisely for each language the frequency of occurrence of
</p>
<p>each letter. Modern cryptography has developed much more elaborate processes.
</p>
<p>It enjoyed an enormous boom in both world wars, where it also provided a strong
</p>
<p>impetus to the development of electronic computers. One of the first, called Colossus,
</p>
<p>was built at the end of the Second World War and was used for decoding purposes.
</p>
<p>A word on nomenclature: One encodes, encrypts or ciphers an unencrypted or
</p>
<p>plain text by means of a cipher or key. The result is a encrypted text or cipher text.
</p>
<p>If it is decoded, decrypted or deciphered, one again recovers the plain text.
</p>
<p>10.3.2 One-Time Pad
</p>
<p>This encryption method was developed in 1917. Gilbert Vernam is usually named as
</p>
<p>its author. In 1949, Claude Shannon proved the absolute security of the method. In
</p>
<p>this process, it is known how to encrypt and decrypt. Its security is based exclusively
</p>
<p>on the fact that the key is secret (and only if this is guaranteed is the process absolutely
</p>
<p>secure).
</p>
<p>The method works as follows: First, the alphabet (and some major punctuation
</p>
<p>marks, etc.) is converted into numbers. As an example, we might have:
</p>
<p>A B C D E ... X Y Z , . ?
</p>
<p>00 01 02 03 04 ... 23 24 25 26 27 28 29
</p>
<p>as our pool of 30 characters. If the message consists of N characters, then the key must
</p>
<p>also consist of N characters. They are pulled out of the pool at random. Compared to
</p>
<p>&lsquo;normal&rsquo; text, this has the advantage that each character occurs on average with equal
</p>
<p>frequency. Thus, even if pieces of the key are known, it cannot be reconstructed.
</p>
<p>As a concrete example, we choose the key 06/29/01/27/ &hellip;. Encrypting the
</p>
<p>message &lsquo;BADE&rsquo; leads to:</p>
<p/>
</div>
<div class="page"><p/>
<p>132 10 Stopover; Then on to Quantum Cryptography
</p>
<p>B A D E Message, plain text T
</p>
<p>01 00 03 04 Plain text T, numbers
</p>
<p>06 29 01 27 Key S
</p>
<p>07 29 04 01 V = (T + S) (mod 30), numbers
H ? E B Cipher text V
</p>
<p>and decrypting leads to
</p>
<p>H ? E B Cipher text V
</p>
<p>07 29 04 01 Ciphertext V, numbers
</p>
<p>06 29 01 27 Key S
</p>
<p>01 00 03 04 T = (V &minus; S) (mod 30), numbers
B A D E Message, plain text T
</p>
<p>Some remarks on practical procedures:
</p>
<p>&bull; The cipher text V is transmitted publicly. The security depends entirely on the fact
that the key is known only to the sender and the recipient.
</p>
<p>&bull; The procedure is absolutely safe if each key is used only once. Hence the name
&lsquo;pad&rsquo;&mdash;one can imagine the sender and receiver each having an identical (writing)
</p>
<p>pad, and there is just one key on each sheet. After encrypting and decrypting the
</p>
<p>key is obsolete; the top sheet of the pad is stripped off and thrown away. The next
</p>
<p>page on the pad contains the next key.
</p>
<p>&bull; In binary notation, the method is basically the same, but more adapted to computers.
This could be as follows (1 + 1 = 0):
</p>
<p>Text T 0 1 1 0 1 0 0 1 0 1 1 1
</p>
<p>Key S 1 0 1 0 0 1 0 0 0 1 1 0
</p>
<p>T + S 1 1 0 0 1 1 0 1 0 0 0 1
S 1 0 1 0 0 1 0 0 0 1 1 0
</p>
<p>&rArr; T = T + S &plusmn; S 0 1 1 0 1 0 0 1 0 1 1 1
</p>
<p>&bull; Especially in the English literature, certain names have become firmly
entrenched. The sender is called &lsquo;Alice&rsquo;, the recipient &lsquo;Bob&rsquo;. We will consider
</p>
<p>in addition a third person, namely a spy. For the name of the spy, one could think
</p>
<p>that it should now begin with &lsquo;C&rsquo;, e.g. &lsquo;Charlotte&rsquo; (and in French texts, one does
</p>
<p>indeed find this name); but the English word &lsquo;eavesdropping&rsquo; suggests immedi-
</p>
<p>ately the name &lsquo;Eve&rsquo;, and that is how the spy is usually named&mdash;not alphabetically,
</p>
<p>but gender-specifically correct.
</p>
<p>&bull; The one-time pad method is thus based on a public exchange of the encrypted
message, while the key is transmitted secretly. The problem of safe and secret
</p>
<p>transfer of keys between Alice and Bob is called key distribution. The great diffi-
</p>
<p>culty here is: How can we be sure that Eve has not read the key in secrecy, without
</p>
<p>leaving traces on the paper or on the CD, or has photographed it? There is a kind of</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Quantum Cryptography 133
</p>
<p>mnemonic in cryptography which describes this classical dilemma ironically: &lsquo;You
</p>
<p>can communicate completely secretly, provided you can communicate completely
</p>
<p>secretly.&rsquo;
</p>
<p>Here, quantum mechanics enters the scene, and brings with it several methods. All
</p>
<p>have in common that they associate the key distribution to quantum-mechanical char-
</p>
<p>acteristics and thus secure it. This is called quantum key distribution. A particularly
</p>
<p>simple method is the so-called BB84 protocol (Bennett and Brassard, 1984).11 It
</p>
<p>is essentially based on the idea of a Ph.D. student in the sixties. Stephen Wies-
</p>
<p>ner at that time devised a method of making counterfeit-proof banknotes using
</p>
<p>polarized photons (so to speak &lsquo;quantum money&rsquo;). Although the practical imple-
</p>
<p>mentation of this idea is not readily possible even today, in retrospect it is not
</p>
<p>really understandable why Wiesner&rsquo;s attempts to publish this idea around 1970 were
</p>
<p>rejected rigorously by the journal reviewers. Wiesner had to wait more than ten years
</p>
<p>before he could describe his proposal in the literature.12 In any case&mdash;at least Charles
</p>
<p>Bennett, a friend of Wiesner&rsquo;s, recognized the cryptographic potential of his idea and
</p>
<p>developed, together with Brassard, the BB84 protocol.
</p>
<p>10.3.3 BB84 Protocol Without Eve
</p>
<p>In the following, the information is transmitted by polarized photons, where we
</p>
<p>will consider only linear polarization. As usual, we denote the horizontally- and
</p>
<p>vertically-polarized states by |h and |v.
As we said above, the secure and confidential transmission of the key is all-
</p>
<p>important. Alice could now send a key by forwarding to Bob a random sequence of
</p>
<p>|h and |v. However, she must tell Bob the orientation of the polarizer (e.g. by phone),
and when Eve overhears this communication, she could listen safely without Alice
</p>
<p>or Bob being aware of her. To increase security, we must use quantum mechanics;
</p>
<p>more precisely, projection and the superposition principle.
</p>
<p>And this is how it works: Alice chooses randomly one of two polarization direc-
</p>
<p>tions: horizontal/vertical or diagonally left/right, symbolized by  and , where the
</p>
<p>crosses in the squares mark the polarization planes.13 We can represent the states
</p>
<p>as |h and |v plus |\ and |/ for the &lsquo;diagonal&rsquo; measurements. The superposition
principle is expressed by the fact that the &lsquo;diagonal&rsquo; states are linear combinations
</p>
<p>of the &lsquo;linear&rsquo; ones, [|h &plusmn; |v] /
&radic;
</p>
<p>2. So if one measures with a &lsquo;linear&rsquo; polarizer a
</p>
<p>&lsquo;diagonal&rsquo; state, one obtains |h and |v, each with probability
(
</p>
<p>1/
&radic;
</p>
<p>2
)2
</p>
<p>= 1/2.
To keep the notation transparent, we assign values to the states:
</p>
<p>11Another method, called the E91 protocol (the &lsquo;E&rsquo; designates Artur Ekert), works with entangled
</p>
<p>photons (for this concept see Chap. 20, Vol. 2).
12Unfortunately, one must not be too far ahead of one&rsquo;s time. Depicting blue horses in the 15th
</p>
<p>century probably caused (at most) some head-shaking. That applies also in science.
13The  plane is of course the  plane, rotated by 45. Moreover, the  states are the eigenvectors
of z , and the  states, up to a sign, are the eigenvectors of x ; cf. Chap. 4.</p>
<p/>
</div>
<div class="page"><p/>
<p>134 10 Stopover; Then on to Quantum Cryptography
</p>
<p>1 = |h1 = |\
0 = |v0 = |/
</p>
<p>The exact choice of this mapping plays no role,14 but it must be agreed upon
</p>
<p>between Alice and Bob. Similarly, the orientation of the polarizers (= basis) is
publicly known.
</p>
<p>The BB84 protocol operates as follows:
</p>
<p>1. Alice and Bob fix the start and the end of the key transmission and the timing
</p>
<p>with which the photons are sent, for example one photon every tenth of a second.
</p>
<p>2. Alice dices (i.e. generates at random) a basis and a value, i.e.  or  and 1 or 0.
</p>
<p>The bit thus described15 is sent to Bob as a polarized photon.
</p>
<p>3. Of course, Bob does not know the basis and the value which Alice has sent. He
</p>
<p>dices a basis and measures the photon in this basis. He may or may not choose
</p>
<p>(by chance, with probability 1/2) the same basis as Alice. In the first case, he
</p>
<p>always measures the same value as Alice&mdash;this is crucial for the functioning of
</p>
<p>the method. If the bases do not match, there is only a probability of 1/2 that Bob
</p>
<p>measures the correct value. Up to this point the whole thing looks, for example,
</p>
<p>like this:
</p>
<p>A basis          
</p>
<p>A value 1 0 0 1 1 0 0 1 0 1
</p>
<p>B basis          
</p>
<p>B possible measurements
1
</p>
<p>0
0
</p>
<p>1
</p>
<p>0
1 1 0 0
</p>
<p>1
</p>
<p>0
0
</p>
<p>1
</p>
<p>0
</p>
<p>B actual measurement 1 0 1 1 1 0 0 0 0 1
</p>
<p>For the first photon, Bob did not choose the basis used by Alice. His measurement
</p>
<p>can then be 1 or 0; we have inserted 1 as a concrete example.16 By the way, the
</p>
<p>results obtained with different bases used by A and B do not matter for the key
</p>
<p>transmission, as we shall see in a moment.
</p>
<p>4. In this way, the necessary number of photons is transmitted, while Alice and Bob
</p>
<p>record their bases and values. The transfer process is then completed. The next
</p>
<p>step is a public exchange: Bob tells Alice which basis he used for each photon,
</p>
<p>and Alice tells Bob whether it was the right one. It is important that the value
</p>
<p>(i.e. 0 or 1) is not made public. After that, Alice and Bob remove all values
</p>
<p>for which the polarization orientations do not match. This also applies to all
</p>
<p>14For example, the mapping 0 = |h and 1 = |v would be just as good.
15By a bit, one denotes a quantity that can take on only two values, here 0 and 1.
16We remark that Bob, in his measurements with a &lsquo;wrong&rsquo; basis, may of course also obtain other
</p>
<p>values, and these with equal probability. The last row in the table above is a concrete example of
</p>
<p>a total of 16. Other possibilities for Bob&rsquo;s actual measurements are e.g. 0 0 0 1 1 0 0 1 0 0 or
</p>
<p>1 0 0 1 1 0 0 0 0 0 .</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Quantum Cryptography 135
</p>
<p>measurements or times at which Alice did not send a photon or Bob did not detect
</p>
<p>one although one was underway (dark counts). Since Alice and Bob always get
</p>
<p>the same values for the same basis, the remaining values make up the key. In this
</p>
<p>eavesdropper-free scenario, it is known to no-one other than Alice and Bob. In
</p>
<p>our example, the key is
</p>
<p>Key &minus; 0 &minus; 1 1 0 0 &minus; 0 &minus; &rarr; 011000.
</p>
<p>However, the world is not so simple, and eavesdroppers and spies are everywhere.
</p>
<p>How do we deal with this problem?
</p>
<p>10.3.4 BB84 Protocol with Eve
</p>
<p>The situation is as follows: Alice sends one photon per time interval, and Eve inter-
</p>
<p>cepts each photon or a certain portion of them (of course without Alice and Bob
</p>
<p>being able to perceive this by ordinary means of observation), using e.g. a PBS, and
</p>
<p>transmits them on to Bob. This may seem simple, but actually it is not so easy for
</p>
<p>Eve to carry out this interception. One of the possible applications is, for example,
</p>
<p>to send keys from the earth (summit stations) to satellites. If one is really dealing
</p>
<p>with single-photon processes, it is impossible for Eve to intercept individual photons
</p>
<p>in transit and remain unnoticed, without in this case the whole world being able to
</p>
<p>look at her. For other types of transmission (via fiber-optic cable, etc.), espionage
</p>
<p>techniques are possible, but certainly not easy to implement.
</p>
<p>But we assume in the following (for the purpose of a conservative estimate) that
</p>
<p>Eve can overcome this problem. However, quantum mechanics ensures that she still
</p>
<p>cannot listen without being recognized.
</p>
<p>The argument runs like this: Since Eve never knows which basis Alice has set,
</p>
<p>she must choose her basis, just like Bob, randomly with a hit rate of 50%. When
</p>
<p>using the wrong basis, Eve will not measure the value chosen by Alice in 50% of the
</p>
<p>cases. Bob in turn measures, if he has chosen at random the same basis as Eve, the
</p>
<p>same value as she does; or otherwise, with probability 1/2, the value 0 or the value 1.
</p>
<p>This could for example look like this:
</p>
<p>A basis          
</p>
<p>A value 1 0 0 1 1 0 0 1 0 1
</p>
<p>E basis          
</p>
<p>E possible measurements
1
</p>
<p>0
</p>
<p>1
</p>
<p>0
</p>
<p>1
</p>
<p>0
1 1
</p>
<p>1
</p>
<p>0
</p>
<p>1
</p>
<p>0
1 0
</p>
<p>1
</p>
<p>0
</p>
<p>E actual measurement 1 0 1 1 1 0 1 1 0 0
</p>
<p>B basis          
</p>
<p>B possible measurements 1
1
</p>
<p>0
1 1 1
</p>
<p>1
</p>
<p>0
</p>
<p>1
</p>
<p>0
</p>
<p>1
</p>
<p>0
0 0
</p>
<p>B actual measurement 1 0 1 1 1 1 1 0 0 0</p>
<p/>
</div>
<div class="page"><p/>
<p>136 10 Stopover; Then on to Quantum Cryptography
</p>
<p>Alice and Bob again compare their bases for each photon and keep only the values
</p>
<p>for which the bases coincide. For example:
</p>
<p>Alice &minus; 0 &minus; 1 1 0 0 &minus; 0 &minus;
Bob &minus; 0 &minus; 1 1 1 1 &minus; 0 &minus;
</p>
<p>And here we see the great advantage of quantum cryptography. The difference in
</p>
<p>the keys of Alice and Bob makes it in principle detectable that Eve was spying!
</p>
<p>Quantum-mechanical methods of key distribution make it virtually impossible for
</p>
<p>Eve to remain unnoticed. In order to detect the spy, Alice and Bob have to compare
</p>
<p>publicly parts of their keys, and cannot use the whole key directly. But since one
</p>
<p>can transmit very large amounts of information very simply with photons, it is not
</p>
<p>a particular disadvantage for Alice and Bob to discard parts of their keys upon
</p>
<p>consultation. The essential details of the procedure can be found in Appendix P,
</p>
<p>Vol. 1.
</p>
<p>We want to pursue the question of which level of certainty can be achieved for
</p>
<p>Eve&rsquo;s unmasking. To quantify the issue, we assume that Alice and Bob have cho-
</p>
<p>sen the same basis (the other photons are eliminated anyway). Eve can then chose
</p>
<p>randomly (and with probability 1/2) the same basis, in which case Alice&rsquo;s value is
</p>
<p>passed on, or the other basis, in which case there are four different possibilities. In
</p>
<p>detail, they are as follows:
</p>
<p>Alice&rsquo;s basis     
</p>
<p>Alice&rsquo;s value 1 1 1 1 1
</p>
<p>Eve&rsquo;s basis     
</p>
<p>Eve&rsquo;s value 1 1 1 0 0
</p>
<p>Bob&rsquo;s basis     
</p>
<p>Bob&rsquo;s value 1 1 0 1 0
</p>
<p>probability 1/2 1/8 1/8 1/8 1/8
</p>
<p>After the elimination of the results of different bases used by Alice and Bob, we have
</p>
<p>the following situation: (a) Eve has 75 agreement with the values of Alice, and (b)
</p>
<p>in a quarter of the cases, a different value results at the corresponding position of
</p>
<p>the keys of Alice and Bob. Thus, there is a chance of 1 &minus; 1/4 per photon that Eve
remains undetected. If Eve has spied on a total of N photons of the key, the chance
</p>
<p>of discovering her is given by pdiscover =
(
</p>
<p>1 &minus; [1 &minus; 1/4]N
)
</p>
<p>. For a very short key or
</p>
<p>very few measurements, Eve may be lucky and stay undetected (e.g. for the first five
</p>
<p>photons in the above example), but uncovering her is practically certain with even a
</p>
<p>moderately long key. Here are some numerical values:
</p>
<p>N 10 102 103 104
</p>
<p>1 &minus; pdiscover 10&minus;1.25 = 0.056 10&minus;12.5 10&minus;125 10&minus;1249</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Quantum Cryptography 137
</p>
<p>Compared to this, the chance to win the lottery (6 out of 49) is relatively high; as
</p>
<p>is well known, it is 1/
</p>
<p>(
</p>
<p>49
</p>
<p>6
</p>
<p>)
</p>
<p>= 1/13983816 &asymp; 10&minus;7.1. Even for only a moderately
large N of the order of 100 or 1000, it is virtually impossible that Eve can listen in
</p>
<p>without being recognized.
</p>
<p>If Eve spies on each photon, this manifests itself in an average error rate of 25%
</p>
<p>when the keys of Alice and Bob are compared. If she spies on every second photon,
</p>
<p>it is 12.5%, etc. So, when Alice and Bob compare their keys, they see not only
</p>
<p>whether Eve has been spying, but also can estimate how many photons she has
</p>
<p>eavesdropped on. However, errors can also arise due to noise and other processes
</p>
<p>which, for example, unintentionally change the polarization. By comparing, Alice
</p>
<p>and Bob can determine which part of the key Eve knows at most. If the error rate is
</p>
<p>too high, say well over 10%, the key is discarded and a new key is transmitted.
</p>
<p>Now one could imagine that Eve calmly replicates the photons sent by Alice,
</p>
<p>transmits the original to Bob and performs appropriate measurements on the copies.
</p>
<p>But this does not work, as is guaranteed by another peculiarity of quantum mechan-
</p>
<p>ics: Namely, the no-cloning theorem of quantum mechanics states that one cannot
</p>
<p>copy an arbitrary state, but only a state that is already known, as well as the states
</p>
<p>orthogonal to it. We will discuss this point in Chap. 26, Vol. 2 (quantum informa-
</p>
<p>tion). In the context of our current considerations, the theorem applies, since the two
</p>
<p>non-mutually-orthogonal basis systems  and  are used.
</p>
<p>Up to this point we have considered the contributions of quantum mechanics.
</p>
<p>What follows are classical, not quantum-mechanical methods; they are outlined in
</p>
<p>Appendix P, Vol. 1.
</p>
<p>A final remark: We have assumed idealized conditions&mdash;all detection devices
</p>
<p>work with one hundred percent efficiency, there is no noise (behind which Eve could
</p>
<p>try to hide), and so on. So the question is whether the method is also suitable for
</p>
<p>actual, practical use. One can investigate this issue theoretically, and finds a positive
</p>
<p>answer. But here it is perhaps more interesting to note that the method indeed works
</p>
<p>well in practice. In fact, a number of quantum cryptographic experiments have been
</p>
<p>performed to date. Among others, the world&rsquo;s first quantum-encrypted money transfer
</p>
<p>was carried out on April 21st, 2004 in Vienna. The photons were guided through a
</p>
<p>1500 m long fiber-optic cable that connected the city hall with a bank. Furthermore,
</p>
<p>there was an experiment in 2002 using a telescopic connection, i.e. without expensive
</p>
<p>fiber-optic cables. Here, the photon travelled through the clear mountain air from the
</p>
<p>summit station of the Karwendelbahn a distance of 23.4 km to the Max Planck hut on
</p>
<p>the Zugspitze.17 But even in the polluted air of an urban area (Munich), the procedure
</p>
<p>has been successfully tested18; the photons travelled a free distance of 500 m. The
</p>
<p>transfer rate was around 60 kbit/s; the system was operated continuously and stably
</p>
<p>17C. Kurtsiefer et al., &lsquo;A step towards global key distribution&rsquo;, Nature 419 (2002), p. 450.
18See the webpage &lsquo;Experimental Quantum Physics&rsquo;, http://xqp.physik.uni-muenchen.de/.</p>
<p/>
<div class="annotation"><a href="http://xqp.physik.uni-muenchen.de/">http://xqp.physik.uni-muenchen.de/</a></div>
</div>
<div class="page"><p/>
<p>138 10 Stopover; Then on to Quantum Cryptography
</p>
<p>for 13 h. A much longer transmission distance was attained in 2007, when a quantum
</p>
<p>key was transferred over 144 km, namely between the Canary Islands of La Palma
</p>
<p>and Tenerife.19 In principle, it therefore appears possible to use satellites for such
</p>
<p>secure and encrypted signaling, e.g. for transatlantic connections.20
</p>
<p>19R. Ursin et al., &lsquo;Entanglement-based quantum communication over 144 km&rsquo;, Nature Physics 3
</p>
<p>(2007), p. 481.
20See e.g. S. Liao et al., Satellite-to-ground quantum key distribution, Nature 549, 43&ndash;47, https://
</p>
<p>doi.org/10.1038/nature23655 (Sep 2017), where a quantum key distribution over a distance of up
</p>
<p>to 1,200 km is reported. Quantum keys may also be distributed in optical fibers over remarkable
</p>
<p>distances of up to 100 km; see e.g. K.A. Patel et al., Coexistence of high-bit-rate quantum key
</p>
<p>distribution and data on optical fiber, Phys. Rev. X 2, 041010 (2012)), or Paul Jouguet et al.,
</p>
<p>Experimental demonstration of long-distance continuous-variable quantum key distribution, Nature
</p>
<p>Photonics (2013), https://doi.org/10.1038/nphoton.2013.63. In addition, the feasibility of BB84
</p>
<p>quantum key distribution between an aircraft moving at 290 km/h at a distance of 20 km was recently
</p>
<p>proven for the first time; see: Sebastian Nauert et al., Air-to-ground quantum communication, Nature
</p>
<p>Photonics (2013), https://doi.org/10.1038/nphoton.2013.46.</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1038/nature23655">https://doi.org/10.1038/nature23655</a></div>
<div class="annotation"><a href="https://doi.org/10.1038/nature23655">https://doi.org/10.1038/nature23655</a></div>
<div class="annotation"><a href="https://doi.org/10.1038/nphoton.2013.63">https://doi.org/10.1038/nphoton.2013.63</a></div>
<div class="annotation"><a href="https://doi.org/10.1038/nphoton.2013.46">https://doi.org/10.1038/nphoton.2013.46</a></div>
</div>
<div class="page"><p/>
<p>Chapter 11
</p>
<p>Abstract Notation
</p>
<p>We are now starting to bring together the analytical and the algebraic approaches to quantum
</p>
<p>mechanics. In this chapter, we first consider the vector space of solutions of the SEq in more
</p>
<p>detail. After a brief excursion into matrix mechanics, we treat the abstract representation of
</p>
<p>quantum mechanics, which is formulated in terms of the familiar bras and kets.
</p>
<p>In Chap. 10, we saw that the analytic and the algebraic approaches lead to very similar
</p>
<p>formulations. We deepen this parallelism in the following sections by showing that
</p>
<p>the expression
&int;
</p>
<p>&lowast;dV is a scalar product. With some additional assumptions, it
follows that the vector spaces of both the algebraic and analytic approaches are Hilbert
</p>
<p>spaces. With this background, we can then formulate a representation-independent,
</p>
<p>i.e. an abstract notation.
</p>
<p>All the spectra which we consider in this chapter are discrete and non-degenerate.
</p>
<p>11.1 Hilbert Space
</p>
<p>11.1.1 Wavefunctions and Coordinate Vectors
</p>
<p>In Chap. 10, we ventured the guess that
&int;
</p>
<p>&lowast;dV is a scalar product. We now want
to provide some additional motivation for this assumption.
</p>
<p>We start with a Hamiltonian H (with a discrete and non-degenerated energy
</p>
<p>spectrum). Its eigenfunctions n (r), i.e. the solutions of the stationary SEq
</p>
<p>Hn (r) = Enn (r); n = 1, 2, . . . (11.1)
</p>
<p>are known and form a CONS. Because of the completeness of n (r), we can write
</p>
<p>any solution  (r, t) of the time-dependent SEq as
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4_11
</p>
<p>139</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_11&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_11&amp;domain=pdf</a></div>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4_11">https://doi.org/10.1007/978-3-030-00464-4_11</a></div>
</div>
<div class="page"><p/>
<p>140 11 Abstract Notation
</p>
<p> (r, t) =
&sum;
</p>
<p>n
cnn (r) e
</p>
<p>&minus;i En t
 ; cn &isin; C. (11.2)
</p>
<p>To save writing, we restrict the following considerations to the initial state (i.e. we
</p>
<p>freeze time at t = 0):
</p>
<p> (r, 0) =
&sum;
</p>
<p>n
cnn (r) . (11.3)
</p>
<p>The total time evolution can be determined easily from (11.2). Due to the orthonor-
</p>
<p>mality of the eigenfunctions, the coefficients cn are specified uniquely by the initial
</p>
<p>condition  (r, 0):
</p>
<p>cn =
&int;
</p>
<p>&lowast;n (r) (r, 0) dV . (11.4)
</p>
<p>We can understand this situation a little differently. To this end, we take into
</p>
<p>account the fact that the eigenfunctions {n (r)} represent an orthonormal basis of
the vector space V of solutions of the SEq&mdash;analogous to the three unit vectors
</p>
<p>ex , ey and ez in the visual space or R
3. In the latter space, we can represent a general
</p>
<p>vector v as v =vx ex + vyey + vzez , where the components or expansion coefficients
vx , vy, vz are usually called the coordinates of v . It makes no difference whether we
</p>
<p>specify v or vx , vy, vz&mdash;we can calculate v uniquely from vx , vy, vz and vice versa,
</p>
<p>if the unit vectors are known.
</p>
<p>The situation described in (11.3) and (11.4) is quite analogous - only we are dealing
</p>
<p>with the function  (r, 0) instead of the vector v, the eigenfunctions n instead of the
</p>
<p>unit vectors ei , and the constants cn instead of the coordinates vx . For example, the
</p>
<p>cn can be determined uniquely (for known n), if  (r, 0) is given, and vice versa.
</p>
<p>We can thus denote the expansion coefficients cn as coordinates and have the same
</p>
<p>information (always assuming that n is known), whether we are given  (r, 0) or
</p>
<p>the coordinate vector
</p>
<p>c =
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>c1
c2
...
</p>
<p>
</p>
<p>
</p>
<p>
(11.5)
</p>
<p>We now consider two wavefunctions  =
&sum;
</p>
<p>i cii and  =
&sum;
</p>
<p>j d j j . Because
</p>
<p>of the orthonormality of the eigenfunctions, we have
</p>
<p>&int;
</p>
<p>&lowast;dV =
&sum;
</p>
<p>i j
</p>
<p>c&lowast;i d j
</p>
<p>&int;
</p>
<p>&lowast;i  j dV =
&sum;
</p>
<p>i j
</p>
<p>c&lowast;i d ji j =
&sum;
</p>
<p>i
</p>
<p>c&lowast;i di . (11.6)
</p>
<p>We find exactly the same result when we take the dot product of the two coordinate
</p>
<p>vectors c and d; it is</p>
<p/>
</div>
<div class="page"><p/>
<p>11.1 Hilbert Space 141
</p>
<p>c&dagger;d =
(
</p>
<p>c&lowast;1 c
&lowast;
2 . . .
</p>
<p>)
</p>
<p>&middot;
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>d1
d2
...
</p>
<p>
</p>
<p>
</p>
<p>
=
</p>
<p>&sum;
</p>
<p>i
</p>
<p>c&lowast;i di (11.7)
</p>
<p>Comparing (11.6) and (11.7), we see that the expression
&int;
</p>
<p>&lowast;dV is clearly a
scalar product.1
</p>
<p>11.1.2 The Scalar Product
</p>
<p>The formal confirmation that
&int;
</p>
<p>&lowast;dV is a scalar product is found in mathematics.
There, the scalar product is generally defined as a rule which assigns to two elements
</p>
<p>x and y of a vector space a scalar (x, y), where the following properties must apply:
</p>
<p>(x, y) is (i) positive definite: (x, x) &ge; 0 and (x, x) = 0 &harr; x = 0; (ii) linear:
(x,y + z) =  (x, y)+ (x, z); (iii) Hermitian or conjugate symmetric: (x, y) =
(y, x)&lowast; (see also Appendix F, Vol. 1). Any rule that meets these requirements is a scalar
product (also called Hermitian form).
</p>
<p>In order to test
&int;
</p>
<p>f &lowast;gdV for these properties, we do not choose the notation ( f, g),
but refer instead to the algebraic approach  f | g, i.e.
</p>
<p> f | g :=
&int;
</p>
<p>f &lowast;gdV . (11.8)
</p>
<p>At this point is not clear how a ket |g and a bra  f | are specifically defined; we
address this question in Chap. 12. But notwithstanding this, we can easily show that
&int;
</p>
<p>f &lowast;gdV is a scalar product - even though the expression may not have looked like
one when we wrote it down for the first time in Chap. 5. For it is immediately apparent
</p>
<p>that
&int;
</p>
<p>f &lowast;gdV assigns a number to two elements.2 Furthermore,
&int;
</p>
<p>f &lowast;gdV is
</p>
<p>1. positive definite:  f | f  =
&int;
</p>
<p>f &lowast; f dV &ge; 0,&isin; R where  f | f  = 0 &hArr; f &equiv; 0.
2. linear3:  f | g + h =
</p>
<p>&int;
</p>
<p>f &lowast; (g + h) dV = 
&int;
</p>
<p>f &lowast;gdV + 
&int;
</p>
<p>f &lowast;hdV =
  f | g +   f | h.
</p>
<p>3. Hermitian or conjugate symmetric:  f | g =
&int;
</p>
<p>f &lowast;gdV =
(&int;
</p>
<p>f g&lowast;dV
)&lowast; =
</p>
<p>g| f &lowast;.
</p>
<p>1We see, by the way, that the scalar product is independent of the representation. The left-hand
</p>
<p>sides of (11.6) and (11.7) are two different representations of the same expression.
2We remark again that in general, we do not specify the integration limits for integrals as in (11.8). It
</p>
<p>is tacitly assumed that one integrates over the entire domain of definition of the integrand. Contrary
</p>
<p>to the initial impression, these integrals are definite integrals - in other words, scalars (which may
</p>
<p>be time dependent).
3More precisely, semi-linear in the first and linear in the second component (also denoted as anti-
</p>
<p>linear or conjugate linear in the first argument and linear in the second argument). Therefore, the
</p>
<p>form is not called bilinear, but sesquilinear. In mathematics, the form is usually defined the other
</p>
<p>way around, as antilinear in the second argument.</p>
<p/>
</div>
<div class="page"><p/>
<p>142 11 Abstract Notation
</p>
<p>By means of the scalar product
&int;
</p>
<p>f &lowast;gdV , we can therefore not only define as
</p>
<p>usual the length or norm of wavefunctions by |||| =
&radic;
|  =
</p>
<p>&radic;
</p>
<p>&int;
</p>
<p>&lowast;dV
</p>
<p>and the orthogonality of two wave functions (as elements of the vector space) by
</p>
<p>|  =
&int;
</p>
<p>&lowast;dV = 0, but also we can use general statements about scalar products
(e.g. the Schwarz and the triangle inequalities) without further ado.
</p>
<p>Thus, the solutions of the SEq span a complex vector space in which a scalar
</p>
<p>product is defined. Such spaces are called unitary spaces, as we know already from
</p>
<p>the algebraic approach (Chap. 4). At this point it is perhaps possible to understand
</p>
<p>somewhat better why the eigenfunctions m,n , obeying
&int;
</p>
<p>&lowast;mndV = nm , are
called orthonormal. On the one hand, one can call an element of a vector space a
</p>
<p>&lsquo;vector&rsquo;&mdash;an eigenfunction as an element of a vector space is an eigenvector. On the
</p>
<p>other hand,
&int;
</p>
<p>&lowast;mndV = 0 for n = m means in the sense of a scalar product that
the eigenfunctions (as eigenvectors) are pairwise orthogonal,4 and
</p>
<p>&int;
</p>
<p>&lowast;nndV = 1
means that they have length 1 or are normalized.
</p>
<p>11.1.3 Hilbert Space
</p>
<p>The way we constructed it, our unitary space is separable.5 This means, essentially,
</p>
<p>that there is a CONS of at most countably infinite dimension. Any vector  can be
</p>
<p>expanded in terms of this CONS (expansion theorem):
</p>
<p> (r, t) =
&sum;
</p>
<p>n
dn(t)n(r) with dn(t) =
</p>
<p>&int;
</p>
<p>&lowast;n(r) (r, t) dV . (11.9)
</p>
<p>This sum and others like | (r, t)|2 =
&sum;
</p>
<p>n
</p>
<p>|dn(t)|2 must of course be meaningful,
i.e. they must converge to an element which itself belongs to the vector space. We
</p>
<p>therefore require that the vector space be complete, which means that sequences6
</p>
<p>have limits which are themselves elements of the vector space.7 A space with all
</p>
<p>these ingredients is called a (separable)8 Hilbert space H. A quantum-mechanical
</p>
<p>state is an element of H and thus may be denoted, as stated above, as a vector, even
</p>
<p>if it is in fact a function in the concrete representation.
</p>
<p>4As said above, this does not mean that the graphs of the functions are orthogonal to each other
</p>
<p>or something similar. The statement refers only to the (abstract) angle between two vectors in the
</p>
<p>vector space.
5The term &lsquo;separable&rsquo; which occurs here has nothing to do with the requirement of &lsquo;separability,&rsquo;
</p>
<p>which means that a system (function) is separable into functions of space and of time.
6The technical term is Cauchy sequences, see Appendix G, Vol. 1.
7The requirement of completeness has no straightforward physical meaning, but it occurs in many
</p>
<p>proofs of laws concerning Hilbert spaces.
8There are also non-separable Hilbert spaces (for example, in the quantization of fields). But in
</p>
<p>&lsquo;our&rsquo; quantum mechanics, they play no role, so here &lsquo;Hilbert space&rsquo; means in general &lsquo;separable
</p>
<p>Hilbert space.&rsquo;</p>
<p/>
</div>
<div class="page"><p/>
<p>11.1 Hilbert Space 143
</p>
<p>We see in retrospect that the unitary spaces which we considered in the algebraic
</p>
<p>approach are also separable Hilbert spaces. Now, the punchline of this story is that
</p>
<p>all Hilbert spaces of the same dimension are isomorphic, i.e. there are reversible
</p>
<p>unique (one-to-one, bijective) mappings between them. That is why we also often
</p>
<p>speak of the Hilbert space H of dimension N , for which there are various realizations
</p>
<p>or representations. In particular, the space of solutions (11.2) of the SEq, as spanned
</p>
<p>by {n (r)}, and the space of the coordinate vectors c are isomorphic; that is they are
just different representations of the same systems. Of course, the question arises as
</p>
<p>to whether there is a representation-independent, i.e. abstract formulation of these
</p>
<p>systems. We take up this issue below.
</p>
<p>If we disregard the technical issues (which we do not consider to a greater extent
</p>
<p>in the following), then H is basically a very intuitive structure. As we have already
</p>
<p>indicated above, we can in principle imagine everything as inR3, despite the possibly
</p>
<p>much higher dimensionality of the Hilbert space and its use of complex numbers. In
</p>
<p>both spaces, mutually orthogonal and normalized vectors, i.e. unit vectors, constitute
</p>
<p>a basis and span the entire space; any vector can therefore be represented as a linear
</p>
<p>combination of basis vectors. In addition, we also have an inner product in both
</p>
<p>spaces, which automatically defines a norm. We can imagine an intuitive analog to
</p>
<p>the time-dependent (normalized) state vector (r, t) &isin; H of (11.9): InR3 this would
be a vector of length 1, which moves in the course of time. A state with a sharp energy
</p>
<p>&isin; H corresponds to a circular motion in R3, because the time dependence in H is
given by exp(&minus;it).9
</p>
<p>11.2 Matrix Mechanics
</p>
<p>We have seen that we obtain the same information if we specify the vector c instead
</p>
<p>of the wavefunction  (r, 0). We will now apply this &lsquo;algebraization&rsquo; to eigenvalue
</p>
<p>problems, also.
</p>
<p>In fact, in the early days of quantum mechanics there were two competing formu-
</p>
<p>lations: Matrix mechanics (associated with the name W. Heisenberg, corresponding
</p>
<p>essentially to our algebraic approach), and wave mechanics (linked to the name
</p>
<p>Schr&ouml;dinger, corresponding essentially to our analytic approach). Quite soon it
</p>
<p>became clear that these formulations, for the same initial physical situation, were
</p>
<p>just two different descriptions of the same facts, which hence could be converted
</p>
<p>one-to-one into each other. This can be shown rather simply in a way similar to the
</p>
<p>above provisional representation using coordinates.
</p>
<p>We start from the formulation of an eigenvalue problem of wave mechanics, con-
</p>
<p>sidering a wavefunction  (x) and an operator A with a discrete and non-degenerate
</p>
<p>spectrum:
</p>
<p>9We note that approaching quantum mechanics by means of the Hilbert space is not the only possible
</p>
<p>option. As a starting point, one could for example consider a C&lowast;-algebra (see Appendix G, Vol. 1),
or the aforementioned replacement process {, }Poisson &rarr; 1i [, ]commutator . This method is called
canonical quantization, see Appendix W, Vol. 2.</p>
<p/>
</div>
<div class="page"><p/>
<p>144 11 Abstract Notation
</p>
<p>A (x) = a (x) . (11.10)
</p>
<p>This problem can be written as a matrix equation (that is, among other things:
</p>
<p>with no spatial dependence). To demonstrate this, in a first step we expand the
</p>
<p>wavefunction in terms of the eigenfunctions {i } of the Hamiltonian (or any other
basis system in H) as  =
</p>
<p>&sum;
</p>
<p>n cnn (x), and obtain
</p>
<p>A
&sum;
</p>
<p>n
cnn (x) = a
</p>
<p>&sum;
</p>
<p>n
cnn (x) . (11.11)
</p>
<p>Then we multiply by &lowast;m and take the scalar product:
</p>
<p>&sum;
</p>
<p>n
cn
</p>
<p>&int;
</p>
<p>&lowast;m AndV =
&sum;
</p>
<p>n
cn
</p>
<p>&int;
</p>
<p>&lowast;mndV = acm (11.12)
</p>
<p>The integral
&int;
</p>
<p>&lowast;m AndV is a number that depends on n and m. We call this number
Amn:
</p>
<p>&sum;
</p>
<p>n
cn Amn = acm . (11.13)
</p>
<p>The expression on the left side is simply the product of the matrix {Amn} &equiv; A with
the column vector c:
</p>
<p>Ac = ac. (11.14)
</p>
<p>The column vector c is of course just the coordinate vector introduced above. In
</p>
<p>this way, we can formulate quantum mechanics as matrix mechanics, representing
</p>
<p>operators as matrices and states as column vectors. In practice, this is not done in
</p>
<p>general, but only in cases where this approach is particularly well-suited (e.g. in
</p>
<p>lower-dimensional systems).
</p>
<p>11.3 Abstract Formulation
</p>
<p>Thus far, we have met up with various formulations of states, which at first glance
</p>
<p>seem to have little in common. In the analytical approach, we started from the wave-
</p>
<p>function  (r), but instead we could have chosen the coordinate vector c. Moreover,
</p>
<p>there are other possibilities, e.g. the Fourier transform of  (k) =
&int;
</p>
<p> (r) eikrd3r ,
</p>
<p>which provides the same information as the wavefunction itself. In the algebraic
</p>
<p>approach, we worked with kets, for which the various representations in the form of
</p>
<p>a column vector are possible. An analogous consideration applies to different for-
</p>
<p>mulations of operators: We have just seen that we can likewise write the operators of
</p>
<p>the analytical approach as matrices. In the algebraic approach, we defined operators
</p>
<p>as dyadic products, or represented them as matrices. All together, we have quite
</p>
<p>different but equivalent ways at hand to describe the same facts.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Abstract Formulation 145
</p>
<p>The circumstance that Hilbert spaces of the same dimension are isomorphic is
</p>
<p>beyond the parallelism of the two approaches established in Chap. 10, and it shows
</p>
<p>that the analytical and the algebraic approaches are actually just different mani-
</p>
<p>festations of the same facts. For whether we work in a Hilbert space of one or
</p>
<p>the other approach (and in which one) is irrelevant, insofar as there are one-to-one
</p>
<p>transformations between all Hilbert spaces of the same dimension. But if we have
</p>
<p>very different representations for the same facts, there must exist a representation-
</p>
<p>independent, i.e. abstract core.10
</p>
<p>The fact that we can represent one and the same (physical) situation in many
</p>
<p>different ways is known to us in a similar form e.g. from R3. There, a vector also has
</p>
<p>different representations (components), depending on how we define our coordinate
</p>
<p>system in space. We can specify this vector abstractly, i.e. in a coordinate-free manner,
</p>
<p>by writing it not just as a column vector with several components, but instead by
</p>
<p>denoting it as a or
&minus;&rarr;
a or something similar. That will not only suffice for many
</p>
<p>formulations (e.g. l = r &times; p), but it also facilitates them or renders them expressible
in a compact form; think of e.g. the Maxwell equations. For concrete calculations,
</p>
<p>however, one often has to specify the vectors in some particular representation.11
</p>
<p>Here, we are in a similar situation, seeking an abstract designation for the elements
</p>
<p>of the Hilbert space. The notations a or
&minus;&rarr;
a are &lsquo;used up&rsquo; and also too strongly suggest
</p>
<p>a column vector. Instead, the convention of writing an abstract vector of the Hilbert
</p>
<p>space as a ket, | has been adopted. It is for this reason that we denoted states as kets
from the start in the algebraic approach12 (a justification in retrospect, so to speak).
</p>
<p>In this way, we can write for example an eigenvalue equation like
</p>
<p>Aspatial (x) = a (x) ; Amatrixc = ac (11.15)
</p>
<p>in the abstract formulation as
</p>
<p>Aabstract | = a | . (11.16)
</p>
<p>Some remarks are in order:
</p>
<p>10Only the dimension of the state space matters here. The physical system can take a variety of forms.
</p>
<p>The electronic spin with its two orientations, the polarization of a photon, e.g. with horizontally
</p>
<p>and vertically linearly-polarized states, the MZI with the basis states |H and |V , in a certain sense
the ammonia molecule (N H3, where the N atom can tunnel through the H3 plane and occupy two
</p>
<p>states with respect to it) are some examples of physically different systems which all &lsquo;live&rsquo; in a
</p>
<p>two-dimensional Hilbert space.
11In fact, the notation a is very abstract&mdash;it does not reveal anything about the dimension nor the
</p>
<p>individual components. We know nothing more than simply that it is a vector. Nevertheless, this
</p>
<p>notation is often not perceived as particularly abstract. This is probably due to the fact that one was
</p>
<p>introduced to it at the beginning of physics courses and it now seems familiar.
12This is also why we chose the symbol &sim;= to distinguish between an abstract ket and its represen-
tation as a column vector.</p>
<p/>
</div>
<div class="page"><p/>
<p>146 11 Abstract Notation
</p>
<p>1. It is not known at this point how the ket | is formulated in detail13; it is just
an abstract notation, comparable to the designation of a vector by the symbol a.
</p>
<p>Similarly, the form of the operator Aabstract is not known at this point; this is
</p>
<p>comparable to the use of the abstract symbol A for a general matrix.14
</p>
<p>2. For the sake of a better distinction, we have denoted the nature of the operators in
</p>
<p>(11.15) and (11.16) by an index. But it is quite common15 to use the same symbol
</p>
<p>for the operators in different concrete and abstract representations, that is to write
</p>
<p>simply A in all three equations:
</p>
<p>A (x) = a (x); Ac = ac;A | = a | . (11.17)
</p>
<p>Strictly speaking, this is evidently wrong, because A refers to quite different math-
</p>
<p>ematical objects e.g. in the expressions A (x) and A |. That this &lsquo;nonchalant&rsquo;
notation is rather ambiguous may seem annoying, but it is widespread and can,
</p>
<p>if one is used to it, even be quite practical. Of course, it must be clear from the
</p>
<p>context what is precisely meant where necessary.
</p>
<p>In the algebraic approach, we introduced the symbol &sim;= to emphasize the differ-
ence between an abstract ket and its representation as a column vector. In the
</p>
<p>following, we will relax this rule and often use = instead of &sim;=, thus following
common practice.
</p>
<p>3. The relationship between  (x) and | will be addressed in Chap. 12.
</p>
<p>The following paragraph is merely a repetition of the facts already discussed in the
</p>
<p>preceding (even-numbered) chapters. We recall that the adjoint (of a column vector)
</p>
<p>means the transposed and complex conjugated vector. The adjoint16 of a ket is a bra:
</p>
<p>(|)&dagger; &equiv; | (11.18)
</p>
<p>(accordingly, the adjoint of a column vector is the row vector with complex conjugate
</p>
<p>elements). The adjoint of an operator A is written as A&dagger;, where AA&dagger; = A&dagger; A holds.
Because the application of an operator A to a ket | gives another ket, one also
writes
</p>
<p>A | &equiv; |A. (11.19)
</p>
<p>The adjoint of a number is its complex conjugate c&dagger; = c&lowast;. In particular, we have for
the scalar product  f | g:
</p>
<p>13It is in any case not a column vector (even if this idea sometimes proves to be helpful).
14To avoid misunderstandings: a is an abstract or general column vector, whereas | is an abstract
state which can be represented, where appropriate, as a column vector, but for which also other rep-
</p>
<p>resentations exist. Quite analogously, A denotes a general matrix and Aabstract an abstract operator,
</p>
<p>which can, where appropriate, be represented as a matrix.
15However, there are books that distinguish them quite consistently.
16Note that here &lsquo;adjoint&rsquo; means the Hermitian adjoint a&dagger;, as always in non-relativistic quantum
</p>
<p>mechanics. In relativistic quantum mechanics, one uses instead the Dirac adjoint a&dagger; 
o
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Abstract Formulation 147
</p>
<p> f | g&dagger; =  f | g&lowast; = g | f . (11.20)
</p>
<p>In the adjoint of a compound expression, the order of the constituents is reversed.
</p>
<p>We give some examples for the adjoint:
</p>
<p>(c |)&dagger; = c&lowast; | = | c&lowast;
(A |)&dagger; = |A&dagger; = A| = | A&dagger;
</p>
<p> |A|&dagger; =
&lang;
</p>
<p>

</p>
<p>A&dagger;

</p>
<p>
&rang;
</p>
<p>.
</p>
<p>(11.21)
</p>
<p>Expressions of the form  |A| are called matrix elements . Finally, we note again
the equations defining a Hermitian operator: In the formulation with integrals, we
</p>
<p>have
&int;
</p>
<p>&lowast;1 A2dV =
&int;
</p>
<p>(A1)
&lowast; 2dV . (11.22)
</p>
<p>With
&int;
</p>
<p>f &lowast;gdV =  f |g, this is written in the bra-ket notation as
&int;
</p>
<p>&lowast;1 A2dV = 1 |A2 = 1 |A|2
&int;
</p>
<p>(A1)
&lowast; 2dV = A1 |2 =
</p>
<p>&lang;
</p>
<p>1

</p>
<p>A&dagger;

</p>
<p>2
&rang;
</p>
<p>.
</p>
<p>(11.23)
</p>
<p>Comparing the right-hand sides exhibits a familiar result: For a Hermitian operator,
</p>
<p>it holds that A = A&dagger;; the operator is self-adjoint.17
Experience shows that it is rather difficult to imagine something quite abstract
</p>
<p>(only kidding!). So here, we give the hint to think of a column (row) vector in the
</p>
<p>case of a ket (bra), and of a matrix in the case of an operator (and not to forget that this
</p>
<p>is an auxiliary notion). In this way, many &lsquo;calculation rules&rsquo; and statements become
</p>
<p>quite familiar, e.g. the rule that operators do not commute in general&mdash;which applies
</p>
<p>also to matrices.
</p>
<p>Although the relation between e.g.(x) and | still needs to be clarified, we can
&lsquo;play around&rsquo; a little with the abstract notation. As an example, we consider a CONS
</p>
<p>{n(x)}. From the expansion theorem, it follows for each wavefunction (x) that
</p>
<p>(x) =
&sum;
</p>
<p>n
cnn(x). (11.24)
</p>
<p>With
</p>
<p>cm =
&int;
</p>
<p>&lowast;m(x)(x)dx = m |, (11.25)
</p>
<p>we find due to
&int;
</p>
<p>&lowast;m(x)n(x)dx = nm (11.26)
</p>
<p>17In fact, there may be a difference between self-adjoint and Hermitian (see Chap. 13 and Appendix I,
</p>
<p>Vol. 1). Among the problems considered here, this difference is not noticeable.</p>
<p/>
</div>
<div class="page"><p/>
<p>148 11 Abstract Notation
</p>
<p>the equation
</p>
<p> | =
&int;
</p>
<p>&lowast;(x)(x)dx =
(11.24)
</p>
<p>&int;
&sum;
</p>
<p>n,m c
&lowast;
ncm
</p>
<p>&lowast;
n(x)m(x)dx =
</p>
<p>(11.26)
</p>
<p>=
(11.26)
</p>
<p>&sum;
</p>
<p>n c
&lowast;
ncn =
</p>
<p>(11.25)
</p>
<p>&sum;
</p>
<p>n  |n n | .
(11.27)
</p>
<p>Comparing the right-hand and left-hand sides, we obtain the completeness relation
</p>
<p>in the abstract notation
&sum;
</p>
<p>n
|n n| = 1, (11.28)
</p>
<p>i.e. a result which we knew already from the algebraic approach.
</p>
<p>11.4 Concrete: Abstract
</p>
<p>Finally, we make a remark about the relationship of the abstract formulation to
</p>
<p>concrete representations.
</p>
<p>In the algebraic approach, the &lsquo;de-abstracting&rsquo; of kets is not problematic. We can
</p>
<p>associate (and have done so occasionally) a ket to a representation as a column vector
</p>
<p>such as e.g. |h &sim;=
(
</p>
<p>1
</p>
<p>0
</p>
<p>)
</p>
<p>. Since we formulate operators in this approach as sums over
</p>
<p>dyadic products, the concrete representation of operators may be easily formulated.
</p>
<p>Thus, there are no difficulties with the algebraic formulation at this level.
</p>
<p>The situation is similar in the analytical approach when we have a discrete spec-
</p>
<p>trum. Again, as we have just explored, we can represent states and operators that
</p>
<p>depend on local variables by vectors and matrices.
</p>
<p>In order to illustrate these relationships through examples, we start with a Hamil-
</p>
<p>tonian H = &minus; 2
2m
</p>
<p>&nabla;2+V (r) with a discrete and nondegenerate spectrum. We want to
derive the matrix representation and the abstract formulation for the stationary SEq.
</p>
<p>(Similar considerations for the time-dependent SEq can be found in the exercises.)
</p>
<p>The eigenvalue problem (stationary SEq) reads
</p>
<p>H (r) = E (r) . (11.29)
</p>
<p>The eigenfunctions n (r) and the eigenvalues En of the Hamiltonian are known:
</p>
<p>Hn (r) = Enn (r); n = 1, 2, . . . (11.30)
</p>
<p>Every state  (r) can be written as a linear combination of the eigenfunctions (which
</p>
<p>form a CONS):
</p>
<p> (r) =
&sum;
</p>
<p>n
cnn (r); cn =
</p>
<p>&int;
</p>
<p>&lowast;n (r) (r) dV . (11.31)</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4 Concrete: Abstract 149
</p>
<p>In the matrix representation, we represent the state by the column vector of the
</p>
<p>coefficients cn:
</p>
<p>c =
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>c1
c2
...
</p>
<p>
</p>
<p>
</p>
<p>
(11.32)
</p>
<p>We identify as follows:
</p>
<p>1 (r) &rarr;
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>1
</p>
<p>0
...
</p>
<p>
</p>
<p>
</p>
<p>
; 2 (r) &rarr;
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>0
</p>
<p>1
...
</p>
<p>
</p>
<p>
</p>
<p>
etc. (11.33)
</p>
<p>Now we have to replace H by a matrix. For this, we repeat the above reasoning
</p>
<p>by inserting (11.31) into (11.29), multiplying by &lowast;m (r), and integrating:
</p>
<p>&sum;
</p>
<p>n
cn
</p>
<p>&int;
</p>
<p>&lowast;m (r) Hn (r) dV = E
&sum;
</p>
<p>n
cn
</p>
<p>&int;
</p>
<p>&lowast;m (r)n (r) dV . (11.34)
</p>
<p>On the left-hand side, we employ (11.30), and on both sides we make use of the
</p>
<p>orthonormality of the eigenfunctions. It follows that
</p>
<p>Emcm = Ecm . (11.35)
</p>
<p>In other words, the Hamiltonian H is replaced by a diagonal matrix Hmatrix:
</p>
<p>Hmatrix =
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>E1 0 . . .
</p>
<p>0 E2 . . .
...
</p>
<p>...
. . .
</p>
<p>
</p>
<p>
</p>
<p>
(11.36)
</p>
<p>and the stationary SEq in the matrix representation reads
</p>
<p>Hmatrixc = Ec. (11.37)
</p>
<p>From this equation, we can reconstruct (11.29)&mdash;but as said above, only if we know
</p>
<p>the eigenfunctions n (r) which do not appear in (11.37).
</p>
<p>In order to arrive at the abstract notation, we interpret the vector
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>1
</p>
<p>0
...
</p>
<p>
</p>
<p>
</p>
<p>
as a
</p>
<p>representation of the ket |1, and analogously for the other components. Then it
follows that
</p>
<p>|1 1| &sim;=
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>1
</p>
<p>0
...
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>(
</p>
<p>1 0 . . .
)
</p>
<p>=
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>1 0 . . .
</p>
<p>0 0 . . .
...
...
. . . ,
</p>
<p>
</p>
<p>
</p>
<p>
(11.38)</p>
<p/>
</div>
<div class="page"><p/>
<p>150 11 Abstract Notation
</p>
<p>and we find for the abstract representation of the Hamiltonian18:
</p>
<p>Habstract =
&sum;
</p>
<p>n
|n n| En. (11.39)
</p>
<p>11.5 Exercises
</p>
<p>1. Show that the equation
&sum;
</p>
<p>i
</p>
<p>ci A j i = ac j (11.40)
</p>
<p>may be written in the matrix representation as
</p>
<p>Ac = ac (11.41)
</p>
<p>with the matrix
{
</p>
<p>A j i
}
</p>
<p>&equiv; A and the column vector c. Is the equation valid also for
non-square matrices?
</p>
<p>2. Do the functions of one variable which are continuous in the interval [0, 1] form
</p>
<p>a Hilbert space?
</p>
<p>3. The space l(2) consists of all vectors | with infinitely many components (coor-
dinates) c1, c2, . . ., such that
</p>
<p>|2 =
&sum;
</p>
<p>n
</p>
<p>|cn|2 &lt; &infin;. (11.42)
</p>
<p>Show that also the linear combination of two vectors | and | belongs to this
space, and that the scalar product  | is defined.
</p>
<p>4. Given the operator A and the equation
</p>
<p>i
d
</p>
<p>dt
| = A |, (11.43)
</p>
<p>which condition must A fulfill so that the norm of | is conserved?
5. Given the operator A, derive the equation
</p>
<p>i
d
</p>
<p>dt
A = [A, H ] + i
</p>
<p>&lang;
</p>
<p>A
&rang;
</p>
<p>(11.44)
</p>
<p>in the bra-ket formalism.
</p>
<p>6. Given a Hamiltonian H with a discrete and nondegenerate spectrum, (a) in the
</p>
<p>formulation with space variables, and (b) as an abstract operator; what is in each
</p>
<p>case the matrix representation of the time-dependent SEq?
</p>
<p>18This form is called spectral representation; we discuss it in more detail in Chap. 13.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 12
</p>
<p>Continuous Spectra
</p>
<p>In this chapter we start by considering continuous spectra, which we have neglected thus
</p>
<p>far. Then we investigate the relationship between  (x) and |. With these results, the
unification of the analytic and the algebraic approaches to quantum mechanics is completed.
</p>
<p>So far, we have excluded continuous spectra from the discussion, e.g. by placing our
</p>
<p>quantum object between infinitely high potential walls,1 thus discretizing the energy
</p>
<p>spectrum. The fact that we adopted this limitation had less to do with physical reasons,
</p>
<p>but rather almost exclusively with mathematical ones.2 From a physical point of
</p>
<p>view, a continuous spectrum (e.g. the energy of a free quantum object) makes perfect
</p>
<p>sense. But we have the problem that the corresponding eigenfunctions are not square
</p>
<p>integrable, and therefore we cannot properly define a scalar product. This hurdle may
</p>
<p>be circumvented or alleviated, as we shall show in a moment, by the construction of
</p>
<p>eigendifferentials which leads to improper vectors. Finally, we examine the question
</p>
<p>of how a ket | is related to the corresponding wavefunction (r), or how we
can transform abstract equations into &lsquo;concrete&rsquo; equations, e.g. in the position or the
</p>
<p>momentum representation.
</p>
<p>1Another possibility would be the introduction of periodic boundary conditions.
2Below the Planck scale (&sim;10&minus;35 m, &sim;10&minus;44 s), neither space nor time may exist, so that ultimately
these variables would become &lsquo;grainy&rsquo; or discrete. (On this scale, space is thought to be something
</p>
<p>like a foam bubbling with tiny black holes, continuously popping in and out of existence.) There
</p>
<p>are attempts to determine whether space is truly grainy, but results have so far been inconclusive.
</p>
<p>Experimentally, these orders of magnitude are still very far from being directly accessible (if they
</p>
<p>ever will be); the currently highest-energy accelerator, the LHC at CERN in Geneva, attains a
</p>
<p>spatial resolution of &lsquo;only&rsquo; &sim;10&minus;19 m. Recently, however, indirect methods were proposed; see
Jakob D. Bekenstein, &lsquo;Is a tabletop search for Planck scale signals feasible?&rsquo;, http://arxiv.org/abs/
</p>
<p>1211.3816 (2012); or Igor Pikovski et al., &lsquo;Probing Planck-scale physics with quantum optics&rsquo;,
</p>
<p>Nature Physics 8, 393&ndash;397 (2012). For a recent paper see e.g. V. Faraoni, &lsquo;Three new roads to the
</p>
<p>Planck scale&rsquo;, American Journal of Physics 85, 865 (2017); https://doi.org/10.1119/1.4994804
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4_12
</p>
<p>151</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_12&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_12&amp;domain=pdf</a></div>
<div class="annotation"><a href="http://arxiv.org/abs/1211.3816">http://arxiv.org/abs/1211.3816</a></div>
<div class="annotation"><a href="http://arxiv.org/abs/1211.3816">http://arxiv.org/abs/1211.3816</a></div>
<div class="annotation"><a href="https://doi.org/10.1119/1.4994804">https://doi.org/10.1119/1.4994804</a></div>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4_12">https://doi.org/10.1007/978-3-030-00464-4_12</a></div>
</div>
<div class="page"><p/>
<p>152 12 Continuous Spectra
</p>
<p>A remark on notation: discrete spectra are often written with Latin, continuous
</p>
<p>spectra with Greek letters (exceptions to this are position x and momentum k, as well
</p>
<p>as the energy E , which can have discrete and/or continuous values). For example, if
</p>
<p>the spectrum of the Hamiltonian is discrete or continuous, we write
</p>
<p>H |l = El |l or H | = E |, (12.1)
</p>
<p>or
</p>
<p>H |El = El |El or H |E = E |E. (12.2)
</p>
<p>In addition, a &lsquo;direct&rsquo; terminology is common, in which a state of quantum number
</p>
<p>n or  (the system &lsquo;has&rsquo; the quantum number n or ) is written as
</p>
<p>|n : discrete quantum number n
| : continuous quantum number . (12.3)
</p>
<p>12.1 Improper Vectors
</p>
<p>Free motion (cf. Chap. 5) is a simple example of the continuous case. We have3
</p>
<p>k (x) = 1&radic;
2
</p>
<p>eikx, and with our notation
</p>
<p>&int;
</p>
<p>&lowast;k &prime; (x)k (x) dx &equiv; k &prime; |k &equiv;
&lang;
</p>
<p>k &prime; |k (12.4)
</p>
<p>for the scalar product, it follows that:
</p>
<p>&lang;
</p>
<p>k &prime; |k = 1
2
</p>
<p>&infin;
&int;
</p>
<p>&minus;&infin;
</p>
<p>ei x(k&minus;k
&prime;)dx = (k &prime; &minus; k). (12.5)
</p>
<p>The difficulty lies in the fact that the physical problem is not properly formu-
</p>
<p>lated: The Heisenberg uncertainty principle tells us that a state with a definite, sharp
</p>
<p>momentum has an infinite position uncertainty, as may indeed be read off directly
</p>
<p>from the function eikx. Mathematically, this is expressed by the fact that the integral
</p>
<p>in (12.5) is not defined &lsquo;properly&rsquo;&mdash;the integral does not exist in the usual sense,
</p>
<p>but is a functional, namely the delta function (k &prime; &minus; k).4 This means that the ket
|k &equiv; |k is well defined, but not the bra k | &equiv; k|. In other words, the state
</p>
<p>3The factor 1&radic;
2
</p>
<p>is due to the normalization of the function, see below.
</p>
<p>4It is clear that the delta function cannot be a function. That it is still denoted as one may be due to
</p>
<p>the often rather nonchalant or easygoing approach of physicists to mathematics. More is given on
</p>
<p>the delta function in Appendix H, Vol. 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.1 Improper Vectors 153
</p>
<p>|k is not normalizable. Such state vectors are commonly called improper states5
(i.e. not square-integrable states), in contrast to the proper states, which are square
</p>
<p>integrable.6
</p>
<p>We want to illustrate the problem by means of a simple example. The Hilbert
</p>
<p>space consists of all functions of x defined on the interval &minus;1 &le; x &le; 1; the scalar
product is given by
</p>
<p>&int; 1
</p>
<p>&minus;1 u
&lowast;(x)v(x)dx . The problem is that the position operator x is
</p>
<p>indeed self-adjoint, but has no eigenvalues (so we cannot measure a local value). For
</p>
<p>if we want to solve the eigenvalue equation xux0(x) = x0ux0(x) for the eigenvalue
x0, we find (x &minus; x0) ux0(x) = 0, so that for x = x0, the trivial solution ux0(x) = 0
is always obtained. The choice ux0(x) =  (x &minus; x0) does not help, because the delta
function is not square integrable and therefore not part of the Hilbert space. One can
</p>
<p>express this fact as mentioned above, by noting that such &lsquo;precise&rsquo; measurements
</p>
<p>are not compatible with the uncertainty principle (and thus can be understood only
</p>
<p>in an idealized formulation).
</p>
<p>We emphasize that for us, the problems with continuous spectra are based not
</p>
<p>primarily on mathematics (e.g. on the fact that such eigenfunctions depart from the
</p>
<p>mathematical structure of Hilbert space), but rather that unphysical states like the
</p>
<p>delta function states appear, which physically are not permissable in the context of
</p>
<p>quantum mechanics.7 Since these unphysical states are not square integrable, the
</p>
<p>previously-developed probability concept of quantum mechanics cannot work with
</p>
<p>them (or at least not readily)&mdash;that is the essential difficulty.
</p>
<p>The basic idea for getting the problem under control is to discretize8 the continuous
</p>
<p>variable and then to let the gaps go to zero. We consider the following manipulations
</p>
<p>for general improper states |, which fulfill the equation (the &lsquo;substitute&rsquo; of the ON
relation for proper vectors)9:
</p>
<p>&lang;
</p>
<p>&prime; | = (&prime; &minus; ). (12.6)
</p>
<p>As indicated in Fig. 12.1, we divide the continuum into fixed intervals of width
</p>
<p> (the process is demonstrated here in one dimension; it works analogously in
</p>
<p>higher dimensions).10 | can be integrated within such an interval:
</p>
<p>5These states are also called Dirac states.
6The strict mathematical theory of continuous spectra is somewhat elaborate (keywords e.g. rigged
</p>
<p>Hilbert space or Gel&rsquo;fand triple). We content ourselves here with a less rigorous and more heuristic
</p>
<p>approach.
7The electron is a point object, but not its wavefunction&mdash;that would be in contradiction to the
</p>
<p>uncertainty principle.
8Discretizations of continuous variables are used also in other areas, e.g. in lattice gauge theories or
</p>
<p>in the numerical treatment of differential equations. Moreover, a discrete space is taken as a basis
</p>
<p>for an alternative derivation/motivation of the SEq (hopping equation; see Appendix J, Vol. 1).
9Whoever wishes may keep in mind 1&radic;
</p>
<p>2
eix instead of | &equiv; | &mdash; this is not quite correct, but
</p>
<p>may be helpful here and is preferable to the auxiliary notion of a column vector.
10Note that we cover the axis completely with non-overlapping intervals .</p>
<p/>
</div>
<div class="page"><p/>
<p>154 12 Continuous Spectra
</p>
<p>Fig. 12.1 Discretization of
</p>
<p>the continuous variable 
</p>
<p>|m, :=
1&radic;

</p>
<p>m+
&int;
</p>
<p>m
</p>
<p>
</p>
<p>&prime;
&rang;
</p>
<p>d&prime;, (12.7)
</p>
<p>where m is an integral multiple of the grid size : m = m with m &isin; Z.
The expression |m, is called an eigendifferential. Eigendifferentials, in con-
trast to continuous functions, are completely &lsquo;well-behaved&rsquo;. In particular, the bras
</p>
<p>belonging to them exist, and they form an ON system:
</p>
<p>n, |m, =
1
</p>
<p>
</p>
<p>n+
&int;
</p>
<p>n
</p>
<p>d |
m+
&int;
</p>
<p>m
</p>
<p>d |
</p>
<p>= 1

</p>
<p>n+
&int;
</p>
<p>n
</p>
<p>d
</p>
<p>m+
&int;
</p>
<p>m
</p>
<p>d | 
</p>
<p>= 1

</p>
<p>n+
&int;
</p>
<p>n
</p>
<p>d
</p>
<p>m+
&int;
</p>
<p>m
</p>
<p>d (&minus; )
</p>
<p>= 1

</p>
<p>m+
&int;
</p>
<p>m
</p>
<p>d nm = nm . (12.8)
</p>
<p>Since we have covered the entire  axis, the states |m, are complete and there-
fore form a CONS. Thus, the eigendifferentials constitute a basis, with which any
</p>
<p>ket | can be represented as
</p>
<p>| =
&sum;
</p>
<p>m
</p>
<p>|m, m, | . (12.9)
</p>
<p>This is an approximation of the continuous system which gets better and better
</p>
<p>with increasingly finer subdivision of the intervals, i.e. with decreasing .11 For
</p>
<p>sufficiently small , we can approximate the eigendifferential (12.7) using the
</p>
<p>mean value theorem for integration (see Appendix D, Vol. 1):
</p>
<p>11A remark on the summation index: the range of values of  runs through all integral multiples of
</p>
<p>the grid size .</p>
<p/>
</div>
<div class="page"><p/>
<p>12.1 Improper Vectors 155
</p>
<p>|m, &asymp;
1&radic;

</p>
<p>
</p>
<p>&micro;
&rang;
</p>
<p> =
&radic;

</p>
<p>
</p>
<p>&micro;
&rang;
</p>
<p>; m &le; &micro; &le; m + 1, (12.10)
</p>
<p>and it follows that
</p>
<p>| &asymp;
&sum;
</p>
<p>&micro;
</p>
<p>
</p>
<p>&micro;
&rang; &lang;
</p>
<p>&micro; |. (12.11)
</p>
<p>We now go to the limiting case12:
</p>
<p>| = lim
&rarr;0
</p>
<p>&sum;
</p>
<p>&micro;
</p>
<p>
</p>
<p>&micro;
&rang; &lang;
</p>
<p>&micro; | =
&int;
</p>
<p>|  | d (12.12)
</p>
<p>or
&int;
</p>
<p>| | d = 1, (12.13)
</p>
<p>so that we can also expand each state in a series of improper vectors. Although this
</p>
<p>process is mathematically not clearly defined, in view of the square integrability, we
</p>
<p>can permit it in the sense that (12.12) is an abbreviation for (12.9) (simply as an
</p>
<p>imagined limiting process).
</p>
<p>The reason for this approach is that it is possibly much easier to work with improper
</p>
<p>vectors than with proper ones. It may well be very useful to describe a quantum object
</p>
<p>using plane waves13 eikx, although they&mdash;being infinitely extended and everywhere
</p>
<p>equal in magnitude&mdash;certainly cannot represent real physical objects. The same is
</p>
<p>true for the delta function. One can imagine a wavefunction concentrated at a point
</p>
<p>and let it tend to a delta function in the mathematical limit&mdash;but it is impossible to
</p>
<p>realize such a state physically.
</p>
<p>In short, delta functions and plane waves are, where appropriate, very practical
</p>
<p>tools for mathematical formulations, but one must not forget that a physical state is
</p>
<p>always represented only by a square-integrable wavefunction.
</p>
<p>With this caveat, we also accept series expansions of improper vectors | &equiv; |.
One speaks in this context of the extended Hilbert space (i.e. the set of proper and
</p>
<p>improper state vectors). Generally, the extended Hilbert space is also denoted by H,
</p>
<p>i.e. by the same symbol as the proper Hilbert space. In summary, this means that we
</p>
<p>can work with improper vectors just as with proper ones, but we have to accept the
</p>
<p>occurrence of functionals such as the delta function. The orthonormality of proper
</p>
<p>and improper vectors is expressed by the equations
</p>
<p>n |n&prime; = nn&prime; and &prime; | = 
(
</p>
<p>&minus; &prime;
)
</p>
<p>(12.14)
</p>
<p>12This is nothing more than the transition from a sum to an integral, well-known from school
</p>
<p>mathematics. One lets the length of the subdivisions tend to zero, so that the upper sum and lower
</p>
<p>sum approach and converge to the integral in the limit, i.e. for infinitesimal interval length. This
</p>
<p>process is reflected in the integral sign
&int;
</p>
<p>&mdash;it is simply a stylized &lsquo;S&rsquo;, for &lsquo;sum&rsquo;.
13We repeat the remark that eikx is actually an oscillation in space. But one always refers in this
</p>
<p>context to a wave, because one keeps the time-dependent factor eit in mind, so to speak.</p>
<p/>
</div>
<div class="page"><p/>
<p>156 12 Continuous Spectra
</p>
<p>and their completeness by14:
</p>
<p>&sum;
</p>
<p>|n n| = 1 and
&int;
</p>
<p>| | d = 1. (12.15)
</p>
<p>The expansion theorem reads
</p>
<p>| =
&sum;
</p>
<p>|n n|  and | =
&int;
</p>
<p>|  | d. (12.16)
</p>
<p>Thus, we can transfer our previous statements for the discrete case to the continuous
</p>
<p>case if we perform the following substitutions:
</p>
<p>n &rarr; ;
&sum;
</p>
<p>&rarr;
&int;
</p>
<p>; nn&prime; &rarr; (&minus; &prime;). (12.17)
</p>
<p>Indeed, we may make life even easier by introducing a new symbol,15 namely
&sum;
&int;
</p>
<p>.
</p>
<p>The expansion theorem is then written as
</p>
<p>| =
&sum;
</p>
<p>&int;
</p>
<p>
</p>
<p> j
&rang; &lang;
</p>
<p> j

</p>
<p>  (12.18)
</p>
<p>with
</p>
<p>&sum;
</p>
<p>&int;
</p>
<p>
</p>
<p> j
&rang; &lang;
</p>
<p> j

</p>
<p>  =
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>&sum;
</p>
<p>j
&int;
</p>
<p>d j
&sum;
</p>
<p>j
</p>
<p>+
&int;
</p>
<p>d j
</p>
<p>for
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>proper
</p>
<p>improper
</p>
<p>proper and improper
</p>
<p>states, (12.19)
</p>
<p>and the completeness relation reads
</p>
<p>&sum;
</p>
<p>&int;
</p>
<p>
</p>
<p> j
&rang; &lang;
</p>
<p> j

</p>
<p> d j = 1. (12.20)
</p>
<p>Similarly, orthonormality can be expressed more compactly with the following new
</p>
<p>symbol (extended or generalized Kronecker symbol:)
</p>
<p> (i, j) =
{
</p>
<p>i j for i, j = discrete
 (i &minus; j) for i, j = continuous, (12.21)
</p>
<p>namely as
</p>
<p>i

</p>
<p> j
&rang;
</p>
<p>= (i, j). (12.22)
</p>
<p>14We recall that the increment of the sum is 1 (so we have n = 1). With this, one can emphasize
the formal similarity between sums and integrals even more, e.g. in the form
</p>
<p>&sum;
</p>
<p>|n n |n = 1.
15There are other notations; e.g. Schwabl uses the symbol S.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Position Representation and Momentum Representation 157
</p>
<p>12.2 Position Representation and Momentum
</p>
<p>Representation
</p>
<p>We will now address the question of how the ket | is related to the wavefunction
 (x).
</p>
<p>First a preliminary remark: We have just considered why and how one can accept
</p>
<p>improper states in quantum mechanics. This allows the following formulation, using
</p>
<p>the example of position measurements considered above: Assume there is a quantum
</p>
<p>object at a point x in space,16 i.e. with regard to its position, it is in the (abstract,
</p>
<p>improper) state |x. The measurement of its position can be symbolized by the posi-
tion operator X , and we then have:
</p>
<p>X |x = x |x . (12.23)
</p>
<p>In words: If we measure the state |x (i.e. if we apply the position operator X to |x),
then we find the number x as the measured value. |x is an improper vector with
</p>
<p>x

</p>
<p>x &prime;
&rang;
</p>
<p>= 
(
</p>
<p>x &minus; x &prime;
)
</p>
<p>(ON) and
</p>
<p>&int;
</p>
<p>|x x | dx = 1 (C). (12.24)
</p>
<p>After this remark, we go to a proper Hilbert space, i.e. a space spanned by a
</p>
<p>CONS of proper vectors n (x). A wavefunction  (x) can be expanded in terms of
</p>
<p>the CONS:
 (x) =
</p>
<p>&sum;
</p>
<p>n
cnn (x) . (12.25)
</p>
<p>For the coefficients, we find:
</p>
<p>cn =
&int;
</p>
<p>&lowast;n (x) (x) dx . (12.26)
</p>
<p>Due to the orthonormality of {n (x)}, we have
&int;
</p>
<p>&lowast; (x) (x) dx =
&sum;
</p>
<p>n
c&lowast;ncn. (12.27)
</p>
<p>We replace the coefficients by using (12.26), and obtain
</p>
<p>&int;
</p>
<p>&lowast; (x) (x) dx =
&sum;
</p>
<p>n
</p>
<p>&int;
</p>
<p>dx &prime; n
(
</p>
<p>x &prime;
)
</p>
<p>
(
</p>
<p>x &prime;
)
</p>
<p>&int;
</p>
<p>dx &lowast;n (x)
&lowast; (x)
</p>
<p>=
&int;
</p>
<p>dx &prime;
&int;
</p>
<p>dx
(
</p>
<p>x &prime;
)
</p>
<p>&lowast; (x)
&sum;
</p>
<p>n
</p>
<p>n
(
</p>
<p>x &prime;
)
</p>
<p>&lowast;n (x). (12.28)
</p>
<p>16It is clear that this is an idealized assumption which is not compatible with the uncertainty
</p>
<p>principle. But we can proceed on this assumption in terms of the above considerations concerning
</p>
<p>the eigendifferential.</p>
<p/>
</div>
<div class="page"><p/>
<p>158 12 Continuous Spectra
</p>
<p>Comparing the right- and left-hand sides, we see that the following equation must
</p>
<p>hold:
&sum;
</p>
<p>n
n
</p>
<p>(
</p>
<p>x &prime;
)
</p>
<p>&lowast;n (x) = 
(
</p>
<p>x &prime; &minus; x
)
</p>
<p>. (12.29)
</p>
<p>We note that the expression on the left-hand side is not a scalar product. Recalling
</p>
<p>our analogy ( &rarr; column vector) introduced earlier, as well as (&dagger; &rarr; row vector),
we can suppose that we have an expression of the form
</p>
<p>&sum;
</p>
<p>|n n|. In fact, we
derived such an expression in the last chapter. Starting with |  and carrying out
exactly the same procedure (but only in the abstract space),17 we obtained there the
</p>
<p>completeness relation in the form (see (12.15))
</p>
<p>&sum;
</p>
<p>n
|n n| = 1. (12.30)
</p>
<p>In other words, the (12.29) and (12.30) describe the same facts (completeness of
</p>
<p>the basis set), just with different notations. We see this more clearly if we transform
</p>
<p>(12.30) using (12.24):
</p>
<p>x |
(
</p>
<p>&sum;
</p>
<p>n
</p>
<p>|n n|
)
</p>
<p>
</p>
<p>x &prime;
&rang;
</p>
<p>=
&sum;
</p>
<p>n
</p>
<p>x | n n

</p>
<p>x &prime;
&rang;
</p>
<p>= x | x &prime;
&rang;
</p>
<p>= 
(
</p>
<p>x &minus; x &prime;
)
</p>
<p>. (12.31)
</p>
<p>The comparison of this equation with (12.29) suggests the following identification:
</p>
<p>n (x) = x | n (12.32)
</p>
<p>n (x) is called the position representation of the ket |n. Formally, it is a scalar
product of two abstract vectors, whose result is&mdash;as always&mdash;a scalar, and to which
</p>
<p>we can apply the (now) well-known rules of calculation. For example, we have
</p>
<p>n| x = x | n&dagger; = x | n&lowast; = &lowast;n (x). (12.33)
</p>
<p>So far so good. We now can &lsquo;play around&rsquo; with this notation a bit. One question
</p>
<p>might be: If x |  =  (x) is the position representation of |, then what is the
position representation of
</p>
<p>
</p>
<p>x &prime;
&rang;
</p>
<p>? This state is characterized by the fact that a position
</p>
<p>measurement returns the result x &prime;&mdash;the quantum object is at x &prime;, and only there can we
</p>
<p>17We repeat this derivation briefly by writing the first line of (12.28) with bra-kets (remember:
&int;
</p>
<p>f &lowast;gdx =  f |g):
&int;
</p>
<p>&lowast; (x) (x) dx =  | =
&sum;
</p>
<p>n
</p>
<p>n |  |n
</p>
<p>=
&sum;
</p>
<p>n
</p>
<p> |n n | = |
(
</p>
<p>&sum;
</p>
<p>n
</p>
<p>|n n |
)
</p>
<p>| .</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Position Representation and Momentum Representation 159
</p>
<p>find it.18 In fact, we have already answered the question with (12.24): The position
</p>
<p>representation of

</p>
<p>x &prime;
&rang;
</p>
<p>is x

</p>
<p>x &prime;
&rang;
</p>
<p>= 
(
</p>
<p>x &minus; x &prime;
)
</p>
<p>.
</p>
<p>Another question: We have worked with states which are characterized not by a
</p>
<p>sharp position, but by a sharp momentum (or k = p/). In the abstract notation, this
is the ket |k, whose position representation we already know&mdash;it is a plane wave19
(the prefactor is due to the normalization):
</p>
<p>x | k = 1&radic;
2
</p>
<p>eikx . (12.34)
</p>
<p>By taking the adjoint, we obtain immediately the momentum representation of a state
</p>
<p>with a sharply-defined position:
</p>
<p>k| x = 1&radic;
2
</p>
<p>e&minus;ikx , (12.35)
</p>
<p>and therefore
</p>
<p>k

</p>
<p>k &prime;
&rang;
</p>
<p>= 
(
</p>
<p>k &minus; k &prime;
)
</p>
<p>(ON) and
</p>
<p>&int;
</p>
<p>dk |k k| = 1 (C). (12.36)
</p>
<p>In short, the improper vectors |k also form a CONS.
We can now write the ket | in both the position and the momentum represen-
</p>
<p>tations, namely as20
</p>
<p>x | = (x): position representation
k | = (k): momentum representation. (12.37)
</p>
<p>How are these two representations related? We multiply by 1 and obtain
</p>
<p>x | = x |
&int;
</p>
<p>dk |k k | =
&int;
</p>
<p>dk x |k k |
</p>
<p>k | = k|
&int;
</p>
<p>dx |x x | =
&int;
</p>
<p>dx k |x x | , (12.38)
</p>
<p>18We note again that this is an idealized formulation.
19Again, the above statement on oscillations and waves applies.
20Because these are two representations of the same ket |, sometimes the same symbol is used
for both representations, i.e. (x) and (k), although these two functions are not the same (as
</p>
<p>mapping, that is, in the sense that one does not obtain (k) by simply replacing x by k in (x)).
</p>
<p>What is precisely meant has to be inferred from the context. To avoid confusion, we use the notation
</p>
<p>(k).</p>
<p/>
</div>
<div class="page"><p/>
<p>160 12 Continuous Spectra
</p>
<p>or in the &lsquo;usual&rsquo; notation:
</p>
<p>(x) = 1&radic;
2
</p>
<p>&int;
</p>
<p>dk eikx(k)
</p>
<p>(k) = 1&radic;
2
</p>
<p>&int;
</p>
<p>dx e&minus;ikx(x). (12.39)
</p>
<p>We see that the position and momentum representations of a ket are Fourier trans-
</p>
<p>forms of each other.21
</p>
<p>Finally the question arises as to how to derive equations and operators in the two
</p>
<p>representations. We consider an abstract eigenvalue equation of the form
</p>
<p>A | = a | (12.40)
</p>
<p>and wish to write it in the position representation. For this purpose, we multiply first
</p>
<p>with a bra x |:
x | A | = a x | (12.41)
</p>
<p>and then multiply the left-hand side by the identity:
</p>
<p>x | A
&int;
</p>
<p>dx &prime;

</p>
<p>x &prime;
&rang; &lang;
</p>
<p>x &prime; | = a x | &rarr;
&int;
</p>
<p>dx &prime; x | A

</p>
<p>x &prime;
&rang; &lang;
</p>
<p>x &prime; | = a x | .
(12.42)
</p>
<p>Now it depends on the matrix element x | A

</p>
<p>x &prime;
&rang;
</p>
<p>how to continue. A significant
</p>
<p>simplification of the equation is obtained only if the following applies:
</p>
<p>x | A

</p>
<p>x &prime;
&rang;
</p>
<p>= (x &minus; x &prime;)A(x). (12.43)
</p>
<p>In this case, one says that A is diagonal in the position representation, or that the
</p>
<p>operator A is a local operator.22 With this understanding, (12.42) apparently may be
</p>
<p>written as
</p>
<p>&int;
</p>
<p>dx &prime;(x &minus; x &prime;)A (x)
&lang;
</p>
<p>x &prime; | = A (x) x | = a x | , (12.44)
</p>
<p>or, in the familiar notation with a position variable,
</p>
<p>A(x) = a(x), (12.45)
</p>
<p>21For an introduction to Fourier transforms, see Appendix H, Vol. 1.
22Quasi-local operators are defined via the derivative of the delta function:
</p>
<p>A (x, y) = a (x)  (x &minus; y) local operator
B (x, y) = b (x) &prime; (x &minus; y) quasi-local operator</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Position Representation and Momentum Representation 161
</p>
<p>where A now stands for the position representation of the operator. We point out
</p>
<p>again that the operators A in (12.40) and (12.45) are not identical. Though one
</p>
<p>usually writes the same symbol, they are quite different mathematical objects.
</p>
<p>Finally, an example: We want to derive the position representation of the momen-
</p>
<p>tum operator pop (which we know already, of course). For better readability, we
</p>
<p>indicate the operator for the moment by an index op.
</p>
<p>We start with the abstract eigenvalue equation
</p>
<p>pop |k = k |k . (12.46)
</p>
<p>For the following considerations, we know nothing about the momentum operator
</p>
<p>apart from this eigenvalue equation. |k is a state of well-defined momentum (note
p = k), and k is its eigenvalue or measured value. Multiplication with x | and
insertion of the identity leads to:
</p>
<p>&int;
</p>
<p>x | pop

</p>
<p>x &prime;
&rang; &lang;
</p>
<p>x &prime;

</p>
<p> k dx &prime; = k
&int;
</p>
<p>x | x &prime;
&rang; &lang;
</p>
<p>x &prime;

</p>
<p> k dx &prime; = k x | k = k&radic;
2
</p>
<p>eikx .
</p>
<p>(12.47)
</p>
<p>It follows that:
</p>
<p>&int;
</p>
<p>x | pop

</p>
<p>x &prime;
&rang;
</p>
<p>eikx
&prime;
dx &prime; = k
</p>
<p>&int;
</p>
<p>(x &minus; x &prime;)eikx &prime;dx &prime; = 
i
</p>
<p>&int;
</p>
<p>(x &minus; x &prime;) &part;
&part;x &prime;
</p>
<p>eikx
&prime;
dx &prime;.
</p>
<p>(12.48)
</p>
<p>Comparing the left- and right-hand sides yields
</p>
<p>x | pop

</p>
<p>x &prime;
&rang;
</p>
<p>= (x &minus; x &prime;)
i
</p>
<p>&part;
</p>
<p>&part;x
. (12.49)
</p>
<p>Thus, the momentum operator is diagonal in the position representation, and takes
</p>
<p>the well-known form&mdash;as was to be expected.
</p>
<p>An example of a non-diagonal operator in the position representation (projection
</p>
<p>operator) can be found in the exercises. We have thus filled in a gap as promised in
</p>
<p>previous chapters.
</p>
<p>12.3 Conclusions
</p>
<p>We started in Chaps. 1 and 2 with two (at first sight completely different) descriptions
</p>
<p>of states, namely as position-dependent wavefunctions  (r) (analytical approach,
</p>
<p>odd chapters) on the one hand, and as kets | (algebraic approach, even chapters),
with their representations as column vectors, on the other hand. After travelling long
</p>
<p>route, which encompassed necessarily a lot of other material (in large part, the path
</p>
<p>was also our destination),23 we have combined the two approaches in this chapter
</p>
<p>23&ldquo;Caminante no hay camino, se hace camino al andar&hellip;&rdquo; Antonio Machado, Spanish poet.</p>
<p/>
</div>
<div class="page"><p/>
<p>162 12 Continuous Spectra
</p>
<p>Fig. 12.2 The same physical
</p>
<p>situation permits various
</p>
<p>representations
</p>
<p>abstract representation
</p>
<p>yet another representation
</p>
<p>another  representation
</p>
<p>coordinate representation
</p>
<p>momentum representation
</p>
<p>position representation
</p>
<p>and have seen that in the end, they are simply different formulations of the same
</p>
<p>facts; this also allows for other representations, cf. Fig. 12.2. We will take advantage
</p>
<p>of this circumstance in the following and will switch back and forth between the two
</p>
<p>formulations as proves be convenient for us&mdash;wave or matrix mechanics, or the
</p>
<p>abstract notation.
</p>
<p>However, it is clear that wave mechanics cannot describe various properties, e.g.
</p>
<p>the spin (or, beyond the scope of this book, strangeness, charm, etc.). In other words,
</p>
<p>for any position-dependent wavefunction, there is a ket, but the converse is not
</p>
<p>true. But this is now no longer a problem, as we have extended wave mechanics to
</p>
<p>a general formalism in this chapter.
</p>
<p>12.4 Exercises
</p>
<p>1. Given an eigenstate |k of the momentum operator; how is this state described
in the position representation?
</p>
<p>2. Show by using x | k = 1&radic;
2
</p>
<p>eikx that the improper vectors |k form a CONS.
3. Given an improper vector |, what is the associated eigendifferential
</p>
<p>
</p>
<p>,
&rang;
</p>
<p>?
</p>
<p>4. Given the state |k with the sharply-defined momentum k; we have x | k =
1&radic;
2
</p>
<p>eikx .
</p>
<p>(a) What is the (abstract) eigendifferential?
</p>
<p>(b) How is the eigendifferential expressed in the position representation?
</p>
<p>(c) Show that the eigendifferentials of (b) are orthonormal.
</p>
<p>5. Given the SEq in the abstract formulation
</p>
<p>i
d
</p>
<p>dt
| = H |, (12.50)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.4 Exercises 163
</p>
<p>(a) Formulate this equation in the position representation and in the momentum
</p>
<p>representation.
</p>
<p>(b) How can one calculate the matrix element k| H

</p>
<p>k &prime;
&rang;
</p>
<p>, if H is known in the
</p>
<p>position representation?
</p>
<p>6. Given a CONS {|n}; formulate the projection operator
</p>
<p>P1 = |1 1| (12.51)
</p>
<p>in the position representation.
</p>
<p>7. A and B are self-adjoint operators with [A, B] = i, and |a is an eigenvector
of A with the eigenvalue a. Then we have
</p>
<p>a |[A, B]| a = a |AB &minus; B A| a = (a &minus; a) a |B| a = 0. (12.52)
</p>
<p>On the other hand, we also have:
</p>
<p>a |[A, B]| a = a |i| a = i = 0. (12.53)
</p>
<p>Question: where is the flaw in this argument?</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 13
</p>
<p>Operators
</p>
<p>In this chapter, we assemble some basic properties of the most important types of operators
</p>
<p>in quantum mechanics.
</p>
<p>As we have seen, the states of quantum mechanics are defined on an (extended)
</p>
<p>Hilbert space H. Changes of these states are caused by operators: This can be, for
</p>
<p>example, the time evolution of the system itself, or the filtering of certain states out
</p>
<p>of a general state. We have already met up with the zoo of operators of quantum
</p>
<p>mechanics (Hermitian, unitary and projection operators). But given the central role
</p>
<p>of operators in quantum mechanics, we want to discuss in this chapter some of their
</p>
<p>properties in more detail, taking the abstract formulation as a basis.1
</p>
<p>With one exception, the operators considered in this book are linear. An operator
</p>
<p>A is called linear if for any two states and any two numbers , &isin; C, it holds that:
</p>
<p>A ( | +  |) = A | + A |. (13.1)
</p>
<p>For the exception, namely an antilinear operator B, it holds that:
</p>
<p>B ( | +  |) = &lowast;B | + &lowast;B |. (13.2)
</p>
<p>An anti-linear map is for example the complex conjugation, and thus also the
</p>
<p>scalar product with respect to the first component, since then a |b = &lowast; a |b.
Furthermore, the time-reversal operator is anti-linear (see Chap. 21, Vol. 2).
</p>
<p>An operator is called bounded if there is a constant C which does not depend on
</p>
<p>the states | &isin; H, so that for all states, it holds that
</p>
<p>A | &le; C |. (13.3)
</p>
<p>1Further material on operators is found in Appendix I, Vol. 1.
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4_13
</p>
<p>165</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_13&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_13&amp;domain=pdf</a></div>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4_13">https://doi.org/10.1007/978-3-030-00464-4_13</a></div>
</div>
<div class="page"><p/>
<p>166 13 Operators
</p>
<p>The domain of definition (or briefly the domain) of an operator A is the set of all
</p>
<p>vectors | &isin; H, such that A | also belongs to H. One can show that the domain
of definition of A is the whole Hilbert space, if and only if A is bounded.
</p>
<p>If two operators A and B commute, one says that they are simultaneously mea-
</p>
<p>surable. However, this notion is not defined by any sort of time consideration, but
</p>
<p>is simply a short form for the fact that the measurement result is independent of the
</p>
<p>chronological order in which we measure A and B.
</p>
<p>13.1 Hermitian Operators, Observables
</p>
<p>We can distinguish three levels: First, there is the measurable physical variable Aphys,
</p>
<p>which is modelled in quantum mechanics by a Hermitian operator Aop = A&dagger;op. This
abstract operator can be expressed, if necessary, in a concrete representation as Arepr.
</p>
<p>As an example, we consider the angular momentum. The measurable physical vari-
</p>
<p>able is lphys, the corresponding operator lop = r &times; p, and in the position represen-
tation, it is lrepr = i r &times; &nabla;, as is well known. As mentioned above, often the same
notation is used for all three objects (in the example l), since usually the context
</p>
<p>makes clear what is meant. We will proceed essentially in this way.
</p>
<p>We start from the eigenvalue equation (the spectrum is assumed to be discrete and
</p>
<p>not degenerate):
</p>
<p>A |n = an |n; n = 1, 2, . . . ; A = A&dagger;. (13.4)
</p>
<p>The possible result of a measurement of the measurable physical variable A is
</p>
<p>one of the eigenvalues of the operator A. Because of the importance of this fact there
</p>
<p>is a special name, namely observable.2 We mean by this a Hermitian operator that
</p>
<p>represents a consistently measurable physical quantity. Some remarks on the concept
</p>
<p>&lsquo;observable&rsquo; are found in Appendix I, Vol. 1.
</p>
<p>We point out that we use &lsquo;self-adjoint&rsquo; and &lsquo;Hermitian&rsquo; as equivalents, which
</p>
<p>applies for all of the systems we consider. In fact, under certain conditions the two
</p>
<p>terms are not identical; in infinite-dimensional vector spaces, Hermiticity does not
</p>
<p>necessarily imply self-adjoint. More on this topic may be found in Appendix I, Vol. 1.
</p>
<p>Two remarks are appropriate here:
</p>
<p>1. An operator A is called anti-Hermitian if A&dagger; = &minus;A. Each operator C can be
broken into a Hermitian and an anti-Hermitian part3:
</p>
<p>2The term is not defined in the same way everywhere, and is sometimes rather avoided. The reason
</p>
<p>for this rejection stems in part from the fact that the name &lsquo;observable&rsquo; suggests that without an
</p>
<p>observer (perhaps even a human), physical quantities cannot become real. We explicitly point out
</p>
<p>that for us, the term observable does not imply this problem, but is simply a technical term in the
</p>
<p>above sense.
3Much in the way that each function can be decomposed into a mirror-symmetric and a point-
</p>
<p>symmetric part.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.1 Hermitian Operators, Observables 167
</p>
<p>C = CHermitian + Canti-Hermitian =
C + C&dagger;
</p>
<p>2
+ C &minus; C
</p>
<p>&dagger;
</p>
<p>2
. (13.5)
</p>
<p>2. The product of an operator with its adjoint is a Hermitian operator:
(
</p>
<p>AA&dagger;
)&dagger; =
</p>
<p>A&dagger;&dagger; A&dagger; = AA&dagger;. In addition, AA&dagger; is a positive operator; that is, for all |, it holds
that | AA&dagger; | &ge; 0. This follows from the fact that | AA&dagger; | is the square
of a norm, since we have | AA&dagger; | =
</p>
<p>
</p>
<p>A&dagger; |

</p>
<p>
</p>
<p>2
.4
</p>
<p>Finally, a word about the symmetrization discussed in Chap. 3. For example, in clas-
</p>
<p>sical mechanics we have xpx = px x , but for the corresponding quantum-mechanical
quantities, xpx 
= px x . For this reason, we introduced the symmetrized form 12
(xpx + px x). We can now deliver the reasoning: Given two Hermitian operators
A and B with [A, B] 
= 0. The product AB is not Hermitian (so it cannot corre-
spond to a measurable variable), for (AB)&dagger; = B A 
= AB. But we can construct a
Hermitian operator by taking the symmetrized form C = 1
</p>
<p>2
(AB + B A), because it
</p>
<p>is C&dagger; = 1
2
(AB + B A)&dagger; = C . According to the above considerations, it cannot be
</p>
<p>guaranteed offhand that this symmetrized operator represents an observable.
</p>
<p>13.1.1 Three Important Properties of Hermitian Operators
</p>
<p>In the following, we want to prove three important properties of Hermitian operators
</p>
<p>using the bra-ket formalism. We know already two of them, namely that the eigen-
</p>
<p>values are real and that the eigenfunctions are pairwise orthogonal (we assume that
</p>
<p>the spectrum is not degenerate). In addition, we will show that commuting Hermitian
</p>
<p>operators have a common CONS.
</p>
<p>13.1.1.1 Eigenvalues Are Real
</p>
<p>Since measurements of physical quantities always mean measurements of real num-
</p>
<p>bers (lengths, angles, arc degrees etc.), we require that the eigenvalues of the mod-
</p>
<p>elling operators also be real. This is indeed the case for Hermitian operators, as we
</p>
<p>now show (again).
</p>
<p>The operator A is Hermitian, A&dagger; = A; its eigenvalue equation is
</p>
<p>A |n = an |n; n = 1, 2, . . . (13.6)
</p>
<p>with eigenvectors |n. We multiply from the left by a bra:
</p>
<p>n |A|n = an n |n  = an. (13.7)
</p>
<p>4We note that the term &lsquo;positive operator&rsquo; is common but not negative or positive-semidefinite would
</p>
<p>be more correct. However, one can make the distinction between positive (&ge;0) and strictly positive
(&gt;0).</p>
<p/>
</div>
<div class="page"><p/>
<p>168 13 Operators
</p>
<p>We then have:
</p>
<p>a&dagger;n = a&lowast;n = n |A|n&dagger; =
&lang;
</p>
<p>n

</p>
<p>A&dagger;

</p>
<p>n
&rang;
</p>
<p>= n |A|n = an. (13.8)
</p>
<p>Thus, the eigenvalues of a Hermitian operator are real.
</p>
<p>13.1.1.2 Eigenvectors Are Orthogonal
</p>
<p>Next, we want to show that we have m |n  = 0 for n 
= m, provided that the
spectrum is not degenerate (degenerate spectra are discussed further below). We start
</p>
<p>with
</p>
<p>A |n = an |n and m | A = am m |, (13.9)
</p>
<p>since A is Hermitian and hence has real eigenvalues. It follows that
</p>
<p>m |A|n = an m |n  and m |A|n = am m |n . (13.10)
</p>
<p>Subtracting the two equations leads to
</p>
<p>(am &minus; an) m |n  = 0. (13.11)
</p>
<p>Therefore, it must hold (since we have assumed non-degeneracy) that m | n = 0
for n 
= m. If we take into account also the normalization of the eigenfunctions, we
find, as expected:
</p>
<p>m |n  = nm . (13.12)
</p>
<p>Thus, the eigenfunctions of a (nondegenerate) Hermitian operator always form an
</p>
<p>orthonormal system.
</p>
<p>13.1.1.3 Commuting Hermitian Operators Have a Common CONS
</p>
<p>Given two Hermitian operators A and B (with nondegenerate spectra). They commute
</p>
<p>if and only if they have a common CONS of eigenvectors. To prove the claim, two
</p>
<p>steps are necessary: Step 1: [A, B] = 0 &rarr; common system; Step 2: common system
&rarr; [A, B] = 0.
Step 1. We start with
</p>
<p>A |i  = ai |i  (13.13)
</p>
<p>where {|i } is a CONS. It follows that
</p>
<p>B A |i  =
{
</p>
<p>Bai |i  = ai B |i 
AB |i , since [A, B] = 0
</p>
<p>(13.14)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.1 Hermitian Operators, Observables 169
</p>
<p>or, in summary,
</p>
<p>AB |i  = ai B |i . (13.15)
</p>
<p>Comparing this equation with (13.13) shows (because in both cases the same
</p>
<p>eigenvalue ai appears) that B |i  must be a multiple of the eigenfunction
|i :
</p>
<p>B |i  &sim; |i . (13.16)
</p>
<p>We call the proportionality constant bi . It follows that
</p>
<p>B |i  = bi |i , (13.17)
</p>
<p>i.e. the operator B has the CONS {i }, also. But as the roles of A and B can
be interchanged in this argument; it follows that both operators have exactly
</p>
<p>the same CONS. Thus step 1 is completed.
</p>
<p>Step 2. On condition that a common CONS exists, it will be shown that the com-
</p>
<p>mutator [A, B] vanishes. Thus we assume:
</p>
<p>A |i  = ai |i  and B |i  = bi |i . (13.18)
</p>
<p>It follows that
</p>
<p>B A |i  = ai B |i  = ai bi |i  and AB |i  = bi A |i  = bi ai |i .
(13.19)
</p>
<p>The right-hand sides of these equations are equal, hence also the left-hand
</p>
<p>sides must be equal, so we find
</p>
<p>(AB &minus; B A) |i  = [A, B] |i  = 0. (13.20)
</p>
<p>This equation does not tell us that the commutator vanishes, but only that its
</p>
<p>application to an eigenvector gives zero. On the other hand, we know that
</p>
<p>the system {i } is complete, meaning that any vector can be represented as
</p>
<p>| =
&sum;
</p>
<p>i
</p>
<p>di |i . (13.21)
</p>
<p>With this, we have for any vector |
</p>
<p>[A, B] | =
&sum;
</p>
<p>i
</p>
<p>di [A, B] |i  = 0 (13.22)
</p>
<p>and hence the statement [A, B] = 0 is true in the entire Hilbert space.</p>
<p/>
</div>
<div class="page"><p/>
<p>170 13 Operators
</p>
<p>Commuting observables thus have a common system of eigenvectors. A remark:
</p>
<p>time-independent observables which commute with the Hamiltonian are conserved
</p>
<p>quantities, see Chap. 9.
</p>
<p>13.1.2 Uncertainty Relations
</p>
<p>13.1.2.1 For Two Hermitian Operators
</p>
<p>In Chap. 9, we defined the standard deviation or uncertainty A by
</p>
<p>(A) =
&radic;
</p>
<p>&lang;
</p>
<p>A2
&rang;
</p>
<p>&minus; A2. (13.23)
</p>
<p>Starting from this, one can derive the uncertainty relation (or uncertainty principle)
</p>
<p>for Hermitian operators A and B. This is carried out in Appendix I, Vol. 1; we note
</p>
<p>here only the result:
</p>
<p>A &middot;B &ge; 1
2
|[A, B]| . (13.24)
</p>
<p>This general uncertainty relation for two Hermitian operators is particularly pop-
</p>
<p>ular for the pair x and px . Because of [x, px ] = i, we have
</p>
<p>x &middot;px &ge;

</p>
<p>2
. (13.25)
</p>
<p>This is sketched in Fig. 13.1.5
</p>
<p>13.1.2.2 When Does the Uncertainty Relation Hold
</p>
<p>and What Does It Mean?
</p>
<p>We emphasize that the derivation of the uncertainty relation assumes ideal (error-free)
</p>
<p>measuring instruments. In fact, the experimental errors of measuring instruments in
</p>
<p>real experiments are usually much larger than the quantum uncertainties. Accord-
</p>
<p>ingly, the uncertainty relation is not a statement about the accuracy of measuring
</p>
<p>instruments, but rather the description of a pure quantum effect.
</p>
<p>We have seen in Chap. 9 that expressions such asx are state-dependent averaging
</p>
<p>processes. If one does not take this into account, one can deduce everything possible
</p>
<p>and impossible, and this is also true for the uncertainty relation. The notation (x)
or x is less common, but its use would prevent this kind of misunderstanding.
</p>
<p>5One can show that violating the uncertainty principle implies that it is also possible to violate
</p>
<p>the second law of thermodynamics; see Esther H&auml;nggi &amp; Stephanie Wehner, &lsquo;A violation of the</p>
<p/>
</div>
<div class="page"><p/>
<p>13.1 Hermitian Operators, Observables 171
</p>
<p>Fig. 13.1 Different
</p>
<p>realizations of the
</p>
<p>uncertainty relation (13.25).
</p>
<p>It is like pressing a
</p>
<p>balloon&mdash;pressed in one
</p>
<p>direction,the balloon evades
</p>
<p>the pressure and expands in
</p>
<p>the other direction
</p>
<p> p
x
</p>
<p> x
</p>
<p>In addition, the uncertainty relation (13.24) makes sense only for those states
</p>
<p>which are in the domain of definition of A and B as well as in those of the products of
</p>
<p>the operators occurring in the derivation. For all other states, the uncertainty principle
</p>
<p>is irrelevant. For example, there are functions which are in the Hilbert space of
</p>
<p>square integrable functions, but outside the domains of definition of the (unbounded)
</p>
<p>operators x and p. For these functions, one cannot establish the inequality (13.25).
</p>
<p>Examples can be found in the exercises and in Appendix I, Vol. 1. Operator equations,
</p>
<p>and generally statements about operators, do not apply to all states, but only to those
</p>
<p>that are in the domain of definition of the operators.6
</p>
<p>Regarding the meaning of the uncertainty relation (13.24), we encounter a typical
</p>
<p>situation in quantum mechanics. The theoretical formulations and derivations are
</p>
<p>quite &lsquo;straightforward&rsquo; and uncontroversial. Problems occur only when one asks
</p>
<p>what all this &lsquo;really&rsquo; means. We illustrate the situation with the example of the two
</p>
<p>positions that we have already briefly mentioned in Chap. 2.
</p>
<p>1. The first position assumes that the relation (13.24) applies only to an ensemble.
</p>
<p>It is, therefore, about the statistical distribution in the measurement results, if
</p>
<p>one measures both A and B in a large number of identically-prepared systems
</p>
<p>(i.e. in each system, either A or B). For [A, B] 
= 0, the measurements are indeed
incompatible, but because they are carried out on different systems or ensemble
</p>
<p>members, these measurements can in no way mutually interfere. In this case,
</p>
<p>the uncertainty principle has nothing to do with the possibility of performing
</p>
<p>simultaneous measurements of two quantities. It can be interpreted at best as a
</p>
<p>fundamental limitation in preparing a state (or the corresponding ensemble) as
</p>
<p>accurately as possible. In this case, the standard deviationA is a straightforward
</p>
<p>concept.
</p>
<p>2. The second position assumes that the relation (13.24) applies to single events. Of
</p>
<p>course, A has nothing to do in this case with a statistical distribution as in the
</p>
<p>ensemble position just mentioned. As we have seen and will develop further, a
</p>
<p>central position of this view of quantum mechanics is that it does not make sense
</p>
<p>uncertainty principle implies a violation of the second law of thermodynamics&rsquo;, Nature Communi-
</p>
<p>cations 4, Article number 1670 (2013), https://doi.org/10.1038/ncomms2665.
6It is not just about the comb, so to speak, but also about the hair that is combed.</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1038/ncomms2665.">https://doi.org/10.1038/ncomms2665.</a></div>
</div>
<div class="page"><p/>
<p>172 13 Operators
</p>
<p>for a typical quantum state to say that A has any value at all (which does not mean
</p>
<p>that A has a value which we do not know). Under this assumption, A can be
</p>
<p>interpreted, as we have seen in Chap. 9, as a numerical measure of the extent to
</p>
<p>which the property A is not owned by the system, since e.g.  A = 0 means that
| is an eigenstate of A. The same applies to  B. The uncertainty relation is
then a statement as to what extent a system can have or cannot have the properties
</p>
<p>A and B at the same time.
</p>
<p>In addition to these two positions or interpretations of quantum mechanics, there
</p>
<p>are several more, as we shall see in later chapters.7 Which of these is the &lsquo;correct&rsquo;
</p>
<p>one is not (yet) clear at present. We can just say at this point that the formalism of
</p>
<p>quantum mechanics is unique, but its interpretation is anything but uncontroversial.
</p>
<p>In Chap. 14, and Chap. 27, Vol. 2, and especially in Chap. 28, Vol. 2, we will address
</p>
<p>these questions again.
</p>
<p>13.1.2.3 Uncertainty Relation for Time and Energy
</p>
<p>In the uncertainty relation (13.24), we cannot insert the time directly into A or
</p>
<p>B, since it is not an operator in quantum mechanics, but a simple parameter (one
</p>
<p>cannot say e.g. that &ldquo;a quantum object has a well-defined time&rdquo;). Nevertheless, one
</p>
<p>can formulate a statement that links time and energy. For this we consider a not
</p>
<p>explicitly time-dependent Hermitian operator A. As described in Sect. 9.3, we have
</p>
<p>i
d
</p>
<p>dt
A = [A, H ] . (13.26)
</p>
<p>Together with the uncertainty relation, it follows that
</p>
<p>A &middot;H &ge; 1
2
|[A, H ]| = 
</p>
<p>2
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>d
</p>
<p>dt
A
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>. (13.27)
</p>
<p>We define a time interval  by
</p>
<p> = A

</p>
<p>d
dt
A
</p>
<p>
</p>
<p>
</p>
<p>. (13.28)
</p>
<p>This is a measure of the time during which the value of A changes by A. For
</p>
<p>example, we have for a conserved quantity d
dt
A = 0, hence  = &infin;. With these
</p>
<p>concepts, we find
</p>
<p>7Quite apart from the literary process, such as with David Foster Wallace in Infinite jest: &ldquo;The mind
</p>
<p>says, a box-and-forest-meadows-mind can move with quantum-speed and be anytime anywhere
</p>
<p>and hear in symphonic sum of the thoughts of the living ... The mind says: It does not really matter
</p>
<p>whether Gately knows what the term quanta means. By and large, it says there are ghosts ... in a
</p>
<p>completely different Heisenberg dimension of exchange rates and time courses.&rdquo;</p>
<p/>
</div>
<div class="page"><p/>
<p>13.1 Hermitian Operators, Observables 173
</p>
<p>H &middot; &ge; 
2
</p>
<p>(13.29)
</p>
<p>which often is written as
</p>
<p>E &middot;t  
2
</p>
<p>(13.30)
</p>
<p>This can be interpreted as a correlation between lifetime and variation in energy.
</p>
<p>13.1.3 Degenerate Spectra
</p>
<p>We have essentially confined ourselves to observables with nondegenerate spectra.
</p>
<p>In the discrete case, the eigenvalue equation is given by
</p>
<p>A |n = an |n, n = 1, 2, . . . (13.31)
</p>
<p>Since A is an observable, the set {|n} of the eigenvectors is a basis for H, in terms
of which any state can be represented by an expansion:
</p>
<p>| =
&sum;
</p>
<p>n
</p>
<p>cn |n . (13.32)
</p>
<p>We can express this also by saying that every eigenvector spans a one-dimensional
</p>
<p>subspace.
</p>
<p>Let us now turn to the case of a degenerate discrete spectrum. The eigenvalue
</p>
<p>equation is
</p>
<p>A

</p>
<p>n,r
&rang;
</p>
<p>= an

</p>
<p>n,r
&rang;
</p>
<p>, n = 1, 2, . . . ; r = 1, 2, . . . , gn . (13.33)
</p>
<p>Here, gn is the degree of degeneracy of the eigenvalue an; for gn = 1, an is
not degenerate. The
</p>
<p>
</p>
<p>n,r
&rang;
</p>
<p>are gn linearly independent eigenvectors (for given n);
</p>
<p>they span the subspace (eigenspace) Hn of dimension gn of the eigenvalue an . The
</p>
<p>eigenvectors

</p>
<p>n,r
&rang;
</p>
<p>with the same index n are not necessarily orthogonal to each
</p>
<p>other (we can always assume, however, that they are normalized), but with the usual
</p>
<p>methods of orthogonalization one can construct an orthogonal system in terms of
</p>
<p>them. The subspaces of different indices n are mutually orthogonal, Hn &perp; Hm for
n 
= m.8
</p>
<p>Finally, the expansion of a state is given by:
</p>
<p>| =
&sum;
</p>
<p>n
</p>
<p>gn
&sum;
</p>
<p>r=1
cn,r
</p>
<p>
</p>
<p>n,r
&rang;
</p>
<p>, n = 1, 2, . . . ; r = 1, 2, . . . , gn (13.34)
</p>
<p>8Two subspaces Hn and Hm are mutually orthogonal if any vector in Hn is orthogonal to any vector
</p>
<p>in Hm .</p>
<p/>
</div>
<div class="page"><p/>
<p>174 13 Operators
</p>
<p>In the case of a continuous spectrum, we obtain the corresponding statements and
</p>
<p>formulations by the usual replacements: discrete index &rarr; continuous variable; sum-
mation &rarr; integration; Kronecker symbol &rarr; Delta function; cf. Chap. 12. See also
the chapter &lsquo;Discrete-continuous&rsquo; in Appendix T, Vol. 1.
</p>
<p>13.2 Unitary Operators
</p>
<p>Before we repeat the definition of a unitary operator, we consider the definition of
</p>
<p>an inverse operator. Thus, we assume that an operator A with A f = g exists. If the
inverse of this mapping also exists, f = A&minus;1g, then A&minus;1 is the inverse operator of A.
We have AA&minus;1 = A&minus;1 A. A unitary operator U is defined by9
</p>
<p>U &dagger; = U&minus;1 &larr;&rarr; U &dagger;U = UU &dagger; = 1. (13.35)
</p>
<p>For the eigenvalues of a unitary operator, with U |u = u |u and u|U &dagger; = u&lowast; u|,
the equation |u|2 = 1 applies. Hence, the eigenvalues of a unitary operator are on
the unit circle.
</p>
<p>13.2.1 Unitary Transformations
</p>
<p>With unitary operators we can define unitary transformations of states and operators.
</p>
<p>Common notations are
</p>
<p>U | =

</p>
<p> &prime;
&rang;
</p>
<p>and U AU &dagger; = A&prime;. (13.36)
</p>
<p>The interesting thing about unitary transformations is that they leave important
</p>
<p>properties and quantities unchanged, namely the lengths of vectors and the &lsquo;angle&rsquo;
</p>
<p>between them, and thus scalar products, as well as matrix elements and eigenvalues
</p>
<p>(see exercises). In this respect, a unitary transformation is an analogue of the rotation
</p>
<p>in elementary vector calculus. We can visualize it as a transition from one basis to
</p>
<p>another basis or coordinate system. Suppose that there two CONS {|n} and {|n}.
Then we have
</p>
<p>|n =
&sum;
</p>
<p>m
</p>
<p>|m m | n =
&sum;
</p>
<p>m
</p>
<p>Umn |m, (13.37)
</p>
<p>9To be exact, there is the second requirement, U | = U |. For antiunitary operators T , it
holds also that T T &dagger; = T &dagger;T = 1, but in contrast to the unitary operators, T | = &lowast;T |.
Anti-unitary operators appear, apart from the complex conjugation, in quantum mechanics only in
</p>
<p>connection with time reversal (see Chap. 21, Vol. 2). So the equation UU &dagger; = U &dagger;U = 1 almost
always refers to unitary operators.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 Unitary Operators 175
</p>
<p>and it follows that
</p>
<p>n&prime;n = n&prime; | n =
&sum;
</p>
<p>m &prime;m
</p>
<p>U &lowast;m &prime;n&prime; m &prime; | mUmn =
&sum;
</p>
<p>m
</p>
<p>U &lowast;mn&prime;Umn =
(
</p>
<p>U &dagger;U
)
</p>
<p>n&prime;n
,
</p>
<p>(13.38)
</p>
<p>which is just another notation for U &dagger;U = 1.
</p>
<p>13.2.2 Functions of Operators, the Time-Evolution Operator
</p>
<p>For a given general operator A, we can define powers of A, or construct other expres-
</p>
<p>sions such as power series of the form
</p>
<p>&infin;
&sum;
</p>
<p>n=0
an A
</p>
<p>n (13.39)
</p>
<p>as we have already done several times in previous chapters. An example is
&sum;&infin;
</p>
<p>n=0 an
dn
</p>
<p>dxn
.
</p>
<p>Of course, in view of such expressions there is generally the question whether a
</p>
<p>series converges at all (i.e. whether it makes sense to write it). The answer depends
</p>
<p>on the coefficients an and the functions to which we apply the operator.
</p>
<p>We want to examine an example in more detail. Consider a time-independent
</p>
<p>Hamiltonian H
</p>
<p>i
d
</p>
<p>dt
|(t) = H |(t). (13.40)
</p>
<p>We wish to show that the time-evolution operator
</p>
<p>U (t) =
&infin;
</p>
<p>&sum;
</p>
<p>n=0
</p>
<p>(
</p>
<p>&minus;i t

</p>
<p>)n
H n
</p>
<p>n! &equiv; e
&minus;i Ht
</p>
<p> (13.41)
</p>
<p>is unitary and transforms the initial state |(0) into the state |(t).10 To that end,
we assume that the state vector can be expanded in a power series about t = 0:
</p>
<p>|(t) = |(0) + t
1!
</p>
<p>(
</p>
<p>d
dt
|(t)
</p>
<p>)
</p>
<p>t=0 +
t2
</p>
<p>2!
</p>
<p>(
</p>
<p>d2
</p>
<p>dt2
|(t)
</p>
<p>)
</p>
<p>t=0
+ &middot; &middot; &middot;
</p>
<p>=
&sum;
</p>
<p>n
</p>
<p>tn
</p>
<p>n!
(
</p>
<p>dn
</p>
<p>dtn
|(t)
</p>
<p>)
</p>
<p>t=0 .
(13.42)
</p>
<p>10Since it, so to say, impels or propagates the state | through time, it is also called propagator.</p>
<p/>
</div>
<div class="page"><p/>
<p>176 13 Operators
</p>
<p>The time derivatives can be expressed with the help of the SEq as powers of H 11:
</p>
<p>i d
dt
|(t) = H |(t)
</p>
<p>(i d
dt
)2 |(t) = i d
</p>
<p>dt
H |(t) = H 2 |(t) &middot; &middot; &middot;
</p>
<p>(i d
dt
)n |(t) = i d
</p>
<p>dt
H n&minus;1 |(t) = H n |(t).
</p>
<p>(13.43)
</p>
<p>We replace the time derivatives in (13.42) by these expressions and find
</p>
<p>|(t) =
&sum;
</p>
<p>n
</p>
<p>tn
</p>
<p>n!
</p>
<p>(
</p>
<p>dn
</p>
<p>dtn
|(t)
</p>
<p>)
</p>
<p>t=0
=
</p>
<p>&infin;
&sum;
</p>
<p>n=0
</p>
<p>(
</p>
<p>&minus;i t

</p>
<p>)n
H n
</p>
<p>n! |(0). (13.44)
</p>
<p>On the right-hand side, we have an operator which acts on the initial state |(0):
</p>
<p>U (t) =
&infin;
</p>
<p>&sum;
</p>
<p>n=0
</p>
<p>(
</p>
<p>&minus;i t

</p>
<p>)n
H n
</p>
<p>n! = e
&minus;i Ht
</p>
<p> , (13.45)
</p>
<p>and can thus write the time evolution compactly as
</p>
<p>|(t) = U (t) |(0) (13.46)
</p>
<p>or more generally, |(t2) = U (t2 &minus; t1) |(t1).
We note that (13.45) and (13.46) are equivalent to the SEq in the form (13.40).
</p>
<p>Ultimately, it is just a matter of personal preference or habit, which one of the two
</p>
<p>formulations one uses. In any case, we can see very clearly in (13.46) the deterministic
</p>
<p>nature of the SEq: specifying an initial condition determines uniquely its solution
</p>
<p>for all times, as we have already derived in an example in Chap. 5.
</p>
<p>Finally, we want to show that the time evolution operator U is unitary. We extend
</p>
<p>the proof and show that in general, the following relation holds:
</p>
<p>For U = ei A with A = A&dagger; follows U&minus;1 = U &dagger;. (13.47)
</p>
<p>For the proof, we use the power series of the exponential function:
</p>
<p>U &dagger; =
(
</p>
<p>ei A
)&dagger; =
</p>
<p>&sum;
</p>
<p>(
</p>
<p>in
</p>
<p>n! A
n
</p>
<p>)&dagger;
</p>
<p>=
&sum; (&minus;i)n
</p>
<p>n! (A
n)&dagger;
</p>
<p>=
&sum; (&minus;i)n
</p>
<p>n! (A
&dagger;)n =
</p>
<p>&sum; (&minus;i)n
n! A
</p>
<p>n = e&minus;i A = U&minus;1. (13.48)
</p>
<p>11Note that H does not depend on time, which is why we obtain such simple formulations.
</p>
<p>Propagators for time-dependent Hamiltonians can also be formulated, but this is somewhat more
</p>
<p>complicated.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 Unitary Operators 177
</p>
<p>It follows U &dagger;U = ei Ae&minus;i A = 1; hence, the operator is unitary and U&minus;1 = e&minus;i A. The
general formulation of this fact is found in the theorem of Stone, see Appendix I,
</p>
<p>Vol. 1.
</p>
<p>We finally mention in passing that the propagator can be written as an integral
</p>
<p>operator, which is advantageous in some contexts. More on this topic in Appendix I,
</p>
<p>Vol. 1. Note that the propagator plays a dominant role in quantum field theory.
</p>
<p>13.3 Projection Operators
</p>
<p>An expression of the form P = |1 1| is the simplest projection operator.
Applying it to a vector, the operator projects it (as the name suggests) onto a subspace;
</p>
<p>in this example, that subspace which is spanned by |1.
Generally, an idempotent operator is defined by
</p>
<p>P2 = P. (13.49)
</p>
<p>If P is Hermitian in addition, it is called a projection operator. In a Hilbert space of
</p>
<p>dimension N , we can define, for example, the projection operator
</p>
<p>P =
&sum;
</p>
<p>n&le;N &prime;
|n n|, (13.50)
</p>
<p>where {n} is an O N system of dimension N &prime; &le; N . That this is indeed a projection
operator can be seen from
</p>
<p>P2 =
&sum;
</p>
<p>n
</p>
<p>|n n|
&sum;
</p>
<p>m
</p>
<p>|m m | =
&sum;
</p>
<p>n
</p>
<p>&sum;
</p>
<p>m
</p>
<p>|n mn m | =
&sum;
</p>
<p>n
</p>
<p>|n n| = P.
</p>
<p>(13.51)
</p>
<p>The eigenvalue equation for a projection operator P reads
</p>
<p>P |p = p |p (13.52)
</p>
<p>with the eigenvectors |p and the eigenvalues p.12 Multiplication by P yields
</p>
<p>P2 |p = Pp |p = pP |p = p2 |p. (13.53)
</p>
<p>On the other hand, because of P2 = P , it follows that
</p>
<p>P2 |p = P |p = p |p, (13.54)
</p>
<p>and therefore,
</p>
<p>12Here the notation p has, of course, nothing to do with the momentum, but with p as projection.</p>
<p/>
</div>
<div class="page"><p/>
<p>178 13 Operators
</p>
<p>p2 = p or p = 0 and 1. (13.55)
</p>
<p>As we well know, the completeness relation of a CONS provides us with a special
</p>
<p>projection operator, namely, a projection onto &lsquo;everything&rsquo;.
</p>
<p>Pwhole space =
&sum;
</p>
<p>n
|n n| = 1. (13.56)
</p>
<p>In words: this projection operator projects onto the whole space.13 As we have seen,
</p>
<p>the completeness relation is not only often a very useful tool in conversions, but is
</p>
<p>also very simple to handle&mdash;because we insert just an identity (and this, by the way,
</p>
<p>gives the procedure its name). A simple example:
</p>
<p>| = 1 &middot; | =
&sum;
</p>
<p>n
|n n | =
</p>
<p>&sum;
</p>
<p>n
|n cn. (13.57)
</p>
<p>We see that cn = n | is the projection of  onto n.
Note: Two projection operators are called (mutually) orthogonal if P1 P2 = 0 (this
also holds for the corresponding subspaces).
</p>
<p>13.3.1 Spectral Representation
</p>
<p>Suppose that in a Hilbert space H, the eigenfunctions of an operator A form a CONS
</p>
<p>{|an , n = 1, 2, . . .}. Then we can express the operator using the projection operators
Pn = |an an| built with the eigenfunctions. For with
</p>
<p>A |an = an |an, (13.58)
</p>
<p>and
&sum;
</p>
<p>n
|an an| =
</p>
<p>&sum;
</p>
<p>n
Pn = 1, (13.59)
</p>
<p>it follows that
</p>
<p>A |an an| = an |an an| &harr;
&sum;
</p>
<p>n
A |an an| =
</p>
<p>&sum;
</p>
<p>n
an |an an| (13.60)
</p>
<p>and therefore
</p>
<p>A =
&sum;
</p>
<p>n
an |an an| =
</p>
<p>&sum;
</p>
<p>n
an Pn. (13.61)
</p>
<p>13To avoid misunderstandings, we repeat the remark that the last equation is an operator equation,
</p>
<p>i.e. simply two different representations of one operator.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Projection Operators 179
</p>
<p>This is called the spectral representation of an operator.14 The spectral representation
</p>
<p>in the degenerate case is treated in the exercises.
</p>
<p>An operator C whose eigenfunctions are not |an can be expressed similarly. It is
</p>
<p>C =
&sum;
</p>
<p>n
|an an|C
</p>
<p>&sum;
</p>
<p>m
|am am | =
</p>
<p>&sum;
</p>
<p>n,m
cnm |an am |, (13.62)
</p>
<p>with cnm = an|C |am.
</p>
<p>13.3.2 Projection and Properties
</p>
<p>Using projectors, we can make a connection to the term property of a system. We
</p>
<p>again start from an operator A with the CONS {|an , n = 1, 2, . . .}, with a non-
degenerate spectrum, and from the projection operators Pn = |an an|. For these
operators, the eigenvalue equation holds:
</p>
<p>Pn |am = |an an |am = nm &middot; |am. (13.63)
</p>
<p>That means that |an is an eigenvector of Pn with eigenvalue 1; all other states |am
with n 
= m are eigenvectors of Pn with the eigenvalue 0. In other words: Pn projects
onto a one-dimensional subspace of H.
</p>
<p>We now consider a system in the normalized state | =
&sum;
</p>
<p>n cn |an and the
projection operator Pk = |ak ak |. We have
</p>
<p>Pk | =
&sum;
</p>
<p>n
cn Pk |an =
</p>
<p>&sum;
</p>
<p>n
cn |ak ak |an = ck |ak. (13.64)
</p>
<p>This means that the state | is an eigenstate of Pk with the eigenvalue 1 iff cn = kn
(ck = 1, because of the normalization of the state), and with the eigenvalue 0 iff
ck = 0:
</p>
<p>Pk | = 1 &middot; | &hArr; | = ck |ak ; ck = 1
Pk | = 0 &middot; | &hArr; | =
</p>
<p>&sum;
</p>
<p>n cn |an ; ck = 0.
(13.65)
</p>
<p>So we can draw the following conclusion: Pk = 1 means &lsquo;if the system is in
the state | and A is measured, then the result is ak&rsquo; or (in a slightly more casual
formulation) &lsquo;A has a value of ak&rsquo; or &lsquo;the system has the property ak&rsquo;. In this sense,
</p>
<p>we can understand projection operators as representing yes-no observables, i.e. as a
</p>
<p>response to the question of whether the value of a physical quantity A is given by ak
(1: Yes, the quantum system has the property ak) or not (0: No, the quantum system
</p>
<p>does not have the property ak).
15
</p>
<p>14We have found it already as an example in an exercise of Chap. 11.
15We have here a connection to logic (via &lsquo;1 = true&rsquo; and &lsquo;0 = false&rsquo;). In classical physics, such a
statement (the quantity A has the value ak ) is either true or false; in quantum mechanics or quantum
</p>
<p>logic, the situation may be more complex.</p>
<p/>
</div>
<div class="page"><p/>
<p>180 13 Operators
</p>
<p>For if a state has a property ak (in the sense that it had it before the measurement and
</p>
<p>the measurement makes us aware of this previously unknown value, e.g. &lsquo;horizontally
</p>
<p>linear-polarized&rsquo;), then Pk = 1 and all other projections equal zero, Pn 
=k = 0.16
</p>
<p>13.3.3 Measurements
</p>
<p>We have formulated the measurement process already in previous chapters with
</p>
<p>the help of projection operators&mdash;a measurement corresponds to the projection of
</p>
<p>a state onto a particular subspace, which is one- or multidimensional, as the case
</p>
<p>may be. Also, the &lsquo;production&rsquo; of an initial state at time t = 0 can be regarded as a
kind of measurement, because here a superposition state is projected onto a certain
</p>
<p>subspace. However, this is generally not called &lsquo;measurement&rsquo; but rather preparation
</p>
<p>of a state.17 We want to set down in this section once again the essential terms in the
</p>
<p>case of degeneracy.
</p>
<p>We start at the initial time with a state that evolves unitarily according to the
</p>
<p>Schr&ouml;dinger equation until the time of measurement. We assume that this state is a
</p>
<p>superposition of basis states, as is described by the expansion theorem. Immediately
</p>
<p>before the measurement, we can for example write
</p>
<p>| =
&sum;
</p>
<p>n
</p>
<p>gn
&sum;
</p>
<p>r=1
cn,r
</p>
<p>
</p>
<p>n,r
&rang;
</p>
<p>, (13.66)
</p>
<p>with
&lang;
</p>
<p>m,s

</p>
<p>n,r
&rang;
</p>
<p>= m,nr,s . (13.67)
</p>
<p>Through the measurement, the state vector is changed; if we measure the system in the
</p>
<p>state m (with or without degeneracy; the denominators are due to the normalization),
</p>
<p>we obtain
</p>
<p>| measurement&rarr; cm |m|cm |
or | measurement&rarr; 1&radic;
</p>
<p>gn
&sum;
</p>
<p>r=1
</p>
<p>
</p>
<p>cm,r

</p>
<p>
</p>
<p>2
</p>
<p>gm
&sum;
</p>
<p>r=1
cm,r
</p>
<p>
</p>
<p>m,r
&rang;
</p>
<p>(13.68)
</p>
<p>(reduction of the wave packet, collapse of the wave function). The vector
&sum;gn
</p>
<p>r=1 cm,r

</p>
<p>m,r
&rang;
</p>
<p>is none other than the projection of | onto the subspace belonging to m; the
projection operator is
</p>
<p>16More on this topic in Chap. 27, Vol. 2.
17Some remarks on terms that arise in connection with &lsquo;measurements&rsquo; are given in Appendix S,
</p>
<p>Vol. 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Projection Operators 181
</p>
<p>Pm = |m m | or Pm =
gm
&sum;
</p>
<p>r=1
</p>
<p>
</p>
<p>m,r
&rang; &lang;
</p>
<p>m,r

</p>
<p> , (13.69)
</p>
<p>so that we can write (13.68) compactly (i.e. irrespective of whether or not there is
</p>
<p>degeneracy) as
</p>
<p>| measurement&rarr; Pm |&radic;
| Pm |
</p>
<p>= Pm |&radic;
Pm
</p>
<p>. (13.70)
</p>
<p>Therefore, a measurement can be understood in this sense as a projection onto a
</p>
<p>corresponding subspace.18
</p>
<p>13.4 Systematics of the Operators
</p>
<p>For greater clarity, we want to discuss briefly the &lsquo;family tree&rsquo; of the operators used
</p>
<p>here (see Fig. 13.2); a similar pedigree for matrices can be found in Appendix F,
</p>
<p>Vol. 1). They are all linear (with the aforementioned exception of the complex con-
</p>
<p>jugation and time reversal operations) and normal. An operator A is called normal
</p>
<p>if it fulfills AA&dagger; = A&dagger; A.
We can easily convince ourselves that the operators which are important for quan-
</p>
<p>tum mechanics (Hermitian, positive, projection, unitary operators) are all normal:
</p>
<p>A Hermitian: A = A&dagger; &rarr; AA&dagger; = A&dagger; A
U unitary: U&minus;1 = U &dagger; &rarr; UU &dagger; = 1 = U &dagger;U. (13.71)
</p>
<p>The interest in normal operators is, among other things, that they can be diag-
</p>
<p>onalized. Actually, we find more generally: An operator can be diagonalized by a
</p>
<p>Fig. 13.2 The family tree of
</p>
<p>linear operators linear
</p>
<p>unitary
</p>
<p>projection
</p>
<p>positive
</p>
<p>Hermitian
</p>
<p>normal
</p>
<p>18We use here the fact that all states ei | are physically equivalent for arbitrary real . See also
Chap. 14.</p>
<p/>
</div>
<div class="page"><p/>
<p>182 13 Operators
</p>
<p>unitary transformation iff it is normal.19 Non-normal operators can also be diag-
</p>
<p>onalized under some circumstances&mdash;but not by a unitary (i.e. length-conserving)
</p>
<p>transformation. An example is given in the exercises.
</p>
<p>Because of the diagonalizability of the operators or matrices occurring in quantum
</p>
<p>mechanics, we can always expand in terms of eigenfunctions without having to
</p>
<p>worry about Jordan normal forms or the like. This contributes significantly to the
</p>
<p>well-behaved character of quantum mechanics.
</p>
<p>13.5 Exercises
</p>
<p>1. Let A be a linear and B an anti-linear operator; | is a state. Compute or simplify
A (i |) and B (i |).
</p>
<p>2. Show that the complex conjugation K is an anti-linear operator.
</p>
<p>3. Show that the commutator C = [A, B] of two Hermitian operators A and B is
anti-Hermitian.
</p>
<p>4. The Hermitian operators A and B fulfill [A, B] 
= 0. Consider the operator
Q = c [A, B]. For which c is Q a Hermitian operator?
</p>
<p>5. Consider the operator Q = AB, where A and B are Hermitian matrices. Under
what conditions is Q a Hermitian operator?
</p>
<p>6. Show in the bra-ket representation that:
</p>
<p>(a) Hermitian operators have real eigenvalues.
</p>
<p>(b) The eigenfunctions of Hermitian operators are pairwise orthogonal (assum-
</p>
<p>ing the spectrum is not degenerate).
</p>
<p>7. Show that the mean value of a Hermitian operator A is real, and the mean value
</p>
<p>of an anti-Hermitian operator B is imaginary.
</p>
<p>8. What is the quantum-mechanical operator for the classical term p &times; l?
9. Calculate the mean value of z for the normalized state
</p>
<p>(
</p>
<p>a
</p>
<p>b
</p>
<p>)
</p>
<p>.
</p>
<p>10. Given the time-independent Hamiltonian H ; what is the associated time evolu-
</p>
<p>tion operator U (t)?
</p>
<p>11. Let U be the operator U = ei A, where A is a Hermitian operator. Show that U
is unitary.
</p>
<p>12. What are the eigenvalues that a unitary operator can have?
</p>
<p>13. Show that the time evolution operator e&minus;i
Ht
 is unitary.
</p>
<p>14. Show that scalar products, matrix elements, eigenvalues and expectation values
</p>
<p>are invariant under unitary transformations.
</p>
<p>15. P1 and P2 are projection operators. Under which conditions are P = P1 + P2
and P = P1 P2 projection operators?
</p>
<p>16. Formulate the matrix representation of the operator P = |e1 e1| in R3.
17. What is the general definition of a projection operator?
</p>
<p>19The proof is found in Appendix I, Vol. 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.5 Exercises 183
</p>
<p>18. Given the CONS {|n}; for which cn is the operator A =
&sum;
</p>
<p>cn |n n| a
projection operator?
</p>
<p>19. Which eigenvalues can a projection operator have?
</p>
<p>20. Given the CONS {|n} in a Hilbert space of dimension N . Consider the operator
</p>
<p>P =
&sum;
</p>
<p>n&le;N
|n n| (13.72)
</p>
<p>with N &prime; &le; N . Show that P is a projection operator.
21. Given the operator A with degenerate spectrum:
</p>
<p>A

</p>
<p>n,r
&rang;
</p>
<p>= an

</p>
<p>n,r
&rang;
</p>
<p>; r = 1, . . . , gn. (13.73)
</p>
<p>(a) Formulate the projection operator onto the states with subscript n.
</p>
<p>(b) Formulate the spectral representation of A.
</p>
<p>22. Given the operators A = | | and B = | |. Let |  =  &isin; C,  
= 0.
For which  is the operator C = AB a projection operator?
</p>
<p>23. Given the operator Q = B&dagger; B, where B is unitary. How can Q be more simply
written?
</p>
<p>24. Given the operator Q = B&dagger; B, where B is not unitary. Show that the eigenvalues
of Q are real and that they are not negative.
</p>
<p>25. Given the operator A =  | |. Let |  =  
= 0;  and  are complex
constants. The states | and | are normalized. Which conditions must |,
|,  and  fulfill to ensure that A is a Hermitian, a unitary, or a projection
operator?
</p>
<p>26. Given a CONS {|n} and an operator
</p>
<p>A =
&sum;
</p>
<p>n,m
</p>
<p>cnm |n m |; cnm &isin; C. (13.74)
</p>
<p>How must the coefficients cnm be chosen in order that A be a Hermitian, a unitary,
</p>
<p>or a projection operator?
</p>
<p>27. A CONS {|n, n = 1, 2, . . . , N } spans a vector space V .
(a) Show that each operator A acting in V can be represented as
</p>
<p>A =
&sum;
</p>
<p>n,m
</p>
<p>cnm |n m | . (13.75)
</p>
<p>(b) Consider the special case N = 3:
</p>
<p>A |1 = &minus; |2; A |2 = &minus; |3; A |3 = &minus; |1 + |2 . (13.76)</p>
<p/>
</div>
<div class="page"><p/>
<p>184 13 Operators
</p>
<p>What is the operator A? (Determine the coefficients cnm , i.e. formulate A as
</p>
<p>a linear combination of products |i 
&lang;
</p>
<p> j

</p>
<p>.)
</p>
<p>28. How is the generalized Heisenberg uncertainty relation formulated for each of
</p>
<p>the pairs (x, lx ),
(
</p>
<p>x, ly
)
</p>
<p>, (x, lz)?
</p>
<p>29. For the Pauli matrices, the following uncertainty relation holds:
</p>
<p>xy &ge; |z| . (13.77)
</p>
<p>For which normalized states =
(
</p>
<p>a
</p>
<p>b
</p>
<p>)
</p>
<p>is the right-hand side a minimum/
</p>
<p>maximum?
</p>
<p>30. What is the generalized uncertainty relation for H and p?
</p>
<p>31. The position operator in the Heisenberg picture,20 xH , is given by
</p>
<p>xH = ei
t H
 xe&minus;i
</p>
<p>t H
 . (13.78)
</p>
<p>How does this operator depend explicitly on time? The potential is assumed to
</p>
<p>be constant, dV
dx
</p>
<p>= 0. Hint: Use the equation
</p>
<p>ei A Be&minus;i A = B + i [A, B] + i
2
</p>
<p>2! [A, [A, B]] +
i3
</p>
<p>3! [A, [A, [A, B]]] + &middot; &middot; &middot;
(13.79)
</p>
<p>or
</p>
<p>i
d
</p>
<p>dt
xH = [xH , H ] (13.80)
</p>
<p>(or both for practice).
</p>
<p>32. A Hamiltonian H depends on a parameter q, H = H (q). In addition, E (q) is
a nondegenerate eigenvalue and | (q) the corresponding eigenvector:
</p>
<p>H (q) | (q) = E (q) | (q). (13.81)
</p>
<p>Show that
</p>
<p>&part;E (q)
</p>
<p>&part;q
=  (q)| &part;H (q)
</p>
<p>&part;q
| (q). (13.82)
</p>
<p>(This equation is also called the Feynman-Hellmann theorem.)
</p>
<p>33. {|n} is a CONS. Every solution of the SEq may be written as
</p>
<p>| =
&sum;
</p>
<p>l
</p>
<p>al |l (13.83)
</p>
<p>20See also Appendix Q, Vol. 1, &lsquo;Schr&ouml;dinger picture, Heisenberg picture and interaction picture&rsquo;.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.5 Exercises 185
</p>
<p>and every operator A as
</p>
<p>A =
&sum;
</p>
<p>mn
</p>
<p>cmn |n m| . (13.84)
</p>
<p>Can the non-Hermitian operator A (i.e. cmn 
= c&lowast;nm for at least one pair n,m)
have a real expectation value (for arbitrary states |) under these conditions?
</p>
<p>34. We consider the Hamiltonian H = 1 + ay , already introduced in the exercises
for Chap. 8.
</p>
<p>(a) What is the expected result of the measurement of the x-component of the
</p>
<p>spin in the state |t  with |0 =
(
</p>
<p>1
</p>
<p>0
</p>
<p>)
</p>
<p>?
</p>
<p>(b) What is the uncertainty sx in this state?
</p>
<p>(c) Calculate the commutator
[
</p>
<p>sx , sy
]
</p>
<p>and formulate the uncertainty relation for
</p>
<p>the observables sx and sy for arbitrary times t .
</p>
<p>35. Given an eigenvalue problem A |am = am |am ({|am} is a CONS); we can
define a function of the operator by
</p>
<p>F (A) |am := F (am) |am. (13.85)
</p>
<p>(a) Show that:
</p>
<p>F (A) =
&sum;
</p>
<p>m
</p>
<p>F (am) Pm (13.86)
</p>
<p>with Pm = |am am |.
(b) Show that if F (a) is real for all eigenvalues am , then F (A) is self-adjoint.
</p>
<p>36. What are the conditions which the elements of a two-dimensional normal matrix
</p>
<p>have to fulfill?
</p>
<p>37. Given the matrix
</p>
<p>A =
(
</p>
<p>0 2
</p>
<p>1 0
</p>
<p>)
</p>
<p>;  
= 0. (13.87)
</p>
<p>(a) Is A normal?
</p>
<p>(b) Show that A is diagonalizable for almost all , but not by a unitary transfor-
</p>
<p>mation.
</p>
<p>38. In the derivation of the uncertainty relation, the functions must be in the domains
</p>
<p>of definition of the operators and of the operator products involved. If they are
</p>
<p>not, we do not obtain meaningful statements. As an example, we consider the
</p>
<p>function:
</p>
<p>f (x) = sin x
2
</p>
<p>x
. (13.88)
</p>
<p>(a) Is f (x) square-integrable?
</p>
<p>(b) Is f (x) within the domains of definition of the operator x?</p>
<p/>
</div>
<div class="page"><p/>
<p>186 13 Operators
</p>
<p>(c) Can a meaningful uncertainty relation be derived for f (x)?
</p>
<p>(d) Can similar statements be made for the function g(x) = sin x
x
</p>
<p>?
</p>
<p>39. Given two operators A and B which commute with their commutator,
</p>
<p>[A, [A, B]] = [B, [A, B]] = 0. Show that:
[
</p>
<p>B, An
]
</p>
<p>= n [B, A] An&minus;1. (13.89)
</p>
<p>40. Show that the momentum operator is given in the coordinate representation by
</p>
<p>p = 
i
</p>
<p>d
dx
</p>
<p>. Make use only of the commutator [x, p] = i and derive, making use
of the previous exercise, the result:
</p>
<p>[p, f (x)] = 
i
</p>
<p>d f (x)
</p>
<p>dx
. (13.90)
</p>
<p>41. Given two operators A and B which commute with their commutator, [A, [A,
B]] = [B, [A, B]] = 0. Show that
</p>
<p>eA+B = eAeBe&minus; 12 [A,B]. (13.91)
</p>
<p>This is a special case of the Baker-Campbell-Hausdorff formula (relation, the-
</p>
<p>orem). The general case considers eA+B for two operators, which do not have
to commute with their commutator (this is used e.g. in (13.79)). By the way,
</p>
<p>these authors published their work in 1900, well before the birth of quantum
</p>
<p>mechanics.
</p>
<p>(a) First, prove the equation
</p>
<p>[
</p>
<p>B, ex A
]
</p>
<p>= ex A [B, A] x . (13.92)
</p>
<p>(b) Define
</p>
<p>G (x) = ex Aex B (13.93)
</p>
<p>and show that the following equation holds:
</p>
<p>dG
</p>
<p>dx
= (A + B + [A, B] x)G. (13.94)
</p>
<p>Integrate this equation.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 14
</p>
<p>Postulates of Quantum Mechanics
</p>
<p>In this chapter, we compile our findings about quantum mechanics from the preceding
</p>
<p>chapters, insofar as they concern its structure, and we formulate some basic rules for its
</p>
<p>application. They make up the general framework for our further considerations.
</p>
<p>In previous chapters, we have frequently mentioned structural elements of quantum
</p>
<p>mechanics, which we now summarize and present systematically, namely in the form
</p>
<p>of postulates or rules. These rules map the behavior of physical systems, or more
</p>
<p>precisely, our methods for describing that behavior.1 In fact, there are basically only
</p>
<p>three questions to which the physical description of a system must provide answers:
</p>
<p>1. How can we describe the state of the system at a given time?
</p>
<p>2. Which variables of the system are measurable and how can we predict the results
</p>
<p>of measurements?
</p>
<p>3. How is the state of the system obtained at time t from its known initial state at
</p>
<p>time t0?
</p>
<p>The answers to these questions differ, of course, depending on the field (classical
</p>
<p>mechanics, quantum mechanics, hydrodynamics, quantum electrodynamics, ...). In
</p>
<p>the following, we will clothe them in the form of postulates for quantum mechanics,
</p>
<p>whereby the word &lsquo;postulate&rsquo; in this context is equivalent to a thesis, a principle or
</p>
<p>a rule which is not proven, but is quite plausible and evident. We do not aim at an
</p>
<p>absolutely rigorous system of axioms (in the sense of a minimal set of statements). It
</p>
<p>is rather a question of creating a viable and practical set of rules (also called &lsquo;quasi-
</p>
<p>axiomatic&rsquo;), and for the sake of practicality we also take into account the fact that
</p>
<p>1&ldquo;The human understanding is of its own nature prone to suppose the existence of more order
</p>
<p>and regularity in the world than it finds.&rdquo; Francis Bacon (1561&ndash;1626), English philosopher and
</p>
<p>statesman, in New Organon. &ldquo;Rain, snow, winds follow each other so that we do not animadvert a
</p>
<p>clear law in their order, but laws again are only conceived by us to facilitate comprehending a thing,
</p>
<p>as we create species.&rdquo; Georg Christoph Lichtenberg, Scrap Books, Vol. A (192).
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4_14
</p>
<p>187</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_14&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-00464-4_14&amp;domain=pdf</a></div>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4_14">https://doi.org/10.1007/978-3-030-00464-4_14</a></div>
</div>
<div class="page"><p/>
<p>188 14 Postulates of Quantum Mechanics
</p>
<p>one postulate might be derivable from others.2 Moreover, the set of rules established
</p>
<p>here is only one of many possible such sets. Other formulations of the postulates can
</p>
<p>be found in Appendix R, Vol. 1.3
</p>
<p>In introductions to quantum mechanics, the postulates are often presented right
</p>
<p>at the beginning, so to speak as the basis for the further development of quantum
</p>
<p>mechanics. This proceeding has the immediate advantage of conceptual clarity, since
</p>
<p>e.g. borrowing from classical mechanics (the correspondence principle, etc.) is not
</p>
<p>necessary. On the other hand, for the &lsquo;uninitiated&rsquo;, the postulates somehow seem to
</p>
<p>fall from the heavens&mdash;without background information, it is probably quite difficult
</p>
<p>to understand how formulations like these were arrived at in the first place.4
</p>
<p>Our access to quantum mechanics was a two-pronged one in the first chapters. In
</p>
<p>the analytical approach, we began with the dynamics (SEq, question 3) and subse-
</p>
<p>quently took up questions 1 and 2. In the algebraic approach, we tried to make the
</p>
<p>postulates plausible (or to anticipate their statements) in the order given here and on
</p>
<p>the basis of simple physical systems.
</p>
<p>Finally, we remark that the postulates do not raise any new difficulties of compre-
</p>
<p>hension, but nevertheless they surely sharpen our view of open problems. In previous
</p>
<p>chapters, we have already mentioned questions of this kind several times, and we
</p>
<p>will do so again at the end of this chapter.
</p>
<p>14.1 Postulates
</p>
<p>The numbering of the postulates refers to the numbers of the questions given above.
</p>
<p>14.1.1 States, State Space (Question 1)
</p>
<p>We have seen that both the solutions of the SEq as well as vectors, e.g. polarization
</p>
<p>vectors, can be linearly superposed and satisfy the axioms of a vector space. The first
</p>
<p>postulate summarizes this situation.
</p>
<p>2Moreover, one cannot of course exclude with certainty today that the rules established in the
</p>
<p>following will be (or will have to be) modified sometime later.
3Indeed, there is no overall agreement about the basic facts. For instance, some of the authors listed
</p>
<p>in Appendix R, Vol. 1 treat the indistinguishability of identical quantum objects as a postulate of
</p>
<p>quantum mechanics, but others do not.
4The (quasi-) axiomatic approach has the great advantage that it does not need false analogies and
</p>
<p>does not implant false images into the minds of students; thus, it has been proposed as a possible
</p>
<p>way to teach quantum mechanics in schools. This is feasible with a (suitably adapted) form of the
</p>
<p>algebraic approach. With the exception of Postulate 3, the postulates can be deduced, or at least
</p>
<p>motivated, if one confines oneself essentially to the two-dimensional case. It is not least for this
</p>
<p>reason that the algebraic approach is of great didactic interest.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.1 Postulates 189
</p>
<p>Postulate 1: The state of a quantum system at a given moment is completely
</p>
<p>defined by giving its state vector (ket), |. The state vector is an element of the
Hilbert space H, which also is called the state space.
</p>
<p>Remarks:
</p>
<p>1. In contrast to e.g. classical mechanics, quantum mechanics describes the states
</p>
<p>by elements of a vector space, i.e. by vectors. The abstract state vector or ket, |,
is the mathematical representation of the information we have about the physical
</p>
<p>state of the system.
</p>
<p>2. Because H is a vector space, the superposition principle holds; it is characteristic
</p>
<p>of the linearity of the theory. As a dominant principle of quantum mechanics, it
</p>
<p>is responsible for many of those phenomena of quantum mechanics which seem
</p>
<p>so strange to our everyday understanding.
</p>
<p>3. Because of the linearity of the theory, we can always assume that the state vectors
</p>
<p>are normalized. If this is not the case, we must, where necessary, normalize them
</p>
<p>post hoc by dividing by the norm.
</p>
<p>4. In anticipation, we note that only eigenvalues and absolute values of scalar prod-
</p>
<p>ucts such as || | are relevant to a measurement. This means that the states
| and
</p>
<p>
</p>
<p>&prime;
&rang;
</p>
<p>= ei | with  &isin; R, differing only in their phase, are physically
equivalent (which fact we have used in considering the infinite potential well).
</p>
<p>Strictly speaking, a (normalized) physical state is therefore not represented by a
</p>
<p>vector, but rather by a ray in H, i.e. the set
{
</p>
<p>ei |, &isin; R
}
</p>
<p>. This fact is called
</p>
<p>the &lsquo;independence of the physics from the global phase&rsquo;. Changing the relative
</p>
<p>phases naturally leads to different states; c1 | + c2 | and c1ei | + c2 |
are physically different for  = 2n.
As we shall see, the difference between a ray and a vector is hereafter (luckily)
</p>
<p>only once of real importance, namely in the consideration of symmetry under
</p>
<p>time reversal (see Chap. 21, Vol. 2). Apart from this exception, we can work with
</p>
<p>state vectors (and ignore rays).
</p>
<p>5. In the examples considered so far, any vector in H is a physically realizable
</p>
<p>state. This is not necessarily the case in all situations, as we shall see later in the
</p>
<p>treatment of identical particles, where there is no superposition of the states of
</p>
<p>fermions and bosons. The nonexistence of such (superposed) states is reflected
</p>
<p>in superselection rules.
</p>
<p>6. It is still controversial just what the state vector &lsquo;really&rsquo; means. The opinion that
</p>
<p>the state vector describes the physical reality of an individual quantum system is
</p>
<p>shared by many (and is also the position taken in this book), but it is by far not
</p>
<p>the only one. For more on this issue see Chap. 28, Vol. 2 on the interpretations
</p>
<p>of quantum mechanics. We stress once again that the state vector does not have
</p>
<p>a direct and concrete (everyday) meaning.</p>
<p/>
</div>
<div class="page"><p/>
<p>190 14 Postulates of Quantum Mechanics
</p>
<p>14.1.2 Probability Amplitudes, Probability (Question 2)
</p>
<p>We have shown, e.g. by considering the polarization, that the absolute square of an
</p>
<p>amplitude gives the probability of finding the system in the respective state. This fact
</p>
<p>is generalized in Postulate 2.1.
</p>
<p>Postulate 2.1: If a system is described by the vector |, and | is another state,
then a probability amplitude exists for finding the system in state |, and it is given
by the scalar product  | in H. The probability that the system is in the state |
is the absolute square | ||2 of the probability amplitude.
</p>
<p>Remarks:
</p>
<p>1. The vectors must be normalized to ensure that the probability concept is inherently
</p>
<p>consistent.
</p>
<p>2. Using the projection operator P = | |, the term | ||2 can be written as
| P |.
</p>
<p>3. The probability statements of this postulate provide a direct link to the term
</p>
<p>&lsquo;expectation value&rsquo; or &lsquo;mean value.&rsquo;
</p>
<p>4. Probabilities usually indicate that the necessary information is not completely
</p>
<p>available. Hence, the idea arose quite early that quantum mechanics is not a
</p>
<p>complete theory, and hidden variables (hidden to us) must be added. But this is
</p>
<p>not the case according to present knowledge, at least not in the sense that the
</p>
<p>hidden variables have the simple and familiar properties of classical physics. We
</p>
<p>will address this issue in later chapters in Vol. 2.
</p>
<p>5. This postulate is also called Born&rsquo;s rule.
</p>
<p>14.1.3 Physical Quantities and Hermitian Operators
</p>
<p>(Question 2)
</p>
<p>We have seen that a measurable physical quantity such as the momentum is repre-
</p>
<p>sented by a Hermitian operator. The next postulate generalizes this relationship.
</p>
<p>Postulate 2.2: Every measurable physical quantity is described by a Hermitian
</p>
<p>operator A acting in H; this operator is an observable.5 If a physical quantity is mea-
</p>
<p>sured, the result can be only one of the eigenvalues of the corresponding observable A.
</p>
<p>Remarks:
</p>
<p>1. Quantum mechanics describes physical quantities by operators (in contrast to
</p>
<p>classical mechanics).
</p>
<p>2. These operators are observables, i.e. Hermitian operators that represent a con-
</p>
<p>sistently measurable physical quantity.6 In this way, we take into account that
</p>
<p>5Recall that we defined &lsquo;observables&rsquo; as those Hermitian operators which represent a consistent
</p>
<p>measurable physical quantity, cf. Sect. 13.1.
6We note again that the word observable does not imply the existence of a (human) observer.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.1 Postulates 191
</p>
<p>not every self-adjoint operator (with reasonable eigenfunctions) must necessarily
</p>
<p>represent a physical observable.7 More on this topic in Vol. 1 Appendix I.
</p>
<p>3. Because the operators are Hermitian, measurements always yield real values.
</p>
<p>4. Not all physically measurable quantities are associated with non-trivial operators.
</p>
<p>Mass and charge, for example, are and remain simple numbers.
</p>
<p>14.1.4 Measurement and State Reduction (Question 2)
</p>
<p>If a right circular-polarized photon |r = (|h + i |v) /
&radic;
</p>
<p>2 passes through a PBS,
</p>
<p>we obtain with a probability 1/2 either a horizontally or a vertically linear-polarized
</p>
<p>photon, i.e. |h or |v. The next postulate formalizes and generalizes this fact.
Postulate 2.3: Suppose that the measurement of A on a system which was origi-
</p>
<p>nally in the state | yielded the value an . Then, immediately after the measurement,
the state of the system is the normalized projection of | onto the eigenspace belong-
ing to an (see Chap. 13)
</p>
<p>8
</p>
<p>| &rarr; | =
Pn |&radic;
| Pn |
</p>
<p>(14.1)
</p>
<p>The state | is normalized:
</p>
<p>Pn |2 = | P&dagger;n Pn | = | Pn | (14.2)
</p>
<p>Remarks:
</p>
<p>1. This postulate assumes an ideal measurement; meaning, among other things, that
</p>
<p>further measurements on the quantum object must be possible.9 Immediately after
</p>
<p>measuring, the state of the system is always an eigenvector of A with eigenvalue
</p>
<p>an . Any immediately following further measurement must, of course, yield the
</p>
<p>same result.10 The transition from a superposition state to a single state is called
</p>
<p>state reduction or collapse of the wavefunction. It is an irreversible evolution
</p>
<p>which marks a direction in time.11
</p>
<p>7By the way, the practical implementation of arbitrary operators often raises some difficulties.
</p>
<p>However, for us this is not a strong constraint, because we need essentially only the well-known
</p>
<p>operators such as position, momentum, etc., or combinations of them.
8Any remaining phase plays no physical role; see the remark following Postulate 1 about states and
</p>
<p>rays.
9The only change in the measured system is the collapse of the wavefunction. In particular, the
</p>
<p>spectrum remains unchanged. One speaks in this context also of &lsquo;recoilless&rsquo;. See also Appendix S,
</p>
<p>Vol. 1, where several remarks pertinent to the topic of &lsquo;measurement&rsquo; can be found.
10Thereby one can prevent a change of state by measuring it repeatedly. The associated keyword
</p>
<p>is &lsquo;quantum Zeno effect&rsquo;. The concept is summarized by the handy phrase &lsquo;a watched pot never
</p>
<p>boils&rsquo;. More about this topic in Appendix L, Vol. 1.
11This applies only if the initial state is not already an eigenstate of the operator.</p>
<p/>
</div>
<div class="page"><p/>
<p>192 14 Postulates of Quantum Mechanics
</p>
<p>2. The position underlying this postulate is not sensitive to the details of the
</p>
<p>measurement process, but rather assumes the measuring apparatus to be a kind
</p>
<p>of black box. A more detailed analysis of the measurement process, including
</p>
<p>interactions of the quantum system with the measuring apparatus and the envi-
</p>
<p>ronment, shows that one can interpret Postulate 2.3 as a consequence of Postu-
</p>
<p>lates 2.1 and 2.2. However, this postulate is fapp, i.e. a useful &lsquo;working tool&rsquo; for
</p>
<p>all the usual applications of quantum mechanics. We will address this question
</p>
<p>again in Chap. 24 (decoherence) and in Chap. 28 (interpretations), both in Vol. 2.
</p>
<p>3. Measurement in quantum mechanics is obviously something very different than
</p>
<p>in classical physics. Classically, a (single) value of a physical quantity is mea-
</p>
<p>sured, which already existed before the measurement (pre-existence). In quantum
</p>
<p>mechanics, this is the case only when the system is initially in an eigenstate of
</p>
<p>the measured observable; otherwise, there is no well-defined measurement value
</p>
<p>before the measurement.12 This fact is also called the eigenvector-eigenvalue
</p>
<p>rule: A state has the value a of a property represented by the operator A if and
</p>
<p>only if the state is an eigenvector of A with eigenvalue a.13 In this case, we can
</p>
<p>say that the system has the property a (For more cautious formulations of this
</p>
<p>relation, see Chap. 13.).
</p>
<p>4. The spreading of the measured values is sometimes attributed to the fact that
</p>
<p>the measurement disturbs the measured quantity (e.g. the spin) uncontrollably.
</p>
<p>But this is wrong from the perspective of Postulate 2.3. For if the system is in
</p>
<p>an eigenstate of the measured quantity before the measurement, it will not be
</p>
<p>disturbed by the measurement. If it is not in an eigenstate, the measured value
</p>
<p>does not exist as such before the measurement&mdash;and what does not exist, cannot
</p>
<p>be disturbed.14
</p>
<p>5. For degenerate and continuous cases, (14.1) must be appropriately modified.
</p>
<p>6. This postulate is also called the projection postulate, the Neumann projection
</p>
<p>postulate, the postulate of Neumann&ndash;L&uuml;ders, etc.
</p>
<p>14.1.5 Time Evolution (Question 3)
</p>
<p>So far, our discussion was limited to a fixed time&mdash;now we start the clock. We recall
</p>
<p>that we have restricted ourselves to time-independent interactions.
</p>
<p>Postulate 3: The temporal evolution of the state vector |(t) of an isolated quan-
tum system is described by the equation (evolution equation, Schr&ouml;dinger equation):
</p>
<p>i
d
</p>
<p>dt
|(t) = H |(t) . (14.3)
</p>
<p>12See also the corresponding remarks in Chap. 13 (projection operators).
13Thus we have here a translation rule which connects physical quantities to mathematical objects.
14This remark is of course a bit shortened and flippant. The point is that the value of a variable is
</p>
<p>determined by the measurement, in general. Before the measurement, the value does not exist and
</p>
<p>thus can not be disturbed, for perturbation means changing an existing value into another one. More
</p>
<p>on this issue e.g. in Chap. 20, Vol. 2.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.1 Postulates 193
</p>
<p>The Hermitian operator H which is associated with the total energy of the system is
</p>
<p>called the Hamiltonian.15
</p>
<p>Remarks:
</p>
<p>1. We consider isolated systems which do not interact with their environments.
</p>
<p>Their realization is anything but trivial, which is one of the obstacles to the rapid
</p>
<p>development of quantum computers. If, on the other hand, there is a coupling
</p>
<p>(observed or unobserved) to the degrees of freedom of the environment, one
</p>
<p>speaks of an open quantum system. More on this issue is to be found in Chap. 24,
</p>
<p>Vol. 2 (decoherence) and in Appendix S, Vol. 1.
</p>
<p>2. The postulate tells us nothing about the specific form of the Hamiltonian. This is
</p>
<p>determined by the physical problem and the accuracy with which one wants to
</p>
<p>describe it. Further considerations are found below in the &lsquo;Concluding Remarks&rsquo;
</p>
<p>section.
</p>
<p>3. We had already stated the main characteristics of the SEq: it is among other things
</p>
<p>(a) complex, (b) linear, (c) of first order with respect to time. Stochastic compo-
</p>
<p>nents do not occur, hence the SEq is deterministic. Stationary states (eigenstates
</p>
<p>of energy E) have the time behavior of | (t) = e&minus;i t E | (0).
4. Since H is Hermitian, the time evolution is unitary and thus reversible and norm-
</p>
<p>preserving:
</p>
<p>d
</p>
<p>dt
(t) |(t) =
</p>
<p>&lang;
</p>
<p>(t) |(t) + (t)

</p>
<p>(t)
&rang;
</p>
<p>= 0. (14.4)
</p>
<p>In contrast, the measurement process according to Postulate 2.3 is in general not
</p>
<p>unitary and is therefore irreversible; the norm is not conserved, but instead, one
</p>
<p>has to normalize the new measurement result. In order that the SEq i

</p>
<p>(t)
&rang;
</p>
<p>=
H |(t) holds, e.g. between two measurements, the system must be isolated.
During the measurement, however, the system is not isolated.
</p>
<p>We can also formulate the time evolution with the help of the propagator rather
</p>
<p>than the differential form (14.3), and thus express Postulate 3 in another form:
</p>
<p>Postulate 3&prime;: The state vector at the initial time | (t0) is transferred into the state
| (t) at time t by a unitary operator U (t, t0), called the time evolution operator
or propagator:
</p>
<p>| (t) = U (t, t0) | (t0) . (14.5)
</p>
<p>Remarks:
</p>
<p>1. The unitarity of the propagator ensures the conservation of the norm.
</p>
<p>2. For time-independent H , the propagator can be represented as U = e&minus;i Ht .16
</p>
<p>15We note that (14.3) applies also to a time-dependent H(t). But since we restrict ourselves in this
</p>
<p>whole book to the consideration of time-independent H , we formulate this postulate only for that
</p>
<p>case.
16For the formulation of the propagator as an integral operator, see Appendix I, Vol. 1; an example
</p>
<p>for the case of free motion is found in Chap. 5, Exercise 11.</p>
<p/>
</div>
<div class="page"><p/>
<p>194 14 Postulates of Quantum Mechanics
</p>
<p>3. With the propagator, the reversibility of the time evolution can be seen in a
</p>
<p>particularly simple manner, since we have | (t0) = U&minus;1 (t, t0) | (t).
4. Postulates 3 and 3&prime; are equivalent for our purposes. Strictly speaking, however,
</p>
<p>there is a difference, since U is bounded even if H is not bounded. In this respect,
</p>
<p>the propagator U appears to be more fundamental than the Hamiltonian H .
</p>
<p>We want to stress here again a fundamental difference between classical mechanics
</p>
<p>and quantum mechanics. While classical mechanics describes the time evolution of
</p>
<p>the factual, quantum mechanics (or the SEq) describes the time evolution of the
</p>
<p>possible. In other words, the possibility structure of our universe is not fixed, but is
</p>
<p>a dynamically evolving structure.
</p>
<p>14.2 Some Open Problems
</p>
<p>As stated in the introduction to this chapter, we summarize here once again problems
</p>
<p>of comprehension which are essentially centered around the concept of measurement,
</p>
<p>a completely innocuous notion in classical physics.17 In contrast, measurement seems
</p>
<p>to play a very special role in quantum mechanics. This was already clear in the early
</p>
<p>days of quantum mechanics, and even today the problem is not solved in depth, but
</p>
<p>remains the subject of current discussions.
</p>
<p>One can of course avoid all these problems by adopting the instrumentalist or
</p>
<p>pragmatic point of view; namely, that we live in a classical world, and that the
</p>
<p>postulates are simply computational tools or instructions that work well without
</p>
<p>asserting the claim of representing reality. Niels Bohr put it this way: &ldquo;There is no
</p>
<p>quantum world. There is only an abstract physical description. It is wrong to think
</p>
<p>that the task of physics is to find out how nature is. Physics concerns what we can say
</p>
<p>about nature.&rdquo; With this position (also called the minimal interpretation), one need
</p>
<p>not worry about the problems listed below, let alone about trying to solve them.
</p>
<p>However, many people are dissatisfied with the idea that the fundamental descrip-
</p>
<p>tion of the world should be a handful of rules which are closed to debate. The realistic
</p>
<p>position assumes that the quantum systems of the theory have real counterparts in
</p>
<p>one way or the other. The postulates, together with this point of view, are called the
</p>
<p>standard interpretation (or standard representation) by many authors.
</p>
<p>17In classical mechanics, the properties of a system are always well defined (where we
</p>
<p>always assume a non-pathological phase space). They can be described as functions of the
</p>
<p>phase space variables (i.e. points in phase space) and thus always have a direct three-
</p>
<p>dimensional spatial significance. In quantum mechanics, properties are not always well
</p>
<p>defined and we cannot represent them mathematically as functions of point sets. Small table:
</p>
<p>Classical mechanics Quantum mechanics
</p>
<p>State space Phase space (point set) Hilbert space
</p>
<p>States Points Vectors
</p>
<p>Properties Functions of points Eigenvalues of operators
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Some Open Problems 195
</p>
<p>Regardless of the question of pragmatic vs. realistic, the problematic concepts play
</p>
<p>a fundamental role in the formulation of quantum mechanics (or its postulates), and
</p>
<p>therefore certainly deserve a deeper understanding, or at least a deeper awareness.
</p>
<p>That is why we want to address briefly the key issues in the following.18
</p>
<p>To avoid misunderstandings, we emphasize one remark: On the formal level&mdash;
</p>
<p>technically, so to speak&mdash;quantum mechanics works perfectly, with often impres-
</p>
<p>sively accurate results. In fact, quantum mechanics is one of the most carefully
</p>
<p>examined and well-tested physical theories in existence; it has yet to be falsified
</p>
<p>experimentally.
</p>
<p>So the problem has to do with the level of understanding of quantum mechanics.
</p>
<p>What does all this imply, what does it mean? If we take the above postulates as
</p>
<p>a basis, some open and interrelated questions arise concerning the measurement
</p>
<p>process, which we summarize here briefly. The discussion is continued in Chap. 24
</p>
<p>(decoherence) and Chap. 28 (interpretations), both in Vol. 2.19
</p>
<p>1. Status of the Measurement Process: It is perhaps time to define more pre-
</p>
<p>cisely the concept of measurement. By measurement, we understand the perfor-
</p>
<p>mance of an irreversible operation on a system which determines the status of one
</p>
<p>(or more) physical quantities, namely as a storable number. More remarks on the
</p>
<p>term &lsquo;measurement&rsquo; (or on different but related concepts such as preparation,
</p>
<p>testing, maximum test etc.) can be found in Appendix S, Vol. 1.
</p>
<p>The special status of the measurement has nothing to do with whether one assumes
</p>
<p>pragmatically that values of a physical quantity have a meaning only as the result
</p>
<p>of a measurement, or if one asserts that the postulates are valid also for indi-
</p>
<p>vidual systems (realistic position). With regard to this relationship between the
</p>
<p>measurement and the values of physical quantities, the pragmatically-oriented
</p>
<p>must explain why the concept of &lsquo;measurement&rsquo; plays such a fundamental role in
</p>
<p>quantum mechanics.20 But also the realistically-oriented assign a prominent role
</p>
<p>to the measurement, since it causes the transition from the possible to the actual.
</p>
<p>It remains to investigate when an interaction between two systems A and B is the
</p>
<p>measurement of a physical quantity of A by B, and, in that context, whether and
</p>
<p>how one can describe the measurement in a quantum-mechanical way, i.e. includ-
</p>
<p>ing the measuring apparatus.
</p>
<p>2. Probability: This term enters into the theory through Postulate 2.1. Before the
</p>
<p>measurement, one can in general specify only a probability that a particular
</p>
<p>18&ldquo;But our present (quantum-mechanical) formalism is not purely epistemological; it is a peculiar
</p>
<p>mixture describing in part realities of Nature, in part incomplete human information about Nature -
</p>
<p>all scrambled up by Heisenberg and Bohr into an omelette that nobody has seen how to unscramble.
</p>
<p>Yet we think that the unscrambling is a prerequisite for any further advance in basic physical theory.
</p>
<p>For, if we cannot separate the subjective and objective aspects of the formalism, we cannot know
</p>
<p>what we are talking about; it is just that simple.&rdquo; E.T. Jaynes in: Complexity, Entropy and the Physics
</p>
<p>of Information (ed. Zurek, W.H.) 381 (Addison-Wesley, 1990).
19In anticipation of our further discussion, we want to point out here that there are not answers to
</p>
<p>all the questions, at least not unique answers.
20This is especially true in the traditional view, according to which the measurement apparatus is
</p>
<p>to be regarded as a classical system.</p>
<p/>
</div>
<div class="page"><p/>
<p>196 14 Postulates of Quantum Mechanics
</p>
<p>measurement result will appear. This is true even if the maximum information
</p>
<p>about the system is known. Through the measurement, one and only one of the
</p>
<p>available options is realized. This means, in other words, that an observable gen-
</p>
<p>erally does not have a definite value before a measurement. Correspondingly,
</p>
<p>measurement does not determine the value of an observable which it already
</p>
<p>has, but rather this value is created by the measurement itself&mdash;the measurement
</p>
<p>determines the reality, and not vice versa.
</p>
<p>As explained in Chap. 2 and later on several times, the occurrence of probabilities
</p>
<p>in classical physics means that we do not have sufficient information at our dis-
</p>
<p>posal in order to calculate certain properties explicitly. In quantum mechanics, the
</p>
<p>situation is different. Here, the term &lsquo;probability&rsquo; or &lsquo;objective chance&rsquo; is literally
</p>
<p>a structural element of the theory and stands for the fact that quantum mechanics
</p>
<p>is concerned with possibilities, one of which is then realized by the measurement
</p>
<p>process. From the classical point of view, one would assume that beneath our level
</p>
<p>of formulation there are further, thus far hidden variables, knowledge of which
</p>
<p>would allow us to avoid the use of probabilities. But this assumption has been
</p>
<p>disproved experimentally (at least in its local or non-contextual, i.e. intuitively
</p>
<p>plausible form, see Chaps. 20 and 27, Vol. 2); apparently, we cannot avoid the
</p>
<p>concept of objective chance.
</p>
<p>3. Collapse: How can we explain the change from a superposition to a single state
</p>
<p>as described in Postulate 2.3? What is the mechanism, what is its time frame? Is
</p>
<p>it a fundamental effect or only a pragmatic approximation to the description of
</p>
<p>a quantum system and a measuring apparatus which can be derived in principle
</p>
<p>from the existing formalism? That would obviously be particularly important if
</p>
<p>one does not want to attribute an essential meaning to the &lsquo;measurement&rsquo;, but
</p>
<p>would rather like to see it as simply one of many possible interactions.
</p>
<p>This is not an exotic, constructed effect. We think e.g. of the right circular-
</p>
<p>polarized photon, already invoked several times, which we sent through a linear
</p>
<p>analyzer, only to find it afterwards in the state of e.g. horizontal linear polariza-
</p>
<p>tion. In this process of measurement, the superposition |h+i |v&radic;
2
</p>
<p>is collapsed into
</p>
<p>the state |h.
Of course the answer to the question also depends on what we mean by &lsquo;state&rsquo;.21
</p>
<p>Does it entail a direct description of the system, or only of our knowledge of the
</p>
<p>system? In the latter case, the collapse would represent a change in our knowl-
</p>
<p>edge, by adding more information. Otherwise, there would have to be a way to
</p>
<p>formulate the state reduction in direct physical terms.
</p>
<p>As we will see in the discussion on entangled systems later on, the collapse of
</p>
<p>states is a non-local (i.e. superluminal) effect.
</p>
<p>21For example, &lsquo;state&rsquo; can have the meanings:&mdash;an individual quantum system A,&mdash;our knowledge
</p>
<p>of the properties of the system A,&mdash;the result of a measurement that has been or could be performed
</p>
<p>on system A,&mdash;an ensemble E (real or hypothetical) of identically prepared copies of a system,&mdash;
</p>
<p>our knowledge of the properties of the ensemble E ,&mdash;the results of repeated measurements that
</p>
<p>have been or could be performed on the ensemble E .</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Some Open Problems 197
</p>
<p>4. Two Time Evolutions: Through the measurement (observation), the wave
</p>
<p>function collapses. This change of state is discontinuous, irreversible, not deter-
</p>
<p>ministic in principle and therefore is in contrast to the continuous and reversible
</p>
<p>time evolution of the SEq. Accordingly, we have two very different processes or
</p>
<p>dynamics. This raises the following questions: Are the rules of quantum mechan-
</p>
<p>ics really different for observed and for unobserved systems? If so, why? What is
</p>
<p>an observer, must it be a human observer? Is the observer also subject to the laws
</p>
<p>of quantum mechanics? If so, how there can be irreversible evolutions that con-
</p>
<p>tradict the SEq? We will later find answers to some of these questions, especially
</p>
<p>in Chap. 24, Vol. 2.
</p>
<p>5. The Boundary Between Classical Mechanics and Quantum Mechanics: If the
</p>
<p>measurement is not included in the SEq, does this mean that measurement is a
</p>
<p>non-quantum-mechanical process, so that the measuring apparatus is not subject
</p>
<p>to the rules of quantum mechanics? If that is the case&mdash;which rules apply instead?
</p>
<p>It is a common notion that the measuring apparatus obeys classical rules. Thus
</p>
<p>there would be two areas, the classical domain and quantum mechanics. But where
</p>
<p>is the cut (also called the Heisenberg cut) between quantum mechanics and clas-
</p>
<p>sical mechanics, what begins where, what ends where? From which size, from
</p>
<p>which particle number on is a system no longer described by quantum mechanics,
</p>
<p>but instead by classical mechanics?
</p>
<p>Of course, the idea of starting with small quantum mechanical systems and study-
</p>
<p>ing larger and larger ones to see whether and if so, how, they become &lsquo;more
</p>
<p>classical&rsquo; is obvious. This fails, however, due to the fact that the description of
</p>
<p>larger quantum-mechanical systems with an increasing number of degrees of free-
</p>
<p>dom becomes very quickly enormously complicated, so that a clear relationship
</p>
<p>between classical mechanics and quantum mechanics is difficult to elucidate.
</p>
<p>One can in principle think of three possibilities: (1) classical mechanics includes
</p>
<p>quantum mechanics; (2) classical mechanics and quantum mechanics are on equal
</p>
<p>footing; and (3) quantum mechanics includes classical mechanics; see Fig. 14.1.
</p>
<p>The majority of the physics community favors the third option, i.e. that classical
</p>
<p>mechanics is based on quantum mechanics. But here, also, it has to be clarified
</p>
<p>Fig. 14.1 Schematic figure of possible boundaries quantum mechanics&ndash;classical mechanics. White:
</p>
<p>classical mechanics, black: quantum mechanics</p>
<p/>
</div>
<div class="page"><p/>
<p>198 14 Postulates of Quantum Mechanics
</p>
<p>where the cut is (if it exists at all). In addition, there is the question: If quantum
</p>
<p>mechanics is the basic theory&mdash;why do we never see certain quantum effects in
</p>
<p>the macroscopic world, such as the superposition of states?
</p>
<p>Let us consider the problem again from a different angle. The measuring appara-
</p>
<p>tus consists of atoms and is thus itself a quantum system. In fact, it can interact
</p>
<p>only in this way with the measured quantum object. On the other hand, the mea-
</p>
<p>suring apparatus has to react as a classical system when it finally returns a result.
</p>
<p>Quantum system and classical system&mdash;these are two requirements for the mea-
</p>
<p>suring apparatus, which seem difficult to reconcile.
</p>
<p>These problems of demarcation are particularly clear-cut in quantum cosmology,
</p>
<p>which attempts to describe the entire universe as a single quantum system. If one
</p>
<p>assumes that the universe is isolated and its dynamics (as a giant quantum sys-
</p>
<p>tem) are described by a single SEq (which is deterministic), then this poses the
</p>
<p>question of how measurement can be a process that is performed on a quantum
</p>
<p>system from the outside.
</p>
<p>In any case, in practice, the distinction between a quantum-mechanical system
</p>
<p>and a classical measurement apparatus has been very successful (fapp). Whoever
</p>
<p>works only in a results-oriented manner may be satisfied with the minimal inter-
</p>
<p>pretation, i.e. with the argument that the wavefunction is not a description of real
</p>
<p>objects, but only a tool by which one can obtain the relevant results.22
</p>
<p>We repeat that we will take up these issues again in later chapters. Especially the
</p>
<p>theory of decoherence (Chap. 24, Vol. 2) will alleviate most of these problems.
</p>
<p>22Discussing the nature of the wavefunction is not an ivory-tower topic, but rather the subject
</p>
<p>of ongoing research. For instance, as said above there is the view that the wavefunction reflects
</p>
<p>the partial knowledge which an experimenter has about the system. But such a view is wrong if
</p>
<p>one follows a recently published theorem; it is a no-go theorem which states that if the quantum
</p>
<p>state represents merely information about the real physical state of a system, then experimental
</p>
<p>predictions will be obtained which contradict those of quantum theory. However, this theorem
</p>
<p>depends on the crucial assumption that quantum systems have an objective underlying physical
</p>
<p>state&mdash;an assumption which is controversial. See e.g. Matthew F. Pusey et al., &lsquo;On the reality of the
</p>
<p>quantum state&rsquo;, Nature Physics 8, 475&ndash;478 (2012) or S. Mansfield, &lsquo;Reality of the quantum state:
</p>
<p>Towards a stronger -ontology theorem&rsquo;, Phys. Rev. A 94, 042124 (Oct 2016).
</p>
<p>Apart from that, we note that even though there is no explanation of the wavefunction in every-
</p>
<p>day terms, it is measurable. To date, the experimental determination of wavefunctions (i.e. modulus
</p>
<p>and phase or real and imaginary parts) has been accomplished by means of certain indirect methods
</p>
<p>(called tomographic methods). However, recently a method was presented for measuring wavefunc-
</p>
<p>tions directly. In it, a special technique is used, called weak measurement. See Jeff S. Lundeen et al.,
</p>
<p>&lsquo;Direct measurement of the wave function&rsquo;, Nature 474, 188&ndash;191 (2011). We note that these mea-
</p>
<p>surements are performed on an ensemble; it is impossible to determine the completely unknown
</p>
<p>wavefunction of a single system. For other experimental methods see e.g. G.C. Knee, &lsquo;Towards
</p>
<p>optimal experimental tests on the reality of the quantum state&rsquo;, New Journal of Physics 19, 023004,
</p>
<p>https://doi.org/10.1088/1367-2630/aa54ab (Feb 2017).</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1088/1367-2630/aa54ab">https://doi.org/10.1088/1367-2630/aa54ab</a></div>
</div>
<div class="page"><p/>
<p>14.3 Concluding Remarks 199
</p>
<p>14.3 Concluding Remarks
</p>
<p>14.3.1 Postulates of Quantum Mechanics as a Framework
</p>
<p>We have distilled out the postulates from considerations of simple example systems.
</p>
<p>This is possible precisely because the postulates do not depend on the specific system,
</p>
<p>but constitute something like the general framework or the general rules of quantum
</p>
<p>mechanics. In other words, the postulates are valid for all possible systems (referred
</p>
<p>to our frame of consideration), simple as well as complicated ones.
</p>
<p>Hence it is clear that the postulates cannot act as instructions for the practical
</p>
<p>calculation of a physical problem. In order to do this, one has to determine the state
</p>
<p>space H and the Hamiltonian H for the system under consideration. Of course, it
</p>
<p>then becomes relevant which concrete physical system is selected, how it is modelled
</p>
<p>physically, which precision is required, and so on.
</p>
<p>We have described e.g. neutrino oscillations without further ado in a two-
</p>
<p>dimensional space. This is obviously a very crude model, but it is entirely adequate
</p>
<p>for the intended purpose. In the analytical approach, we obtained the Hamiltonian by
</p>
<p>using the correspondence principle,23 i.e. by replacement of the classical quantities
</p>
<p>(r, p) which occur in the energy by
(
</p>
<p>r, 
i
&nabla;
)
</p>
<p>. This allows us to represent a non-
</p>
<p>relativistic quantum object in a scalar potential, while vector potentials, interactions
</p>
<p>between a number of quantum objects, relativistic effects such as spin, etc. are not
</p>
<p>considered. We can see our simple model as the beginning of a &lsquo;hierarchy of models&rsquo;;
</p>
<p>we address this point briefly again in Chap. 17, Vol. 2.
</p>
<p>In short, the choice of H and H always means that one operates with certain mo-
</p>
<p>dels and approximations.24 The postulates, however, are strict. This is schematically
</p>
<p>indicated in Fig. 14.2.
</p>
<p>14.3.2 Outlook
</p>
<p>The postulates in the form in which we have presented them give the foundation of
</p>
<p>quantum mechanics, but there are some complements needed. In subsequent chapters
</p>
<p>in Vol. 2, we will become acquainted with the following three:
</p>
<p>1. We will extend the concept of state and also look at states that are no longer
</p>
<p>represented as vectors of an (extended) Hilbert space (so-called mixed states,
</p>
<p>keyword density operator, Chap. 22, Vol. 2).
</p>
<p>23We repeat the remark that this principle has a mainly heuristic value. A more convincing method
</p>
<p>is e.g. the introduction of position and momentum operators by means of symmetry transformations
</p>
<p>(see Chap. 21, Vol. 2, and Appendix L, Vol. 2).
24&ldquo;Although this may be seen as a paradox, all exact science is dominated by the idea of approxi-
</p>
<p>mation.&rdquo; (Bertrand Russell).</p>
<p/>
</div>
<div class="page"><p/>
<p>200 14 Postulates of Quantum Mechanics
</p>
<p>Fig. 14.2 The postulates as
</p>
<p>framework for the quantum
</p>
<p>mechanical description of
</p>
<p>physical systems
</p>
<p>Postulates
</p>
<p>System
physical model, precision
</p>
<p>2. The systems considered so far are isolated, i.e. they are not coupled to any envi-
</p>
<p>ronment whatsoever. We will consider in the following also systems that are
</p>
<p>composed of several interacting subsystems (keyword open systems, Chap. 24,
</p>
<p>Vol. 2).
</p>
<p>3. By an extension to open systems, we want to trace the separate role of the mea-
</p>
<p>surement (i.e. the projection postulate 2.3) back to the other postulates (keyword
</p>
<p>decoherence, Chap. 24, Vol. 2).
</p>
<p>But first, in the initial chapters of Vol. 2, we will fill in the conceptual framework
</p>
<p>given above with some applications, some specialized subjects and extensions.
</p>
<p>14.4 Exercises
</p>
<p>1. Given are an observable A and a state |. Show by means of Postulates (2.1) and
(2.2) that the expected result of a measurement of A is given by A = | A |.
To simplify the discussion, we consider an observable A whose eigenvalues are
</p>
<p>discrete and nondegenerate and whose eigenvectors form a CONS, A |n = an |n.
2. Show that the operator sx + sz is Hermitian, but does not represent a measurable
</p>
<p>physical quantity if understood literally, i.e., as the instruction to measure the
</p>
<p>x-component plus (and) the z-component of the spin. The spin matrices si are
</p>
<p>related to the Pauli matrices i by si = 2 i .
3. (An example concerning projections, probabilities and expectation values.) The
</p>
<p>angular momentum operator L for angular momentum 1 can be represented in
</p>
<p>the vector space C3 by the following matrices (see Chap. 16, Vol. 2):
</p>
<p>L x =

&radic;
</p>
<p>2
</p>
<p>
</p>
<p>
</p>
<p>0 1 0
</p>
<p>1 0 1
</p>
<p>0 1 0
</p>
<p>
</p>
<p> ; L y =

&radic;
</p>
<p>2
</p>
<p>
</p>
<p>
</p>
<p>0 &minus;i 0
i 0 &minus;i
0 i 0
</p>
<p>
</p>
<p> ; L z = 
</p>
<p>
</p>
<p>
</p>
<p>1 0 0
</p>
<p>0 0 0
</p>
<p>0 0 &minus;1
</p>
<p>
</p>
<p>
</p>
<p>(14.6)
</p>
<p>(a) Which measured results are possible in a measurement of L i (i = x, y, z)?
(b) What are the corresponding eigenvectors for L z?
</p>
<p>(c) What are the probabilities of measuring the results +, 0,&minus; on the state
</p>
<p>| =
</p>
<p>
</p>
<p>
</p>
<p>1
</p>
<p>i
</p>
<p>&minus;2
</p>
<p>
</p>
<p>? (14.7)</p>
<p/>
</div>
<div class="page"><p/>
<p>14.4 Exercises 201
</p>
<p>4. Given the state
</p>
<p>|v =
|x1 e&minus;it + |x2 e&minus;2it&radic;
</p>
<p>2
(14.8)
</p>
<p>with normalized and mutually orthogonal states |xi . We measure the x1 compo-
nent of |v . After the measurement, we have
</p>
<p>|n = |x1 e&minus;it (14.9)
</p>
<p>Illustrate this state reduction by considering the change in the real or imaginary
</p>
<p>part of |.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix A
</p>
<p>Abbreviations and Notations
</p>
<p>For a better overview, we collect here some abbreviations and specific notations.
</p>
<p>Abbreviations
</p>
<p>ala Algebraic approach
</p>
<p>ana Analytical approach
</p>
<p>ClM Classical mechanics
</p>
<p>CONS Complete orthonormal system
</p>
<p>CSCO Complete system of commuting observables
</p>
<p>DEq Differential equation
</p>
<p>EPR Einstein&ndash;Podolsky&ndash;Rosen paradox
</p>
<p>fapp &lsquo;Fine for all practical purposes&rsquo;
</p>
<p>MZI Mach&ndash;Zehnder interferometer
</p>
<p>ONS Orthonormal system
</p>
<p>PBS Polarizing beam splitter
</p>
<p>QC Quantum computer
</p>
<p>QM Quantum mechanics
</p>
<p>QZE Quantum Zeno effect
</p>
<p>SEq Schr&ouml;dinger equation
</p>
<p>Operators
</p>
<p>There are several different notations for an operator which is associated with a phys-
</p>
<p>ical quantity A; among others: (1) A, that is the symbol itself, (2) A, notation with
</p>
<p>hat (3) A, calligraphic typefont, (4) Aop, notation with index. It must be clear from
</p>
<p>the context what is meant in each case.
</p>
<p>For special quantities such as the position x , one also finds the uppercase notation
</p>
<p>X for the corresponding operator.
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>203</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>204 Appendix A: Abbreviations and Notations
</p>
<p>The Hamiltonian and the Hadamard Transformation
</p>
<p>We denote the Hamiltonian by H . With reference to questions of quantum informa-
</p>
<p>tion, H stands for the Hadamard transformation.
</p>
<p>Vector Spaces
</p>
<p>We denote a vector space by V , a Hilbert space by H.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix B
</p>
<p>Units and Constants
</p>
<p>B.1 Systems of Units
</p>
<p>Units are not genuinely natural (although some are called that), but rather are
</p>
<p>man-made and therefore in some sense arbitrary. Depending on the application or
</p>
<p>scale, there are various options that, of course, are fixed precisely in each case by
</p>
<p>convention.
</p>
<p>Those unit systems in which some fundamental constants are set equal to 1 and
</p>
<p>are dimensionless are generally referred to as natural unit systems. As we mentioned,
</p>
<p>the word &lsquo;natural&rsquo; here is understood as part of the name and not as a descriptive
</p>
<p>adjective. We consider the following natural units: Planck units, the unit system of
</p>
<p>high-energy physics (theoretical units, also called natural units), and the unit system
</p>
<p>of atomic physics (atomic units).
</p>
<p>B.1.1 Planck Units
</p>
<p>Here, the speed of light c, the Planck constant , the gravitational constant G as well
</p>
<p>as the Boltzmann constant kB and the electric field constant (multiplied by 4), 40,
</p>
<p>are all set equal to 1. Their relations to the SI values are shown in the following table:
</p>
<p>Quantity Expression Value (SI)
</p>
<p>Charge qP =
&radic;
</p>
<p>40c 1.876 &times; 10&minus;18 C
Length lP =
</p>
<p>&radic;
G
c3
</p>
<p>1.616 &times; 10&minus;35 m
Mass m P =
</p>
<p>&radic;
c
G
</p>
<p>2.177 &times; 10&minus;8 kg
Temperature TP = m P c
</p>
<p>2
</p>
<p>kB
1.417 &times; 1032 K
</p>
<p>Time tP = lPc 5.391 &times; 10&minus;44 s
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>205</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>206 Appendix B: Units and Constants
</p>
<p>The Planck scale probably marks a limit to the applicability of the known laws of
</p>
<p>physics. Distances substantially smaller than the Planck length cannot be considered
</p>
<p>as meaningful. The same holds true for processes that are shorter than the Planck
</p>
<p>time. Because of lP = ctP , such a process would take place on a distance scale
that would be smaller than the Planck length. By comparison, the LHC accelerator
</p>
<p>has a spatial resolution of about 10&minus;19 m; its accessible energy is of the order of
ELHC = 10 TeV.
</p>
<p>B.1.2 Theoretical Units (Units of High-Energy Physics)
</p>
<p>Here, c and  are set equal to 1; the other constants remain unchanged. The unit of
</p>
<p>energy is not determined by the choice of c and ; it is usually expressed in eV (or
</p>
<p>MeV, GeV etc.). Energy and mass have the same unit; this applies also to space and
</p>
<p>time.
</p>
<p>Quantity Unit Expression Value (SI)
</p>
<p>Energy eV 1.602 &times; 10&minus;19 J
Length 1/eV c/eV 1.973 &times; 10&minus;7 m
Mass eV eV/c2 1.783 &times; 10&minus;36 kg
Temperature eV eV/kB 1.160 &times; 104 K
Time 1/eV /eV 6.582 &times; 10&minus;16 s
</p>
<p>In SI units, c = 3.1616 &middot;10&minus;26 Jm = 0.1973 GeVfm. Since c = 1 in theoretical
units, we have the rule of thumb
</p>
<p>1 fm (SI) = 5/GeV (TE) (B.1)
</p>
<p>B.1.3 Atomic Units
</p>
<p>In atomic units, e = me =  = 1. These units, which are related to properties of the
electron and the hydrogen atom, are mainly used in atomic and molecular physics.
</p>
<p>All quantities are formally dimensionless which are multiples of the basic units. If
</p>
<p>they are not dimensionless in SI units, they are generally marked by the formal &lsquo;unit
</p>
<p>character&rsquo; a.u. (the dots are part of the unit symbol).
</p>
<p>The Hartree unit of energy is twice the ionization potential of the hydrogen atom.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix B: Units and Constants 207
</p>
<p>Quantity Atomic unit Value (SI)
</p>
<p>Angular momentum Planck constant  1.055 &times; 10&minus;34 Js
Charge Elementary charge e 1.602 &times; 10&minus;19 C
Energy Hartree energy Eh , H 4.360 &times; 10&minus;18 J
Length Bohr radius a0 = mc 5.292 &times; 10&minus;11 m
Mass Electron mass me 9.109 &times; 10&minus;31 kg
Time unit Atomic time unit, 1 a.t.u. = 
</p>
<p>Eh
2.419 &times; 10&minus;17 s
</p>
<p>B.1.4 Units of Energy
</p>
<p>Energy is a central concept of physics, and this manifests itself among other things
</p>
<p>in the multitude of units used. The most common are summarized in the following
</p>
<p>table. Thereby, one uses E =  = h and c = .
</p>
<p>Unit Conversion factor Comment
</p>
<p>eV 1
</p>
<p>Joule 1.602 &times; 10&minus;19 J
Kilowatt hour 4.451 &times; 10&minus;26 kWh
Calorie 3.827 &times; 10&minus;20 cal
Wavelength in nanometer 1239.85 nm From E = hc/
Frequency in Hertz 2.41797 &times; 1014 Hz From E = h/T (T = time)
Wave number 8065.48 cm&minus;1 From E = hc/
Temperature 11, 604.5 K From E = kB T (T = temperature)
Rydberg 0.07350 Ry Ionization potential of the H atom
</p>
<p>Hartree 0.03675 H
</p>
<p>Energy equivalent mass, E/c 1.783 &times; 10&minus;36 kg
</p>
<p>B.2 Some Constants
</p>
<p>Derived units in the SI:
</p>
<p>1 N (Newton) = 1 kgms&minus;2; 1 W (Watt) = 1 Js&minus;1; 1 C (Coulomb) = 1 As
1 F (Farad) 0 1 AsV&minus;1; 1 T (Tesla) = 1 Vsm&minus;2; 1 WB (Weber) = 1 Vs
</p>
<p>Important constants in eV:
</p>
<p>h = 4, 1357 &middot; 10&minus;16 eVs;  = 6, 5821 &middot; 10&minus;16 eVs
mec
</p>
<p>2 = 0, 511 MeV; mec22 = 2 &middot; 13, 6 eV; mec24 = 1, 45 &middot; 10&minus;3 eV</p>
<p/>
</div>
<div class="page"><p/>
<p>208 Appendix B: Units and Constants
</p>
<p>Quantity Symbol Value Unit
</p>
<p>Speed of light in vacuum c 299, 792, 458 (exact) ms&minus;1
</p>
<p>Magnetic field constant o 4 &times; 10&minus;7 (exact) TmA&minus;1
Electric field constant o 8.85419 &times; 10&minus;12 Fm&minus;1
Planck constant h 6.62618 &times; 10&minus;34 Js
(Reduced) Planck constant  1.05459 &times; 10&minus;34 Js
Elementary charge e 1.60219 &times; 10&minus;19 C
Newtonian gravitational constant G 6.672 &times; 10&minus;11 m3 kg&minus;1s&minus;2
Boltzmann constant kB 1.381 &times; 10&minus;23 JK&minus;1
Rest mass of the electron me 9.10953 &times; 10&minus;31 kg
Rest mass of the proton m p 1.67265 &times; 10&minus;27 kg
Fine structure constant  1/137.036
</p>
<p>Rydberg constant R 2.17991 &times; 10&minus;18 J
Bohr radius ao 5.29177 &times; 10&minus;11 m
Magnetic flux quantum o 2.068 &times; 10&minus;15 Wb
Stefan&ndash;Boltzmann constant  5.671 &times; 10&minus;8 Wm&minus;2 K&minus;4
Magnetic moment of the electron e 9.28483 &times; 10&minus;24 JT&minus;1
Magnetic moment of the proton p 1.41062 &times; 10&minus;26 JT&minus;1
</p>
<p>Large and small: Size comparisons
</p>
<p>Size of the universe 1028 m; diameter of an atomic nucleus 10&minus;15 m; Planck
length 10&minus;35 m
</p>
<p>Ratio universe/Planck length 1063
</p>
<p>B.3 Dimensional Analysis
</p>
<p>One advantage of physics over mathematics lies in the existence of physical units.
</p>
<p>Thus, one can test results, conjectures etc. by checking their units. Can e.g. the
</p>
<p>expression T = 2&radic;l &middot; g be correct? No, the unit of the left-hand side is s, while at
the right hand side it is m/s. This principle can be applied constructively (so-called
</p>
<p>dimensional analysis, Buckingham -theorem). As an example, consider once more
</p>
<p>the pendulum. The system data are the mass of the pendulum bob, the length of
</p>
<p>the string and the acceleration of gravity. A time (oscillation time or period) can be
</p>
<p>represented only by the combination
&radic;
</p>
<p>l/g. So we must have T &sim; &radic;l/g.
In addition, the physical units show that an expression such as eir (so long as r
</p>
<p>has the unit m) can not be right; alone for dimensional reasons, it must read eikr ,
</p>
<p>where k has the unit m&minus;1.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix B: Units and Constants 209
</p>
<p>B.4 Powers of 10 and Abbreviations
</p>
<p>deci, d &minus;1 deka, da 1
centi, c &minus;2 hecto, h 2
milli, m &minus;3 kilo, k 3
micro,  &minus;6 mega, M 6
nano, n &minus;9 giga, G 9
pico, p &minus;12 tera, T 12
femto, f &minus;15 peta, P 15
atto, a &minus;18 exa, E 18
zepto, z &minus;21 zetta, Z 21
yocto, y &minus;24 yotta, Y 24
</p>
<p>B.5 The Greek Alphabet
</p>
<p>Name Lower case Upper case Name Lower case Upper case
</p>
<p>alpha  A nu  N
</p>
<p>beta  B xi  
</p>
<p>gamma   omicron o O
</p>
<p>delta   pi  
</p>
<p>epsilon ,  E rho  P
</p>
<p>zeta  Z sigma ,  (coda) 
</p>
<p>eta  H tau  T
</p>
<p>theta ,   upsilon  Y
</p>
<p>iota  I phi , 
</p>
<p>kappa  K chi  X
</p>
<p>lambda  	 psi  

</p>
<p>mu  M omega  </p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix C
</p>
<p>Complex Numbers
</p>
<p>C.1 Calculating with Complex Numbers
</p>
<p>In the algebraic representation, a complex number z is given by
</p>
<p>z = a + ib (C.1)
</p>
<p>where a &isin; R is the real part and b &isin; R the imaginary part of z (and not ib); a = Re (z)
and b = Im (z). The number i is the imaginary unit defined by i2 = &minus;1, for which
the &lsquo;normal&rsquo; rules of calculation hold, e.g. ib = bi . The conjugate complex number
z&lowast; is defined by1
</p>
<p>z&lowast; = a &minus; ib. (C.2)
</p>
<p>Accordingly, a real number u can be characterized by u = u&lowast;, an imaginary number
v by v = &minus;v&lowast;.
</p>
<p>Addition, subtraction and multiplication of two complex numbers zk = ak + ibk
follow familiar rules:
</p>
<p>z1 &plusmn; z2 = a1 &plusmn; a2 + i (b1 &plusmn; b2)
z1 &middot; z2 = a1a2 &minus; b1b2 + i (a1b2 + a2b1) (C.3)
</p>
<p>and in particular for c &isin; R, we have
</p>
<p>c &middot; z2 = ca2 + icb2. (C.4)
</p>
<p>For division, we use the complex conjugate:
</p>
<p>z1
</p>
<p>z2
= z1
</p>
<p>z2
</p>
<p>z&lowast;2
z&lowast;2
</p>
<p>= a1a2 + b1b2 + i (&minus;a1b2 + a2b1)
a22 + b22
</p>
<p>. (C.5)
</p>
<p>1The notation z is also common.
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>211</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>212 Appendix C: Complex Numbers
</p>
<p>Fig. C.1 Algebraic
</p>
<p>representation in the
</p>
<p>complex plane
</p>
<p>Im
</p>
<p>z*
</p>
<p>Rea
</p>
<p>b
</p>
<p>z=a+ib
</p>
<p>Fig. C.2 Polar
</p>
<p>representation in the
</p>
<p>complex plane
z =  sin 
</p>
<p>z
</p>
<p>z
</p>
<p>z
</p>
<p>z ( )+i
</p>
<p>
</p>
<p>
</p>
<p>Im
</p>
<p>Re
</p>
<p>
</p>
<p>cos
</p>
<p>sin
</p>
<p>cos
</p>
<p>Complex numbers can be represented in an intuitive manner in the Gaussian
</p>
<p>plane (complex plane), see Fig. C.1. For example, we see that the conjugate complex
</p>
<p>number z&lowast; is the reflection of z through the real axis.
In many cases (and almost always in quantum mechanics), the algebraic form
</p>
<p>(C.1) is not particularly useful. A different representation is more suitable, namely
</p>
<p>polar coordinates in the complex plane. A complex number z is then determined by
</p>
<p>the length of the radius vector and the angle  between the radius vector and the
</p>
<p>positive real axis; cf. Fig. C.2. The length of the radius vector of a complex number
</p>
<p>z = a + ib is called its modulus |z| or complex norm2
</p>
<p>|z| =
&radic;
</p>
<p>a2 + b2 &ge; 0. (C.6)
</p>
<p>We have the relations:
</p>
<p>z &middot; z&lowast; = |z|2 ; |z1 &middot; z2| = |z1| &middot; |z2| . (C.7)
</p>
<p>With |z| and , we can write3
</p>
<p>z = |z| (cos+ i sin) . (C.8)
</p>
<p>Evidently (and quite intuitively), the complex number does not change if we add
</p>
<p>a multiple of 2 to ,  &rarr;  + 2m with m &isin; Z. All angles are allowed, of
</p>
<p>2Also called its absolute value. |z|2 is often called the absolute square.
3Also called trigonometric form of the complex numbers.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix C: Complex Numbers 213
</p>
<p>course, but the principal value of the angle is confined to the interval &minus; &lt;  &le; .
The ambiguity of the angle is typical of complex numbers (there is nothing similar
</p>
<p>in R), and is used constructively, for example in the theory of functions (of complex
</p>
<p>variables) and other areas.
</p>
<p>With the help of the fundamental equation4
</p>
<p>ei x = cos x + i sin x (C.9)
</p>
<p>we obtain finally the exponential representation of a complex number z
</p>
<p>z = |z| ei. (C.10)
</p>
<p>We know how to determine the modulus of a complex number z = a + ib.
The determination of the angle , called the phase or argument, is somewhat more
</p>
<p>complicated. Equation (C.8) suggests the relation  = arctan b
a
</p>
<p>(and it is often given
</p>
<p>in collections of formulas, etc.). But this cannot always hold true, since otherwise
</p>
<p>z1 = 3 + 4i and z2 = &minus;3 &minus; 4i would have the same phase, which is obviously
wrong. The correct relation can be formulated differently; a possibility is5
</p>
<p> = arctan b
a
+ |b|
</p>
<p>b
</p>
<p>1&minus; |a|
a
</p>
<p>2
; a, b 	= 0
</p>
<p> = 1&minus;
|a|
a
</p>
<p>2
 for a 	= 0, b = 0 and  = |b|
</p>
<p>b

2
</p>
<p>for a = 0, b 	= 0.
(C.11)
</p>
<p>Of course, one can add 2m to the phase if necessary. The only number without a
</p>
<p>well-defined phase is the complex number 0. It has the value 0 , while its phase is
</p>
<p>indeterminate.
</p>
<p>We want to point out some relations which are sometimes quite handy. Namely,
</p>
<p>as is seen from (C.9), we have
</p>
<p>i = ei/2; &minus;1 = ei; 1 = e2i (C.12)
</p>
<p>where, of course, 2im can be added the exponent. Due to (C.12), a factor i can be
</p>
<p>interpreted as a phase (or phase shift) of /2 or 90; for &minus;1, we have accordingly 
or 180.
</p>
<p>We can utilize the ambiguity of the phase in a constructive manner, e.g. to find
</p>
<p>the roots of a number. We demonstrate this by means of a concrete example: Find
</p>
<p>all the numbers z for which
</p>
<p>z3 = 7 (C.13)
</p>
<p>holds. Taking the modulus on both sides leads to |z|3 = 7, with the solution |z| = 71/3.
We can therefore write z = |z| ei = 71/3ei and thus obtain
</p>
<p>4The Feynman Lectures on Physics, 5th Edition, 1970, Vol I, p.22&ndash;10, &ldquo;We summarize with this,
</p>
<p>the most remarkable formula in mathematics: ei = cos  + i sin . This is our jewel.&rdquo;
5For a positive real part, we have  = arctan b
</p>
<p>a
; for a negative real part, we have to add or subtract
</p>
<p>, depending on the sign of the imaginary part, to obtain the principal value.</p>
<p/>
</div>
<div class="page"><p/>
<p>214 Appendix C: Complex Numbers
</p>
<p>Fig. C.3 Third roots of 1 Im
</p>
<p>Re
</p>
<p>e3i = 1. (C.14)
</p>
<p>For the right-hand side, we write down all the complex possibilities that exist for 1,
</p>
<p>namely
</p>
<p>1 = e2im; m &isin; Z. (C.15)
</p>
<p>It follows that
</p>
<p>e3i = e2im; m &isin; Z (C.16)
</p>
<p>and this leads to
</p>
<p> = 0,&plusmn;2
3
,&plusmn;4
</p>
<p>3
,&plusmn;6
</p>
<p>3
, . . . (C.17)
</p>
<p>or, more compactly,
</p>
<p> = m mod(2). (C.18)
</p>
<p>If we restrict ourselves to the principal values, we obtain the three solutions
</p>
<p> = 0,&plusmn;2
3
 or z1 = 71/3, z2,3 = 71/3e&plusmn;i2/3. (C.19)
</p>
<p>In the complex plane, this can be understood quite readily: Taking the third or nth root
</p>
<p>means&mdash;with regard to the phase&mdash;dividing the full circle by 3 or n. In the example
</p>
<p>of the &lsquo;third root&rsquo;, we obtain the angle 0 and &plusmn;120 (or 0, 120, 240); see Fig. C.3.
In this context, we recall the fundamental theorem of algebra which states that each
</p>
<p>polynomial of order n has n zeros.
</p>
<p>Finally, we note that the number system is complete with the complex numbers&mdash;
</p>
<p>no arithmetic operation leads out of it.6 Expressions such as i i or (a + ib)(c+id) may
be unfamiliar, but they are reasonable and calculable (see exercises).
</p>
<p>6In contrast, e.g. subtraction leads out of the natural numbers and division out of whole numbers.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix C: Complex Numbers 215
</p>
<p>C.2 Are Complex Numbers Less Intuitive than Real
</p>
<p>Numbers?
</p>
<p>The resistance to complex numbers7 is often justified by the claim that complex
</p>
<p>numbers are not intuitive, are unreal and too abstract. This is hard to comprehend,
</p>
<p>if one thinks of the perfectly clear description of taking the roots in the complex
</p>
<p>plane. Presumably, this claim has to do less with the facts themselves and more with
</p>
<p>psychology&mdash;perhaps, since the adjectives &lsquo;complex&rsquo; and &lsquo;imaginary&rsquo; suggest that
</p>
<p>these numbers are difficult (complex) and not really existent (imaginary).
</p>
<p>Several hundred years ago, when the child &lsquo;complex number&rsquo; was baptized, it
</p>
<p>was perhaps even wise to choose those names to avoid discussions with the more
</p>
<p>conservative. Another example of this prudence is the question of whether the earth
</p>
<p>orbits the sun. Today we know that the sun does not move around the earth, but we
</p>
<p>still say, &lsquo;the sun rises&rsquo;, knowing that it is an outdated convention of speech (which
</p>
<p>also has its own beauty), far from taking it literally. And just as the sun does not rise
</p>
<p>in reality, complex numbers are not difficult or counterintuitive. If a point on the real
</p>
<p>line is intuitively clear, then so is a point in the complex plane.
</p>
<p>The real problem probably lies somewhere else, but we do not perceive it anymore,
</p>
<p>perhaps because we have become accustomed to it. It is that we can not think of a
</p>
<p>point in the mathematical sense, i.e. an entity with dimension zero. A number (on a
</p>
<p>numerical axis) corresponds to a point. And this point may be hiding an incredible
</p>
<p>amount of information, for instance in a rational number (i.e. a fraction), let alone in
</p>
<p>an irrational number.8 The following little digression is intended to show this.
</p>
<p>Suppose we want to save the contents of all the books in all the libraries of the
</p>
<p>world with minimum space requirements. To this end, we encode all existing char-
</p>
<p>acters, numbers, punctuation marks, Chinese characters, hieroglyphics, cuneiform
</p>
<p>characters&mdash;everything used for writing. If we assume that the number of all those
</p>
<p>diverse characters and symbols is less than a million, we can characterize each of
</p>
<p>them by a six-digit decimal number (digits 0&ndash;9).9 And now we translate one book
</p>
<p>after another into our new code, simply by inserting the appropriate numbers for the
</p>
<p>characters; the codes for different books are simply written down one after another.
</p>
<p>When we are finished, we have a very long number N before us.10 If we want to
</p>
<p>7By the way, complex numbers are not new-fangled stuff; they have been in use for more than
</p>
<p>400 years. Apparently, the northern Italian mathematician Rafael Bombelli (1526&ndash;1572) in his
</p>
<p>work L&rsquo;Algebrawas the first to introduce imaginary numbers.
8We can grasp small natural numbers directly as a set, but to distinguish immediately between 39,
</p>
<p>40 and 41 will overcharge most of us (in mother ducks, this limit seems to be reached at 6 or 7
</p>
<p>ducklings). Large numbers are completely beyond our comprehension (how much time would it
</p>
<p>take to count to a million or a billion?). Fractions are also difficult, even very simple ones (&lsquo;just
</p>
<p>give me the smaller half&rsquo;).
9Of course, a binary or hexadecimal notation would work as well. And if there are more than a
</p>
<p>million symbols, we need seven decimal digits or more. But this changes nothing in our main
</p>
<p>argument.
10For a rough estimate of the order of magnitude, we assume that a line encompasses 70 characters
</p>
<p>and a page 50 lines. Hence, a book of 300 pages contains about a million characters. In a library with</p>
<p/>
</div>
<div class="page"><p/>
<p>216 Appendix C: Complex Numbers
</p>
<p>write it down the &lsquo;usual&rsquo; way, we need more or less six times as much space as for
</p>
<p>the originals.
</p>
<p>A more space-saving procedure would be the following: ahead of N , we write a
</p>
<p>zero and a decimal point, thus obtaining a number between 0 and 1, namelyM = 0.N
which we mark exactly on a ruler&mdash;if we could in fact do this. It is essential at this
</p>
<p>point that we can save within a rational number (i.e. a fraction!) between 0 and 1
</p>
<p>the content of all the libraries of the world. And of course there is still more: In a
</p>
<p>neighborhood of M, there exists a number where every &lsquo;i&rsquo; is exchanged with an &lsquo;o&rsquo;,
</p>
<p>and another where every book is encoded backwards except the first 1,000 positions;
</p>
<p>another which contains all the books that ever appeared or will appear in the future.
</p>
<p>This holds analogously for encoding music. Is there a number that contains all the
</p>
<p>works that Mozart would have written if he had lived for 10, 20, 30 years longer?
</p>
<p>Is there a number that includes all the books or music that have not been written to
</p>
<p>date, and another that includes everything that will never be written?
</p>
<p>Who says that there is no poetry in numbers? And in an irrational number, which
</p>
<p>consists of an infinite number of decimal places without a period, arbitrarily more
</p>
<p>information (in fact, unimaginably more) can of course be stored.
</p>
<p>The real problem lies in the fact that we can imagine small spots, but not a
</p>
<p>mathematical point. Given this, it is not really understandable why it should be so
</p>
<p>much harder to imagine a point not on a straight line, but on a plane. Complex
</p>
<p>numbers are not more &lsquo;difficult&rsquo; or &lsquo;counterintuitive&rsquo; than real numbers&mdash;rather the
</p>
<p>opposite, because operations like taking roots have a clear and intuitive meaning in
</p>
<p>the complex plane.
</p>
<p>C.3 Exercises
</p>
<p>1. Given z1 = 3 &minus; i , z2 = 3 + i , z3 = 1 &minus; 3i , z4 = 1 + 3i ; sketch the points in the
complex plane and calculate their complex norms.
</p>
<p>2. Given z1 = 3 &minus; 4i and z2 = &minus;1 + 2i ; calculate |z1|, |z2|, z1 &plusmn; z2, z1 &middot; z2, z1z2 ,
1
z1
.
</p>
<p>3. Given
</p>
<p>z = 3 &minus; 4i
6 + i
</p>
<p>&radic;
2
&middot; (8 &minus; 7i)+ 6i. (C.20)
</p>
<p>Determine z&lowast;.
4. Write the following complex numbers in the form ei:
</p>
<p>10 million books, we must therefore encode around 1013 characters and will obtain a number of
</p>
<p>6 &middot; 1013 digits. Suppose that on the world average, there is one such library per 40, 000 inhabitants.
Then, assuming a world population of eight billion, we have globally about 200, 000 libraries. (Of
</p>
<p>course, this is certainly an overoptimistic estimate, not only in view of the situation in developing
</p>
<p>countries. But here, we are considering only rough orders of magnitude.) This would result in the
</p>
<p>&lsquo;literary number&rsquo; N = 1.2 &middot; 1019&mdash;hence something around 1019 (although of course many books
would be repeated numerous times, and their copies could be eliminated to reduce the number). For
</p>
<p>comparison, the Loschmidt constant NL , which specifies the number of molecules per unit volume
</p>
<p>of an ideal gas under normal conditions, is NL = 2.7 &middot; 1019/cm3.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix C: Complex Numbers 217
</p>
<p>3 + 4i; 3 &minus; 4i; &minus; 3 + 4i; &minus; 3 &minus; 4i. (C.21)
</p>
<p>5. Given z = &minus;1&plusmn;i
&radic;
</p>
<p>3
2
</p>
<p>. Represent the numbers in the complex plane. Calculate z3.
</p>
<p>6. What is the polar representation of z = 1&plusmn;i
&radic;
</p>
<p>3
2
</p>
<p>and z = &minus;1&plusmn;i
&radic;
</p>
<p>3
2
</p>
<p>?
</p>
<p>7. Show that all complex numbers of the form ei lie on the unit circle around the
</p>
<p>origin.
</p>
<p>8. Show that the multiplication of a complex number by i means rotating this
</p>
<p>number by 
2
</p>
<p>or 90.
9. Show that ei = cos+i sin by means of the power series of the trigonometric
</p>
<p>functions.
</p>
<p>Solution:
</p>
<p>ei =
&infin;&sum;
</p>
<p>n=0
</p>
<p>inn
</p>
<p>n! =
&infin;&sum;
</p>
<p>n=0
</p>
<p>i2n2n
</p>
<p>(2n)! +
&infin;&sum;
</p>
<p>n=0
</p>
<p>i2n+12n+1
</p>
<p>(2n + 1)!
</p>
<p>=
&infin;&sum;
</p>
<p>n=0
</p>
<p>(&minus;1)n 2n
(2n)! + i
</p>
<p>&infin;&sum;
n=0
</p>
<p>(&minus;1)n 2n+1
(2n + 1)! = cos+ i sin.
</p>
<p>(C.22)
</p>
<p>10. Show that ei = cos+ i sin by taking derivatives.
Solution:
</p>
<p>(
ei
</p>
<p>)&prime; = iei = i cos&minus; sin = (cos+ i sin)&prime;. (C.23)
</p>
<p>11. Show that
</p>
<p>cos x = e
i x + e&minus;i x
</p>
<p>2
; sin x = e
</p>
<p>i x &minus; e&minus;i x
2i
</p>
<p>. (C.24)
</p>
<p>Solution:
</p>
<p>ei x = cos x + i sin x; e&minus;i x = cos x &minus; i sin x;&rarr; ei x + e&minus;i x = 2 cos x . (C.25)
</p>
<p>12. Given a function
</p>
<p>f = (a + ib)eikx &minus; (a &minus; ib)e&minus;ikx . (C.26)
</p>
<p>This function may be brought into the form
</p>
<p>f = A sin B. (C.27)
</p>
<p>Determine A and B.
</p>
<p>Solution: With a + ib =
&radic;
</p>
<p>a2 + b2eid and d = arctan b
a
</p>
<p>, it follows that11:
</p>
<p>11This value for d holds for a &gt; 0; otherwise, d is given by the more complex expression C.11.</p>
<p/>
</div>
<div class="page"><p/>
<p>218 Appendix C: Complex Numbers
</p>
<p>f =
&radic;
</p>
<p>a2 + b2ei(kx+d) &minus;
&radic;
</p>
<p>a2 + b2e&minus;i(kx+d) = 2i
&radic;
</p>
<p>a2 + b2 sin (kx + d) .
(C.28)
</p>
<p>13. Using only ei x = cos x + i sin x , show that
</p>
<p>sin 2x = 2 sin x &middot; cos x; cos 2x = cos2 x &minus; sin2 x (C.29)
</p>
<p>is valid.
</p>
<p>Solution:
</p>
<p>2 sin x &middot; cos x = 2e
i x &minus; e&minus;i x
</p>
<p>2i
</p>
<p>ei x + e&minus;i x
2
</p>
<p>= e
2i x &minus; e&minus;2i x
</p>
<p>2i
= sin 2x . (C.30)
</p>
<p>14. Using ei x = cos x + i sin x , determine the coefficients a and b in the equation
</p>
<p>cos3  = a cos+ b cos 3. (C.31)
</p>
<p>15. Show that:
</p>
<p>sin2 x = 1
2
(1 &minus; cos 2x) ; cos2 x = 1
</p>
<p>2
(1 + cos 2x) . (C.32)
</p>
<p>16. Show that:
</p>
<p>sin 3x = 3 sin x &minus; 4 sin3 x; cos 3x = 4 cos3 x &minus; 3 cos x . (C.33)
</p>
<p>17. Is the equation
</p>
<p>(cos x + i sin x)n = cos nx + i sin nx (C.34)
</p>
<p>correct?
</p>
<p>Solution: Yes, since
</p>
<p>(cos x + i sin x)n =
(
ei x
</p>
<p>)n = einx = cos nx + i sin nx . (C.35)
</p>
<p>18. We start from
</p>
<p>ei x = A cos x + B sin x (C.36)
</p>
<p>where A and B are to be determined (rule of the game: we have only this equation
</p>
<p>and do not know at this point that in fact, ei x = cos x + i sin x holds). First show
that A = 1. Then show that B = &plusmn;i must hold, using complex conjugation.
How can one fix the sign of B?
</p>
<p>Solution: For x = 0, we have
</p>
<p>ei &middot;0 = e0 = 1 = A &middot; 1 + B &middot; 0 = A. (C.37)
</p>
<p>In addition, we have with A = 1</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix C: Complex Numbers 219
</p>
<p>e&minus;i x = ei(&minus;x) = cos x &minus; B sin x . (C.38)
</p>
<p>It follows that
</p>
<p>1 = ei x e&minus;i x = (cos x + B sin x) (cos x &minus; B sin x) = cos2 x &minus; B2 sin2 x .
(C.39)
</p>
<p>With cos2 x + sin2 x = 1 (Pythagoras), it follows that B2 = &minus;1, and therefore
B = &plusmn;i . For the decision as to which sign holds, additional information is
needed (power series, derivatives).
</p>
<p>19. Calculate ei

2
</p>
<p>m for m &isin; Z.
20. Given
</p>
<p>z8 = 16; z3 = &minus;8; (C.40)
</p>
<p>calculate all solutions.
</p>
<p>21. Calculate i i and (a + ib)(c+id).
Solution: With i = ei( 2 +2m);m = 0,&plusmn;1,&plusmn;2, . . ., it follows that
</p>
<p>i i =
(
</p>
<p>ei(

2
+2m)
</p>
<p>)i
= e&minus;( 2 +2m);m = 0,&plusmn;1,&plusmn;2, . . . (C.41)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix D
</p>
<p>Calculus I
</p>
<p>In the following, some general basic relations from calculus are compiled.
</p>
<p>D.1 One Real Independent Variable
</p>
<p>D.1.1 The Taylor Expansion
</p>
<p>If a function is differentiable sufficiently often, we can write it as a Taylor series,12
</p>
<p>i.e. the function at a point a + x can be expressed as the sum of the functions and
derivatives at the neighboring point a:
</p>
<p>f (a + x)
= f (a)+ x
</p>
<p>1! f
(1) (a)+ x2
</p>
<p>2! f
(2) (a)+ &middot; &middot; &middot; + xn
</p>
<p>n! f
(N ) (a)+ x N
</p>
<p>(N+1)! f
(N+1) (a + x)
</p>
<p>=
N&sum;
</p>
<p>n=0
</p>
<p>xn
</p>
<p>n! f
(n) (a)+ RN .
</p>
<p>(D.1)
</p>
<p>The term RN is called remainder term or Lagrange remainder; it is 0 &lt;  &lt; 1.
</p>
<p>Under suitable conditions, the remainder term vanishes for N &rarr; &infin; and the sum
converges, so that we may write
</p>
<p>f (a + x) =
&infin;&sum;
</p>
<p>n=0
</p>
<p>xn
</p>
<p>n! f
(n) (a) (D.2)
</p>
<p>We thus have written the function as power series
</p>
<p>&infin;&sum;
</p>
<p>k=0
ck x
</p>
<p>k (with cn = f (n) (a) /n!).
</p>
<p>12Brook Taylor, British mathematician, 1685&ndash;1731.
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>221</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>222 Appendix D: Calculus I
</p>
<p>In general, a power series does not converge for all x , but only for |x | &lt; . The
convergence radius  can be determined by
</p>
<p> = lim
n&rarr;&infin;
</p>
<p>
cn
</p>
<p>cn+1
</p>
<p> ;  = limn&rarr;&infin;
1&radic;|cn|
</p>
<p>(D.3)
</p>
<p>if the limits exist. For x =  and x = &minus;, the power series may converge or diverge.
The three &lsquo;most important&rsquo; functions ex , cos x and sin x have power series with
</p>
<p>infinite convergence radii and are therefore particularly well behaved:
</p>
<p>ex =
&infin;&sum;
</p>
<p>n=0
</p>
<p>xn
</p>
<p>n! ; cos x =
&infin;&sum;
</p>
<p>n=0
(&minus;1)n x
</p>
<p>2n
</p>
<p>(2n)! ; sin x =
&infin;&sum;
</p>
<p>n=0
(&minus;1)n x
</p>
<p>2n+1
</p>
<p>(2n + 1)! . (D.4)
</p>
<p>In other words, in the exponent of the exponential function, we can insert &lsquo;anything&rsquo;
</p>
<p>for x , and this expression is always defined by the power series as long as xn exists.
</p>
<p>For example, eM for a square matrix M is defined as eM = &sum;&infin;n=0 M
n
</p>
<p>n! , while the
exponential function of a non-square matrix is not defined.
</p>
<p>Examples of power series with finite radius of convergence ( = 1) are (1 + x)
as well as ln (1 + x) and arctan x :
</p>
<p>(1 + x) = 1+ 
1! x+
</p>
<p> (&minus; 1)
2! x
</p>
<p>2+&middot; &middot; &middot; =
&infin;&sum;
</p>
<p>n=0
</p>
<p>(

</p>
<p>n
</p>
<p>)
xn; |x | &le; 1 for  &gt; 0|x | &lt; 1 for  &lt; 0 (D.5)
</p>
<p>and
</p>
<p>ln (1 + x) = x &minus; x
2
</p>
<p>2
+ x
</p>
<p>3
</p>
<p>3
&minus; &middot; &middot; &middot; =
</p>
<p>&infin;&sum;
</p>
<p>n=1
(&minus;1)n+1 x
</p>
<p>n
</p>
<p>n
; &minus; 1 &lt; x &le; 1 (D.6)
</p>
<p>and
</p>
<p>arctan x = x &minus; x
3
</p>
<p>3
+ x
</p>
<p>5
</p>
<p>5
&minus; &middot; &middot; &middot; =
</p>
<p>&infin;&sum;
</p>
<p>n=0
(&minus;1)n x
</p>
<p>2n+1
</p>
<p>2n + 1 ; &minus; 1 &lt; x &lt; 1. (D.7)
</p>
<p>By means of the power series, one can also find very practical approximations for
</p>
<p>functions, if the x values are sufficiently small; for example
</p>
<p>ex &asymp; 1 + x; cos x &asymp; 1 &minus; x2
2
; sin x &asymp; x &minus; x3
</p>
<p>6
</p>
<p>(1 + x) &asymp; 1 + x; ln (1 + x) &asymp; x &minus; x2
2
; arctan x &asymp; x &minus; x3
</p>
<p>3
</p>
<p>(D.8)
</p>
<p>For sufficiently small x , often the first term is sufficient. For instance, we have
</p>
<p>sin x &asymp; x in the interval |x | &lt; 0.077 (corresponding to an angle of 4.4) with an
accuracy of less than or equal to one part per thousand.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix D: Calculus I 223
</p>
<p>D.1.2 L&rsquo;H&ocirc;pital&rsquo;s Rule
</p>
<p>This rule concerns indefinite expressions such as 0
0
</p>
<p>or &infin;&infin; . If we have e.g. limx&rarr;x0
f (x) =
</p>
<p>0 and lim
x&rarr;x0
</p>
<p>g(x) = 0, then the expression lim
x&rarr;x0
</p>
<p>f (x)
</p>
<p>g(x)
has this form. The rule of
</p>
<p>L&rsquo;H&ocirc;pital13 states that under this assumption, it holds that:
</p>
<p>lim
x&rarr;x0
</p>
<p>f (x)
</p>
<p>g(x)
= lim
</p>
<p>x&rarr;x0
</p>
<p>f &prime;(x)
</p>
<p>g&prime;(x)
. (D.9)
</p>
<p>One can easily prove this by substituting the corresponding Taylor expansions around
</p>
<p>x0 for the functions. If the right-hand side of the equation is again an indefinite term,
</p>
<p>we apply the rule again. Example:
</p>
<p>lim
x&rarr;0
</p>
<p>sin x
</p>
<p>x
= lim
</p>
<p>x&rarr;0
cos x
</p>
<p>1
= 1; lim
</p>
<p>x&rarr;0
ex &minus; 1 &minus; x
</p>
<p>x2
= lim
</p>
<p>x&rarr;0
ex &minus; 1
</p>
<p>2x
= lim
</p>
<p>x&rarr;0
ex
</p>
<p>2
= 1
</p>
<p>2
.
</p>
<p>(D.10)
</p>
<p>In indefinite terms of other types one has to rearrange accordingly. We sketch this
</p>
<p>only symbolically:
</p>
<p>0 &middot; &infin; = 0 &middot; 1
0
</p>
<p>or
1
</p>
<p>&infin; &middot;&infin;; &infin;&minus;&infin; = &infin;
(
</p>
<p>1 &minus; &infin;&infin;
)
. (D.11)
</p>
<p>Example:
</p>
<p>lim
x&rarr;0
</p>
<p>x ln x = lim
x&rarr;0
</p>
<p>ln x
</p>
<p>1/x
= lim
</p>
<p>x&rarr;0
1/x
</p>
<p>&minus;1/x2 = &minus; limx&rarr;0 x = 0. (D.12)
</p>
<p>In terms of the form 00 or similar expressions, one takes the logarithm. Example:
</p>
<p>lim
x&rarr;0
</p>
<p>x x = lim
x&rarr;0
</p>
<p>ex ln x = lim
x&rarr;0
</p>
<p>e&minus;x = 1. (D.13)
</p>
<p>D.1.3 Mean Value Theorem for Integration
</p>
<p>We consider the definite integral
</p>
<p>I =
b&int;
</p>
<p>a
</p>
<p>f (x)dx (D.14)
</p>
<p>13Guillaume Fran&ccedil;ois Antoine, Marquis de L&rsquo;H&ocirc;pital (also written L&rsquo;Hospital), French mathemati-
</p>
<p>cian, 1661&ndash;1704.</p>
<p/>
</div>
<div class="page"><p/>
<p>224 Appendix D: Calculus I
</p>
<p>Fig. D.1 On the mean value
</p>
<p>theorem of integration
</p>
<p>x
</p>
<p>f(x)
</p>
<p>f
</p>
<p>f
</p>
<p>max
</p>
<p>min
</p>
<p>a b
</p>
<p>where the function f (x) is sufficiently well behaved. Then it holds that
</p>
<p>I =
b&int;
</p>
<p>a
</p>
<p>f (x)dx = f () (b &minus; a) wi th  &isin; [a, b] . (D.15)
</p>
<p>We know about  only that it lies within the interval [a, b]; where, exactly, is not
</p>
<p>determined by the theorem.
</p>
<p>The reason for the theorem is that if fmin and fmax are the minimal and maximal
</p>
<p>values of f (x) in the interval, then we have (see Fig. D.1):
</p>
<p>fmin &middot; (b &minus; a) &le; I &le; fmax &middot; (b &minus; a) . (D.16)
</p>
<p>Accordingly, the exact value of I must exist for some intermediate value fmin &le;
f () &le; fmax, i.e. for a &le;  &le; b.
</p>
<p>D.2 Several Independent Variables
</p>
<p>D.2.1 Differentiation
</p>
<p>The partial derivative of a function of several independent variables f (x1, x2, . . .)
</p>
<p>with respect to e.g. x1 is defined as
</p>
<p>&part; f (x1, x2, . . .)
</p>
<p>&part;x1
= lim
</p>
<p>&rarr;0
f (x1 + , x2, . . .)&minus; f (x1, x2, . . .)
</p>
<p>
(D.17)
</p>
<p>In this definition, the variables x2, x3, . . . play the role of constants.
</p>
<p>The use of the symbol &part; has been adopted to make clear from the outset that it is a
</p>
<p>partial derivative. Besides &part;
&part;x
</p>
<p>, there are also notations such as &part;x or the like; instead
</p>
<p>of
&part; f
&part;x
</p>
<p>, one can also write fx or f|x .</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix D: Calculus I 225
</p>
<p>The term
&part; f (x1,x2,...)
</p>
<p>&part;x1
denotes the change of the function if x1 is varied and all other
</p>
<p>independent variables stay fixed. By varying all variables simultaneously, we obtain
</p>
<p>the total change of the function, which can be expressed as the total derivative:
</p>
<p>d f (x1, x2, . . .) =
&part; f (x1, x2, . . .)
</p>
<p>&part;x1
dx1 +
</p>
<p>&part; f (x1, x2, . . .)
</p>
<p>&part;x2
dx2 + &middot; &middot; &middot; . (D.18)
</p>
<p>Higher derivatives are defined accordingly; for example, the term
</p>
<p>&part;2 f (x1, x2, . . .)
</p>
<p>&part;xi&part;x j
&equiv; &part;xi&part;x j f (x1, x2, . . .) (D.19)
</p>
<p>means that we first have to take the derivative of the function with respect to x j ,
</p>
<p>and then with respect to xi (execution of the steps from right to left). The order of
</p>
<p>the derivatives does not play a role iff the first and second partial derivatives of f
</p>
<p>are continuous; in this case, we have &part;xi&part;x j f (x1, x2, . . .) = &part;x j&part;xi f (x1, x2, . . .).
We will always assume that all functions are sufficiently smooth and thus satisfy
</p>
<p>this condition, so that we never have to pay attention to the order of the derivatives.
</p>
<p>A counterexample is found in the exercises.
</p>
<p>By the way, we generally assume in quantum mechanics that we can interchange
</p>
<p>limit processes (differentiation, integration, summation). As a concrete example, we
</p>
<p>consider the equation
</p>
<p>d
</p>
<p>dt
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p> (x, t) dx =
&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>&part;
</p>
<p>&part;t
 (x, t) dx (D.20)
</p>
<p>which we used in Chap. 7 (time invariance of the total probability). We can take the
</p>
<p>differentiation into the integral iff  is continuous with respect to x and is differen-
</p>
<p>tiable with respect to t , and &part;/&part;t is continuous with respect to x .
</p>
<p>Similar considerations would have to be made in other cases; but we will not do
</p>
<p>so, presuming on quantum mechanics&rsquo; &lsquo;good nature&rsquo; (and, of course, knowing that
</p>
<p>others have already provided the necessary proofs).
</p>
<p>D.2.2 Taylor Series
</p>
<p>For a function of several variables, the Taylor series reads
</p>
<p>f (x1 + a1, . . . , xn + an) =
&infin;&sum;
</p>
<p>j=0
</p>
<p>1
</p>
<p>j !
</p>
<p>[
n&sum;
</p>
<p>k=1
ak
</p>
<p>&part;
</p>
<p>&part;xk
</p>
<p>] j
f (x1, . . . , xn) (D.21)</p>
<p/>
</div>
<div class="page"><p/>
<p>226 Appendix D: Calculus I
</p>
<p>or, in compact form (for the definition of &nabla;, see below):
</p>
<p>f (r + a) =
&infin;&sum;
</p>
<p>j=0
</p>
<p>1
</p>
<p>j ! (a &middot;&nabla;)
j f (r) . (D.22)
</p>
<p>The first terms of the expansion are
</p>
<p>f (r + a) = f (r)+ (a &middot;&nabla;) f (r)+ 1
2
(a &middot;&nabla;) (a &middot;&nabla;) f (r)+ &middot; &middot; &middot; (D.23)
</p>
<p>D.2.3 Vector Algebra
</p>
<p>A closer look at the total derivative (D.18) shows that the right-hand side can be
</p>
<p>expressed as a scalar product14:
</p>
<p>d f = &part; f
&part;x1
</p>
<p>dx1 +
&part; f
</p>
<p>&part;x2
dx2 + &middot; &middot; &middot; =
</p>
<p>(
&part; f
</p>
<p>&part;x1
,
&part; f
</p>
<p>&part;x2
, . . .
</p>
<p>)
&middot; (dx1, dx2, . . .) (D.24)
</p>
<p>The second vector can be written as dr = (dx1, dx2, . . .). For the first vector, the
notation (
</p>
<p>&part; f
</p>
<p>&part;x1
,
&part; f
</p>
<p>&part;x2
, . . .
</p>
<p>)
= &nabla; f (D.25)
</p>
<p>has become established; here the nabla operator (or briefly just nabla) is formally a
</p>
<p>vector with the components15
</p>
<p>&nabla; =
(
</p>
<p>&part;
</p>
<p>&part;x1
,
</p>
<p>&part;
</p>
<p>&part;x2
, . . .
</p>
<p>)
(D.26)
</p>
<p>Being a vector operator, nabla can be applied to scalar functions f (x1, x2, . . .)
</p>
<p>and vector functions F (x1, x2, . . .). The application to f is referred to as gradient
</p>
<p>of f and is also written as grad f :
</p>
<p>&nabla; f =
(
&part; f
</p>
<p>&part;x1
,
&part; f
</p>
<p>&part;x2
, . . .
</p>
<p>)
= grad f. (D.27)
</p>
<p>14Here, we use a row vector (dx1, dx2, . . .) and not the corresponding column vector; this is only
</p>
<p>for typographical convenience.
15The symbol &nabla; is not a Hebrew letter, but an upside down Delta. This sign was named &lsquo;nabla&rsquo; in
</p>
<p>the nineteenth century, because it resembles an ancient harp (n&eacute;vel in Hebrew, n&aacute;bla in Greek). The
</p>
<p>&lsquo;D&rsquo; (as in Delta) in the Hebrew alphabet is the &lsquo;Daleth&rsquo; , the &lsquo;N&rsquo; (as in nabla) the &lsquo;Nun&rsquo; .</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix D: Calculus I 227
</p>
<p>The application to F is called the divergence and is also written as div F:
</p>
<p>&nabla; &middot; F = &part;F1
&part;x1
</p>
<p>+ &part;F2
&part;x2
</p>
<p>+ &middot; &middot; &middot; = div F. (D.28)
</p>
<p>In three dimensions, one can also take the vector product of &nabla; with a vector function;
</p>
<p>this is called the curl:
</p>
<p>&nabla; &times; F =
(
&part;F3
</p>
<p>&part;x2
&minus; &part;F2
</p>
<p>&part;x3
,
&part;F1
</p>
<p>&part;x3
&minus; &part;F3
</p>
<p>&part;x1
,
&part;F2
</p>
<p>&part;x1
&minus; &part;F1
</p>
<p>&part;x2
</p>
<p>)
= curl F. (D.29)
</p>
<p>We note the different character of the operations:
</p>
<p>gradient:
</p>
<p>divergence:
</p>
<p>curl:
</p>
<p>&nabla;scalar &rarr; vector
&nabla; &middot; vector &rarr; scalar
</p>
<p>&nabla; &times; vector &rarr; vector.
(D.30)
</p>
<p>The two notations with &nabla; and with grad &minus; div &minus; curl are equivalent; each one has
advantages and disadvantages.
</p>
<p>Multiple Applications
</p>
<p>For appropriate combinations, multiple applications of the nabla operator are defined:
</p>
<p>&nabla; f &nabla; (&nabla; &times; f ) = div grad f = &nabla;2 f &nabla; &times; (&nabla; f ) = curl grad f = 0
&nabla; &times; F &nabla; (&nabla; &times; F) = grad div F &nabla; &times; (&nabla; &times; F) not defined
&nabla; &times; F &nabla; &times; (&nabla; &times; F) = div curl F = 0 &nabla; &times; (&nabla; &times; F) = curl curl F == &nabla; (&nabla; &times; F)&minus; &nabla;2F = grad div F&minus; &nabla;2F
</p>
<p>Here, &nabla;2 is the Laplacian (Laplace&rsquo;s differential operator), &nabla;2 = &part;2
&part;x21
</p>
<p>+ &part;2
&part;x22
</p>
<p>+ &part;2
&part;x23
</p>
<p>.
</p>
<p>As we see from the table, it is defined by &nabla; &middot; (&nabla; f ) = &nabla;2 f .
Integral Theorems
</p>
<p>For completeness, we note the three main integral theorems in short form.
</p>
<p>The line integral of a gradient field on a curve C depends only on its endpoints:
</p>
<p>r2&int;
</p>
<p>r1,C
</p>
<p>&nabla; f (r) dr = f (r2)&minus; f (r1) . (D.31)
</p>
<p>Given a volume V which is enclosed by a surface S; the orientation of the sur-
</p>
<p>face is such that its normal points outwards. Then the Gaussian integral theorem
</p>
<p>(or Gauss-Ostrogradski, also called the divergence theorem) reads:
</p>
<p>&int;
</p>
<p>V
</p>
<p>(&nabla; &middot; F (r)) dV =
&int;
</p>
<p>S
</p>
<p>F (r) &middot; dS (D.32)</p>
<p/>
</div>
<div class="page"><p/>
<p>228 Appendix D: Calculus I
</p>
<p>Fig. D.2 Polar coordinates
</p>
<p>r
</p>
<p>
</p>
<p>y
</p>
<p>x
</p>
<p>where the left-hand side is a volume integral and the right-hand side is a surface
</p>
<p>integral.
</p>
<p>Given an oriented surface S which is enclosed by a curve C (the sense of rotation
</p>
<p>is chosen so that it forms a right-hand helix with the surface normal). Then the Stokes
</p>
<p>integral theorem (also called the curl theorem) reads
</p>
<p>&int;
</p>
<p>S
</p>
<p>(&nabla; &times; F (r)) &middot; dS =
&int;
</p>
<p>C
</p>
<p>F (r) &middot; dr (D.33)
</p>
<p>where the left-hand side is a surface integral and the right-hand side is a line integral.
</p>
<p>D.3 Coordinate Systems
</p>
<p>D.3.1 Polar Coordinates
</p>
<p>The polar coordinates (r,) are related to the Cartesian coordinates (x, y) by
</p>
<p>x = r cos
y = r sin 0 &le; r; 0 &le;  &le; 2 (D.34)
</p>
<p>Here, r is the distance from the origin, see Fig. D.2.
</p>
<p>As a simple application, we derive the transformation equations for an active rota-
</p>
<p>tion. If we rotate a point described by (r,) through an angle , its new coordinates
</p>
<p>are
x = r cos
y = r sin &rarr;
</p>
<p>x &prime; = r cos (+ )
y&prime; = r sin (+ ) . (D.35)
</p>
<p>It follows from the addition theorems of trigonometric functions16 that
</p>
<p>16sin(+ ) = sin cos + cos sin , cos(+ ) = cos cos &minus; sin sin .</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix D: Calculus I 229
</p>
<p>x &prime; = r cos cos &minus; r sin sin = x cos &minus; y sin
y&prime; = r sin cos + r cos sin = y cos + x sin (D.36)
</p>
<p>or, in compact form, (
x &prime;
</p>
<p>y&prime;
</p>
<p>)
=
</p>
<p>(
cos &minus; sin
sin cos
</p>
<p>)(
x
</p>
<p>y
</p>
<p>)
(D.37)
</p>
<p>as the representation of an active rotation through the angle .
</p>
<p>D.3.2 Cylindrical Coordinates
</p>
<p>Cylindrical coordinates (,, z) are not used in this text, but we show them here for
</p>
<p>the sake of completeness. They are related to the Cartesian coordinates (x, y, z) by
</p>
<p>x =  cos
y =  sin
z = z.
</p>
<p>0 &le; ; 0 &le;  &le; 2 (D.38)
</p>
<p>Here,  is the distance from the z axis (cylinder axis), see Fig. D.3.
</p>
<p>The transformation between the two coordinate systems is carried out with the
</p>
<p>help of e.g.
</p>
<p>&part;
</p>
<p>&part;
= &part;x
</p>
<p>&part;
</p>
<p>&part;
</p>
<p>&part;x
+ &part;y
</p>
<p>&part;
</p>
<p>&part;
</p>
<p>&part;y
= cos &part;
</p>
<p>&part;x
+ sin &part;
</p>
<p>&part;y
(D.39)
</p>
<p>and analogously for the other variables.
</p>
<p>For the unit vectors, it follows that:
</p>
<p>e =
</p>
<p>

</p>
<p>cos
</p>
<p>sin
</p>
<p>0
</p>
<p>
 ; e =
</p>
<p>

&minus; sin
cos
</p>
<p>0
</p>
<p>
 ; ez =
</p>
<p>

</p>
<p>0
</p>
<p>0
</p>
<p>1
</p>
<p>
 . (D.40)
</p>
<p>Fig. D.3 Cylinder
</p>
<p>coordinates
</p>
<p>x 
</p>
<p>z
</p>
<p>
</p>
<p>y</p>
<p/>
</div>
<div class="page"><p/>
<p>230 Appendix D: Calculus I
</p>
<p>Fig. D.4 Spherical
</p>
<p>coordinates
</p>
<p>x
</p>
<p>y
</p>
<p>r
</p>
<p>
</p>
<p>z
</p>
<p>
</p>
<p>D.3.3 Spherical Coordinates
</p>
<p>The spherical coordinates17 (r,,) are related to the Cartesian coordinates (x, y, z)
</p>
<p>by
</p>
<p>x = r cos sin 
y = r sin sin 
z = r cos.
</p>
<p>0 &le; r; 0 &le;  &le; ; 0 &le;  &le; 2 (D.41)
</p>
<p>Here, r is the distance from the origin, see Fig. D.4.
</p>
<p>The reversed relations are given by
</p>
<p>r =
&radic;
</p>
<p>x2 + y2 + z2
 = arccos z&radic;
</p>
<p>x2+y2+z2
 = arctan x
</p>
<p>y
.
</p>
<p>(D.42)
</p>
<p>The transformation between the two coordinate systems is carried out with the
</p>
<p>help of e.g.
</p>
<p>&part;
</p>
<p>&part;r
= &part;x
</p>
<p>&part;r
</p>
<p>&part;
</p>
<p>&part;x
+ &part;y
</p>
<p>&part;r
</p>
<p>&part;
</p>
<p>&part;y
+ &part;z
</p>
<p>&part;r
</p>
<p>&part;
</p>
<p>&part;z
= cos sin  &part;
</p>
<p>&part;x
+ sin sin  &part;
</p>
<p>&part;y
+ cos &part;
</p>
<p>&part;z
(D.43)
</p>
<p>and analogously for the other variables. For convenience, we give the transformation
</p>
<p>matrix:
</p>
<p>17Also called spherical polar coordinates.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix D: Calculus I 231
</p>
<p>&part;r
</p>
<p>&part;x
= sin  cos &part;r
</p>
<p>&part;y
= sin  sin &part;r
</p>
<p>&part;z
= cos
</p>
<p>&part;
</p>
<p>&part;x
= cos cos
</p>
<p>r
</p>
<p>&part;
</p>
<p>&part;y
= cos sin
</p>
<p>r
</p>
<p>&part;
</p>
<p>&part;z
= &minus; sin 
</p>
<p>r
</p>
<p>&part;
</p>
<p>&part;x
= &minus; sin
</p>
<p>r sin 
</p>
<p>&part;
</p>
<p>&part;y
= cos
</p>
<p>r sin 
</p>
<p>&part;
</p>
<p>&part;z
= 0.
</p>
<p>(D.44)
</p>
<p>For the unit vectors, it follows that:
</p>
<p>er =
</p>
<p>

</p>
<p>cos sin 
</p>
<p>sin sin 
</p>
<p>cos
</p>
<p>
 ; e =
</p>
<p>

</p>
<p>cos cos
</p>
<p>sin cos
</p>
<p>&minus; sin 
</p>
<p>
 ; e =
</p>
<p>

&minus; sin
cos
</p>
<p>0
</p>
<p>
 . (D.45)
</p>
<p>The components of a vector A which is written as A = Axex + Ayey + Azez in
Cartesian coordinates are:
</p>
<p>A =Arer + Ae + Ae (D.46)
</p>
<p>with
</p>
<p>Ar = A &middot; er = Ax cos sin + Ay sin sin + Az cos (D.47)
</p>
<p>and analogously for the other components.
</p>
<p>Volume Elements, Surface Elements
</p>
<p>In Cartesian coordinates, the infinitesimal volume element is
</p>
<p>dV = dx dy dz. (D.48)
</p>
<p>In cylindrical coordinates, we have
</p>
<p>dV = d d dz =  d d dz; (D.49)
</p>
<p>and in spherical coordinates (see Fig. D.5),
</p>
<p>dV = dr rd r sin d = r2 sin  dr d d. (D.50)
</p>
<p>In particular, for radially symmetrical functions, we have
</p>
<p>&int;
f (r)dV =
</p>
<p>&infin;&int;
</p>
<p>0
</p>
<p>dr
</p>
<p>&int;
</p>
<p>0
</p>
<p>d
</p>
<p>2&int;
</p>
<p>0
</p>
<p>d r2 sin  f (r) = 4
&infin;&int;
</p>
<p>0
</p>
<p>dr r2 f (r). (D.51)
</p>
<p>For a surface element in spherical coordinates, it follows with (D.5) that
</p>
<p>d f = r2 sin  d d (D.52)</p>
<p/>
</div>
<div class="page"><p/>
<p>232 Appendix D: Calculus I
</p>
<p>Fig. D.5 Volume element in
</p>
<p>spherical coordinates
d
</p>
<p>r
</p>
<p>dr
</p>
<p>r d 
</p>
<p>r sin 
</p>
<p>and, making use of d f = r2d, the solid angle d is given by
</p>
<p>d = sin  d d. (D.53)
</p>
<p>The Gradient and Laplace Operators in Spherical Coordinates
</p>
<p>Gradient:
</p>
<p>The gradient of a function f (r) can be written in Cartesian coordinates as
</p>
<p>&nabla; f (r) = &part; f
&part;x
</p>
<p>ex +
&part; f
</p>
<p>&part;y
ey +
</p>
<p>&part; f
</p>
<p>&part;z
ez . (D.54)
</p>
<p>With the above transformations, we obtain in spherical coordinates
</p>
<p>&nabla; f (r) = &part; f
&part;r
</p>
<p>er +
1
</p>
<p>r
</p>
<p>&part; f
</p>
<p>&part;
e +
</p>
<p>1
</p>
<p>r sin 
</p>
<p>&part; f
</p>
<p>&part;
e. (D.55)
</p>
<p>In particular, for a function g(r) depending on r only, we have
</p>
<p>&nabla;g(r) = dg(r)
dr
</p>
<p>er =
dg(r)
</p>
<p>dr
</p>
<p>r
</p>
<p>r
. (D.56)
</p>
<p>The Laplacian:
</p>
<p>The Laplacian in Cartesian coordinates reads18:
</p>
<p>&nabla;2 = &part;
2
</p>
<p>&part;x2
+ &part;
</p>
<p>2
</p>
<p>&part;y2
+ &part;
</p>
<p>2
</p>
<p>&part;z2
. (D.57)
</p>
<p>With the transformations (D.43), it can be converted to spherical coordinates:
</p>
<p>18When there are different sets of coordinates, it is standard to index the nabla operators accordingly:
</p>
<p>&nabla;2r =
&part;2
</p>
<p>&part;x2
+ &part;
</p>
<p>2
</p>
<p>&part;y2
+ &part;
</p>
<p>2
</p>
<p>&part;z2
; &nabla;2r&prime; =
</p>
<p>&part;2
</p>
<p>&part;x &prime;2
+ &part;
</p>
<p>2
</p>
<p>&part;y&prime;2
+ &part;
</p>
<p>2
</p>
<p>&part;z&prime;2
.
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix D: Calculus I 233
</p>
<p>&nabla;2 = &part;
2
</p>
<p>&part;r2
+ 2
</p>
<p>r
</p>
<p>&part;
</p>
<p>&part;r
+ 1
</p>
<p>r2
</p>
<p>[
1
</p>
<p>sin 
</p>
<p>&part;
</p>
<p>&part;
</p>
<p>(
sin 
</p>
<p>&part;
</p>
<p>&part;
</p>
<p>)
+ 1
</p>
<p>sin2 
</p>
<p>&part;2
</p>
<p>&part;2
</p>
<p>]
. (D.58)
</p>
<p>Using the angular momentum operator l, this expression can be written more com-
</p>
<p>pactly. Because of lx = i
(
</p>
<p>y &part;
&part;z
</p>
<p>&minus; z &part;
&part;y
</p>
<p>)
etc., we have in spherical coordinates
</p>
<p>lx =

</p>
<p>i
</p>
<p>(
&minus; sin &part;
</p>
<p>&part;
&minus; cot  cos &part;
</p>
<p>&part;
</p>
<p>)
</p>
<p>ly =

</p>
<p>i
</p>
<p>(
cos
</p>
<p>&part;
</p>
<p>&part;
&minus; cot  sin &part;
</p>
<p>&part;
</p>
<p>)
</p>
<p>lz =

</p>
<p>i
</p>
<p>&part;
</p>
<p>&part;
</p>
<p>(D.59)
</p>
<p>and therefore
</p>
<p>l2 = &minus;2
[
</p>
<p>1
</p>
<p>sin 
</p>
<p>&part;
</p>
<p>&part;
</p>
<p>(
sin 
</p>
<p>&part;
</p>
<p>&part;
</p>
<p>)
+ 1
</p>
<p>sin2 
</p>
<p>&part;2
</p>
<p>&part;2
</p>
<p>]
. (D.60)
</p>
<p>Thus, the Laplacian can be written as
</p>
<p>&nabla;2 = &part;
2
</p>
<p>&part;r2
+ 2
</p>
<p>r
</p>
<p>&part;
</p>
<p>&part;r
&minus; l
</p>
<p>2
</p>
<p>2r2
. (D.61)
</p>
<p>For the sum of the first two terms, there are also other common expressions:
</p>
<p>&part;2
</p>
<p>&part;r2
+ 2
</p>
<p>r
</p>
<p>&part;
</p>
<p>&part;r
= 1
</p>
<p>r2
</p>
<p>&part;
</p>
<p>&part;r
r2
</p>
<p>&part;
</p>
<p>&part;r
= 1
</p>
<p>r
</p>
<p>&part;2
</p>
<p>&part;r2
r. (D.62)
</p>
<p>One can also introduce the radial momentum pr . It is defined as
</p>
<p>pr =

</p>
<p>i
</p>
<p>1
</p>
<p>r
</p>
<p>&part;
</p>
<p>&part;r
r = 
</p>
<p>i
</p>
<p>(
&part;
</p>
<p>&part;r
+ 1
</p>
<p>r
</p>
<p>)
(D.63)
</p>
<p>and it gives:
</p>
<p>&nabla;2 = &minus; p
2
r
</p>
<p>2
&minus; l
</p>
<p>2
</p>
<p>2r2
(D.64)
</p>
<p>and
</p>
<p>[r, pr ] = i. (D.65)</p>
<p/>
</div>
<div class="page"><p/>
<p>234 Appendix D: Calculus I
</p>
<p>D.4 Exercises
</p>
<p>1. Calculate the following limiting values
</p>
<p>lim
x&rarr;&infin;
</p>
<p>xne&minus;x ; lim
x&rarr;&infin;
</p>
<p>x&minus;n ln x; lim
x&rarr;0
</p>
<p>sin x
</p>
<p>x
; lim
</p>
<p>x&rarr;0
sin kx &minus; kx
</p>
<p>kx (1 &minus; cos kx) . (D.66)
</p>
<p>2. Given a function h(x) and a function g(x2); write the derivatives of the following
</p>
<p>functions:
</p>
<p>f (x) = 1
h (x)
</p>
<p>; f (x) = h2 (x) ; f (x) = eh(x);
</p>
<p>f (x) = x &middot; g
(
x2
)
; f (x) = eg(x2). (D.67)
</p>
<p>3. Determine the Taylor series around x = 0 for the functions (a &isin; R)
</p>
<p>(1 + x)a ; ln (1 + x) ; arctan x . (D.68)
</p>
<p>4. Given the operator e
d
</p>
<p>dx , determine e
d
</p>
<p>dx ex .
</p>
<p>5. Find the first partial derivatives with respect to x, y, z of
</p>
<p>r; 1
r
; ra; r; r. (D.69)
</p>
<p>Define r = (x, y, z), r = |r|; r is the unit vector in the direction of r.
6. Show that:
</p>
<p>&part;2
</p>
<p>&part;r2
+ 2
</p>
<p>r
</p>
<p>&part;
</p>
<p>&part;r
= 1
</p>
<p>r2
</p>
<p>&part;
</p>
<p>&part;r
r2
</p>
<p>&part;
</p>
<p>&part;r
= 1
</p>
<p>r
</p>
<p>&part;2
</p>
<p>&part;r2
r. (D.70)
</p>
<p>7. Given a function g(r), depending only on the norm (magnitude) of r , for which
</p>
<p>&nabla;2g(r) = 0 holds. Determine g(r) using the result of the last exercise.
8. Given a scalar function f (r) and a vector function F (r); which of the following
</p>
<p>expressions are meaningful?
</p>
<p>grad f ; div f ; curl f ; grad F; div F; curl F; &nabla; f ; &nabla; &middot; F; &nabla; &times; F. (D.71)
</p>
<p>Write these expressions using the nabla &nabla;.
</p>
<p>9. Calculate &nabla;r, &nabla;2xr and &nabla;r.
10. Given the plane wave F (r, t) = Aei(kr&minus;t), with A and k constant vectors.
</p>
<p>(a) Determine the first time derivative as well as the divergence and the curl of
</p>
<p>F (r, t).
</p>
<p>(b) Assume divF (r, t) = 0. What does this mean physically?
(c) Determine (k &middot;&nabla;)F and k (&nabla; &middot; F).</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix D: Calculus I 235
</p>
<p>11. Show that:
</p>
<p>div grad f = &nabla;2 f ; &nabla; &middot; (&nabla; f ) = &nabla;2 f
curl grad f = 0; &nabla; &times;&nabla; f = 0
</p>
<p>div curl F = 0; &nabla; &middot; (&nabla; &times; F) = 0
curl curl F = grad div F&minus; &nabla;2F; &nabla; &times; (&nabla; &times; F) = &nabla; (&nabla; &middot; F)&minus;&nabla;2F
</p>
<p>(D.72)
</p>
<p>Assume that the partial derivatives commute,
&part;2 f
</p>
<p>&part;xi&part;x j
= &part;2 f
</p>
<p>&part;x j&part;xi
.
</p>
<p>12. Given a homogeneously-charged nonconducting sphere of radius R, with a total
</p>
<p>charge of Q. Using the divergence theorem, determine its electric field E. Derive
</p>
<p>the potential .
</p>
<p>13. Given two point masses with the spherical coordinates (r,1,1) and (r,2,2).
</p>
<p>Calculate their distance d
</p>
<p>(a) for 1 = 2 and 1 	= 2;
(b) for 1 	= 2 and 1 = 2
</p>
<p>One of the results contains and; the other, only. Give an intuitive expla-
</p>
<p>nation of this. Check (a) the special cases (1,2) = (0,) and (0,/2);
and (b) (1,2) = (0,) and (0,/2).
Hint: cos (a &minus; b) = cos a cos b + sin a sin b, and 1&minus; cos a = 2 sin2 a
</p>
<p>2
. And
</p>
<p>there is, of course, Pythagoras.
</p>
<p>14. Show for the functions
</p>
<p>f (x, y) = x
3 y &minus; xy3
x2 + y2 (D.73)
</p>
<p>that the derivatives &part;
&part;x
</p>
<p>and &part;
&part;y
</p>
<p>do not commute at the origin, &part;
&part;x
</p>
<p>&part; f
&part;y
</p>
<p>	= &part;
&part;y
</p>
<p>&part; f
&part;x
</p>
<p>.
</p>
<p>Solution: Away from the origin, f (x, y) is arbitrarily often continuously differ-
</p>
<p>entiable, so there the derivatives always commute. The only problem is at the
</p>
<p>origin. First, we find the first derivatives:
</p>
<p>&part; f
</p>
<p>&part;x
= y x
</p>
<p>4 + 4x2 y2 &minus; y4
(
x2 + y2
</p>
<p>)2 ;
&part; f
</p>
<p>&part;y
= &minus;x y
</p>
<p>4 + 4y2x2 &minus; x4
(
y2 + x2
</p>
<p>)2 . (D.74)
</p>
<p>Both derivatives have removable discontinuities at the origin with the value 0.
</p>
<p>The mixed derivative &part;
&part;y
</p>
<p>&part; f
&part;x
</p>
<p>is not continuous at the origin. This appears in the
</p>
<p>inequality of the mixed derivatives. In fact, we have
</p>
<p>&part;
</p>
<p>&part;y
</p>
<p>&part; f (x, y)
</p>
<p>&part;x
</p>
<p>/
x=0,y=0
</p>
<p>= &part;
&part;y
</p>
<p>y
0 &minus; y4
</p>
<p>(
0 + y2
</p>
<p>)2
/
</p>
<p>y=0
= &minus; &part;
</p>
<p>&part;y
y
/
</p>
<p>y=0
= &minus;1 (D.75)
</p>
<p>and
</p>
<p>&part;
</p>
<p>&part;x
</p>
<p>&part; f (x, y)
</p>
<p>&part;y
</p>
<p>/
x=0,y=0
</p>
<p>= &minus; &part;
&part;x
</p>
<p>x
0 &minus; x4
</p>
<p>(
x2 + 0
</p>
<p>)2
/
</p>
<p>x=0
= &part;
</p>
<p>&part;x
x
/
</p>
<p>y=0
= 1. (D.76)</p>
<p/>
</div>
<div class="page"><p/>
<p>236 Appendix D: Calculus I
</p>
<p>We can also show this using polar coordinates in order to get an intuitive picture.
</p>
<p>The second derivative
</p>
<p>&part;
</p>
<p>&part;y
</p>
<p>&part; f (x, y)
</p>
<p>&part;x
= x
</p>
<p>6 + 9x4 y2 &minus; 9x2 y4 &minus; y6
(
x2 + y2
</p>
<p>)3 (D.77)
</p>
<p>in polar coordinates reads (after some calculation):
</p>
<p>&part;y&part;x f = 2 sin 2 sin 4+ cos 2 cos 4. (D.78)
</p>
<p>We see that the result is independent of r and depends only on the angle . Thus,
</p>
<p>this derivative is not defined at r = 0. It follows, for example, that
</p>
<p>&part;y&part;x f = 1 for  = 0; &part;y&part;x f = &minus;1 for  = /2 (D.79)
</p>
<p>i.e. the same result as above.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix E
</p>
<p>Calculus II
</p>
<p>E.1 Differential Equations: Some General Remarks
</p>
<p>A large part of physics is formulated using differential equations&mdash;classical and quan-
</p>
<p>tum mechanics, hydro- and electrodynamics, string theory and general relativity, and
</p>
<p>so on. Differential equations occur also in other areas: Climatology, oceanography,
</p>
<p>biology (population dynamics), chemistry (reaction kinetics), economics (growth
</p>
<p>processes) and many more. In short, differential equations are a very important means
</p>
<p>for the mathematical description of our environment.
</p>
<p>Unfortunately, there is no general method of solving differential equations. In
</p>
<p>fact, even questions about the mere existence of solutions of certain differential
</p>
<p>equations cannot be answered to the present day, e.g. of the Navier&ndash;Stokes equations
</p>
<p>of hydrodynamics.
</p>
<p>We briefly discuss below some of the basics.
</p>
<p>Differential equations (DEq) are equations that link a function f with its deriva-
</p>
<p>tives &part; f . If the function depends on one variable only, they are called ordinary
</p>
<p>differential equations; if it depends on several independent variables (and if there
</p>
<p>are partial derivatives with respect to more than one variable), they are called partial
</p>
<p>differential equations.
</p>
<p>The highest derivative of f which occurs in the equation determines the order
</p>
<p>of the DEq; the highest power of f and its derivatives which occurs determines
</p>
<p>the degree. The integration of the differential equation is another term for finding
</p>
<p>its solution. The general solution of a differential equation of order n has n free
</p>
<p>parameters ( integration constants); if these free parameters are fixed by n conditions,
</p>
<p>we have a particular or special solution. These conditions may be initial and/or
</p>
<p>boundary conditions. One refers to an initial condition when one of the variables is
</p>
<p>the time (then at t = 0). If the differential equation is of order m with respect to time,
then the specification of m (suitably chosen) initial conditions determines uniquely
</p>
<p>the time evolution of the solution (deterministic evolution). The boundary conditions
</p>
<p>refer to the boundaries &part;G of a spatial domain G within which the solution of the
</p>
<p>differential equation is considered.
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>237</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>238 Appendix E: Calculus II
</p>
<p>Fig. E.1 Family tree of
</p>
<p>solutions of differential
</p>
<p>equations
</p>
<p>mathematical solution
</p>
<p>solution
</p>
<p>correct
</p>
<p>physically
</p>
<p>bounded solution
</p>
<p>If the unknown function f occurs in each term of the differential equation, is
</p>
<p>called a homogeneous differential equation, otherwise the differential equation is
</p>
<p>inhomogeneous.
</p>
<p>In this text we deal (almost) exclusively with linear differential equations, where
</p>
<p>the function f and its derivatives occur only linearly (i.e. with the power 1). The
</p>
<p>basic property of linear differential equations is that linear combinations of solutions
</p>
<p>are again solutions. In essence, this means that the solutions span a vector space.
</p>
<p>This fact (and the underlying linearity of the SEq) is central to quantum mechanics.
</p>
<p>Before we look at those differential equations which are important in the frame-
</p>
<p>work of quantum mechanics, we make a general comment: Compared to mathemat-
</p>
<p>ics, we have in physics the advantage that we can sort out, on the basis of general
</p>
<p>considerations, mathematically absolutely correct but physically irrelevant solutions.
</p>
<p>For example, we require physically relevant solutions to be bounded in the domain
</p>
<p>of definition; so we can omit unbounded solutions. But even bounded solutions do
</p>
<p>not always fulfill the requirements; suppose, for example, that a differential equation
</p>
<p>has as its solutions two plane waves, one running from right to left, the other running
</p>
<p>oppositely. If it is clear from physical reasons that there can be, for example, only
</p>
<p>the wave running from right to left, we have to eliminate the other wave, although it
</p>
<p>is mathematically a completely valid solution. Figure E.1 illustrates this situation.
</p>
<p>E.2 Ordinary Differential Equations
</p>
<p>To oversimplify somewhat, we need only two representatives of ordinary differential
</p>
<p>equations. We formulate them for the independent variable t ; of course, the results
</p>
<p>hold analogously for the independent variable x .
</p>
<p>The first differential equation is the general differential equation of first order
</p>
<p>(which occurs in radioactive decay, absorption of radiation, etc.):
</p>
<p>f (t) = g(t) f (t) (E.1)
</p>
<p>with a given function g(t). The solution reads</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix E: Calculus II 239
</p>
<p>f (t) = Ce
&int;
g(t)dt (E.2)
</p>
<p>where C is the free integration constant (n = 1).
The second DEq, which is especially important (not only for quantum mechanics),
</p>
<p>is the differential equation of second order:
</p>
<p>f (t) = z2 f (t); z &isin; C (E.3)
</p>
<p>with the general solution
</p>
<p>f (t) = c1ezt + c2e&minus;zt . (E.4)
</p>
<p>The integration constants c1 and c2 may be fixed by initial conditions, e.g.
</p>
<p>f (0) = f0; f (0) = f0. (E.5)
</p>
<p>It follows that
</p>
<p>f (t) = z f0 + f0
2z
</p>
<p>ezt + z f0 &minus; f0
2z
</p>
<p>e&minus;zt . (E.6)
</p>
<p>The cases z &isin; R and z &isin; I are of fundamental importance; they are customarily
written as:
</p>
<p>f (t) = 2 f (t) and f (t) = &minus;2 f (t);  &isin; R (E.7)
</p>
<p>with the solutions
</p>
<p>f (t) = c1et + c2e&minus;t and f (t) = c1eit + c2e&minus;it . (E.8)
</p>
<p>The first equation describes exponential behavior, the second a harmonic oscillation.
</p>
<p>With x instead of t , the differential equations read19
</p>
<p>g&prime;&prime;(x) = k2g(x) and g&prime;&prime;(x) = &minus;k2g(x) ; k &isin; R (E.9)
</p>
<p>with the solutions
</p>
<p>g(x) = c1ekx + c2e&minus;kx and g(x) = c1eikx + c2e&minus;ikx . (E.10)
</p>
<p>19For a clearer distinction, one often writes for exponential behavior and k for oscillatory behavior:
</p>
<p>g&prime;&prime;(x) = 2g(x) and g&prime;&prime;(x) = &minus;k2g(x); , k &isin; R
</p>
<p>with the solutions
</p>
<p>g(x) = c1ex + c2e&minus;x and g(x) = c1eikx + c2e&minus;ikx .
.</p>
<p/>
</div>
<div class="page"><p/>
<p>240 Appendix E: Calculus II
</p>
<p>E.3 Partial Differential Equations
</p>
<p>Apart from the continuity equation used in Chap. 7,
</p>
<p>&part;
</p>
<p>&part;t
+&nabla;j = 0 (E.11)
</p>
<p>(derived in Appendix N, Vol. 1), the partial differential equations of interest to
</p>
<p>us in the framework of quantum mechanics are of second order with respect to
</p>
<p>the spatial variables. An external characteristic is the appearance of the Laplacian
</p>
<p>&nabla;2 = &part;2x + &part;2y + &part;2z . Another feature of this differential equation is its linearity. For
completeness, we also cite in the following some differential equations which are
</p>
<p>not used elsewhere in the text. All the functions which occur are functions of r.
</p>
<p>Laplace&rsquo;s (homogeneous) equation
</p>
<p>&nabla;2 = 0 (E.12)
</p>
<p>is a special case of the (inhomogeneous) Poisson equation
</p>
<p>&nabla;2 = f. (E.13)
</p>
<p>With f = &minus; 1
0
, this is the conditional equation of a potential  for given charge
</p>
<p>density  in electrostatics.
</p>
<p>In electrodynamics, this equation is replaced by the potential equation
</p>
<p>(
&nabla;2 &minus; 1
</p>
<p>c2
</p>
<p>&part;2
</p>
<p>&part;t2
</p>
<p>)
 =  = &minus; 1
</p>
<p>0
 (E.14)
</p>
<p>where we have used the d&rsquo;Alembertian (also called quabla)  = &nabla;2 &minus; 1
c2
</p>
<p>&part;2
</p>
<p>&part;t2
.20 We
</p>
<p>find analogous equations for the vector potential A and the current density j by
</p>
<p>inserting the replacement ,  &rarr; Ai , ji/c2 with i = 1, 2, 3 or, more compactly,
,  &rarr; A, j/c2.
</p>
<p>For  = 0, the following homogeneous wave equation results from E.13:
</p>
<p>1
</p>
<p>c2
</p>
<p>&part;2
</p>
<p>&part;t2
 = &nabla;2. (E.15)
</p>
<p>The differential equations of second order in the spatial coordinates considered so
</p>
<p>far are of zeroth or second order in time, and thus require no or two initial conditions.
</p>
<p>In contrast, one initial condition suffices for the heat flow equation:
</p>
<p>&part;
</p>
<p>&part;t
T = &nabla;2T (E.16)
</p>
<p>20The d&rsquo;Alembertian is defined by some authors with the opposite sign as  = 1
c2
</p>
<p>&part;2
</p>
<p>&part;t2
&minus; &nabla;2.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix E: Calculus II 241
</p>
<p>and for the time-dependent SEq
</p>
<p>i
&part;
</p>
<p>&part;t
 = H =
</p>
<p>(
&minus; 
</p>
<p>2
</p>
<p>2m
&nabla;2 + V
</p>
<p>)
. (E.17)
</p>
<p>We note the great similarity of these two equations&mdash;the main difference is &lsquo;just&rsquo; the
</p>
<p>occurrence of the factor i in the SEq.21 Both equations are deterministic in the sense
</p>
<p>that the specification of the initial conditions T (r, 0) or  (r, 0) uniquely determines
</p>
<p>the solutions T (r, t) and  (r, t) for all times.
</p>
<p>With the separation ansatz
</p>
<p> (r, t) =  (r) e&minus;i Et , (E.18)
</p>
<p>we obtain the stationary SEq from (E.17):
</p>
<p>E = H =
(
&minus; 
</p>
<p>2
</p>
<p>2m
&nabla;2 + V
</p>
<p>)
 (E.19)
</p>
<p>This equation is an eigenvalue problem. In general, solutions exist only for certain
</p>
<p>values of E . These values E are called the eigenvalues, and the associated solutions
</p>
<p>eigenfunctions or eigenvectors. The set of all eigenvalues is called the spectrum. The
</p>
<p>spectrum can contain a finite or an infinite number of elements. The eigenvalues
</p>
<p>can be countable (discrete spectrum) or uncountable (continuous spectrum). Spectra
</p>
<p>may also contain both discrete and continuous elements; these two components can
</p>
<p>furthermore overlap.
</p>
<p>An eigenvalue is called degenerate if there are a number of different eigenfunctions
</p>
<p>belonging to this eigenvalue. The simplest case of the eigenvalue problem (E.19) is
</p>
<p>a non-degenerate, discrete spectrum; in this case,
</p>
<p>Hn = Enn; n = 1, 2, . . . (E.20)
</p>
<p>applies. In the case of degeneracy, we have
</p>
<p>Hn,r = Enn,r ; n = 1, 2, . . . ; r = 1, 2, . . . , gn (E.21)
</p>
<p>where gn is the degree of degeneracy.
</p>
<p>Closed analytical solutions of the stationary SEq exist only for a handful of poten-
</p>
<p>tials. In particular, the free three-dimensional problem
</p>
<p>E (r) = &minus; 
2
</p>
<p>2m
&nabla;2 (r) (E.22)
</p>
<p>21As noted in the text, due to this &lsquo;small difference&rsquo; i , there are worlds between the solutions of the
</p>
<p>heat-conduction equation and the Schr&ouml;dinger equation.</p>
<p/>
</div>
<div class="page"><p/>
<p>242 Appendix E: Calculus II
</p>
<p>has the solutions
</p>
<p> (r) =
&sum;
</p>
<p>l,m
</p>
<p>[al jl (kr)+ blnl (kr)] Y ml (,) ; k2 =
2m
</p>
<p>2
E . (E.23)
</p>
<p>The jl (kr) and nl (kr) are spherical Bessel functions, the Y
m
l (,) spherical har-
</p>
<p>monics. For these functions and other analytical solutions of the SEq, see Appendix B,
</p>
<p>Vol. 2.
</p>
<p>E.4 Exercises
</p>
<p>1. Given the eigenvalue problem
</p>
<p>d2
</p>
<p>dx2
f (x) = &minus;k2 f (x); k &gt; 0; 0 &le; x &le; a (E.24)
</p>
<p>with the boundary condition
</p>
<p>f (0) = f (a) = 0, (E.25)
</p>
<p>determine the allowed values of k and the associated eigenfunctions.
</p>
<p>2. Given the differential equations
</p>
<p>f &prime;&prime;(x)+ k2 f (x) = 0 and f &prime;&prime;(x)&minus; k2 f (x) = 0 (E.26)
</p>
<p>with k &isin; R; what are the general solutions of these equations?
3. Given the differential equation
</p>
<p>y(n) = d
n
</p>
<p>dxn
y(x) = y(x); (E.27)
</p>
<p>what is its general solution?
</p>
<p>4. Show that the linear combination of solutions of the SEq (time-dependent and
</p>
<p>stationary) are again themselves solutions.
</p>
<p>5. Given the wave equation
</p>
<p>&part;2t f (r, t) = c2&nabla;2 f (r, t). (E.28)
</p>
<p>The initial conditions f (r, 0) and f (r, 0) are known. Formulate the general
</p>
<p>solution.
</p>
<p>6. The heat conduction equation
</p>
<p>&part;t T (r, t) = D&nabla;2T (r, t) (E.29)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix E: Calculus II 243
</p>
<p>is solved by
</p>
<p>T (r, t) = et D&nabla;2 T (r, 0) . (E.30)
</p>
<p>Determine the solution T (r, t) for the initial condition T (r, 0) = T0 + T1 cos
(kr). Discuss the result; is it physically plausible?
</p>
<p>Solution: First we show that (E.30) satisfies the heat conduction equation. We
</p>
<p>have
</p>
<p>&part;t T (r, t) = &part;t et D&nabla;
2
</p>
<p>T (r, 0) = D&nabla;2et D&nabla;2 T (r, 0) = D&nabla;2T (r, t) . (E.31)
</p>
<p>Next, we determine et D&nabla;
2
</p>
<p>(T0 + T1 cos (kr)). It is
</p>
<p>et D&nabla;
2
</p>
<p>(T0 + T1 cos (kr)) =
&infin;&sum;
</p>
<p>n=0
</p>
<p>tn Dn&nabla;2n
n! (T0 + T1 cos (kr))
</p>
<p>= T0 + T1
&infin;&sum;
</p>
<p>n=0
</p>
<p>tn Dn&nabla;2n
n! cos (kr) .
</p>
<p>(E.32)
</p>
<p>Because of &nabla;2 cos (kr) = &minus;k2 cos (kr), it follows that
</p>
<p>T (r, t) = et D&nabla;2 (T0 + T1 cos (kr))
</p>
<p>= T0 + T1
&infin;&sum;
</p>
<p>n=0
</p>
<p>tn Dn
(
&minus;k2
</p>
<p>)n
</p>
<p>n! cos (kr) = T0 + T1 cos (kr) e
&minus;Dk2t . (E.33)
</p>
<p>The initial condition is a starting temperature T0 with a superposed variation
</p>
<p>&sim; T1 which levels off more and more in the course of time, according to T1e&minus;Dk2t .
7. Show that
</p>
<p>F (x, t) = 1
(at)1/2
</p>
<p>e&minus;b
x2
</p>
<p>t (E.34)
</p>
<p>is a solution of the one-dimensional heat conduction equation. Determine the
</p>
<p>constants a and b. What would be a similar solution of the SEq?
</p>
<p>8. Show that
</p>
<p>(r) = 1
40
</p>
<p>&int;
(r&prime;)
</p>
<p>|r &minus; r&prime;|d
3r &prime; (E.35)
</p>
<p>is a solution of
</p>
<p>&nabla;2 = &minus; 1
0
(r). (E.36)
</p>
<p>Hint: Use Fourier transformation (see Appendix H, Vol. 1):
</p>
<p>&nabla;2r
1
</p>
<p>|r &minus; r&prime;| = &minus;4(r &minus; r
&prime;). (E.37)</p>
<p/>
</div>
<div class="page"><p/>
<p>244 Appendix E: Calculus II
</p>
<p>Remark: If several sets of coordinates occur, it is not clear in the notation &nabla;2
with respect to which coordinates the differentiation should be carried out. In
</p>
<p>this case, one frequently writes the corresponding coordinates as an index: &nabla;2r
means the derivative with respect to the corresponding components of r.
</p>
<p>Solution: In the following, the difference between r and r&prime; is essential. We have
</p>
<p>&nabla;2r(r) = 140 &nabla;
2
r
</p>
<p>&int;
(r&prime;)
|r&minus;r&prime;|d
</p>
<p>3r &prime; = 1
40
</p>
<p>&int;
(r&prime;)&nabla;2r 1|r&minus;r&prime;|d3r &prime;
</p>
<p>= &minus; 1
40
</p>
<p>&int;
(r&prime;)4(r &minus; r&prime;)d3r &prime; = &minus; 1
</p>
<p>0
(r).
</p>
<p>(E.38)
</p>
<p>9. Given a function g(r) with &nabla;2g(r) = 0. Determine g(r).
10. Solve the equation
</p>
<p>(
d2
</p>
<p>dr2
+ 2
</p>
<p>r
</p>
<p>d
</p>
<p>dr
+ 1 &minus; l (l + 1)
</p>
<p>r2
</p>
<p>)
fl (r) = 0 (E.39)
</p>
<p>by means of a power series expansion. Write down explicitly the regular and the
</p>
<p>irregular solution for l = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix F
</p>
<p>Linear Algebra I
</p>
<p>F.1 Vectors (Real, Three Dimensional)
</p>
<p>In this section, we consider &lsquo;physical&rsquo; vectors, i.e. triples of (real) measurable
</p>
<p>quantities, which are referred to a coordinate system so that a change of the coor-
</p>
<p>dinate system leads to an analogous change of the components of the vector. These
</p>
<p>vectors are initially written as row vectors; the distinction between the column and
</p>
<p>row vector is introduced later on in the section on matrix calculus. In print, vectors are
</p>
<p>frequently denoted by boldface type, r; handwritten, by an arrow, r . The prototype
of a &lsquo;physical&rsquo; vector is the position vector
</p>
<p>r = (x, y, z) . (F.1)
</p>
<p>A general vector is given by
</p>
<p>v =
(
vx , vy, vz
</p>
<p>)
or v = (v1, v2, v3) (F.2)
</p>
<p>or a similar notation. The norm (magnitude, value) of this vector reads
</p>
<p>|v| = v =
&radic;
v2x + v2y + v2z . (F.3)
</p>
<p>If |v| = 1, the vector is said to be normalized. The space spanned by the set of all
these vectors is denoted by R3 (R for real, 3 means the dimension).
</p>
<p>F.1.1 Basis, Linear Independence
</p>
<p>With the help of the Cartesian unit vectors
</p>
<p>ex = (1, 0, 0) ; ey = (0, 1, 0) ; ez = (0, 0, 1) (F.4)
&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>245</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>246 Appendix F: Linear Algebra I
</p>
<p>each vector can be written as
</p>
<p>v = aex + bey + cez . (F.5)
</p>
<p>The terms a, b, c are called the components or coordinates of the vector. Unit vectors
</p>
<p>are frequently written with a hat: ex &equiv; x, etc.
These unit vectors have important properties: namely, they are linearly indepen-
</p>
<p>dent and they form a complete set. A set of vectors {v1, v2, . . .} is linearly independent
if the equation
</p>
<p>1v1 + 2v2 + &middot; &middot; &middot; = 0 (F.6)
</p>
<p>can be satisfied only for 1 = 2 = &middot; &middot; &middot; = 0. The completeness of the system{
ex , ey, ez
</p>
<p>}
implies that every vector (F.2) can be represented in the form (F.5). In
</p>
<p>other words: the Cartesian unit vectors (F.4) form a basis of R3.
</p>
<p>F.1.2 Scalar Product, Vector Product
</p>
<p>The scalar product (inner product, dot product) of two vectors v = (v1, v2, v3) and
w = (w1, w2, w3) is a number and is defined by22
</p>
<p>vw = v &middot; w = v1w1 + v2w2 + v3w3. (F.7)
</p>
<p>Another representation is
</p>
<p>vw = vw cos (F.8)
</p>
<p>where  is the angle between the two vectors (intermediate angle), see Fig. F.1. This
</p>
<p>relation shows at once that two vectors are orthogonal iff vw = 0. Indeed, the scalar
product is closely linked to the term projection. The perpendicular projection of w
</p>
<p>onto v (i.e. the component of w parallel to v) clearly has the length w cos; the
</p>
<p>component of w which is parallel to v is thus given by w&prime; = w cos &middot; v = vw
vv
</p>
<p>&middot; v,
and the scalar product is then vw = vw&prime;. Of course, this reasoning could be repeated
with the roles reversed.
</p>
<p>The vector product (cross product or skew product) of two three-dimensional
</p>
<p>vectors is defined by
</p>
<p>v &times; w = (v2w3 &minus; v3w2, v3w1 &minus; v1w3, v1w2 &minus; v2w1) . (F.9)
</p>
<p>Another formulation uses the Levi&ndash;Civita symbol (permutation symbol, epsilon ten-
</p>
<p>sor) i jk :
</p>
<p>22The definition of the scalar product for complex vectors is given in the Sect. F.2.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix F: Linear Algebra I 247
</p>
<p>Fig. F.1 Projection of
</p>
<p>w onto v
</p>
<p>i jk =
</p>
<p>


</p>
<p>1
</p>
<p>&minus;1
0
</p>
<p>if
</p>
<p>i jk is an even permutation of 123
</p>
<p>i jk is an odd permutation of 123
</p>
<p>otherwise.
</p>
<p>(F.10)
</p>
<p>Then the vector product is written as23
</p>
<p>(v &times; w) j =
3&sum;
</p>
<p>k,m=1
 jkmvkwm . (F.11)
</p>
<p>The norm of the vector product is given by
</p>
<p>|v &times; w| = vw |sin| . (F.12)
</p>
<p>As can be seen, the vector product vanishes for  = 0, i.e. for collinear vectors.
</p>
<p>F.1.3 Polar and Axial Vectors
</p>
<p>Vectors can be distinguished according to how they react to transformation of the
</p>
<p>spatial coordinates (x, y, z) &rarr; (&minus;x,&minus;y,&minus;z) or r &rarr; &minus;r (parity transformation,
coordinate inversion).
</p>
<p>A polar vector transforms like a position vector, for example the momentum
</p>
<p>(because of p = mr):
p &rarr; &minus;p (F.13)
</p>
<p>An axial vector (= pseudovector) transforms like the angular momentum, according
</p>
<p>to
</p>
<p>l &rarr; l, (F.14)
</p>
<p>23With Einstein&rsquo;s summation convention (repeated indices are implicitly summed over, i.e. without
</p>
<p>explicitly noting the summation), this is written (v &times; w) j =  jkmvkwm . We will not use this
convention, however.</p>
<p/>
</div>
<div class="page"><p/>
<p>248 Appendix F: Linear Algebra I
</p>
<p>Since l = r &times; p &rarr; l = (&minus;r) &times; (&minus;p) = r &times; p. It generally applies that the vector
product of two polar vectors is a pseudovector.
</p>
<p>These definitions allow us, moreover, to make the distinction between scalars such
</p>
<p>as r &middot; p, which do not change under the transformation r &rarr; &minus;r, and pseudoscalars,
which change their signs. All scalar products of an axial and a polar vector are
</p>
<p>pseudoscalars; an example is l &middot; p.
The distinction between polar and axial vectors plays a role in e.g. the study of
</p>
<p>parity violation, occurring e.g. in beta decay and generally in the weak interactions.
</p>
<p>F.2 Matrix Calculus
</p>
<p>One can modify a vector by multiplying it by a number (a scalar), resulting in a change
</p>
<p>in length of the vector, but not in a change of its direction. For other transformations
</p>
<p>such as the rotation of a vector, one requires matrices.
</p>
<p>Matrices have rows and columns, so we have to distinguish between column and
</p>
<p>row vectors. Therefore, in this section, vectors are no longer denoted by boldface type,
</p>
<p>but rather are written as column and row vectors. Furthermore, we no longer limit
</p>
<p>ourselves to real numbers, but use more generally complex numbers. In addition, we
</p>
<p>consider arbitrary dimensions; in this sense, the vectors occurring in the following
</p>
<p>are no longer the &lsquo;physical&rsquo; vectors of the last section, but vectors in the general
</p>
<p>sense of linear algebra.
</p>
<p>Matrices can be represented as rectangular arrays of numbers with m rows and n
</p>
<p>columns:
</p>
<p>A =
</p>
<p>

</p>
<p>a11 a12 . . . a1n
a21 a22 . . . a2n
...
</p>
<p>...
. . .
</p>
<p>...
am1 am2 . . . amn
</p>
<p>
 = (amn) . (F.15)
</p>
<p>This is called an m &times; n matrix. The set of all m &times; n matrices is denoted by K m&times;n or
M (m &times; n, K ), where K is the underlying field of numbers. We restrict ourselves in
the following to complex matrices K = C (or possibly to the subset of real matrices
K = R).
</p>
<p>Multiplying a matrix by a scalar c (scalar multiplication) means that all its elements
</p>
<p>are multiplied by c; the addition of two matrices (which have to be of the same
</p>
<p>dimensions, of course) is term by term:
</p>
<p>cA =
</p>
<p>

</p>
<p>ca11 ca12 . . . ca1n
ca21 ca22 . . . ca2n
...
</p>
<p>...
. . .
</p>
<p>...
</p>
<p>cam1 cam2 . . . camn
</p>
<p>
 = (camn) (F.16)
</p>
<p>and
</p>
<p>(amn)+ (bmn) = (amn + bmn) . (F.17)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix F: Linear Algebra I 249
</p>
<p>The product of two matrices A and B (matrix multiplication) can be carried out
</p>
<p>if the number of columns of A is equal to the number of rows of B. If A is a k &times; m
matrix and B is an m &times; n matrix, then A &middot; B is a k &times; n matrix. The calculation is
performed according to the rule &lsquo;row times column&rsquo;:
</p>
<p>A &middot; B = (ckn) ; ci j=
m&sum;
</p>
<p>l=1
ailbl j . (F.18)
</p>
<p>Some examples: First the product of a 2 &times; 3 and a 3 &times; 2 matrix:
</p>
<p>(
1 2 3
</p>
<p>a b c
</p>
<p>)

</p>
<p>4 7
</p>
<p>5 8
</p>
<p>6 9
</p>
<p>
 =
</p>
<p>(
1 &middot; 4 + 2 &middot; 5 + 3 &middot; 6 1 &middot; 7 + 2 &middot; 8 + 3 &middot; 9
a &middot; 4 + b &middot; 5 + c &middot; 6 a &middot; 7 + b &middot; 8 + c &middot; 9.
</p>
<p>)
(F.19)
</p>
<p>Column vectors can be understood as n &times; 1 matrices, row vectors as 1&times; n matrices:
(
</p>
<p>1 2
</p>
<p>3 4
</p>
<p>)(
a
</p>
<p>b
</p>
<p>)
=
</p>
<p>(
a + 2b
3a + 4b
</p>
<p>)
(F.20)
</p>
<p>(
a b
</p>
<p>) ( 1 2
3 4
</p>
<p>)
=
</p>
<p>(
a + 3b 2a + 4b
</p>
<p>)
. (F.21)
</p>
<p>The product of a row vector and a column vector is a number (scalar product), the
</p>
<p>product of a column and a row vector a matrix (dyadic product)
</p>
<p>(
c d
</p>
<p>) (a
b
</p>
<p>)
= ca + db (F.22)
</p>
<p>(
a
</p>
<p>b
</p>
<p>) (
c d
</p>
<p>)
=
</p>
<p>(
ac ad
</p>
<p>bc bd
</p>
<p>)
. (F.23)
</p>
<p>Multiple products are also defined, where applicable:
</p>
<p>(
c d
</p>
<p>) (1 2
3 4
</p>
<p>)(
a
</p>
<p>b
</p>
<p>)
=
</p>
<p>(
c d
</p>
<p>) ( a + 2b
3a + 4b
</p>
<p>)
= c (a + 2b)+ d (3a + 4b) . (F.24)
</p>
<p>Even if the product AB exists, the product B A is not automatically defined (see
</p>
<p>exercises). But this always holds true for square matrices, i.e. n&times;n matrices. However,
in this case, the product is not commutative as a rule, and we have AB 	= B A in
general. Example:
</p>
<p>(
1 2
</p>
<p>3 4
</p>
<p>)(
0 1
</p>
<p>1 0
</p>
<p>)
=
</p>
<p>(
2 1
</p>
<p>4 3
</p>
<p>)
;
(
</p>
<p>0 1
</p>
<p>1 0
</p>
<p>)(
1 2
</p>
<p>3 4
</p>
<p>)
=
</p>
<p>(
3 4
</p>
<p>1 2
</p>
<p>)
. (F.25)</p>
<p/>
</div>
<div class="page"><p/>
<p>250 Appendix F: Linear Algebra I
</p>
<p>For the remainder of this section, we restrict ourselves to square matrices.
</p>
<p>The identity matrix (unit matrix) is the matrix which has 1 for each element on
</p>
<p>the principal diagonal and 0 for all other elements. It is denoted by E , En , I, I d, 1
</p>
<p>or the like; often just by 1. The zero matrix has, according to its name, only zeroes
</p>
<p>as entries; it is usually denoted by 0.
</p>
<p>For a square matrix A, any power An with n &isin; N is defined (A0 is the identity
matrix). For this reason, we can also insert matrices into polynomials or power series,
</p>
<p>e.g. in exponential functions such as
</p>
<p>eA =
&infin;&sum;
</p>
<p>n=0
</p>
<p>1
</p>
<p>n! A
n. (F.26)
</p>
<p>The power Am of a square matrix A can be the zero matrix (in contrast to complex
</p>
<p>numbers z; zn is always unequal to zero for z 	= 0). In this case the matrix is called
nilpotent with index m. The simplest example is the matrix A with index 2:
</p>
<p>A =
(
</p>
<p>0 1
</p>
<p>0 0
</p>
<p>)
; A2 =
</p>
<p>(
0 0
</p>
<p>0 0
</p>
<p>)
. (F.27)
</p>
<p>Every square matrix A is associated with two scalar parameters, its trace tr(A) =
tr A and its determinant det A. The trace of a square matrix is defined as the sum of
</p>
<p>all its diagonal elements:
</p>
<p>tr (A) = tr (ann) =
n&sum;
</p>
<p>j=1
a j j . (F.28)
</p>
<p>We have tr(AB) = tr(B A), even if the matrices A and B do not commute. It follows
that the trace is invariant under cyclic permutations, e.g. tr(ABC) = tr(BC A) =
tr(C AB).
</p>
<p>The determinant of a square matrix (an alternating multilinear form) is also a
</p>
<p>number (&isin; K ). For a 2 &times; 2 matrix, it is given by
</p>
<p>det
</p>
<p>(
a b
</p>
<p>c d
</p>
<p>)
=
</p>
<p>
a b
</p>
<p>c d
</p>
<p> = ad &minus; bc. (F.29)
</p>
<p>Determinants of higher-dimensional matrices may be calculated by means of the
</p>
<p>Laplace expansion of determinants; two equivalent formulations are
</p>
<p>det A =
n&sum;
</p>
<p>j=1
(&minus;1)i+ j ai j &middot; det Ai j expansion with respect to row i
</p>
<p>det A =
n&sum;
</p>
<p>i=1
(&minus;1)i+ j ai j &middot; det Ai j expansion with respect to column j
</p>
<p>(F.30)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix F: Linear Algebra I 251
</p>
<p>where Ai j denotes that (n &minus; 1) &times; (n &minus; 1)-matrix which arises from A by deleting
the i th row and j th column. An example can be found in the exercises.
</p>
<p>Determinants are zero iff rows (or columns) are linearly dependent; this holds
</p>
<p>true in particular if two rows (or two columns) are identical.
</p>
<p>The determinant of matrix products can be reduced to the individual determinants:
</p>
<p>det (A &middot; B) = det A &middot; det B. (F.31)
</p>
<p>Finally, we note a relation between the trace and the determinant:
</p>
<p>det eA = etr(A). (F.32)
</p>
<p>F.2.1 Special Matrices
</p>
<p>From now on we use the bra-ket notation: |a denotes a column vector, b| a row
vector.
</p>
<p>The importance of matrices in mathematics and mathematical physics is reflected
</p>
<p>among other things by the fact that there are a number of special matrices. Before
</p>
<p>we go into details, we will define the important terms transposed and adjoint.
</p>
<p>The transposed matrix AT of a given matrix A is obtained by interchanging the
</p>
<p>roles of its rows and columns:
</p>
<p>A =
</p>
<p>

</p>
<p>a11 . . . a1n
...
</p>
<p>. . .
...
</p>
<p>am1 . . . amn
</p>
<p>
 ; AT =
</p>
<p>

</p>
<p>a11 . . . am1
...
</p>
<p>. . .
...
</p>
<p>a1n . . . amn.
</p>
<p>
 (F.33)
</p>
<p>Taking in addition the complex conjugate of all the matrix elements, we obtain the
</p>
<p>adjoint matrix A&dagger;:
</p>
<p>A =
</p>
<p>

</p>
<p>a11 . . . a1n
...
</p>
<p>. . .
...
</p>
<p>am1 . . . amn
</p>
<p>
 ; A&dagger; =
</p>
<p>

</p>
<p>a&lowast;11 . . . a
&lowast;
m1
</p>
<p>...
. . .
</p>
<p>...
</p>
<p>a&lowast;1n . . . a
&lowast;
mn.
</p>
<p>
 (F.34)
</p>
<p>For the determinants, we then find
</p>
<p>det AT = det A; det A&dagger; = det A&lowast;. (F.35)
</p>
<p>The adjoint of a column vector is thus a row vector with complex conjugated
</p>
<p>entries and vice versa:</p>
<p/>
</div>
<div class="page"><p/>
<p>252 Appendix F: Linear Algebra I
</p>
<p>

</p>
<p>a1
a2
...
</p>
<p>

</p>
<p>&dagger;
</p>
<p>=
(
</p>
<p>a&lowast;1 a
&lowast;
2 . . .
</p>
<p>)
;
(
</p>
<p>a1 a2 . . .
)&dagger; =
</p>
<p>

</p>
<p>a&lowast;1
a&lowast;2
...
</p>
<p>
 (F.36)
</p>
<p>or, compactly:
</p>
<p>|a&dagger; = a| ; a|&dagger; = |a . (F.37)
</p>
<p>The product of a row vector and a column vector (i.e. the scalar product) is
</p>
<p>written24
</p>
<p>a| b =
(
</p>
<p>a&lowast;1 a
&lowast;
2 . . .
</p>
<p>)


</p>
<p>b1
b2
...
</p>
<p>
 = a&lowast;1 b1 + a&lowast;2 b2 + &middot; &middot; &middot; (F.38)
</p>
<p>This expression generalizes the formulations of the last section for real vectors.
</p>
<p>In contrast, the dyadic product |a b| is itself a matrix:
</p>
<p>|a b| =
</p>
<p>

</p>
<p>a1
a2
...
</p>
<p>

(
</p>
<p>b&lowast;1 b
&lowast;
2 . . .
</p>
<p>)
=
</p>
<p>

</p>
<p>a1b
&lowast;
1 a1b
</p>
<p>&lowast;
2 . . .
</p>
<p>a2b
&lowast;
1 a2b
</p>
<p>&lowast;
2 . . .
</p>
<p>...
...
</p>
<p>...
</p>
<p>
 (F.39)
</p>
<p>In the rest of this section, we confine ourselves to square matrices. We list some
</p>
<p>important types of matrices; a graphical summary is given in Fig. F.2.
</p>
<p>If the determinant of a matrix A is not equal to zero, A is said to be regular. In
</p>
<p>this case there exists another matrix A&minus;1, such that25 AA&minus;1 = A&minus;1 A = E . The
matrix A&minus;1 is called the inverse matrix to A. Matrices with vanishing determinants
are called singular.
</p>
<p>A matrix A is termed diagonalizable if there is a regular matrix B such that
</p>
<p>D = B AB&minus;1 is a diagonal matrix. A subset of diagonalizable matrices are normal
matrices that commute with their adjoints: AA&dagger; = A&dagger; A.
</p>
<p>Very important for physical description are two types of matrices with special
</p>
<p>symmetry. A matrix is called symmetric if A = AT , and it is called Hermitian if
A = A&dagger;.
</p>
<p>A real matrix A is called orthogonal, if A&minus;1 = AT or AAT = E . These matrices
represent e.g. rotations in an n dimensional space. A complex matrix is called unitary
</p>
<p>if A&minus;1 = A&dagger; or AA&dagger; = E . These can be thought of as rotations in the n dimensional
complex space.
</p>
<p>For a projection(matrix), we have A2 = A. We call such matrices idempotent. If
the projection is in addition Hermitian, it is called a Hermitian projection (or normal
</p>
<p>projection, orthogonal projection or projector).
</p>
<p>24One does not write a| |b, but instead a|b, saving a vertical line in this and similar expressions.
25In finite-dimensional spaces, the left inverse is equal to the right inverse. For dim = &infin; this is not
necessarily the case.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix F: Linear Algebra I 253
</p>
<p>nxn matrix
</p>
<p>projection
</p>
<p>normal projection
</p>
<p>unitaryHermitian
</p>
<p>normal
</p>
<p>diagonalizable
</p>
<p>Fig. F.2 Family tree of square matrices
</p>
<p>Unitary and Hermitian (normal) matrices are related in various ways. For example,
</p>
<p>a normal matrix A can be diagonalized unitarily, i.e. there is a unitary matrix U such
</p>
<p>that U AU&minus;1 is diagonal (for the proof see the exercises). In fact, the following
applies: A can be diagonalized unitarily iff A is normal. This &lsquo;iff&rsquo; is valid only
</p>
<p>for unitary diagonalization&mdash;non-unitary diagonalizability may also occur for non-
</p>
<p>normal operators (see exercises).
</p>
<p>Hermitian and unitary matrices and Hermitian projections (which are all normal
</p>
<p>matrices, i.e. diagonalizable) and their generalizations for corresponding operators
</p>
<p>play an important role in quantum mechanics.
</p>
<p>Furthermore, in quantum mechanics, matrices with a countably infinite number
</p>
<p>of columns or rows are found. To multiply them, one has to impose additional con-
</p>
<p>ditions on the components, because the sums which occur are infinite series and do
</p>
<p>not necessarily converge. These issues are treated in more detail e.g. in functional
</p>
<p>analysis.
</p>
<p>F.2.2 The Eigenvalue Problem
</p>
<p>If A is an n &times; n matrix and v an n-dimensional vector, then the eigenvalue problem
has the form26:
</p>
<p>26In this section, we drop the bra-ket notation, since only column vectors occur (denoted by v), and
</p>
<p>no row vectors.</p>
<p/>
</div>
<div class="page"><p/>
<p>254 Appendix F: Linear Algebra I
</p>
<p>Av = v (F.40)
</p>
<p>where  is an (in general complex) number. Thus, we have to find vectors which are
</p>
<p>mapped by A onto the -fold of themselves. These vectors are called eigenvectors,
</p>
<p>and the corresponding numbers  are their eigenvalues. The eigenvectors indicate
</p>
<p>the directions in which A acts as a multiplication by  (i.e. a number), while in other
</p>
<p>directions, Av is no longer proportional to v.
</p>
<p>We first calculate the eigenvalues. We rewrite (F.40)27 to give
</p>
<p>(E &minus; A) v = 0. (F.41)
</p>
<p>In order that this system have other solutions besides just the trivial solution v = 0,
the following condition must be fulfilled:
</p>
<p>det (E &minus; A) = 0. (F.42)
</p>
<p>If we write this determinant out in full according to the rules for the n &times; n matrix A,
we see that it is a polynomial of order n in . This polynomial pn () is called the
</p>
<p>characteristic polynomial of A
</p>
<p>pn () = det (E &minus; A) . (F.43)
</p>
<p>The determination of the eigenvalues is thus equivalent to finding of the zeros of
</p>
<p>pn (). The fundamental theorem of algebra (see also Appendix C, Vol. 1) says that
</p>
<p>each polynomial of order n has n zeros (which are complex, in general). Thus, we
</p>
<p>can write the polynomial as product of linear factors (polynomial factorization)
</p>
<p>pn () = (&minus; 1) (&minus; 2) . . . (&minus; n) . (F.44)
</p>
<p>Multiple zeros occur in this factorization a number of times according to their
</p>
<p>multiplicity. The set of all eigenvalues is called the spectrum. For an example see the
</p>
<p>exercises.
</p>
<p>We note in addition that trace and determinant of a matrix A are directly related
</p>
<p>to its eigenvalues, namely by
</p>
<p>tr (A) =
&sum;
</p>
<p>j
</p>
<p> j ; det(A) =
&prod;
</p>
<p>j
</p>
<p> j . (F.45)
</p>
<p>Now that we know the eigenvalues, we have to determine the eigenvectors. For
</p>
<p>this, the eigenvalues are inserted into (F.40), where one usually indexes from the
</p>
<p>outset:
</p>
<p>Avi = ivi ; i = 1, . . . , n. (F.46)
</p>
<p>27Often this is simply written as (&minus; A) v = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix F: Linear Algebra I 255
</p>
<p>This system of linear equations is now solved by the usual techniques. Example:
</p>
<p>A =
(
</p>
<p>0 1
</p>
<p>1 0
</p>
<p>)
; 1 = 1; 2 = &minus;1. (F.47)
</p>
<p>Written out in full, we have e.g. for the eigenvalue 1
</p>
<p>(
0 1
</p>
<p>1 0
</p>
<p>)(
v1,1
v1,2
</p>
<p>)
=
</p>
<p>(
v1,1
v1,2
</p>
<p>)
(F.48)
</p>
<p>or
v1,2 = v1,1
v1,1 = v1,2 (F.49)
</p>
<p>It follows that:
</p>
<p>v1 =
(
</p>
<p>1
</p>
<p>1
</p>
<p>)
v1,1 and analogously v2 =
</p>
<p>(
1
</p>
<p>&minus;1
</p>
<p>)
v2,1 (F.50)
</p>
<p>where v1,1 and v2,1 are arbitrary complex numbers. All vectors v1 of this form are
</p>
<p>solutions of the eigenvalue equation (F.48). In other words, these vectors span a one-
</p>
<p>dimensional subspace which is called the eigenspace of the eigenvalue 1 (the zero
</p>
<p>vector is not considered to be an eigenvector, but is element of the eigenspace). That
</p>
<p>one nevertheless speaks of the eigenvector (and not of an eigenvector) is due to the
</p>
<p>fact that one refers in general to normalized vectors; in the example these are
</p>
<p>v1 =
1&radic;
2
</p>
<p>(
1
</p>
<p>1
</p>
<p>)
and v2 =
</p>
<p>1&radic;
2
</p>
<p>(
1
</p>
<p>&minus;1
</p>
<p>)
. (F.51)
</p>
<p>If there are multiple eigenvalues, the situation may be more complicated; in this
</p>
<p>case one speaks of degeneracy. For simplicity, we restrict ourselves to the case of
</p>
<p>normal matrices, which is relevant to quantum mechanics. If an eigenvalue  occurs
</p>
<p>d-fold, it is d-fold degenerate (degree of degeneracy d). In this case, the eigenspace
</p>
<p>of  has the dimension d.
</p>
<p>F.2.3 A Remark on Hermitian Matrices
</p>
<p>Measured variables are represented in quantum mechanics by Hermitian matrices (or,
</p>
<p>more generally, by Hermitian operators). In particular in Chap. 13, the properties of
</p>
<p>these operators are discussed. Among other things, it is found that their eigenvalues
</p>
<p>are real, that eigenvectors belonging to different eigenvalues are orthogonal to each
</p>
<p>other, and that two commuting Hermitian operators possess a common CONS.</p>
<p/>
</div>
<div class="page"><p/>
<p>256 Appendix F: Linear Algebra I
</p>
<p>For an Hermitian (or more generally normal) matrix A, one can always find a
</p>
<p>unitary matrix so that the operator U&minus;1 AU is diagonal, i.e. U&minus;1 AU = D applies.
As the columns of U , one can choose the eigenvectors; the diagonal elements of D are
</p>
<p>the eigenvalues that occur as frequently as their degree of degeneracy indicates.28 The
</p>
<p>explicit calculation is given in the exercises. By the way, the spectral representation
</p>
<p>in Chap. 13 is just another formulation of this fact.
</p>
<p>From this, the rule follows that commuting Hermitian operators can be diagonal-
</p>
<p>ized simultaneously (since they have a common CONS). Because of the diagonaliz-
</p>
<p>ability of the operators or matrices occurring in quantum mechanics, one can always
</p>
<p>expand in terms of the eigenfunctions, without having to worry about Jordan normal
</p>
<p>forms or the like.
</p>
<p>F.3 Exercises
</p>
<p>1. Given x = (4,&minus;2, 5); determine a, b, c, d so that the three vectors x, y =
(&minus;1, a, b) and z = (&minus;1, c, d) are pairwise orthogonal.
</p>
<p>2. x and y are three-dimensional vectors. Show that x and x &times; y as well as y and
x &times; y are mutually orthogonal.
</p>
<p>3. Given the vectors
</p>
<p>a =
</p>
<p>

</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>
 ; b =
</p>
<p>

</p>
<p>3
</p>
<p>2
</p>
<p>1
</p>
<p>
 ; c =
</p>
<p>

</p>
<p>0
</p>
<p>A
</p>
<p>B
</p>
<p>
 , (F.52)
</p>
<p>calculate the scalar product a &middot;b, the vector product a&times;b, and the dyadic product
ab. For which A, B are the three vectors a, b, c linearly independent?
</p>
<p>4. Given the Coriolis force FC = 2m (v &times; ); in what direction does it act on a
freely falling body?
</p>
<p>Solution: The Earth&rsquo;s rotation is counterclockwise (seen from above the north
</p>
<p>pole), and is thus mathematically positive, i.e.  = (0, 0,) with  &gt; 0. The
speed of a body falling to Earth&rsquo;s surface is v = (v, 0, 0) with v &lt; 0; for the sake
of simplicity, we have assumed that the mass falls along the x axis (i.e. at the
</p>
<p>equator); for v &gt; 0, the mass would be moving away from the Earth&rsquo;s surface.
</p>
<p>It follows that FC = 2m (v &times; ) = 2m (0,&minus;v, 0). Because of v &lt; 0, the term
&minus;v is positive and a deflection towards positive values of y results, i.e. to the
east.
</p>
<p>5. Given the matrix M
</p>
<p>M =
(
</p>
<p>0 1
</p>
<p>1 0
</p>
<p>)
, (F.53)
</p>
<p>calculate eM , ei M , cos M and sin M .
</p>
<p>28The geometric multiplicity of these eigenvalues equals their algebraic multiplicity.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix F: Linear Algebra I 257
</p>
<p>6. Given A =
(
</p>
<p>1 0 2
</p>
<p>0 &minus;2i 1
</p>
<p>)
and B =
</p>
<p>

</p>
<p>1 0
</p>
<p>5i 2
</p>
<p>&minus;i 1
</p>
<p>
; determine, if defined, A2, AB,
</p>
<p>B A, B2, and (AB)2.
</p>
<p>7. Given
</p>
<p>A =
(
</p>
<p>1 0 2i
</p>
<p>3 &minus;2i &minus;2
</p>
<p>)
; (F.54)
</p>
<p>find AT and A&dagger;. Solution:
</p>
<p>AT =
</p>
<p>

</p>
<p>1 3
</p>
<p>0 &minus;2i
2i &minus;2
</p>
<p>
 ; A&dagger; =
</p>
<p>

</p>
<p>1 3
</p>
<p>0 2i
</p>
<p>&minus;2i &minus;2
</p>
<p>
 . (F.55)
</p>
<p>8. Given A =
(
</p>
<p>1 2
</p>
<p>&minus;2 &minus;1
</p>
<p>)
. Is A normal? Answer the same question for B =
</p>
<p>(
1 2
</p>
<p>3 4
</p>
<p>)
.
</p>
<p>Solution: With A&dagger; =
(
</p>
<p>1 &minus;2
2 &minus;1
</p>
<p>)
, it follows that
</p>
<p>(
1 2
</p>
<p>&minus;2 &minus;1
</p>
<p>)(
1 &minus;2
2 &minus;1
</p>
<p>)
=
</p>
<p>(
5 &minus;4
</p>
<p>&minus;4 5
</p>
<p>)
;
(
</p>
<p>1 &minus;2
2 &minus;1
</p>
<p>)(
1 2
</p>
<p>&minus;2 &minus;1
</p>
<p>)
=
</p>
<p>(
5 4
</p>
<p>4 5
</p>
<p>)
.
</p>
<p>(F.56)
</p>
<p>Hence, A is not normal.
</p>
<p>9. Show that the matrix
</p>
<p>A =
(
</p>
<p>0 1
</p>
<p>a 0
</p>
<p>)
; a &isin; R, a 	= 0 (F.57)
</p>
<p>is diagonalizable, but for a 	= 1 not normal.
Solution: To see if A is normal, we calculate
</p>
<p>AA&dagger; =
(
</p>
<p>0 1
</p>
<p>a 0
</p>
<p>)(
0 a
</p>
<p>1 0
</p>
<p>)
=
</p>
<p>(
1 0
</p>
<p>0 a2
</p>
<p>)
; A&dagger; A =
</p>
<p>(
0 a
</p>
<p>1 0
</p>
<p>)(
0 1
</p>
<p>a 0
</p>
<p>)
=
</p>
<p>(
a2 0
</p>
<p>0 1
</p>
<p>)
.
</p>
<p>(F.58)
</p>
<p>Hence, the matrix is not normal for a2 	= 1.
Concerning the diagonalization, we note that A has the eigenvalues 1 =
</p>
<p>&radic;
a
</p>
<p>and 2 = &minus;
&radic;
</p>
<p>a and the eigenvectors v1 = c1
(
</p>
<p>1&radic;
a
</p>
<p>)
and v2 = c2
</p>
<p>(
1
</p>
<p>&minus;&radic;a
</p>
<p>)
.
</p>
<p>Hence, for the transformation matrix T we can use the ansatz
</p>
<p>T =
(
</p>
<p>1 1&radic;
a &minus;&radic;a
</p>
<p>)
; T &minus;1 = 1
</p>
<p>2
&radic;
</p>
<p>a
</p>
<p>(&radic;
a 1&radic;
a &minus;1
</p>
<p>)
(F.59)</p>
<p/>
</div>
<div class="page"><p/>
<p>258 Appendix F: Linear Algebra I
</p>
<p>The check gives
</p>
<p>T &minus;1 AT = 1
2
&radic;
</p>
<p>a
</p>
<p>(&radic;
a 1&radic;
a &minus;1
</p>
<p>)(
0 1
</p>
<p>a 0
</p>
<p>)(
1 1&radic;
a &minus;&radic;a
</p>
<p>)
=
</p>
<p>(&radic;
a 0
</p>
<p>0 &minus;&radic;a
</p>
<p>)
. (F.60)
</p>
<p>Due to
</p>
<p>T &dagger; =
(
</p>
<p>1
&radic;
</p>
<p>a&lowast;
</p>
<p>1 &minus;
&radic;
</p>
<p>a&lowast;
</p>
<p>)
, (F.61)
</p>
<p>we have T &minus;1 	= T &dagger;. Thus, the transformation is not unitary (as is the case with
normal matrices).
</p>
<p>10. Show that a Hermitian matrix A is diagonalizable by a unitary transformation&mdash;
</p>
<p>in other words that there is a unitary matrix U with U&minus;1 AU = D.
Solution: The (nondegenerate) eigenvalue problem is Avn = cnvn . If we denote
the components of the vector vn by vn|m , we can write this using A =
</p>
<p>(
ai j
</p>
<p>)
as:
</p>
<p>Avn = cnvn or
&sum;
</p>
<p>j
</p>
<p>al jvn| j = cnvn|l . (F.62)
</p>
<p>For the transforming unitary matrix, we choose U with the components
</p>
<p>uk j = v j |k . (F.63)
</p>
<p>Thus, the columns of this matrix are the eigenvectors.
</p>
<p>We have to check whether
</p>
<p>U&minus;1 AU = D or AU = U D (F.64)
</p>
<p>holds, where D is a diagonal matrix with the entries di j = d j ji j . We have
</p>
<p>(AU )i j =
&sum;
</p>
<p>k
</p>
<p>aikuk j =
&sum;
</p>
<p>k
</p>
<p>aikv j |k = c jv j |i (F.65)
</p>
<p>and
</p>
<p>(U D)i j =
&sum;
</p>
<p>k
</p>
<p>uikdk j =
&sum;
</p>
<p>k
</p>
<p>uikd j jk j = ui j d j j = d j jv j |i (F.66)
</p>
<p>and clearly the last two results are identical for the choice d j j = c j .
To show that U is unitary, we use the fact that the columns of U are the eigen-
</p>
<p>vectors, which are mutually orthogonal and can be assumed to be normalized.
</p>
<p>It follows then that
</p>
<p>(
U &dagger;U
</p>
<p>)
i j
=
</p>
<p>&sum;
</p>
<p>k
</p>
<p>u&lowast;ki uk j =
&sum;
</p>
<p>k
</p>
<p>v&lowast;i |kv j |k = i j (F.67)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix F: Linear Algebra I 259
</p>
<p>and (
UU &dagger;
</p>
<p>)
i j
=
</p>
<p>&sum;
</p>
<p>k
</p>
<p>uiku
&lowast;
jk =
</p>
<p>&sum;
</p>
<p>k
</p>
<p>v&lowast;k|ivk| j = i j . (F.68)
</p>
<p>In case of degeneracy, the proof is somewhat more extensive, but analogous.
</p>
<p>11. Hermitian matrices are unitarily diagonalizable. Using this fact, prove that also
</p>
<p>normal matrices are unitarily diagonalizable.
</p>
<p>Solution: A matrix A is called normal, if
[
A, A&dagger;
</p>
<p>]
= 0. We see that the two
</p>
<p>matrices B = A + A&dagger; and C = i
(
</p>
<p>A &minus; A&dagger;
)
</p>
<p>are Hermitian; they commute and
</p>
<p>therefore can be diagonalized simultaneously by a unitary transformation. This
</p>
<p>we can write as
</p>
<p>U BU&minus;1 = U AU&minus;1 + U A&dagger;U&minus;1 = D
UCU&minus;1 = iU AU&minus;1 &minus; iU A&dagger;U&minus;1 = D&prime; (F.69)
</p>
<p>where D and D&prime; are diagonal matrices. Because of
</p>
<p>2U AU&minus;1 = D &minus; i D&prime;, (F.70)
</p>
<p>we have demonstrated the proposition.
</p>
<p>12. Calculate the determinant of A =
</p>
<p>

</p>
<p>1 2 3
</p>
<p>4 5 6
</p>
<p>7 8 9
</p>
<p>
.
</p>
<p>Solution: For practice, we calculate the determinant twice. The upper calculation
</p>
<p>is an expansion in terms of the first row, the lower of the second column. The
</p>
<p>results are of course identical.
</p>
<p>det
</p>
<p>

</p>
<p>1 2 3
</p>
<p>4 5 6
</p>
<p>7 8 9
</p>
<p>
 =
</p>
<p>


</p>
<p>1 &middot; det
(
</p>
<p>5 6
</p>
<p>8 9
</p>
<p>)
&minus; 2 &middot; det
</p>
<p>(
4 6
</p>
<p>7 9
</p>
<p>)
+ 3 &middot; det
</p>
<p>(
4 5
</p>
<p>7 8
</p>
<p>)
</p>
<p>&minus;2 &middot; det
(
</p>
<p>4 6
</p>
<p>7 9
</p>
<p>)
+ 5 &middot; det
</p>
<p>(
1 3
</p>
<p>7 9
</p>
<p>)
&minus; 8 &middot; det
</p>
<p>(
1 3
</p>
<p>4 6
</p>
<p>)
</p>
<p>=
{
</p>
<p>45 &minus; 48 &minus; 2 (36 &minus; 42)+ 3 (32 &minus; 35)
&minus;2 (36 &minus; 42)+ 5 (9 &minus; 21)&minus; 8 (6 &minus; 12) = 0.
</p>
<p>(F.71)
</p>
<p>13. Determine the eigenvalues and the polynomial factorization for the matrix
</p>
<p>A =
</p>
<p>

</p>
<p>i
&radic;
</p>
<p>3 0 0
</p>
<p>0 1 2
</p>
<p>0 &minus;2 &minus;1
</p>
<p>
 (F.72)
</p>
<p>as well as its trace and determinant.</p>
<p/>
</div>
<div class="page"><p/>
<p>260 Appendix F: Linear Algebra I
</p>
<p>Solution: We have
</p>
<p>p3 () = det (E &minus; A) =
</p>
<p>
</p>
<p>&minus; i
&radic;
</p>
<p>3 0 0
</p>
<p>0 &minus; 1 &minus;2
0 2 + 1
</p>
<p>
=
</p>
<p>(
&minus; i
</p>
<p>&radic;
3
) (
</p>
<p>2 + 3
)
.
</p>
<p>(F.73)
</p>
<p>The zeros of p () follow from this as
</p>
<p>1 = i
&radic;
</p>
<p>3; 2 = i
&radic;
</p>
<p>3; 3 = &minus;i
&radic;
</p>
<p>3, (F.74)
</p>
<p>and the polynomial factorization of the characteristic polynomial reads
</p>
<p>p3 () =
(
&minus; i
</p>
<p>&radic;
3
)2 (
</p>
<p>+ i
&radic;
</p>
<p>3
)
. (F.75)
</p>
<p>We find the double zero i
&radic;
</p>
<p>3 and the simple zero &minus;i
&radic;
</p>
<p>3.
</p>
<p>Trace and determinant are given by
</p>
<p>tr (A) = i
&radic;
</p>
<p>3+1&minus;1 = i
&radic;
</p>
<p>3;
&sum;
</p>
<p>j
</p>
<p> j = 1 = i
&radic;
</p>
<p>3+i
&radic;
</p>
<p>3&minus;i
&radic;
</p>
<p>3 = i
&radic;
</p>
<p>3 (F.76)
</p>
<p>and
</p>
<p>det(A) = i
&radic;
</p>
<p>3 (&minus;1 + 4) = 3i
&radic;
</p>
<p>3;
&prod;
</p>
<p>j
</p>
<p> j = 1 = i
&radic;
</p>
<p>3&middot;i
&radic;
</p>
<p>3&middot;
(
&minus;i
</p>
<p>&radic;
3
)
= 3i
</p>
<p>&radic;
3.
</p>
<p>(F.77)
</p>
<p>14. Given the eigenvalue problem
</p>
<p>Mv = v (F.78)
</p>
<p>with the matrix
</p>
<p>M =
(
</p>
<p>0 &minus;2i
2i 3
</p>
<p>)
, (F.79)
</p>
<p>determine the eigenvalues and the associated normalized eigenvectors. Are the
</p>
<p>two eigenvectors orthogonal?
</p>
<p>Solution: The eigenvalues are given as solutions of the secular equation:
</p>
<p>det
</p>
<p>(
&minus; &minus;2i
2i 3 &minus; 
</p>
<p>)
= 0 &rarr;  (&minus; 3)&minus; 4 = 0 &rarr; 1 = 4; 2 = &minus;1. (F.80)
</p>
<p>The eigenvectors v j =
(
</p>
<p>a j
b j
</p>
<p>)
are solutions of
</p>
<p>(
0 &minus;2i
2i 3
</p>
<p>)(
a j
b j
</p>
<p>)
=  j
</p>
<p>(
a j
b j
</p>
<p>)
(F.81)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix F: Linear Algebra I 261
</p>
<p>and are given by (without normalization):
</p>
<p>v1 = a1
(
</p>
<p>1
</p>
<p>2i
</p>
<p>)
; v2 = a2
</p>
<p>(
1
</p>
<p>&minus; i
2
</p>
<p>)
; (F.82)
</p>
<p>the normalized eigenvectors are:
</p>
<p>v1 =
1&radic;
5
</p>
<p>(
1
</p>
<p>2i
</p>
<p>)
; v2 =
</p>
<p>2&radic;
5
</p>
<p>(
1
</p>
<p>&minus; i
2
</p>
<p>)
. (F.83)
</p>
<p>For the scalar product, we find:
</p>
<p>v
&dagger;
1v2 =
</p>
<p>2
</p>
<p>5
</p>
<p>(
1 &minus;2i
</p>
<p>) ( 1
&minus; i
</p>
<p>2
</p>
<p>)
= 0. (F.84)
</p>
<p>Thus, the eigenvalues are orthogonal&mdash;as they must be, because M is Hermitian.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix G
</p>
<p>Linear Algebra II
</p>
<p>Quantum mechanics operates in complex vector spaces with scalar products. In this
</p>
<p>appendix, we compile some basic concepts.
</p>
<p>G.1 Groups
</p>
<p>Groups are important structures not only in linear algebra; they also occur in many
</p>
<p>areas of mathematical physics. They consist of a set of elements (finite or infinite),
</p>
<p>which can be combined by a calculation rule or operation, frequently written as +, &lowast;,
 or &times;. Here, the notation does not necessarily imply &lsquo;the usual&rsquo; arithmetic addition
or multiplication.
</p>
<p>Given a non-empty set of elements G and a binary operation &lowast;, whereby the
combination of two elements of the set is again an element of G (closure); then the
</p>
<p>pair (G, &lowast;) is called a group if it satisfies
&bull; a &lowast; (b &lowast; c) = (a &lowast; b) &lowast; c: the operation is associative.
&bull; There is a neutral element e with a &lowast; e = e &lowast; a = a.
&bull; For each a &isin; G, there is an inverse element a&minus;1 &isin; G with a &lowast; a&minus;1 = a&minus;1 &lowast; a = e.
&bull; If in addition, it holds that a &lowast; b = b &lowast; a, the group is called Abelian or commu-
</p>
<p>tative.
</p>
<p>If the operation is addition/multiplication, the group is called additive/multipli-
</p>
<p>cative; the neutral element is then the zero/the one, and the inverse element is &minus;a/ 1
a
</p>
<p>.
</p>
<p>Examples of Abelian groups are the real numbers with addition as the group
</p>
<p>operation and zero as a neutral element, or with multiplication as operation and one
</p>
<p>as neutral element (in the latter case, zero must be eliminated, because it has no
</p>
<p>inverse). An example of a non-Abelian group are the invertible n &times; n matrices with
matrix multiplication as operation.
</p>
<p>Because of the very general definition of groups, &lsquo;all sorts of things&rsquo; can form
</p>
<p>a group; well-known examples from physics and symmetry transformations are
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>263</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>264 Appendix G: Linear Algebra II
</p>
<p>rotations and reflections, or the Lorentz transformations. We consider in the fol-
</p>
<p>lowing some cases explicitly.
</p>
<p>First, an example of a discrete group (countably many elements): The parity trans-
</p>
<p>formation P has the eigenvalues &plusmn;1 (because of P2 = 1). The group corresponding
to P is the multiplicative group with the two elements 1 and &minus;1, the group Z2.
</p>
<p>Continuous groups have uncountably many elements. An example is the general
</p>
<p>linear group GL(n, K ). It is the group of all invertible n &times; n matrices with elements
of the field K (for us, either R or C; if it is clear which set is meant, one usually omits
</p>
<p>K ). Restricting this set to the matrices with determinant 1, one obtains the special
</p>
<p>linear group SL(n).
</p>
<p>Special cases of GL(n) are the unitary group U (n) and the orthogonal group O(n),
</p>
<p>i.e. the groups of unitary and orthogonal n &times; n matrices. If we restrict ourselves to
matrices with determinant 1, we obtain the special unitary group SU (n) and the
</p>
<p>special orthogonal group SO(n). To give a concrete example: SO(3) is the group of
</p>
<p>all rotations in three dimensions around an axis passing through (0, 0, 0).
</p>
<p>The group GL(n, K ) and its subsets are groups that form a continuum, which is
</p>
<p>obvious from the (older) names continuous or continuous group. Today, however,
</p>
<p>they are usually called Lie groups.
</p>
<p>G.2 Vector Spaces
</p>
<p>One can imagine that the concept of vector space actually originated from the &lsquo;arrows&rsquo;
</p>
<p>or the vectors of physics. But it turns out that there are many sets of very different
</p>
<p>objects that follow the same rules of calculation, i.e. have the same structure. For
</p>
<p>this reason, one abstracted from the &lsquo;arrows&rsquo; and defined the structure itself.
</p>
<p>A non-empty set V is called a vector space over a field K (for us almost exclusi-
</p>
<p>vely C), if in V an addition and a multiplication operation with numbers from K are
</p>
<p>defined,29 where the usual rules of vector calculus apply. These are:
</p>
<p>With u and v in the space, u + v also belongs to the vector space. In addition, V
contains a specific element 0, and the following rules are valid30:
</p>
<p>&bull; u + v = v + u: commutativity of the addition;
&bull; u + (v + w) = (u + v)+ w: associativity of the addition;
&bull; u + 0 = u: existence of the zero element;
&bull; u + x = v has always exactly one solution x .
</p>
<p>With u &isin; V ,  &isin; C, then  &middot; u also belongs to the vector space, and the following
rules hold:
</p>
<p>&bull; (+ ) &middot; u =  &middot; u +  &middot; u: distributive property;
</p>
<p>29Note that it is thus postulated that one can add two states and multiply a state by a number. This
</p>
<p>is a strong requirement which many state spaces do not meet. An example: the state space of all
</p>
<p>possible positions on a chessboard.
30It is an additive Abelian group.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix G: Linear Algebra II 265
</p>
<p>&bull;  &middot; (u + v) =  &middot; u +  &middot; v: distributive property;
&bull; ( &middot; ) &middot; u = () &middot; u: associative property of the multiplication;
&bull; 1 &middot; u = u: existence of the unit element.
</p>
<p>Elements of V are called vectors, elements of the field K scalars.
</p>
<p>There are many concrete examples of vector spaces; we mention the space of
</p>
<p>n &times; n matrices, the space of polynomials of degree n, the space of the functions
continuous within the interval 0 &le; x &le; 1, the space of the solutions of a linear
differential equation such as the wave equation or the Schr&ouml;dinger equation, the
</p>
<p>space of sequences x = (x0, x1, x2, . . .). In these spaces, addition and multiplication
are the usual operations.
</p>
<p>We point out again that in this context (so to speak, in the algebraic sense), all
</p>
<p>possible objects may be called vectors, insofar as they are elements of a vector
</p>
<p>space&mdash;functions, polynomials, matrices, etc., and also the solutions of the SEq.
</p>
<p>This is not because they were set up like a column vector, but simply because they
</p>
<p>are elements of a vector space. It is certainly advisable to distinguish between the
</p>
<p>meanings of &lsquo;physical&rsquo; and &lsquo;algebraic&rsquo; vectors.
</p>
<p>G.3 Scalar Product
</p>
<p>Considering the concepts of scalar product (angle), norm (length) and metric (dis-
</p>
<p>tance), one can also imagine that they arose in the context of &lsquo;arrows&rsquo;; in connection
</p>
<p>with the abstraction process leading to the vector space, they were correspondingly
</p>
<p>abstracted from the actual objects and the structure was set up and expanded, e.g. to
</p>
<p>include complex numbers or general vectors, among other things.
</p>
<p>A scalar product, written here as (x, y), assigns a scalar to two elements x, y &isin; V .
It must meet the following requirements: The scalar product is
</p>
<p>1. positive definite
</p>
<p>(x, x) &ge; 0; (x, x) = 0 &harr; x = 0;
</p>
<p>2. linear (more exactly: semilinear in the first component, linear in the second
</p>
<p>(sesquilinearity))31
</p>
<p>(x,y + z) =  (x, y)+  (x, z) ; (G.1)
</p>
<p>3. Hermitian or conjugate symmetric
</p>
<p>(x, y) = (y, x)&lowast; . (G.2)
</p>
<p>31A sesquilinear form is a function that assigns a number to two vectors, and which is linear in
</p>
<p>one and antilinear in the other argument. A sesqulinear form with Hermitian symmetry is called a
</p>
<p>Hermitian form.</p>
<p/>
</div>
<div class="page"><p/>
<p>266 Appendix G: Linear Algebra II
</p>
<p>Due to the last equation, we always have (x, x) &isin; R. Clearly, the expression&int;
f &lowast;gdV is a scalar product, ( f, g) =
</p>
<p>&int;
f &lowast;gdV , or in our preferred notation,
</p>
<p> f |g.32
There is also the notation antilinear in the first argument, linear in the second argu-
</p>
<p>ment. In mathematics, this is usually defined the other way around&mdash;there, generally,
</p>
<p>the second element is complex conjugated, not as here the first one.
</p>
<p>G.4 Norm
</p>
<p>The norm intuitively means simply the length of a vector (as an element of a vector
</p>
<p>space). The properties of a (general) norm, written here as   (or with the alternative
notation | |), are
1. x &ge; 0; x = 0 &harr; x = 0;
2. x = || &middot; x;
3. x + y &le; x + y (triangle inequality).
</p>
<p>Clearly, the expression
</p>
<p>&radic;&int;
f &lowast; f dV is a norm.
</p>
<p>G.5 Metric
</p>
<p>We do not need this term for our discussion of quantum mechanics, but we include it
</p>
<p>for completeness: A distance term (= metric) can be defined by d(x, y) = x &minus; y.
A general metric must meet the requirements:
</p>
<p>1. d(x, y) &isin; R, 0 &le; d (x, y) &lt; &infin;;
2. d(x, y) = 0 &harr; x = y;
3. d(x, y) &le; d(x, z)+ d(z, y) (triangle inequality);
4. d(x, y) = d(y, x).
</p>
<p>32The insight that
&int;
</p>
<p>f &lowast;gdV is a scalar product is apparently only about 100 years old. So all those
who did not see this at first glance may take comfort from the consideration that, evidently, this fact
</p>
<p>does not spring to everyone&rsquo;s eye immediately.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix G: Linear Algebra II 267
</p>
<p>G.6 Schwarz&rsquo;s Inequality
</p>
<p>The Schwarz inequality33 establishes an important connection between the scalar
</p>
<p>product and the norms of two vectors. For the familiar &lsquo;arrow vectors&rsquo;, the scalar
</p>
<p>product is intuitively the product of the length of the first vector times the length of
</p>
<p>the vertical projection of the second vector onto the first vector (see Appendix F,
</p>
<p>Vol. 1). Hence it is clear that this quantity is smaller (or equal for parallel vectors)
</p>
<p>than the product of the lengths of the two vectors. The Schwarz inequality generalizes
</p>
<p>this relation. It reads:
</p>
<p>|(x, y)| &le; ||x || &middot; ||y|| . (G.3)
</p>
<p>Proof: The inequality is fulfilled, if x = 0 or y = 0. Otherwise, it follows with
 = y2 &isin; R and  = &minus;(y, x) that:
</p>
<p>0 &le; (x + y,x + y) = 2 x2 + (x, y)+ &lowast;(y, x)+ &lowast; y2
= 2 x2 &minus; y2 |(x, y)|2 &minus; y2 |(x, y)|2 + |(x, y)|2 y2
</p>
<p>= y4 x2 &minus; y2 |(x, y)|2 .
(G.4)
</p>
<p>Because of y 	= 0, the inequality |(x, y)|2 &le; x2 y2 follows.
</p>
<p>G.7 Orthogonality
</p>
<p>We write the Schwarz inequality for ||x || , ||y|| 	= 0 in the form
</p>
<p>|(x, y)|
||x || &middot; ||y|| &le; 1. (G.5)
</p>
<p>We assume for the moment a real vector space and carry over the only interesting
</p>
<p>result to the complex case. Thus, x and y are elements of a real vector space. Then
</p>
<p>we can write the last equation as
</p>
<p>&minus; 1 &le; (x, y)||x || &middot; ||y|| &le; 1. (G.6)
</p>
<p>33This inequality may be considered as one of the most important inequalities in mathemat-
</p>
<p>ics. It is also called Cauchy&ndash;Schwarz, Bunyakovsky, Cauchy&ndash;Bunyakovsky&ndash;Schwarz or Cauchy&ndash;
</p>
<p>Bunyakovsky inequality. Quote (http://en.wikipedia.org/wiki/Cauchy-Schwarz_inequality, August
</p>
<p>2012): &ldquo;The inequality for sums was published by Augustin-Louis Cauchy (1821), while the
</p>
<p>corresponding inequality for integrals was first stated by Viktor Bunyakovsky (1859) and redis-
</p>
<p>covered by Hermann Amandus Schwarz (1888)&rdquo;. We see again how careless and unfair history
</p>
<p>may be.</p>
<p/>
<div class="annotation"><a href="http://en.wikipedia.org/wiki/Cauchy-Schwarz_inequality">http://en.wikipedia.org/wiki/Cauchy-Schwarz_inequality</a></div>
</div>
<div class="page"><p/>
<p>268 Appendix G: Linear Algebra II
</p>
<p>This allows us to define an abstract angle x,y (up to multiples of 2) between x
</p>
<p>and y (as elements of the vector space, not as functions of the position!), namely
</p>
<p>(x, y)
</p>
<p>||x || &middot; ||y|| = cosx,y . (G.7)
</p>
<p>In particular, x and y are perpendicular to each other (= are orthogonal) iff (x, y) = 0.
We transfer this result to complex vector spaces (in fact, only this result is of interest).
</p>
<p>In other words, two vectors (or states or wavefunctions)  	= 0 and 
 	= 0 are
orthogonal iff it holds that (,
) = 0, i.e.
</p>
<p>(,
) = 0 &harr;  &perp; 
. (G.8)
</p>
<p>As may be seen, the zero vector is orthogonal to itself and all other vectors.
</p>
<p>G.8 Hilbert Space
</p>
<p>Hilbert spaces are special vector spaces, which have a high degree of structure and
</p>
<p>therefore have very useful properties for all possible calculations. We introduced
</p>
<p>them in Chap. 11 and summarize here some of their basic properties.
</p>
<p>In general, a vector space with a scalar product is called scalar product space or
</p>
<p>pre-Hilbert space, where one distinguishes between the Euclidean (real case) and
</p>
<p>the unitary (complex case) vector space. A complete pre-Hilbert space is called a
</p>
<p>Hilbert space H.34
</p>
<p>A space is called complete if every Cauchy sequence35 of elements of this space
</p>
<p>converges. For example, the space of rational numbers (i.e. fractions) is not complete
</p>
<p>because sequences of rational numbers can converge to real numbers, as the example
</p>
<p>of
(
1 + 1
</p>
<p>n
</p>
<p>)n
shows. Here we have a rational number for any finite n, while the
</p>
<p>sequence converges towards the real (transcendental) number e which does not belong
</p>
<p>to the rational numbers.
</p>
<p>In Hilbert spaces, the parallelogram law holds:
</p>
<p>x + y2 + x &minus; y2 = 2
(
x2 + y2
</p>
<p>)
. (G.9)
</p>
<p>Furthermore, we can form an orthonormal basis in Hilbert spaces. This is a set of
</p>
<p>normalized vectors {vn &isin; H} which are pairwise orthogonal and whose linear span
(the set of all their linear combinations) is the entire Hilbert space (completeness).
</p>
<p>Because of these properties, such a basis is called a complete orthonormal system,
</p>
<p>or CONS.
</p>
<p>34Spaces with |  &lt; 0 are called &lsquo;pseudo-Hilbert spaces&rsquo;.
35A sequence {an} is called a Cauchy sequence if for every  &gt; 0 there is a N () &isin; N such that for
all n,m &gt; N (), the inequality an &minus; am &lt;  holds. Note that there is no limit in this definition.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix G: Linear Algebra II 269
</p>
<p>The Hilbert spaces we consider are separable,36 i.e. they have a CONS of at most
</p>
<p>countably infinite dimension.
</p>
<p>Furthermore, according to a theorem, there are in each Hilbert space finite sets
</p>
<p>of self-adjoint operators which commute pairwise and whose common eigenvectors
</p>
<p>form a basis of the Hilbert space without degeneracy. This set is called CSCO (com-
</p>
<p>plete system of commuting observables); an example is given in Chap. 17, Vol. 2
</p>
<p>(hydrogen atom).
</p>
<p>For Hilbert spaces, the saying is apt: &ldquo;If you know one, you know them all&rdquo;.
</p>
<p>More precisely, all Hilbert spaces which are separable and have the same dimension
</p>
<p>(finite or infinite), are isomorphic (i.e. geometrically identical; there are one-to-one
</p>
<p>length-preserving mappings between these spaces). Therefore, one often speaks of
</p>
<p>the Hilbert space of dimension n or &infin;.
There are very different realizations of H; we want to present two of them.
</p>
<p>The prototype of all Hilbert spaces of dimension &infin; is the sequence space l(2).
It consists of all infinite sequences of complex numbers x = (x1, x2, x3, . . .) with
the scalar product (x, y) =
</p>
<p>&sum;
n
</p>
<p>x&lowast;n yn and the property that the sum of the absolute
</p>
<p>squares is finite: (x, x) =
&sum;
</p>
<p>n
|xn|2 &lt; &infin;. This space was introduced in 1912 by
</p>
<p>David Hilbert, after whom these spaces are named.37 We note that the spaces l(p) for
</p>
<p>p 	= 2 are not Hilbert spaces; see the exercises.
Another space, important for quantum mechanics, is L(2) (a, b), the space of the
</p>
<p>functions which are square integrable in the interval (a, b) (here we restrict the
</p>
<p>discussion to one dimension). The scalar product is defined as
</p>
<p>&int; b
</p>
<p>a
</p>
<p>f &lowast; (x) g (x) dx ;
</p>
<p>and for the norm,
</p>
<p> f  =
</p>
<p>

</p>
<p>b&int;
</p>
<p>a
</p>
<p>| f (x)|2 dx
</p>
<p>

</p>
<p>1/2
</p>
<p>&lt; &infin; (G.10)
</p>
<p>must hold. If the limits of the integral are infinite, one writes L(2) (&minus;&infin;,&infin;) or
L(2) (R) . An extension encompasses the L(p) spaces with the norm
</p>
<p> f  =
</p>
<p>

</p>
<p>b&int;
</p>
<p>a
</p>
<p>| f (x)|p dx
</p>
<p>

</p>
<p>1/p
</p>
<p>. (G.11)
</p>
<p>However, these spaces are not Hilbert spaces for p 	= 2; see the exercises.
Hilbert spaces occur in many different areas of mathematics and physics, such
</p>
<p>as in the spectral theory of ordinary differential equations, the theory of partial
</p>
<p>differential equations, the ergodic theory, Fourier analysis, quantum mechanics, and
</p>
<p>36Non-separable Hilbert spaces occur e.g. in the quantization of fields.
37The axiomatic definition of the Hilbert space was given in 1927 by J. von Neumann, in the context
</p>
<p>of the mathematical treatment of quantum mechanics.</p>
<p/>
</div>
<div class="page"><p/>
<p>270 Appendix G: Linear Algebra II
</p>
<p>others. For example, in Fourier analysis one can expand 2 periodic functions f (x)
</p>
<p>(see Appendix H, Vol. 1)
</p>
<p>f (x) =
&infin;&sum;
</p>
<p>n=&minus;&infin;
fne
</p>
<p>inx ; fn =
1
</p>
<p>2
</p>
<p>&int;
</p>
<p>&minus;
</p>
<p>f (x) e&minus;inx dx . (G.12)
</p>
<p>One can show that the functions einx form an orthogonal basis in the space L2, i.e. that
</p>
<p>the Fourier expansion holds for all functions in this space. Other known expansions
</p>
<p>use wavelets or spherical harmonics (e.g. the multipole expansion) as CONS.
</p>
<p>G.9 C&lowast; Algebra
</p>
<p>In footnotes, we have pointed out that one can formulate a (rather abstract) entry
</p>
<p>into quantum mechanics by making use of a C&lowast; algebra. More precisely, we refer
to the fact that the observables of classical mechanics (e.g. polynomials of phase-
</p>
<p>space variables) and quantum mechanics have this structure, although, of course, in
</p>
<p>different forms. For completeness and as an example of advanced formulations, we
</p>
<p>briefly give the basic definitions:
</p>
<p>We start with a complex vector space V , in which as above the operations +
and &middot; are defined for x, y &isin; V and  &isin; C. If we define in addition for x, y &isin; V a
multiplication x  y, which is associative and distributive and has a unit element, we
have a complex algebra A. It is:
</p>
<p>(x  y)  z = x  (y  z) ; (x + y)  z = x  z + y  z
 (x  y) = (x)  y; 1  x = x  1 = x . (G.13)
</p>
<p>A normed algebra is an algebra where for x &isin; V , a norm x is defined. A &lowast;
algebra (pronounced star algebra) is an algebra with a mapping &lowast;, called involution,
with (
</p>
<p>x&lowast;
)&lowast; = x; (x)&lowast; = x&lowast;; (x  y)&lowast; = y&lowast;  x&lowast;. (G.14)
</p>
<p>Here, we have denoted the complex conjugation with  in order to distinguish it from
</p>
<p>the &lowast; mapping.
A Banach algebra is a normed &lowast; algebra with the condition x = x&lowast;, and a
</p>
<p>C&lowast; algebra (pronounced C-star algebra) is a Banach algebra in which the so-called
C&lowast; condition x&lowast;  x = x2 applies.
</p>
<p>An example of a C&lowast; algebra are the complex quadratic n &times; n matrices with a
correspondingly defined norm. The &lowast; mapping means in this case taking the adjoint.
In other words, the C&lowast; algebra can be thought of as an abstraction of bounded linear
operators on Hilbert spaces.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix G: Linear Algebra II 271
</p>
<p>G.10 Exercises
</p>
<p>1. Vector space; the + operation is the usual addition, the field is R.
Which of the following sets is a vector space? The set of
</p>
<p>(a) the natural numbers;
</p>
<p>(b) the rational numbers;
</p>
<p>(c) the functions continuous on the interval (&minus;1, 1);
(d) all 4 &times; 4-matrices?
</p>
<p>2. Consider the 2-periodic functions. Do they form a vector space?
</p>
<p>3. Derive from the Schwarz inequality |ab| &le; |a| |b| that ab = |a| |b| cos holds.
4. Show that the scalar product of two states |x and |y does not depend on the
</p>
<p>representation.
</p>
<p>Solution: Intuitively, the assertion is clear, since the scalar product is a projection
</p>
<p>of one state onto another one. In order to show this also formally, we start with
</p>
<p>two basis systems {|l , l = 1, 2, . . .} and {|l , l = 1, 2, . . .}, which we can
assume to be CONS without loss of generality. The transformation between the
</p>
<p>two bases is
</p>
<p>|l =
&sum;
</p>
<p>j
</p>
<p>l j
 j
</p>
<p>&rang;
. (G.15)
</p>
<p>Since they are both CONS, we have
</p>
<p>nn&prime; = n&prime; | n =
&sum;
</p>
<p>j &prime; j
</p>
<p>&lowast;n&prime; j &prime;
&lang;
 j &prime;
</p>
<p> nj
 j
</p>
<p>&rang;
=
</p>
<p>&sum;
</p>
<p>j
</p>
<p>&lowast;njnj . (G.16)
</p>
<p>The expansion of the states in terms of the two CONS reads
</p>
<p>|x =
&sum;
</p>
<p>l
</p>
<p>cxl |l =
&sum;
</p>
<p>l
</p>
<p>dxl |l
</p>
<p>|y =
&sum;
</p>
<p>l
</p>
<p>cyl |l =
&sum;
</p>
<p>l
</p>
<p>dyl |l .
(G.17)
</p>
<p>Because of (G.15), the expansions in terms of {|l} and {|l} are related by
</p>
<p>|x =
&sum;
</p>
<p>m
</p>
<p>cxm |m =
&sum;
</p>
<p>n
</p>
<p>dxn |n =
&sum;
</p>
<p>nm
</p>
<p>dxnnm |m (G.18)
</p>
<p>and analogously for |y, which leads to
</p>
<p>cxm =
&sum;
</p>
<p>n
</p>
<p>dxnnm; cym =
&sum;
</p>
<p>n
</p>
<p>dynnm . (G.19)
</p>
<p>For the scalar product, it follows in the basis {|l} that</p>
<p/>
</div>
<div class="page"><p/>
<p>272 Appendix G: Linear Algebra II
</p>
<p>x | y =
&sum;
</p>
<p>mm &prime;
</p>
<p>c&lowast;xm &prime; m | cym |m =
&sum;
</p>
<p>mm &prime;
</p>
<p>c&lowast;xm &prime;cymm &prime;m =
&sum;
</p>
<p>m
</p>
<p>c&lowast;xmcym (G.20)
</p>
<p>and analogously in the basis {|l},
</p>
<p>x | y =
&sum;
</p>
<p>m
</p>
<p>d&lowast;xmdym . (G.21)
</p>
<p>So we have to prove the equality
&sum;
</p>
<p>m
c&lowast;xmcym =
</p>
<p>&sum;
m
</p>
<p>d&lowast;xmdym . With (G.16) and
(G.19), we obtain
</p>
<p>&sum;
</p>
<p>m
</p>
<p>c&lowast;xmcym =
&sum;
</p>
<p>m
</p>
<p>&sum;
</p>
<p>n&prime;
</p>
<p>d&lowast;xn&prime;
&lowast;
n&prime;m
</p>
<p>&sum;
</p>
<p>n
</p>
<p>dynnm
</p>
<p>=
&sum;
</p>
<p>n&prime;n
</p>
<p>d&lowast;xn&prime;dyn
&sum;
</p>
<p>n
</p>
<p>&lowast;n&prime;mnm =
&sum;
</p>
<p>n&prime;n
</p>
<p>d&lowast;xn&prime;dynn&prime;n =
&sum;
</p>
<p>n
</p>
<p>d&lowast;xndyn .
(G.22)
</p>
<p>5. Prove the parallelogram law which applies in a Hilbert space:
</p>
<p>x + y2 + x &minus; y2 = 2
(
x2 + y2
</p>
<p>)
. (G.23)
</p>
<p>Solution: We have
</p>
<p>x + y2 + x &minus; y2 = (x + y, x + y)2 + (x &minus; y, x &minus; y)2
= (x, x)2 + (x, y)+ (y, x)+ (y, y)2 + (x, x)2 &minus; (x, y)&minus; (y, x)+ (y, y)2
</p>
<p>= 2
(
x2 + y2
</p>
<p>)
.
</p>
<p>(G.24)
</p>
<p>6. Show that l p with p 	= 2 is not a Hilbert space.
Solution: In a l p space, the norm of x = (x1, x2, x3, . . .) is defined by
</p>
<p>x =
(&sum;
</p>
<p>n
</p>
<p>|xn|p
)1/p
</p>
<p>. (G.25)
</p>
<p>We show that this norm does not satisfy the parallelogram law which applies
</p>
<p>in Hilbert spaces. For a proof we need just one counterexample; we choose
</p>
<p>x = (1, 1, 0, 0, . . .) and y = (1,&minus;1, 0, 0, . . .). It follows then that
</p>
<p>x + y = x &minus; y = 2 and x = y = 21/p. (G.26)
</p>
<p>Obviously, the parallelogram law is satisfied only for p = 2. Thus, the claim is
proved.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix H
</p>
<p>Fourier Transforms and the Delta Function
</p>
<p>H.1 Fourier Transforms
</p>
<p>With the Fourier series, one can represent and analyze periodic functions; for aperi-
</p>
<p>odic functions we use the Fourier transformation.
</p>
<p>Definition: Given a function f (x) with
</p>
<p>f (x) = 1&radic;
2
</p>
<p>+&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>f (k) eikx dk. (H.1)
</p>
<p>Then the function f (k) can be calculated by
</p>
<p>f (k) = 1&radic;
2
</p>
<p>+&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>f (x) e&minus;ikx dx . (H.2)
</p>
<p>One calls f (x) also a function in position space, f (k) a function in momentum
</p>
<p>space (because of p = k).38
In three dimensions, the transformations are given by39
</p>
<p>38Another common definition of the Fourier transform uses an asymmetric normalization:
</p>
<p>f (x) =
+&infin;&int;
</p>
<p>&minus;&infin;
f (k) eikx dk; f (k) = 1
</p>
<p>2
</p>
<p>+&infin;&int;
</p>
<p>&minus;&infin;
f (x) e&minus;ikx dx .
</p>
<p>39Besides dkx dkydkz and d
3k, there are other notations for volume elements, cf. Appendix D, Vol. 1.
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>273</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>274 Appendix H: Fourier Transforms and the Delta Function
</p>
<p>f (r) = 1
(2)3/2
</p>
<p>+&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>f (k) eikrdkx dkydkz =
1
</p>
<p>(2)3/2
</p>
<p>+&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>f (k) eikrd3k (H.3)
</p>
<p>and
</p>
<p>f (k) = 1
(2)3/2
</p>
<p>+&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>f (r) e&minus;ikrdxdydz (H.4)
</p>
<p>with r = (x, y, z) and k =
(
kx , ky, kz
</p>
<p>)
.
</p>
<p>Closer inspection shows that a broad spatial distribution f (x) is related to a narrow
</p>
<p>momentum distribution f (k), and vice versa; cf. exercises. This can be seen very
</p>
<p>clearly in the extreme case of the function f (x) = f0eikx . Here, k (and hence p) is
a well-defined real number: Thus we have p = 0; hence the position uncertainty
must be, cum grano salis, infinite, x = &infin;. Indeed, f (x) = f0eikx is &lsquo;smeared&rsquo; all
over a large region (in fact, it is infinite), | f (x)| = | f0| for all x . The next question
is that for an object in position space with position uncertainty zero and infinite
</p>
<p>momentum uncertainty; as we shall see below, this is the so-called delta function.
</p>
<p>Fourier transformation is a linear operation. The functions involved, f (x) and
</p>
<p>f (k), are in general complex. For real f (x), we have f (k) = f &lowast; (&minus;k). The Fourier
transform of f &prime;(x) is ik f (k), and a shifted function g(x) = f (x &minus;a) has the Fourier
transform g(k) = e&minus;ika f (k) (proofs of these properties are found in the exercises).
</p>
<p>Finally, we mention the convolution theorem. A convolution of the functions f
</p>
<p>and g is an operation providing a third function h of the form
</p>
<p>+&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>f (x &minus; y) g (y) dy = h (x) . (H.5)
</p>
<p>Such a convolution corresponds to the product of the Fourier transforms (for the
</p>
<p>proof see the exercises):
</p>
<p>h(k) = 1&radic;
2
</p>
<p>f (k) g(k). (H.6)
</p>
<p>H.2 The Delta Function
</p>
<p>The delta function (also called Dirac delta function or Dirac function after its
</p>
<p>&lsquo;inventor&rsquo;, P.A.M. Dirac; for short, -function) is an important tool not only for
</p>
<p>the mathematical formulation of quantum mechanics.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix H: Fourier Transforms and the Delta Function 275
</p>
<p>H.2.1 Formal Derivation
</p>
<p>Starting point are the equations for the Fourier transform,
</p>
<p>f (x) = 1&radic;
2
</p>
<p>&infin;&int;
&minus;&infin;
</p>
<p>dk f (k) eikx
</p>
<p>f (k) = 1&radic;
2
</p>
<p>&infin;&int;
&minus;&infin;
</p>
<p>dx &prime; f
(
x &prime;
)
</p>
<p>e&minus;ikx
&prime;
.
</p>
<p>(H.7)
</p>
<p>This notation (quite popular in physics, but apparently less welcome in mathematics)
</p>
<p>of the integral (position of dk, dx) signifies that the integration over k acts rightward
</p>
<p>on all terms, until an addition sign, a bracket or an equals sign occurs. This is a
</p>
<p>conventional notation which, for example, allows multiple integrals to be written
</p>
<p>more concisely and is therefore often used in this context. Inserting f (k) on the
</p>
<p>right side of the first equation leads to:
</p>
<p>f (x) = 1
2
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>dk
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>dx &prime; f
(
x &prime;
)
</p>
<p>e&minus;ikx
&prime;
eikx . (H.8)
</p>
<p>We assume (as always) that we can interchange the integrations, which gives:
</p>
<p>f (x) = 1
2
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>dx &prime;
&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>dk f
(
x &prime;
)
</p>
<p>e&minus;ikx
&prime;
eikx . (H.9)
</p>
<p>f
(
x &prime;
)
</p>
<p>does not depend on k and thus can be taken out of the second integral:
</p>
<p>f (x) =
&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>dx &prime; f
(
x &prime;
) 1
</p>
<p>2
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>dkeik(x&minus;x
&prime;). (H.10)
</p>
<p>The further considerations are based on the fact that the function f on the left side
</p>
<p>appears as f (x) and on the right side under the integral as f (x &prime;). The term
</p>
<p>1
</p>
<p>2
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>dk eik(x&minus;x
&prime;) (H.11)
</p>
<p>depends only on x and x &prime; (k is the integration variable). It is called the delta function
(-function)40:
</p>
<p>40Actually, this name is incorrect, because it is not a function in the usual sense. Below, a few
</p>
<p>comments are made on this issue.</p>
<p/>
</div>
<div class="page"><p/>
<p>276 Appendix H: Fourier Transforms and the Delta Function
</p>
<p>
(
x &minus; x &prime;
</p>
<p>)
= 1
</p>
<p>2
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>dk eik(x&minus;x
&prime;). (H.12)
</p>
<p>Thus, (H.10) can be written
</p>
<p>f (x) =
&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>dx &prime; f
(
x &prime;
)

(
x &minus; x &prime;
</p>
<p>)
. (H.13)
</p>
<p>Evidently, the -function projects the function f out of the integral, and does this
</p>
<p>at the value for which the argument of the -function vanishes.
</p>
<p>H.2.2 Heuristic Derivation of the Delta Function
</p>
<p>The -function can be thought of as an infinitely high and infinitely thin needle at
</p>
<p>the position x &minus; x &prime; = 0, as the following derivation shows.
We assume an interval (x &prime; &minus; , x &prime; + ) on the x-axis, a rectangular function
</p>
<p>H (x) =
{
</p>
<p>1
2
</p>
<p>0
</p>
<p>for x &prime; &minus;  &lt; x &lt; x &prime; + 
otherwise
</p>
<p>(H.14)
</p>
<p>of area 1, and an arbitrary function f (x). For the integral over the product of the
</p>
<p>two functions, we find:
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>f (x) H (x) dx =
x &prime;+&int;
</p>
<p>x &prime;&minus;
</p>
<p>f (x) H (x) dx = 1
2
</p>
<p>x &prime;+&int;
</p>
<p>x &prime;&minus;
</p>
<p>f (x) dx . (H.15)
</p>
<p>We rearrange the last integral using the first mean value theorem for integration
</p>
<p>(see Appendix D, Vol. 1), which states that there is a value  (which value this is, is
</p>
<p>not revealed by the theorem), such that
</p>
<p>b&int;
</p>
<p>a
</p>
<p>g (x) dx = (b &minus; a) g () with a &le;  &le; b. (H.16)
</p>
<p>It then follows that
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>f (x) H (x) dx = 1
2
</p>
<p>x &prime;+&int;
</p>
<p>x &prime;&minus;
</p>
<p>f (x) dx = f () ; x &prime; &minus;  &le;  &le; x &prime; + . (H.17)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix H: Fourier Transforms and the Delta Function 277
</p>
<p>Fig. H.1 On the derivation
</p>
<p>of the delta function
</p>
<p>x'+
</p>
<p>2(/1 )
</p>
<p>f(x)
</p>
<p>f,H
</p>
<p>x'-  x'  x
</p>
<p>
</p>
<p>Now we let the interval boundaries approach x &prime; (i.e.  &rarr; 0). This means that
the width of the interval shrinks to 0, while the height of the rectangular function
</p>
<p>approaches infinity, since its area has the fixed value 1; see Fig. H.1. This function
</p>
<p>becomes more and more like an infinitely high and infinitely thin needle, as said
</p>
<p>above. Formally, we obtain in the limit  &rarr; 0:
&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>f
(
x &prime;
)

(
x &minus; x &prime;
</p>
<p>)
dx &prime; = f (x) . (H.18)
</p>
<p>This derivation starting from a rectangular function is not the only possible one;
</p>
<p>there are many others. They have in common the assumption of a &lsquo;proper&rsquo; function,
</p>
<p>which in the limit (zero width, height tends to infinity, area remains constant) becomes
</p>
<p>the delta function. These functions are called representations of the delta function,
</p>
<p>whereby one always keeps in mind that the limit is to be taken at some point in the
</p>
<p>calculation. We give a small selection of representing functions:
</p>
<p> (x) = 12 for &minus;  &lt; x &lt;  (rectangular function);
 (x) = 1&radic; e&minus;x
</p>
<p>2/2 (Gaussian function);
 (x) = x2 sin2
</p>
<p>(
x

</p>
<p>)
;
</p>
<p> (x) = 1 x2+2 ;
</p>
<p> (x) = 12
&infin;&int;
</p>
<p>&minus;&infin;
dk eikx e&minus;|k|.
</p>
<p>(H.19)
</p>
<p>As we said above, at some point the limit  &rarr; 0 must be taken. Knowing this,
one frequently omits the convergence generating factor e&minus;|k| in the last equation.
Alternatively, one can write
</p>
<p> (x) = lim
&rarr;0
</p>
<p>1
</p>
<p>
&radic;

</p>
<p>e&minus;x
2/2 (H.20)
</p>
<p>and accordingly for the other representations.</p>
<p/>
</div>
<div class="page"><p/>
<p>278 Appendix H: Fourier Transforms and the Delta Function
</p>
<p>If one picks out a suitable representation, certain properties of the delta function
</p>
<p>can quite easily be proven.
</p>
<p>H.2.3 Examples, Properties, Applications
</p>
<p>Examples: The argument of the -function in the integral
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>dy
(
y2 + 4
</p>
<p>)
 (y &minus; 1) = 1 + 4 = 5 (H.21)
</p>
<p>vanishes for y = 1; accordingly, the integral has the value 5.
In the next example, the argument of the -function vanishes for z = &minus;1:
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>dz e&minus;z
3
</p>
<p> (z + 1) = e&minus;(&minus;1)3 = e (H.22)
</p>
<p>Properties: Regarding the name of the delta function, inspection of equa-
</p>
<p>tion (H.12) reveals that the -function cannot be a function&mdash;otherwise the integral
</p>
<p>would converge, which is clearly not the case. The -function is instead a map that
</p>
<p>assigns a number to a function, i.e. a functional (distribution). The fact that it is not
</p>
<p>called correctly a &lsquo;delta functional&rsquo;, but rather &lsquo;delta function&rsquo;, is due to the laxity
</p>
<p>of the physicists. Precisely because it is not in fact a function, but is only called one,
</p>
<p>the -function has some unusual features besides the familiar ones.
</p>
<p>The delta function can be understood as the derivative of the Heaviside (step)
</p>
<p>function (x) (or unit step function):
</p>
<p>x&int;
</p>
<p>&minus;&infin;
</p>
<p>
(
x &prime;
)
</p>
<p>dx &prime; =
{
</p>
<p>0
</p>
<p>1
for
</p>
<p>x &lt; 0
</p>
<p>x &gt; 0
= (x) ; &prime;(x) = (x). (H.23)
</p>
<p>The Heaviside function is point symmetrical, the -function symmetrical: (x) =
(&minus;x). Derivatives of the -function may be defined by means of partial integration,
that is &infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>f (x &prime;) (n)(x &minus; x &prime;) dx &prime; = (&minus;1)n f (n)(x). (H.24)
</p>
<p>An important property is (see exercises)
</p>
<p>(ax) = 1|a|(x) (H.25)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix H: Fourier Transforms and the Delta Function 279
</p>
<p>from which the symmetry (&minus;x) = (x) follows for a = &minus;1. Generalizing to a
function g(x) which has only simple zeros xn , we have (see exercises):
</p>
<p>(g(x)) =
&sum;
</p>
<p>n
</p>
<p>1
</p>
<p>|g&prime;(xn)|
(x &minus; xn). (H.26)
</p>
<p>Applications: As (H.12) shows, the delta function is an object with an infinitely
</p>
<p>large momentum uncertainty&mdash;one integrates over all momenta k. Thus, the delta
</p>
<p>function describes an object with a precisely defined position and a completely unde-
</p>
<p>fined momentum. This makes it the &lsquo;counterpart&rsquo; of a plane wave, which indeed
</p>
<p>describes an object with a precisely defined momentum and a completely undefined
</p>
<p>position.
</p>
<p>The use of the -function is not limited to quantum mechanics. For example, it
</p>
<p>can be used to describe the mass density of a classical point mass. The point mass
</p>
<p>of mass m0 is at the position x = x0. Then we find:
</p>
<p> (x) = m0 (x &minus; x0) , (H.27)
</p>
<p>and for the total mass, it follows that:
</p>
<p>M =
&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p> (x) dx = m0
&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p> (x &minus; x0) dx = m0. (H.28)
</p>
<p>H.2.4 The Delta Function and the Laplace Operator
</p>
<p>With E = &minus;&nabla;, the first Maxwell equation &nabla;E = /0 can be written as &nabla;2 =
&minus;/0. The potential of a point charge, whose charge density we can describe by
a delta function, is known to be proportional to 1/r . In other words, the term &nabla;2 1
</p>
<p>r
</p>
<p>should give (so to speak, for physical reasons) essentially a delta function. This we
</p>
<p>now wish to demonstrate.
</p>
<p>If one approaches the task directly (and a little too naively), one would simply
</p>
<p>calculate &nabla;2 1
r
. Since this is a radially symmetric problem, we need only consider the
</p>
<p>radial part of &nabla;2, i.e. 1
r
</p>
<p>&part;2
</p>
<p>&part;r2
r . We insert and obtain
</p>
<p>&nabla;2 1
r
= 1
</p>
<p>r
</p>
<p>&part;2
</p>
<p>&part;r2
r
</p>
<p>1
</p>
<p>r
</p>
<p>?= 0 (H.29)
</p>
<p>instead of a delta function. The reason is that 1
r
</p>
<p>is not defined for r = 0&mdash;but our
charge is sitting just there.
</p>
<p>To resolve this shortcoming, we assume a function g (r)</p>
<p/>
</div>
<div class="page"><p/>
<p>280 Appendix H: Fourier Transforms and the Delta Function
</p>
<p>g (r) =
1&radic;
</p>
<p>r2 + 2
;  arbitrary small (H.30)
</p>
<p>defined everywhere. Other functions41 would also be suitable for the following con-
</p>
<p>siderations: It is important only that g(r) is defined everywhere (and twice differ-
</p>
<p>entiable), and goes to 1
r
</p>
<p>in the limit  &rarr; 0.
Inserting and computing gives
</p>
<p>&nabla;2g (r) = &minus;
32
</p>
<p>(
r2 + 2
</p>
<p>)5/2 . (H.31)
</p>
<p>On the right-hand side, we have (except for the sign and possibly a multiplicative
</p>
<p>constant) another representation  (r) of the delta function (see above); the limit
</p>
<p> &rarr; 0 leads for r &gt; 0 to zero and for r = 0 to infinity.
Thus we have &nabla;2g (r) = &minus; (r). To determine the multiplicative constant ,
</p>
<p>we use
&int;
(r)dV = 1 or
</p>
<p>&int;
(r)dV &rarr;
</p>
<p>&rarr;0
1 and obtain
</p>
<p> = 
&int;
 (r) dV =
</p>
<p>&int;
32
</p>
<p>(r2+2)5/2
dV
</p>
<p>= 4
&infin;&int;
</p>
<p>0
</p>
<p>32r2
</p>
<p>(r2+2)5/2
dr = 4
</p>
<p>{
r3
</p>
<p>(r2+2)3/2
</p>
<p>}&infin;
</p>
<p>0
</p>
<p>= 4.
(H.32)
</p>
<p>In summary, the result is
</p>
<p>&nabla;2 1
r
= &minus;4 (r) . (H.33)
</p>
<p>Since r vanishes iff r vanishes, we can also write
</p>
<p>&nabla;2 1
r
= &minus;4 (r) . (H.34)
</p>
<p>Here  (r) is defined in Cartesian coordinates by
</p>
<p> (r) =  (x)  (y)  (z) (H.35)
</p>
<p>Finally, we note as an extension of (H.34) the equation
</p>
<p>&nabla;2 1|r &minus; r&prime;| = &minus;4
(
r &minus; r&prime;
</p>
<p>)
(H.36)
</p>
<p>and (without proof)
</p>
<p>(
&nabla;2 + k2
</p>
<p>) e&plusmn;ik|r&minus;r&prime;|
|r &minus; r&prime;| = &minus;4
</p>
<p>(
r &minus; r&prime;
</p>
<p>)
or
</p>
<p>(
&nabla;2 + k2
</p>
<p>) e&plusmn;ikr
r
</p>
<p>= &minus;4 (r) . (H.37)
</p>
<p>41For example, 1&minus;e
&minus;r/
r
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix H: Fourier Transforms and the Delta Function 281
</p>
<p>The solutions that we have found for the differential equations (H.34&ndash;H.37) are
</p>
<p>examples of Green&rsquo;s functions . Their general definition is: Given an inhomogeneous
</p>
<p>differential equation, a Green&rsquo;s function is defined as a specific (particular) solution
</p>
<p>of that differential equation, if the inhomogeneity is a delta function. For an example,
</p>
<p>see the exercises.
</p>
<p>H.3 Fourier Series
</p>
<p>Fourier series are restricted to the realm of periodic functions. Without loss of gen-
</p>
<p>erality, we assume a 2-periodic function (for other periods, the unit of length must
</p>
<p>be rescaled accordingly), which we can write as a superposition of plane waves:
</p>
<p>f (x) =
&infin;&sum;
</p>
<p>n=&minus;&infin;
fne
</p>
<p>inx ; fnC, if f (x) is 2 periodic. (H.38)
</p>
<p>We multiply by exp(&minus;imx), integrate over x from &minus; to , and obtain (assuming
that integration and summation commute):
</p>
<p>&int;
&minus;
</p>
<p>f (x)e&minus;imx dx =
&infin;&sum;
</p>
<p>n=&minus;&infin;
fn
</p>
<p>&int;
&minus;
</p>
<p>e&minus;imx einx dx
</p>
<p>=
&infin;&sum;
</p>
<p>n=&minus;&infin;
fn 2 nm = 2 fm
</p>
<p>(H.39)
</p>
<p>making use of
</p>
<p>nm =
1
</p>
<p>2
</p>
<p>&int;
</p>
<p>&minus;
</p>
<p>ei(n&minus;m)x dx . (H.40)
</p>
<p>In summary, we have for 2-periodic functions (Fourier series):
</p>
<p>f (x) =
&infin;&sum;
</p>
<p>n=&minus;&infin;
fne
</p>
<p>inx and fn =
1
</p>
<p>2
</p>
<p>&int;
</p>
<p>&minus;
</p>
<p>f (x)e&minus;inx dx . (H.41)
</p>
<p>What is the distribution in momentum space? We use the general relations of
</p>
<p>Fourier transformation and obtain:
</p>
<p>f (k) = 1
2
</p>
<p>&infin;&int;
&minus;&infin;
</p>
<p>f (x)e&minus;ikx dx = 1
2
</p>
<p>&infin;&int;
&minus;&infin;
</p>
<p>&infin;&sum;
n=&minus;&infin;
</p>
<p>fne
inx e&minus;ikx dx
</p>
<p>=
&infin;&sum;
</p>
<p>n=&minus;&infin;
fn
</p>
<p>1
2
</p>
<p>&infin;&int;
&minus;&infin;
</p>
<p>ei(n&minus;k)x dx =
&infin;&sum;
</p>
<p>n=&minus;&infin;
fn(n &minus; k)
</p>
<p>(H.42)</p>
<p/>
</div>
<div class="page"><p/>
<p>282 Appendix H: Fourier Transforms and the Delta Function
</p>
<p>Thus, in momentum space, we have a series of infinitely high, infinitely thin needles
</p>
<p>at the points k = n with the respective weights fn .
</p>
<p>H.4 Discrete and Quantum Fourier Transforms
</p>
<p>Until now we have assumed for the Fourier transform that the data sets being trans-
</p>
<p>formed are continuous. For discrete data sets, as are typically produced by experi-
</p>
<p>ments, the discrete Fourier transform (DFT) is applied.
</p>
<p>We suppose that we have a set of N data42 x j , with j = 0 . . . N &minus; 1.43 Then the
DFT is
</p>
<p>yk =
1&radic;
N
</p>
<p>N&minus;1&sum;
</p>
<p>j=0
e
</p>
<p>2i jk
N x j ; xk =
</p>
<p>1&radic;
N
</p>
<p>N&minus;1&sum;
</p>
<p>j=0
e&minus;
</p>
<p>2i jk
N y j . (H.43)
</p>
<p>We choose a simple example with N = 2, i.e. x = (x0, x1). With
</p>
<p>y0 =
1&radic;
2
</p>
<p>1&sum;
</p>
<p>j=0
x j ; y1 =
</p>
<p>1&radic;
2
</p>
<p>1&sum;
</p>
<p>j=0
ei j x j , (H.44)
</p>
<p>it follows that
</p>
<p>x = (1, 0) : y0 = 1&radic;
2
; y1 = 1&radic;
</p>
<p>2
&rarr; y = 1&radic;
</p>
<p>2
(1, 1)
</p>
<p>x = (0, 1) : y0 = 1&radic;
2
; y1 = &minus; 1&radic;
</p>
<p>2
&rarr; y = 1&radic;
</p>
<p>2
(1,&minus;1)
</p>
<p>(H.45)
</p>
<p>or, written compactly,
</p>
<p>x = H y; H = 1&radic;
2
</p>
<p>(
1 1
</p>
<p>1 &minus;1.
</p>
<p>)
(H.46)
</p>
<p>The Hadamard matrix H plays an important role in quantum information (see
</p>
<p>Chap. 26, Vol. 2, and Appendices P to S in Vol. 2).
</p>
<p>The discrete Fourier transformation of quantum states is called (discrete) quantum
</p>
<p>Fourier transformation (QFT, occasionally DQFT ). We formulate it as operator in
</p>
<p>the bra-ket notation:
</p>
<p>UQFT =
1&radic;
N
</p>
<p>N&minus;1&sum;
</p>
<p>j,k=0
e2i
</p>
<p>jk
</p>
<p>N | j k| (H.47)
</p>
<p>where {| j} is a CONS. In the matrix representation, this reads
</p>
<p>42The data can be complex.
43A practical example: When scanning digital music from a CD, the sampling frequency is 44.1 kHz,
</p>
<p>so 44 100 values must be processed per second.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix H: Fourier Transforms and the Delta Function 283
</p>
<p>UQFT &sim;=
1&radic;
N
</p>
<p>

</p>
<p>1 1 1 . . . 1
</p>
<p>1  2 . . . N&minus;1
</p>
<p>1 2 4 . . . 2(N&minus;1)
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>1 N&minus;1 2(N&minus;1) . . . (N&minus;1)(N&minus;1)
</p>
<p>

;  = e 2iN . (H.48)
</p>
<p>The QFT is a unitary transformation (see exercises):
</p>
<p>UU &dagger; = U &dagger;U = 1. (H.49)
</p>
<p>A linear combination of basis states
</p>
<p>| =
N&minus;1&sum;
</p>
<p>n=0
n |n (H.50)
</p>
<p>is mapped by the QFT onto
</p>
<p>| = UQFT | =
1&radic;
N
</p>
<p>N&minus;1&sum;
</p>
<p>n, j,k=0
ne
</p>
<p>2i jk
N | j k |n = 1&radic;
</p>
<p>N
</p>
<p>N&minus;1&sum;
</p>
<p>n, j=0
ne
</p>
<p>2i jn
N | j
</p>
<p>(H.51)
</p>
<p>or, compactly,
</p>
<p>| = 1&radic;
N
</p>
<p>N&minus;1&sum;
</p>
<p>n=0
n |n ; n =
</p>
<p>N&minus;1&sum;
</p>
<p>j=0
 j e
</p>
<p>2i nj
N . (H.52)
</p>
<p>H.5 Exercises
</p>
<p>1. Calculate &infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>e4x
2 sin x (x &minus; 3) dx;
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>cos 4x  (x) dx
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p> (x) ex dx;
&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p> (x &minus; 2) f (x) dx .
(H.53)
</p>
<p>2. Given an operator X with  (x &minus; a) as eigenfunction (or eigen&lsquo;function&rsquo;), so that
X (x &minus; a) = a (x &minus; a) holds; show that X is the position operator.
Solution: For an arbitrary function f (x), it holds that
</p>
<p>X f (x) = X
&int;
</p>
<p>da f (a)  (x &minus; a)
=
</p>
<p>&int;
da f (a) X (x &minus; a) =
</p>
<p>&int;
da f (a) a (x &minus; a) = x f (x) . (H.54)</p>
<p/>
</div>
<div class="page"><p/>
<p>284 Appendix H: Fourier Transforms and the Delta Function
</p>
<p>From this, it follows that the position operator multiplies an arbitrary space-
</p>
<p>dependent function by x .
</p>
<p>3. Show that for the derivative of the delta function,
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>&prime; (x &minus; x0) f (x) dx = &minus; f &prime; (x0) (H.55)
</p>
<p>holds. Hint: Use partial integration.
</p>
<p>4. Representations of the delta function are e.g.
</p>
<p> (x) = lim
&rarr;0
</p>
<p>1
</p>
<p>2
</p>
<p>2
</p>
<p>x2 + 2 ;  (x) = lim&rarr;0
e&minus;x
</p>
<p>2/2
</p>
<p>&radic;

</p>
<p>. (H.56)
</p>
<p>(a) Discuss the functions and sketch their graphs for different values of .
</p>
<p>(b) Show that
&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>1
</p>
<p>2
</p>
<p>2
</p>
<p>x2 + 2 dx = 1 (H.57)
</p>
<p>holds. (Hint: differentiate arctan x). Is
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>e&minus;x
2/2
</p>
<p>&radic;

</p>
<p>dx = 1 (H.58)
</p>
<p>also valid? (Check with a formula tabulation.)
</p>
<p>(c) Show with the help of a representation that
</p>
<p> (ax) = 1|a| (x) (H.59)
</p>
<p>is valid.
</p>
<p>Solution: We start from  (x) = lim&rarr;0 1 x2+2 . We have:
</p>
<p> (ax) = 1

</p>
<p>lim
&rarr;0
</p>
<p>
</p>
<p>a2x2 + 2 =
1
</p>
<p>a2
</p>
<p>1
</p>
<p>
lim
&rarr;0
</p>
<p>
</p>
<p>x2 + 2/a2 . (H.60)
</p>
<p>On the right-hand side, we no longer have any information about the sign
</p>
<p>of a. With the new variable  = / |a|, it follows that
</p>
<p> (ax) = 1|a|
1
</p>
<p>
lim
&rarr;0
</p>
<p>/ |a|
x2 + 2/a2 =
</p>
<p>1
</p>
<p>|a|
1
</p>
<p>
lim
&rarr;0
</p>
<p>
</p>
<p>x2 + 2 =
1
</p>
<p>|a| (x) .
(H.61)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix H: Fourier Transforms and the Delta Function 285
</p>
<p>5. Prove the equation
</p>
<p> (ax) = 1|a| (x) (H.62)
</p>
<p>by means of a suitable substitution under the integral.
</p>
<p>Solution: In the integral
</p>
<p>A =
&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p> (ax) f (x) dx (H.63)
</p>
<p>we substitute y = ax , obtaining
</p>
<p>A =
</p>
<p>|a|
a
&infin;&int;
</p>
<p>&minus; |a|
a
&infin;
</p>
<p> (y) f
( y
</p>
<p>a
</p>
<p>) 1
a
</p>
<p>dy = 1|a|
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p> (y) f
( y
</p>
<p>a
</p>
<p>)
dy = 1|a| f (0) . (H.64)
</p>
<p>Because of
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p> (ax) f (x) dx = 1|a| f (0) =
1
</p>
<p>|a|
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p> (x) f (x) dx, (H.65)
</p>
<p>the proposition follows directly.
</p>
<p>6. Show that for a function g(x) which has only simple zeros, the following relation
</p>
<p>holds:
</p>
<p>(g(x)) =
&sum;
</p>
<p>n
</p>
<p>1
</p>
<p>|g&prime;(xn)|
(x &minus; xn). (H.66)
</p>
<p>Solution: The delta function makes a contribution only at those points where the
</p>
<p>function g(x) vanishes, i.e. at the zeros of g(x). In the vicinity of the zeros, the
</p>
<p>Taylor expansion can be applied:
</p>
<p>g(x) = g(xn)+ g&prime;(xn)(x &minus; xn)+ O
(
(x &minus; xn)2
</p>
<p>)
. (H.67)
</p>
<p>If we are &lsquo;very close&rsquo; to the zeros (and only then does the delta function con-
</p>
<p>tribute), we can replace g(x) by g&prime;(xn)(x &minus; xn). It follows initially that
</p>
<p>(g(x)) =
&sum;
</p>
<p>n
</p>
<p>(g&prime;(xn)(x &minus; xn)). (H.68)
</p>
<p>Using the result of the last exercise, we can write
</p>
<p>(g(x)) =
&sum;
</p>
<p>n
</p>
<p>1
</p>
<p>|g&prime;(xn)|
(x &minus; xn). (H.69)</p>
<p/>
</div>
<div class="page"><p/>
<p>286 Appendix H: Fourier Transforms and the Delta Function
</p>
<p>7. Assume
</p>
<p>g () =
{
</p>
<p>G &gt; 0
</p>
<p>0
for
</p>
<p>0 &lt; 1 &lt;  &lt; 2
otherwise.
</p>
<p>(H.70)
</p>
<p>Determine the Fourier transform f (t). What is the value of f (t) at time t = 0?
Calculate the intensity | f (t)|2 and show that it depends only on the difference
of the frequencies 1 and 2. Sketch | f (t)|2.
</p>
<p>8. Determine the Fourier transform f (t) of the function
</p>
<p>g () =
{

</p>
<p>0
for
</p>
<p>0 &le;  &le; 
otherwise.
</p>
<p>(H.71)
</p>
<p>Determine and sketch the intensity | f (t)|2.
9. Formulate the potential equations in the Fourier representation.
</p>
<p>10. Show that for real f (x), f (k) = f &lowast; (&minus;k) applies.
Solution: For real f (x), we have f (x) = f &lowast;(x), and it follows that
</p>
<p>+&infin;&int;
&minus;&infin;
</p>
<p>f (k) eikx dk = f (x) =
+&infin;&int;
&minus;&infin;
</p>
<p>f &lowast; (k) e&minus;ikx dk
</p>
<p>= &minus;
&minus;&infin;&int;
&infin;
</p>
<p>f &lowast; (&minus;k) eikx dk =
+&infin;&int;
&minus;&infin;
</p>
<p>f &lowast; (&minus;k) eikx dk;
(H.72)
</p>
<p>thus the proposition follows directly.
</p>
<p>11. Show that the Fourier transform of f &prime;(x) is ik f (k).
Solution: We have
</p>
<p>f &prime; (x) = 1&radic;
2
</p>
<p>d
</p>
<p>dx
</p>
<p>+&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>f (k) eikx dk = 1&radic;
2
</p>
<p>+&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>ik f (k) eikx dk. (H.73)
</p>
<p>12. Show that a shifted function g(x) = f (x &minus; a) has the Fourier transform g(k) =
e&minus;ika f (k).
Solution: We have
</p>
<p>g (k) = 1&radic;
2
</p>
<p>+&infin;&int;
&minus;&infin;
</p>
<p>f (x &minus; a) e&minus;ikx dx = 1&radic;
2
</p>
<p>+&infin;&int;
&minus;&infin;
</p>
<p>e&minus;ika f (x &minus; a) e&minus;ik(x&minus;a)dx
</p>
<p>= e&minus;ika 1&radic;
2
</p>
<p>+&infin;&int;
&minus;&infin;
</p>
<p>f (z) e&minus;ikzdz = e&minus;ika f (k) .
(H.74)
</p>
<p>13. Show for the convolution
</p>
<p>h(x) =
+&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>f (x &minus; y) g (y) dy (H.75)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix H: Fourier Transforms and the Delta Function 287
</p>
<p>that the following relation applies (convolution theorem):
</p>
<p>h(k) =
&radic;
</p>
<p>2 f (k) g(k). (H.76)
</p>
<p>Solution: We have
</p>
<p>h(k) = 1&radic;
2
</p>
<p>+&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>dx
</p>
<p>+&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>dy f (x &minus; y) g (y) e&minus;ikx . (H.77)
</p>
<p>We assume that we can interchange the integrations, and first perform the inte-
</p>
<p>gration over x :
</p>
<p>h(k) = 1&radic;
2
</p>
<p>+&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>dy g (y)
</p>
<p>+&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>dx f (x &minus; y) e&minus;ikx . (H.78)
</p>
<p>According to the previous exercise, the integration of the shifted function f
</p>
<p>yields
</p>
<p>1&radic;
2
</p>
<p>+&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>dx f (x &minus; y) e&minus;ikx = e&minus;iky f (k) , (H.79)
</p>
<p>and thus it follows that
</p>
<p>h(k) =
+&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>dy g (y) e&minus;iky f (k) =
&radic;
</p>
<p>2 f (k) g (k) . (H.80)
</p>
<p>14. Determine the Fourier transformation of the rectangular function
</p>
<p>f (x) =
{
</p>
<p>A for &minus; b &lt; x &lt; b
0 otherwise
</p>
<p>(H.81)
</p>
<p>Solution: Start with
</p>
<p>f (k) = A&radic;
2
</p>
<p>+b&int;
</p>
<p>&minus;b
</p>
<p>e&minus;ikx dx = A&radic;
2
</p>
<p>e&minus;ikx
</p>
<p>&minus;ik
</p>
<p>
b
</p>
<p>&minus;b
</p>
<p>= A&radic;
2
</p>
<p>e&minus;ikb &minus; eikb
&minus;ik =
</p>
<p>A&radic;
2
</p>
<p>2i sin kb
</p>
<p>&minus;ik
</p>
<p>(H.82)
</p>
<p>from which
</p>
<p>f (k) = A
&radic;
</p>
<p>2
</p>
<p>
</p>
<p>sin kb
</p>
<p>k
. (H.83)</p>
<p/>
</div>
<div class="page"><p/>
<p>288 Appendix H: Fourier Transforms and the Delta Function
</p>
<p>What is happening at k = 0? Either we compute the integral
&int; +b
&minus;b e
</p>
<p>&minus;ikx dx once
more for k = 0, or we start with sin kb
</p>
<p>k
and apply l&rsquo;H&ocirc;pital&rsquo;s rule, or we remember
</p>
<p>sin x &asymp; x for small x (first term of the power series expansion). Howsoever, in
any case it follows that
</p>
<p>f (k = 0) = A
&radic;
</p>
<p>2
</p>
<p>
b. (H.84)
</p>
<p>The first zero of f (k) is found at
</p>
<p>kb =  or k = 
b
. (H.85)
</p>
<p>The last equation shows that the narrower we make the distribution in position
</p>
<p>space (i.e. the smaller is b), the broader is the distribution in momentum space&mdash;
</p>
<p>and vice versa44 To quantify this, we choose the position of the first zero as a
</p>
<p>rough measure k of the width of f (k):
</p>
<p>k &asymp; 
b
&sim; &prime;breadth&prime;of f (k) . (H.86)
</p>
<p>As a measure of the breadth of f (x), we choose b:
</p>
<p>x &asymp; b &sim; &prime;breadth&prime;of f (x) . (H.87)
</p>
<p>It follows that
</p>
<p>kx &asymp;  (H.88)
</p>
<p>or, with p = k,
xp &asymp; . (H.89)
</p>
<p>This is simply a &lsquo;raw form&rsquo; of Heisenberg&rsquo;s uncertainty principle, which is
</p>
<p>derived exactly in Chap. 13. According to this relation, there is no quantum
</p>
<p>object to which we can attribute a precise position (i.e. x = 0) and at the same
time a precise momentum (p = 0).
Note: this is not a statement about the quality of our measurement apparatus
</p>
<p>or something similar, but rather the statement that the concepts &lsquo;position&rsquo; and
</p>
<p>&lsquo;momentum&rsquo; lose their meaning in quantum mechanics, or at least do not main-
</p>
<p>tain it in terms of our everyday understanding.
</p>
<p>15. Show that the QFT
</p>
<p>U = 1&radic;
N
</p>
<p>N&minus;1&sum;
</p>
<p>j,k=0
e
</p>
<p>2i jk
N | j k| (H.90)
</p>
<p>44This is similar to pressing a balloon&mdash;pressed in one direction, it evades and expands out in
</p>
<p>another direction.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix H: Fourier Transforms and the Delta Function 289
</p>
<p>is unitary. {| j} is a CONS.
Solution: We have
</p>
<p>UU &dagger; = 1
N
</p>
<p>N&minus;1&sum;
</p>
<p>j,k, j &prime;,k &prime;=0
e
</p>
<p>2i jk
N | j k| e&minus; 2i j
</p>
<p>&prime;k&prime;
N
</p>
<p>k &prime;
&rang; &lang;
</p>
<p>j &prime;

</p>
<p>= 1
N
</p>
<p>N&minus;1&sum;
</p>
<p>j,k, j &prime;=0
e
</p>
<p>2i( j&minus; j &prime;)k
N | j
</p>
<p>&lang;
j &prime;
 . (H.91)
</p>
<p>We distinguish the cases j = j &prime; and j 	= j &prime;. For j = j &prime;, we find using complete-
ness,
</p>
<p>1
</p>
<p>N
</p>
<p>N&minus;1&sum;
</p>
<p>j,k=0
| j  j | = 1
</p>
<p>N
&middot; N
</p>
<p>N&minus;1&sum;
</p>
<p>j=0
| j  j | = 1. (H.92)
</p>
<p>For j 	= j &prime;, it holds (geometrical series) that:
</p>
<p>N&minus;1&sum;
</p>
<p>k=0
e
</p>
<p>2i( j&minus; j &prime;)k
N = 1 &minus; e
</p>
<p>2i( j&minus; j &prime;)
</p>
<p>1 &minus; e 2i( j&minus; j
&prime;)
</p>
<p>N
</p>
<p>= 0. (H.93)
</p>
<p>16. Determine explicitly the QFT matrix (H.48) for the cases N = 2, 3, 4.
Solution: We have
</p>
<p>N = 2 : UQFT &sim;=
1&radic;
2
</p>
<p>

</p>
<p>1 1
</p>
<p>1 e
</p>
<p>2i
</p>
<p>2
</p>
<p>
 = 1&radic;
</p>
<p>2
</p>
<p>(
1 1
</p>
<p>1 &minus;1
</p>
<p>)
</p>
<p>N = 3 : UQFT &sim;=
1&radic;
3
</p>
<p>

</p>
<p>1 1 1
</p>
<p>1 e
</p>
<p>2i
</p>
<p>3 e
</p>
<p>4i
</p>
<p>3
</p>
<p>1 e
</p>
<p>4i
</p>
<p>3 e
</p>
<p>8i
</p>
<p>3
</p>
<p>

</p>
<p>= 1&radic;
3
</p>
<p>

</p>
<p>1 1 1
</p>
<p>1
&minus;1 + i
</p>
<p>&radic;
3
</p>
<p>2
</p>
<p>&minus;1 &minus; i
&radic;
</p>
<p>3
</p>
<p>2
</p>
<p>1
&minus;1 &minus; i
</p>
<p>&radic;
3
</p>
<p>2
</p>
<p>&minus;1 + i
&radic;
</p>
<p>3
</p>
<p>2
</p>
<p>

</p>
<p>N = 4 : UQFT &sim;=
1&radic;
4
</p>
<p>

</p>
<p>1 1 1 1
</p>
<p>1 e
</p>
<p>2i
</p>
<p>4 e
</p>
<p>4i
</p>
<p>4 e
</p>
<p>6i
</p>
<p>4
</p>
<p>1 e
</p>
<p>4i
</p>
<p>4 e
</p>
<p>8i
</p>
<p>4 e
</p>
<p>12i
</p>
<p>4
</p>
<p>1 e
</p>
<p>6i
</p>
<p>4 e
</p>
<p>12i
</p>
<p>4 e
</p>
<p>18i
</p>
<p>4
</p>
<p>

</p>
<p>= 1&radic;
4
</p>
<p>

</p>
<p>1 1 1 1
</p>
<p>1 i &minus;1 &minus;i
1 &minus;1 1 &minus;1
1 &minus;i &minus;1 i
</p>
<p>
 .
</p>
<p>(H.94)
</p>
<p>17. Determine by making use of the Green&rsquo;s function the solution of the first Maxwell
</p>
<p>equation &nabla;E = /0 for the time-independent charge density  (r). Use E =</p>
<p/>
</div>
<div class="page"><p/>
<p>290 Appendix H: Fourier Transforms and the Delta Function
</p>
<p>&minus;&nabla;, i.e. &nabla;2 = &minus;/0.
Solution: We start with (H.36), i.e.
</p>
<p>&nabla;2r
1
</p>
<p>|r &minus; r&prime;| = &minus;4
(
r &minus; r&prime;
</p>
<p>)
. (H.95)
</p>
<p>The index r denotes the variables with respect to which the differentiation is to
</p>
<p>be performed. Multiplication by 
(
r&prime;
)
</p>
<p>leads to
</p>
<p>r

(
r&prime;
)
</p>
<p>|r &minus; r&prime;| = &minus;4
(
r &minus; r&prime;
</p>
<p>)

(
r&prime;
)
. (H.96)
</p>
<p>Integration with respect to r&prime; yields
</p>
<p>r
</p>
<p>&int;

(
r&prime;
)
</p>
<p>|r &minus; r&prime;|d
3r &prime; = &minus;4
</p>
<p>&int;

(
r &minus; r&prime;
</p>
<p>)

(
r&prime;
)
</p>
<p>d3r &prime; = &minus;4 (r) . (H.97)
</p>
<p>Comparison with  = &minus;/0 leads immediately to
</p>
<p>(r) = 1
40
</p>
<p>&int;

(
r&prime;
)
</p>
<p>|r &minus; r&prime;|d
3r &prime;. (H.98)
</p>
<p>In principle, additive terms f with &nabla;2r f = 0 could occur; if necessary they can
be excluded by considering the asymptotic bahavior.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix I
</p>
<p>Operators
</p>
<p>We take a closer look at some issues from Chap. 13 and provide some additional
</p>
<p>material, insofar as it may be useful for understanding the text.
</p>
<p>I.1 Norm, Domain of Definition
</p>
<p>I.1.1 The Norm
</p>
<p>The norm of an operator is defined by A = sup A|| or A = sup|=1
A | .
</p>
<p>Here, an example in a real vector space:
</p>
<p>A =
(
</p>
<p>1 2
</p>
<p>0 &minus;1
</p>
<p>)
; | =
</p>
<p>(
a
</p>
<p>b
</p>
<p>)
&rarr;
</p>
<p>A = sup
&radic;
(a + 2b)2 + b2&radic;
</p>
<p>a2 + b2
= sup
</p>
<p>&radic;
(x + 2)2 + 1
</p>
<p>x2 + 1 with x =
a
b
.
</p>
<p>(I.1)
</p>
<p>The function on the right-hand side is maximal for x =
&radic;
</p>
<p>2 &minus; 1; it follows that
A =
</p>
<p>&radic;
3 + 2
</p>
<p>&radic;
2.
</p>
<p>The operator norm for bounded operators is a &lsquo;proper&rsquo; norm and complies with
</p>
<p>the three rules, among them the triangle inequality (see Appendix G, Vol. 1).45
</p>
<p>45Interestingly, the norm of A is related to the spectral radius  (A), which is defined as the largest
</p>
<p>absolute value of the eigenvalues of A, i.e. as (A) = max
i
</p>
<p>|i |. In general, it holds that (A) &le; A.
For normal operators
</p>
<p>[
A, A&dagger;
</p>
<p>]
= 0, the inequality is sharpened to give  (A) = A.
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>291</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>292 Appendix I: Operators
</p>
<p>I.1.2 Bounded Operators
</p>
<p>An operator is called bounded if there is a constant C &lt; &infin;, independent of the
states |, such that for all states | &isin; H, we have:
</p>
<p>A | &le; C | or A &le; C. (I.2)
</p>
<p>With A =
(
</p>
<p>1 2
</p>
<p>0 &minus;1
</p>
<p>)
, we have just seen an example of a bounded operator. For an
</p>
<p>unbounded operator, we consider the Hilbert space L(2) [0,&infin;] and the operator x .
Its norm is given by
</p>
<p>x = sup
</p>
<p>&infin;&int;
</p>
<p>0
</p>
<p>f &lowast; (x) x2 f (x) dx
</p>
<p>&infin;&int;
</p>
<p>0
</p>
<p>f &lowast; (x) f (x) dx
</p>
<p>. (I.3)
</p>
<p>If we now find even one single function for which x = &infin;, we have shown that
x is an unbounded operator (in this Hilbert space). Such a function is, for example,
</p>
<p>f (x) = sin x2
x
</p>
<p>. Like x , p is also an unbounded operator; see the exercises.
</p>
<p>In a finite-dimensional Hilbert space, all operators are bounded (cf. the exercises);
</p>
<p>unbounded operators can therefore occur only in infinite-dimensional Hilbert spaces.
</p>
<p>I.1.3 Domain of Definition
</p>
<p>The domain of definition (or briefly, domain) DA of an operator A is the set of all
</p>
<p>vectors | &isin; H such that A | is also in H. One can show that the domain of
definition is the whole Hilbert space iff A is bounded. Hence, the problem with an
</p>
<p>unbounded operator A is that its domain of definition is not the whole Hilbert space.
</p>
<p>An example: In the case of the function f (x) = sin x2
x
</p>
<p>just considered, we have
</p>
<p>seen that f is square integrable, but not x f (x); in addition, the mean value x f
does not exist. Thus, the domain of definition of the unbounded operator x is not the
</p>
<p>whole Hilbert space L(2) [0,&infin;].
Also, problems may occur in other respects with unbounded operators. For exam-
</p>
<p>ple, in the equation [x, p] = i, the right-hand side is defined for the whole of H, but
the left-hand side only for a subset (see also the remarks on the uncertainty principle
</p>
<p>below).
</p>
<p>Bounded operators on a Hilbert space are very well behaved. This can be seen
</p>
<p>by&mdash;among others&mdash;the fact that a special name was given to them: The set of all
</p>
<p>bounded operators on a Hilbert space forms a C&lowast; algebra (see Appendix G, Vol. 1;
the operator norm and the adjoint must of course be defined).</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix I: Operators 293
</p>
<p>I.2 Hermitian, Self-adjoint
</p>
<p>The difference between the terms Hermitian and self-adjoint has to do with the
</p>
<p>fact that the domain of definition of unbounded operators is not the entire Hilbert
</p>
<p>space. Thus, the difference can occur only in infinite-dimensional Hilbert spaces; for
</p>
<p>finite-dimensional vector spaces, the two concepts are identical.
</p>
<p>Basically, it is therefore necessary to identify not only the comb, but also the hair
</p>
<p>that is combed&mdash;the properties of an operator depend on its domain of definition.
</p>
<p>A simple example: In L(2) [0,&infin;], the operator x is unbounded, but it is bounded in
L(2) [0, 1].
</p>
<p>The technical resources needed for the following are simple; just integration by
</p>
<p>parts as known from school.
</p>
<p>I.2.1 Definitions and Differences
</p>
<p>We begin with three definitions:
</p>
<p>(1) An operator A, for which Au |v = u |Av holds, is called symmetric or
Hermitian.46
</p>
<p>(2) Given an operator A. The adjoint47 A&dagger; of the operator A is defined as&lang;
A&dagger;u |v = u |Av. We note that A&dagger; is a distinct operator which can have its separate
</p>
<p>domain of definition. The equality
&lang;
A&dagger;u |v = u |Av must apply for all vectors
</p>
<p>within the domain of definition.
</p>
<p>(3) In general, for an unbounded Hermitian operator A, it is not true that DA =
DA&dagger; , but rather DA &sub; DA&dagger; (or DA&dagger; &sub; DA). In order that A be self-adjoint, we must
have A = A&dagger; and the two domains of definition must coincide.
</p>
<p>A symmetric linear operator defined everywhere is self-adjoint. According to a
</p>
<p>theorem of functional analysis (Hellinger&ndash;Toeplitz theorem), such an operator is
</p>
<p>bounded. Conversely, it follows that an unbounded operator cannot be defined on
</p>
<p>the entire Hilbert space. The theorem combines two completely different properties,
</p>
<p>namely to be defined everywhere and to be bounded.
</p>
<p>We now illustrate these concepts by means of two examples.
</p>
<p>I.2.2 Two Examples
</p>
<p>The standard operator used in the following is the (one-dimensional) momentum.
</p>
<p>For both examples, the Hilbert space is L2 [0, 1].
</p>
<p>46Actually, there is a minor difference between the two terms, which has to do with the question of
</p>
<p>whether the domain of definition of A is dense. But since this question has nothing to do with the
</p>
<p>following considerations, we will omit it here.
47Occasionally also called the Hermitian conjugate operator.</p>
<p/>
</div>
<div class="page"><p/>
<p>294 Appendix I: Operators
</p>
<p>Example 1:
</p>
<p>The operator is p0 = i ddx . Its domain of definition Dp0 consists of all functions
g(x) &isin; L2 [0, 1] which are differentiable, have square-integrable derivatives and
fulfill the boundary conditions g(0) = g(1) = 0 (it is this 0 to which the index in p0
refers).
</p>
<p>We consider the adjoint operator p
&dagger;
0 . It is defined by
</p>
<p>&lang;
p
</p>
<p>&dagger;
0 f
</p>
<p> g =  f | p0g, and it
follows that
</p>
<p>&lang;
p
</p>
<p>&dagger;
0 f
</p>
<p> g =  f | p0g =

</p>
<p>i
</p>
<p>1&int;
</p>
<p>0
</p>
<p>f &lowast;(x)g&prime;(x)dx = 
i
</p>
<p>


[
</p>
<p>f &lowast;g
]1
</p>
<p>0
&minus;
</p>
<p>1&int;
</p>
<p>0
</p>
<p>f &lowast;&prime;(x)g(x)dx
</p>
<p>

 .
</p>
<p>(I.4)
</p>
<p>The integrated term on the right-hand side gives zero, and we have
</p>
<p>&lang;
p
</p>
<p>&dagger;
0 f
</p>
<p> g =
1&int;
</p>
<p>0
</p>
<p>(

</p>
<p>i
</p>
<p>d
</p>
<p>dx
f (x)
</p>
<p>)&lowast;
g(x)dx = p0 f | g . (I.5)
</p>
<p>One might now think that p0 is self-adjoint&mdash;but that is wrong, because the integrated
</p>
<p>term f &lowast; (1) g (1) &minus; f &lowast; (0) g (0) in (I.4) vanishes independently of the values of f
at the boundary. Therefore, the domain of definition of p
</p>
<p>&dagger;
0 is larger than that of p0,
</p>
<p>Dp0 &sub; Dp&dagger;0 .
Example 2:
</p>
<p>The same example&mdash;but with different boundary conditions: We allow for arbitrary
</p>
<p>boundary conditions of g (x) and thus write simply p instead of p0. For the adjoint
</p>
<p>operator p&dagger;,
</p>
<p>&lang;
p&dagger; f
</p>
<p> g =  f | pg = 
i
</p>
<p>1&int;
</p>
<p>0
</p>
<p>f &lowast;(x)g&prime;(x)dx = 
i
</p>
<p>


[
</p>
<p>f &lowast;g
]1
</p>
<p>0
&minus;
</p>
<p>1&int;
</p>
<p>0
</p>
<p>f &lowast;&prime;(x)g(x)dx
</p>
<p>


</p>
<p>= 
i
</p>
<p>[ f &lowast; (1) g (1)&minus; f &lowast; (0) g (0)] +
1&int;
</p>
<p>0
</p>
<p>(

</p>
<p>i
d
</p>
<p>dx
f (x)
</p>
<p>)&lowast;
g(x)dx .
</p>
<p>(I.6)
</p>
<p>In order for this equality to be valid, it must hold that f &lowast; (1) = f &lowast; (0) = 0. In other
words, the domain of definition of p&dagger; is smaller than that of p: Dp&dagger;0
</p>
<p>&sub; Dp0 . Thus,
this operator is also not self-adjoint.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix I: Operators 295
</p>
<p>Symmetry of the Examples:
</p>
<p>We want to check if the operators p0, p
&dagger;
0 , p, p
</p>
<p>&dagger; are symmetric. For p0, we have
</p>
<p>p0 f | g &minus;  f | p0g =
1&int;
</p>
<p>0
</p>
<p>(

</p>
<p>i
</p>
<p>d
</p>
<p>dx
f (x)
</p>
<p>)&lowast;
g(x)dx &minus;
</p>
<p>1&int;
</p>
<p>0
</p>
<p>f &lowast;(x)

</p>
<p>i
</p>
<p>d
</p>
<p>dx
g(x)dx
</p>
<p>= &minus;
i
</p>
<p>1&int;
</p>
<p>0
</p>
<p>d
dx
</p>
<p>f &lowast;(x)g(x)dx &minus; 
i
</p>
<p>1&int;
</p>
<p>0
</p>
<p>f &lowast;(x) d
dx
g(x)dx = &minus;
</p>
<p>i
[ f &lowast;(x)g(x)]10 = 0.
</p>
<p>(I.7)
</p>
<p>The last equals sign is valid, since the domain of definition of p0 is restricted to
</p>
<p>functions which vanish at the boundaries of the interval. Hence p0 is symmetric.
</p>
<p>We apply the same considerations to p
&dagger;
0:
</p>
<p>&lang;
p
</p>
<p>&dagger;
0 f
</p>
<p> g &minus;  f | p&dagger;0g
&rang;
=
</p>
<p>1&int;
</p>
<p>0
</p>
<p>(

</p>
<p>i
</p>
<p>d
</p>
<p>dx
f (x)
</p>
<p>)&lowast;
g(x)dx &minus;
</p>
<p>1&int;
</p>
<p>0
</p>
<p>f &lowast;(x)

</p>
<p>i
</p>
<p>d
</p>
<p>dx
g(x)dx
</p>
<p>= &minus;
i
</p>
<p>1&int;
</p>
<p>0
</p>
<p>d
dx
</p>
<p>f &lowast;(x)g(x)dx &minus; 
i
</p>
<p>1&int;
</p>
<p>0
</p>
<p>f &lowast;(x) d
dx
g(x)dx = &minus;
</p>
<p>i
[ f &lowast;(x)g(x)]10 .
</p>
<p>(I.8)
</p>
<p>The domain of definition of p
&dagger;
0 also comprises functions which do not vanish at the
</p>
<p>boundary; hence, p
&dagger;
0 is not a symmetric operator.
</p>
<p>Analogous considerations show that the operator p&dagger; in the second example is
</p>
<p>symmetric, but not the the operator p.
</p>
<p>Extension of the Domain of Definition:
</p>
<p>The example of p0 and p
&dagger;
0 has shown that the domains of definition of operator
</p>
<p>and adjoint operator may differ. However, one can often &lsquo;repair&rsquo; this. Let us define
</p>
<p>p = i ddx , i.e. once more the the derivative acting on the functions g(x) &isin; L2 [0, 1]
(of course, the derivatives must exist and also be square integrable). The difference
</p>
<p>w.r.t. p0 consists in the different boundary conditions, namely g(1) = eig(0) with
0 &le;  &lt; 1 and g(0) 	= 0. Thus, the domain of definition of p differs from that of
p0 (we emphasize again that we are dealing indeed with different operators&mdash;all of
</p>
<p>them are written as 
i
</p>
<p>d
dx
</p>
<p>, but they have different domains of definition). The operator
</p>
<p>p is again symmetric, but it is also self-adjoint, in contrast to p0. This holds owing
</p>
<p>to
</p>
<p> f | pg &minus; p f | g =

</p>
<p>i
</p>
<p>[
f &lowast;(x)g(x)
</p>
<p>]1
0
= 
</p>
<p>i
</p>
<p>[
f &lowast;(1)g(1)&minus; f &lowast;(0)g(0)
</p>
<p>]
</p>
<p>= 
i
</p>
<p>[
f &lowast;(1)ei &minus; f &lowast;(0)
</p>
<p>]
g(0) = 
</p>
<p>i
</p>
<p>[
f (1)&minus; ei f (0)
</p>
<p>]&lowast;
eig(0).
</p>
<p>(I.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>296 Appendix I: Operators
</p>
<p>The right side vanishes iff f (1) = ei f (0). In other words, the domains of definition
of p and p
</p>
<p>&dagger;
 are identical. We have achieved this by expanding the domain of
</p>
<p>definition of p0.
</p>
<p>In fact, with p we have constructed an entire class of operators, because if we
</p>
<p>choose a different constant , we obtain a different domain of definition and thus a
</p>
<p>different operator, although they of course always refer to 
i
</p>
<p>d
dx
</p>
<p>. For a closer look, we
</p>
<p>consider again the eigenvalue equation
</p>
<p>
</p>
<p>i
</p>
<p>d
</p>
<p>dx
g(x) = g(x) ; boundary condition g(1) = eig(0). (I.10)
</p>
<p>The solution of this equation is g(x) = ce i x , with the boundary condition g(1) =
eig(0). It follows that ce
</p>
<p>i
 = eic, or
</p>
<p> =  (m + ) ; m &isin; Z (I.11)
</p>
<p>Hence, the eigenvalues (i.e. the measurable quantities) of the operator p are different
</p>
<p>for each , and we have correspondingly each time another operator p (again:
</p>
<p>although it always contains the &lsquo;same derivative&rsquo; 
i
</p>
<p>d
dx
</p>
<p>).
</p>
<p>Furthermore, one cannot modify the domain of definition area for every symmetric
</p>
<p>operator in such a way that it becomes self-adjoint. These facts can also be demon-
</p>
<p>strated in terms of the momentum. We choose here p&infin; = i ddx ; the domain of defini-
tion consists of the differentiable and square integrable functions g(x) &isin; L2 [0,&infin;]
with g(0) = g (&infin;) = 0 (e.g. g(x) = xe&minus;x ). The operator p&infin; is symmetric because
of
</p>
<p> f | p&infin;g &minus; p&infin; f | g =

</p>
<p>i
</p>
<p>&infin;&int;
</p>
<p>0
</p>
<p>f &lowast;(x)
dg(x)
</p>
<p>dx
dr + 
</p>
<p>i
</p>
<p>&infin;&int;
</p>
<p>0
</p>
<p>(
d f (x)
</p>
<p>dx
</p>
<p>)&lowast;
g(x)dr
</p>
<p>= 
i
</p>
<p>[
f &lowast;(x)g(x)
</p>
<p>]&infin;
0
</p>
<p>= 0. (I.12)
</p>
<p>For the adjoint operator, we see that:
</p>
<p>&lang;
p
</p>
<p>&dagger;
&infin; f
</p>
<p> g=  f | p&infin;g=

</p>
<p>i
</p>
<p>&infin;&int;
</p>
<p>0
</p>
<p>f &lowast;(x)
dg(x)
</p>
<p>dx
dx=
</p>
<p>i
</p>
<p>[
f &lowast;(x)g(x)
</p>
<p>]&infin;
0
</p>
<p>&minus; 
i
</p>
<p>&infin;&int;
</p>
<p>0
</p>
<p>d f &lowast;(x)
dx
</p>
<p>g(x)dx
</p>
<p>= 
i
</p>
<p>[
f &lowast;(x)g(x)
</p>
<p>]&infin;
0
</p>
<p>+
&infin;&int;
</p>
<p>0
</p>
<p>(

</p>
<p>i
d f (x)
</p>
<p>dx
</p>
<p>)&lowast;
g(x)dx = 
</p>
<p>i
</p>
<p>[
f &lowast;(x)g(x)
</p>
<p>]&infin;
0
</p>
<p>+ p&infin; f | g .
</p>
<p>(I.13)
</p>
<p>The integrated term on the right-hand side always vanishes because of g(0) =
g (&infin;) = 0, regardless of the values of f ; the domain of definition of p&dagger;&infin; is therefore
larger than that of p&infin;. It can be shown in this case that there is no adjustment which
will make the domains of definition of p&infin; and p&dagger;&infin; coincide.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix I: Operators 297
</p>
<p>I.2.3 A Note on Terminology
</p>
<p>The use of the terms &lsquo;symmetric&rsquo; and &lsquo;self-adjoint&rsquo; in the mathematical literature is
</p>
<p>very consistent, while &lsquo;Hermitian&rsquo; appears there occasionally with different mean-
</p>
<p>ings. In physics, however, the terms Hermitian and self-adjoint are often used without
</p>
<p>distinction; but one finds also Hermitian conjugate, adjoint, symmetric. The fact that
</p>
<p>physics can allow itself to be a bit negligent with regard to these differences is mainly
</p>
<p>due to the circumstance that we can imagine &lsquo;difficult&rsquo; spaces as limiting cases of
</p>
<p>simpler spaces&mdash;e.g. by discretization, as we have seen in Chap. 12.
</p>
<p>In addition, we must not forget that the goal of physics is the description and the
</p>
<p>widest possible understanding of the &lsquo;physical&rsquo; world, which means, among other
</p>
<p>things, that for us, mathematics is not an end in itself, but rather an essential and
</p>
<p>powerful tool.
</p>
<p>I.3 Unitary Operators; Stone&rsquo;s Theorem
</p>
<p>We consider unitary operators briefly once more, along with the theorem of Stone.
</p>
<p>As a definition, we can use the fact that an operator is unitary on a Hilbert space H
</p>
<p>if it has an inverse and if it conserves all scalar products, i.e. the equality U |U =
 | is valid for all vectors &isin; H.
</p>
<p>This definition is equivalent to the formulation that UU &dagger; = U &dagger;U = 1. We note
that in finite-dimensional spaces, the left inverse is automatically equal to the right
</p>
<p>inverse. In infinite-dimensional spaces, this is not necessarily true, and that is why
</p>
<p>one needs both formulations there, UU &dagger; = 1 and U &dagger;U = 1. As an example, we
consider vectors (c1, c2, c3, . . .) &isin; C&infin;, on which two operators A and B act accord-
ing to A (c1, c2, c3, . . .) = (c2, c3, c4, . . .) and B (c1, c2, c3, . . .) = (0, c1, c2, . . .).
Evidently, we have AB = 1 and B A 	= 1. In other words, B is the right inverse of
A, but not the left inverse (see also the exercises).
</p>
<p>We note in passing that an operator which conserves the norm is called isometric.
</p>
<p>In a finite-dimensional vector space, an isometry is automatically a unitary operator.
</p>
<p>Because of the independence of the physical predictions of unitary transforma-
</p>
<p>tions, we can conclude that the relation of physical variables with their mathematical
</p>
<p>representations is defined only up to unitary transformations. More generally, one
</p>
<p>could consider transformations | &rarr;
&prime;
</p>
<p>&rang;
, for which
</p>
<p>&lang;&prime;
&prime;
</p>
<p>&rang; = | || holds for
all vectors. Such transformations apparently do not change probability statements.
</p>
<p>However, there is no obvious reason that such transformations should be linear, let
</p>
<p>alone that they must be unitary transformations. In this situation, Wigner&rsquo;s theorem
</p>
<p>(see also Chap. 21, Vol. 2) comes to our aid; it states that there is an operator U which
</p>
<p>is either unitary or anti-unitary, and which satisfies the equation U | =
&prime;
</p>
<p>&rang;
for all
</p>
<p>vectors in H.</p>
<p/>
</div>
<div class="page"><p/>
<p>298 Appendix I: Operators
</p>
<p>I.3.1 Stone&rsquo;s Theorem
</p>
<p>Unitary operators occur naturally (so to speak automatically) if the system has a
</p>
<p>symmetry (see Chap. 21, Vol. 2). In this context, the theorem of Stone is of impor-
</p>
<p>tance.48
</p>
<p>It reads: A set of unitary operators U depending on a continuous parameter 
</p>
<p>satisfies the rule of an Abelian group:
</p>
<p>U (1 + 2) = U (2)U (1) . (I.14)
</p>
<p>Then there exists an Hermitian operator T such that
</p>
<p>U () = eiT . (I.15)
</p>
<p>We see that ei A is unitary if A is self-adjoint.49
</p>
<p>An equivalent formulation of this theorem is e.g.: If U (),  &isin; R satisfies the
following three conditions: (1) the matrix element  |U ()| is for all vectors
a continuous function of ; (2) U (0) = 1; (3) for all 1,2 &isin; R, it holds that
U (1)U (2) = U (1 + 2)&mdash;then there is a unique self-adjoint operator such that
U () = eiA and
</p>
<p>i A | = lim
&rarr;0
</p>
<p>U ()&minus; 1

</p>
<p>| for all | &isin; H. (I.16)
</p>
<p>In Chap. 13, we established the relation between a Hamiltonian H and a propagator
</p>
<p>U = e&minus;i Ht/; we now see that it was practically a derivation by example of Stone&rsquo;s
theorem.50
</p>
<p>I.3.2 Unitary or Hermitian?
</p>
<p>Finally, a word about the relation between unitary and Hermitian operators, related
</p>
<p>to the question of the boundedness of operators:
</p>
<p>We know that Hermitian operators can cause problems if they are not bounded. We
</p>
<p>also know that the unitary operator U () = eiT is bounded, even for an unbounded
Hermitian operator T . Thus, one might regard the unitary operator U as more fun-
</p>
<p>damental than the Hermitian operator T in this case.
</p>
<p>48In practice, the theorem of Stone is one of the most important ways by which self-adjoint operators
</p>
<p>enter quantum mechanics (symmetry &rarr; unitary operator &rarr; self-adjoint operator).
49 &rarr; U () is called a unitary representation of the additive group of real numbers if for a one-
parameter family of unitary operators U () = eiA,  &isin; R, the following applies: (1) U (0) = 1;
(2) U (1)U (2) = U (1 + 2); (3) U (&minus;) = U&minus;1().
50A similar consideration can be entertained for time-dependent Hamiltonians, but the result is
</p>
<p>somewhat more complicated, as in this case different times occur, which must be placed in the
</p>
<p>correct order (keyword: time-ordering operator).</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix I: Operators 299
</p>
<p>As an example, we consider free one-dimensional motion with &minus;&infin; &lt; x &lt; &infin;.
The momentum operator p (and hence the Hamiltonian
</p>
<p>p2
</p>
<p>2m
) is not bounded; its
</p>
<p>domain of definition comprises all functions whose derivatives are square integrable.
</p>
<p>We now choose the function  (x, 0) = e&minus;i x2 sin x
x
</p>
<p>which is continuous and differen-
</p>
<p>tiable, but does not belong to the domain of definition of the momentum operator,
</p>
<p>because its derivative is not square integrable. This means, strictly speaking, that
</p>
<p>the free time-dependent SEq is not meaningful for this initial condition&mdash;we cannot
</p>
<p>&lsquo;really&rsquo; allow such an initial condition. But on the other hand, the time-evolution
</p>
<p>operator U (t) = e&minus;i Ht/ is bounded (its norm is 1); its domain of definition is thus
the entire Hilbert space. One can rewrite U in this case so that differential operators
</p>
<p>no longer appear in the exponent; the result can be written as an integral operator
</p>
<p>and reads (see also the exercises for Chap. 5):
</p>
<p> (x, t) =
&radic;
</p>
<p>m
</p>
<p>2it
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>ei
m(x&minus;y)2
</p>
<p>2t  (y, 0) dy. (I.17)
</p>
<p>In this formulation of the free SEq, the above problems do not occur. In other words,
</p>
<p>the unitary time-evolution operator is more fundamental than the Hamiltonian H .
</p>
<p>As a further example, we consider the position-momentum commutation relation
</p>
<p>[x, p] = i. (I.18)
</p>
<p>x and p are unbounded Hermitian operators; thus, the right side of this relation is
</p>
<p>always defined, but not necessarily also the left side. But one can rewrite this relation;
</p>
<p>its Weyl form reads
</p>
<p>ei
pa
</p>
<p> eibx e&minus;i
pa
</p>
<p> = eibx eiba (I.19)
</p>
<p>(see Chap. 21, Vol. 2). On both sides of this equation, there are only bounded (unitary)
</p>
<p>operators; hence, this form is more universal than [x, p] = i.
We will not delve further into this topic. Perhaps we should make only the remark
</p>
<p>that these and similar considerations contribute to the somewhat nonchalant attitude
</p>
<p>towards mathematics among physicists: We can often treat problems with the usual
</p>
<p>instruments, although they are &lsquo;strictly speaking&rsquo; not well defined. Of course this is
</p>
<p>not always true&mdash;one can fail miserably if one does not consider essential conditions.
</p>
<p>But by and large, quantum mechanics is quite well behaved.
</p>
<p>I.4 The Uncertainty Principle
</p>
<p>Those relations which concern two Hermitian operators A and B have to do with
</p>
<p>variances or standard deviations (see Chap. 9). With the deviation from the mean
</p>
<p>value</p>
<p/>
</div>
<div class="page"><p/>
<p>300 Appendix I: Operators
</p>
<p>A&minus; = A &minus; A , (I.20)
</p>
<p>we obtain &lang;
A2&minus;
</p>
<p>&rang;
=
</p>
<p>&lang;
(A &minus; A)2
</p>
<p>&rang;
=
</p>
<p>&lang;
A2
</p>
<p>&rang;
&minus; A2 = (A)2 . (I.21)
</p>
<p>We derive the uncertainty principle in two different ways.
</p>
<p>I.4.1 Derivation 1
</p>
<p>First, we note the general relation that the commutator of two Hermitian operators
</p>
<p>is an anti-Hermitian operator:
</p>
<p>[A, B]&dagger; = (AB &minus; B A)&dagger; = B A &minus; AB = &minus; [A, B] . (I.22)
</p>
<p>Thus, we can always write [A, B] = iC with C = C&dagger;. Next, we consider the
following norm:
</p>
<p>(A&minus; + iB&minus;) |2 &ge; 0,  &isin; R. (I.23)
</p>
<p>It holds that
</p>
<p>(A&minus; + iB&minus;) |2 = | (A&minus; &minus; iB&minus;) (A&minus; + iB&minus;) |
= | A2&minus; + i
</p>
<p>[
A&minus;, B&minus;
</p>
<p>]
+ 2 B2&minus; | .
</p>
<p>(I.24)
</p>
<p>Evaluation of the commutator gives
</p>
<p>[
A&minus;, B&minus;
</p>
<p>]
= [A, B] = iC; C = C&dagger;. (I.25)
</p>
<p>With this, (I.24) can be written as
</p>
<p>(A&minus; + iB&minus;) |2 =
&lang;
A2&minus;
</p>
<p>&rang;
&minus; C+2
</p>
<p>&lang;
B2&minus;
</p>
<p>&rang;
= (A)2&minus; C+2 (B)2 &ge; 0.
</p>
<p>(I.26)
</p>
<p>Since C is Hermitian, C is real (see Chap. 9). The last inequality must be satisfied
for all ; hence there is at most one zero of the quadratic polynomial in . This
</p>
<p>means51 that
</p>
<p>C2 &minus; 4 (A)2 (B)2 &le; 0. (I.27)
</p>
<p>It follows that (A)2 (B)2 &ge; C2 /4, or
</p>
<p>A &middot;B &ge; 1
2
|[A, B]| . (I.28)
</p>
<p>51The function f (x) = x2 + bx + c has the zeros x0 = &minus;b&plusmn;
&radic;
</p>
<p>b2&minus;4ac
2a
</p>
<p>. If we require that f (x) be
</p>
<p>non-negative (restricting ourselves to real numbers) then the radicand must satisfy b2 &minus; 4ac &le; 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix I: Operators 301
</p>
<p>This is the general uncertainty principle for two Hermitian operators. It is especially
</p>
<p>popular in terms of the pair x and px . Because of [x, px ] = i, it follows that
</p>
<p>x &middot;px &ge;

</p>
<p>2
. (I.29)
</p>
<p>I.4.2 Derivation 2
</p>
<p>For this derivation, we use | ||2 &le;  |  |, i.e. the Schwarz inequality.
Here, we insert | = A&minus; | = |A&minus; and | = B&minus; | = |B&minus;, where | is
an arbitrary state. It follows that
</p>
<p>|A&minus; |B&minus;|2 &le; A&minus; |A&minus; B&minus; |B&minus; . (I.30)
</p>
<p>Next, we use the fact that A and B are Hermitian, i.e. their mean values are real and
</p>
<p>consequently A&minus; and B&minus; are Hermitian, too.52 This leads to
</p>
<p>| |A&minus;B&minus;|2 &le; 
A2&minus;
</p>
<p>&rang;

</p>
<p>B2&minus;
&rang;
=
</p>
<p>&lang;
A2&minus;
</p>
<p>&rang; &lang;
B2&minus;
</p>
<p>&rang;
. (I.31)
</p>
<p>On the right side, we already have acceptable terms, with
</p>
<p>&lang;
A2&minus;
</p>
<p>&rang;
=
</p>
<p>&lang;
(A &minus; A)2
</p>
<p>&rang;
= (A)2 . (I.32)
</p>
<p>Now we consider the transformation of the left side. Here we use the fact that we
</p>
<p>can write any product of operators as a sum of a Hermitian and an anti-Hermitian
</p>
<p>part. We realize that the anticommutator of two Hermitian operators
</p>
<p>{A, B} = AB + B A (I.33)
</p>
<p>is Hermitian, while the commutator is anti-Hermitian (if it does not vanish), as shown
</p>
<p>above:
</p>
<p>[A, B]&dagger; = &minus; [A, B] . (I.34)
</p>
<p>We know that the mean value of a Hermitian operator is real: it remains to show that
</p>
<p>the mean value of an anti-Hermitian operator is imaginary (see the exercises). Now
</p>
<p>we write
</p>
<p>AB = 1
2
{A, B} + 1
</p>
<p>2
[A, B] . (I.35)
</p>
<p>It follows first of all that
</p>
<p>52In general, this statement is not satisfied for non-Hermitian operators.</p>
<p/>
</div>
<div class="page"><p/>
<p>302 Appendix I: Operators
</p>
<p>||A&minus;B&minus;|2 =
1
</p>
<p>4
||({A&minus;, B&minus;} + [A&minus;, B&minus;])|2
</p>
<p>= 1
4
|{A&minus;, B&minus;} + [A&minus;, B&minus;]|2. (I.36)
</p>
<p>Due to {A&minus;, B&minus;} &isin; R and
[
A&minus;, B&minus;
</p>
<p>]
&isin; I, it follows that53
</p>
<p>| |A&minus;B&minus;|2 =
1
</p>
<p>4
|{A&minus;, B&minus;}|2 +
</p>
<p>1
</p>
<p>4
</p>
<p>&lang;[A&minus;, B&minus;
]&rang;2 (I.37)
</p>
<p>so that we can write (I.31), considering (I.32), as
</p>
<p>(A)2 (B)2 &ge; 1
4
|{A&minus;, B&minus;}|2 +
</p>
<p>1
</p>
<p>4
</p>
<p>&lang;[A&minus;, B&minus;
]&rang;2 . (I.38)
</p>
<p>The second term on the right-hand side can be written as:
</p>
<p>[
A&minus;, B&minus;
</p>
<p>]
= [A &minus; A , B &minus; B] = AB &minus; B A = [A, B] . (I.39)
</p>
<p>Since there is no corresponding simplification for the anticommutator, it is simply
</p>
<p>omitted; this gives the inequality
</p>
<p>(A)2 (B)2 &ge; 1
4
|[A, B]|2 . (I.40)
</p>
<p>Taking the root, we obtain the uncertainty principle:
</p>
<p>(A) (B) &ge; 1
2
|[A, B]| . (I.41)
</p>
<p>I.4.3 Remarks on the Uncertainty Principle
</p>
<p>The first remark concerns a common misinterpretation of the uncertainty principle,
</p>
<p>according to which the product of the uncertainties for non-commuting operators is
</p>
<p>always greater than zero. But this is not true, because the right side of the uncertainty
</p>
<p>principle contains not the bare commutator, but rather its expectation value&mdash;and
</p>
<p>that can vanish, even if the commutator itself is not zero. This can be seen perhaps
</p>
<p>most clearly if one notes explicitly the dependence on the state. As an example, we
</p>
<p>consider a general angular momentum J, whose components satisfy
[
Jx , Jy
</p>
<p>]
= iJz
</p>
<p>(and cyclically interchanged equations); see Chap. 16, Vol. 2. Then we have
</p>
<p>53As is well known, for a + ib with a, b &isin; R, |a + ib|2 = |a|2 + |b|2.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix I: Operators 303
</p>
<p>(
 Jx
</p>
<p>) (
 Jy
</p>
<p>)
&ge; 
</p>
<p>2
</p>
<p>Jz
 , (I.42)
</p>
<p>and for states |with Jz = 0, there is no lower positive limit for
(
 Jx
</p>
<p>) (
 Jy
</p>
<p>)
.
</p>
<p>An explicit example is found in the exercises.
</p>
<p>The second remark concerns the domain of validity of the uncertainty principle:
</p>
<p>For the position x , with 0 &le; x &le; 1, we define the corresponding momentum by
p = 
</p>
<p>i
d
</p>
<p>dx
. Both operators are self-adjoint if the scalar product is defined as usual and
</p>
<p>the domain of definition of p is restricted to differentiable functions g which satisfy
</p>
<p>g(1) = g(0) (see exercises). Does the uncertainty principle xp &ge; 
2
</p>
<p>apply under
</p>
<p>these premises?
</p>
<p>The answer is no. We consider first the eigenfunctions of p. They are determined
</p>
<p>by

</p>
<p>i
</p>
<p>d
</p>
<p>dx
g(x) = g(x); g(1) = g(0) (I.43)
</p>
<p>to give
</p>
<p>g(x) = g0e
i

</p>
<p>x and g0e
i
 = g0e2im or  = 2m; m &isin; Z (I.44)
</p>
<p>This means that
</p>
<p>gm(x) = g0e2imx ; m &isin; Z (I.45)
</p>
<p>We fix the constant g0 by means of the normalization, i.e.
&int; 1
</p>
<p>0
g&lowast;m(x)gm(x)dx = 1,
</p>
<p>and obtain for the eigenfunctions54
</p>
<p>gm(x) = eie2imx ; m &isin; Z (I.46)
</p>
<p>For these states, we now calculate the quantities occurring in the uncertainty principle.
</p>
<p>We have
</p>
<p>(p)2 =
&lang;
p2
&rang;
&minus; p2 =
</p>
<p>&lang;
gm
</p>
<p>p2
 gm
</p>
<p>&rang;
&minus; gm |p| gm2
</p>
<p>=
1&int;
</p>
<p>0
</p>
<p>g&lowast;m p
2gmdx &minus;
</p>
<p>

</p>
<p>1&int;
</p>
<p>0
</p>
<p>g&lowast;m pgmdx
</p>
<p>

</p>
<p>2
</p>
<p>=
1&int;
</p>
<p>0
</p>
<p>(2im)2 dx &minus;
</p>
<p>

</p>
<p>1&int;
</p>
<p>0
</p>
<p>(2im) dx
</p>
<p>

</p>
<p>2
</p>
<p>= 0
</p>
<p>(I.47)
</p>
<p>and
</p>
<p>(x)2 =
&lang;
x2
&rang;
&minus; x2 =
</p>
<p>&lang;
gm
</p>
<p>x2
 gm
</p>
<p>&rang;
&minus; gm |x | gm2
</p>
<p>=
1&int;
</p>
<p>0
</p>
<p>g&lowast;m x
2gmdx &minus;
</p>
<p>

</p>
<p>1&int;
</p>
<p>0
</p>
<p>g&lowast;m xgmdx
</p>
<p>

</p>
<p>2
</p>
<p>=
1&int;
</p>
<p>0
</p>
<p>x2dx &minus;
</p>
<p>

</p>
<p>1&int;
</p>
<p>0
</p>
<p>xdx
</p>
<p>

</p>
<p>2
</p>
<p>= 1
3
&minus;
</p>
<p>[
1
2
</p>
<p>]2 = 1
12
.
</p>
<p>(I.48)
</p>
<p>54By the way, this is essentially the basis of the Fourier series for periodic functions.</p>
<p/>
</div>
<div class="page"><p/>
<p>304 Appendix I: Operators
</p>
<p>Following this argumentation, we should obtain (p) (x) = 0 and not (p) (x) &ge;

</p>
<p>2
. Where have we made a mistake?
</p>
<p>Answer: The eigenfunctions gm are not in the domain of definition of the operator
</p>
<p>product px , since xgm = xeie2imx does not satisfy the periodicity condition g(1) =
g(0) and therefore does not belong to the domain of definition of p.
</p>
<p>This is an example of the fact that the uncertainty principle applies only when all
</p>
<p>terms are defined, including the terms appearing in intermediate calculations. This
</p>
<p>is once more an indication that in computations involving unbounded operators, one
</p>
<p>always has to be careful.
</p>
<p>I.5 Hermitian Operators, Observables
</p>
<p>We want to give a brief note on a terminology problem. It addresses the term observ-
</p>
<p>able, i.e. an observable and measurable quantity. Examples where it is intuitively clear
</p>
<p>that we are dealing with an observable are obvious (position, momentum, energy,
</p>
<p>angular momentum, etc.). But a unique and precise meaning of the term in the context
</p>
<p>of quantum mechanics does not exist.
</p>
<p>For some (e.g. Schwabl), the term observable stands for &lsquo;physical quantity&rsquo;, and
</p>
<p>is different from the operators that are associated with them in quantum mechanics.
</p>
<p>For reasons of clarity, though, usually the same symbol is chosen for both; but in
</p>
<p>principle, in this terminology, the term &lsquo;observable A&rsquo; is simply a shorthand notation
</p>
<p>for &lsquo;the physical quantity Ameas represented by the operator Aop&rsquo;.
</p>
<p>For others, observable denotes a Hermitian operator whose eigenvectors form a
</p>
<p>complete orthonormal system (CONS). One can understand this as a technical term
</p>
<p>that has nothing to do with the question of whether one can assign a corresponding
</p>
<p>physical quantity to an observable. However, if one wants to establish this relation-
</p>
<p>ship, it has to be realized that this definition is not a sharp criterion, because there are
</p>
<p>indeed Hermitian operators of this type which, in a certain sense, do not correspond
</p>
<p>to measurable quantities; an example can be found below.
</p>
<p>Finally, there are still others who in the face of these difficulties and diffuseness
</p>
<p>declare the term &lsquo;observable&rsquo; to be dispensable, because it is of no real interest. In
</p>
<p>fact, it appears that the use of the term is not a compelling necessity, but rather is due
</p>
<p>to convenience and has become simply a habit.
</p>
<p>The fact that we still use the term &lsquo;observable&rsquo; in this text is due to the circumstance
</p>
<p>that, despite its ambiguity, it makes us aware of two issues: First, it indicates that we
</p>
<p>are dealing not only with an abstract operator in an abstract space, but also with a
</p>
<p>physical quantity which we can concretely measure in the laboratory. On the other
</p>
<p>hand, the concept tells us that it is an Hermitian operator, with a real spectrum,
</p>
<p>orthogonal eigenvectors, etc.
</p>
<p>Now for the example of the Hermitian operators just mentioned which do not
</p>
<p>correspond to any measured variable - at least in a certain sense. It is perhaps a rather
</p>
<p>subtle point, but the consideration may help to clarify the concepts.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix I: Operators 305
</p>
<p>We note that an operator can be seen as a measuring instruction. For instance, sx
means to measure the x-component of the spin, i.e. to measure the spin along the unit
</p>
<p>vector (1, 0, 0). A difficulty arises when we combine operators in a way which does
</p>
<p>not result in a reasonable measuring instruction at first glance. Take, for example,
</p>
<p>sums of Hermitian operators that do not commute:
</p>
<p>C = A + B ; A = A&dagger; ; B = B&dagger; ; [A, B] 	= 0 (I.49)
</p>
<p>C is Hermitian, of course. The problem is that the order in which measurements of
</p>
<p>A and B are carried out plays a role (because of [A, B] 	= 0); this, however, is not
reflected anywhere in C = A + B. Specifically, we consider the spin- 1
</p>
<p>2
matrices
</p>
<p>A = sx and B = sz :
</p>
<p>C = 
2
</p>
<p>(
1 1
</p>
<p>1 &minus;1
</p>
<p>)
;
[
sx , sz
</p>
<p>]
= &minus;isy . (I.50)
</p>
<p>Clearly, C is a Hermitian operator; its eigenvalues are  = &plusmn; &radic;
2
. However, C =
</p>
<p>sx +sz is not a measurable quantity in the sense of the measuring instruction &rsquo;measure
the x-component plus (and) the z-component of the spin&rsquo;. Even if we could measure
</p>
<p>sx and sz simultaneously, the result would not be equal to the measurement of C .
</p>
<p>Indeed, a measurement of C gives the value &radic;
2
</p>
<p>or &minus; &radic;
2
,55 while that of sx and sz
</p>
<p>respectively gives 
2
</p>
<p>or &minus;
2
</p>
<p>, i.e. in sum (regardless of the order of the measurement
</p>
<p>of sx and sz), one of three values  or 0 or &minus;, but never &plusmn; &radic;
2
.
</p>
<p>Thus, C is not an observable in the literal sense suggested by the notation that
</p>
<p>namely sx and sz are to be measured.
56 However, we can C represent as the spin
</p>
<p>operator along the vector x z = (1, 0, 1):
</p>
<p>C = x z &middot; s = sx + sz (I.51)
</p>
<p>Usually, the spin is measured with respect to the unit vector, and we can write
</p>
<p>C &prime; = 1&radic;
2
</p>
<p>C = x z&radic;
2
&middot; s = sx + sz&radic;
</p>
<p>2
(I.52)
</p>
<p>A short calculation shows that C &prime; has the eigenvalues &plusmn;
2
</p>
<p>. Thus, C is an observable
</p>
<p>in the sense of an instruction to measure the spin along (1, 0, 1) (which is one
</p>
<p>measurement), but not in the sense to measure the spin along the x-.axis and the
</p>
<p>z-axis (which would be two measurements).
</p>
<p>A similar consideration applies, e.g., for the harmonic oscillator with H = 1
2m
</p>
<p>p2+
m2
</p>
<p>2
x2. The energy eigenvalues are not related in a simple way to those of the operators
</p>
<p>55See exercises to Chap. 14.
56Note that the eigenvectors of C form a basis in the state space, as the above definition of an
</p>
<p>observable requires.</p>
<p/>
</div>
<div class="page"><p/>
<p>306 Appendix I: Operators
</p>
<p>p2
</p>
<p>2m
and m
</p>
<p>2
</p>
<p>2
x2. Also here holds that the result of an energy measurement would not
</p>
<p>equal the sum of the separate measurements of
p2
</p>
<p>2m
and m
</p>
<p>2
</p>
<p>2
x2 even if we could
</p>
<p>measure position and momentum simultaneously.
</p>
<p>In quantum field theory, there is another type (case) of Hermitian operators which
</p>
<p>are not observable. There, the Hamiltonians are expressed in terms of creation and
</p>
<p>annihilation operators which is possible in several distinct ways. But only one of these
</p>
<p>representations corresponds (for certain applications) to an observable, namely the
</p>
<p>so-called normal-ordered form; see Vol. 2 Appendix W.
</p>
<p>I.6 Exercises
</p>
<p>1. The action of two operators A and B on a vector (c1, c2, c3, . . .) is A(c1, c2, c3,
</p>
<p>. . .) = (c2, c3, c4, . . .) and B (c1, c2, c3, . . .) = (0, c1, c2, . . .). What is the matrix
representation of the two operators? Determine AB and B A.
</p>
<p>Solution: We see that
</p>
<p>A =
</p>
<p>

</p>
<p>0 1 0 0 . . .
</p>
<p>0 0 1 0 . . .
</p>
<p>0 0 0 1 . . .
...
...
...
...
</p>
<p>...
</p>
<p>
 ; B =
</p>
<p>

</p>
<p>0 0 0 0 . . .
</p>
<p>1 0 0 0 . . .
</p>
<p>0 1 0 0 . . .
...
...
...
...
</p>
<p>...
</p>
<p>
 . (I.53)
</p>
<p>Furthermore, we have
</p>
<p>AB =
</p>
<p>

</p>
<p>1 0 0 0 . . .
</p>
<p>0 1 0 0 . . .
</p>
<p>0 0 1 0 . . .
...
...
...
...
</p>
<p>...
</p>
<p>
 ; B A =
</p>
<p>

</p>
<p>0 0 0 0 . . .
</p>
<p>0 1 0 0 . . .
</p>
<p>0 0 1 0 . . .
...
...
...
...
</p>
<p>...
</p>
<p>
 . (I.54)
</p>
<p>Hence, B is the right inverse of A, but not the left inverse. In a finite vector space,
</p>
<p>these terms always coincide.
</p>
<p>2. Show that p is not bounded in the space L(2) of the functions which are defined on
</p>
<p>the interval [0, b] and are continuous there. Solution: Consider e.g. the function
</p>
<p>f (x) = x&minus;a with a &gt; 0. In order that
&int; b
</p>
<p>0
</p>
<p>x&minus;2adx be defined, it must hold that
</p>
<p>&minus;2a + 1 &gt; 0 or a &lt; 1
2
. Hence, all f (x) = x&minus;a with 0 &lt; a &lt; 1
</p>
<p>2
are in L(2), but
</p>
<p>not in the domain of definition of p , because we have
</p>
<p>b&int;
</p>
<p>0
</p>
<p>f &prime;2dx = a2
b&int;
</p>
<p>0
</p>
<p>x&minus;2a&minus;2dx = a2 x
&minus;2a&minus;1
</p>
<p>&minus;2a &minus; 1
/b
</p>
<p>0
. (I.55)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix I: Operators 307
</p>
<p>Evidently, the last term exists only for &minus;2a &minus; 1 &gt; 0, i.e. for a &lt; &minus; 1
2
</p>
<p>.
</p>
<p>3. Show that in a finite-dimensional Hilbert space, all operators are bounded.
</p>
<p>Solution: All Hilbert spaces of the same dimension are isomorphic. Thus, we can
</p>
<p>choose the space Cn (complex-valued n-tuple) as our finite-dimensional space;
</p>
<p>correspondingly, the operators are matrices.
</p>
<p>Define
</p>
<p>A =
</p>
<p>

</p>
<p>a11 a12 . . . a1n
a21 a22 . . . a
...
</p>
<p>...
. . .
</p>
<p>...
</p>
<p>an1 an2 . . . ann
</p>
<p>
 ; | =
</p>
<p>

</p>
<p>1
2
...
</p>
<p>n
</p>
<p>
 . (I.56)
</p>
<p>Then we have
</p>
<p>A |2 = | A&dagger; A | =
n&sum;
</p>
<p>j,k,l=1
&lowast;ka
</p>
<p>&lowast;
jka jll =
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>n&sum;
</p>
<p>k=1
&lowast;ka
</p>
<p>&lowast;
jk
</p>
<p>n&sum;
</p>
<p>l=1
a jll .
</p>
<p>(I.57)
</p>
<p>We define the i th column of A as ai |, i.e.
</p>
<p>|ai  =
</p>
<p>

</p>
<p>a&lowast;i1
a&lowast;i2
...
</p>
<p>a&lowast;in
</p>
<p>
 ; 
</p>
<p>a j
&rang;
=
</p>
<p>n&sum;
</p>
<p>k=1
&lowast;ka
</p>
<p>&lowast;
jk . (I.58)
</p>
<p>It follows that
</p>
<p>A |2 =
n&sum;
</p>
<p>j=1
</p>
<p>n&sum;
</p>
<p>k=1
&lowast;ka
</p>
<p>&lowast;
jk
</p>
<p>n&sum;
</p>
<p>l=1
a jll =
</p>
<p>n&sum;
</p>
<p>j=1

</p>
<p>a j
&rang;

</p>
<p>a j
&rang;&lowast; =
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>
a j
</p>
<p>&rang;2 .
</p>
<p>(I.59)
</p>
<p>with the Schwarz inequality, we obtain
</p>
<p>A |2 =
n&sum;
</p>
<p>j=1
</p>
<p>
a j
</p>
<p>&rang;2 &le;
n&sum;
</p>
<p>j=1
|2
</p>
<p>a j
&rang;2 = |2
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>a j
&rang;2 .
</p>
<p>(I.60)
</p>
<p>For the norm, it results that
</p>
<p>A = sup A || &le; sup
</p>
<p>&radic;&radic;&radic;&radic;&radic;&radic;
|2
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>a j
&rang;2
</p>
<p>|2
</p>
<p>=
&radic;&radic;&radic;&radic;
</p>
<p>n&sum;
</p>
<p>j=1
</p>
<p>a j
&rang;2 &le; &radic;n &middot; max
</p>
<p>a j
&rang; .
</p>
<p>(I.61)</p>
<p/>
</div>
<div class="page"><p/>
<p>308 Appendix I: Operators
</p>
<p>The right-hand side is apparently finite and may be estimated by a constant C .
</p>
<p>4. Given two Hermitian operators A and B with the commutator [A, B] = i , show
that at least one of the two operators is not bounded.
</p>
<p>Solution: We first assume that one of the operators is bounded, say B &le; 1.
Then we prove the relation
</p>
<p>[
An, B
</p>
<p>]
= in An&minus;1. (I.62)
</p>
<p>This is done by induction. Clearly, the base for n = 1 is correct. Then it holds
that57
</p>
<p>[
An+1, B
</p>
<p>]
= A
</p>
<p>[
An, B
</p>
<p>]
+ [A, B] An = Ain An&minus;1 + i An = i (n + 1) An,
</p>
<p>(I.63)
</p>
<p>whereby the proposition is proved.
</p>
<p>Thus we have An B &minus; B An = in An&minus;1 or
An B &minus; B An
</p>
<p> = n
An&minus;1
</p>
<p> or n
An&minus;1
</p>
<p> &le;
An B
</p>
<p>+
B An
</p>
<p> , (I.64)
</p>
<p>where we have used the triangle inequality. For bounded operators, AB &le;
A B applies, and it follows that
</p>
<p>n
An&minus;1
</p>
<p> &le; 2
An
</p>
<p> B &le; 2
An
</p>
<p> . (I.65)
</p>
<p>In a Hilbert space,
A&dagger; A
</p>
<p> = A2 for bounded operators; from this, for Hermitian
operators, it follows that
</p>
<p>A2
 = A2. This leads to
</p>
<p>n An&minus;1 &le; 2 An or n
2
&le; A , (I.66)
</p>
<p>i.e. a contradiction to the proposition that A is bounded.
</p>
<p>5. Positive matrices:
</p>
<p>(a) Show that a positive matrix is self-adjoint.
</p>
<p>(b) Show that a matrix is positive iff all its eigenvalues are &ge; 0.
6. Show that the mean value of an anti-Hermitian operator is imaginary.
</p>
<p>Solution: For an anti-Hermitian operator A, we know that (see Chap. 9) A&dagger; = &minus;A.
It follows that
</p>
<p>A&lowast; = | A |&lowast; = | A&dagger; | = &minus; | A | = &minus; A (I.67)
</p>
<p>and thus A is imaginary.
7. The domain of definition of p = 
</p>
<p>i
d
</p>
<p>dx
comprises all functions g(x) &isin; L(2) [0, 1]
</p>
<p>which are differentiable, whose derivatives are square integrable, and which sat-
</p>
<p>isfy the boundary condition g(1) = g(0). Show that p is self-adjoint.
</p>
<p>57We use the equation [AB,C] = A [B,C] + [A,C] B.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix I: Operators 309
</p>
<p>Solution: We see that
</p>
<p>&lang;
p&dagger; f |g =  f |pg
</p>
<p>=
1&int;
</p>
<p>0
</p>
<p>f &lowast; 
i
</p>
<p>dg
dx
</p>
<p>dx = f (1) g (1)&minus; f (0) g (0)+
1&int;
</p>
<p>0
</p>
<p>(

</p>
<p>i
</p>
<p>d f
</p>
<p>dx
</p>
<p>)&lowast;
gdx = p f |g .
</p>
<p>(I.68)
</p>
<p>Thus p is symmetric. The integrated term on the right side vanishes for f (1) =
f (0); hence p&dagger; has the same domain of definition as p. In other words: p is
</p>
<p>self-adjoint.
</p>
<p>8. We consider a spin- 1
2
</p>
<p>system with the uncertainty principle
</p>
<p>(
sx
</p>
<p>) (
sy
</p>
<p>)
&ge; 
</p>
<p>2
</p>
<p>sz
 . (I.69)
</p>
<p>Show that the right-hand side may vanish.
</p>
<p>Solution: For a state | =
(
</p>
<p>a
</p>
<p>b
</p>
<p>)
, it holds that
</p>
<p>sz =
| sz |
 | =
</p>
<p>|a|2 &minus; |b|2
|a|2 + |b|2 . (I.70)
</p>
<p>If we assume that this expression vanishes and that | is normalized, it follows
that
</p>
<p>| = 1&radic;
2
</p>
<p>(
ei
</p>
<p>ei
</p>
<p>)
, (I.71)
</p>
<p>or (extracting a global phase),
</p>
<p>| = e
i
</p>
<p>&radic;
2
</p>
<p>(
1
</p>
<p>ei(&minus;)
</p>
<p>)
= e
</p>
<p>i
</p>
<p>&radic;
2
</p>
<p>(
1
</p>
<p>ei
</p>
<p>)
. (I.72)
</p>
<p>For theses states, it always holds that
</p>
<p>(
sx
</p>
<p>) (
sy
</p>
<p>)
&ge; 0. (I.73)
</p>
<p>We now determine the uncertainties on the left-hand side. First we have</p>
<p/>
</div>
<div class="page"><p/>
<p>310 Appendix I: Operators
</p>
<p>sx  =

</p>
<p>4
</p>
<p>(
1 e&minus;i
</p>
<p>) (0 1
1 0
</p>
<p>)(
1
</p>
<p>ei
</p>
<p>)
</p>
<p>= 
4
</p>
<p>(
1 e&minus;i
</p>
<p>) ( ei
1
</p>
<p>)
= 
</p>
<p>2
cos 
</p>
<p>&lang;
sy
&rang;

= 
</p>
<p>4
</p>
<p>(
1 e&minus;i
</p>
<p>) (0 &minus;i
i 0
</p>
<p>)(
1
</p>
<p>ei
</p>
<p>)
</p>
<p>= 
4
</p>
<p>(
1 e&minus;i
</p>
<p>) (&minus;iei
i
</p>
<p>)
= 
</p>
<p>2
sin .
</p>
<p>(I.74)
</p>
<p>Then we obtain with s2x = s2y = 
2
</p>
<p>4
and (A)2 =
</p>
<p>&lang;
A2
</p>
<p>&rang;
&minus; A2
</p>
<p>(
sx
</p>
<p>)2 = 
2
</p>
<p>4
&minus; 
</p>
<p>2
</p>
<p>4
cos2  = 
</p>
<p>2
</p>
<p>4
sin2 
</p>
<p>(
sy
</p>
<p>)2 = 2
4
&minus; 2
</p>
<p>4
sin2  = 2
</p>
<p>4
cos2 ,
</p>
<p>(I.75)
</p>
<p>and the uncertainty principle is reduced in this case to the inequality
</p>
<p>2
</p>
<p>4
|sin  cos | = 
</p>
<p>2
</p>
<p>2
|sin 2| &ge; 0. (I.76)
</p>
<p>Depending on the choice of , the uncertainty vanishes and with it also the product
</p>
<p>of the two uncertainties.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix J
</p>
<p>From Quantum Hopping to the Schr&ouml;dinger
</p>
<p>Equation
</p>
<p>This alternative derivation of the SEq58 is based on general principles, namely sym-
</p>
<p>metry and superposition in combination with the idea of a discretized space.59 The
</p>
<p>approach emphasizes the structurally simple side of quantum mechanics, and not
</p>
<p>the paradoxical side. It uses the idea that a quantum object which &lsquo;lives&rsquo; only on
</p>
<p>the discrete sites of a lattice can still move quantum mechanically via the partial
</p>
<p>overlap of the position states on neighboring sites. This corresponds to the &lsquo;hopping
</p>
<p>equation&rsquo; which we derive below.
</p>
<p>Hopping Equation
</p>
<p>We define a grid on the one-dimensional space in contiguous intervals of lengths l
</p>
<p>which we number consecutively by n = &middot; &middot; &middot; &minus; 3,&minus;2,&minus;1, 0, 1, 2, 3 . . .. In this one-
dimensional lattice, we place detectors which determine where the quantum object is
</p>
<p>located at a time t with the resolution l; e.g. detector n reacts at time t . The expression
</p>
<p>|nl, t (J.1)
</p>
<p>thus means that the quantum object is detected at time t at the (interval) position nl,
</p>
<p>in other words, that it is at the position nl at time t . We can think of |nl, t as a column
vector, where at time t , a 1 is at position n and everywhere else 0. For example, the
</p>
<p>state |2l, t (see Fig. J.1) is given by:
</p>
<p>58See also J. Pade and L. Polley, &lsquo;Quanten-H&uuml;pfen auf Gittern&rsquo;, Physik in der Schule 36/11 (1998)
</p>
<p>363, (&lsquo;Quantum hopping on lattices&rsquo;, Physics in School 36/11 (1998) 363).
59In this discretization of space (which is also used in the lattice gauge theory), it is not necessarily
</p>
<p>assumed that such a lattice exists in nature, but rather that there is a limit to the degree of precision
</p>
<p>with which the position can be measured. Consequently, one always has to divide the space into a
</p>
<p>grid in practice, which means that also the laws of physics initially appear in discetized form. In
</p>
<p>general, it is expected that in the continuum limit, i.e. in the limit of infinitely fine resolution, the
</p>
<p>usual laws would be obtained.
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>311</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>312 Appendix J: From Quantum Hopping to the Schr&ouml;dinger Equation
</p>
<p>Fig. J.1 The state |2l, t
</p>
<p>-1 0 1 2 3 4
</p>
<p> l
</p>
<p>Fig. J.2 Possible
</p>
<p>movements during the
</p>
<p>period 
</p>
<p>n-1 n n+1
</p>
<p>|2l, t =
</p>
<p>

</p>
<p>...
</p>
<p>0
</p>
<p>1
</p>
<p>0
...
</p>
<p>

</p>
<p>Position
...
</p>
<p>n = 3
n = 2
n = 1
</p>
<p>...
</p>
<p>(J.2)
</p>
<p>The normalization condition60
</p>
<p>nl, t | nl, t != 1 (J.3)
</p>
<p>must be satisfied for all times. Since the quantum object cannot be detected simulta-
</p>
<p>neously at two places, it must hold (orthogonality) that:
</p>
<p>nl, t | n&prime;l, t
&rang;
= 0; n 	= n&prime;. (J.4)
</p>
<p>Thus, the states form an orthonormal system (ONS), nl, t | n&prime;l, t
&rang;
= nn&prime; .
</p>
<p>In (J.1), we assume that the state |nl, t of a (spineless) quantum object is com-
pletely determined by specifying its position at a given time. This is a clear contrast to
</p>
<p>classical mechanics, where we need in general two quantities in order to characterize
</p>
<p>the state of a particle, such as its position and its velocity. Thus, we cannot assign a
</p>
<p>direction of motion to a quantum object in the state |nl, t.61 To avoid this dilemma,
we employ the superposition principle, which allows us to superpose different states
</p>
<p>so that the object is moving so to speak in all directions at once. We formulate this
</p>
<p>as follows: After a short time step  , the quantum object is either still in the same
</p>
<p>place or it has moved on to the next adjacent interval (for sufficiently small  , we
</p>
<p>can rule out a move to next-nearest neighboring positions; cf. Fig. J.2).
</p>
<p>60As usual, we denote the adjoint row vector by | .
61A snapshot of a pendulum also gives no information about its direction of motion. But while
</p>
<p>classically, a second picture taken a short time later can clarify this question (one then knows the
</p>
<p>initial velocity in addition to the initial position), in quantum mechanics a second image would
</p>
<p>produce (prepare) a new state.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix J: From Quantum Hopping to the Schr&ouml;dinger Equation 313
</p>
<p>In this way, we obtain the hopping equation
</p>
<p>|nl, t =  |nl, t +   +  |nl + l, t +   +  |nl &minus; l, t +   . (J.5)
</p>
<p>This equation can of course only &lsquo;work&rsquo; if the sum of states is in fact defined&mdash;in other
</p>
<p>words, if the superposition principle holds true; this is here the basic assumption. It
</p>
<p>leads necessarily to the appearance of probabilities. For if we assume that, at a fixed
</p>
<p>time, our quantum object can be measured by only one detector, the numbers  and
</p>
<p> are related to the probabilities of finding the quantum object at the position n or
</p>
<p>n + 1 or n &minus; 1.
Properties of the Coefficients
</p>
<p>The coefficients of the states in (J.5) do not depend on t or n, since we are considering
</p>
<p>free quantum objects. The coefficients of the two states |nl &plusmn; l, t +   must be the
same, since there is no preferred direction for a free quantum object.62 For  &rarr; 0,
 &rarr; 1 and  &rarr; 0 must hold, i.e. the quantum object remains at its initial position.
Rearranging and dividing by  , we find
</p>
<p>&minus; |nl, t+ + |nl, t

</p>
<p>= (&minus; 1)

</p>
<p>|nl, t +   + 

|nl + l, t +   + 
</p>
<p>
|nl &minus; l, t +   .
</p>
<p>(J.6)
</p>
<p>In order for this formulation to make sense, the limit  &rarr; 0 must be defined. We set
</p>
<p>&minus; 1 =  ;  =  (J.7)
</p>
<p>where  and  are complex numbers yet to be determined. Then we can take the
</p>
<p>limit  &rarr; 0:
&minus; d
</p>
<p>dt
|nl, t =  |nl, t +  |nl + l, t +  |nl &minus; l, t . (J.8)
</p>
<p>We take the derivative of (J.3) with respect to t
</p>
<p>0 = d
dt
</p>
<p>nl, t | nl, t =
(
</p>
<p>d
</p>
<p>dt
nl, t |
</p>
<p>)
|nl, t + nl, t |
</p>
<p>(
d
</p>
<p>dt
|nl, t
</p>
<p>)
(J.9)
</p>
<p>and insert (J.8):
</p>
<p>0 =
[
&lowast; nl, t | + &lowast; nl + l, t | + &lowast; |nl &minus; l, t|
</p>
<p>]
|nl, t
</p>
<p>+ nl, t |
[
 |nl, t +  |nl + l, t +  |nl &minus; l, t
</p>
<p>]
.
</p>
<p>(J.10)
</p>
<p>With (J.3) and (J.4), it follows that
</p>
<p>62Since objects with greater mass are less mobile,  must become smaller with increasing mass.
</p>
<p>Below, we show that  &sim; 1/m.</p>
<p/>
</div>
<div class="page"><p/>
<p>314 Appendix J: From Quantum Hopping to the Schr&ouml;dinger Equation
</p>
<p>&lowast; +  = 0 (J.11)
</p>
<p>This implies
</p>
<p> &isin; I or  = ia; a &isin; R (J.12)
</p>
<p>We obtain analogously
</p>
<p> = ib; b &isin; R (J.13)
</p>
<p>Thus, we have for the coefficients in (J.5)
</p>
<p> = 1 + ia ; a &isin; R
 = ib ; b &isin; R. (J.14)
</p>
<p>Schr&ouml;dinger Equation
</p>
<p>We now consider a superposition of all states of the form
</p>
<p>| =
&infin;&sum;
</p>
<p>n=&minus;&infin;

 (nl, t) |nl, t (J.15)
</p>
<p>where the coefficients 
 (nl, t) are the &lsquo;weights&rsquo; of the individual positions. Since
</p>
<p>we sum from &minus;&infin; to &infin;, we have
&infin;&sum;
</p>
<p>n=&minus;&infin;

 (nl, t) |nl, t =
</p>
<p>&infin;&sum;
</p>
<p>n=&minus;&infin;

 (nl, t +  ) |nl, t +   . (J.16)
</p>
<p>We insert the hopping equation (J.5) into this equation:
</p>
<p>&infin;&sum;
</p>
<p>n=&minus;&infin;

 (nl, t) [ |nl, t +   +  |nl + l, t +   +  |nl &minus; l, t +  ]
</p>
<p>=
&infin;&sum;
</p>
<p>n=&minus;&infin;

 (nl, t +  ) |nl, t +   .
</p>
<p>(J.17)
</p>
<p>Due to the orthonormality of the states |nl, t +   , it follows directly that:
</p>
<p>
 (nl, t +  ) = 
 (nl, t)+ 
 (nl &minus; l, t)+ 
 (nl + l, t) . (J.18)
</p>
<p>We want to transform this expression into the SEq. First we rearrange:
</p>
<p>
 (nl, t +  )&minus;
 (nl, t)

</p>
<p>= (&minus; 1)

</p>
<p>
 (nl, t)+ 


 (nl &minus; l, t)+ 
</p>
<p>

 (nl + l, t) .
</p>
<p>(J.19)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix J: From Quantum Hopping to the Schr&ouml;dinger Equation 315
</p>
<p>Substituting (J.14) leads in the limit  &rarr; 0 to63
</p>
<p>&part;
</p>
<p>&part;t

 (nl, t) = ia
 (nl, t)+ ib
 (nl &minus; l, t)+ ib
 (nl + l, t) . (J.20)
</p>
<p>Now we have to include the spatial dependence. It is
</p>
<p>
 (nl + l, t)+
 (nl &minus; l, t)
= [
 (nl+l, t)&minus;
 (nl, t)] &minus; [
 (nl, t)&minus;
 (nl &minus; l, t)] + 2
 (nl, t) , (J.21)
</p>
<p>and from this, it follows that:
</p>
<p>&part;
</p>
<p>&part;t

 (nl, t)
</p>
<p>= (ia + 2ib)
 (nl, t)+ ib {[
 (nl + l, t)&minus;
 (nl, t)] &minus; [
 (nl, t)&minus;
 (nl &minus; l, t)]} .
(J.22)
</p>
<p>This implies
</p>
<p>&part;
</p>
<p>&part;t

 (nl, t)
</p>
<p>= ibl2 {[
 (nl + l, t)&minus;
 (nl, t)] &minus; [
 (nl, t)&minus;
 (nl &minus; l, t)]}
l2
</p>
<p>+ (ia + 2ib)
 (nl, t) .
(J.23)
</p>
<p>In the following, we set a = &minus;2b, which is not necessary, but just serves to simplify
the discussion.64 We require b = Bl&minus;2 so that it makes sense to take the limit l &rarr; 0,
where B is a constant independent of l. Then it follows that
</p>
<p>&part;
</p>
<p>&part;t

 (nl, t) = i B {[
 (nl + l, t)&minus;
 (nl, t)] &minus; [
 (nl, t)&minus;
 (nl &minus; l, t)]}
</p>
<p>l2
,
</p>
<p>(J.24)
</p>
<p>which we may write, for l &rarr; 0 with x = nl, as the second derivative w.r.t the spatial
coordinate,
</p>
<p>&part;
</p>
<p>&part;t

 (x, t) = i B &part;
</p>
<p>2
</p>
<p>&part;x2

 (x, t) . (J.25)
</p>
<p>The precise value of B cannot be specified uniquely here; but at least we know that
</p>
<p>the equation of motion for a free quantum object must have the form
</p>
<p>i
&part;
</p>
<p>&part;t

 (x, t) = &minus;B &part;
</p>
<p>2
</p>
<p>&part;x2

 (x, t) . (J.26)
</p>
<p>To obtain the usual form of the SEq, we multiply by . Then we have for the units
</p>
<p>of B
</p>
<p>63We use &part; from the start, because we subsequently consider the spatial coordinates.
64For a 	= 2b, one obtains a contribution in the SEq which corresponds to a constant potential.</p>
<p/>
</div>
<div class="page"><p/>
<p>316 Appendix J: From Quantum Hopping to the Schr&ouml;dinger Equation
</p>
<p>[B] = m
2
</p>
<p>s
Js = Js
</p>
<p>kg
Js = (Js)
</p>
<p>2
</p>
<p>kg
=
</p>
<p>[
2
]
</p>
<p>[m]
, (J.27)
</p>
<p>and the final result reads
</p>
<p>i
&part;
</p>
<p>&part;t

 (x, t) = &minus;B 
</p>
<p>2
</p>
<p>2m
</p>
<p>&part;2
</p>
<p>&part;x2

 (x, t) . (J.28)
</p>
<p>The number B, which depends on the system of units chosen, cannot be determined
</p>
<p>here without additional information.
</p>
<p>A possible item of additional information would be for example that plane waves
</p>
<p>ei(kx&minus;t) must be solutions of the last equation (under nonrelativistic conditions). It
follows that
</p>
<p>i (&minus;i) = &minus;B 
2
</p>
<p>2m
</p>
<p>(
&minus;k2
</p>
<p>)
or B = 2m
</p>
<p>2k2
= 2m E
</p>
<p>p2
= 1. (J.29)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix K
</p>
<p>The Phase Shift at a Beam Splitter
</p>
<p>In Chap. 6, we used the fact that the relative phase shift between transmitted and
</p>
<p>reflected waves at a beam splitter is 90. This will be demonstrated in detail here.65
</p>
<p>We consider in Fig. K.1 a plane wave which is incident on a beam splitter with
</p>
<p>amplitude 1 and is split into a reflected wave with complex amplitude R = ei
and a transmitted wave with complex amplitude T = ei . The refractive index n
is assumed to be constant throughout the beam splitter. Since the beam splitter is
</p>
<p>symmetric, the same amplitude ratios would occur if the plane wave were incident
</p>
<p>from the right instead of from the left.
</p>
<p>The intensity is proportional to the absolute square of the amplitude. We assume
</p>
<p>that there are no absorption processes, and that the medium outside of the beam
</p>
<p>splitter is homogeneous. Then, due to energy conservation, it follows that:
</p>
<p>1 = R&lowast;R + T &lowast;T . (K.1)
</p>
<p>Now we consider a superposition of two incoming waves with amplitudes R&lowast;
</p>
<p>and T &lowast;; cf. Fig. K.2. These waves are split as shown in Fig. K.1, and superpose to
give two outgoing total waves. According to (K.1), the amplitude of the top right
</p>
<p>outgoing wave is 1. Thus, this wave already transports all of the incoming energy.
</p>
<p>Consequently, the amplitude of the top left outgoing wave must vanish:
</p>
<p>R&lowast;T + T &lowast;R = 0. (K.2)
</p>
<p>Since T &lowast;R is the complex conjugate of R&lowast;T , it follows from (K.2) that T &lowast;R is purely
imaginary.66 With R = ei and T = ei , this leads to
</p>
<p>cos ( &minus; ) = 0. (K.3)
</p>
<p>65See also: J. Pade and L. Polley, Phasenverschiebung am Strahlteiler, PhyDid 1/3 (2004) 39, (Phase
</p>
<p>shift at a beam splitter, PhyDid 1/3 (2004) 39).
66Or it is real and zero, but this case is obviously of no interest.
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>317</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>318 Appendix K: The Phase Shift at a Beam Splitter
</p>
<p>R
</p>
<p>1
</p>
<p>R
TT
</p>
<p>1
</p>
<p>Fig. K.1 Amplitudes at the beam splitter, laterally reversed on the right side
</p>
<p>Fig. K.2 Superposition of
</p>
<p>two incoming waves
R*T+T*R=0
</p>
<p>T*R*
</p>
<p>R*R+T*T=1
</p>
<p>This corresponds to a relative phase of 90 between the reflected and transmitted
waves (The choice of &minus;90, at this point possible in principle, can be excluded
because of other considerations. It is important above all that the amplitudes be
</p>
<p>perpendicular to each other.)
</p>
<p>We note without proof (see textbooks on experimental physics) that the phase
</p>
<p>shift between the incident and the reflected wave at a mirror is 180.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix L
</p>
<p>The Quantum Zeno Effect
</p>
<p>Reducing the time between successive measurements further and further, one ide-
</p>
<p>ally approaches continuous measurements. In this context, new quantum-mechanical
</p>
<p>phenomena such as the quantum Zeno effect 67 (QZE) can occur. This effect exhibits
</p>
<p>the fact that the more often one measures an unstable system, the more one prevents
</p>
<p>its decay. The effect has been known experimentally for about 20 years; a catchy
</p>
<p>formulation is &ldquo;a watched pot never boils&rdquo;.
</p>
<p>For some years now, the opposite effect has been discovered&mdash;the anti-quantum
</p>
<p>Zeno effect. Here, a more frequent observation of an unstable quantum system does
</p>
<p>not have a stabilizing action (as in the Zeno effect), but rather a destabilizing effect.
</p>
<p>The more often one raises the lid, the faster the water is boiling: &ldquo;boiling the pot
</p>
<p>by watching&rdquo;. And, recently, a third related phenomenon has been under discussion,
</p>
<p>the Hamlet effect . Here, measurements of a quantum system destroy the prognosis
</p>
<p>options to the point that no prediction is possible at all.68 &ldquo;Boiling or not boiling,
</p>
<p>that is the question&rdquo;.69
</p>
<p>In the following, we want to make some illustrative comments on the QZE and the
</p>
<p>anti-QZE in unstable systems, before we present a simple calculation for the QZE.
</p>
<p>67Zenon (or Zeno) of Elea (490&ndash;430 BC), Greek philosopher, was mainly concerned with the
</p>
<p>problem of the continuum. Perhaps best known is his paradox of the swift-footed Achilles and
</p>
<p>the tortoise: In a race, Achilles gives the tortoise a head start and therefore can never overtake
</p>
<p>it. Because, to achieve this, he must first catch up its lead. But during this time, the tortoise has
</p>
<p>gained a new (smaller) lead, which Achilles also has to catch up to. When he arrives at that point,
</p>
<p>the tortoise has again gained a (still smaller) lead and so on. From a present-day perspective, the
</p>
<p>argument misses the fact, among other things, that an infinite series can still have a finite sum.
68V. Pankovic, &lsquo;Quantum Hamlet effect&mdash;a new example&rsquo;, http://arxiv.org/PS_cache/arxiv/pdf/
</p>
<p>0908/0908.1301v2.pdf, (2009).
69Apropos Hamlet: &ldquo;A somewhat saucy philosopher, I think Hamlet, Prince of Denmark, said that
</p>
<p>there were many things in heaven and earth not dreamt of in our Compendiis. If that simple-minded
</p>
<p>man, who was not in his right mind as is well known, made digs at our Compendia of physics, so
</p>
<p>one can confidently answer him, &lsquo;Well, but instead there are also many things in our Compendiis
</p>
<p>which do not exist, neither in heaven nor on earth&rdquo;&rsquo;. Georg Christoph Lichtenberg, Scrap Books,
</p>
<p>Vol. L (155).
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>319</p>
<p/>
<div class="annotation"><a href="http://arxiv.org/PS_cache/arxiv/pdf/0908/0908.1301v2.pdf">http://arxiv.org/PS_cache/arxiv/pdf/0908/0908.1301v2.pdf</a></div>
<div class="annotation"><a href="http://arxiv.org/PS_cache/arxiv/pdf/0908/0908.1301v2.pdf">http://arxiv.org/PS_cache/arxiv/pdf/0908/0908.1301v2.pdf</a></div>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>320 Appendix L: The Quantum Zeno Effect
</p>
<p>Then we consider how we can improve the efficiency of an interaction-free quantum
</p>
<p>measurement using the QZE.70
</p>
<p>L.1 Unstable Systems
</p>
<p>We want to provide a conceptual idea here, without going into formal details.
</p>
<p>An unstable state evolves eventually into a linear superposition of states, one of
</p>
<p>which can be observed in a measurement. The decay rate depends on several factors,
</p>
<p>among others on the energy spectrum of the final states (also called reservoir states)
</p>
<p>to which the unstable state is coupled.
</p>
<p>Measurements which are performed at the frequency cause an energy uncertainty
</p>
<p>&sim; h, according to the energy-time uncertainty principle, which affects the range
of accessible reservoir states and thus the decay rate. If the energy uncertainty due
</p>
<p>to successive measurements is large compared with both the width of the reservoir
</p>
<p>spectrum and the energy separation between the unstable state and the average energy
</p>
<p>of the reservoir, then the QZE should occur. If, on the other hand, the energy spread
</p>
<p>is initially comparatively small, it increases with , and therefore the number of
</p>
<p>attainable reservoir states into which transitions can occur also increases. In this
</p>
<p>case, the anti-QZE should occur first.
</p>
<p>Indeed, this has been observed, for example in an experiment71 in which sodium
</p>
<p>atoms are trapped in an optical standing wave. The atoms can escape this potential by
</p>
<p>the tunneling effect. The experimental result was that measurement intervals of 1 &micro;s
</p>
<p>reduced the tunneling (i.e. the decay), while measurement intervals of 5 &micro;s enhanced
</p>
<p>the tunneling.
</p>
<p>L.2 Simple Model Calculation
</p>
<p>We want to illustrate the basic idea of the QZE by a simple calculation. We start from
</p>
<p>the SEq in the form:
</p>
<p>| (t) = e&minus;i Ht/ | (0) , (L.1)
</p>
<p>as well with an observable A whose spectrum is discrete and not degenerate for
</p>
<p>simplicity. Thus we may write
</p>
<p>A =
&sum;
</p>
<p>m
</p>
<p>am |m m | . (L.2)
</p>
<p>70We mention that the quantum Zeno effect may also be used to generate entanglement as well to
</p>
<p>suppress decoherence (cf. Chaps. 20 and 24, Vol. 2).
71 M.C. Fischer et al., Observation of the Quantum Zeno and Anti-Zeno Effects in an Unstable
</p>
<p>System, Phys. Rev. Lett. 87(4) (2001), 040402.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix L: The Quantum Zeno Effect 321
</p>
<p>The scenario is now that we carry out repeated measurements of A at fixed time
</p>
<p>intervals  ; between the measurements, the SEq determines the evolution of the
</p>
<p>state.
</p>
<p>The initial state is | (0) = |n. We ask for the probability that after N measure-
ments, the system is still in the initial state. For sufficiently small t , it holds because
</p>
<p>of (L.1) that
</p>
<p>| (t) =
[
</p>
<p>1 &minus; i Ht

</p>
<p>&minus; H
2t2
</p>
<p>22
+ O
</p>
<p>(
t3
)]
</p>
<p>|n . (L.3)
</p>
<p>The first measurement takes place at t =  . The probability pn of measuring the
value an is given by
</p>
<p>pn ( ) = |n | ( )|2 =
n|
</p>
<p>[
1 &minus; i H
</p>
<p>
&minus; H
</p>
<p>2 2
</p>
<p>22
+ O
</p>
<p>(
 3
)]
</p>
<p>|n

2
</p>
<p>. (L.4)
</p>
<p>Solving the brackets and rearranging yields
</p>
<p>pn ( ) = 1 &minus;
 2
</p>
<p>2
(H)2n + O
</p>
<p>(
 3
)
</p>
<p>(L.5)
</p>
<p>with the energy uncertainty
</p>
<p>(H)2n = n| H 2 |n &minus; n| H |n2 . (L.6)
</p>
<p>In the context of these considerations, the time tZ = (H)n is called the Zeno time.
The quantity pn ( ) is the probability that the system is still in the initial state
</p>
<p>|n after the time  . After N measurements, the total time is T = N ; it follows
for the probability (henceforth, we omit the terms of higher order):
</p>
<p>pn (T ) &asymp;
[
</p>
<p>1 &minus; 
2
</p>
<p>2
(H)2n
</p>
<p>]N
=
</p>
<p>[
1 &minus; T
</p>
<p>2 N
(H)2n
</p>
<p>]N
. (L.7)
</p>
<p>If we now fix T and let N become very large (i.e. the measurement intervals become
</p>
<p>shorter and approximate more and more closely a continuous measurement72), then
</p>
<p>we can use the definition of the exponential function
[
1 + x
</p>
<p>N
</p>
<p>]N &rarr;
N&rarr;&infin;
</p>
<p>ex and obtain
</p>
<p>pn (T ) &asymp; exp
(
&minus; (H)
</p>
<p>2
n
</p>
<p>2
T
</p>
<p>)
&rarr; 1
&rarr;0
</p>
<p>. (L.8)
</p>
<p>Hence, the system stays in the initial state in the limit of a continuous measurement:
</p>
<p>a watched pot never boils.
</p>
<p>72This is of course an idealization. The measurement process always has a certain finite duration,
</p>
<p>even if it can possibly be made very short compared to the relevant time constants of the system.</p>
<p/>
</div>
<div class="page"><p/>
<p>322 Appendix L: The Quantum Zeno Effect
</p>
<p>The formal reason for this is that, according to (L.5), the probability of leaving
</p>
<p>the initial state is given by
</p>
<p>1 &minus; pn ( ) &sim;  2, (L.9)
</p>
<p>while the number of measurements increases &sim; 1

</p>
<p>. Consequently, the state reduction
</p>
<p>caused by the successive measurements is faster than possible transitions into other
</p>
<p>states, provided that  is sufficiently small.
</p>
<p>L.3 Interaction-Free Quantum Measurement
</p>
<p>We consider an interaction-free quantum measurement making use of the quantum
</p>
<p>Zeno effect. Here, the scenario is somewhat different, for we do not consider unstable
</p>
<p>states, but rather we want to force a system from an initial state into a different state
</p>
<p>by repeated measurements, and this should be all the &lsquo;smoother&rsquo;, the more often one
</p>
<p>repeats the measurements.73
</p>
<p>The basic idea74 is quite simple: We let light pass through N polarization rotators,
</p>
<p>each of which rotates the plane of polarization of the incident state by 
2N
</p>
<p>. Added
</p>
<p>together, the N rotators, connected in series, turn the state by 
2
</p>
<p>, so that e.g. an
</p>
<p>initially horizontally polarized state becomes vertically polarized. Now we add a
</p>
<p>horizontal analyzer behind each rotator. The probability that a photon passes one of
</p>
<p>these polarizers is then given by p = cos2
(
</p>
<p>
2N
</p>
<p>)
, and the probability of passing all N
</p>
<p>polarizers is pN = cos2N
(
</p>
<p>
2N
</p>
<p>)
; for sufficiently large N , we thus have pN &asymp; 1&minus; 
</p>
<p>2
</p>
<p>4N
.
</p>
<p>Accordingly, the absorption probability is given by 
2
</p>
<p>4N
.
</p>
<p>As to the interaction-free quantum measurement, we have seen in Chap. 6 that in
</p>
<p>one-fourth of the trials, the &lsquo;bomb test&rsquo; works without the bomb blowing up. This
</p>
<p>percentage can be increased considerably by using the setup shown schematically in
</p>
<p>Fig. L.1. The &lsquo;inner&rsquo; part, i.e. the arrangement of two mirrors and polarizing beam
</p>
<p>splitters (PBS), is called a polarization Mach&ndash;Zehnder interferometer (PMZI). The
</p>
<p>setup (L.1) allows, in principle, the detection of an object in the beam path with the
</p>
<p>probability 1 in an &lsquo;interaction-free&rsquo; manner.
</p>
<p>The basis states are not, as in Chap. 6, the horizontal and vertical directions of
</p>
<p>propagation (i.e. |H and |V ), but instead the horizontal and vertical polarization
states of the photons, |h and |v; the propagation direction does not matter.
</p>
<p>At the beginning, the lower left mirror is switched open. It is closed after the
</p>
<p>photon has entered the setup. The photon can then make N rounds, after which the
</p>
<p>lower-right mirror is opened and the photon is directed out to further analysis.
</p>
<p>On each iteration, first the polarizer is passed, whereby the plane of polarization
</p>
<p>is rotated in each passage by 
2N
</p>
<p>. In the PBS, the horizontally-polarized component
</p>
<p>is transmitted and the vertically-polarized component is reflected.
</p>
<p>73Actually it is therefore more like the anti-Zeno effect, but the name &lsquo;Zeno effect&rsquo; has been adopted
</p>
<p>in this context.
74This effect can be detected also in classical optics; the quantum-mechanical aspect lies in the fact
</p>
<p>that it is considered below for a single photon.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix L: The Quantum Zeno Effect 323
</p>
<p>Fig. L.1 Setup for the zeno
</p>
<p>effect. mM = switchable
</p>
<p>mirror, M = mirror, P =
</p>
<p>polarization rotator, pBS =
</p>
<p>polarizing beam splitter, mB
</p>
<p>= movable blocker; red =
</p>
<p>vertically polarized
</p>
<p>component, blue =
</p>
<p>horizontally polarized
</p>
<p>component
</p>
<p>mMmM
</p>
<p>mB
</p>
<p>M
</p>
<p>P
</p>
<p>MM
</p>
<p>pBS
</p>
<p>pBS
</p>
<p>We start from a purely horizontally-polarized initial state. If no object is in the
</p>
<p>PMZI, the plane of polarization is finally vertical due to N rotations of the polarization
</p>
<p>plane by 
2N
</p>
<p>; if there is a obstacle, the interference is disturbed and the final state has
</p>
<p>e.g. only a horizontal component.
</p>
<p>In this case also, the formal description is quite simple. The basis vectors (linear
</p>
<p>horizontal and vertical polarization), are as usual75
</p>
<p>|h =
(
</p>
<p>1
</p>
<p>0
</p>
<p>)
, |v =
</p>
<p>(
0
</p>
<p>1
</p>
<p>)
(L.10)
</p>
<p>The polarizer (rotation of the polarization plane by the angle ) can be represented
</p>
<p>by (
a
</p>
<p>b
</p>
<p>)
&rarr;
</p>
<p>(
cos &minus; sin
sin cos
</p>
<p>)(
a
</p>
<p>b
</p>
<p>)
(L.11)
</p>
<p>and the combined action of PBS and obstacle which blocks the vertical component
</p>
<p>in the PMZI is given by
</p>
<p>(
a
</p>
<p>b
</p>
<p>)
&rarr;
</p>
<p>(
1 0
</p>
<p>0 
</p>
<p>)(
a
</p>
<p>b
</p>
<p>)
,  &isin; C (L.12)
</p>
<p>with
 = 1 : without obstacle
 = 0 : with obstacle. (L.13)
</p>
<p>We summarize the effects of polarizer, PBS and obstacle (the mirrors need not be
</p>
<p>considered because they produce the same phase shift for both polarization compo-
</p>
<p>nents), and obtain for one iteration
</p>
<p>75For simplicity we dispense here with the distinction between &sim;= and =.</p>
<p/>
</div>
<div class="page"><p/>
<p>324 Appendix L: The Quantum Zeno Effect
</p>
<p>(
a
</p>
<p>b
</p>
<p>)
&rarr;
</p>
<p>(
1 0
</p>
<p>0 
</p>
<p>)(
cos &minus; sin
sin cos
</p>
<p>)(
a
</p>
<p>b
</p>
<p>)
</p>
<p>=
(
</p>
<p>cos &minus; sin
 sin  cos
</p>
<p>)(
a
</p>
<p>b
</p>
<p>)
.
</p>
<p>(L.14)
</p>
<p>For N iterations, each with the angle  = 
2N
</p>
<p>, we find
</p>
<p>(
a
</p>
<p>b
</p>
<p>)
&rarr;
</p>
<p>(
cos 
</p>
<p>2N
&minus; sin 
</p>
<p>2N
</p>
<p> sin 
2N
</p>
<p> cos 
2N
</p>
<p>)N (
a
</p>
<p>b
</p>
<p>)
= M(N , )
</p>
<p>(
a
</p>
<p>b
</p>
<p>)
. (L.15)
</p>
<p>The matrix M(N , ) can readily be calculated for the special cases of  = 1 and
 = 0.76 It follows that
</p>
<p>M(N ,  = 1) =
(
</p>
<p>cos 
2
&minus; sin 
</p>
<p>2
</p>
<p>sin 
2
</p>
<p>cos 
2
</p>
<p>)
=
</p>
<p>(
0 &minus;1
1 0
</p>
<p>)
(L.16)
</p>
<p>and
</p>
<p>M(N ,  = 0) =
(
</p>
<p>cos

</p>
<p>2N
</p>
<p>)N&minus;1 ( cos 
2N
</p>
<p>&minus; sin 
2N
</p>
<p>0 0
</p>
<p>)
. (L.17)
</p>
<p>For a purely horizontally-polarized initial state, we have in the absence of an obstacle
</p>
<p>( = 1) (
1
</p>
<p>0
</p>
<p>)
&rarr; M(N ,  = 1)
</p>
<p>(
1
</p>
<p>0
</p>
<p>)
=
</p>
<p>(
0
</p>
<p>1
</p>
<p>)
. (L.18)
</p>
<p>Thus, the original horizontal polarization is completely converted into vertical polar-
</p>
<p>ization.
</p>
<p>With an obstacle ( = 0), we have instead:
(
</p>
<p>1
</p>
<p>0
</p>
<p>)
&rarr; M(N ,  = 0)
</p>
<p>(
1
</p>
<p>0
</p>
<p>)
=
</p>
<p>(
cos
</p>
<p>
</p>
<p>2N
</p>
<p>)N ( 1
0
</p>
<p>)
. (L.19)
</p>
<p>Thus, the original horizontal polarization is completely conserved. For sufficiently
</p>
<p>large N , we have
(
cos 
</p>
<p>2N
</p>
<p>)2N &asymp; 1 &minus; 2
8N
</p>
<p>. The term 
2
</p>
<p>8N
describes the &lsquo;loss&rsquo;, i.e. the
</p>
<p>absorption by the obstacle; this part can in principle be made arbitrarily small for
</p>
<p>N &rarr; &infin;.
In summary: The experimental arrangement makes it possible to determine the
</p>
<p>presence of an obstacle in an &lsquo;interaction-free&rsquo; manner, substantially more efficiently
</p>
<p>than with the Mach&ndash;Zehnder setup from Chap. 6. There, the &lsquo;bomb test&rsquo; worked in
</p>
<p>25% of the cases; here, the percentage is
(
cos 
</p>
<p>2N
</p>
<p>)N &asymp; 1 &minus; 2
8N
</p>
<p>.
</p>
<p>76Partly transparent obstacles ( 	= 0, 1) are discussed in J. Pade and L. Polley, &lsquo;Wechselwirkungs-
freie Quantenmessung&rsquo;, Physik in der Schule 38/5 (2000) 343, (&lsquo;interaction-free quantum measure-
</p>
<p>ment&rsquo;, Physics in School 38/5 (2000) 343).</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix L: The Quantum Zeno Effect 325
</p>
<p>In practice, of course, N cannot be made arbitrarily large due to various experi-
</p>
<p>mental difficulties (e.g. the components are not ideal, there is some absorption, etc.).
</p>
<p>However, numbers such as N &asymp; 15 can be attained.77
</p>
<p>77P.G. Kwiat et al., High-efficiency quantum interrogation measurements via the quantum Zeno
</p>
<p>effect, http://de.arxiv.org/abs/quant-ph/9909083.</p>
<p/>
<div class="annotation"><a href="http://de.arxiv.org/abs/quant-ph/9909083">http://de.arxiv.org/abs/quant-ph/9909083</a></div>
</div>
<div class="page"><p/>
<p>Appendix M
</p>
<p>Delayed Choice and the Quantum Eraser
</p>
<p>The experiments discussed in this appendix are all based on the Mach&ndash;Zehnder
</p>
<p>interferometer (MZI). They show that the experimental setup or the observation,
</p>
<p>respectively, will decide whether a quantum object will behave (mainly) as a particle
</p>
<p>or (mainly) as a wave. Here, the so-called which-way information is crucial: if one
</p>
<p>can distinguish and identify the paths taken, then the photons behave like particles
</p>
<p>(no interference); if the paths are indistinguishable, then their behavior is like that
</p>
<p>of waves (interference). The consequences of the experiments lead right up to the
</p>
<p>question of whether we must also take into account a time-reversed effect of events.
</p>
<p>Since the experiments are relatively simple, they are found more and more frequently
</p>
<p>in textbooks and curricula for physics at the school level.78
</p>
<p>M.1 Delayed Choice Experiments
</p>
<p>The term &lsquo;delayed-choice experiment&rsquo; (or &lsquo;experiment with delayed decision&rsquo;)
</p>
<p>denotes an experimental setup where it is decided whether one will allow
</p>
<p>(self-)interference or not, or which variables are measured, only in the course of the
</p>
<p>experiment. Proposed in 1978 as a thought experiment by John Archibald Wheeler,
</p>
<p>the effect has been confirmed experimentally in the meantime, including a measure-
</p>
<p>ment based on a setup similar to a Mach&ndash;Zehnder interferometer.79
</p>
<p>78See e.g. the many hits of an internet search using the keywords &lsquo;quantum eraser&rsquo; and &lsquo;school&rsquo;.
79See V. Jacques et al., &lsquo;Experimental Realization of Wheeler&rsquo;s Delayed-choice Gedanken Experi-
</p>
<p>ment?, Science 315, 966 (2007), and references therein. In the experiment cited, polarization beam
</p>
<p>splitters are used instead of simple beam splitters. Recently, a delayed-choice experiment was per-
</p>
<p>formed with single photons, where the counters are also quantum objects and not (as usual) classical
</p>
<p>detecting devices; see Jian-Shun Tang et al., &lsquo;Realization of Wheeler&rsquo;s delayed-choice quantum
</p>
<p>experiment&rsquo;, Nature Photonics 6, 600&ndash;604 (2012), https://doi.org/10.1038/nphoton.2012.179.
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>327</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1038/nphoton.2012.179.">https://doi.org/10.1038/nphoton.2012.179.</a></div>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>328 Appendix M: Delayed Choice and the Quantum Eraser
</p>
<p>Fig. M.1 Delayed
</p>
<p>choice&mdash;removing the
</p>
<p>second beam splitter
</p>
<p>D1
</p>
<p>M
</p>
<p>M BS2
</p>
<p>BS1
</p>
<p>D2
</p>
<p>M.1.1 Setup 1
</p>
<p>The basic idea: As in Chap. 6, we have a MZI setup through which a single photon
</p>
<p>passes. While the photon is in the apparatus, the second beam splitter BS2 can be
</p>
<p>removed or inserted, as shown in Fig. M.1, and this in such a way that an &lsquo;information&rsquo;
</p>
<p>to the photon would have to be sent with a superluminal velocity.80
</p>
<p>There are four ways to perform the experiment, which we denote by M1-M4. In
</p>
<p>the first two, the beam splitter BS2 is inserted or removed at the start and remains so
</p>
<p>during the entire experiment
</p>
<p>M1: The photon is incident; BS2 remains inserted. Then, because of (self-)
</p>
<p>interference, only detector D1 is activated (D2 remains silent), and we cannot say
</p>
<p>which one of the two paths the photon has taken (wave nature).
</p>
<p>M2: The photon is incident; BS2 remains removed. Then, with 50% probability,
</p>
<p>either D1 or D2 is activated. There is no interference and we can say clearly which
</p>
<p>path the photon has taken (particle nature).
</p>
<p>The next two methods involve removing or inserting the second beam splitter
</p>
<p>BS2, after the photon has passed the first beam splitter (and possibly the mirror);
</p>
<p>these are the delayed decisions.81
</p>
<p>M3: The photon is incident; BS2 is inserted. After the photon has passed BS1 and
</p>
<p>M, one removes BS2. With 50% probability, D1 or D2 is activated (particle nature).
</p>
<p>M4: The photon is incident; BS2 is removed. After the photon has passed BS1
</p>
<p>and M, one inserts BS2. Only D1 is activated, D2 remains silent (wave nature).
</p>
<p>80In theory, we can assume identical optical paths, perfect 90 angles, etc.; as shown in Chap. 6, this
leads to the result that with BS2 inserted, detector D1 is always activated and D2 never. In a real
</p>
<p>experiment, these ideal conditions are not found; the path difference therefore depends e.g. on the
</p>
<p>angle. When using laser light, with BS2 inserted we in fact obtain interference fringes on screens
</p>
<p>which are placed at the positions of the detectors; if BS2 is removed, we see only a &lsquo;bright spot&rsquo; on
</p>
<p>the screen.
81The removal and insertion of BS2 can be delayed arbitrarily&mdash;it just has to occur before the photon
</p>
<p>arrives at the position of BS2.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix M: Delayed Choice and the Quantum Eraser 329
</p>
<p>It is interesting to consider M3 (or M4) more closely. The photon enters the MZI
</p>
<p>and passes BS1 and M, then BS2 is inserted. Consequently, as in M1, the photon has
</p>
<p>to explore both paths, since otherwise interference could not occur in principle (the
</p>
<p>photon cannot &lsquo;know&rsquo; that we will remove BS2 a moment later). Therefore, it has to
</p>
<p>be in a coherent superposition state (wave nature).
</p>
<p>Now we remove BS2, immediately before the photon passes this point. It will then
</p>
<p>end up in one of the two detectors, and we can tell which way it has gone (particle
</p>
<p>nature). Thus, the photon cannot be in a coherent superposition state&mdash;contrary to
</p>
<p>what we just said. From a classical point of view, we can resolve the conflict only
</p>
<p>if we assume that the photon, when entering BS1, already knows whether BS2 will
</p>
<p>remain or will be removed&mdash;i.e. it had to know the future. The delayed choice seems
</p>
<p>to cause an effect on the events in the past. If this interpretation is wrong, where
</p>
<p>should we look for the error?
</p>
<p>The usual answer is that we cannot say anything about how the photon propa-
</p>
<p>gates in the MZI (&lsquo;one path&rsquo; or &lsquo;two-path&rsquo;) before an appropriate measurement is
</p>
<p>undertaken. Prior to the measurement, there is nothing that can be associated with
</p>
<p>such a which-way statement. Thus, the question of which path the photon takes is
</p>
<p>not meaningful before a measurement (and this also applies to the above argument,
</p>
<p>insofar as it is based on the observation of the paths taken).82 There are questions
</p>
<p>that simply are not meaningful, just for the reason that we cannot answer them in
</p>
<p>principle.
</p>
<p>On the other hand, there are voices that propose considering a time-reversed effect
</p>
<p>of events.83 Actually, the fundamental laws of physics are all symmetric with respect
</p>
<p>to time reversal, and do not reflect the time-asymmetric notion of cause and effect.
</p>
<p>However this discussion may turn out, we see that a photon is not just a particle
</p>
<p>or a wave, but something else (i.e. a quantum object), which can be forced to behave
</p>
<p>like a particle or like a wave only by performing a measurement.
</p>
<p>M.1.2 Setup 2
</p>
<p>A variant of the experimental setup, as shown in Fig. M.2, leaves both beam
</p>
<p>splitters in place, but one can introduce additional detectors (D3 and D4) into the
</p>
<p>paths of the photon streams. For instance, if we insert D3 (D4 remains outside), we
</p>
<p>have the information about which path the photon has taken.
</p>
<p>The delayed choice here is to bring D3 and/or D4 into the paths (or to remove
</p>
<p>them) after the photon has passed the first beam splitter and the mirrors. The argument
</p>
<p>is analogous to that used in setup 1.
</p>
<p>82&ldquo;The past has no existence except as it is recorded in the present.&rdquo; (J.A. Wheeler, in Mathematical
</p>
<p>Foundations of Quantum Theory (ed A.R. Marlow), 9&ndash;48 (Academic, New York, 1978).
83See also the discussion in Chap. 27, Vol. 2 on locality and reality in quantum mechanics.</p>
<p/>
</div>
<div class="page"><p/>
<p>330 Appendix M: Delayed Choice and the Quantum Eraser
</p>
<p>Fig. M.2 Delayed
</p>
<p>Choice&mdash;inserting additional
</p>
<p>detectors D2
</p>
<p>D1
</p>
<p>BS
</p>
<p>M
</p>
<p>M
</p>
<p>BS
</p>
<p>D4
</p>
<p>D3
</p>
<p>M.2 The Quantum Eraser
</p>
<p>A quantum eraser can generally be understood as an experimental setup with which
</p>
<p>one can delete information (i.e. &lsquo;erase&rsquo; it) about the course of an experiment. Specif-
</p>
<p>ically, this usually concerns the possibility of restoring the ability to interfere (in
</p>
<p>retrospect, so to speak) by the destruction of information. A quite simple example
</p>
<p>is an MZI with a beam splitter (for example only BS1) as shown in Fig. M.1, which
</p>
<p>delivers the information about which path the photon has taken through the appa-
</p>
<p>ratus. If we insert a second beam splitter BS2, we lose this information&mdash;it will be
</p>
<p>rubbed out, so to speak.
</p>
<p>A slightly more elaborate setup is shown in Fig. M.3. Is an ideal MZI with fixed
</p>
<p>beam splitters into which adjustable polarizers are inserted. Initially, the polarizers
</p>
<p>P3 and P4 are not in the paths. The state entering the MZI is horizontally polarized.
</p>
<p>If the polarizers are all set to zero, we have the usual finding that only D1 and not D2
</p>
<p>is activated (interference, wave character, no path information). If we rotate P1 to
</p>
<p>Fig. M.3 Quantum eraser
</p>
<p>P1
</p>
<p>BS
</p>
<p>BS M
</p>
<p>M
</p>
<p>D2
</p>
<p>D1
</p>
<p>P2
</p>
<p>P4
</p>
<p>P3</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix M: Delayed Choice and the Quantum Eraser 331
</p>
<p>+45 and P2 by &minus;45, we imprint on the photons a which-way information; there is
no more interference and D1 and D2 are activated with 50% probabilities (whereby,
</p>
<p>of course, due to the setting of the polarizers, the number of transmitted photons or
</p>
<p>the intensity decreases to half its original value). Now we insert P3 and P4 into the
</p>
<p>paths, say with the setting 0. Hence, only one half of the &plusmn;45-polarized photons
can pass detectors D3 and D4&mdash;but these photons are now capable of interference.
</p>
<p>Accordingly, we have again the finding that D1 is activated, while D2 is silent.
</p>
<p>In effect, from a classical point of view, we have therefore deleted the which-way
</p>
<p>information by means of the settings of P3 and P4, even though it was already present.
</p>
<p>One can make a delayed choice, of course, by setting P3 and P4 only if the photon
</p>
<p>has passed the first beam splitter. Here also, one can delete the path information.
</p>
<p>Thus, this experiment again indicates that it is not meaningful to speak about physical
</p>
<p>reality without performing a measurement.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix N
</p>
<p>The Equation of Continuity
</p>
<p>In the following, the derivation of the equation of continuity is briefly recapitulated,
</p>
<p>using as an example the mass density.
</p>
<p>Mass and mass density are connected by the differential relation dm = dV or
its integral formulation
</p>
<p>M =
&int;
</p>
<p>G
</p>
<p>dV, (N.1)
</p>
<p>where the integration is over a certain closed (fixed) volume G. We want to exclude
</p>
<p>all processes in the following which destroy or create mass; the total mass in G can
</p>
<p>thus change only by mass transport through the surface of G. This formulation relates
</p>
<p>to the region G as a whole, and is therefore a global or integral principle. The local
</p>
<p>(that is, for a particular space-time point) or differential formulation is given by the
</p>
<p>continuity equation, which we will now derive.
</p>
<p>The change of the mass with time is given by
</p>
<p>d
</p>
<p>dt
M = d
</p>
<p>dt
</p>
<p>&int;
</p>
<p>G
</p>
<p>dV =
&int;
</p>
<p>G
</p>
<p>&part;
</p>
<p>&part;t
dV . (N.2)
</p>
<p>According to our assumption, it can occur only by means of mass transport through
</p>
<p>the surface &part;G of the volume G. With the usual definition of the current density j as
</p>
<p>j =v (magnitude of current density = mass flow per unit time through a unit area),
we obtain &int;
</p>
<p>G
</p>
<p>&part;
</p>
<p>&part;t
dV = &minus;
</p>
<p>&int;
</p>
<p>&part;G
</p>
<p>j &middot; dA. (N.3)
</p>
<p>Here, dA is an oriented surface element (oriented to the outside in the case of a closed
</p>
<p>volume); the minus sign indicates that the mass within the volume G decreases if there
</p>
<p>is an outward flow from G. This equation can be regarded as an integral formulation
</p>
<p>of the continuity equation. To arrive at the differential formulation, we transform
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>333</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>334 Appendix N: The Equation of Continuity
</p>
<p>the surface integral into a volume integral by using the Gaussian integral theorem
</p>
<p>(Appendix D, Vol. 1):
</p>
<p>&int;
</p>
<p>G
</p>
<p>&part;
</p>
<p>&part;t
dV = &minus;
</p>
<p>&int;
</p>
<p>&part;G
</p>
<p>j &middot; dA = &minus;
&int;
</p>
<p>G
</p>
<p>divj dV &equiv; &minus;
&int;
</p>
<p>G
</p>
<p>&nabla; &middot; j dV
</p>
<p>or
&int;
G
</p>
<p>&part;
&part;t
</p>
<p>dV = &minus;
&int;
G
</p>
<p>&nabla; &middot; j dV
(N.4)
</p>
<p>Since the last equation holds for any arbitrary volume G, the integrands must be
</p>
<p>equal and it follows that:
&part;
</p>
<p>&part;t
+&nabla; &middot; j = 0 (N.5)
</p>
<p>with  =  (r, t) and j = j (r, t). This is the differential formulation of the conser-
vation of mass, called the continuity equation. Moreover, this equation is not only
</p>
<p>valid for the mass density, but also e.g. for the charge density and any other density
</p>
<p>subject to an integral conservation law.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix O
</p>
<p>Variance, Expectation Values
</p>
<p>O.1 Variance, Moments
</p>
<p>If one measures a quantity x several times (e.g. the duration of an oscillation, the
</p>
<p>lifetime of a radioactive nucleus, etc.), one generally obtains different values xn with
</p>
<p>a relative frequency of occurrence84 fn . The mean value is then given by
</p>
<p>x = x =
&sum;
</p>
<p>n
</p>
<p>fn xn. (O.1)
</p>
<p>The mean value tells us nothing about how much the data spread; very different
</p>
<p>sets of data can yield the same mean value; cf. Fig. O.1.
</p>
<p>One might think of taking the sum of the deviations of the data points from the
</p>
<p>mean value as a measure of the dispersion, i.e.
&sum;
</p>
<p>n fn |xn &minus; x|. This idea is quite
correct in itself, but has certain disadvantages. Thus, one sums instead first the squares
</p>
<p>of the deviations and takes the square root afterwards:
</p>
<p>2 =
&sum;
</p>
<p>n
</p>
<p>fn (xn &minus; x)2 ;  =
&radic;&sum;
</p>
<p>n
</p>
<p>fn (xn &minus; x)2. (O.2)
</p>
<p>We cast this in a more pleasing form:
</p>
<p>2 =
&sum;
</p>
<p>n
</p>
<p>fn (xn &minus; x)2 =
&sum;
</p>
<p>n
</p>
<p>fn
[
x2n &minus; 2xn x + x2
</p>
<p>]
</p>
<p>=
&sum;
</p>
<p>n
</p>
<p>fn x
2
n &minus; 2 x
</p>
<p>&sum;
</p>
<p>n
</p>
<p>fn xn + x2
&sum;
</p>
<p>n
</p>
<p>fn
</p>
<p>=
&lang;
x2
&rang;
&minus; 2 x2 + x2 =
</p>
<p>&lang;
x2
&rang;
&minus; x2 .
</p>
<p>(O.3)
</p>
<p>84Also called weight. The reliability of data can be characterized by the weights; a lower weight is
</p>
<p>allocated to less reliable data than to reliable data. It must hold that
&sum;
</p>
<p>n
fn = 1.
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>335</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>336 Appendix O: Variance, Expectation Values
</p>
<p>Fig. O.1 Data with different scatter but with the same mean value
</p>
<p>The quantity 2 is called the variance;  is the standard deviation (or mean square
</p>
<p>deviation, root deviation, dispersion).
</p>
<p>Generalizing these concepts, one defines the Nth moment by
</p>
<p>&lang;
x N
</p>
<p>&rang;
=
</p>
<p>&sum;
</p>
<p>n
</p>
<p>fn x
N
n , (O.4)
</p>
<p>and the central Nth moment as
&lang;
(x &minus; x)N
</p>
<p>&rang;
.
</p>
<p>O.2 Expectation Value, Mean Value
</p>
<p>These two terms are often used synonymously (not only in quantum mechanics),
</p>
<p>but strictly speaking, we should note that the mean value relates to a record of past
</p>
<p>data and is formulated in terms of relative frequencies, while the expectation value
</p>
<p>is meant to predict future occurrences in terms of probabilities. As an illustrative
</p>
<p>example, consider random dice throws, repeated 18 times in a (hypothetical) test
</p>
<p>series.:
</p>
<p>Spots an 1 2 3 4 5 6
</p>
<p>Probability wn
1
6
</p>
<p>1
6
</p>
<p>1
6
</p>
<p>1
6
</p>
<p>1
6
</p>
<p>1
6
</p>
<p>Number of throws 3 4 2 2 3 4
</p>
<p>Relative frequency fn
1
6
</p>
<p>2
9
</p>
<p>1
9
</p>
<p>1
9
</p>
<p>1
6
</p>
<p>2
9
</p>
<p>Thus we obtain the expectation value E =
&sum;6
</p>
<p>n=1
wnan = 3.5 and the mean
</p>
<p>value M =
&sum;6
</p>
<p>n=1
fnan = 3.56.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix O: Variance, Expectation Values 337
</p>
<p>O.3 Discrete and Continuous85
</p>
<p>In the discrete case, the mean value is given by
</p>
<p>x =
&sum;
</p>
<p>n
</p>
<p>fn xn. (O.5)
</p>
<p>In the continuous case, the summation becomes as usual an integration, and we find
</p>
<p>x =
&int;
</p>
<p> (x) x dx (O.6)
</p>
<p>with the density function  (x). For the variance, it follows accordingly:
</p>
<p>2 =
&lang;
x2
&rang;
&minus; x2 =
</p>
<p>&int;
 (x) x2 dx &minus;
</p>
<p>(&int;
 (x) .x dx
</p>
<p>)2
(O.7)
</p>
<p>In a physical terms, this concept is familiar from the first semester on. The center
</p>
<p>of mass86 R of a set of point masses, each with mass mi at the position ri , is defined
</p>
<p>by
</p>
<p>R =
&sum;
</p>
<p>miri&sum;
mi
</p>
<p>=
&sum;
</p>
<p>miri
</p>
<p>M
=
</p>
<p>&sum; mi
M
</p>
<p>ri . (O.8)
</p>
<p>For a continuous mass distribution or a mass density  (r), it follows with dm = dV
(as always the integrals are over the entire domain of definition) that:
</p>
<p>R =
&int;
 (r) r dV&int;
 (r) dV
</p>
<p>=
&int;
 (r) r dV
</p>
<p>M
=
</p>
<p>&int;
 (r)
</p>
<p>M
r dV . (O.9)
</p>
<p>O.4 Standard Deviation in Quantum Mechanics
</p>
<p>The following are some remarks about the standard deviation in quantum mechanics.
</p>
<p>O.4.1 Example: Two-State System
</p>
<p>We calculate the dispersion for the example of the Pauli matrix z with eigenvalues
</p>
<p>1,2 and eigenvectors v1,2:
</p>
<p>85See also the chapter &lsquo;Discrete-continuous&rsquo; in Appendix T, Vol. 1.
86Conventionally, one writes R instead of r.</p>
<p/>
</div>
<div class="page"><p/>
<p>338 Appendix O: Variance, Expectation Values
</p>
<p>z =
(
</p>
<p>1 0
</p>
<p>0 &minus;1
</p>
<p>)
; 1,2 = &plusmn;1; v1 =
</p>
<p>(
1
</p>
<p>0
</p>
<p>)
; v2 =
</p>
<p>(
0
</p>
<p>1
</p>
<p>)
. (O.10)
</p>
<p>If the system is in the state v1, its spin component has the value +1 , and in the state
v2, it has the value &minus;1. For all other states, a definite value cannot be assigned.
</p>
<p>We now calculate the dispersion. Because of 2z = 1, we find that
</p>
<p>(z)
2 =
</p>
<p>&lang;
2z
</p>
<p>&rang;
&minus; z2 = 1 &minus; z2 . (O.11)
</p>
<p>For a normalized state, it then holds that
</p>
<p>(z)
2 = 1 &minus;
</p>
<p>[(
a&lowast;b&lowast;
</p>
<p>) ( 1 0
0 &minus;1
</p>
<p>)(
a
</p>
<p>b
</p>
<p>)]2
= 1 &minus;
</p>
<p>(
|a|2 &minus; |b|2
</p>
<p>)2 = 4 |a|2
(
1 &minus; |a|2
</p>
<p>)
.
</p>
<p>(O.12)
</p>
<p>Hence, the standard deviation vanishes for the eigenvectors v1,2; for all other states,
</p>
<p>it is in principle not zero. It is therefore a measure of the extent to which a system
</p>
<p>does not have a value for z (i.e. one of the two allowed values &plusmn;1).
</p>
<p>O.4.2 General Case
</p>
<p>We assume a Hermitian operator A with eigenvectors |an and eigenvalues an:
</p>
<p>A |an = an |an . (O.13)
</p>
<p>The variance for a normalized state | is then given by
(
 A
</p>
<p>)2 =
&lang;
A2
</p>
<p>&rang;

&minus; A2 = | A2 | &minus; | A |2 . (O.14)
</p>
<p>We want to show that it vanishes iff | is an eigenstate of A. To this end we use
the Schwarz inequality in the form:
</p>
<p>|a |b|2 &le; a |a b |b , (O.15)
</p>
<p>where the equality holds iff |a and |b are collinear, i.e. |a &sim; |b.
Because of the Hermiticity of A, we have | A2 | = A |A; in addition,
</p>
<p>| A |&lowast; = | A |, and consequently | A |2 = | |A|2. We identify |a
with | and |b with |A; then the Schwarz inequality reads
</p>
<p>| |A|2 = | A |2 &le;  | A |A = | A2 | . (O.16)
</p>
<p>The equals sign applies iff | &sim; A | is valid&mdash;in other words, if | is an eigen-
function of A.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix P
</p>
<p>On Quantum Cryptography
</p>
<p>P.1 Verification of the Key
</p>
<p>After the procedure was performed as described in Chap. 10, two questions still need
</p>
<p>to be answered: (1) How can Alice and Bob ascertain that they have the same key, and
</p>
<p>how they can reliably eliminate discrepancies that may occur? (2) How can Alice and
</p>
<p>Bob ensure that no one but themselves, especially not Eve, possesses this improved
</p>
<p>key? These two problems are solved using classical (i.e. non-quantum-mechanical)
</p>
<p>multistage processes, which offer again not absolute, but in a practical sense adequate
</p>
<p>security. In each stage, the length of the key is reduced. It should again be emphasized
</p>
<p>that the following exchange of information between Alice and Bob is in every phase
</p>
<p>public. This can, in principle, of course, pose a problem if Eve exercises absolute
</p>
<p>control over the public channel. She could then pick up every message from Alice
</p>
<p>and Bob, modify it suitably and finally retransmit it. For example, she could lead
</p>
<p>them to believe that the error rate is zero. However, if Alice and Bob have remained
</p>
<p>watchful and adhere to the procedures described below, they can avoid this trap.
</p>
<p>We consider once more the whole process. The transfer of NA photons with fixed
</p>
<p>time intervals from Alice to Bob has ended. The first phase is then that Bob tells Alice
</p>
<p>at which times he did not receive a signal, although it was supposed to arrive. Both
</p>
<p>eliminate these dark counts and now each has a key of the same length N , the raw
</p>
<p>key. Now they compare publicly the settings of their polarizers, thereby excluding
</p>
<p>all measurements where the basis systems do not match. This key is often called the
</p>
<p>sifted key; it has a length of n &asymp; N/2.
The next step is the estimation of the rate of eavesdropping e. Suppose Eve spied
</p>
<p>on each q&minus;th bit. Then the two keys of Alice and Bob differ at approximately n/(4q)
sites; the interception rate is e = 1/(4q). To estimate this number, Alice and Bob
compare publicly t individual bits of their two keys and then delete them. Of course,
</p>
<p>the eavesdropping rate should not be too high; if it is above an agreed threshold, the
</p>
<p>whole key is discarded and the process is restarted again. With a threshold of e.g.
</p>
<p>12.5%, it is guaranteed that Eve has at most spied on every second photon. Alice
</p>
<p>and Bob are now in possession of keys (also called the plain key) of length n &minus; t and
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>339</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>340 Appendix P: On Quantum Cryptography
</p>
<p>have at their disposal an estimate of the rate of interception, e. Eve knows (n &minus; t) 3e
bits of Alice&rsquo;s key.
</p>
<p>In the next stage, Alice and Bob make sure that they eliminate the bits distorted
</p>
<p>by Eve, and will therefore obtain the same key, called the reconciled key. This can
</p>
<p>be done by various methods, which can also be carried out in series. The common
</p>
<p>feature is that not all individual bits are compared, but only some properties of subsets
</p>
<p>of the key, e.g. the parity of these subsets.
</p>
<p>For example, Alice and Bob can choose publicly a random permutation of their
</p>
<p>key. This series is then cut into blocks of length l, where l is selected in such a way
</p>
<p>that the probability of the occurrence of two or more errors per block is sufficiently
</p>
<p>small. Another method is quite similar; here Alice and Bob publicly extract random
</p>
<p>series from their keys and use them to make blocks of length l. The parity of these
</p>
<p>blocks of length l is then compared publicly. If it does not coincide, a binary search is
</p>
<p>started for the wrong bit, i.e. halving the block, comparing the parities of the halves,
</p>
<p>again bisecting the block with the different parities, etc. up to a certain minimum
</p>
<p>length of the sub-blocks of different parities; this piece of the key is then eliminated.
</p>
<p>In this way, all l blocks are worked through. Subsequently, the next permutation or
</p>
<p>the next random set is selected and the comparison is started again; the entire process
</p>
<p>is performed several times. In all, d bits will be removed in this procedure. By the
</p>
<p>way, a nice side effect is that Alice and Bob get a confirmation of the error rate
</p>
<p>estimated in the first phase&mdash;if not, then something is suspect and the key will be
</p>
<p>discarded.
</p>
<p>Once these procedures have been run through, one can assume, finally, that it is
</p>
<p>highly probable that the remaining key is without error, i.e. that the keys of Alice and
</p>
<p>Bob match in every position. Due to its construction, this improved or reconciled key
</p>
<p>has the length nv = n &minus; t &minus; d. We note that it is only a partially secret key, because
Eve knows (n &minus; t &minus; d) 3e bits.
</p>
<p>In the last stage, Alice and Bob make sure that this &lsquo;flaw&rsquo; is corrected and the
</p>
<p>partially secret key becomes a totally secret one. This process is called privacy
</p>
<p>amplification. It can work like this: Alice and Bob have the reconciled key of length
</p>
<p>nv and know that Eve knows about tv bits of this key. Let 0 &lt; sv &lt; nv &minus; tv be a
safety parameter, and let rv = nv &minus; tv &minus; sv . Then, Alice and Bob may chose e.g. rv
subsets of the reconciled key. The parities of these subsets are determined, and this
</p>
<p>is not done publicly. They then make up the final secret key. One can show that Eve&rsquo;s
</p>
<p>information about this key is at most 2&minus;sv/ ln 2.
We suppose, for example, that Alice sends 500 photons to Bob. After the elim-
</p>
<p>ination of the dark counts and the comparison of the basis systems, a key of say
</p>
<p>n = 233 positions remains. To estimate the interception rate, t = 48 bits are used.
We assume that Eve&rsquo;s interception rate e is e = 0.02 or 2%. The key now has
a length n &minus; t = 185 bits. To construct the reconciled key by comparing the parities,
we have to drop say d = 67 bits. The key now has a length of nv = n&minus; t &minus;d = 118;
Eve knows 118 &middot;0.06 &asymp; 7 bits. We finally choose, for example, sv = 10 and make up
the final key using the parities of 108 random subsets. An explicitly specified short
</p>
<p>example illustrates the situation.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix P: On Quantum Cryptography 341
</p>
<p>P.2 An Example
</p>
<p>Alice sends 64 photons to Bob. Obviously, this is a very small number for such
</p>
<p>purposes, as becomes apparent, inter alia, from the fact that the statistical errors are
</p>
<p>quite large. But for a short and concise toy example we accept this disadvantage.
</p>
<p>Eve spies on every second photon; thus, q = 2 and e = 1
4q
</p>
<p>= 1
8
. As an example,
</p>
<p>this might appear as shown in Table P.1.
</p>
<p>The first and second columns indicate the polarization direction and polarization
</p>
<p>value of the photons sent by Alice. The third column denotes the polarization direction
</p>
<p>chosen by Eve. If two numbers appear in the fourth column, the result of measurement
</p>
<p>by Eve is 0 or 1. The fifth column shows one of Eve&rsquo;s possible measurement series.
</p>
<p>Analogous remarks apply to Bob&rsquo;s columns. Since there are no erroneous readings,
</p>
<p>Alice and Bob are now in possession of their raw keys.
</p>
<p>Alice and Bob publicly compare their polarization directions and delete all the
</p>
<p>results with different settings. The result (sifted key) is given in Table P.2.
</p>
<p>A glance at the table shows that Alice and Bob have different entries in three
</p>
<p>places; the predicted value is (see above) n
4q
</p>
<p>= 31 &middot; e = 31
8
</p>
<p>= 3.9. In addition,
we see that the keys of Alice and Eve match in 12 locations; the predicted value is
</p>
<p>31 &middot; 3e = 93
8
</p>
<p>= 11.6. Alice and Bob cannot, of course, look at this table, but they
need to estimate the error rate. To this end, they compare publicly e.g. 7 out of these
</p>
<p>31 bits, at the positions 4, 8, 12, 16, 20, 24, 28. There is one deviation (bit 4); the
</p>
<p>error rate can therefore be estimated to be e &asymp; 1
7
. Accordingly, Eve has eavesdropped
</p>
<p>on about every second bit ( 7
4
&asymp; 2). The checked bits are deleted,87 and we obtain
</p>
<p>Table P.3 (plain key).
</p>
<p>Alice and Bob have different entries in two places; the predicted value is 24 &middot; e =
24
8
</p>
<p>= 3. The keys of Alice and Eve agree in 10 positions; the predicted value is
24 &middot; 3e = 72
</p>
<p>8
= 9. In order to eliminate the influence of Eve, Alice and Bob now
</p>
<p>compare the parities of subsets. For the sake of simplicity, we choose in our toy
</p>
<p>example the consecutive blocks of length 4 as subsets. The two 4-blocks (5&ndash;8) and
</p>
<p>(9&ndash;12) have different parities, whereas all other blocks match. Halving the &lsquo;wrong&rsquo;
</p>
<p>4-blocks shows that the two 2-blocks 7, 8 and 9, 10 have different parities; they are
</p>
<p>deleted. Table P.4 shows the reconciled key.
</p>
<p>Alice and Bob now have identical keys. Eve&rsquo;s key agrees with theirs at seven
</p>
<p>positions; the predicted value is 20 &middot; 3e = 60
8
= 7.5, while at 11 positions (predicted
</p>
<p>value 10), Eve has no information.
</p>
<p>The method outlined above for privacy amplification starts from nv = 20 and
tv = 7.5. For the security parameter, we have 0 &lt; sv &lt; 12.5. We choose sv = 3.5
and obtain rv = nv &minus; tv &minus; sv = 9. Alice and Bob choose rv = 9 subsets of the
reconciled key (there are
</p>
<p>(
20
</p>
<p>9
</p>
<p>)
= 167, 960 of length 9); the parities (not made
</p>
<p>public) of these subsets are then the final secret key. Eve&rsquo;s information about this key
</p>
<p>is not more than 13%.
</p>
<p>87We see that the key must not be too short.</p>
<p/>
</div>
<div class="page"><p/>
<p>342 Appendix P: On Quantum Cryptography
</p>
<p>Table P.1 Initial data
</p>
<p>A A E E E B B B A A E E E B B B
</p>
<p>1  1  01 0 33  0  0 0
</p>
<p>2  0  0 0  0 0 34  1  1 1  01 1
</p>
<p>3  0  01 0 35  1  01 1
</p>
<p>4  1  01 0  01 1 36  0  01 1  01 0
</p>
<p>5  1  1 1 37  0  0 0
</p>
<p>6  0  01 1  01 1 38  0  0 0  0 0
</p>
<p>7  1  01 0 39  0  01 1
</p>
<p>8  0  0 0  0 0 40  1  1 1  01 1
</p>
<p>9  1  1 1 41  0  01 0
</p>
<p>10  1  1 1  01 1 42  1  1 1  01 1
</p>
<p>11  0  0 0 43  0  01 0
</p>
<p>12  1  01 0  01 1 44  0  01 1  1 1
</p>
<p>13  0  01 1 45  1  1 1
</p>
<p>14  1  1 1  1 1 46  1  1 1  1 1
</p>
<p>15  0  01 0 47  0  0 0
</p>
<p>16  1  01 1  01 0 48  0  0 0  01 0
</p>
<p>17  0  01 0 49  0  01 1
</p>
<p>18  0  0 0  0 0 50  1  1 1  01 1
</p>
<p>19  0  01 1 51  1  01 0
</p>
<p>20  0  0 0  01 0 52  0  01 0  01 0
</p>
<p>21  0  0 0 53  0  01 0
</p>
<p>22  0  01 1  01 1 54  1  1 1  01 0
</p>
<p>23  1  01 1 55  1  1 1
</p>
<p>24  0  0 0  0 0 56  1  1 1  01 1
</p>
<p>25  0  0 0 57  1  1 1
</p>
<p>26  1  01 0  0 0 58  0  0 0  0 0
</p>
<p>27  0  01 1 59  1  1 1
</p>
<p>28  1  1 1  1 1 60  1  1 1  01 1
</p>
<p>29  1  01 1 61  1  1 1
</p>
<p>30  1  01 0  0 0 62  1  01 0  0 0
</p>
<p>31  0  0 0 63  0  01 0
</p>
<p>32  0  0 0  01 0 64  1  1 1  1 1</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix P: On Quantum Cryptography 343
</p>
<p>Table P.2 Sifted key
</p>
<p>A E B A E B
</p>
<p>1 0 0 0 17 0 0
</p>
<p>2 1 0 1 18 0 0
</p>
<p>3 1 1 19 0 1 0
</p>
<p>4 0 1 1 20 0 0
</p>
<p>5 0 0 0 21 0 0 0
</p>
<p>6 1 1 22 1 1
</p>
<p>7 0 0 23 1 1 1
</p>
<p>8 1 0 1 24 0 0
</p>
<p>9 1 1 1 25 0 0 0
</p>
<p>10 1 1 0 26 1 1
</p>
<p>11 0 0 0 27 1 1
</p>
<p>12 0 0 28 0 0 0
</p>
<p>13 0 1 1 29 1 1
</p>
<p>14 0 0 0 30 1 1
</p>
<p>15 0 0 31 1 1 1
</p>
<p>16 1 1 1
</p>
<p>Table P.3 Plain key
</p>
<p>A E B A E B
</p>
<p>1 0 0 0 13 0 0
</p>
<p>2 1 0 1 14 0 0
</p>
<p>3 1 1 15 0 1 0
</p>
<p>4 0 0 0 16 0 0 0
</p>
<p>5 1 1 17 1 1
</p>
<p>6 0 0 18 1 1 1
</p>
<p>7 1 1 1 19 0 0 0
</p>
<p>8 1 1 0 20 1 1
</p>
<p>9 0 0 0 21 1 1
</p>
<p>10 0 1 1 22 1 1
</p>
<p>11 0 0 0 23 1 1
</p>
<p>12 0 0 24 1 1 1</p>
<p/>
</div>
<div class="page"><p/>
<p>344 Appendix P: On Quantum Cryptography
</p>
<p>Table P.4 Reconciled key
</p>
<p>A E B A E B
</p>
<p>1 0 0 0 11 0 1 0
</p>
<p>2 1 0 1 12 0 0 0
</p>
<p>3 1 1 13 1 1
</p>
<p>4 0 0 0 14 1 1 1
</p>
<p>5 1 1 15 0 0 0
</p>
<p>6 0 0 16 1 1
</p>
<p>7 0 0 0 17 1 1
</p>
<p>8 0 0 18 1 1
</p>
<p>9 0 0 19 1 1
</p>
<p>10 0 0 20 1 1 1
</p>
<p>Table P.5 Final key
</p>
<p>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
</p>
<p>A,
</p>
<p>B
</p>
<p>0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0
</p>
<p>This is a conservative estimate. If we proceed less stringently, we can choose,
</p>
<p>for example, 20 &minus; 3 = 17 random subsets (of which there are
(
</p>
<p>20
</p>
<p>17
</p>
<p>)
=1, 140 of
</p>
<p>length 17), and determine their parities. In principle, one can also choose smaller
</p>
<p>subsets; it is important only that the subsets are large enough that Eve has, on average,
</p>
<p>always at least one &lsquo;flaw&rsquo;. To make life easy here, we assume e.g. that the first six are
</p>
<p>from neighboring triplets 1&ndash;3, etc., the next five from groups of four, the next four
</p>
<p>from groups of five, the last two from the first two groups of six (a schematic approach
</p>
<p>like this is all right for our toy example, but not of course for serious cryptography).
</p>
<p>We see this final key in Table P.5.
</p>
<p>There is not a single parity which Eve can determine exactly, since she always
</p>
<p>misses the information by at least one bit. Of course she can guess&mdash;but in doing so,
</p>
<p>she is in the same situation as if she had not tried to spy on the key. Thus, in our toy
</p>
<p>example, Alice and Bob have a common secret key. Quantum mechanics makes it
</p>
<p>possible.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix Q
</p>
<p>Schr&ouml;dinger Picture, Heisenberg Picture,
</p>
<p>Interaction Picture
</p>
<p>Q.1 Schr&ouml;dinger and Heisenberg Picture
</p>
<p>The Schr&ouml;dinger equation in the form that we have used has time-dependent solutions
</p>
<p>or states, while operators such as the angular momentum do not depend on time. This
</p>
<p>type of description is called the Schr&ouml;dinger picture. But there are also other forms
</p>
<p>for the state description, for example the Heisenberg picture, in which the states
</p>
<p>are constant and the operators change with time. In the Schr&ouml;dinger picture, the
</p>
<p>time-variable state is given by
</p>
<p>|
(t) = e&minus;i Ht |
(0) . (Q.1)
</p>
<p>In the Heisenberg picture, the same state is defined as
</p>
<p>|
H = ei
Ht
 |
(t) . (Q.2)
</p>
<p>Accordingly, the operator A, which is time independent in the Schr&ouml;dinger picture,
</p>
<p>becomes the time-dependent operator AH in the Heisenberg picture, with:
</p>
<p>AH = ei
Ht
 Ae&minus;i
</p>
<p>Ht
 . (Q.3)
</p>
<p>For the time evolution, we have
</p>
<p>i
d
</p>
<p>dt
AH = [AH , H ] + i
</p>
<p>&part;
</p>
<p>&part;t
AH (Q.4)
</p>
<p>with
&part;
</p>
<p>&part;t
AH = ei
</p>
<p>Ht

</p>
<p>(
&part;
</p>
<p>&part;t
A
</p>
<p>)
e&minus;i
</p>
<p>Ht
 . (Q.5)
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>345</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>346 Appendix Q: Schr&ouml;dinger Picture, Heisenberg Picture, Interaction Picture
</p>
<p>Intuitively, the difference between the two representations corresponds to the repre-
</p>
<p>sentation with a fixed coordinate system88 and a moving vector (Schr&ouml;dinger image),
</p>
<p>as compared to a representation with a fixed vector and a coordinate system moving
</p>
<p>in a corresponding manner (Heisenberg picture).
</p>
<p>We use the Schr&ouml;dinger picture almost exclusively in this book.
</p>
<p>Q.2 Interaction Picture
</p>
<p>The interaction picture (or interaction representation) is a third way in addition to
</p>
<p>the Schr&ouml;dinger and the Heisenberg picture. It is central in quantum field theory.
</p>
<p>In the Schr&ouml;dinger picture, the operators are time-independent and the states time-
</p>
<p>dependent, in the Heisenberg picture the operators are time-dependent and the states
</p>
<p>time-independent. A certain &lsquo;division&rsquo; of the time dependence is obtained in the
</p>
<p>interaction picture, where both the operators and the states can be (and are usually)
</p>
<p>time-dependent. It is especially useful if the Hamiltonian can be written as the sum
</p>
<p>H = H0 + H1, where H1 is a small term89 compared to H0. H0 is called free part
and H1 interaction part. Usually, H0 is time-independent and allows for analytical
</p>
<p>solutions.
</p>
<p>We start in the Schr&ouml;dinger picture
</p>
<p>i
d
</p>
<p>dt
| (t) = (H0 + H1) | (t) = H | (t) . (Q.6)
</p>
<p>We define states |I (t) and operators AI (t) in the interaction picture by
</p>
<p>|I (t) = ei
H0 t
</p>
<p> | (t) ; AI (t) = ei
H0 t
</p>
<p> Ae&minus;i
H0 t
</p>
<p> . (Q.7)
</p>
<p>Note that in these expressions only the free Hamiltonian H0 occurs.
</p>
<p>To calculate the equation of motion for |I (t), we start from the Schr&ouml;dinger
equation. With | (t) = e&minus;i H0 t |I (t) we obtain
</p>
<p>i
d
</p>
<p>dt
e&minus;i
</p>
<p>H0 t
</p>
<p> |I (t) = (H0 + H1) e&minus;i
H0 t
</p>
<p> |I (t) . (Q.8)
</p>
<p>This gives for time-independent H0
</p>
<p>i
</p>
<p>(
&minus;i H0
</p>
<p>
e&minus;i
</p>
<p>H0 t
</p>
<p> |I (t) + e&minus;i
H0 t
</p>
<p>
</p>
<p>d
</p>
<p>dt
|I (t)
</p>
<p>)
= (H0 + H1) e&minus;i
</p>
<p>H0 t
</p>
<p> |I (t) (Q.9)
</p>
<p>88Or measuring apparatus.
89Hence, the interaction picture is particularly suitable for perturbation theory.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix Q: Schr&ouml;dinger Picture, Heisenberg Picture, Interaction Picture 347
</p>
<p>or
</p>
<p>i
d
</p>
<p>dt
|I (t) = HI (t) |I (t) (Q.10)
</p>
<p>with
</p>
<p>HI (t) = ei
H0 t
</p>
<p> H1e
&minus;i H0 t
</p>
<p> |I (t) . (Q.11)
</p>
<p>Vividly, the state |I (t) changes much more slowly than | (t), since the energy
associated with H1 is small compared to that of H0, in general.
</p>
<p>Next, we want to determine the time evolution operator in the interaction picture.
</p>
<p>In the Schr&ouml;dinger picture, see (Q.6),
</p>
<p>| (t) = e&minus;i
H(t&minus;t0)
</p>
<p> | (t0) = US (t, t0) | (t0) . (Q.12)
</p>
<p>With the definition of |I (t) follows
</p>
<p>ei
H0 t
</p>
<p> | (t) = |I (t) = ei
H0 t
</p>
<p> e&minus;i
H(t&minus;t0)
</p>
<p> e&minus;i
H0 t0
</p>
<p> |I (t0) (Q.13)
</p>
<p>which means that the time evolution operator in the interaction picture has the form
</p>
<p>UI (t, t0) = ei
H0 t
</p>
<p> US (t, t0) e
&minus;i H0 t0
</p>
<p> = ei H0 t e&minus;i
H(t&minus;t0)
</p>
<p> e&minus;i
H0 t0
</p>
<p> (Q.14)
</p>
<p>and we have
</p>
<p>|I (t) = UI (t, t0) |I (t0) . (Q.15)
</p>
<p>As is seen, the knowledge of UI (t, t0) enables us to calculate |I (t) for a given
|I (t0).
</p>
<p>Note that transition probabilities are independent from the picture chosen. Assume
</p>
<p>that in a certain process the system is at time t0 in the initial state |I (t0) = |i.
Then we have |I (t) = UI (t, t0) |I (t0), see (Q.15), and the probability P f i to
find it at time t in a final state |I (t) = | f  is given by
</p>
<p>P f i = | f |UI (t, t0) |i|2 . (Q.16)
</p>
<p>As one can show, the transition amplitude is equal in the Schr&ouml;dinger and the inter-
</p>
<p>action picture, see the exercises.
</p>
<p>Due to (Q.14), the equation of motion for UI (t, t0) is given by
</p>
<p>i
d
</p>
<p>dt
UI (t, t0) = HI (t)UI (t, t0) . (Q.17)
</p>
<p>A formal solution of equation (Q.17) with the initial condition UI (t0, t0) = 1
reads
</p>
<p>UI (t, t0) = 1 &minus;
i
</p>
<p>
</p>
<p>&int; t
</p>
<p>t0
</p>
<p>dt1 HI (t1)UI (t1, t0) . (Q.18)</p>
<p/>
</div>
<div class="page"><p/>
<p>348 Appendix Q: Schr&ouml;dinger Picture, Heisenberg Picture, Interaction Picture
</p>
<p>Iterating this solution gives
</p>
<p>UI (t, t0) = 1 &minus; i
&int; t
</p>
<p>t0
dt1 HI (t1)UI (t1, t0) =
</p>
<p>= 1 &minus; i

</p>
<p>&int; t
t0
</p>
<p>dt1 HI (t1)
[
1 &minus; i
</p>
<p>
</p>
<p>&int; t1
t0
</p>
<p>dt2 HI (t2)UI (t2, t0)
]
=
</p>
<p>= 1 +
(
&minus; i
</p>
<p>
</p>
<p>) &int; t
t0
</p>
<p>dt1 HI (t1)+
(
&minus; i
</p>
<p>
</p>
<p>)2 &int; t
t0
</p>
<p>&int; t1
t0
</p>
<p>dt2 HI (t2)UI (t2, t0)
</p>
<p>(Q.19)
</p>
<p>and further iteration results in
</p>
<p>UI (t, t0) = 1 +
(
&minus; i
</p>
<p>
</p>
<p>) &int; t
t0
</p>
<p>dt1 HI (t1)+
(
&minus; i
</p>
<p>
</p>
<p>)2 &int; t
t0
</p>
<p>dt1
&int; t1
</p>
<p>t0
dt2 HI (t1) HI (t2)+
</p>
<p>+
(
&minus; i
</p>
<p>
</p>
<p>)3 &int; t
t0
</p>
<p>dt1
&int; t1
</p>
<p>t0
dt2
</p>
<p>&int; t2
t0
</p>
<p>dt3 HI (t1) HI (t2) HI (t3)+ &middot; &middot; &middot;
.
</p>
<p>(Q.20)
</p>
<p>Thus, we can write the (formal) solution in form of an infinite series
</p>
<p>UI (t, t0) =
&infin;&sum;
</p>
<p>n=0
</p>
<p>(
&minus; i

</p>
<p>)n &int; t
</p>
<p>t0
</p>
<p>dt1
</p>
<p>&int; t1
t0
</p>
<p>dt2 . . .
</p>
<p>&int; tn&minus;1
t0
</p>
<p>dtn HI (t1) HI (t2) . . . HI (tn) .
</p>
<p>(Q.21)
</p>
<p>This series is called Dyson series.90 Note that the HI (tm) at different times will
</p>
<p>not commute, [HI (t1) , HI (t2)] 	= 0, in general. Thus, the order of time is of great
importance. Note furthermore that the upper limits of the integrals are all different
</p>
<p>and are ordered, t0 &le; tn &le; tn&minus;1 &lt; &middot; &middot; &middot; &le; t2 &le; t1 &le; t .
The Dyson series plays a important role in quantum field theory, among others.
</p>
<p>There, one applies an operation called time ordering to the series (Q.21) in order
</p>
<p>to get rid of the problem of different upper limits of the integrals, see Appendix W,
</p>
<p>Vol. 2.
</p>
<p>Q.2.1 Exercises and Solutions
</p>
<p>1. Given an operator A in the Schr&ouml;dinger picture, show that the time evolution of
</p>
<p>the operator AH in the Heisenberg picture is given by
</p>
<p>i
d
</p>
<p>dt
AH = [AH , H ] + i
</p>
<p>&part;
</p>
<p>&part;t
AH with
</p>
<p>&part;
</p>
<p>&part;t
AH = ei
</p>
<p>Ht

</p>
<p>(
&part;
</p>
<p>&part;t
A
</p>
<p>)
e&minus;i
</p>
<p>Ht
 . (Q.22)
</p>
<p>Solution: We have
</p>
<p>i
d
</p>
<p>dt
AH = &minus;Hei
</p>
<p>Ht
 Ae&minus;i
</p>
<p>Ht
 + iei Ht
</p>
<p>(
&part;
</p>
<p>&part;t
A
</p>
<p>)
e&minus;i
</p>
<p>Ht
 + ei Ht Ae&minus;i Ht H
</p>
<p>= AH H &minus; H AH + iei
Ht

</p>
<p>(
&part;
&part;t
</p>
<p>A
)
</p>
<p>e&minus;i
Ht
 = [AH , H ] + i &part;&part;t AH .
</p>
<p>(Q.23)
</p>
<p>2. Given |I (t0) = |i I  and |I (t) = | fH . The corresponding states in the
Schr&ouml;dinger picture are |iS and | fS. Show  f I |i I  =  fS |iS .
</p>
<p>90It is assumed that the series is enough good-natured and will converge.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix Q: Schr&ouml;dinger Picture, Heisenberg Picture, Interaction Picture 349
</p>
<p>Solution: States in the Schr&ouml;dinger and the interaction picture are related by
</p>
<p>|I (t) = ei
H0 t
</p>
<p> |S (t). It follows
</p>
<p> f I |i I  =  fs | e&minus;i
H0 t
</p>
<p> ei
H0 t
</p>
<p> |iS =  fS |iS . (Q.24)
</p>
<p>3. Show  f I |UI (t, t0) |i I  =  fS|US (t, t0) |iS.
Solution: We have
</p>
<p> f I |UI (t, t0) |i I  =  fS| e&minus;i
H0 t
</p>
<p> UI (t, t0) e
i
</p>
<p>H0 t
</p>
<p> |iS =  fS|US (t, t0) |iS
(Q.25)
</p>
<p>due to the definition HI (t) = ei
H0 t
</p>
<p> HSe
&minus;i H0 t
</p>
<p> .
</p>
<p>4. Prove (Q.17).
</p>
<p>Solution: Due to (Q.14) we have
</p>
<p>i d
dt
</p>
<p>UI (t, t0) = i ddt ei
H0 t
</p>
<p> e&minus;i
H(t&minus;t0)
</p>
<p> e&minus;i
H0 t0
</p>
<p> =
= i
</p>
<p>(
ei
</p>
<p>H0 t
</p>
<p>
i

</p>
<p>H0e
&minus;i H(t&minus;t0)
</p>
<p> e&minus;i
H0 t0
</p>
<p> &minus; ei H0 t i

</p>
<p>H0e
&minus;i H(t&minus;t0)
</p>
<p> e&minus;i
H0 t0
</p>
<p>
</p>
<p>)
=
</p>
<p>= ei H0 t He&minus;i H(t&minus;t0) e&minus;i H0 t0 = ei H0 t He&minus;i H0 t ei H0 t e&minus;i H(t&minus;t0) e&minus;i H0 t0 =
= HI (t)UI (t, t0) .
</p>
<p>(Q.26)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix R
</p>
<p>The Postulates of Quantum Mechanics
</p>
<p>After a few remarks on the term &lsquo;postulate&rsquo;, we present in this chapter various versions
</p>
<p>of the postulates as given in current textbooks.
</p>
<p>R.1 Postulate, Axiom, Rule?
</p>
<p>In Chap. 14, we formulated the &lsquo;rules of the game&rsquo; of quantum mechanics by introduc-
</p>
<p>ing several postulates. This term (from the Latin postulatum, demand) is widespread,
</p>
<p>but there are also other terms such as axiom, rule or principle.91
</p>
<p>The variety of names alone is enough to show that here, the formal term &lsquo;axiom&rsquo;
</p>
<p>is not appropriate; it is defined as a fundamental statement about a system S that
</p>
<p>is assumed without proof. All axioms together form a consistent (i.e. contradiction-
</p>
<p>free), minimal, independent system (independence: no axiom can be derived from
</p>
<p>another), from which all statements about S are logically derivable.
</p>
<p>In our context, &lsquo;postulate&rsquo; stands in contrast for a basic rule that is plausible in
</p>
<p>terms of physical considerations and has been very well confirmed experimentally.
</p>
<p>The postulates all together must be consistent, of course; but the question of whether
</p>
<p>they actually are all mutually independent is considered secondary. It is rather impor-
</p>
<p>tant that they are few and concise formulations; they are, so to speak, the load-bearing
</p>
<p>structure of quantum mechanics.
</p>
<p>Currently, the postulates of quantum mechanics are not deducible in a strictly
</p>
<p>logically manner from a broader theory&mdash;which does not mean that this will not be
</p>
<p>possible someday. But even then, the postulates of quantum mechanics would retain
</p>
<p>their value because they describe the relevant phenomena very well. This is quite
</p>
<p>similar to the Newtonian axioms: Their limits were shown by quantum mechanics and
</p>
<p>special relativity; but within this framework, the Newtonian axioms are still useful
</p>
<p>because they are simple and their predictions accurate enough for many purposes.
</p>
<p>91&ldquo;I am a quantum engineer, but on Sundays I have principles.&rdquo; John Stewart Bell, Irish physicist,
</p>
<p>&lsquo;inventor&rsquo; of Bell&rsquo;s inequality.
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>351</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>352 Appendix R: The Postulates of Quantum Mechanics
</p>
<p>Which facts of quantum mechanics have to be formulated as a postulate/axiom/rule
</p>
<p>/principle and how, is to some extent a matter of personal taste, as we shall see in the
</p>
<p>next section.
</p>
<p>R.2 Formulations of Some Authors
</p>
<p>We wish to get to know different representations of the basic rules of quantum
</p>
<p>mechanics. We do not aim at an exhaustive survey; rather, we give an illustration of
</p>
<p>how different and how similar the approaches can be.
</p>
<p>We cite only the postulates themselves, giving for better comparability all the texts
</p>
<p>in English, and quoting them verbatim (apart from translation). Therefore, some
</p>
<p>of the following quotes are very sparse. To save space, we dispense with further
</p>
<p>explanations. These are found in extenso in the relevant sources, where they can be
</p>
<p>consulted if desired.
</p>
<p>First, some remarks:
</p>
<p>(a) Some books do not refer explicitly to &lsquo;rules&rsquo; under any name. The subject
</p>
<p>matter itself is treated, of course&mdash;in any quantum mechanics textbook, e.g. the
</p>
<p>Hilbert space and time evolution are presented, but these issues are not always explic-
</p>
<p>itly listed as postulates, for example in A. Messiah, Quantum Mechanics (1964) or
</p>
<p>T. Flie&szlig;bach. Quantum Mechanics (2000). This can apply even if there is a separate
</p>
<p>chapter such as &lsquo;The principles of quantum dynamics&rsquo; in E. Merzbacher, Quantum
</p>
<p>Mechanics (1998).
</p>
<p>(b) Some authors summarize only kinematics, but not dynamics in the form of
</p>
<p>postulates (although dynamics is treated in great detail in their texts), e.g. W. Nolting,
</p>
<p>Quantum Mechanics (1992), or K. Gottfried, T.-M. Yan, Quantum Mechanics (2006).
</p>
<p>(c) The formulations of many quantum mechanics books are more or less uniform
</p>
<p>and differ substantially only in details of the wording or the order of the postulates.
</p>
<p>It is striking that often the indistinguishability of identical quantum objects or the
</p>
<p>Pauli principle is not formulated as a postulate.
</p>
<p>(d) In some quantum-mechanics books, such a peculiar terminology is used that
</p>
<p>simply citing the postulates without further comments would be incomprehensible,
</p>
<p>e.g. in A. Peres, Quantum Theory (1995). For space reasons, we refrain from citing
</p>
<p>such works.
</p>
<p>(e) The placement of the postulates varies considerably; sometimes they are sum-
</p>
<p>marized compactly in a few pages, either at the beginning of the book or more often
</p>
<p>in the middle. In other cases, they are scattered throughout the book and are not
</p>
<p>recognizable at first glance as a coherent system.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix R: The Postulates of Quantum Mechanics 353
</p>
<p>R.2.1 J. Audretsch
</p>
<p>Entangled Systems (2007), p. 32 ff, postulates
</p>
<p>Postulate 1 (pure state) An isolated quantum system which is in a pure state is
</p>
<p>described by its state vector | . This is a normalised vector in a Hilbert space Hd
which is associated with the quantum system.
</p>
<p>Postulate 2 (projective measurements, non-deterministic dynamic evolution)
</p>
<p>(a) A projective measurement of a physical quantity (e.g. of the energy, angular
</p>
<p>momentum, etc.) carried out on a quantum system is described by an Hermitian
</p>
<p>operator which can be time dependent and acts on the vectors of Hd . We speak of a
</p>
<p>measurement of the observable A and denote the operator with the same symbol A.
</p>
<p>(b) The possible measured values which can occur as a result of a measurement of
</p>
<p>the observable A are the eigenvalues an of the associated operator A. For simplicity,
</p>
<p>we assume that its spectrum is discrete:
</p>
<p>A
uin
</p>
<p>&rang;
= an
</p>
<p>uin
&rang;
; i = 1, . . . , gn . (R.1)
</p>
<p>The eigenvectors
uin
</p>
<p>&rang;
form an orthonormal basis or can, in the case of degeneracy,
</p>
<p>be correspondingly chosen. The gn give the degree of degeneracy of the eigenvalues
</p>
<p>an .
</p>
<p>(c) When a selective measurement of the observables A of a system with a nor-
</p>
<p>malised state vector | leads to the result an , then the non-normalised state vector&prime;n
&rang;
</p>
<p>immediately following the measurement is given by the projection of |
</p>
<p>| &rarr;
&prime;n
</p>
<p>&rang;
= Pn | (R.2)
</p>
<p>with the projection operator
</p>
<p>Pn =
gn&sum;
</p>
<p>i=1
</p>
<p>uin
&rang; &lang;
</p>
<p>uin
</p>
<p> (R.3)
</p>
<p>which projects onto the space of the eigenvectors corresponding to an . Through
</p>
<p>normalisation of
</p>
<p>&prime;n
&rang;
, the state vector
</p>
<p>&prime;n
&rang;
</p>
<p>after the measurement is obtained.
</p>
<p>(d) We denote by N (an) the frequency with which a measured value an is obtained
</p>
<p>when the measurement is carried out on N identically prepared systems in the state
</p>
<p>|. The relative frequencies N (an)
N
</p>
<p>for all these ensembles approach the probability
</p>
<p>p(an) as a limiting value in the limit N &rarr; &infin;:
N (an)
</p>
<p>N
</p>
<p>N&rarr;&infin;&rarr; p (an) . (R.4)
</p>
<p>(e) The probability p(an) of obtaining a particular measured value an at a certain
</p>
<p>time is equal to the expectation value of the projection operator Pn computed with
</p>
<p>the state | prior to the measurement. Equivalently, it is equal to the square of the
norm of the non-normalised state vector
</p>
<p>&prime;n
&rang;
</p>
<p>after the measurement:</p>
<p/>
</div>
<div class="page"><p/>
<p>354 Appendix R: The Postulates of Quantum Mechanics
</p>
<p>p (an) = | Pn | =

&prime;n
</p>
<p>&rang;
2
</p>
<p>. (R.5)
</p>
<p>Postulate 3 (deterministic dynamic evolution between preparation and measure-
</p>
<p>ment)
</p>
<p>(a) For isolated systems, the probability distribution p (an) evolves in a determin-
</p>
<p>istic and reversible manner between the preparation and the measurement. Its time
</p>
<p>development between two times t0 and t1 is described by a unitary time-development
</p>
<p>operator U (t1, t0):
</p>
<p>U &dagger;(t1, t0) = U&minus;1(t1, t0). (R.6)
</p>
<p>This operator fulfills the conditions U (t0, t0) = 1 and
</p>
<p>U (t2, t1)U (t1, t0) = U (t2, t0) (R.7)
</p>
<p>for arbitrary times t0, t1, t2.
</p>
<p>(b) The dynamic equation for U (t, t0) is
</p>
<p>i
d
</p>
<p>dt
U (t, t0) = H(t)U (t, t0). (R.8)
</p>
<p>(c) The Schr&ouml;dinger representation is one of the many possible formulations of
</p>
<p>this time development. In this representation, the dynamic evolution of the state is
</p>
<p>given by the state vector alone, according to
</p>
<p>| (t) = U (t, t0) | (t0) . (R.9)
</p>
<p>Observables can be only explicitly time dependent. At each time t , there is a cor-
</p>
<p>responding probability distribution p(an, t) for the results of a measurement of A
</p>
<p>given by (R.5). From (R.8) and (R.9), the Schr&ouml;dinger equation follows
</p>
<p>i
d
</p>
<p>dt
| (t) = H | (t) . (R.10)
</p>
<p>R.2.2 J.-L. Basdevant and J. Dalibard
</p>
<p>Quantum Mechanics (2002), p. 100 ff; Principles
</p>
<p>First Principle: The Superposition Principle With each physical system one
</p>
<p>can associate an appropriate Hilbert space EH . At each time t , the state of the system
</p>
<p>is completely determined by a normalized vector | (t) of EH .</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix R: The Postulates of Quantum Mechanics 355
</p>
<p>Second Principle: Measurements of Physical Quantities
</p>
<p>(a) With each physical quantity A one can associate a linear Hermitian operator
</p>
<p>A acting in EH : A is the observable which represents the quantity A.
</p>
<p>(b) We denote by | the state of the system before the measurement of A is
performed. Whatever | may be, the only possible results of the measurement are
the eigenvalues a of A.
</p>
<p>(c) We denote by P the projector onto the subspace associated with the eigenvalue
</p>
<p>a. The probability of finding the value a in a measurement of A is
</p>
<p>P (a) = 2 , where | = P | . (R.11)
</p>
<p>(d) Immediately after the measurement of A has been performed and has given
</p>
<p>the result a, the new state
&prime;
</p>
<p>&rang;
of the system is
</p>
<p>&prime;
&rang;
= |
</p>
<p>. (R.12)
</p>
<p>Third Principle: Time Evolution We denote by | (t) the state of the system
at time t . As long as the system does not undergo any observation, its time evolution
</p>
<p>is given by the Schr&ouml;dinger equation:
</p>
<p>i
d
</p>
<p>dt
| (t) = H | (t) (R.13)
</p>
<p>where H is the energy observable, or Hamiltonian, of the system.
</p>
<p>R.2.3 D.R. Bes
</p>
<p>Quantum Mechanics (2004), p. 9 (1&ndash;3), p. 96 (4), p. 137 (5); Basic principles
</p>
<p>Principle 1. The state of a system is completely described by a vector 
&mdash;the
</p>
<p>state vector or state function&mdash;belonging to a Hilbert space.
</p>
<p>Principle 2. To every physical quantity there corresponds a single operator. In
</p>
<p>particular, the operators x and p, corresponding to the coordinate and momentum of
</p>
<p>a particle, satisfy the commutation relation
</p>
<p>[
x, p
</p>
<p>]
= i. (R.14)
</p>
<p>Principle 3. The eigenvalues qi of an operator Q constitute the possible results
</p>
<p>of a measurement of the physical quantity Q . The probability of obtaining the
</p>
<p>eigenvalue qi is the modulus squared |ci |2 of the amplitude of the eigenvector i in
the state vector 
 representing the state of the system.</p>
<p/>
</div>
<div class="page"><p/>
<p>356 Appendix R: The Postulates of Quantum Mechanics
</p>
<p>Principle 4. There are only two kinds of particles in nature: bosons described
</p>
<p>by symmetric state vectors, and fermions described by antisymmetric state vectors.
</p>
<p>Principle 5. The operator yielding the change of a state vector over time is
</p>
<p>proportional to the Hamiltonian
</p>
<p>&part;
</p>
<p>&part;t &prime;
U
</p>
<p>(
t &prime;, t
</p>
<p>)
t &prime;=t
</p>
<p>= &minus; i

</p>
<p>H (t) . (R.15)
</p>
<p>R.2.4 B.H. Bransden and C.J. Joachain
</p>
<p>Quantum Mechanics (2000), p. 194 to p. 231; postulates
</p>
<p>Postulate 1 To an ensemble of physical systems one can, in certain cases,
</p>
<p>associate a wave function or state function which contains all the information that
</p>
<p>can be known about the ensemble. This function is in general complex; it may be
</p>
<p>multiplied by an arbitrary complex number without altering its physical significance.
</p>
<p>Postulate 2 The superposition principle.92
</p>
<p>Postulate 3 With every dynamical variable is associated a linear operator.
</p>
<p>Postulate 4 The only result of a precise measurement of the dynamical variable
</p>
<p>A is one of the eigenvalues an of the linear operator A associated with A.
</p>
<p>Postulate 5 If a series of measurements is made of the dynamical variable A
</p>
<p>on an ensemble of systems, described by the wave function 
, the expectation or
</p>
<p>average value of this dynamical variable is
</p>
<p>A = 
| A |

 |
 . (R.16)
</p>
<p>Postulate 6 A wave function representing any dynamical state can be expressed
</p>
<p>as a linear combination of the eigenfunctions of A, where A is the operator associated
</p>
<p>with a dynamical variable.
</p>
<p>Postulate 7 The time evolution of the wave function of a system is determined
</p>
<p>by the time-dependent Schr&ouml;dinger equation
</p>
<p>i
&part;
</p>
<p>&part;t

(t) = H(t)
(t) (R.17)
</p>
<p>where H is the Hamiltonian, or total energy operator of the system.
</p>
<p>R.2.5 C. Cohen-Tannoudji, B. Diu, and F. Lalo&euml;
</p>
<p>Quantum mechanics, Volume 1 (1977), p. 215 ff; postulates
</p>
<p>92It stands there really so short and crisp.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix R: The Postulates of Quantum Mechanics 357
</p>
<p>First Postulate: At a fixed time t0, the state of a physical system is defined by
</p>
<p>specifying a ket |(t0) belonging to the state space E .
Second Postulate: Every measurable physical quantity A is described by an
</p>
<p>operator A acting in E ; this operator is an observable.
</p>
<p>Third Postulate: The only possible result of the measurement of a physical quan-
</p>
<p>tity A is one of the eigenvalues of the corresponding observable A.
</p>
<p>Fourth Postulate (case of a discrete non-degenerate spectrum): When the
</p>
<p>physical quantity A is measured on an system in the normalized state |, the prob-
ability P(an) of obtaining the non-degenerate eigenvalue an of the corresponding
</p>
<p>variable A is:
</p>
<p>P(an) = |un| |2 (R.18)
</p>
<p>where |un is the normalized eigenvector of A associated with the eigenvalue an .
Fourth Postulate (case of a discrete spectrum): When the physical quantity
</p>
<p>A is measured on an system in the normalized state |, the probability P(an) of
obtaining the eigenvalue an of the corresponding variable A is:
</p>
<p>P(an) =
gn&sum;
</p>
<p>i=1
</p>
<p>&lang;uin
 
</p>
<p>2 (R.19)
</p>
<p>where gn is the degree of degeneracy of an and
{uin
</p>
<p>&rang;}
{i = 1, 2, . . . , gn} is an
</p>
<p>orthonormal set of vectors which forms a basis in the eigensubspace En associated
</p>
<p>with the eigenvalue an of A.
</p>
<p>Fourth Postulate (case of a continuous non-degenerate spectrum): When the
</p>
<p>physical quantity A is measured on an system in the normalized state |, the prob-
ability dP(an) of obtaining a result included between  and + d is equal to:
</p>
<p>dP(a) = |v| |2 d (R.20)
</p>
<p>where |v is the eigenvector corresponding to the eigenvalue  of the observable A
associated with A.
</p>
<p>Fifth Postulate: If the measurement of the physical quantity A on the system
</p>
<p>in the state | gives the result an , the state of the system immediately after the
measurement is the normalized projection,
</p>
<p>Pn |&radic;|Pn | , of | onto the eigensubspace
associated with an .
</p>
<p>Sixth Postulate: The time evolution of the state vector |(t) is governed by the
Schr&ouml;dinger equation</p>
<p/>
</div>
<div class="page"><p/>
<p>358 Appendix R: The Postulates of Quantum Mechanics
</p>
<p>i
d
</p>
<p>dt
|(t) = H(t) |(t) (R.21)
</p>
<p>where H(t) is the observable associated with the total energy of the system.
</p>
<p>R.2.6 K. Gottfried and T.-M. Yan
</p>
<p>Quantum Mechanics: Fundamentals (2006), p. 40ff; postulates
</p>
<p>1. The most complete possible description of the state of any physical system S at any
</p>
<p>instant is provided by some particular vector | in the Hilbert space H appropriate
to the system. Every linear combination of such state vectors represents a possible
</p>
<p>physical state of S.
</p>
<p>2. The physically meaningful entities of classical mechanics, such as momentum,
</p>
<p>energy, position and the like, are represented by Hermitian operators.
</p>
<p>3. A set of N identically prepared replicas of a system S described by the pure state
</p>
<p>|, when subjected to a measurement designed to display the physical quantity
represented by the observable A, will in each individual case display one of the
</p>
<p>values93
(
a, a&prime;, . . .
</p>
<p>)
, and as N &rarr; &infin;will do so with the probabilities p (a), p
</p>
<p>(
a&prime;
)
,
</p>
<p>&hellip;, where
</p>
<p>p (a) = |a ||2 . (R.22)
</p>
<p>(The dynamics is not given in the form of postulates.)
</p>
<p>R.2.7 C.J. Isham
</p>
<p>Lectures on Quantum Theory (2008), p. 84ff; rules
</p>
<p>Rule 1: The predictions of results of measurements made on an otherwise iso-
</p>
<p>lated system are probabilistic in nature. In situations where the maximum amount of
</p>
<p>information is available, this probabilistic information is represented mathematically
</p>
<p>by a vector in a complex Hilbert space H that forms the state space of the quantum
</p>
<p>theory. In so far as it gives the most precise predictions that are possible, the vector is
</p>
<p>to be thought of as the mathematical representative of the physical notion of &lsquo;state&rsquo;
</p>
<p>of the system.
</p>
<p>Rule 2: The observables of the system are represented mathematically by self-
</p>
<p>adjoint operators that act on the Hilbert space H.
</p>
<p>Rule 3: If an observable quantity A and a state are represented respectively by the
</p>
<p>self-adjoint operator A and the normalised vector  &isin; H, then the expected result
A of measuring A is
</p>
<p>93These are the eigenvalues of A.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix R: The Postulates of Quantum Mechanics 359
</p>
<p>A = | A | . (R.23)
</p>
<p>Rule 4: In the absence of any external influence (i.e., in a closed system), the state
</p>
<p>vector  changes smoothly in time t according to the time-dependent Schr&ouml;dinger
</p>
<p>equation
</p>
<p>i
d
</p>
<p>dt
 = H (R.24)
</p>
<p>where H is a special operator known as the Hamiltonian.
</p>
<p>Another formulation of Rule 3 (p. 99 ff): (1) The only possible result of a mea-
</p>
<p>surement of A is one of the eigenvalues of the operator A that represents it. (2)
</p>
<p>If the state vector is | and a measurement of A is made, the probability that the
result will be the particular eigenvalue an is Prob (A = an; |) = | Pn | where
Pn =
</p>
<p>&sum;d(n)
j=1
</p>
<p>an, j
&rang; &lang;
</p>
<p>an, j
 is the projector onto the eigenspace of vectors with eigen-
</p>
<p>value an . The rule A = | A | is entirely equivalent to this pair of rules.
</p>
<p>R.2.8 M. LeBellac
</p>
<p>Quantum Physics (2006), pp. 96&ndash;108; postulates
</p>
<p>Postulate I: The space of states The properties of a quantum system are com-
</p>
<p>pletely defined by specification of its state vector |, which fixes the mathematical
representation of the physical state of the system. The state vector is an element of a
</p>
<p>complex Hilbert space H called the space of states. It will be convenient to choose
</p>
<p>| to be normalized that is, to have unit norm: 2 = |  = 1.
Postulate II: Probability amplitudes and probabilities If | is the vector repre-
</p>
<p>senting the state of a system and if | represents another physical state, there exists
a probability amplitude a ( &rarr; ) of finding | in state |, which is given by a
scalar product on H: a ( &rarr; ) =  |. The probability p ( &rarr; ) for the state
| to pass the test | is obtained by taking the squared modulus | ||2 of this
amplitude
</p>
<p>p ( &rarr; ) = |a ( &rarr; )|2 = | ||2 . (R.25)
</p>
<p>Postulate III: Physical properties and operators With every physical property
</p>
<p>A (energy, position, momentum, angular momentum, and so on) there exists an
</p>
<p>associated Hermitian operator A which acts in the space of states H: A fixes the
</p>
<p>mathematical representation of A.
</p>
<p>The WFC Postulate(wave function collapse), complement to postulate II If a
</p>
<p>system is initially in a state |, and if the result of an ideal measurement of A is an ,
then immediately after this measurement the system is in the state projected on the
</p>
<p>subspace of the eigenvalue an:
</p>
<p>| &rarr; | = Pn|&radic;|Pn|
. (R.26)</p>
<p/>
</div>
<div class="page"><p/>
<p>360 Appendix R: The Postulates of Quantum Mechanics
</p>
<p>Postulate IV: the evolution equation The time evolution of the state vector | (t)
of a quantum system is governed by the evolution equation
</p>
<p>i
d | (t)
</p>
<p>dt
= H(t) | (t) . (R.27)
</p>
<p>The Hermitian operator H is called the Hamiltonian.
</p>
<p>Postulate IV&prime;: the evolution operator, alternative to postulate IV The state vector
| (t) at time t is derived from the state vector | (t0) at time t0 by applying an
unitary operator U (t, t0), called the evolution operator:
</p>
<p>| (t) = U (t, t0) | (t0) . (R.28)
</p>
<p>R.2.9 G. M&uuml;nster
</p>
<p>Quantentheorie (Quantum Theory) (2006), p. 84; postulates (translated from the
</p>
<p>German)
</p>
<p>I Pure states are represented by normalized vectors (or rays) of a complex Hilbert
</p>
<p>space.
</p>
<p>Superposition principle: Each vector corresponds to a possible pure state.
</p>
<p>II Self-adjoint operators are associated with the observables of a system. The
</p>
<p>possible measurement values are the eigenvalues of the operator.
</p>
<p>III The expectation value of an observable A in the state | is given by
</p>
<p>A = | A | . (R.29)
</p>
<p>IV The time evolution of states is determined by the Schr&ouml;dinger equation:
</p>
<p>i
&part;
</p>
<p>&part;t
| = H | (R.30)
</p>
<p>where H is the Hamiltonian.
</p>
<p>V If the observable A is measured on a system in the state | and the measured
value a is found, the system changes by the measurement to the corresponding
</p>
<p>eigenstate |a (state reduction).
</p>
<p>R.2.10 W. Nolting
</p>
<p>Quantenmechanik, Teil 1: Grundlagen (Quantum Mechanics, Part 1: Fundamentals)
</p>
<p>(1992), p. 181ff; postulates (translated from the German)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix R: The Postulates of Quantum Mechanics 361
</p>
<p>1. Postulate: Measuring device for a certain physical quantity (observable) &hArr;
linear Hermitian operator.
</p>
<p>2. Postulate: Pure state of the quantum system &hArr; Hilbert vector.
3. Postulate: Measurement = Interaction between system and apparatus&hArr; appli-
</p>
<p>cation of the operator A onto the state | : A | = &sum;
&int;
</p>
<p>ai |ai  ai | filter&rarr;a j
&rang; &lang;
</p>
<p>a j | .
4. Postulate: Measurement results &hArr; eigenvalues ai of the operator A.
5. Postulate: Measurement probability for ai &hArr; w(ai |) =
</p>
<p>&lang;a j |
2.
</p>
<p>(The dynamics is not given in the form of postulates.)
</p>
<p>R.2.11 A.I.M. Rae
</p>
<p>Quantum Mechanics (2008), p. 68ff; postulates
</p>
<p>Postulate 1 For every dynamical system there exists a wave function that is a
</p>
<p>continuous, square-integrable, single-valued function of the parameters of the system
</p>
<p>and of time, and from which all possible predictions about the physical properties of
</p>
<p>the system can be obtained.
</p>
<p>Postulate 2 Every dynamical variable may be represented by a Hermitian oper-
</p>
<p>ator whose eigenvalues represent the possible results of carrying out a measurement
</p>
<p>of the value of the dynamical variable. Immediately after such a measurement, the
</p>
<p>wave function of the system is identical to the eigenfunction corresponding to the
</p>
<p>eigenvalue obtained as a result of measurement.
</p>
<p>Postulate 3 The operators representing the position and momentum of a particle
</p>
<p>are r and&minus;i&nabla;, respectively. Operators representing other dynamical quantities bear
the same functional relation to these, as do the corresponding classical quantities to
</p>
<p>the classical position and momentum variables.
</p>
<p>Postulate 4 When a measurement of a dynamic variable represented by a Her-
</p>
<p>mitian operator Q, is carried out on a system whose wave function is , then the
</p>
<p>probability of the result being equal to a particular eigenvalue qm will be |am |2,
where  = &sum;n ann and the n are the eigenfunctions of Q corresponding to the
eigenvalues qn .
</p>
<p>R.2.12 H. Rollnik
</p>
<p>Quantentheorie I (Quantum Theory I) (2003), p. 212ff; axioms (translated from the
</p>
<p>German)
</p>
<p>&bull; State axiom: Physical states are described by the vectors of a Hilbert space H.
More precisely: Physical states are mapped injectively onto the rays of H.
</p>
<p>&bull; Observable axiom 1: Each physical observable A is represented by a linear
Hermitian operator A of the state space H (p. 224).</p>
<p/>
</div>
<div class="page"><p/>
<p>362 Appendix R: The Postulates of Quantum Mechanics
</p>
<p>&bull; Observable axiom 2: The expectation value A of A in the state  is given
by
</p>
<p>A =
&sum;
</p>
<p>i
</p>
<p>aiwi =
| A |
 | . (R.31)
</p>
<p>For  = 1, it holds that:
A = | A | . (R.32)
</p>
<p>Later on, Rollnik invokes: (1) Symmetry axiom: Physical symmetry groups are
</p>
<p>represented by unitary or anti-unitary operators, (2) Axiom of nonrelativistic quantum
</p>
<p>mechanics: For an N -particle system, the position operators Qi (t), i = 1, . . . , 3N
form a complete set of commuting observables. The same applies for the momentum
</p>
<p>operators Pi (t). The commutation relation
[
Pj (t), Qk(t)
</p>
<p>]
= 
</p>
<p>2
 jk holds.
</p>
<p>R.2.13 H. Schulz
</p>
<p>Physik mit Bleistift (Physics with a pencil) (2001), p. 302ff; postulates (translated
</p>
<p>from the German)
</p>
<p>I. The complete information about a quantum system is contained in a one-valued
</p>
<p>function  (x, t) &isin; C (the information carrier). x is a set of variables, one for each
degree of freedom. In general, one can write x = 1, 2, . . . with 1 := set of variables
for particle 1 and so on.
</p>
<p>II. A linear Hermitian operator A is associated with each observable. A table of
</p>
<p>such associations is a constituent of the postulate:
</p>
<p>Class. quantity Name in quantum m. Letter Space Action
</p>
<p>Position (1D) Position X  (x) X = x
Momentum (1D) Momentum p  (x) p = 
</p>
<p>i
&part;x
</p>
<p>Momentum (3D) Momentum p  (r) p = 
i
&nabla;
</p>
<p>Angular momentum Angular momentum L  (r) L = r &times; p = r &times; 
i
&nabla;
</p>
<p>Parity (3D) P  (r) P (r) =  (&minus;r)
Spin (1. comp.) x Two-component x =
</p>
<p>(
0 1
</p>
<p>1 0
</p>
<p>)
</p>
<p>Energy Hamiltonian H E.g.  (r) E.g. H = &minus; 2
2m
</p>
<p>&nabla;2 + V (r , t)
.
.
.
</p>
<p>.
</p>
<p>.
</p>
<p>.
.
.
.
</p>
<p>.
</p>
<p>.
</p>
<p>.
.
.
.
</p>
<p>III. Possible measurement values are the eigenvalues of A, obtainable by solving
</p>
<p>Aa = aa and requiring univalence and normalizability.
IV. The eigenstates of A are to be normalized according to</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix R: The Postulates of Quantum Mechanics 363
</p>
<p>&int;
dx &lowast;ab = ab ,  (a &minus; b)  , ab (&minus; ) ,  (a &minus; b)  (&minus; ) (R.33)
</p>
<p>depending on whether the index is in a discrete or continuous region of the spectrum
</p>
<p>of A. The actual state  of the system must always be normalized to one:
</p>
<p>&int;
dx | (x)|2 = 1. (R.34)
</p>
<p>V. The probability of obtaining a discrete measured value a and the probability
</p>
<p>density for continuous measured values a follow the same formula:
</p>
<p>P (a, t) =
&sum;
</p>
<p>
</p>
<p>|ca |2 =
&sum;
</p>
<p>
</p>
<p>
&int;
</p>
<p>dx &lowast;a (x, t)
</p>
<p>
2
</p>
<p>. (R.35)
</p>
<p>If  is continuous, then
&sum;
</p>
<p> is to be replaced by
&int;
</p>
<p>d.
</p>
<p>VI. The equation of motion of quantum mechanics is
</p>
<p>i = H, (R.36)
</p>
<p>where the operator H is given in the table in II. The equation also applies if H is
</p>
<p>time dependent (for example due to V (r , t); see table).
VII. Pauli exclusion principle: Under permutation of the sets of variables of two
</p>
<p>identical particles, one must require
</p>
<p> (1, 2, . . .) =  (2, 1, . . .) ; (R.37)
</p>
<p>negative sign for fermions, positive sign for bosons.
</p>
<p>R.2.14 F. Schwabl
</p>
<p>Quantum Mechanics, 3. ed. (2002), p. 40; axioms
</p>
<p>I. The state is described by the wavefunction  (x).
</p>
<p>II. The observables are represented by Hermitian operators A . . ., with functions
</p>
<p>of observables being represented by the corresponding functions of the operators.
</p>
<p>III. The expectation value of the observable represented by the operator A is
</p>
<p>given in the state  by A = (, A).
IV. The time evolution of the states is given by the Schr&ouml;dinger equation
</p>
<p>i
&part;
</p>
<p>&part;t
 = H; H = &minus; 
</p>
<p>2
</p>
<p>2m
&nabla;
</p>
<p>2 + V (x) . (R.38)</p>
<p/>
</div>
<div class="page"><p/>
<p>364 Appendix R: The Postulates of Quantum Mechanics
</p>
<p>V. If in a measurement of A the value an is found, the wavefunction changes to
</p>
<p>the corresponding eigenfunction n .
94
</p>
<p>From axioms II and III, it follows that the only possible results of a measurement
</p>
<p>of an observable are the eigenvalues of the corresponding operator A, and the prob-
</p>
<p>abilities are given by |cn|2, where cn are the expansion coefficients of  (x) in the
eigenfunctions of A. In particular, it follows that | (x)|2 is the probability density
for the position.
</p>
<p>R.2.15 N. Zettili
</p>
<p>Quantum Mechanics, Concepts and Applications (2009), p. 165ff; postulates
</p>
<p>Postulate 1: State of a system The state of any physical system is specified, at
</p>
<p>each time t , by a state vector |(t) in a Hilbert space H; |(t) contains (and serves
as the basis to extract) all the needed information about the system. Any superposition
</p>
<p>of state vectors as also a state vector.
</p>
<p>Postulate 2: Observables and operators To every physically measurable quan-
</p>
<p>tity A, called an observable or dynamical variable, there corresponds a linear Her-
</p>
<p>mitian operator A whose eigenfunctions form a complete basis.
</p>
<p>Postulate 3: Measurements and eigenvalues of operators The measurement
</p>
<p>of an observable A may be represented formally by the action of A on a state vector
</p>
<p>|(t). The only possible result of such a measurement is one of the eigenvalues an
(which are real) of the operator A. If the result of a measurement of A on a state
</p>
<p>|(t) is an , the state of the system immediately after the measurement changes to
|n:
</p>
<p>A |(t) = an |(t)
</p>
<p>where an = n |(t) . Note: an is the component of |(t) when projected onto
the eigenvector |n.
</p>
<p>Postulate 4: Probabilistic outcome of measurements
</p>
<p>Discrete spectra: When measuring an observable A of a system in a state |, the
probability of obtaining one of the nondegenerate eigenvalues an of the corresponding
</p>
<p>operator A is given by
</p>
<p>Pn (an) =
|n |)|2
 | =
</p>
<p>|an|2
 | (R.39)
</p>
<p>where |n is the eigenstate of A with eigenvalue an . If the eigenvalue an is
m-degenerate, Pn becomes
</p>
<p>94Should perhaps preferably be formulated as: The wavefunction has changed to the corresponding
</p>
<p>eigenfunction n .</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix R: The Postulates of Quantum Mechanics 365
</p>
<p>Pn (an) =
</p>
<p>m&sum;
</p>
<p>j=1
</p>
<p>
&lang;

</p>
<p>j
n |)
</p>
<p>
2
</p>
<p> | =
</p>
<p>m&sum;
</p>
<p>j=1
</p>
<p>a( j)n

2
</p>
<p> | . (R.40)
</p>
<p>The act of measurement changes the state of the system from | to |n. If the system
is already in an eigenstate |n of A, a measurement of A yields with certainty the
corresponding eigenvalue an : A |n = an |n.
</p>
<p>Continuous spectra: The relation (R.39), which is valid for discrete spectra, can
</p>
<p>be extended to determine the probability density that a measurement of A yields a
</p>
<p>value between a and a + da on a system which is initially in a state |:
</p>
<p>dP (a)
</p>
<p>da
= | (a)|
</p>
<p>2
</p>
<p> | =
| (a)|2
</p>
<p>&infin;&int;
&minus;&infin;
</p>
<p>| (a&prime;)|2 da&prime;
. (R.41)
</p>
<p>For instance, the probability density for finding a particle between x and x + dx is
given by dP(x)/dx = | (x)|2 /  |.
</p>
<p>Postulate 5: Time evolution of a system The time evolution of the state vector
</p>
<p>|(t) of a system is governed by the time-dependent Schr&ouml;dinger equation
</p>
<p>i
&part; |(t)
</p>
<p>&part;t
= H(t) |(t) (R.42)
</p>
<p>where H is the Hamiltonian operator corresponding to the total energy of the system.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix S
</p>
<p>System and Measurement: Some Concepts
</p>
<p>We compile in this appendix some common notations concerning quantum systems
</p>
<p>and measurements.
</p>
<p>S.1 System: Isolated, Closed, Open
</p>
<p>In the following, we consider the relationship between a system S and its environment
</p>
<p>U .
</p>
<p>Here, a system is that part of the universe which we are examining. It must not
</p>
<p>necessarily be separated from the rest of the real universe in reality; there can also
</p>
<p>be imagined boundaries. Everything that lies outside these borders is called the
</p>
<p>system&rsquo;s surroundings or environment.95 The individual parts of a system have to
</p>
<p>interact somehow with each other. Examples of systems are an atom, a pendulum or
</p>
<p>the Earth&rsquo;s ecosystem. Specifically, a quantum system is everything that allows for
</p>
<p>a consistent dynamic description in terms of quantum mechanics.
</p>
<p>There are different types of interactions between system and environment. The
</p>
<p>nomenclature is derived from thermodynamics, where it has a well-defined meaning.
</p>
<p>In quantum mechanics, the situation is somewhat less consistent.
</p>
<p>Thermodynamics
</p>
<p>In an isolated system, there is no exchange of matter and energy (work, heat) with
</p>
<p>the surroundings. Hence, the total energy and mass stay constant. In a closed system,
</p>
<p>there is no exchange of matter with the environment, but only of energy. Accordingly,
</p>
<p>the total mass is constant, but not the total energy. In an open system, the system
</p>
<p>boundaries are permeable to matter and energy exchange; neither energy nor mass
</p>
<p>are constant.
</p>
<p>Of course, the terms &lsquo;isolated&rsquo; and &lsquo;closed&rsquo; are nearly always approximations. In
</p>
<p>particular, there is no &lsquo;real&rsquo; isolated system (perhaps apart from the entire universe);
</p>
<p>95If necessary, the &lsquo;measuring apparatus&rsquo; can be introduced as mediator between system and envi-
</p>
<p>ronment.
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>367</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>368 Appendix S: System and Measurement: Some Concepts
</p>
<p>this is prevented in any case by the ubiquitous gravitational field. As an approxi-
</p>
<p>mation, we can of course consider specific systems for some period of time to be
</p>
<p>isolated; for example, a Thermos bottle or its contents.
</p>
<p>An example of a closed system is the earth, at least to a good approximation: There
</p>
<p>is an energy exchange with its environment (incoming solar radiation, radiation from
</p>
<p>the Earth into space), but no significant transfer of matter.
</p>
<p>Examples of open systems are the ecological and the economic systems of a
</p>
<p>region. In this sense, also a human being or any living being is an open system;
</p>
<p>continually, matter and energy are taken up and given off.
</p>
<p>For the environment, in thermodynamics there exists the term reservoir (an envi-
</p>
<p>ronment with an infinite number of degrees of freedom) and (heat or thermal) bath
</p>
<p>(a reservoir which is in thermal equilibrium).
</p>
<p>Quantum Mechanics
</p>
<p>Even in thermodynamics, where the terms &lsquo;isolated&rsquo; and &lsquo;closed&rsquo; are properly
</p>
<p>defined, one occasionally finds blurred formulations. A quote from the internet:
</p>
<p>&ldquo;A closed system is a system in the state of being isolated from its surrounding envi-
</p>
<p>ronment.&rdquo;96 In quantum mechanics, these two terms are often even used interchange-
</p>
<p>ably. On the other hand, one can find also the distinction based on thermodynamics.
</p>
<p>An isolated system (sometimes called a totally isolated system) is completely
</p>
<p>decoupled from its environment. In particular, its total energy is constant, which
</p>
<p>means that the Hamiltonian H is not time dependent. The complete separation from
</p>
<p>the environment also means that there must be no entanglement between system and
</p>
<p>environment.
</p>
<p>If the environment is acting through external forces on the system, and one can
</p>
<p>formulate the dynamics of the system in terms of a possibly time-dependent Hamil-
</p>
<p>tonian, the system is called closed. It is commonly assumed that there is no feedback
</p>
<p>from the system onto the environment, i.e. the interactions environment&mdash;system are
</p>
<p>a one-way street.
</p>
<p>A system is called open if interactions and entanglements between system and
</p>
<p>environment are allowed (to and from). Usually, it is assumed that the aggregate (sys-
</p>
<p>tem + environment) is isolated or at least closed, and that its dynamics is described
by a Hamiltonian.
</p>
<p>S.2 Measurement
</p>
<p>Measurement
</p>
<p>By measurement, we understand an operation on a system which determines the
</p>
<p>values of one or more physical variables of the system immediately before the mea-
</p>
<p>surement in the form of distinct and storable numbers (see also Chap. 14).
</p>
<p>96See e.g. http://en.wikidoc.org/index.php/Closed_system (November 2011) as one of many
</p>
<p>references.</p>
<p/>
<div class="annotation"><a href="http://en.wikidoc.org/index.php/Closed_system">http://en.wikidoc.org/index.php/Closed_system</a></div>
</div>
<div class="page"><p/>
<p>Appendix S: System and Measurement: Some Concepts 369
</p>
<p>Fig. S.1 Ideal measurement
</p>
<p>of a right circular-polarized
</p>
<p>photon with regard to linear
</p>
<p>polarization
</p>
<p>v
</p>
<p>hr
</p>
<p>Classically, a value of a physical quantity A is measured which already exists
</p>
<p>before the measurement is made (pre-existing value). In quantum mechanics, this
</p>
<p>is the case only if the system is initially in an eigenstate of the measured observ-
</p>
<p>able; otherwise there is no unique observable value before the measurement.97 The
</p>
<p>transition from a superposition to a single state is called state reduction or collapse
</p>
<p>of the wavefunction. It is an irreversible evolution that characterizes a direction in
</p>
<p>time (except for the case that the initial state is already an eigenstate of the operator,
</p>
<p>i.e. there is no initial superposition).
</p>
<p>Ideal Measurement, QND
</p>
<p>Actual measurements on quantum objects often destroy them or make them disappear
</p>
<p>for the observer. They can therefore be carried out only once.98 Other measurements
</p>
<p>influence the objects so strongly that after the measurement or through the measure-
</p>
<p>ment, they take on a different value of the measured physical quantity.99
</p>
<p>These types of measurements (also known as real measurements) are common in
</p>
<p>practice,100 but for theoretical considerations it is useful to consider ideal measure-
</p>
<p>ments.101 An ideal measurement is non-destructive and recoilless. In other words,
</p>
<p>an ideal measurement affects the system so little that the repetition of such a mea-
</p>
<p>surement within a short time interval 102 gives the same result.
</p>
<p>As an example, we consider in Fig. S.1 a circular-polarized photon |r incident on a
polarizing beam splitter. The photon (irreversibly) changes to a linear-polarized state,
</p>
<p>say |h. A further measurement of this state again yields the same result. Therefore,
measurements of this kind are also called quantum non-demolition measurements,
</p>
<p>QND.103
</p>
<p>97We assume here that quantum mechanics is complete and there are no hidden variables.
98Example 1: A photon triggers a photomultiplier. It is absorbed and its energy is converted into
</p>
<p>an electrical signal. Example 2: An electron falls on a photographic plate and disappears among all
</p>
<p>the other electrons.
99Example: Measurement of the momentum of a neutron by observation of a recoil proton, which
</p>
<p>changes the momentum of the neutron during the interaction.
100Moreover, in these cases the probability statements concerning the measurement results also
</p>
<p>apply.
101Older designation by Pauli: measurement of the first kind (ideal) and the second kind (real).
102This means in such a short time interval that external influences cannot make themselves felt.
103The term &lsquo;non demolition&rsquo; does not mean, however, that the wavefunction does not collapse.</p>
<p/>
</div>
<div class="page"><p/>
<p>370 Appendix S: System and Measurement: Some Concepts
</p>
<p>We speak of an ideal measurement of a physical quantity A when the system is
</p>
<p>transferred by the measurement to an eigenstate of A.104 If the system, for example,
</p>
<p>is in a superposition of energy eigenstates |En, then an ideal measurement of the
energy will yield one of the the values EN and the system will be transferred into the
</p>
<p>eigenstate |EN . With a continuous quantity, of course, we can carry out only more
or less ideal measurements (loosely formulated). Moreover, in the discrete case, also,
</p>
<p>one will not always be able to measure the exact spectrum in practice, because of
</p>
<p>the limited resolution of the detector or other instrumental limitations; here also, one
</p>
<p>often has to make do with only approximately ideal measurements.
</p>
<p>Preparation
</p>
<p>The term preparation refers to an operation which is intended to impose a given
</p>
<p>(initial) state on the system. The system (or the ensemble,105 if one prefers) is thus
</p>
<p>forced into a certain state after the operation, while by a measurement the state of the
</p>
<p>system immediately before the measurement is probed. These different objectives
</p>
<p>are reflected in the fact that a preparation does not yield unique and storable numbers,
</p>
<p>in contrast to a measurement. But otherwise, ideal measurements and preparations
</p>
<p>share many properties, including the fact that both operations are nondestructive.
</p>
<p>Therefore, there are different opinions in the literature about the relationship between
</p>
<p>preparation and ideal measurement, ranging from &ldquo;It is important to distinguish
</p>
<p>between measurement and preparation.&rdquo; through &ldquo;Not all the processes of preparation
</p>
<p>are measurements in the traditional sense.&rdquo; to &ldquo;Preparation is of course only one form
</p>
<p>of non-destructive measurement.&rdquo; (all quotes from the literature).
</p>
<p>For example, consider the setup shown in Fig. S.1. Each measurement of the
</p>
<p>polarization by the second polarizing beam splitter necessarily gives |h. So one can
understand this as indicating that the first polarizing beam splitter has prepared the
</p>
<p>system. This particular preparation is not a measurement&mdash;at least not in the sense
</p>
<p>that the result will be recorded before the following polarization measurement is
</p>
<p>made.
</p>
<p>The following definition summarizes the situation: Let A be an observable with
</p>
<p>eigenvalues ai (discrete spectrum). An operation on a set E of physical systems is
</p>
<p>called preparation (state preparation) for A if it leads to a division of E into subsets
</p>
<p>Em such that for each m, a measurement of A immediately following the preparation
</p>
<p>is guaranteed to yield the result am for each system in the subset Em . If the operation
</p>
<p>is also a true measurement,106 it is called an ideal measurement of A.
</p>
<p>104For simplicity we assume that there is no degeneracy.
105The use of the word &lsquo;ensemble&rsquo; does not mean that physical variables &lsquo;have&rsquo; values that are
</p>
<p>distributed in an unknown way among the members of the ensemble. It is more of a code word that
</p>
<p>reminds us that in the pragmatic or instrumentalist approach, the predictions of the theory concern
</p>
<p>only the dispersion of the results of repeated measurements.
106That is, if a number is determined and stored.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix S: System and Measurement: Some Concepts 371
</p>
<p>r h
</p>
<p>v
</p>
<p>r
</p>
<p>v
</p>
<p>h
</p>
<p>Fig. S.2 Nonselective (left) and selective (right) measurements
</p>
<p>Indirect Quantum Measurement
</p>
<p>To measure properties of a quantum object Q, one may allow another quantum object
</p>
<p>S (called the quantum probe) to interact with Q in a suitable way. S is then measured
</p>
<p>by a usual measuring apparatus.
</p>
<p>Continuous Measurement
</p>
<p>If one measures a system repeatedly, thereby letting the intervals between the times
</p>
<p>of measurement approach zero, one speaks (in the limiting case107) of a continuous
</p>
<p>measurement. Since in an ideal measurement, repeating the measurement within a
</p>
<p>sufficiently short time interval guarantees the same result, a continuous measurement
</p>
<p>may inhibit (under appropriate circumstances) the system from taking a different
</p>
<p>state. This is the quantum Zeno effect; the topic is formulated compactly in the
</p>
<p>sentence, &lsquo;a watched pot never boils&rsquo;. More details are given in Appendix L, Vol. 1.
</p>
<p>Selective Measurement
</p>
<p>Given an observable A with a discrete non-degenerate spectrum; a measurement that
</p>
<p>selects only one of the eigenstates and does not register any other states is called a
</p>
<p>selective measurement (also known as filtering). Generalized, this means that if we
</p>
<p>have an initial ensemble and can split it by means of the measurement into different
</p>
<p>sub-ensembles, each of which would yield a different measurement result, then we
</p>
<p>speak of a selective measurement. If we mix the subsets after the measurement and
</p>
<p>then process them further (or do not select from the beginning), we refer to a non-
</p>
<p>selective measurement; cf. Fig. S.2.
</p>
<p>107As an idealization, this limit is achievable, but not in reality, since every measurement takes a
</p>
<p>certain time.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix T
</p>
<p>Recaps and Outlines
</p>
<p>In this chapter, we compile some material which is fundamental for the discussion
</p>
<p>of relativistic quantum theory and of relativistic quantum field theory in Vols. 1 and
</p>
<p>2. The topics are special relativity, classical field theory and electrodynamics. We
</p>
<p>limit ourselves to issues actually needed for further discussion. We begin with a short
</p>
<p>comparison of the discrete and continuous description of functions.
</p>
<p>T.1 Discrete - Continuous
</p>
<p>There are two ways to describe a system. One can assume that it lives in either a
</p>
<p>finite or an infinite domain. Both versions occur, and to avoid mistakes one has to
</p>
<p>take care not to mingle them. This is made a little bit harder by a certain nonchalant
</p>
<p>way of notation. Therefore, here some words about the issue.
</p>
<p>We start with the case that the system is confined to a finite volume (which
</p>
<p>may be arbitrarily large) with impermeable walls (infinite potential). Then we have
</p>
<p>something like the potential well (see Vol. 1 Chap. 5), and we know that the values of
</p>
<p>the momentum are not arbitrary but quantized, i.e., multiples of a basic wave length
</p>
<p>(in fact, of the half wave length). So we can count the allowed momenta and write
</p>
<p>them as e.g. kn , n = 0,&plusmn;1,&plusmn;2, . . ., hence the name &lsquo;discrete case&rsquo;. In addition,
certain properties of the system are typically given by sums.
</p>
<p>In contrast, if the system lives in an infinite volume, there is no basic wave length,
</p>
<p>i.e., every momentum is allowed; this is the continuous case. Thus, certain properties
</p>
<p>of the system are typically given by integrals, not by sums.
</p>
<p>Now here enters the said nonchalant way of notation. In the discrete case, the
</p>
<p>sum over all possible plane waves should read
&sum;&infin;
</p>
<p>n=&minus;&infin; e
&minus;iknx or something like that,
</p>
<p>but what one finds in literally all textbooks is a sort of shorthand notation, namely&sum;
k e
</p>
<p>&minus;ikx. The summation &lsquo;index&rsquo; k indicates that we have the discrete case and
means summation over all allowed momenta.
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>373</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>374 Appendix T: Recaps and Outlines
</p>
<p>Sometimes it is convenient to switch between the two ways of description. We
</p>
<p>compile here a few formulas. Essentially, they are based on the fact that each allowed
</p>
<p>k-value occupies a volume (2)3 /V in reciprocal space.
</p>
<p>1. The free solutions of the Klein&ndash;Gordon equation read in the case of a finite
</p>
<p>volume V
</p>
<p> (x) =
&sum;
</p>
<p>k
</p>
<p>1&radic;
2Vk
</p>
<p>(
ake
</p>
<p>i(kr&minus;k t) + a&dagger;ke&minus;i(kr&minus;k t)
)
</p>
<p>(T.1)
</p>
<p>where the sum runs over all allowed discrete values of k. The continuous variant
</p>
<p>reads :
</p>
<p> (x) = 1
(2)3/2
</p>
<p>&int;
d3k&radic;
2k
</p>
<p>(
ake
</p>
<p>i(kr&minus;k t) + a&dagger;ke&minus;i(kr&minus;k t)
)
. (T.2)
</p>
<p>2. It may happen that one has to calculate an integral like
&int;
</p>
<p>d3x eikx. Here one
</p>
<p>has to distinguish if it is about the continuous case or the discrete case. Usually,
</p>
<p>this is not noted by writing kn or something like that, but is determined by the
</p>
<p>context. The result reads108
</p>
<p>&int;
d3x eikx =
</p>
<p>{
k,0 &middot; V
</p>
<p> (k) &middot; (2)3 for the
discrete
</p>
<p>continuous
case. (T.4)
</p>
<p>3. In the discrete case holds
</p>
<p>&sum;
</p>
<p>k
</p>
<p>eik(x&minus;y) = V  (x &minus; y) (T.5)
</p>
<p>and in the continuous case
</p>
<p>&int;
d3k eik(x&minus;y) = (2)3  (x &minus; y) . (T.6)
</p>
<p>4. As the examples show, one can skip back and forth between &lsquo;discrete&rsquo; and
</p>
<p>&lsquo;continuous&rsquo; by certain replacements. Cum grano salis holds
</p>
<p>&sum;
</p>
<p>k
</p>
<p>&hArr;
&int;
</p>
<p>d3k ; V &hArr; (2)3 ; k,0 &hArr;  (k) . (T.7)
</p>
<p>108Due to the different factors V and (2)3, the use of the generalized Kronecker symbol (a, b)
</p>
<p>introduced in Vol. 1, Chap. 12:
</p>
<p>(a, b) =
{
</p>
<p>ab
(a &minus; b) for
</p>
<p>a, b discrete
</p>
<p>a, b continuous
(T.3)
</p>
<p>does not facilitate the formulation considerably.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix T: Recaps and Outlines 375
</p>
<p>T.1.1 Exercises and Solutions
</p>
<p>1. Prove (T.4).
</p>
<p>Solution: In the discrete case, the integration volume V is finite. The momentum
</p>
<p>k has only discrete values. We first consider the one dimensional case where V
</p>
<p>is the distance between say 0 and L x (see also Vol. 1 Chap. 5). It follows
</p>
<p>&int; Lx
0
</p>
<p>dx eikx x =
[
</p>
<p>eikx x
</p>
<p>ikx
</p>
<p>]Lx
</p>
<p>0
</p>
<p>= e
ikx Lx &minus; 1
</p>
<p>ikx
for kx 	= 0 ;
</p>
<p>&int; Lx
0
</p>
<p>dx eikx x = Lx for kx = 0.
</p>
<p>(T.8)
</p>
<p>The allowed wave lengths are given by n 
2
= L x . With = 2k follows n kx = L x
</p>
<p>or kx = nL x . This yields
</p>
<p>&int; L x
0
</p>
<p>dx eikx x = e
i n
</p>
<p>Lx
Lx &minus;1
</p>
<p>i n
Lx
</p>
<p>= ein&minus;1
i n
</p>
<p>Lx
</p>
<p>= 1&minus;1
i n
</p>
<p>Lx
</p>
<p>= 0 for kx 	= 0 or n 	= 0
&int; L x
</p>
<p>0
dx eikx x = L x for n = 0
</p>
<p>(T.9)
</p>
<p>or &int; L x
0
</p>
<p>dx eikx x = kx ,0 &middot; L x (T.10)
</p>
<p>Thus, we have in three dimensions
</p>
<p>&int;
</p>
<p>V
dx eikx =
</p>
<p>&int; Lx
0
</p>
<p>dx eikx x &middot;
&int; L y
</p>
<p>0
dx eiky y &middot;
</p>
<p>&int; Lz
0
</p>
<p>dx eikz z = kx ,0 &middot;Lx &middot;ky ,0 &middot;L y &middot;kz ,0 &middot;Lz = k,0 &middot;V . (T.11)
</p>
<p>In the continuous case, we use the definition of the delta function and obtain
</p>
<p>immediately &int;
d3x eikx = (2)3  (k) . (T.12)
</p>
<p>T.2 Special Relativity
</p>
<p>To define the notation and for the sake of completeness, we compile here some
</p>
<p>elements of special relativity (SR).109
</p>
<p>109SR uses special notations and conventions which on the first view perhaps seem to be a little bit
</p>
<p>strange. But actually, they are sophisticated, perfectly adapted to the purpose and indispensable for
</p>
<p>topics like Quantum Field Theory.</p>
<p/>
</div>
<div class="page"><p/>
<p>376 Appendix T: Recaps and Outlines
</p>
<p>T.2.1 Lorentz Boost and Four-Vectors
</p>
<p>SR describes how the coordinates of an event in two inertial systems I and I are
</p>
<p>related. If I is moving relative to the frame I with velocity v along the x-axis, then
</p>
<p>the relation is given by the following Lorentz boost (in x-direction)
</p>
<p>t = 
(
t &minus; vx
</p>
<p>c2
</p>
<p>)
</p>
<p>x =  (x &minus; vt)
y = y ; z = z
</p>
<p>(T.13)
</p>
<p>where  =
(
1 &minus; 2
</p>
<p>)&minus;1/2
and  = v/c.
</p>
<p>In SR, time and space are on an equal footing. Thus, it is plausible to extend the
</p>
<p>notion of the three-dimensional position vector to a four-dimensional vector which
</p>
<p>also includes the time. These four-vectors (4-vectors) which are essential ingredients
</p>
<p>of SR have one time-like and three space-like components. We write the coordinates
</p>
<p>of an event in space and time as
</p>
<p>x0 = ct ; x1 = x ; x2 = y ; , x3 = z (T.14)
</p>
<p>or
</p>
<p>x =
(
x0, x1, x2, x3
</p>
<p>)
. (T.15)
</p>
<p>Note that these are upper indices and not powers. x is the prototype of a 4-vector.
</p>
<p>A remark on notation: In general, 4-vectors are displayed in italic script and
</p>
<p>3-vectors in bold italic. Thus
</p>
<p>x =
(
x0, x1, x2, x3
</p>
<p>)
=
</p>
<p>(
x0, x
</p>
<p>)
. (T.16)
</p>
<p>x0 is always the time-component (x0 = ct). In addition, the components of a 4-vector
are listed with a Greek index like ,  or ; the components of a 3-vector are usually
</p>
<p>labelled with a Roman index from i to n.
</p>
<p>As in (T.15), 4-vectors are often written as row vectors which improves the read-
</p>
<p>ability of texts. However, if one wants to perform matrix calculations, it is better to
</p>
<p>think of x as a column vector. This is clearly seen by writing the Lorentz transfor-
</p>
<p>mation (T.13) in matrix form. On the left and the right there are the components of
</p>
<p>the four-vector x and x . Thus we have
</p>
<p>

</p>
<p>x0
</p>
<p>x1
</p>
<p>x2
</p>
<p>x3
</p>
<p>
 =
</p>
<p>

</p>
<p> &minus; 0 0
&minus;  0 0
</p>
<p>0 0 1 0
</p>
<p>0 0 0 1
</p>
<p>

</p>
<p>

</p>
<p>x0
</p>
<p>x1
</p>
<p>x2
</p>
<p>x3
</p>
<p>
 . (T.17)
</p>
<p>We denote the transformation matrix by 	 with elements 	 ( = 0, 1, 2, 3 labels
the rows,  = 0, 1, 2, 3 the columns). Apparently, 	 is symmetrical. With the usual</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix T: Recaps and Outlines 377
</p>
<p>notation for matrix multiplication, we can write the Lorentz transformation (T.17) as
</p>
<p>x =
3&sum;
</p>
<p>=0
	 x
</p>
<p> ;  = 0, 1, 2, 3. (T.18)
</p>
<p>In SR, there is second kind of vector whose prototype is the vector of derivatives
</p>
<p>(
&part;
</p>
<p>c&part;t
,&nabla;
</p>
<p>)
=
</p>
<p>(
&part;
</p>
<p>c&part;t
,
&part;
</p>
<p>&part;x
,
&part;
</p>
<p>&part;y
,
&part;
</p>
<p>&part;z
</p>
<p>)
=
</p>
<p>(
&part;
</p>
<p>&part;x0
,
</p>
<p>&part;
</p>
<p>&part;x1
,
</p>
<p>&part;
</p>
<p>&part;x2
,
</p>
<p>&part;
</p>
<p>&part;x3
</p>
<p>)
. (T.19)
</p>
<p>Under a Lorentz-transformation, this vector does not transform as given in ( T.18),
</p>
<p>but as
</p>
<p>&part;
</p>
<p>&part; x
=
</p>
<p>3&sum;
</p>
<p>=0
</p>
<p>(
&part;x
</p>
<p>&part; x
</p>
<p>)
&part;
</p>
<p>&part;xv
(T.20)
</p>
<p>which expression may be written using the common short-hand notation &part;
&part;x
</p>
<p>= &part;
as
</p>
<p>&part; =
3&sum;
</p>
<p>=0
</p>
<p>(
&part;x
</p>
<p>&part; x
</p>
<p>)
&part; =
</p>
<p>3&sum;
</p>
<p>=0
	
</p>
<p> &part; (T.21)
</p>
<p>where 	
 = &part;x
</p>
<p>&part; x
is the inverse of 	 = &part; x
</p>
<p>
</p>
<p>&part;x
or in matrix notation:
</p>
<p>	 = (	) =
</p>
<p>

</p>
<p> &minus; 0 0
&minus;  0 0
</p>
<p>0 0 1 0
</p>
<p>0 0 0 1
</p>
<p>
 ; 	
</p>
<p>&minus;1 =
(
	
</p>
<p>
)
=
</p>
<p>

</p>
<p>  0 0
</p>
<p>  0 0
</p>
<p>0 0 1 0
</p>
<p>0 0 0 1
</p>
<p>
 . (T.22)
</p>
<p>Note that 	 and 	&minus;1 only differ in the sign of  which is essentially the relative
velocity of the two reference frames.
</p>
<p>We now generalize (T.18) and (T.21) and define general 4-vectors by means of
</p>
<p>their behavior under Lorentz transformations. A general 4-vector with components
</p>
<p>a0, a1, a2, a3 has to fulfill
</p>
<p>a =
3&sum;
</p>
<p>=0
</p>
<p>(
&part; x
</p>
<p>&part;x
</p>
<p>)
a =
</p>
<p>3&sum;
</p>
<p>=0
	 a
</p>
<p> (T.23)
</p>
<p>and a general 4-vector with components a0, a1, a2, a3 has to fulfill
</p>
<p>a =
3&sum;
</p>
<p>=0
</p>
<p>(
&part;x
</p>
<p>&part; x
</p>
<p>)
a =
</p>
<p>3&sum;
</p>
<p>=0
	
</p>
<p> a . (T.24)
</p>
<p>As an example, the written-out (T.23) is found below in (T.25 ).</p>
<p/>
</div>
<div class="page"><p/>
<p>378 Appendix T: Recaps and Outlines
</p>
<p>Note that the 4-vectors of SR are defined by their behavior under Lorentz trans-
</p>
<p>formations, and not, as the &lsquo;usual&rsquo; 3-vectors, by the behavior under space transfor-
</p>
<p>mations. This leads among others to a different definition of the inner product. For
</p>
<p>this and some examples for 4-vectors see below.
</p>
<p>T.2.1.1 Contra- and Covariant Vectors
</p>
<p>In SR, we have two ways to define the components of a vector a, namely
(
a0, a1, a2 ,
</p>
<p>a3
)
</p>
<p>and (a0, a1, a2, a3). The names of the the two types are owed to the histor-
</p>
<p>ical heritage: contravariant vector for
(
a0, a1, a2, a3
</p>
<p>)
and covariant vector for
</p>
<p>(a0, a1, a2, a3). This naming is common, but a little bit unfortunate and a misnomer
</p>
<p>at least for two reasons.
</p>
<p>(1) A covariant transformation is a properly defined transformation in the SR; in
</p>
<p>this sense, both types of vectors transform covariantly.
</p>
<p>(2) Contravariant vector and covariant vector are not two different vectors, as
</p>
<p>maybe the names would suggest, but the very same vector a with different compo-
</p>
<p>nents, i.e., formulated in different coordinate systems. Take as an example the plane
</p>
<p>with a skewed coordinate system, i.e., the coordinate axes x1 and x2 enclose an angle
</p>
<p>	= 90. There are two ways to define the components of a vector a in such an oblique
system, simply since the parallel to one axis is not perpendicular to the other axis.
</p>
<p>&minus; For the first way, one drops a line from the tip of the vector, parallel to the x2
axis, onto the x1 axis, and another line, parallel to the x1 axis, onto the x2 axis. The
</p>
<p>intercepts of these lines with the axes are called the contravariant components of the
</p>
<p>vector, a1 and a2.
</p>
<p>&minus; For the second way, one drops a perpendicular from the tip of the vector onto the
x1 axis and another perpendicular from the tip of a onto the x2 axis. The intercepts
</p>
<p>are called covariant components of the vector, a1 and a2.
</p>
<p>In a Cartesian coordinate system, the perpendiculars onto one axis are parallel
</p>
<p>to the other. Thus, contravariant and covariant components coincide, a1 = a1 and
a2 = a2.
</p>
<p>To circumvent the unfortunate naming contra- and covariant, some textbooks
</p>
<p>prefer terms like upstairs and downstairs vector or something like that.
</p>
<p>T.2.1.2 Remarks on Notation
</p>
<p>The way to index terms in SR has great advantages; it is excellently adapted to the
</p>
<p>questions of SR, simple, concise and elegant. However, for the beginner it offers
</p>
<p>perhaps some difficulties.
</p>
<p>Labeling contra- and covariant 4-vectors Often it is not sufficient to label a vector
</p>
<p>simply a because it would be not clear if it is meant in its upstairs or downstairs
</p>
<p>formulation. It has become established to write a for a contravariant and a for
</p>
<p>a covariant vector. This could be confusing since  is an index which takes values</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix T: Recaps and Outlines 379
</p>
<p>0, 1, 2, 3. Thus, it is not clear whether a means the vector with its 4 components
</p>
<p>or just a single component (as e.g. in (T.23) and (T.24)). However, in general, the
</p>
<p>context clearly defines the meaning. If necessary for the sake of clarity, one can use
</p>
<p>the notation acov and a
con instead of just a.
</p>
<p>Labeling Lorentz transformations We are dealing with two types of vectors, co-
</p>
<p>and contravariant, distinguished by the position of the index. Thus, it is useful to have
</p>
<p>also upstairs and downstairs indices for operators (matrices, tensors) - the operators
</p>
<p>themselves should carry the information on which objects they act. The usual notation
</p>
<p>for a matrix like	 would not provide this information. Thus, we write	

 to make
</p>
<p>clear that this Lorentz transformation acts on a contravariant vector, and 	
 on a
</p>
<p>covariant vector. Note that the notation 	 and 	
 comes from tensor calculus.
</p>
<p>Since we use this special way of indexing operators only for these two terms, we
</p>
<p>will not discuss it further. Just imagine that the purpose is to make clear from the
</p>
<p>notation what the terms are supposed to act on: 	 on a vector of type a
 , 	
</p>
<p> on
</p>
<p>a vector of type a .
</p>
<p>Labeling 3-vectors We repeat that for 3-vectors covariant and contravariant com-
</p>
<p>ponents are the same and therefore, the position of the indices is irrelevant. In the
</p>
<p>context of our discussion, we can write e.g. for the Pauli matrices either (1,2,3)
</p>
<p>or
(
1,2,3
</p>
<p>)
.
</p>
<p>A certain caution may be required when dealing with the indices. For instance,
</p>
<p>consider the 4-momentum p with p =
(
</p>
<p>p0,p
)
</p>
<p>and p = (p0,&minus;p). Written out,
we have p =
</p>
<p>(
p0, p1, p2, p3
</p>
<p>)
and p = (p0, p1, p2, p3), i.e., p0 = p0 and
</p>
<p>p1 = &minus;p1 and so on. Now take the 3-momentum p. If we write the components
as p =
</p>
<p>(
p1, p2, p3
</p>
<p>)
, we have p = (p1, p2, p3) since for a 3-vector, contravariant
</p>
<p>and covariant components are equal, as said above. Thus, we would have for the
</p>
<p>4-vector p1 = &minus;p1 and for the 3-vector p1 = p1. One can avoid this ambiguity by
writing p =
</p>
<p>(
px , py .pz
</p>
<p>)
, for instance.
</p>
<p>T.2.2 Four-Vector Inner Product, Metric Tensor, Einstein
</p>
<p>Convention
</p>
<p>T.2.2.1 Four-Vector Inner Product
</p>
<p>Being a scalar, the inner product a &middot; b of two 4-vectors should not depend on the
reference frame, but should be invariant with respect to Lorentz transformations, i.e.,
</p>
<p>a &middot; b = a &middot; b. With (T.23), i.e.,
</p>
<p>a0 = 
(
a0 &minus; a1
</p>
<p>)
; a1 = 
</p>
<p>(
&minus;a0 + a1
</p>
<p>)
; a2 = a2 ; a3 = a3 (T.25)
</p>
<p>follows immediately that the &lsquo;familiar rule&rsquo; does not work:
&sum;3
</p>
<p>=0 a
b 	= &sum;3=0 a
</p>
<p>b, since on the l.h.s there are mixed terms like a0b1 which do not cancel and are not
</p>
<p>found on the r.h.s.. Thus, we make the ansatz</p>
<p/>
</div>
<div class="page"><p/>
<p>380 Appendix T: Recaps and Outlines
</p>
<p>a &middot; b = pa0b0 + qa &middot; b (T.26)
</p>
<p>with two yet to be determined constants p and q. We insert (T.25) into the equation
</p>
<p>a &middot; b = pa0b0 + qa &middot; b != a &middot; b = pa0b0 + qa &middot; b. (T.27)
</p>
<p>This yields
</p>
<p>a0b0
[
</p>
<p>p2 + q22
]
+ a0b1
</p>
<p>[
&minus;p2 &minus; q2
</p>
<p>]
+ a1b0
</p>
<p>[
&minus;p2 &minus; q2
</p>
<p>]
+
</p>
<p>a1b1
[
</p>
<p>p22 + q2
]
+ qa2b2 + qa3b3 != pa0b0 + qa1b1 + qa2b2 + qa3b3.
</p>
<p>(T.28)
</p>
<p>As mentioned before, the mixed terms have to vanish which leads to q = &minus;p.
Accordingly, since 2
</p>
<p>(
1 &minus; 2
</p>
<p>)
= 1, the prefactors of a0b0 (and of a1b1) are equal
</p>
<p>on both sides. Finally, to hold things simple, we choose p = 1 = &minus;q and obtain the
expression 110
</p>
<p>a &middot; b = a0b0 &minus; a &middot; b. (T.29)
</p>
<p>With contra- and covariant vectors, we have
</p>
<p>a &middot; b = ab = ab. (T.30)
</p>
<p>Inner products always involve a contra- and a covariant vector. It does not import
</p>
<p>which one of the two vectors is written as contravariant or as covariant.
</p>
<p>Thus, the definition of the inner product for two four-vectors in SR differs obvi-
</p>
<p>ously from that of Euclidean 3-vectors. Especially, this leads to another definition of
</p>
<p>the length of a 4-vector, namely
</p>
<p>a &middot; a = a0a0 &minus; a &middot; a = a0a0 &minus;
(
a1a1 + a2a2 + a3a3
</p>
<p>)
. (T.31)
</p>
<p>Note that this term is not positive definite.
</p>
<p>Let us stress once more that the inner product of two 4-vectors, defined in
</p>
<p>this way, does not depend on the reference frame, i.e., it is invariant under
</p>
<p>Lorentz transformations. As an example, we consider the length of the 4-vector
</p>
<p>x =
(
x0, x1,x2, x3
</p>
<p>)
=
</p>
<p>(
x0, x
</p>
<p>)
. It is given by
</p>
<p>x2 =
(
x0
)2 &minus; x &middot; x =
</p>
<p>(
x0
)2 &minus;
</p>
<p>[(
x1
)2 +
</p>
<p>(
x2
)2 +
</p>
<p>(
x3
)2]
</p>
<p>. (T.32)
</p>
<p>110One can equally well choose p = &minus;1 and q = 1 which is done in some books. Then, for
time-like and space-like vectors a holds a2 &lt; 0 and a2 &gt; 0. See below the discussion about
the metric tensor.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix T: Recaps and Outlines 381
</p>
<p>This expression is an invariant known as the spacetime interval. Four-vectors x with
</p>
<p>x2 &gt; 0 are called time-like vectors, with x2 &lt; 0 space-like vectors, and with
x2 = 0 light-like.111
</p>
<p>T.2.2.2 Metric Tensor
</p>
<p>We now look for the relation between the two types of four-vectors. We write (T.23)
</p>
<p>and (T.24) in the form acon = 	acon and acov = 	&minus;1acov . Our ansatz reads acov =
Gacon , where G is a 4 &times; 4-matrix yet to be determined. Since the transformation
should not depend on the frame of reference, we have also acov = Gacon . Thus,
starting with (T.23) yields
</p>
<p>acon = 	acon &rarr; Gacon = G	G&minus;1Gacon &rarr; acov = 	&minus;1acov = G	G&minus;1acov.
(T.33)
</p>
<p>In other words, G must fulfill the equation G	G&minus;1 = 	&minus;1 or G	 = 	&minus;1G. As it
turns out (see exercise), the simplest solution is given by
</p>
<p>G =
(
g
</p>
<p>)
=
</p>
<p>

</p>
<p>1 0 0 0
</p>
<p>0 &minus;1 0 0
0 0 &minus;1 0
0 0 0 &minus;1
</p>
<p>
 . (T.34)
</p>
<p>This object (also named ) is called metric tensor; it is at the heart of SR. Obviously,
</p>
<p>the metric tensor is symmetrical.
</p>
<p>Note that the other simplest and equivalent solution has reversed signs, g00 = &minus;1
and g11 = g22 = g33 = 1 which choice is made in some textbooks. In this context
one uses the term metric signature which gives the number of positive, negative and
</p>
<p>zero eigenvalues. For the metric tensor in SR, it is often denoted by (+,&minus;,&minus;,&minus;)
or (&minus;,+,+,+). Of course, it reflects the choice of the sign for the inner product
(T.29). The choice of the metric signature is theoretically inconsequential, but one
</p>
<p>has simply to choose one of the two alternatives for purposes of internal consistency.
</p>
<p>For later purposes, it is useful to introduce g = g . With (T.34), the connection
between the two types of four-vectors is given by
</p>
<p>a =
3&sum;
</p>
<p>v=0
ga
</p>
<p> (T.35)
</p>
<p>or explicitly in matrix form
</p>
<p>111The relativistic line element ds is given by ds2 =
(
dx0
</p>
<p>)2&minus;(dx)2 and the eigentime by d = 1
c
</p>
<p>ds.
</p>
<p>We see, that by construction the line element and the eigentime are Lorentz invariant quantities.</p>
<p/>
</div>
<div class="page"><p/>
<p>382 Appendix T: Recaps and Outlines
</p>
<p>

</p>
<p>a0
a1
a2
a3
</p>
<p>
 =
</p>
<p>

</p>
<p>1 0 0 0
</p>
<p>0 &minus;1 0 0
0 0 &minus;1 0
0 0 0 &minus;1
</p>
<p>

</p>
<p>

</p>
<p>a0
</p>
<p>a1
</p>
<p>a2
</p>
<p>a3
</p>
<p>
 =
</p>
<p>

</p>
<p>a0
</p>
<p>&minus;a1
&minus;a2
&minus;a3
</p>
<p>
 (T.36)
</p>
<p>or
</p>
<p>acon =
(
a0, a
</p>
<p>)
, acov =
</p>
<p>(
a0,&minus;a
</p>
<p>)
. (T.37)
</p>
<p>T.2.2.3 Einstein Summation Convention
</p>
<p>Since summations over the four indices  = 0, 1, 2, 3 as in (T.35) occur quite often
in SR, we adopt the extremely useful Einstein summation convention by which twice
</p>
<p>appearing indices are to be summed up (provided, one is &lsquo;upstairs&rsquo; and one &lsquo;down-
</p>
<p>stairs&rsquo;), thereby omitting the summation sign. Using this convention, the connection
</p>
<p>between co- and contravariant vectors (T.35) reads
</p>
<p>a = ga (T.38)
</p>
<p>and the inverse transformation is given by
</p>
<p>a = ga (T.39)
</p>
<p>with g = g . As is seen, we can lower or raise an index by inserting the metric
tensor.112
</p>
<p>With the summation convention, the inner product (T.29) is written as113
</p>
<p>a &middot; b = gab = ab = ab. (T.40)
</p>
<p>Finally, the behavior of contra- and covariant vectors under Lorentz transforma-
</p>
<p>tions (see (T.23) and ( T.24)) is written as
</p>
<p>a = 	 a ; a = 	 a . (T.41)
</p>
<p>T.2.2.4 Special Four-Vectors
</p>
<p>As stated above, a four-vector in SR transforms in a specific way under Lorentz
</p>
<p>transformations. It has four components, but its length is determined differently from
</p>
<p>an Euclidean vector. In SR, there is a bunch of common 4-vectors, as for instance
</p>
<p>position, the momentum or the potential.
</p>
<p>112This holds also for tensors, e.g. g	

 = 	.
</p>
<p>113Note that if the same index occurs upstairs and downstairs, it is used up by the summation;
</p>
<p>accordingly, it may be named arbitrarily (dummy index): ab
 = ab = ab.
</p>
<p>The inner product always involves one contravariant and one covariant vector.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix T: Recaps and Outlines 383
</p>
<p>It is clear that all problems in SR could be solved without invoking 4-vectors.114
</p>
<p>But they are a powerful tool which makes life very much easier. In fact, many
</p>
<p>problems would be nearly impossible to treat without the use of 4-vectors. On the
</p>
<p>one hand, equations between 4-vectors which hold in a particular inertial system
</p>
<p>are automatically valid in all systems. On the other hand, the 4-vector inner product
</p>
<p>is invariant and the same in all frames. So in treating a problem, one may choose
</p>
<p>the frame in which the problem appears in its simplest form. In addition, the use of
</p>
<p>4-vectors may give new insights. For example, the conservation of the 4-momentum
</p>
<p>includes the conservation of energy.
</p>
<p>Here are some examples for 4-vectors.
</p>
<p>(1) The position 4-vector or 4-position could be denoted by x = (ct, x) or x =
(x0, x). But in this notation it is not clear if we mean the contravariant or the covariant
</p>
<p>version. It is common practice to write x = (ct, x) though it is, as stated above, a
certain misuse of notation; the context has to clear if x means one component of
</p>
<p>the 4-vector or the whole 4-vector. So we write
</p>
<p>x = (x0, x) (T.42)
</p>
<p>and its inner product, called spacetime interval, is given by
</p>
<p>xx
 =
</p>
<p>(
x0
)2 &minus; x &middot; x. (T.43)
</p>
<p>(2) The energy-momentum 4-vector or 4-momentum is given by
</p>
<p>p =
(
</p>
<p>E
</p>
<p>c
,p
</p>
<p>)
. (T.44)
</p>
<p>The inner product reads
</p>
<p>p p
 = E
</p>
<p>2
</p>
<p>c2
&minus; p2 = m
</p>
<p>2c4 + c2p2
c2
</p>
<p>&minus; p2 = m2c2 (T.45)
</p>
<p>where m is the rest mass. Note that mc2 is an invariant. For objects with vanishing
</p>
<p>rest mass like the photon, we have p p
 = 0.
</p>
<p>(3) In connection with the 4-momentum we can also define the 4-wavenumber
</p>
<p>k = p/:
k =
</p>
<p>(
E
</p>
<p>c
,
p
</p>
<p>
</p>
<p>)
=
</p>
<p>(
c
,k
</p>
<p>)
. (T.46)
</p>
<p>The inner product of the two 4-vectors k and x is given by
</p>
<p>114The same holds for contra- and covariant vectors. Due to the relations (T.38) and (T.39), one
</p>
<p>could formulate the SR with one sort of index only, e.g. contravariant vectors only. But this would
</p>
<p>result in a quite cumbersome formalism without the transparency and elegance of the established
</p>
<p>method.</p>
<p/>
</div>
<div class="page"><p/>
<p>384 Appendix T: Recaps and Outlines
</p>
<p>kx = kx = kx = k0x0 &minus; kx = t &minus; kx (T.47)
</p>
<p>and it follows
</p>
<p>eikx = ei(t&minus;kx). (T.48)
</p>
<p>(4) The vector potential 4-vector is given by
</p>
<p>A =
(

</p>
<p>c
,A
</p>
<p>)
. (T.49)
</p>
<p>(5) The four-dimensional derivative operator or 4-gradient is given by
</p>
<p>&part; =
&part;
</p>
<p>&part;x
=
</p>
<p>(
1
</p>
<p>c
</p>
<p>&part;
</p>
<p>&part;t
,&nabla;
</p>
<p>)
= (&part;0,&nabla;) (T.50)
</p>
<p>and the contravariant form by
</p>
<p>&part; = &part;
&part;x
</p>
<p>=
(
</p>
<p>1
</p>
<p>c
</p>
<p>&part;
</p>
<p>&part;t
,&minus;&nabla;
</p>
<p>)
= (&part;0,&minus;&nabla;) . (T.51)
</p>
<p>The inner product (also called d&rsquo;Alembert operator) reads
</p>
<p>&part;2 = &part;&part; =
1
</p>
<p>c2
</p>
<p>&part;2
</p>
<p>&part;t2
&minus;&nabla;2. (T.52)
</p>
<p>Note that since &part; is an operator, one can not attribute a length to it.
</p>
<p>(6) The 4-current (4-current density, current density 4-vector) is defined as
</p>
<p>j = (c, j) (T.53)
</p>
<p>where  and j are the charge and current density. With j, the continuity equation
&part;
&part;t
</p>
<p>+&nabla; j = 0 reads
&part; j
</p>
<p> = 0. (T.54)
</p>
<p>(7) The sum of two 4-vectors is a 4-vector. An example is given by the difference
</p>
<p>of the 4-momentum and the 4-potential
</p>
<p>p &minus; q A =
(
</p>
<p>E
</p>
<p>c
,p
</p>
<p>)
&minus; q
</p>
<p>(

</p>
<p>c
,A
</p>
<p>)
=
</p>
<p>(
E &minus; q
</p>
<p>c
,p&minus; qA
</p>
<p>)
. (T.55)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix T: Recaps and Outlines 385
</p>
<p>T.2.3 Exercises and Solutions
</p>
<p>1. Show 	 &middot;	&minus;1 = 1.
Solution:
</p>
<p>	 &middot;	&minus;1 =
</p>
<p>

</p>
<p> &minus; 0 0
&minus;  0 0
</p>
<p>0 0 1 0
</p>
<p>0 0 0 1
</p>
<p>

</p>
<p>

</p>
<p>  0 0
</p>
<p>  0 0
</p>
<p>0 0 1 0
</p>
<p>0 0 0 1
</p>
<p>
 =
</p>
<p>

</p>
<p>2(1 &minus; )2 0 0 0
0 2(1 &minus; )2 0 0
0 0 1 0
</p>
<p>0 0 0 1
</p>
<p>
 . (T.56)
</p>
<p>It is  =
(
1 &minus; 2
</p>
<p>)&minus;1/2
and therefore 	 &middot;	&minus;1 = 1.
</p>
<p>2. Show
3&sum;
</p>
<p>=0
		
</p>
<p> =
3&sum;
</p>
<p>=0
</p>
<p>(
&part; x
</p>
<p>&part;x
</p>
<p>)(
&part;x
</p>
<p>&part; x
</p>
<p>)
=  (T.57)
</p>
<p>where  is the Kronecker symbol in the SR:
</p>
<p> =
{
</p>
<p>1
</p>
<p>0
for
</p>
<p> = 
 	=  . (T.58)
</p>
<p>Solution: This is the same question as in exercise 1, written out explicitly.
</p>
<p>3. Solve the equation G	 = 	&minus;1G and determine the two simplest solutions.
Solution: From G	 = 	&minus;1G we have
</p>
<p>

</p>
<p>g00 g01 g02 g03
g10 g11 g12 g13
g20 g21 g22 g23
g30 g31 g32 g33
</p>
<p>

</p>
<p>

</p>
<p> &minus; 0 0
&minus;  0 0
</p>
<p>0 0 1 0
</p>
<p>0 0 0 1
</p>
<p>
 =
</p>
<p>

</p>
<p>  0 0
</p>
<p>  0 0
</p>
<p>0 0 1 0
</p>
<p>0 0 0 1
</p>
<p>

</p>
<p>

</p>
<p>g00 g01 g02 g03
g10 g11 g12 g13
g20 g21 g22 g23
g30 g31 g32 g33
</p>
<p>
 . (T.59)
</p>
<p>Due to the block structure of 	, it is advantageous to define the matrices
</p>
<p>A =
(
g00 g01
g10 g11
</p>
<p>)
, B =
</p>
<p>(
g02 g03
g12 g13
</p>
<p>)
, C =
</p>
<p>(
g20 g21
g30 g31
</p>
<p>)
, D =
</p>
<p>(
g22 g23
g32 g33
</p>
<p>)
</p>
<p>(T.60)
</p>
<p>and
</p>
<p>S () = 
(
</p>
<p>1 
</p>
<p> 1
</p>
<p>)
. (T.61)
</p>
<p>Then, (T.59) reads
</p>
<p>(
A B
</p>
<p>C D
</p>
<p>)(
S (&minus;) 0
</p>
<p>0 1
</p>
<p>)
=
</p>
<p>(
S () 0
</p>
<p>0 1
</p>
<p>)(
A B
</p>
<p>C D
</p>
<p>)
. (T.62)
</p>
<p>Multiplying the matrices yields</p>
<p/>
</div>
<div class="page"><p/>
<p>386 Appendix T: Recaps and Outlines
</p>
<p>(
AS (&minus;) B
C S (&minus;) D
</p>
<p>)
=
</p>
<p>(
S () A S () B
</p>
<p>C D
</p>
<p>)
. (T.63)
</p>
<p>One sees immediately that the block matrix D remains undetermined. For B we
have
</p>
<p>B = S () B &rarr;
(
g02 g03
g12 g13
</p>
<p>)
= 
</p>
<p>(
1 
</p>
<p> 1
</p>
<p>)(
g02 g03
g12 g13
</p>
<p>)
= 
</p>
<p>(
g02 + g12 g03 + g13
g02 + g12 g03 + g13
</p>
<p>)
</p>
<p>(T.64)
</p>
<p>which leads to g02 = g03 = g12 = g13 = 0 or B = 0. Analogously we have
C = 0.
The remaining equation AS (&minus;) = S () A reads explicitly
</p>
<p>(
g00 g01
g10 g11
</p>
<p>)(
1 &minus;
&minus; 1
</p>
<p>)
=
</p>
<p>(
1 
</p>
<p> 1
</p>
<p>)(
g00 g01
g10 g11
</p>
<p>)
(T.65)
</p>
<p>or (
g00 &minus; g01 &minus;g00 + g01
g10 &minus; g22 &minus;g10 + g11
</p>
<p>)
=
</p>
<p>(
g00 + g10 g01 + g11
g00 + g10 g01 + g11
</p>
<p>)
. (T.66)
</p>
<p>The equation is apparently fulfilled for g11 = &minus;g00 and g10 = &minus;g01. Thus, the
result is
</p>
<p>G =
</p>
<p>

</p>
<p>g00 g01 0 0
</p>
<p>&minus;g01 &minus;g00 0 0
0 0 g22 g23
0 0 g32 g33
</p>
<p>
 . (T.67)
</p>
<p>We can choose all these entries arbitrarily. First, we want the off-diagonal ele-
</p>
<p>ments to be zero. Second, treating the space coordinates on a equal footing, we
</p>
<p>put g22 = g33 = &minus;g00. Finally, we choose as simplest case g00 = 1 and arrive at
</p>
<p>G =
(
g
</p>
<p>)
=
</p>
<p>

</p>
<p>1 0 0 0
</p>
<p>0 &minus;1 0 0
0 0 &minus;1 0
0 0 0 &minus;1
</p>
<p>
 . (T.68)
</p>
<p>The other simplest solution is given by the choice g00 = &minus;1 and g11 = g22 =
g33 = 1.
</p>
<p>4. We have acov = Gacon with G given in (T.34). Show that also holds acon =
Gacov .
</p>
<p>Solution: Obviously, it is G&minus;1 = G. Thus, from acov = Gacon follows acon =
G&minus;1acov = Gacov .
</p>
<p>5. Check that
4&sum;
</p>
<p>=0
gg
</p>
<p> =  (T.69)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix T: Recaps and Outlines 387
</p>
<p>6. Show that one can write
</p>
<p>a &middot; b = gab = ab (T.70)
</p>
<p>Solution: It is115
</p>
<p>ab = gab = agb = ab. (T.71)
</p>
<p>7. Show that &part;x
 = 4.
</p>
<p>Solution:
</p>
<p>&part;x
 = 4 due to &part;x = . (T.72)
</p>
<p>8. The current density 4-vector is given by
</p>
<p>j = (c, j) . (T.73)
</p>
<p>Calculate &part; j
 in terms of  and j.
</p>
<p>Solution:
</p>
<p>&part; j
 = &part;0 j0 + &part;k j k = &part;t+&nabla; j = + div j. (T.74)
</p>
<p>9. Regarding E and p as operators i&part;t and

</p>
<p>i
&nabla;, show that holds
</p>
<p>p = i&part; (and, of course, p = i&part;). (T.75)
</p>
<p>Solution:
</p>
<p>p =
(
</p>
<p>p0, pk
)
=
</p>
<p>(
E
</p>
<p>c
, p
</p>
<p>)
=
</p>
<p>(
1
</p>
<p>c
i&part;t ,
</p>
<p>
</p>
<p>i
&nabla;
</p>
<p>)
= i
</p>
<p>(
1
</p>
<p>c
&part;t ,&minus;&nabla;
</p>
<p>)
= i
</p>
<p>(
&part;0,&minus;&part;k
</p>
<p>)
= i&part;.
</p>
<p>(T.76)
</p>
<p>10. The velocity 4-vector (4-velocity) is given by
</p>
<p>v = dx
d
</p>
<p>= dx
dt
</p>
<p>dt
</p>
<p>d
= (c, v) (T.77)
</p>
<p>where  is the proper time. Determine vv
.
</p>
<p>Solution:116
</p>
<p>vv
 = 2
</p>
<p>(
c2 &minus; v2
</p>
<p>)
= 1
</p>
<p>1 &minus;
(
v
c
</p>
<p>)2 c
2
</p>
<p>(
1 &minus;
</p>
<p>(v
c
</p>
<p>)2)
= c2. (T.78)
</p>
<p>115Note that all terms are scalars and thus may be written in arbitrary order.
116Remind  =
</p>
<p>(
1 &minus; 2
</p>
<p>)&minus;1/2
and  = v/c.</p>
<p/>
</div>
<div class="page"><p/>
<p>388 Appendix T: Recaps and Outlines
</p>
<p>T.3 Classical Field Theory
</p>
<p>In order to quantize a classical system one is interested in a generally valid approach
</p>
<p>which has not to be tailored to the special system under consideration. As it is known,
</p>
<p>this universal method is the Lagrange-Hamilton formalism. It answers the relevant
</p>
<p>questions as, for instant, how to find those variables which in the process of quantizing
</p>
<p>will become non-commuting operators, how to find the energy density (Hamiltonian
</p>
<p>density) and so on.
</p>
<p>After a compressed revision of the formalism for particles, we review the basics
</p>
<p>of classical field theory. The emphasis is on the presentation of the most important
</p>
<p>results, not on their derivation.
</p>
<p>T.3.1 Particles
</p>
<p>T.3.1.1 One Coordinate q
</p>
<p>We consider the one-dimensional motion of a particle whose (generalized) coordinate
</p>
<p>q depends on time, q = q(t). Let L be a given function, called Lagrange function or
Lagrangian117 which depends on q and its time derivative q , i.e., L = L (q, q). The
action S is defined as the integral of L over time, S =
</p>
<p>&int; t2
t1
</p>
<p>L (q, q) dt . Hamilton&rsquo;s
</p>
<p>principle of least action states that the motion of the particle is determined by the
</p>
<p>condition that the variation of the action disappears118, S = 
&int; t2
</p>
<p>t1
L (q, q) dt = 0.
</p>
<p>Thus, the orbit of the particle between (q1, t1) and (q2, t2) is that one for which the
</p>
<p>action is stationary. Performing the variation leads to the Euler&ndash;Lagrange equation
</p>
<p>&part;L
</p>
<p>&part;q
&minus; d
</p>
<p>dt
</p>
<p>&part;L
</p>
<p>&part;q
= 0. (T.79)
</p>
<p>The conjugated momentum p is defined by
</p>
<p>p = &part;L
&part;q
</p>
<p>. (T.80)
</p>
<p>The Hamiltonian H is a function of q and p and is given by119
</p>
<p>H(p, q) = pq &minus; L (q, q) . (T.81)
</p>
<p>The equations of motion, known as canonical equations of Hamilton , are given by
</p>
<p>117Often, L is given by the difference of kinetic and potential energy, i.e., L = T &minus; V .
118One can imagine that the difference between kinetic and potential energy T &minus;V becomes minimal
if it is averaged over the entire motion.
119If L = T &minus; V, then H = T + V .</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix T: Recaps and Outlines 389
</p>
<p>q = {H, q}P B =
&part;H
</p>
<p>&part; p
; p = {H, p}P B = &minus;
</p>
<p>&part;H
</p>
<p>&part;q
(T.82)
</p>
<p>where the Poisson bracket is defined by
</p>
<p>{A, B}P B =
&part;A
</p>
<p>&part;q
</p>
<p>&part;B
</p>
<p>&part; p
&minus; &part;A
</p>
<p>&part; p
</p>
<p>&part;B
</p>
<p>&part;q
. (T.83)
</p>
<p>T.3.1.2 Several Coordinates qk
</p>
<p>If L is a function of several coordinates qk , k = 1, 2, . . ., i.e., L = L (q1, q2, . . . , q1,
q2, . . .), we have
</p>
<p>&part;L
</p>
<p>&part;qk
&minus; d
</p>
<p>dt
</p>
<p>&part;L
</p>
<p>&part;qk
= 0 ; k = 1, 2, . . . (T.84)
</p>
<p>and, correspondingly,
</p>
<p>pk =
&part;L
</p>
<p>&part;qk
; H =
</p>
<p>&sum;
</p>
<p>k
</p>
<p>pk qk &minus; L . (T.85)
</p>
<p>The equations of motion read
</p>
<p>qk =
&part;H
</p>
<p>&part; pk
; pk = &minus;
</p>
<p>&part;H
</p>
<p>&part;qk
(T.86)
</p>
<p>and the Poisson bracket is given by
</p>
<p>{A, B}P B =
&sum;
</p>
<p>k
</p>
<p>&part;A
</p>
<p>&part;qk
</p>
<p>&part;B
</p>
<p>&part; pk
&minus; &part;A
</p>
<p>&part; pk
</p>
<p>&part;B
</p>
<p>&part;qk
. (T.87)
</p>
<p>Especially, we have
</p>
<p>{
qi , p j
</p>
<p>}
P B
</p>
<p>=
&sum;
</p>
<p>k
</p>
<p>&part;qi
</p>
<p>&part;qk
</p>
<p>&part; p j
</p>
<p>&part; pk
&minus; &part;qi
</p>
<p>&part; pk
</p>
<p>&part; p j
</p>
<p>&part;qk
=
</p>
<p>&sum;
</p>
<p>k
</p>
<p>ik jk &minus; 0 = i j (T.88)
</p>
<p>and {
qi , q j
</p>
<p>}
P B
</p>
<p>=
{
</p>
<p>pi , p j
}
</p>
<p>P B
= 0. (T.89)
</p>
<p>These relations are the starting point for the canonical quantization, see below.</p>
<p/>
</div>
<div class="page"><p/>
<p>390 Appendix T: Recaps and Outlines
</p>
<p>T.3.2 Fields
</p>
<p>T.3.2.1 Lagrangian Density
</p>
<p>The formalism developed so far is used for systems with a finite number of degrees of
</p>
<p>freedom. We now expand the formalism to cover continua and fields with an infinite
</p>
<p>number of degrees of freedom (called Lagrangian field theory). We consider three
</p>
<p>space dimensions x = (x, y, z) or the spacetime (t, x, y, z). The basic term is the
Lagrangian density L which for one field  (t, x, y, z) is written as
</p>
<p>L = L
(
,
</p>
<p>&part;
</p>
<p>&part;t
,
&part;
</p>
<p>&part;x
,
&part;
</p>
<p>&part;y
,
&part;
</p>
<p>&part;z
</p>
<p>)
= L
</p>
<p>(
,
</p>
<p>&part;
</p>
<p>&part;t
,&nabla;
</p>
<p>)
= L
</p>
<p>(
, &part;
</p>
<p>)
. (T.90)
</p>
<p>Remember that the last expression means that L is a function of all derivatives,
</p>
<p> = 0, 1, 2, 3. We point out that we assume that L is a function of the field and its
first derivatives only and does not depend explicitly on the space-time coordinates.
</p>
<p>In other words, we consider only closed systems which do not exchange energy and
</p>
<p>momentum with the environment.120
</p>
<p>Lagrangian L and Lagrangian density L are related by
</p>
<p>L (t) =
&int;
</p>
<p>d3x L
(
, &part;
</p>
<p>)
(T.91)
</p>
<p>where
&int;
</p>
<p>d3x means the integration over space. The action is given by
</p>
<p>S =
&int;
</p>
<p>dt L =
&int;
</p>
<p>dt d3x L =
&int;
</p>
<p>d4x L. (T.92)
</p>
<p>The variation S = 0 leads to the equations of motion (Euler-Lagrange equations).
We have
</p>
<p>S =
&int;
</p>
<p>d4x
</p>
<p>[
&part;L
</p>
<p>&part;
+ &part;L
</p>
<p>&part;
(
&part;
</p>
<p>)
(
&part;
</p>
<p>)
]
</p>
<p>(T.93)
</p>
<p>where in the second term we have adopted the summation convention (i.e., summation
</p>
<p>over ). With
</p>
<p>&part;L
</p>
<p>&part;
(
&part;
</p>
<p>)
(
&part;
</p>
<p>)
= &part;L
</p>
<p>&part;
(
&part;
</p>
<p>)&part; () = &part;
{
</p>
<p>&part;L
</p>
<p>&part;
(
&part;
</p>
<p>) ()
}
&minus;
</p>
<p>(
&part;
</p>
<p>&part;L
</p>
<p>&part;
(
&part;
</p>
<p>)
)
()
</p>
<p>(T.94)
</p>
<p>120Given a physical system, the Lagrangian L is the central expression from which &lsquo;everything&rsquo; can
</p>
<p>be derived. However, there does not seem to be a unique way to identify L. Indeed, to find the right
</p>
<p>expression seems to be more a matter of experience, based on trial and error. Ultimately, the exact
</p>
<p>form of the Lagrangians has to be confirmed by experiment. Of course, there are some guidelines
</p>
<p>in tayloring L which can be adressed, apart from general principles as symmetries and so on.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix T: Recaps and Outlines 391
</p>
<p>we arrive at
</p>
<p>S =
&int;
</p>
<p>d4x
</p>
<p>[{
&part;L
</p>
<p>&part;
&minus;
</p>
<p>(
&part;
</p>
<p>&part;L
</p>
<p>&part;
(
&part;
</p>
<p>)
)}
</p>
<p>+ &part;
{
</p>
<p>&part;L
</p>
<p>&part;
(
&part;
</p>
<p>) ()
}]
</p>
<p>. (T.95)
</p>
<p>By means of the divergence theorem, the last term may be written as a surface integral
</p>
<p>&int;
d4x &part;
</p>
<p>{
&part;L
</p>
<p>&part;
(
&part;
</p>
<p>) ()
}
=
</p>
<p>&int;
</p>
<p>
</p>
<p>d
&part;L
</p>
<p>&part;
(
&part;
</p>
<p>) () (T.96)
</p>
<p>where  denotes the surface of the (4-dimensional) integration volume and d is
</p>
<p>the -component of the of the surface element. Assuming that &part;L
&part;(&part;)
</p>
<p>goes to zero
</p>
<p>sufficiently quickly at infinity, the surface integral vanishes. Finally, demanding again
</p>
<p>S = 0 for arbitrary variations  we get the Euler&ndash;Lagrange equation(s)
</p>
<p>&part;L
</p>
<p>&part;
&minus; &part;
</p>
<p>&part;L
</p>
<p>&part;
(
&part;
</p>
<p>) = 0. (T.97)
</p>
<p>Remind the summation convention. If the Lagrangian is a function of several fields
</p>
<p>i , i = 1, 2, . . ., the equation holds for each field separately.
Equation (T.97) can be written more compactly if one introduces the variational
</p>
<p>derivative.121 For a function which depends as in our case on the field  and its
</p>
<p>derivatives &part;, the variational derivative is defined by
</p>
<p>
</p>
<p>
= &part;
</p>
<p>&part;
&minus; &part;
</p>
<p>&part;
</p>
<p>&part;
(
&part;
</p>
<p>) . (T.98)
</p>
<p>Note the minus on the r.h.s.; in addition, the letter  differs from the &part; of the partial
</p>
<p>derivative &part;. Using this notation, (T.97) may be written L

</p>
<p>= 0.
</p>
<p>T.3.2.2 Hamiltonian Density
</p>
<p>With the abbreviation  = &part;
&part;t
</p>
<p>, the conjugated momentum (or conjugated momentum
</p>
<p>field) is defined by
</p>
<p> = L

</p>
<p>= &part;L
&part;
</p>
<p>(T.99)
</p>
<p>and the Hamiltonian density is given by
</p>
<p>H (,) = &minus; L. (T.100)
</p>
<p>121The variational derivative can be seen as a generalization of the directional derivative, so to speak
</p>
<p>as the derivative &lsquo;in direction of a function&rsquo;.</p>
<p/>
</div>
<div class="page"><p/>
<p>392 Appendix T: Recaps and Outlines
</p>
<p>The Hamiltonian is given by
</p>
<p>H(t) =
&int;
</p>
<p>d3x H (,) . (T.101)
</p>
<p>The equations of motion are given by
</p>
<p> = H

</p>
<p>;  = &minus;H

</p>
<p>. (T.102)
</p>
<p>This leads to
</p>
<p> = &part;H
&part;
</p>
<p>&minus; &part;
&part;H
</p>
<p>&part;
(
&part;
</p>
<p>) ;  = &minus;&part;H
&part;
</p>
<p>+ &part;
&part;H
</p>
<p>&part;
(
&part;
</p>
<p>) . (T.103)
</p>
<p>T.3.2.3 Poisson Brackets
</p>
<p>In field theory, the derivation of the Poisson brackets is a little bit cumbersome and
</p>
<p>not as straightforward as for the discrete case. We report just the result:
</p>
<p>{
 (t, x) ,
</p>
<p>(
t, x&prime;
</p>
<p>)}
P B
</p>
<p>= (3)
(
x &minus; x&prime;
</p>
<p>)
{
 (t, x) ,
</p>
<p>(
t, x&prime;
</p>
<p>)}
P B
</p>
<p>= 0{
 (t, x) ,
</p>
<p>(
t, x&prime;
</p>
<p>)}
P B
</p>
<p>= 0.
(T.104)
</p>
<p>Here, the Poisson bracket is defined by
</p>
<p>{A, B}P B =
&int;
</p>
<p>d3x
&sum;
</p>
<p>i
</p>
<p>(
A
</p>
<p>i
</p>
<p>B
</p>
<p>i
&minus; B
</p>
<p>i
</p>
<p>A
</p>
<p>i
</p>
<p>)
. (T.105)
</p>
<p>T.3.2.4 Several Fields
</p>
<p>If there are several fields r , r = 1, 2, . . . we have
</p>
<p>r =
&part;L
</p>
<p>&part;r
and H =
</p>
<p>&sum;
</p>
<p>r
</p>
<p>r r &minus; L. (T.106)
</p>
<p>The equations of motion are given by
</p>
<p>r =
&part;H
</p>
<p>&part;r
&minus; &part;
</p>
<p>&part;H
</p>
<p>&part;
(
&part;r
</p>
<p>) ; r = &minus;
&part;H
</p>
<p>&part;r
&minus; &part;
</p>
<p>&part;H
</p>
<p>&part;
(
&part;r
</p>
<p>) (T.107)
</p>
<p>and the Poisson brackets by</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix T: Recaps and Outlines 393
</p>
<p>{
r (t, x) ,s
</p>
<p>(
t, x&prime;
</p>
<p>)}
P B
</p>
<p>= 3
(
x &minus; x&prime;
</p>
<p>)
rs{
</p>
<p>r (t, x) ,s
(
t, x&prime;
</p>
<p>)}
P B
</p>
<p>= 0{
r (t, x) ,s
</p>
<p>(
t, x&prime;
</p>
<p>)}
P B
</p>
<p>= 0.
(T.108)
</p>
<p>T.3.3 Canonical Quantization
</p>
<p>The Lagrangian contains the complete information about the physical system. It
</p>
<p>enables us to derive the equations of motions, the conjugated momentum and the
</p>
<p>Hamiltonian.122 In addition, there is another benefit: the knowledge of the (classical,
</p>
<p>i.e., non-quantum mechanical) Lagrangian offers the means to quantize this classical
</p>
<p>system.
</p>
<p>We sketch the essential steps first for a Lagrange function L . The conjugated
</p>
<p>momenta pk and the Hamiltonian H are given by
</p>
<p>pk =
&part;L
</p>
<p>&part;qk
k = 1, . . . N ; H =
</p>
<p>N&sum;
</p>
<p>k=1
pk qk &minus; L . (T.109)
</p>
<p>The Poisson brackets for two quantities qi and p j are given by
</p>
<p>{
qi , p j
</p>
<p>}
P B
</p>
<p>= i j ;
{
qi , q j
</p>
<p>}
P B
</p>
<p>=
{
</p>
<p>pi , p j
}
</p>
<p>P B
= 0. (T.110)
</p>
<p>This relation can be considered as the key element of quantization. The method runs
</p>
<p>as follows: We replace (1) the variables qi , p j by operators qi , p j ; (2) the Poisson
</p>
<p>bracket {, } by a commutator [, ], (3) the Kronecker symbol i j by ii j . The well-
known result reads
</p>
<p>[
qi , p j
</p>
<p>]
= ii j ;
</p>
<p>[
qi , q j
</p>
<p>]
=
</p>
<p>[
pi , p j
</p>
<p>]
= 0. (T.111)
</p>
<p>This three-step procedure is called canonical quantization.
</p>
<p>We adopt this approach for fields. From the Lagrangian density L we can deduce
</p>
<p>the conjugated momentum fields and the Hamiltonian density H as
</p>
<p>r =
&part;L
</p>
<p>&part; (&part;0r )
r = 1, . . . N ; H =
</p>
<p>N&sum;
</p>
<p>r=1
r (&part;0r )&minus; L. (T.112)
</p>
<p>We can introduce Poisson brackets; for r (t, x) and s
(
t, x&prime;
</p>
<p>)
they read
</p>
<p>122We point out that the information content of the Lagrangian, of the Hamiltonian and of the
</p>
<p>equations of motion is equivalent.</p>
<p/>
</div>
<div class="page"><p/>
<p>394 Appendix T: Recaps and Outlines
</p>
<p>{
r (t, x) ,s
</p>
<p>(
t, x&prime;
</p>
<p>)}
P B
</p>
<p>= 3
(
x &minus; x&prime;
</p>
<p>)
rs{
</p>
<p>r (t, x) ,s
(
t, x&prime;
</p>
<p>)}
P B
</p>
<p>= 0{
r (t, x) ,s
</p>
<p>(
t, x&prime;
</p>
<p>)}
P B
</p>
<p>= 0.
(T.113)
</p>
<p>Again, we perform the canonical quantization by the above-mentioned three steps
</p>
<p>and arrive at
</p>
<p>[
r (t, x) , s
</p>
<p>(
t, x&prime;
</p>
<p>)]
= i3
</p>
<p>(
x &minus; x&prime;
</p>
<p>)
rs[
</p>
<p>r (t, x) , s
(
t, x&prime;
</p>
<p>)]
= 0 ;
</p>
<p>[
r (t, x) , s
</p>
<p>(
t, x&prime;
</p>
<p>)]
= 0 (T.114)
</p>
<p>where r (t, x) and s (t, x) are now field operators.
123
</p>
<p>One may ask if the step from the Poisson bracket to the commutator is logically
</p>
<p>mandatory. The answer is &lsquo;no&rsquo;. But the step is, in a certain sense, very plausible, and,
</p>
<p>most importantly, the method works, i.e., the resulting equations lead to outcomes
</p>
<p>which agree very well with the experiment. However, there is an important limitation
</p>
<p>of the method, since it requires the knowledge of the classical Lagrangian. In other
</p>
<p>words: If there is no macroscopic Lagrangian, the canonical quantization can not be
</p>
<p>applied.
</p>
<p>T.3.4 Some Lagrangian Densities
</p>
<p>Oscillating string
</p>
<p>L = 1
2
</p>
<p>[

</p>
<p>(
&part;
</p>
<p>&part;t
</p>
<p>)2
&minus; E
</p>
<p>(
&part;
</p>
<p>&part;x
</p>
<p>)2]
(T.115)
</p>
<p>Newtonian gravity
</p>
<p>L = &minus; (t, x) (t, x)&minus; 1
8G
</p>
<p>(&nabla; (t, x))2 (T.116)
</p>
<p>where G is the gravitational constant.
</p>
<p>Klein&ndash;Gordon Lagrangian
</p>
<p>L = 1
2
</p>
<p>(
&part;
</p>
<p>&part;x
&part;
</p>
<p>&part;x
&minus; m22
</p>
<p>)
(T.117)
</p>
<p>123For the sake of clearness, we write here r (t, x) and s (t, x) for the field operators. Otherwise,
</p>
<p>we will omit the hats and simply write r (t, x) and s (t, x) for the operators (as is common in
</p>
<p>many textbooks).</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix T: Recaps and Outlines 395
</p>
<p>Dirac Lagrangian
</p>
<p>L = ic&part;/&minus; mc2 (T.118)
</p>
<p>where  is a Dirac spinor,  = &dagger;0 is its Dirac adjoint, and &part;/ is the Feynman slash
notation for &part;.
</p>
<p>For more Lagrangians see the next section.
</p>
<p>T.3.5 Exercises and Solutions
</p>
<p>1. Show (T.79).
</p>
<p>Solution: We have124
</p>
<p>S = 
&int; t2
</p>
<p>t1
</p>
<p>L (q, q) dt =
&int; t2
</p>
<p>t1
</p>
<p>(
&part;L
</p>
<p>&part;q
q + &part;L
</p>
<p>&part;q
q
</p>
<p>)
dt. (T.120)
</p>
<p>Using integration by parts for the second term, we arrive at
</p>
<p>S =
&int; t2
</p>
<p>t1
</p>
<p>&part;L
</p>
<p>&part;q
qdt +
</p>
<p>[
&part;L
</p>
<p>&part;q
q
</p>
<p>]t2
</p>
<p>t1
</p>
<p>&minus;
&int; t2
</p>
<p>t1
</p>
<p>(
d
</p>
<p>dt
</p>
<p>&part;L
</p>
<p>&part;q
</p>
<p>)
qdt. (T.121)
</p>
<p>The term in brackets vanishes due to q (t1) = q (t2) = 0. So we have
</p>
<p>S =
&int; t2
</p>
<p>t1
</p>
<p>[
&part;L
</p>
<p>&part;q
&minus; d
</p>
<p>dt
</p>
<p>&part;L
</p>
<p>&part;q
</p>
<p>]
qdt. (T.122)
</p>
<p>Demanding S = 0 for arbitrary variations q yields
</p>
<p>&part;L
</p>
<p>&part;q
&minus; d
</p>
<p>dt
</p>
<p>&part;L
</p>
<p>&part;q
= 0. (T.123)
</p>
<p>2. Prove
{
 (t, x) ,
</p>
<p>(
t, x&prime;
</p>
<p>)}
P B
</p>
<p>= 3
(
x &minus; x&prime;
</p>
<p>)
.
</p>
<p>124Variation means in this context, that we change q(t) by q(t) &rarr; q(t) + q(t) with q(t1) =
q(t2) = 0 which, of course, means that q is changed correspondingly. This induces a change of L
with respect to its two arguments q and q . It follows (we use Taylor expansion)
</p>
<p>S = 
&int; t2
</p>
<p>t1
L (q, q) dt =
</p>
<p>&int; t2
t1
</p>
<p>[L (q + q, q + q)&minus; L (q, q)] dt =
=
</p>
<p>&int; t2
t1
</p>
<p>[
L (q, q)+ &part;L(q,q)&part;q q +
</p>
<p>&part;L(q,q)
&part;q
</p>
<p>q &minus; L (q, q)
]
</p>
<p>dt =
=
</p>
<p>&int; t2
t1
</p>
<p>[
&part;L(q,q)
</p>
<p>&part;q
q + &part;L(q,q)
</p>
<p>&part;q
q
</p>
<p>]
dt
</p>
<p>(T.119)</p>
<p/>
</div>
<div class="page"><p/>
<p>396 Appendix T: Recaps and Outlines
</p>
<p>Solution:
</p>
<p>{
 (t, x) ,
</p>
<p>(
t, x&prime;
</p>
<p>)}
P B
</p>
<p>=
&int;
</p>
<p>d3x &prime;&prime;
(
</p>
<p>(t,x)
(t,x&prime;&prime;)
</p>
<p>(t,x&prime;)
(t,x&prime;&prime;) &minus;
</p>
<p>(t,x&prime;)
(t,x&prime;&prime;)
</p>
<p>(t,x)
(t,x&prime;&prime;)
</p>
<p>)
=
</p>
<p>=
&int;
</p>
<p>d3x &prime;&prime; (3)
(
x &minus; x&prime;&prime;
</p>
<p>)
(3)
</p>
<p>(
x &minus; x&prime;
</p>
<p>) (
(t,x)
(t,x&prime;&prime;)
</p>
<p>(t,x&prime;)
(t,x&prime;&prime;) &minus;
</p>
<p>(t,x&prime;)
(t,x&prime;&prime;)
</p>
<p>(t,x)
(t,x&prime;&prime;)
</p>
<p>)
=
</p>
<p>=
&int;
</p>
<p>d3x &prime;&prime; (3)
(
x &minus; x&prime;&prime;
</p>
<p>)
(3)
</p>
<p>(
x &minus; x&prime;
</p>
<p>)
= (3)
</p>
<p>(
x &minus; x&prime;
</p>
<p>)
.
</p>
<p>(T.124)
</p>
<p>3. Consider
</p>
<p>L = 1
2
</p>
<p>[

</p>
<p>(
&part;
</p>
<p>&part;t
</p>
<p>)2
&minus; E
</p>
<p>(
&part;
</p>
<p>&part;x
</p>
<p>)2]
. (T.125)
</p>
<p>This is the Lagrangian of a one-dimensional oscillating string; is the linear mass
</p>
<p>density and E the modulus of elasticity. Determine the Euler&ndash;Lagrange equation,
</p>
<p>the conjugated momentum, the Hamiltonian and Hamilton&rsquo;s equations.
</p>
<p>Solution: We have
</p>
<p>&part;L
</p>
<p>&part;
= 0 ; &part;L
</p>
<p>&part; &part;
&part;t
</p>
<p>= &part;
&part;t
</p>
<p>; &part;L
&part; &part;
</p>
<p>&part;x
</p>
<p>= &minus;E &part;
&part;x
</p>
<p>. (T.126)
</p>
<p>Thus, the Euler&ndash;Lagrange equation reads
</p>
<p>0&minus; d
dt
</p>
<p>
&part;
</p>
<p>&part;t
&minus; d
</p>
<p>dx
</p>
<p>(
&minus;E &part;
</p>
<p>&part;x
</p>
<p>)
= 0 &rarr; &part;
</p>
<p>2
</p>
<p>&part;t2
= E &part;
</p>
<p>2
</p>
<p>&part;x2
or  = E&prime;&prime;. (T.127)
</p>
<p>The conjugated momentum is given by
</p>
<p> = &part;L
&part;
</p>
<p>= &part;
&part;t
</p>
<p>&rarr;  = 

</p>
<p>(T.128)
</p>
<p>and the Hamiltonian density reads
</p>
<p>H = &minus; 1
2
</p>
<p>[

</p>
<p>(
&part;
</p>
<p>&part;t
</p>
<p>)2
&minus; E
</p>
<p>(
&part;
</p>
<p>&part;x
</p>
<p>)2]
= 
</p>
<p>2
</p>
<p>
&minus; 
</p>
<p>2
</p>
<p>(

</p>
<p>
</p>
<p>)2
+ E
</p>
<p>2
</p>
<p>(
&part;
</p>
<p>&part;x
</p>
<p>)2
= 
</p>
<p>2
</p>
<p>2
+ E
</p>
<p>2
</p>
<p>(
&part;
</p>
<p>&part;x
</p>
<p>)2
.
</p>
<p>(T.129)
</p>
<p>Hamilton&rsquo;s equations are given by
</p>
<p> = &part;H
&part;
</p>
<p>&minus; &part;
&part;x
</p>
<p>(
&part;H
</p>
<p>&part;( &part;&part;x )
</p>
<p>)
= 
</p>
<p>
</p>
<p> = &minus; &part;H
&part;
</p>
<p>+ &part;
&part;x
</p>
<p>(
&part;H
</p>
<p>&part;
(
</p>
<p>&part;
&part;x
</p>
<p>)
)
= &part;
</p>
<p>&part;x
E
(
&part;
&part;x
</p>
<p>)
= E &part;2
</p>
<p>&part;x2
</p>
<p>(T.130)
</p>
<p>and the equation of motion reads
</p>
<p> = 

</p>
<p>&rarr;  = E

&prime;&prime;. (T.131)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix T: Recaps and Outlines 397
</p>
<p>4. Show
&part;
</p>
<p>&part;
(
&part;
</p>
<p>) (&part;) (&part;) = 2 (&part;) . (T.132)
</p>
<p>Solution: We have
</p>
<p>&part;
&part;
(
&part;
</p>
<p>) (&part;)
(
&part;
</p>
<p>)
= 
</p>
<p>(
&part;
</p>
<p>)
+ (&part;) &part;&part;(&part;
</p>
<p>)
(
g&part;
</p>
<p>)
=
</p>
<p>=
(
&part;
</p>
<p>)
+ (&part;) g =
</p>
<p>(
&part;
</p>
<p>)
+
</p>
<p>(
&part;
</p>
<p>)
 =
</p>
<p>(
&part;
</p>
<p>)
+
</p>
<p>(
&part;
</p>
<p>)
= 2
</p>
<p>(
&part;
</p>
<p>)
.
</p>
<p>(T.133)
</p>
<p>5. By adding appropriate terms to the Lagrangians (Hamiltonians) of free fields,
</p>
<p>we can model or describe interactions of fields. One of the simplest examples,
</p>
<p>by obvious reasons called 4-theory, is based on the Klein&ndash;Gordon field. Its
</p>
<p>Lagrangian is given by
</p>
<p>L = 1
2
</p>
<p>(
&part;
</p>
<p>)
(&part;)&minus; 1
</p>
<p>2
m2 &minus; g
</p>
<p>4!
4. (T.134)
</p>
<p>Determine the equations of motion.
</p>
<p>Solution: The Euler&ndash;Lagrange equations are given by
</p>
<p>(
&part;
</p>
<p>&part;L
</p>
<p>&part;
(
&part;
</p>
<p>)
)
&minus; &part;L
</p>
<p>&part;
= 0. (T.135)
</p>
<p>With
&part;L
</p>
<p>&part;
(
&part;
</p>
<p>) = &part; ; &part;L
&part;
</p>
<p>= &minus;m&minus; g
3!
</p>
<p>3 (T.136)
</p>
<p>follows
</p>
<p>&part;&part;
+ m+ g
</p>
<p>3!
3 = 0. (T.137)
</p>
<p>T.4 Electrodynamics
</p>
<p>We repeat here some facts from electrodynamics as far as we need them in further
</p>
<p>chapters. Apart from providing the necessary formulas and expressions, we aim at a
</p>
<p>consistent notation.
</p>
<p>T.4.1 Maxwell Equations, Potentials, Gauge
</p>
<p>In SI-units, the maxwell equations are given by</p>
<p/>
</div>
<div class="page"><p/>
<p>398 Appendix T: Recaps and Outlines
</p>
<p>&nabla;&middot; E (r, t) = 1
0
 ; &nabla;&middot; B (r, t) = 0
</p>
<p>&nabla; &times; E (r, t) = &minus; &part;
&part;t
B (r, t) ; &nabla; &times; B (r, t) = 1
</p>
<p>c2
&part;
&part;t
E (r, t)+ 0j
</p>
<p>(T.138)
</p>
<p>with
</p>
<p>c200 = 1. (T.139)
</p>
<p>Introducing the scalar and the vector potential  and A
</p>
<p>B = &nabla; &times; A ; E = &minus;&nabla;&minus; &part;A
&part;t
</p>
<p>(T.140)
</p>
<p>transforms (T.138) into
</p>
<p>&nabla;&middot;
(
&minus;&nabla;&minus; &part;A
</p>
<p>&part;t
</p>
<p>)
= 1
</p>
<p>0
 ; &nabla;&middot; (&nabla; &times; A) = 0
</p>
<p>&nabla; &times;
(
&minus;&nabla;&minus; &part;A
</p>
<p>&part;t
</p>
<p>)
= &minus; &part;
</p>
<p>&part;t
&nabla; &times; A ; &nabla; &times; (&nabla; &times; A) = 1
</p>
<p>c2
&part;
&part;t
</p>
<p>(
&minus;&nabla;&minus; &part;A
</p>
<p>&part;t
</p>
<p>)
+ 0j.
</p>
<p>(T.141)
</p>
<p>The source-free equations are automatically satisfied. The other two equations are
</p>
<p>&minus;&nabla;2&minus; &part;
&part;t
&nabla;A = 1
</p>
<p>0

</p>
<p>1
c2
</p>
<p>&part;2A
&part;t2
</p>
<p>&minus;&nabla;2A+&nabla;
(
&nabla; A
</p>
<p>)
+ 1
</p>
<p>c2
&nabla;
</p>
<p>&part;
&part;t
 = 0j.
</p>
<p>(T.142)
</p>
<p>Adding zero to the first equation gives
</p>
<p>1
c2
</p>
<p>&part;2
&part;t2
</p>
<p>&minus;&nabla;2&minus; &part;
&part;t
</p>
<p>(
1
c2
</p>
<p>&part;
&part;t
</p>
<p>+&nabla; A
)
= 1
</p>
<p>0

</p>
<p>1
c2
</p>
<p>&part;2A
&part;t2
</p>
<p>&minus;&nabla;2A+&nabla;
(
</p>
<p>1
c2
</p>
<p>&part;
&part;t
+&nabla; A
</p>
<p>)
= 0j
</p>
<p>(T.143)
</p>
<p>or (
1
c2
</p>
<p>&part;2
</p>
<p>&part;t2
&minus;&nabla;2
</p>
<p>)
&minus; &part;
</p>
<p>&part;t
</p>
<p>(
1
c2
</p>
<p>&part;
&part;t
</p>
<p>+&nabla; A
)
= 1
</p>
<p>0
(
</p>
<p>1
c2
</p>
<p>&part;2
</p>
<p>&part;t2
&minus;&nabla;2
</p>
<p>)
A+&nabla;
</p>
<p>(
1
c2
</p>
<p>&part;
&part;t
+&nabla; A
</p>
<p>)
= 0j.
</p>
<p>(T.144)
</p>
<p>Using the 4-vectors j = (c, j), A = (
c
,A), &part; =
</p>
<p>(
1
c
</p>
<p>&part;
&part;t
,&nabla;
</p>
<p>)
and &part; =(
</p>
<p>1
c
</p>
<p>&part;
&part;t
,&minus;&nabla;
</p>
<p>)
, we can cast (T.144) into the form
</p>
<p>&part;&part;
 A0 &minus; &part;0
</p>
<p>(
&part;0 A
</p>
<p>0 + &part;k Ak
)
= 1
</p>
<p>c20
j0
</p>
<p>&part;&part;
 Ak&minus;&part;k
</p>
<p>(
&part;0 A
</p>
<p>0 + &part;k Ak
)
= 0 j k
</p>
<p>(T.145)
</p>
<p>which may be written in covariant manner as
</p>
<p>&part;&part;
 A &minus; &part; (&part; A) = 0 j. (T.146)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix T: Recaps and Outlines 399
</p>
<p>This equation, formulated in terms of the potentials A, replaces the Maxwell
</p>
<p>equations,125 formulated in terms of the fields E and B. However, there is one dis-
</p>
<p>tinctive difference: by equation (T.146), the potentials are not determined uniquely, in
</p>
<p>contrast to the fieldsE andB. Indeed, a transformation (called gauge transformation)
</p>
<p>with an arbitrary 
</p>
<p>A &rarr; A = A + &part; (T.147)
</p>
<p>leaves the fields E and B invariant as well as the equation (T.146). This fact is called
</p>
<p>gauge invariance.
</p>
<p>One can exploit the freedom of the choice of the gauge to make things as simple
</p>
<p>as possible. Depending on the system under consideration, there are among others
</p>
<p>two common choices, namely the Lorenz126 gauge &part; A
 = 0 and the Coulomb (or
</p>
<p>radiation) gauge &part;k A
k = &nabla; &middot; A = 0. As an example, (T.146) reads in the Lorenz
</p>
<p>gauge simply
</p>
<p>&part;&part;
 A = 0 j. (T.148)
</p>
<p>T.4.2 Free Solutions
</p>
<p>Without sources, the equations of motion read
</p>
<p>&part;&part;
A = 0. (T.149)
</p>
<p>This is essentially the Klein&ndash;Gordon equation for vanishing mass, apart from the
</p>
<p>fact that the potential is not a scalar, but a 4-vector. This means, we can immediately
</p>
<p>write down the solutions which read in the discrete and continuous case127
</p>
<p>A (x) = &sum;k,r
&radic;
</p>
<p>1
2Vk
</p>
<p>

r (k)
</p>
<p>[
r (k) e
</p>
<p>&minus;ikx + &dagger;r (k) eikx
]
</p>
<p>A (x) = &sum;r
&int;
</p>
<p>d3k&radic;
2(2)3k
</p>
<p>

r (k)
</p>
<p>[
r (k) e
</p>
<p>&minus;ikx + &dagger;r (k) eikx
]
.
</p>
<p>(T.150)
</p>
<p>Obviously, the potential is real, as it should be. The 4-vectors 

r (k) are called polar-
</p>
<p>ization vectors. Their specific form depends on the chosen gauge. In the Coulomb
</p>
<p>gauge &part;k A
k = &nabla; &middot; A = 0 we have for the polarization vectors
</p>
<p>0r (k) = 0 ; kllr (k) = k &middot; r (k) = 0 ; r (k) &middot; r &prime; (k) = rr &prime; . (T.151)
</p>
<p>In other words, in this gauge we need only two polarization vectors 

r , r = 1, 2. They
</p>
<p>are orthogonal to each other and orthogonal to k (i.e., transversal). Moreover, in the
</p>
<p>125Note that the homogenous Maxwell equations in (T.138) are automatically fulfilled.
126It is indeed Lorenz (Ludvig Valentin Lorenz, Dane, 1829&ndash;1891), and not Lorentz (Hendrik
</p>
<p>Antoon Lorentz, Dutch, 1853&ndash;1928).
127The normalization is chosen with regard to the application in quantum field theory.</p>
<p/>
</div>
<div class="page"><p/>
<p>400 Appendix T: Recaps and Outlines
</p>
<p>Lorenz gauge &part; A
 = 0 we have kr (k) = 0, i.e., three independent polarization
</p>
<p>vectors (see exercises).
</p>
<p>T.4.3 Electromagnetic Field Tensor
</p>
<p>A very compact and elegant description of electrodynamics is provided by the elec-
</p>
<p>tromagnetic field tensor F . It is defined by
</p>
<p>F = 1
c
</p>
<p>

</p>
<p>0 &minus;E1 &minus;E2 &minus;E3
E1 0 &minus;cB3 cB2
E2 cB3 0 &minus;cB1
E3 &minus;cB2 cB1 0
</p>
<p>
 ; F =
</p>
<p>1
</p>
<p>c
</p>
<p>

</p>
<p>0 E1 E2 E3
</p>
<p>&minus;E1 0 &minus;cB3 cB2
&minus;E2 cB3 0 &minus;cB1
&minus;E3 &minus;cB2 cB1 0
</p>
<p>

</p>
<p>(T.152)
</p>
<p>(due to F = gFg). With F , the inhomogeneous and homogenous Maxwell
equations (T.148) read
</p>
<p>&part; F
 = 0 j and &part;F + &part;F + &part; F = 0. (T.153)
</p>
<p>Expressing F by means of the 4-potential Aa, i.e., by
</p>
<p>F = &part;A &minus; &part; A ; F = &part; A &minus; &part;A (T.154)
</p>
<p>gives for the inhomogeneous equations
</p>
<p>&part;&part;
 A &minus; &part; (&part; A) = 0 j (T.155)
</p>
<p>while the homogenous equations are automatically fulfilled.
</p>
<p>Note that the electromagnetic field tensor is gauge invariant:
</p>
<p>&part; A &minus; &part; A = &part;
[
A + &part;
</p>
<p>]
&minus; &part;
</p>
<p>[
A + &part;
</p>
<p>]
= &part; A + &part;&part; &minus; &part;A &minus; &part;&part; =
</p>
<p>= &part; A &minus; &part;A + &part;&part; &minus; &part;&part; = &part; A &minus; &part;A .
(T.156)
</p>
<p>A Lorentz scalar is given e.g. by
</p>
<p>F F
 = 2
</p>
<p>(
B2 &minus; E
</p>
<p>2
</p>
<p>c2
</p>
<p>)
(T.157)
</p>
<p>which is, up to a constant factor, the energy density of the electromagnetic field. As
</p>
<p>is seen, the term is gauge invariant.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix T: Recaps and Outlines 401
</p>
<p>T.4.4 Lagrangian L
</p>
<p>Due to gauge invariance, there is no unique Lagrangian density L for the electromag-
</p>
<p>netic field. The only criterion is that L reproduces the correct equations of motion,
</p>
<p>i.e., the Maxwell equations. The criterion is fulfilled, for instance, by the choice
</p>
<p>L = &minus; 1
40
</p>
<p>F F
 &minus; jA (T.158)
</p>
<p>which leads to the Euler&ndash;Lagrange equations &part; F
 = 0 j.
</p>
<p>Another common Lagrangian density L reads
</p>
<p>L = &minus; 1
20
</p>
<p>(
&part; A
</p>
<p>)
(&part; A)&minus; jA (T.159)
</p>
<p>which leads to &part;&part;
 A = 0 j.
</p>
<p>T.4.5 Some Lagrangian Densities
</p>
<p>The Lagrangian for the electromagnetic field (T.158) is something like the starting
</p>
<p>point for advanced formulations. In quantum electrodynamics (QED), the Lagrangian
</p>
<p>L may be written
</p>
<p>L = icD/ &minus; mc2 &minus; 1
40
</p>
<p>F F
 (T.160)
</p>
<p>where D/ = D is the QED gauge covariant derivative with D = &part; &minus; iq A
and  the Dirac field. The Lagrangian for quantum chromodynamics (QCD) follows
</p>
<p>in a certain sense the same pattern. Of course it is more complex, but the structure
</p>
<p>inherited from QED is clearly apparent. We report the result without going into
</p>
<p>details:
</p>
<p>L =
&sum;
</p>
<p>n
</p>
<p>(
icn D/n &minus; mnc2nn
</p>
<p>)
&minus; 1
</p>
<p>4
GG
</p>
<p>
 . (T.161)
</p>
<p>Here, D is the QCD gauge covariant derivative, n = 1, . . . , 6 counts the quark types,
and G is the gluon field strength tensor.
</p>
<p>T.4.6 Exercises and Solutions
</p>
<p>1. Starting from (T.145), derive (T.146).
</p>
<p>Solution: We insert</p>
<p/>
</div>
<div class="page"><p/>
<p>402 Appendix T: Recaps and Outlines
</p>
<p>&part;&part;
 = 1
</p>
<p>c2
</p>
<p>&part;2
</p>
<p>&part;t2
&minus;&nabla;2 ;  = cA0 ; A = Ak ; &part;
</p>
<p>&part;t
= c&part;0 ; &nabla; = &part;k (T.162)
</p>
<p>and arrive at
&part;&part;
</p>
<p>cA0 &minus; c&part;0
(
</p>
<p>1
c2
</p>
<p>c&part;0cA0 + &part;k Ak
)
= 1
</p>
<p>0

</p>
<p>&part;&part;
 Ak &minus; &part;k
</p>
<p>(
1
c2
</p>
<p>c&part;0cA0 + &part;k Ak
)
= 0j
</p>
<p>(T.163)
</p>
<p>or
&part;&part;
</p>
<p> A0 &minus; &part;0
(
&part;0 A
</p>
<p>0 + &part;k Ak
)
= 1
</p>
<p>c0
 = 1
</p>
<p>c20
j0 = 0 j0
</p>
<p>&part;&part;
 Ak &minus; &part;k
</p>
<p>(
&part;0 A
</p>
<p>0 + &part;k Ak
)
= 0j =0 j k .
</p>
<p>(T.164)
</p>
<p>Merging the two equations leads to
</p>
<p>&part;&part;
 A &minus; &part; (&part; A) = 0 j (T.165)
</p>
<p>2. Show that (T.146) is invariant with respect to the transformation (T.147).
</p>
<p>Solution: We have
</p>
<p>&part;&part;
 A &minus; &part;
</p>
<p>(
&part; A
</p>
<p>
)
= &part;&part; [A + &part;	] &minus; &part; (&part; [A + &part;	]) =
</p>
<p>= [&part;&part; A + &part;&part;&part;	] &minus; [&part;&part; A + &part;&part;&part;	] =
= &part;&part; A &minus; &part;&part; A + &part;&part;&part;	&minus; &part;&part;&part;	 = &part;&part; A &minus; &part;&part; A .
</p>
<p>(T.166)
</p>
<p>3. Derive the Maxwell equations from the Lagrangian (T.159).
</p>
<p>Solution: The Euler&ndash;Lagrange equations for the electromagnetic field (in terms
</p>
<p>of A and &part; A
) are
</p>
<p>&part;L
</p>
<p>&part;A
&minus; &part; &part;L
</p>
<p>&part;(&part;A)
= 0. (T.167)
</p>
<p>This yields with (T.159)
</p>
<p>&part;
&part;
</p>
<p>&part;(&part;A )
</p>
<p>(
&minus; 1
</p>
<p>20
&part;A&part;
</p>
<p>A &minus; jA
)
= &part;
</p>
<p>&part;A
</p>
<p>(
&minus; 1
</p>
<p>20
&part;A&part;
</p>
<p>A &minus; jA
)
. (T.168)
</p>
<p>From this follows step by step
</p>
<p>&part; &part;
&part;(&part; A )
</p>
<p>(
1
</p>
<p>20
gg&part;
</p>
<p> A&part; A
)
= &part;
</p>
<p>&part;A
j A
</p>
<p>
</p>
<p>1
20
</p>
<p>&part;gg
(

</p>
<p>
&part;
</p>
<p> A + &part; A
)
= j
</p>
<p>1
20
</p>
<p>&part;
(
gg&part;
</p>
<p> A + gg&part; A
)
= j
</p>
<p>1
20
</p>
<p>&part;
(
&part;A + &part;A
</p>
<p>)
= j
</p>
<p>1
0
&part;&part;A = j or &part;&part;A = 0 j .
</p>
<p>(T.169)
</p>
<p>4. Determine the polarization vectors in (T.150) for the Coulomb and the Lorenz
</p>
<p>gauge.
</p>
<p>Solution: W.l.o.g we can identify the z-axis with the direction of propagation,
</p>
<p>k = (0, 0, k3).</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix T: Recaps and Outlines 403
</p>
<p>(a) Coulomb gauge &part;m A
m = &nabla; &middot; A = 0. It follows
</p>
<p>&part;m A
m (x) = &sum;k,r
</p>
<p>&radic;
1
</p>
<p>2Vk
mr (k) ikm
</p>
<p>[
r (k) e
</p>
<p>&minus;ikx &minus; &dagger;r (k) eikx
]
=
</p>
<p>= i &sum;k,r
&radic;
</p>
<p>1
2Vk
</p>
<p>r (k) &middot; k
[
r (k) e
</p>
<p>&minus;ikx &minus; &dagger;r (k) eikx
]
= 0 &rarr; r (k) &middot; k != 0.
</p>
<p>(T.170)
</p>
<p>Thus, we can focus on the inner product r (k) &middot; k.which tells that the polariza-
tions vectors are orthogonal to k. Due to k = (0, 0, k3), we have 3r (k) = 0
whereas 1r and 
</p>
<p>2
r are undetermined. We choose the simplest solution,
</p>
<p>128
</p>
<p>11 (k) = 1; 21 (k) = 0 and 12 (k) = 0; 22 (k) = 1. In addition, also 0r (k)
may be chosen freely; we set 0r (k) = 0. Thus, we have two polarization vec-
tors, namely 1 (k) = (0, 1, 0, 0) and 2 (k) = (0, 0, 1, 0).
(b) Lorenz gauge &part; A
</p>
<p> = &part;0 A0 &minus; &nabla; A = 0 Here the defining inner prod-
uct is kr (k) = k00r (k) &minus; k33r (k) = 0. Again, 1r and 2r are undetermined;
we choose them as in the Coulomb case to be 11 (k) = 1; 21 (k) = 0 and
12 (k) = 0; 22 (k) = 1. The remaining components are connected by 0r (k) =
(k3/k0) 
</p>
<p>3
r (k). Thus we have three pairwise orthogonal polarization vectors, e.g.
</p>
<p>1 (k) = (0, 1, 0, 0) and 2 (k) = (0, 0, 1, 0) and 3 (k) = N
(
</p>
<p>k3
k0
, 0, 0, 1
</p>
<p>)
with
</p>
<p>N&minus;2 = 1 &minus;
(
</p>
<p>k3
k0
</p>
<p>)2
.
</p>
<p>128The two polarization vectors have to be orthogonal to the direction of propagation. Thus, other
</p>
<p>solutions, e.g. circular polarization and so on, are possible, of course.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix U
</p>
<p>Elements of Relativistic Quantum Mechanics
</p>
<p>U.1 Introduction
</p>
<p>The bulk of the two volumes of this book is devoted to non-relativistic quantum
</p>
<p>mechanics (NRQM). In the following, we will discuss relativistic quantum mechan-
</p>
<p>ics (RQM). This field of physics is important in itself and show issues that do not
</p>
<p>(and can not) emerge in non-relativistic theories - we mention just spin or antiparti-
</p>
<p>cles. In addition, it is an indispensable prerequisite for advanced theories as quantum
</p>
<p>field theory (QFT), elements of which are presented in Vol. 2. Hence, by providing
</p>
<p>some of the basics RQM we hope that this will help to a better understanding of some
</p>
<p>fundamentals of modern physics and an easier access to some of the latest topics in
</p>
<p>modern physics.
</p>
<p>The content plan is as follows: We first derive the Klein&ndash;Gordon and the Dirac
</p>
<p>equation as basic elements of RQM. Then we construct plane wave solutions for
</p>
<p>both equations. As we will see, we encounter immediately the notorious problem
</p>
<p>of solutions with negative energy which will be solved in a satisfying manner only
</p>
<p>within the context of QFT. In case of the Dirac equation, spin 1/2 emerges quasi
</p>
<p>unexpectedly by our quantization procedure, without any requirement about angular
</p>
<p>momentum, let alone spin 1/2. After that, we prove that the Dirac equation is in
</p>
<p>accordance with special relativity (i.e., is covariant) and couple it to the electromag-
</p>
<p>netic field. This formulation allows among others for deriving the Pauli equation as
</p>
<p>the non-relativistic limit of the Dirac equation. Finally, we discuss the pros and cons
</p>
<p>of the Dirac equation and discuss how modern interpretations show a way out of the
</p>
<p>dilemma of negative energies.
</p>
<p>In addition, there is a section about the relativistic description of the Hydrogen
</p>
<p>atom and its energy spectrum. Because of the thematic context, it is found in Vol. 2.
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>405</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>406 Appendix U: Elements of Relativistic Quantum Mechanics
</p>
<p>U.2 Constructing Relativistic Equations
</p>
<p>In this section, we construct relativistic quantum-mechanical equations of motion.
</p>
<p>The guideline is our derivation of the free Schr&ouml;dinger equation (see Vol. 1, Chap. 1).
</p>
<p>It was based on the non-relativistic dispersion relation
</p>
<p>E = p
2
</p>
<p>2m
(U.1)
</p>
<p>and the substitutions
</p>
<p>E &rarr; i &part;
&part;t
</p>
<p>; p &rarr; 
i
&nabla;. (U.2)
</p>
<p>Following these principles, we will derive in the following the Klein&ndash;Gordon equa-
</p>
<p>tion and the Dirac equation.
</p>
<p>U.2.1 Klein&ndash;Gordon Equation
</p>
<p>The relativistic dispersion relation is given by
</p>
<p>E2 = m2c4 + c2p2 (U.3)
</p>
<p>where m is the rest mass. The substitution (U.2) leads directly to the Klein&ndash;Gordon
</p>
<p>equation (cf. Vol. 1, Chap. 3)
</p>
<p>&minus; 2 &part;
2
</p>
<p>&part;t2
 (r, t) = &minus;c22&nabla;2 (r, t)+ c4m2 (r, t) . (U.4)
</p>
<p>As it turns out, this equation is valid for quantum objects with spin zero. In the
</p>
<p>beginnings of Quantum Mechanics, it was Schr&ouml;dinger who found this equation.
</p>
<p>However, he discarded it since it does not allow for a positive definite probability
</p>
<p>density.
</p>
<p>U.2.1.1 Probability Density
</p>
<p>Remember that for the Schr&ouml;dinger equation, we have found that  = &lowast; can be
regarded as probability density, since it is positive definite, ||2 &ge; 0 (see Vol. 1,
Chap. 7) and fulfills the continuity equation
</p>
<p>&part;
</p>
<p>&part;t
+&nabla; &middot; j = 0 ; j = 
</p>
<p>2im
</p>
<p>(
&lowast;&nabla; &minus; &nabla;&lowast;
</p>
<p>)
. (U.5)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix U: Elements of Relativistic Quantum Mechanics 407
</p>
<p>In case of the Klein&ndash;Gordon equation (U.4), we multiply the equation and its complex
</p>
<p>conjugate by &lowast; and  and obtain the equations
</p>
<p>&lowast; &part;
2
</p>
<p>&part;t2
 = c2&lowast;&nabla;2&minus; c4m2
</p>
<p>2
&lowast;
</p>
<p> &part;
2
</p>
<p>&part;t2
&lowast; = c2&nabla;2&lowast; &minus; c4m2
</p>
<p>2
&lowast;.
</p>
<p>(U.6)
</p>
<p>Subtracting the two equations gives
</p>
<p>&lowast;
&part;2
</p>
<p>&part;t2
&minus;  &part;
</p>
<p>2
</p>
<p>&part;t2
&lowast; = c2&lowast;&nabla;2&minus; c2&nabla;2&lowast; (U.7)
</p>
<p>which can be written as
</p>
<p>&part;
</p>
<p>&part;t
</p>
<p>(
&lowast;
</p>
<p>&part;
</p>
<p>&part;t
&minus;  &part;
</p>
<p>&part;t
&lowast;
</p>
<p>)
= c2&nabla;
</p>
<p>(
&lowast;&nabla;&minus; &nabla;&lowast;
</p>
<p>)
. (U.8)
</p>
<p>Again, we define the probability current density by j = 
2im
</p>
<p>(&lowast;&nabla;&minus; &nabla;&lowast;). Since
the right hand side of (U.8) is given by 2im
</p>
<p>
c2&nabla; j , comparison with the continuity
</p>
<p>equation leads to the conclusion that the probability density is defined by
</p>
<p> = i
2mc2
</p>
<p>(
&lowast;
</p>
<p>&part;
</p>
<p>&part;t
&minus;  &part;
</p>
<p>&part;t
&lowast;
</p>
<p>)
.
</p>
<p>This expression is not positive definite which means that the concepts of probability
</p>
<p>cannot be applied for the Klein&ndash;Gordon equation - a sufficient argument for Erwin
</p>
<p>Schr&ouml;dinger to reject this equation. We note that the problem stems from the fact
</p>
<p>that in the Klein&ndash;Gordon equation there occurs a second time derivative &part;
2
</p>
<p>&part;t2
; a first
</p>
<p>derivative &part;
&part;t
</p>
<p>would lead to  = ||2 as in the case of the Schr&ouml;dinger equation.
</p>
<p>U.2.1.2 Plane Waves and Negative Energies
</p>
<p>Let us look for plane wave solutions of equation (U.4). The ansatz
</p>
<p> =  (k) eikr&minus;it (U.9)
</p>
<p>with an amplitude  (k) yields
</p>
<p>
22 = c22k2 + c4m2. (U.10)
</p>
<p>As expected, by means of the deBroglie relations E =  and p = k we arrive at
</p>
<p>E2 = c2p2 + c4m2. (U.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>408 Appendix U: Elements of Relativistic Quantum Mechanics
</p>
<p>Thus, the general plane wave solution is a linear combination of all partial solutions
</p>
<p>(U.9) and their complex conjugates, i.e.,
</p>
<p> =
&int;
</p>
<p>d3k
[
 (k) eikr&minus;it + &lowast; (k) e&minus;ikr+it
</p>
<p>]
. (U.12)
</p>
<p>Now we ask which values of E (or) are allowed in (U.11) for a given momentum
</p>
<p>p. The answer, of course, is that we have two solutions, namely
</p>
<p>E = &plusmn;
&radic;
</p>
<p>c2p2 + c4m2. (U.13)
</p>
<p>This means that we have also solutions with negative energy (or frequency). The
</p>
<p>problem is that nature does not know negative energies. So what to do with these
</p>
<p>solutions?
</p>
<p>We remark that negative energies are not a speciality of the Klein&ndash;Gordon equa-
</p>
<p>tion, but that this problem is in a certain sense common to all relativistic equations.
</p>
<p>We will meet and discuss it also when considering the Dirac equation, see below.
</p>
<p>In classical physics, there occur negative energies, but this is due to a shifted
</p>
<p>energy zero point. As an example we consider the Hydrogen atom. The energies are
</p>
<p>given by (see Appendix F, Vol. 2)129
</p>
<p>Enj = m0c2
</p>
<p>
1 +
</p>
<p>

</p>
<p>
</p>
<p>n &minus;
(
</p>
<p>j + 12
)
+
</p>
<p>&radic;(
j + 12
</p>
<p>)2
&minus; 2
</p>
<p>

</p>
<p>2

</p>
<p>&minus;1/2
</p>
<p>&asymp; m0c2
[
</p>
<p>1 &minus; 
2
</p>
<p>2n2
+ &middot; &middot; &middot;
</p>
<p>]
</p>
<p>(U.14)
</p>
<p>where we have expanded the root with respect to the fine structure constant  &asymp; 1
137
</p>
<p>.
</p>
<p>Obviously, the exact expression Enj is always positive. In order to obtain in classical
</p>
<p>physics expressions which are easy to handle, one subtracts the rest energy from the
</p>
<p>total energy and obtains Enj , classical =
(
Enj &minus; m0c2
</p>
<p>)
&asymp; &minus; 2
</p>
<p>2n2
m0c
</p>
<p>2. So these classical
</p>
<p>negative energies are not &lsquo;true&rsquo; negative energies, in contrast to e.g. those with the
</p>
<p>lower sign given in (U.13).
</p>
<p>U.2.2 Dirac Equation
</p>
<p>Since the Klein&ndash;Gordon equation does not allow for the familiar probability inter-
</p>
<p>pretation, it was rejected. About the year 1928, Paul Dirac130 found the equation
</p>
<p>which later was named after him. Since the problem of negative probabilities in the
</p>
<p>Klein&ndash;Gordon equation is connected with the second time derivative, he made the
</p>
<p>ansatz
</p>
<p>129m0 is the electron mass,  the fine structure constant, n the main quantum number and j the total
</p>
<p>angular momentum, j = l &plusmn; 1
2
</p>
<p>.
130Dirac, Paul Adrien Maurice, 1902&ndash;1984; British physicist, nobel prize 1933.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix U: Elements of Relativistic Quantum Mechanics 409
</p>
<p>i
&part;
</p>
<p>&part;t
 = H (U.15)
</p>
<p>with a yet to be determined operator H . As mentioned above in the introduction,
</p>
<p>it is perhaps surprising that this assumption together with the relativistic dispersion
</p>
<p>relation leads so to say automatically to an equation for particles with spin 1/2,
</p>
<p>without any additional requirements about angular momentum.
</p>
<p>U.2.2.1 Statement of the Problem
</p>
<p>We note that the dispersion relation (U.3) is not linear in E , as it is in the nonrelativistic
</p>
<p>case. To arrive at an equation of the form (U.15), i.e. a linear expression for E , one
</p>
<p>could take the root of (U.3)
</p>
<p>E = &plusmn;
&radic;
</p>
<p>c2p2 + c4m2 (U.16)
</p>
<p>which leads to
</p>
<p>i
&part;
</p>
<p>&part;t
 = &plusmn;
</p>
<p>&radic;
&minus;2c2&nabla;2 + c4m2 . (U.17)
</p>
<p>But this formulation is problematic. First, since it is a relativistic theory, time and
</p>
<p>space coordinates should be on equal footing. But in (U.17), spatial and temporal
</p>
<p>derivatives appear in an unsymmetrical manner. Second, how to cope with the square
</p>
<p>root operator? If we expand the square root in a power series by
&radic;
&minus;2c2&nabla;2 + c4m2 =
</p>
<p>mc2
&radic;
</p>
<p>1 &minus; 2
c2m2
</p>
<p>&nabla;
2 = mc2
</p>
<p>[
1 &minus; 2
</p>
<p>2c2m2
&nabla;
</p>
<p>2 + 1
8
</p>
<p>(

</p>
<p>2
</p>
<p>2c2m2
&nabla;
</p>
<p>2
)2
</p>
<p>&plusmn; . . .
]
</p>
<p>, we get an equation
</p>
<p>which contains all powers of the differential operator &nabla;2, i.e. a nonlocal theory.
</p>
<p>How to get rid of these problems? The basic idea is to use matrices in taking
</p>
<p>the square root. Consider the equation x2 = 1. If x is an ordinary number, we have
the two solutions x = &plusmn;1. But allowing for matrices and understanding the 1 as n-
dimensional unit matrix 1n , we can find other solutions (literally without the explicit
</p>
<p>use of the square root symbol
&radic;
</p>
<p>) as is seen in the next exercise. It was Paul Dirac
</p>
<p>who applied this basic idea in this context.
</p>
<p>One problem remains, namely the meaning of the two signs on the right hand side
</p>
<p>of (U.16) and (U.17). This issue is not only a technical one, but is deeply connected
</p>
<p>with the structure of the world. We see that we have to deal not only with positive,
</p>
<p>but also with negative energies - if we have found a solution for positive energy,
</p>
<p>there exists a solution for negative energy, too. One could perhaps suppose that the
</p>
<p>solutions with negative energy are an artefact and could be neglected. But as it turns
</p>
<p>out, this is not true. Indeed, solutions with negative energy are a common feature of
</p>
<p>relativistic theories and are connected to the existence of antiparticles; the point is
</p>
<p>treated in further chapters in this and the second volume.</p>
<p/>
</div>
<div class="page"><p/>
<p>410 Appendix U: Elements of Relativistic Quantum Mechanics
</p>
<p>U.2.2.2 The Structure of the Dirac Equation
</p>
<p>We want to find an expression which is linear in energy and momentum (considering
</p>
<p>relativistic time and space coordinates to be on an equal footing), whereby the squared
</p>
<p>expression must give the dispersion relation (U.3). The ansatz reads131
</p>
<p>E = c &middot; &middot; p+ mc2 (U.18)
</p>
<p>where  	= 0 and  	= 0 are mathematical objects whose properties are to be
determined. With the substitution (U.2), we arrive at the so-called Dirac equation,
</p>
<p>i.e. the quantum mechanical equation corresponding to (U.18):
</p>
<p>i
&part;
</p>
<p>&part;t
 = c
</p>
<p>i
 &middot;&nabla; + mc2 (U.19)
</p>
<p>where  =  (r, t) is the wave function.
Before discussing this equation in more detail, we need to know more about 
</p>
<p>and . Like p, the term  has three components;  has one component. The four
</p>
<p>terms and  do not necessarily commute - we keep in mind the use of matrices.
</p>
<p>Information about  and  is obtained by comparison with (U.3). Squaring the
</p>
<p>ansatz (U.18) yields
</p>
<p>E2 = c2 &middot; ( &middot; p) ( &middot; p)+ mc3 ( &middot; p) + mc3 ( &middot; p)+ m2c42 (U.20)
</p>
<p>and with E2 = c2p2 + m2c4, we get
</p>
<p>( &middot; p) ( &middot; p) = p2
( &middot; p) +  ( &middot; p) = 0
</p>
<p>2 = 1.
(U.21)
</p>
<p>By means of these equations, we have to determine  and the three components of
</p>
<p> as far as possible. We assume that the momentum p commutes with  and .
</p>
<p>To get an idea why we introduce matrices, let us first look at the second equation.
</p>
<p>Since it must hold for arbitrary momentum, we can write
</p>
<p>( + ) &middot; p = 0 &rarr;  +  = 0. (U.22)
</p>
<p>Evidently, the last equation cannot be fulfilled if  and are &lsquo;ordinary&rsquo; numbers - but
</p>
<p>with matrices, it works!
</p>
<p>Rearranging the equations (U.21), we can write them as
</p>
<p>131The factors c and c2 ensure that the matrices  and  have no physical dimension.
</p>
<p>Note that here  is not v
c
</p>
<p>. Moreover, the matrices  have nothing to do with the fine structure
</p>
<p>constant . To label two totally different things by the same symbol is perhaps annoying and
</p>
<p>confusing especially for beginners in SR, but it is common practice.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix U: Elements of Relativistic Quantum Mechanics 411
</p>
<p> jk + k j = 2 jk
 j +  j = 0
</p>
<p>2 = 1
(U.23)
</p>
<p>where all indices run from 1 to 3 and the unit matrix is abbreviated by 1 (yet we do
</p>
<p>not know the dimension of the matrices).
</p>
<p>General properties of the matrices  and  With (U.23), we have ten equations
</p>
<p>for the four matrices 1, 2, 3 and . Firstly, we demand that the matrices are
</p>
<p>hermitian in order that the ansatz (U.18 ) gives an hermitian Hamilton operator (this
</p>
<p>and the required commutativity with the momentum are assumptions to make life
</p>
<p>easier - they do not follow per se). In addition, the matrices are unitary; this is due
</p>
<p>to 2 = 1 &rarr;  = &minus;1 (analogously for the i ). This means that the eigenvalues of
the matrices are +1 or &minus;1.
</p>
<p>Thus, in (U.23) we have four unitary matrices which anticommute pairwise.132
</p>
<p>To get some information about the dimension of the matrices, we use the trace of
</p>
<p>the matrices. As an example, we consider the equation  j +  j = 0 which we
multiply from the right by  to arrive at.
</p>
<p> j = &minus; j. (U.24)
</p>
<p>Taking the trace of both sides and making use of the cyclical commutativity under
</p>
<p>the trace,133 we get
</p>
<p>tr  j = &minus;tr  j = &minus;tr  j2 = &minus;tr  j (U.25)
</p>
<p>from which follows tr  j = 0; analogously tr  = 0. The argument now runs as
follows: since the matrices are unitary, they are diagonalizable. Thus, they can be
</p>
<p>represented by diagonal matrices, whereby the eigenvalues appear in the diagonal;
</p>
<p>all other entries vanish.134 Since the eigenvalues are restricted to be +1 or &minus;1, the
condition tr  j = 0 can only be fulfilled if the matrices have even dimension - 2, 4,
6 and so on.
</p>
<p>The question, which dimension it should be, cannot be answered unambiguously.
</p>
<p>But at least, we can exclude dimension 2. This is due to the fact that in the space
</p>
<p>of 2&times; 2 matrices there are only three linearly independent anticommuting matrices,
but in (U.23) we need four matrices.
</p>
<p>Remembering Occham&rsquo;s razor, we attempt (as the next simplest case) to satisfy
</p>
<p>equations (U.23) by use of unitary 4&times; 4 -matrices. Since these are ten equations for
the four hermitian matrices 1, 2, 3 and  (with altogether 64 complex entries), it
</p>
<p>comes as no surprise that the problem is underdetermined and, correspondingly, that
</p>
<p>no unique solution exists.
</p>
<p>132A system like (U.23) is called a Clifford algebra.
133Remind tr AB = tr B A.
134Remember that eigenvalues and trace do not depend on the representation.</p>
<p/>
</div>
<div class="page"><p/>
<p>412 Appendix U: Elements of Relativistic Quantum Mechanics
</p>
<p>Standard representation of the matrices We summarize our results: under some
</p>
<p>weak assumptions, we arrived at a relativistic quantum mechanical description (the
</p>
<p>Dirac equation)
</p>
<p>i
&part;
</p>
<p>&part;t
 = c
</p>
<p>i
 &middot;&nabla; + mc2. (U.26)
</p>
<p> and are 4&times;4-matrices which fulfill (U.23). These conditions do not give an unique
solution; correspondingly, there are different representations of the Dirac equation.
</p>
<p>Some of them are common, and in the following, we want to make plausible one of
</p>
<p>them, the so-called standard representation of the Dirac equation.
</p>
<p>We start with 2 = 1, the third equation in (U.23 ), and choose
</p>
<p> =
(
</p>
<p>1 0
</p>
<p>0 &minus;1
</p>
<p>)
. (U.27)
</p>
<p>The 1 in the diagonal is the 2 &times; 2 unit matrix (writing 1 instead of E is common in
this context). We emphasize that (U.27 ) is not logically mandatory and every other
</p>
<p>choice is possible, provided it is an unitary matrix with vanishing trace.
</p>
<p>To evaluate the second equation of (U.23), we write the hermitian matrices i as
</p>
<p>i =
(
</p>
<p>Ai Bi
</p>
<p>B
&dagger;
i Di
</p>
<p>)
. (U.28)
</p>
<p>Note that Ai and the other entries in (U.28) are 2 &times; 2 matrices. Inserting (U.28) in
i + i = 0, we obtain
</p>
<p>i + i =
(
</p>
<p>2Ai 0
</p>
<p>0 &minus;2Di
</p>
<p>)
= 0. (U.29)
</p>
<p>It follows Ai = Di = 0, i.e.
i =
</p>
<p>(
0 Bi
</p>
<p>B
&dagger;
i 0
</p>
<p>)
. (U.30)
</p>
<p>Obviously, the condition tr i = 0 is fulfilled.
Finally, we evaluate the first equation of (U.23), jk+ jk = 2 jk . With (U.30)
</p>
<p>it follows
</p>
<p> jk +  jk =
(
</p>
<p>B j B
&dagger;
k + Bk B&dagger;j 0
</p>
<p>0 B
&dagger;
j Bk + B&dagger;k B j
</p>
<p>)
= 2 jk (U.31)
</p>
<p>or in short
</p>
<p>B j B
&dagger;
k + Bk B&dagger;j = 2 jk . (U.32)
</p>
<p>We assume that the B j are Hermitian matrices, B j = B&dagger;j (again, this is not mandatory,
but just convenient) and arrive at</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix U: Elements of Relativistic Quantum Mechanics 413
</p>
<p>B j Bk + Bk B j = 2 jk . (U.33)
</p>
<p>Comparing this equation with the anticommutation rule (U.125) of the Pauli matrices
</p>
<p>shows that we can identify the B j with the Pauli matrices  j , i.e.
</p>
<p> =
(
</p>
<p>0 
</p>
<p> 0
</p>
<p>)
;  =
</p>
<p>(
1 0
</p>
<p>0 &minus;1
</p>
<p>)
. (U.34)
</p>
<p>Note the block structure of the matrices  and . This is the so-called standard
</p>
<p>representation of these matrices. Depending on the matter of question, other repre-
</p>
<p>sentations as the Weyl or the Majorana representation may be better suited, but we
</p>
<p>won&rsquo;t use them.
</p>
<p>In this way we get as final result the standard representation of the Dirac equation
</p>
<p>i &part;
&part;t
 = c 
</p>
<p>i
 &middot;&nabla; + mc2 = c &middot; p + mc2
</p>
<p>with
</p>
<p> =
(
</p>
<p>0 
</p>
<p> 0
</p>
<p>)
;  =
</p>
<p>(
1 0
</p>
<p>0 &minus;1
</p>
<p>)
.
</p>
<p>(U.35)
</p>
<p>Remind that  and  are a 4 &times; 4-matrices.
</p>
<p>U.3 Plane Wave Solutions
</p>
<p>In this section, we consider the plane wave solutions of the Klein&ndash;Gordon and the
</p>
<p>Dirac equation with the focus on the results for the Dirac equation. We present
</p>
<p>solutions for the discrete case (finite Volume V ) and the continuous case (infinite
</p>
<p>volume).
</p>
<p>U.3.1 Klein&ndash;Gordon Equation
</p>
<p>The free solutions in the case of a finite volume V read135
</p>
<p> (x) =
&sum;
</p>
<p>k
</p>
<p>1&radic;
2Vk
</p>
<p>(
a (k) ei(kr&minus;k t) + a&dagger; (k) e&minus;i(kr&minus;k t)
</p>
<p>)
(U.36)
</p>
<p>where the sum runs over all allowed discrete values of k. The term a (k) is here an
</p>
<p>arbitrary amplitude. We state in advance that in quantum field theory, it is this term
</p>
<p>which will be quantized.
</p>
<p>135From now on, we write Ek and k instead of E and .</p>
<p/>
</div>
<div class="page"><p/>
<p>414 Appendix U: Elements of Relativistic Quantum Mechanics
</p>
<p>As stated above, energy and momentum are related by the relativistic dispersion
</p>
<p>relation
</p>
<p>Ek = k =
&radic;
</p>
<p>c22k2 + c4m2. (U.37)
</p>
<p>The continuous variant reads (cf. the preceding section) :
</p>
<p> (x) = 1
(2)3/2
</p>
<p>&int;
d3k&radic;
2k
</p>
<p>(
a (k) ei(kr&minus;k t) + a&dagger; (k) e&minus;i(kr&minus;k t)
</p>
<p>)
. (U.38)
</p>
<p>Notes: (1) There are different conventions for normalization. This one is chosen in
</p>
<p>view of later applications in quantum field theory; see the exercises. (2) With regard
</p>
<p>to considerations in quantum field theory, we write a&dagger; (k) (Hermitian adjoint) and
</p>
<p>not simply a&lowast; (k) (complex conjugated) since later on, in quantum field theory, the
amplitudes will be quantized, i.e., turned into operators. At this point, the notation
</p>
<p>a&dagger; is is not per se evident. (3) Note that we use the same symbol a (k) in the discrete
</p>
<p>and the continuous case. Strictly speaking one would have to make a distinction e.g.
</p>
<p>by different names. But using the same notation is quite common and functional, and
</p>
<p>confusion should be unlikely.
</p>
<p>All three points apply the Dirac equation, too.
</p>
<p>U.3.2 Dirac Equation
</p>
<p>Next we want to find the free solutions of the Dirac equation
</p>
<p>i
&part;
</p>
<p>&part;t
 = c
</p>
<p>i
 &middot;&nabla; + mc2 = c &middot; p + mc2 (U.39)
</p>
<p>where the matrices are given e.g. in standard representation
</p>
<p> =
(
</p>
<p>0 
</p>
<p> 0
</p>
<p>)
;  =
</p>
<p>(
1 0
</p>
<p>0 &minus;1
</p>
<p>)
. (U.40)
</p>
<p>In contrast to the Klein&ndash;Gordon case, the solutions are not scalar, since  has four
</p>
<p>components which complicates things a little bit. In addition,  is not a &lsquo;normal&rsquo;
</p>
<p>4-vector but a 4-spinor. The name has to do with the transformation behavior of 
</p>
<p>under Lorentz transformations (see Appendix T, Vol. 1) which in turn leads to another
</p>
<p>definition of the inner product of two spinors. We know that the inner product of two
</p>
<p>vectors a and b is defined by a&dagger;b where a&dagger; is the Hermitian adjoint of a. In contrast,
</p>
<p>the inner product of two 4-spinors  and  is defined by  where the adjungated
</p>
<p>spinor or Dirac adjoint  is defined by  := &dagger; with &dagger; the Hermitian adjoint.136
We remark that in the bulk of the book, treating nonrelativistic quantum mechanics,
</p>
<p>136Note the differences in the definition of the inner product of 3-vectors, 4-vectors and 4-spinors.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix U: Elements of Relativistic Quantum Mechanics 415
</p>
<p>it was not necessary to distinguish between &lsquo;adjungated&rsquo; and &lsquo;hermitian adjungated&rsquo;.
</p>
<p>Thus, these terms are often used synonymously. But here, in the context of the Dirac
</p>
<p>equation, we have to make precise distinctions.
</p>
<p>U.3.2.1 Particle at Rest
</p>
<p>Before we attempt the general plane wave solution, it is instructive to consider the
</p>
<p>case of a particle at rest, i.e., p = 0. This means that the spatial derivatives vanish
and we have to look for solutions of the simpler equation
</p>
<p>i
&part;
</p>
<p>&part;t
p=0 = mc2p=0 = mc2
</p>
<p>(
1 0
</p>
<p>0 &minus;1
</p>
<p>)
p=0. (U.41)
</p>
<p>The ansatz p=0 = 0e&minus;i Et/ leads to
</p>
<p>E0 = mc20 = mc2
</p>
<p>

</p>
<p>1 0 0 0
</p>
<p>0 1 0 0
</p>
<p>0 0 &minus;1 0
0 0 0 &minus;1
</p>
<p>
0. (U.42)
</p>
<p>We see immediately that there are two solutions (+) with positive energy, E = mc2,
and two solutions (&minus;) with negative energy, E = &minus;mc2. With  = mc2
</p>
<p>
they read
</p>
<p>explicitly
</p>
<p>
(+)
1,p=0 =
</p>
<p>

</p>
<p>1
</p>
<p>0
</p>
<p>0
</p>
<p>0
</p>
<p>
 e&minus;it ; 
</p>
<p>(+)
2,p=0 =
</p>
<p>

</p>
<p>0
</p>
<p>1
</p>
<p>0
</p>
<p>0
</p>
<p>
 e&minus;it
</p>
<p>
(&minus;)
1,p=0 =
</p>
<p>

</p>
<p>0
</p>
<p>0
</p>
<p>1
</p>
<p>0
</p>
<p>
 eit ; 
</p>
<p>(&minus;)
2,p=0 =
</p>
<p>

</p>
<p>0
</p>
<p>0
</p>
<p>0
</p>
<p>1
</p>
<p>
 eit .
</p>
<p>(U.43)
</p>
<p>Introducing the 2-spinors
</p>
<p>1 =
(
</p>
<p>1
</p>
<p>0
</p>
<p>)
, 2 =
</p>
<p>(
0
</p>
<p>1
</p>
<p>)
(U.44)
</p>
<p>we can write this more compactly as
</p>
<p>
(+)
s,p=0 =
</p>
<p>(
s
0
</p>
<p>)
e&minus;it ; (&minus;)s,p=0 =
</p>
<p>(
0
</p>
<p>s
</p>
<p>)
eit ; s = 1, 2. (U.45)</p>
<p/>
</div>
<div class="page"><p/>
<p>416 Appendix U: Elements of Relativistic Quantum Mechanics
</p>
<p>U.3.2.2 Moving Particle
</p>
<p>Guided by these results, we make for the general plane wave solution the ansatz for
</p>
<p>positive and negative energy
</p>
<p>(+) = us ei(kr&minus;t) and (&minus;) = vs e&minus;i(kr&minus;t) ; s = 1, 2. (U.46)
</p>
<p>Inserting (U.46) into the Dirac equation yields
</p>
<p>us = c &middot; kus + mc2us and &minus; vs = &minus;c &middot; kvs + mc2vs . (U.47)
</p>
<p>Using E =  and p =k brings E0 = c &middot; p0 + mc20 or
(
c &middot; p+ mc2 &minus; E
</p>
<p>)
us = 0 and
</p>
<p>(
c &middot; p&minus;mc2 &minus; E
</p>
<p>)
vs = 0. (U.48)
</p>
<p>With Ep =
&radic;
</p>
<p>m2c4 + c2p2 and the 2-spinors s , the normalized solutions read
</p>
<p>us (p) =
&radic;
</p>
<p>Ep + mc2
2mc2
</p>
<p>(
s
</p>
<p>c p
Ep+mc2
</p>
<p>s
</p>
<p>)
; vs (p) =
</p>
<p>&radic;
Ep + mc2
</p>
<p>2mc2
</p>
<p>(
c p
</p>
<p>Ep+mc2
s
</p>
<p>s
</p>
<p>)
; s = 1, 2. (U.49)
</p>
<p>The four spinors us (p) and vs (p) as given in (U.49) are linearly independent.
</p>
<p>They satisfy the following relations:
</p>
<p>ur (p) us (p) = rs ; vr (p) vs (p) = &minus;rs
ur (p) vs (p) = 0 ; vr (p) us (p) = 0 &forall;r, s (U.50)
</p>
<p>where ur is the Dirac adjoint of ur .
</p>
<p>The general solution is a superposition of all allowed partial solutions .It reads
</p>
<p> (x) = &sum;p,s
&radic;
</p>
<p>m
Vp
</p>
<p>(
bs (p) us (p) e
</p>
<p>i(pr&minus;Ep t)/ + d&dagger;s (p) vs (p) e&minus;i(pr&minus;Ep t)/
)
</p>
<p>discrete case
</p>
<p> (x) = &sum;s
&int;
</p>
<p>d3 p
&radic;
</p>
<p>m
</p>
<p>(2)3p
</p>
<p>(
bs (p) us (p) e
</p>
<p>i(pr&minus;p t)/ + d&dagger;s (p) vs (p) e&minus;i(pr&minus;p t)/
)
</p>
<p>continuous case
</p>
<p>(U.51)
</p>
<p>where bs (p) and ds (p) are arbitrary amplitudes. We write d
&dagger;
s (p) and not d
</p>
<p>&lowast;
s (p) in
</p>
<p>view of later applications in Quantum Field where the terms bs (p) and ds (p) are
</p>
<p>changed into operators.
</p>
<p>U.3.2.3 Plane Waves, Limiting Cases
</p>
<p>To shed some light on the physical meaning of the plane waves (U.51), we consider
</p>
<p>the basic spinors (U.49) for the two limiting cases p &rarr; 0 (i.e., the nonrelativistic
case) and p &rarr; &infin;. Remember that us (p) describes solutions with positive energy
and vs (p) solutions with negative energy.
</p>
<p>Case p &rarr; 0 In the limit p &rarr; 0, we have from (U.49)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix U: Elements of Relativistic Quantum Mechanics 417
</p>
<p>us (p) &rarr;
(
s
0
</p>
<p>)
; vs (p) &rarr;
</p>
<p>(
0
</p>
<p>s
</p>
<p>)
; s = 1, 2 (U.52)
</p>
<p>which agrees with (U.43).
</p>
<p>Considering the solution for positive energy, we see that there is a certain resem-
</p>
<p>blance to the two basic states of a spin-1/2-particle, at least with respect to the first
</p>
<p>and second entry. This assumption is enhanced by the fact that the third and fourth
</p>
<p>entry vanish and the solutions for positive and negative energy are strictly separated.
</p>
<p>At this point, one could perhaps nurture the hope of just neglecting the &lsquo;lower&rsquo; parts
</p>
<p>with negative energy, being something like an artefact of the theory.
</p>
<p>Plane wave solutions, p &rarr; &infin; Unfortunately (indeed rather fortunately) this faint
hope is immediately destroyed by inspection of (U.49). To get the point clearer we
</p>
<p>consider the fully relativistic case p &rarr; &infin; which means Ep &rarr; c |p|  mc2. This
yields
</p>
<p>us (p) &rarr;
&radic;
</p>
<p>|p|
2mc
</p>
<p>(
s
</p>
<p>
p
</p>
<p>|p|s
</p>
<p>)
; vs (p) =
</p>
<p>&radic;
|p|
</p>
<p>2mc
</p>
<p>(

</p>
<p>p
</p>
<p>|p|s
s
</p>
<p>)
; s = 1, 2. (U.53)
</p>
<p>We see that all four entries in the solutions for positive and negative energy have the
</p>
<p>same order of magnitude and are inextricably coupled (note
</p>
<p> p|p|
 = 1).137 This
</p>
<p>means we cannot neglect neither the third and fourth entry in us nor vs as such. Thus,
</p>
<p>we have to discuss the question how to interpret the solutions for negative energy
</p>
<p>and, in addition, clarify the question why we have dimension 4 for spin 1/2 instead
</p>
<p>of the familiar dimension 2 of the state space.
</p>
<p>U.3.2.4 Spin
</p>
<p>The spinors (U.45) look very much like particles with spin 1/2, and this impression
</p>
<p>is confirmed by application of the spin operator . Since we here have 4-spinors,
</p>
<p>the spin operator is not given just by , but by
</p>
<p> =
2
</p>
<p>(
 0
</p>
<p>0 
</p>
<p>)
. (U.54)
</p>
<p>This relation can be shown formally, but the derivation is quite lengthy, so we just
</p>
<p>report the result.138 Applying e.g. 3, we have
</p>
<p>3
(+)
1,p=0 =
</p>
<p>
</p>
<p>2
</p>
<p>(
3 0
</p>
<p>0 3
</p>
<p>)(
1
0
</p>
<p>)
e&minus;it = 
</p>
<p>2
</p>
<p>(
31
</p>
<p>0
</p>
<p>)
e&minus;it = 
</p>
<p>2
</p>
<p>(
1
0
</p>
<p>)
= 
</p>
<p>2

(+)
1,p=0
</p>
<p>(U.55)
</p>
<p>137By the way, the operator 
p
|p| is called helicity operator.
</p>
<p>138Note that  has 3 components (like ), but each of the components is a 4 &times; 4 matrix in spinor
space.</p>
<p/>
</div>
<div class="page"><p/>
<p>418 Appendix U: Elements of Relativistic Quantum Mechanics
</p>
<p>and analogously
</p>
<p>3
(+)
2,p=0 = &minus;
</p>
<p>
</p>
<p>2

(+)
1,p=0 ; 3(&minus;)1,p=0 =
</p>
<p>
</p>
<p>2

(&minus;)
1,p=0 ; 3(&minus;)2,p=0 = &minus;
</p>
<p>
</p>
<p>2

(&minus;)
1,p=0. (U.56)
</p>
<p>Thus, we have two particles with spin 1/2, one for positive energy and one for negative
</p>
<p>energy. They have four components instead of two which fact is connected with the
</p>
<p>occurrence of negative energies. We postpone the interpretation of the negative energy
</p>
<p>solutions, but accept them temporarily as mathematical correct solutions (in fact, as
</p>
<p>we will see below, they are also physical correct solutions).
</p>
<p>If the particles are moving, the situation is not as simple. For faster and faster
</p>
<p>relativistic particles, the spin aligns more and more to the velocity vector. Indeed, for
</p>
<p>massless particles which travel at c, the spin is always directed parallel or antiparallel
</p>
<p>to the velocity vector. Thus, in general, things are more complicated than in the rest
</p>
<p>system (i.e., p = 0).There is one exception, namely p1 = p2 = 0 and p = p3 &gt; 0,
i.e., translation in the spin axis direction. We have for instance (with E =
</p>
<p>&radic;
Ep+mc2
</p>
<p>2mc2
)
</p>
<p>3u1 (0, 0, p3) = E

</p>
<p>2
</p>
<p>(
3 0
</p>
<p>0 3
</p>
<p>)(
1
</p>
<p>c3 p3
Ep+mc2
</p>
<p>1
</p>
<p>)
= E 
</p>
<p>2
</p>
<p>(
31
</p>
<p>c3 p33
Ep+mc2
</p>
<p>1
</p>
<p>)
= 
</p>
<p>2
u1 (0, 0, p3) (U.57)
</p>
<p>(due to 31 = 1) and analogously for the other spinors.
Taken all together we can state the solutions (U.51) describe two types of particles
</p>
<p>with spin 1/2. One type is related to positive energy, the other to negative energy. In
</p>
<p>the frame of Dirac equation there is no really convincing explanation of the particle
</p>
<p>with negative energies but there is reason to believe that it is the antiparticle of the
</p>
<p>electron, i.e. the positron. This will be corroborated in quantum field theory.
</p>
<p>In any case, the important point here is that the Dirac equation allows for the
</p>
<p>description of relativistic particles with spin 1/2 whereby this fact follows without
</p>
<p>further assumptions from the ansatz itself.
</p>
<p>U.4 Covariant Formulation of the Dirac Equation
</p>
<p>We have derived the Dirac equation (DE)
</p>
<p>i
&part;
</p>
<p>&part;t
 = c
</p>
<p>i
 &middot;&nabla; + mc2. (U.58)
</p>
<p> and  are 4 &times; 4 matrices, and the state  has four components.
Deriving such an equation is only the first step and several questions are open at
</p>
<p>this stage. For instance, we have to verify that (U.58) is fully compatible with all
</p>
<p>requirements of special relativity. Since this is most easily done using a covariant
</p>
<p>notation of the equation, we will tackle this issue now. Thereafter, we will connect
</p>
<p>the DE to the electromagnetic field. In this way, we will obtain the Pauli equation as
</p>
<p>the non-relativistic approximation of the DE.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix U: Elements of Relativistic Quantum Mechanics 419
</p>
<p>First a short comment on the term covariance (covariant). It has distinct, but
</p>
<p>related meanings which all have to do with the behavior under transformations.
</p>
<p>(1) A physical quantity can be (Lorentz) covariant. Consider e.g. a 4-vector. Its
</p>
<p>length remains unchanged under Lorentz transformations (coordinate transforma-
</p>
<p>tions), but its components change covariantly. Scalars are invariant under coordinate
</p>
<p>transformations, vector components are covariant. Other covariant objects are spinors
</p>
<p>and tensors.
</p>
<p>(2) An equation or a theory is said to be covariant if it can be written in terms of
</p>
<p>covariant-only quantities. As a consequence, such a theory or equation has the same
</p>
<p>form in all reference frames (inertial systems) and is said to be form invariant (this
</p>
<p>is the main criterion). As an example, the Dirac equation must have the same form
</p>
<p>for all observers, independently from their inertial system.
</p>
<p>(3) Do not confuse this meaning of &lsquo;covariant&rsquo; with the use of the terms covariant
</p>
<p>and contravariant vectors, common in SR. These labels are established, but quite
</p>
<p>unfortunate. In any case, covariant and contravariant vectors are both transforming
</p>
<p>covariantly.
</p>
<p>U.4.1 Introducing  Matrices
</p>
<p>To tackle the mentioned questions, we first introduce and discuss the so-called 
</p>
<p>matrices. We start with the Dirac equation (U.58), i.e.,
</p>
<p>i
&part;
</p>
<p>&part;t
 = c
</p>
<p>i
 &middot;&nabla; + mc2 (U.59)
</p>
<p>where  has the components i , i = 1, 2, 3.139 Dividing both sides by c and using
x0 = ct , we arrive at140
</p>
<p>i
&part;
</p>
<p>&part;x0
 = 
</p>
<p>i
</p>
<p>3&sum;
</p>
<p>k=1
k
</p>
<p>&part;
</p>
<p>&part;xk
 + mc. (U.60)
</p>
<p>Note that we use the covariant form of the gradient (see Appendix T, Vol. 1). Mul-
</p>
<p>tiplying both sides by  and writing &part;
&part;x
</p>
<p>= &part; and dividing by i yields (remember
2 = 1) (
</p>
<p>&part;0 +
3&sum;
</p>
<p>k=1
k&part;k
</p>
<p>)
 = mc
</p>
<p>i
. (U.61)
</p>
<p>139Note that in this context  behaves like a 3-vector; hence, we can use upper or lower indices,
</p>
<p>i = i .
140&part; = &part;&part;x =
</p>
<p>(
1
c
</p>
<p>&part;
&part;t
,&nabla;
</p>
<p>)
= (&part;0,&nabla;) and &part; = &part;&part;x =
</p>
<p>(
1
c
</p>
<p>&part;
&part;t
,&minus;&nabla;
</p>
<p>)
= (&part;0,&minus;&nabla;)</p>
<p/>
</div>
<div class="page"><p/>
<p>420 Appendix U: Elements of Relativistic Quantum Mechanics
</p>
<p>In order to simplify the notation, we define new matrices by141
</p>
<p>0 :=  ; k := k . (U.62)
</p>
<p>These matrices are called  matrices or Dirac matrices; note that they are upstairs
</p>
<p>objects, i.e., contravariant. Invoking the summation rule, we can write
</p>
<p>(
0&part;0 + k&part;k
</p>
<p>)
 = &part; =
</p>
<p>mc
</p>
<p>i
. (U.63)
</p>
<p>With p = i&part; it follows
 p = mc. (U.64)
</p>
<p>Thus, on the l.h.s, we have with  p somewhat like a inner product (somewhat,
</p>
<p>because the  are matrices; but this point can be cleared positively), and we know
</p>
<p>that inner products are invariant; the same holds for the scalar mc on the r.h.s. This
</p>
<p>means that ( U.64) is a good candidate for Lorentz covariance.
</p>
<p>Since inner products like &part; occur often in relativistic theories, a special short-
</p>
<p>hand has been established for these objects, also called Feynman slash notation,
</p>
<p>namely
</p>
<p>a/ =  &middot; a = a = a = 0a0 &minus; kak = 0a0 &minus;  &middot; a. (U.65)
</p>
<p>With this notation, the Dirac equation reads
</p>
<p>&part;/ = mc
i
</p>
<p>. (U.66)
</p>
<p>With p = i&part;, we arrive at the presumably most streamlined form of the Dirac
equation
</p>
<p>p/ = mc. (U.67)
</p>
<p>U.4.1.1 Properties of the  Matrices
</p>
<p>The new found  matrices play a dominant role in &lsquo;higher&rsquo; relativistic theories where
</p>
<p>they replace completely the matrices  and . In the following, we will discuss some
</p>
<p>of their properties. Their covariant form reads  = g =
(
0,&minus;1,&minus;2,&minus;3
</p>
<p>)
.
</p>
<p>In most manipulations, it is easiest to think of the 4-tuple  as of a matrix-valued
</p>
<p>4-vector, though it is a slight misnomer.
</p>
<p>141Unfortunately, these matrices are named also by the letter  but should not confused with
</p>
<p> = 1&radic;
1&minus;( vc )
</p>
<p>2
.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix U: Elements of Relativistic Quantum Mechanics 421
</p>
<p>Explicit formulation in the standard representation In the standard representation
</p>
<p>(also called Dirac representation), i.e.,
</p>
<p> =
(
</p>
<p>1 0
</p>
<p>0 &minus;1
</p>
<p>)
;  =
</p>
<p>(
0 
</p>
<p> 0
</p>
<p>)
(U.68)
</p>
<p>the  matrices are given by
</p>
<p>0 =
(
</p>
<p>1 0
</p>
<p>0 &minus;1
</p>
<p>)
;  =
</p>
<p>(
0 
</p>
<p>&minus; 0
</p>
<p>)
(U.69)
</p>
<p>or explicitly by
</p>
<p>0 =
</p>
<p>

</p>
<p>1 0 0 0
</p>
<p>0 1 0 0
</p>
<p>0 0 &minus;1 0
0 0 0 &minus;1
</p>
<p>
 ; 1 =
</p>
<p>

</p>
<p>0 0 0 1
</p>
<p>0 0 1 0
</p>
<p>0 &minus;1 0 0
&minus;1 0 0 0
</p>
<p>

</p>
<p>2 =
</p>
<p>

</p>
<p>0 0 0 &minus;i
0 0 i 0
</p>
<p>0 i 0 0
</p>
<p>&minus;i 0 0 0
</p>
<p>
 ; 3 =
</p>
<p>

</p>
<p>0 0 1 0
</p>
<p>0 0 0 &minus;1
&minus;1 0 0 0
0 1 0 0
</p>
<p>
 .
</p>
<p>(U.70)
</p>
<p>There are other representations of the gamma matrices like the Weyl or chiral
</p>
<p>representation or the Majorana representation. They have different useful properties
</p>
<p>for certain calculations, but we will not need them. For details see the literature.142
</p>
<p>U.4.1.2 (Anti-) Commutation Rules for the  Matrices
</p>
<p>The matrices  and  obey the commutation rules
</p>
<p> jk + k j = 2 jk
k + k = 0 or k = &minus;k
</p>
<p>2i = 2 = 1.
(U.71)
</p>
<p>With
</p>
<p>0 :=  ; k := k &rarr;  = 0 ; k = k = &minus;k (U.72)
</p>
<p>we arrive for the Dirac matrices at
</p>
<p>142In addition to the four matrices  one defines a matrix 5 by 5 = i0123. The index 5
stems from the former notation of 4 instead of today&rsquo;s 0. 5 is hermitian, its eigenvalues are &plusmn;1,
and it anticommutes with the four .</p>
<p/>
</div>
<div class="page"><p/>
<p>422 Appendix U: Elements of Relativistic Quantum Mechanics
</p>
<p>&minus; jk &minus; k j = &minus; jk &minus; k j = 2 jk
k + k = 0 &rarr; 0k + k0 = 0(
</p>
<p>0
)2 = 1 ;
</p>
<p>(
i
)2 = &minus;1.
</p>
<p>(U.73)
</p>
<p>This relations may be formulated compactly with the help of the elements g of the
</p>
<p>metric tensor:
</p>
<p> +  = {, } = 2g &middot; 1 (U.74)
</p>
<p>where 1 is the 4 &times; 4 unit matrix.143
</p>
<p>U.4.1.3 Adjoint Dirac Equation, Continuity Equation
</p>
<p>We start from the DE in the form
</p>
<p>i&part; &minus;
mc
</p>
<p>
 = 0. (U.75)
</p>
<p>Taking the hermitian conjugate of this equation and multiplying it from the right by
</p>
<p>0 gives the adjoint Dirac equation ( = &dagger;0):
</p>
<p>i
(
&part;
</p>
<p>)
 + mc
</p>
<p>
 = 0. (U.76)
</p>
<p>We multiply the Dirac equation from left by  and the adjoint Dirac equation from
</p>
<p>right by :
</p>
<p>i&part; &minus;
mc
</p>
<p>
 = 0 ; i
</p>
<p>(
&part;
</p>
<p>)
 + mc
</p>
<p>
 = 0. (U.77)
</p>
<p>Adding theses two equations gives
</p>
<p>&part; +
(
&part;
</p>
<p>)
 = 0 or &part;
</p>
<p>(

</p>
<p>)
= 0. (U.78)
</p>
<p>Reading this equation as continuity equation144 &part; j
 = 0 defines the Dirac 4-current
</p>
<p>by
</p>
<p>j = . (U.79)
</p>
<p>Hence, the probability density  = j0 is given by
</p>
<p> = j0 = 0 = &dagger;. (U.80)
</p>
<p>Evidently, the probability density  is positive definite.
</p>
<p>143Like the matrices  and , also the  matrices generate a Clifford algebra.
144See Appendix T, Vol. 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix U: Elements of Relativistic Quantum Mechanics 423
</p>
<p>U.4.2 How to Show the Covariance of the Dirac Equation - A
</p>
<p>Short Outline
</p>
<p>In theoretical physics, an important principle is that coordinates are manmade and
</p>
<p>do not exist in nature; hence, they should play no role in the formulation of physical
</p>
<p>laws. In our case this means that the Dirac equation has to be form invariant, i.e., has
</p>
<p>to have the same form in all inertial systems.
</p>
<p>We assume two inertial systems I and I with coordinates x and x = 	x and wave
functions  (x) and  (x).145 There has to exist a unique relation between  (x) and
</p>
<p> (x). Since both the Dirac equation and the Lorentz transformation are linear, this
</p>
<p>relation has to be linear; furthermore, since the wave functions have 4 components,
</p>
<p>this relation is a 4 &times; 4-matrix S (	), i.e.,
</p>
<p> (x) = S (	) (x) = S (	)
(
	&minus;1 x
</p>
<p>)
. (U.81)
</p>
<p>Thus, Lorentz covariance means that, by use of x = 	x and  (x) = S (	)
(
	&minus;1 x
</p>
<p>)
,
</p>
<p>the Dirac equation in I is transformed into a Dirac equation in I . In other words: in
</p>
<p>I and I , the Dirac equations read
</p>
<p>(
i&part; &minus; mc
</p>
<p>)
 (x) = 0 and
</p>
<p>(
i&part; &minus; mc
</p>
<p>)
 (x) = 0 (U.82)
</p>
<p>with &part; = &part;&part; x . To arrive at an equation for S (	) , we use
</p>
<p>&part; =
&part;
</p>
<p>&part;x
= &part; x
</p>
<p>
</p>
<p>&part;x
&part;
</p>
<p>&part; x
= 	
</p>
<p>&part;
</p>
<p>&part; x
. (U.83)
</p>
<p>We insert this and  (x) = S&minus;1 (	)  (x) into the Dirac equation of I . This yields
(
</p>
<p>i	
&part;
</p>
<p>&part; x
&minus; mc
</p>
<p>)
S&minus;1 (	)  (x) = 0. (U.84)
</p>
<p>We multiply from the left with S:
</p>
<p>iS (	) 	&part;S
&minus;1 (	)  (x)&minus; mc (x) = 0. (U.85)
</p>
<p>Comparison with the Dirac equation in I shows that we have to look for a solution
</p>
<p>S of the equation
</p>
<p>S (	) 	&part;S
&minus;1 (	) = &part; =  &part; . (U.86)
</p>
<p>145We here can assume that 	 encompasses all Lorentz transformations, i.e., apart from the boost
</p>
<p>also rotations, space reflections etc. In advanced theories there occurs e.g. parity violation (in weak
</p>
<p>interactions), but we are not concerned with this.
</p>
<p>One also can perform the considerations for the case of the more general Poincare transformation
</p>
<p>x = 	x + a, of course with the same result.</p>
<p/>
</div>
<div class="page"><p/>
<p>424 Appendix U: Elements of Relativistic Quantum Mechanics
</p>
<p>It follows
</p>
<p>	&part; = S&minus;1 (	)  &part;S (	) (U.87)
</p>
<p>or146
</p>
<p>S&minus;1 (	) S (	) = 	. (U.88)
</p>
<p>This is the fundamental equation to determine S (	) . Finding a solution for all 	
</p>
<p>proves the covariance of the Dirac equation. As an example, consider space reflection
</p>
<p>which is given by
</p>
<p>	 =
</p>
<p>

</p>
<p>1 0 0 0
</p>
<p>0 &minus;1 0 0
0 0 &minus;1 0
0 0 0 &minus;1
</p>
<p>
 = g
</p>
<p>. (U.89)
</p>
<p>Equation (U.88) reads in this case
</p>
<p>S&minus;1S = 	 = g = g (U.90)
</p>
<p>without summation over  in the last expression. It follows
</p>
<p>S&minus;10S = 0 ; S&minus;1k S = &minus;k &rarr; S = 0 (U.91)
</p>
<p>where we have taken into account (U.74).147 In this way, the transformation (U.81)
</p>
<p>reads
</p>
<p> (x) =  (t, x) =  (t,&minus;x) = 0 (t, x) . (U.92)
</p>
<p>In total, the parity transformation for spinors can be written as
</p>
<p>P = 0 P (x) (U.93)
</p>
<p>where P (x) causes the space reflection x &rarr; &minus;x.
In a similar way one can show that for all Lorentz transformations there is a
</p>
<p>solution S (	) of equation (U.88) which fact states the covariance of the Dirac
</p>
<p>equation. The calculations are a little bit lengthy and cumbersome and we omit
</p>
<p>them. For the extensive details see the literature.
</p>
<p>A remark on nomenclature: A wave function  which transforms corresponding
</p>
<p>to (U.81) with S given by (U.88) is called (4-component) Lorentz spinor or 4-spinor.
</p>
<p>Note that  it is not a 4-vector which is defined by its behavior under Lorentz
</p>
<p>transformation as a = 	 a .
</p>
<p>146Note that 	 are matrix elements which, as being scalars, commute with the Dirac matrices
</p>
<p>, of course.
147One can allow for an arbitrary phase factor ei, in addition.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix U: Elements of Relativistic Quantum Mechanics 425
</p>
<p>U.4.3 Coupling to the Electromagnetic Field
</p>
<p>Finally, we want to formulate the Dirac equation in an electromagnetic field. To this
</p>
<p>end, we start with the free Dirac equation in the form
</p>
<p> p = p/ = mc. (U.94)
</p>
<p>As in nonrelativistic quantum mechanics, we invoke the principle of minimal cou-
</p>
<p>pling,148 i.e., replace the 4-momentum p by149
</p>
<p>p &rarr; p &minus; q A. (U.95)
</p>
<p>A is the 4-potential, A =
(

c
,A
</p>
<p>)
with the scalar potential  and the three compo-
</p>
<p>nents of the vector potential A. The resulting equation reads
</p>
<p>
(
</p>
<p>p &minus; q A
)
 = (p/&minus; q A/) = mc. (U.96)
</p>
<p>The substitution does not affect the considerations on the covariance of the Dirac
</p>
<p>equation. This argumentation was based on the fact that p is a 4-vector, but A is a
</p>
<p>4-vector, too, and of course also the difference p &minus; q A. Thus, (U.96) is invariant
in the sense that every observer would find exactly this form of the equation in his
</p>
<p>reference frame, i.e., inertial system.
</p>
<p>To make contact with the Dirac equation in the formulation (U.58), i.e. i &part;
&part;t
 =
</p>
<p>c 
i
 &middot; &nabla; + mc2, we use p = i&part; (cf. Appendix T, Vol. 1). This means
</p>
<p>explicitly
</p>
<p>p0 &rarr; p0 &minus; q A0 =&rArr; i&part;0 &rarr; i&part;0 &minus; q
c
 =&rArr; i &part;
</p>
<p>&part;t
&rarr; i &part;
</p>
<p>&part;t
&minus; q
</p>
<p>pk &rarr; pk &minus; q Ak =&rArr; 
i
&part;k &rarr; 
</p>
<p>i
&part;k &minus; q Ak =&rArr; 
</p>
<p>i
&nabla; &rarr; 
</p>
<p>i
&nabla; &minus; qA. (U.97)
</p>
<p>Thus, the Dirac equation in an electromagnetic field reads
</p>
<p>i
&part;
</p>
<p>&part;t
 = c
</p>
<p>(

</p>
<p>i
&nabla; &minus; qA
</p>
<p>)
 + q + mc2. (U.98)
</p>
<p>U.4.4 Nonrelativistic Limit of the Dirac Equation
</p>
<p>Since the Dirac equation is formulated for 4-spinors and the nonrelativistic Pauli
</p>
<p>equation for 2-spinors, it seems advantageous to consider the 4-spinor as composed
</p>
<p>of two 2-spinors, i.e.,
</p>
<p>148Called minimal coupling, because it is the simplest nontrivial coupling compatible with gauge
</p>
<p>invariance. As far as we know, it is also the possibility which is realized by nature.
149q is the charge of the particle under consideration, e.g. q = &minus;e0 for an electron.</p>
<p/>
</div>
<div class="page"><p/>
<p>426 Appendix U: Elements of Relativistic Quantum Mechanics
</p>
<p> =
(

</p>
<p>
</p>
<p>)
. (U.99)
</p>
<p>With  =
</p>
<p>(
0 
</p>
<p> 0
</p>
<p>)
and  =
</p>
<p>(
1 0
</p>
<p>0 &minus;1
</p>
<p>)
and the abbreviation
</p>
<p> = p&minus; qA (U.100)
</p>
<p>we can write the Dirac equation (U.98) in the form
</p>
<p>i &part;
&part;t
 = c &middot; + q+ mc2
</p>
<p>i &part;
&part;t
 = c &middot; + q&minus; mc2. (U.101)
</p>
<p>In the nonrelativistic limit, the rest mass mc2 is the by far the largest energy in the
</p>
<p>expression E =
&radic;
</p>
<p>m2c4 + c2 p2. Therefore, for p &rarr; 0 the ansatz
(

</p>
<p>
</p>
<p>)
= e&minus;imc2t/
</p>
<p>(
nr
nr
</p>
<p>)
(U.102)
</p>
<p>is appropriate to describe the solution for positive energy. It follows
</p>
<p>i &part;
&part;t
nr = c &middot; nr + qnr
</p>
<p>i &part;
&part;t
nr = c &middot; nr + qnr &minus; 2mc2nr .
</p>
<p>(U.103)
</p>
<p>In the non-relativistic limit, we have |q|  2mc2. In addition, the functions nr
and nr vary only very slowly with respect to time. Thus, we have in the second
</p>
<p>equation
i &part;
</p>
<p>&part;t
nr
</p>
<p> 
2mc2nr
</p>
<p>, from which follows c &middot; nr &asymp; 2mc2nr or, as
an estimation of the order of magnitude,
</p>
<p>
nr
</p>
<p>nr
</p>
<p> &asymp;
c &middot; 
</p>
<p>2mc2
</p>
<p> &asymp;
 p
2mc
</p>
<p> &asymp;
 v
2c
</p>
<p> . (U.104)
</p>
<p>We see that in the nonrelativistic limit, nr is smaller than nr by a factor &sim; v/c.
Therefore, for v &rarr; 0, one often calls  and  large and small component of the
spinor .
</p>
<p>Using this result, we can write the first equation in (U.103) in a good approximation
</p>
<p>as
</p>
<p>i
&part;
</p>
<p>&part;t
nr =  &middot; 
</p>
<p> &middot; 
2m
</p>
<p>nr + qnr . (U.105)
</p>
<p>To evaluate the term  &middot;  &middot;, we use  &middot; a  &middot; b = a &middot; b+ i &middot; (a &times; b). Thus, we
have
</p>
<p> &middot;   &middot;  = 2 + i &middot; ( &times; ) . (U.106)
</p>
<p>Note that  &times;  does not vanish, since  is an operator. In fact, we have</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix U: Elements of Relativistic Quantum Mechanics 427
</p>
<p>&times;  = (p&minus; qA)&times; (p&minus; qA) = &minus;q (p&times; A+ A&times; p) (U.107)
</p>
<p>from which follows150
</p>
<p>( &times; )nr=&minus;

</p>
<p>i
q
(
&nabla; &times; A + A&times;&nabla;
</p>
<p>)
nr = &minus;
</p>
<p>
</p>
<p>i
q [(&nabla; &times; A)nr + (&nabla;nr )&times; A+ A&times;&nabla;nr ]
</p>
<p>(U.108)
</p>
<p>or with B = rotA
</p>
<p>( &times; )nr=&minus;

</p>
<p>i
q
(
&nabla; &times; A
</p>
<p>)
nr = &minus;
</p>
<p>
</p>
<p>i
qBnr . (U.109)
</p>
<p>Thus, we can write (U.105) in the form
</p>
<p>i
&part;
</p>
<p>&part;t
nr =
</p>
<p>
2
</p>
<p>2m
nr &minus;
</p>
<p>q
</p>
<p>2m
 &middot; Bnr + qnr (U.110)
</p>
<p>which is the nonrelativistic Pauli equation for the Pauli spinor nr .
</p>
<p>A note concerning the interaction of spin and magnetic field. Formulated by means
</p>
<p>of the spin vector s = /2, it is given by
</p>
<p>&minus; q
2m
</p>
<p> &middot; B = &minus;2 q
2m
</p>
<p>s &middot; B =&minus; g q
2m
</p>
<p>s &middot; B. (U.111)
</p>
<p>The factor g = 2 is called g-factor or Land&eacute; factor (or, more precisely, electron
g-factor ge). It relates the magnetic moment of the electron to its spin, &micro; = ge q2m s.
Remind that a classical consideration (e.g., for the orbital angular momentum) leads
</p>
<p>to g = 1. High precision experiments show that ge is somewhat greater than given by
the Dirac equation, namely roughly equal to 2, 002319. The reason is explained by
</p>
<p>quantum field theory (see Appendix W, Vol. 2). Indeed, ge.is known with a striking
</p>
<p>precision, both theoretically and experimentally.
</p>
<p>U.5 Dirac Equation and the Hydrogen Atom
</p>
<p>Due to reasons of content, this section is found in Appendix F, Vol. 2.
</p>
<p>150Remind &nabla; &times; ( f F) = f &middot; (&nabla; &times; F)+ (&nabla; f )&times; F and (&nabla;)&times; A+ A&times; (&nabla;) = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>428 Appendix U: Elements of Relativistic Quantum Mechanics
</p>
<p>U.6 Discussion of the Dirac Equation
</p>
<p>In this section, we compile the pros and cons of the Dirac equation (DE).151 Despite all
</p>
<p>the convincing properties of the equation, there is a major problem, namely negative
</p>
<p>energies. This point remains without a convincing solution within the framework
</p>
<p>of the equation. A way out of this dilemma towards an advanced theory offers the
</p>
<p>Feynman&ndash;St&uuml;ckelberg interpretation which paves the way for quantum field theory.
</p>
<p>U.6.1 Pros and Cons of the Dirac Equation
</p>
<p>We briefly mention some advantages and disadvantages of the DE.
</p>
<p>The list of advantages is impressive:
</p>
<p>1. The Dirac equation allows for the description of relativistic particles with spin
</p>
<p>1/2. The fact is all the more remarkable since in the derivation of the DE there
</p>
<p>is no assumption spin 1/2 or any angular momentum. The DE follows from
</p>
<p>two basic ingredients, namely (1) the relativistic dispersion relation, and (2) the
</p>
<p>assumption that there exists a Hamilton function H so that i = H.
2. The equation provides the hydrogen spectrum with high accuracy, far better than
</p>
<p>the Schr&ouml;dinger equation.
</p>
<p>3. The DE provides the g-factor152 of the electron (g = 2); its non-relativistic limit
leads correctly to the Pauli equation.
</p>
<p>4. The DE allows for an explanation of the Zitterbewegung (see below).
</p>
<p>5. Historically, relativistic particles with spin 1/2 could not be described before
</p>
<p>1928 when the DE appeared. In addition, the DE was the first one which sug-
</p>
<p>gested the existence of antiparticles. Indeed, shortly after the publication of the
</p>
<p>DE, the positron e+ was discovered experimentally, i.e., the antiparticle of the
electron e&minus;.
</p>
<p>But there are also disadvantages, among them:
</p>
<p>1. There are solutions with negative energies. From a mathematical point of view,
</p>
<p>these solutions are perfectly correct, but the problem is that negative energies
</p>
<p>are not realized in nature and would lead to paradoxical consequences. So how
</p>
<p>to deal with the negative energy solutions?
</p>
<p>2. In deriving the DE, we started with the aim of finding a single-particle theory.
</p>
<p>But as we have seen, it is impossible to achieve this objective. We always have
</p>
<p>151For each spin there exists a special equation, e.g. Klein&ndash;Gordon for s = 0, Dirac for s = 1/2,
Proca for s = 1, Rarita&ndash;Schwinger for s = 3/2.
152The electron g-factor is one of the most precisely measured values in physics. The DE says
</p>
<p>g = 2, and quantum electrodynamics corrects this value to g = 2.00231930436182 with a relative
standard uncertainty of 10&minus;13 (see Appendix W, Vol. 2).</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix U: Elements of Relativistic Quantum Mechanics 429
</p>
<p>a 4-spinor, i.e., vividly the simultaneous occurrence of two particles with spin
</p>
<p>1/2, particle and antiparticle. Only in the ultra-nonrelativistic case, they are
</p>
<p>decoupled.
</p>
<p>U.6.2 Antiparticles
</p>
<p>As far as is known, for each elementary particle there exists an antiparticle. Mass,
</p>
<p>lifetime and spin of particle and antiparticle are the same, likewise the nature and
</p>
<p>strength of their interactions. Some neutral particles are their own antiparticle (e.g. the
</p>
<p>photon), but others are not (e.g. the neutron). Electrical charge, magnetic moment, and
</p>
<p>all charge-like quantum numbers are opposite. Antiparticles are produced naturally
</p>
<p>in different processes, e.g. beta decay (the electron antineutrino) or interactions of
</p>
<p>cosmic rays in the Earth&rsquo;s atmosphere. Antiparticles can build up antimatter, just as
</p>
<p>particles can build up matter. If a particle and an antiparticle are brought into contact,
</p>
<p>they annihilate eventually. Electron and positron decay into two or three photons,
</p>
<p>proton and antiproton into several pions. Vice versa, a photon can be converted into
</p>
<p>an electron and a positron, provided that the energy of the photon is sufficiently
</p>
<p>high. By the way, electron-positron annihilation is exploited in positron emission
</p>
<p>tomography.
</p>
<p>U.6.3 Negative Energies
</p>
<p>A main drawback of the DE is certainly the appearance of negative energies, not
</p>
<p>existing in nature. In this section we present the hole theory as a former way to
</p>
<p>handle this puzzling problem.
</p>
<p>U.6.3.1 Do We Need Negative Energy Solutions?
</p>
<p>The simplest approach would be to ignore these solutions. But we cannot do so since
</p>
<p>they are inextricably coupled to the solutions with positive energy. In addition, there
</p>
<p>are also physical phenomena which can be ascribed to these &lsquo;negative&rsquo; solutions. For
</p>
<p>a simple example, consider an one-dimensional plane wave
</p>
<p> = Aei(pz&minus;Et)/ + Be&minus;i(pz&minus;Et)/ (U.112)
</p>
<p>where A and B are the amplitudes of the parts with positive and with negative energy.
</p>
<p>Taking the inner product leads to</p>
<p/>
</div>
<div class="page"><p/>
<p>430 Appendix U: Elements of Relativistic Quantum Mechanics
</p>
<p> =
[
Ae&minus;i(pz&minus;Et)/ + Bei(pz&minus;Et)/
</p>
<p>] [
Aei(pz&minus;Et)/ + Be&minus;i(pz&minus;Et)/
</p>
<p>]
=
</p>
<p>= AA + B B + ABe&minus;2i(pz&minus;Et)/ + B Ae2i(pz&minus;Et)/.
(U.113)
</p>
<p>Thus, if only negative or only positive energies exist,  is constant, as we know
</p>
<p>it from nonrelativistic plane waves. However, if both positive and negative energies
</p>
<p>occur, an interference term appears in form of a high-frequency oscillation e2i Et/. In
</p>
<p>a similar way, this also applies to other systems in which both negative and positive
</p>
<p>energy solutions are superimposed.
</p>
<p>This effect is called Zitterbewegung (German, means &lsquo;trembling motion&rsquo;). For a
</p>
<p>free relativistic electron, the effect has never been observed; it is very tiny with a
</p>
<p>frequency of about 2mc2/ = 1.6 &times; 1021s&minus;1 and an amplitude of about 10&minus;13m.
But it was simulated in two different experimental situations, firstly in 2010 with
</p>
<p>a trapped ion in an appropriate environment and secondly in 2013 in a setup with
</p>
<p>Bose&ndash;Einstein condensates.153
</p>
<p>Thus, for several reasons, negative energy solutions cannot be simply neglected.
</p>
<p>We have to look for an physically reasonable interpretation.
</p>
<p>U.6.3.2 Why Does the World Still Exist?
</p>
<p>We know that in a Hydrogen atom there are certain discrete energy levels with
</p>
<p>positive energy. Excited states disintegrate eventually, i.e., the electron falls down into
</p>
<p>states of lower energy, provided these are not occupied, thereby emitting radiation.
</p>
<p>The ground state with the lowest energy E&gt; = Erest mass &minus; E0 is stable and does
not disintegrate. Now let us assume there are states with negative energy. If these
</p>
<p>states would be empty, the electron could fall into the highest negative state with
</p>
<p>E&lt; = &minus;Erest mass + E0, thereby emitting radiation of energy 2 (Erest mass &minus; E0).
Since there are infinitely many negative energy states available, the electron would
</p>
<p>keep falling &lsquo;down&rsquo; and radiating, in the end, infinite amounts of energy. In other
</p>
<p>words, in this scenario all matter would be unstable and disintegrating eventually,
</p>
<p>until there is nothing left than radiation. Apparently, this is not observed at all.
</p>
<p>U.6.3.3 Hole Theory
</p>
<p>The hole theory154 attempted to eliminate the problem of radiation disintegration
</p>
<p>and thus to &lsquo;save&rsquo; the Dirac equation. Hardly surprising, it was suggested by Paul
</p>
<p>Dirac himself (first in 1928, and in an improved version in 1931). It is assumed that
</p>
<p>all states of negative energy are occupied, in accordance with the Pauli exclusion
</p>
<p>principle (note that we describe electrons, i.e., fermions). Thus an electron with
</p>
<p>positive energy remains in the ground state and can not &lsquo;fall down&rsquo; into the range
</p>
<p>153I. Stepanov et al., &lsquo;Coherent Electron Zitterbewegung&rsquo;, arXiv:1612.06190v1, [cond-mat.mes-
</p>
<p>hall], 19.12.2016; and references therein.
154In fact, it is not a theory, but more an interpretation.</p>
<p/>
<div class="annotation"><a href="https://arxiv.org/abs/1612.06190">https://arxiv.org/abs/1612.06190</a></div>
</div>
<div class="page"><p/>
<p>Appendix U: Elements of Relativistic Quantum Mechanics 431
</p>
<p>of negative energies since all those states are occupied and a further occupation is
</p>
<p>forbidden according to the Pauli principle. The vacuum state is thus the state in which
</p>
<p>all states of negative energy are occupied and all states of positive energy are empty.
</p>
<p>Accordingly, the negative states form something like an &lsquo;underworld&rsquo; which also
</p>
<p>is called Dirac sea. One can argue that we do not notice any dynamics of the Dirac
</p>
<p>sea because if a particle of this sea would change its state, it would have to assume
</p>
<p>another, already occupied state, and this is forbidden due to the Pauli principle.
</p>
<p>Actually, we can imagine to see something of this underworld - keyword pair
</p>
<p>production. We assume the vacuum state and consider the particle with the highest
</p>
<p>negative energy of about &minus;mc2. If this particle absorbs radiation with an energy
&gt; 2mc2, it can leave the Dirac sea and become an electron with positive energy
</p>
<p>which has the charge &minus;e and the energy mc2. Simultaneously, a hole emerges in the
Dirac sea (hence the name hole theory) which shows the absence of the charge &minus;e
with the energy &minus;E . With respect to the vacuum, this corresponds to a charge +e
with the energy +E , and accordingly, this hole (object) is interpreted by an observer
as a positron, the antiparticle of the electron. The process is called pair production,
</p>
<p>since a electron&ndash;positron pair is produced by radiation with an energy &gt; 2mc2.
</p>
<p>The reverse process exists also: an electron falls into a free hole, whereby radiation
</p>
<p>is emitted. This looks like that an electron and a positron collide and dissolve in
</p>
<p>radiation. The process is called electron-positron annihilation or pair annihilation.
</p>
<p>U.6.3.4 Hole Theory From Today&rsquo;s View
</p>
<p>It is certainly a merit of this interpretation to predict the existence of positrons. The
</p>
<p>hole theory was set up by Paul Dirac in 1928. Initially, he assumed protons as &lsquo;holes&rsquo;,
</p>
<p>but in 1931 he changed over to positrons. Indeed, Carl David Anderson155 detected
</p>
<p>the positron experimentally in 1932.
</p>
<p>On the other hand, the hole theory has some severe flaws; we mention three of
</p>
<p>them.
</p>
<p>(1) In order to maintain stable ground states, the Dirac sea has to consist of an
</p>
<p>infinite number of electrons with negative energy. This means an infinite mass and
</p>
<p>infinite negative charge, not interacting with the environment. In addition, one has
</p>
<p>to assume that these electrons do not interact with each other. Dirac was aware of
</p>
<p>this problem; he tried to argue that for us this situation would be the &lsquo;normal&rsquo; state
</p>
<p>of charge zero.
</p>
<p>(2) Also bosons have antiparticles. But since they are not subjected to the Pauli
</p>
<p>principle, no sort of hole theory would work for them.
</p>
<p>(3) The hole theory is unsymmetrical with respect to the role of electrons and
</p>
<p>positrons - one electron hovers over an &lsquo;sea&rsquo; of infinitively many positrons. The
</p>
<p>same holds true with interchanged roles if we start from the Dirac equation for
</p>
<p>positrons.
</p>
<p>155Anderson, Carl David, 1905&ndash;1991, US-American physicist, nobel prize 1936.</p>
<p/>
</div>
<div class="page"><p/>
<p>432 Appendix U: Elements of Relativistic Quantum Mechanics
</p>
<p>Today, the hole theory is considered obsolete. A consistent description of particles
</p>
<p>and antiparticles is provided by quantum field theory which solves the addressed
</p>
<p>problems and makes the interpretation of antiparticles as holes unnecessary. Note
</p>
<p>that this does not hold for the Dirac equation itself which is still the basic equation
</p>
<p>from which e.g. quantum electrodynamics emerges.
</p>
<p>Thus, the hole theory is no longer up-to-date, but it is interesting from the point of
</p>
<p>view of science history and/or sociology. Despite all contradictions, it was accepted
</p>
<p>as a working hypothesis for quite some time. This shows that scientists, if necessary,
</p>
<p>bite the bullet to retain a theory which they instinctively are convinced of.
</p>
<p>U.6.4 Feynman&ndash;St&uuml;ckelberg Interpretation
</p>
<p>As mentioned above, quantum field theory is the established theory to describe par-
</p>
<p>ticles and antiparticles. To make clear one of its basic ideas, we briefly outline the
</p>
<p>so-called Feynman&ndash;St&uuml;ckelberg interpretation.156 Here, the antiparticles are consid-
</p>
<p>ered as the corresponding particles traveling backwards in time. Perhaps this seems
</p>
<p>at the first sight to be a weird idea from science fiction, and replacing the ominous
</p>
<p>infinite Dirac sea by a motion backwards in time may give the impression of jumping
</p>
<p>out of a frying pan into the fire. But the approach is compatible with the fundamental
</p>
<p>symmetry principles of physics in contrast to the Dirac sea.
</p>
<p>To motivate this somewhat surprising approach in a simple manner, we consider
</p>
<p>a one-dimensional plane wave  = Aei(pz&minus;Et)/ + Be&minus;i(pz&minus;Et)/, and especially the
part with negative energy:
</p>
<p>neg = Be&minus;i(pz&minus;Et)/. (U.114)
</p>
<p>Now assume that we change the sign of the time t . Replacing t by &minus;t results in
an exponent pz &minus; E (&minus;t). This means an object travelling backward in time, like
playing a film backwards. In addition, reversing the time reverses all momenta so we
</p>
<p>also need to change p by p &rarr; &minus;p for consistency. In order that the term pz does
not change its sign, this has to be compensated by changing the sign of the position
</p>
<p>z &rarr; &minus;z, i.e., we change the parity. In short, we start with an expression Et &minus; pz
where the energy is negative, E &lt; 0. We replace this term simply by the equivalent
</p>
<p>expression (&minus;E) (&minus;t) &minus; (&minus;p) (&minus;z). In this way, we have a particle with positive
energy which moves backwards in time.
</p>
<p>We have to take into account another point, namely the coupling to the electro-
</p>
<p>magnetic field. Denote the charge of the electron or positron by q. For the sake of
</p>
<p>simplicity, we consider only the Lorentz force F = q (v &times; B). As it is seen, chang-
ing the direction of motion has the same effect as changing the sign of the charge q
</p>
<p>(which corresponds to the transition from particle to antiparticle).
</p>
<p>156Feynman, Richard Phillips, 1918&ndash;1988; American physicist, Nobel prize 1965. St&uuml;ckelberg,
</p>
<p>Ernst Carl Gerlach, 1905&ndash;1984; Swiss physicist.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix U: Elements of Relativistic Quantum Mechanics 433
</p>
<p>Thus, we have changed the sign of the time (time reversal, T), of the position
</p>
<p>(parity transformation, P), and of the charge (charge conjugation, C). In other words,
</p>
<p>we have performed a CPT-transformation. In this way, we get rid of negative energies
</p>
<p>and turn them into positive energies.
</p>
<p>To summarize: The relativistic dispersion relation E = &plusmn;
&radic;
p2 + m2 allows for
</p>
<p>solutions with negative energy, but nature does not know those energies. The way out
</p>
<p>of this dilemma consists in interpreting the formerly negative solutions as antiparti-
</p>
<p>cles with positive energy whereas their momenta point in the opposite direction of
</p>
<p>the corresponding particle; in addition, particle and antiparticle have the same mass
</p>
<p>and spin, but opposite charges.
</p>
<p>In quantum field theory, these ideas lead to diagrams, in which lines or arrows
</p>
<p>represent particles propagating for- or backwards in time. These diagrams are precise
</p>
<p>graphical realizations of scattering processes; they are called Feynman diagrams, see
</p>
<p>Appendix W, Vol. 2.
</p>
<p>Supplement: CPT theorem The CPT symmetry is a fundamental symmetry of
</p>
<p>physics. It states the following: For a given physical process, exchanging matter by
</p>
<p>antimatter (which covers changing the sign of the charge) and performing in addition
</p>
<p>a reflection of the space and a reversal of the time direction yields again an allowed
</p>
<p>physical process. This is also called CPT invariance of the physical laws. The CPT
</p>
<p>theorem says that CPT symmetry holds for all physical phenomena, or to put it more
</p>
<p>technically that any Lorentz invariant local quantum field theory with a Hermitian
</p>
<p>Hamiltonian must have CPT symmetry.
</p>
<p>Vividly, the CPT theorem states that a &lsquo;mirror universe&rsquo; of our universe is possible.
</p>
<p>We can build it by replacing all matter by antimatter (charge inversion C), reversing
</p>
<p>all momenta (time inversion T) and reflecting all positions (parity inversion P). This
</p>
<p>mirror universe evolves exactly according to our known physical laws. The CPT
</p>
<p>transformation changes our universe to its mirror image and vice versa.
</p>
<p>The Dirac equation has the individual symmetries C, P and T and their combi-
</p>
<p>nations as, for example, CP. This is in contrast to other fundamental equations (e.g.
</p>
<p>weak interaction) where CPT is the only combination of the three transformations
</p>
<p>C, P and T that is observed to be an exact symmetry of nature.
</p>
<p>A violation of CPT symmetry would automatically indicate a violation of the spe-
</p>
<p>cial relativity. Within today&rsquo;s limits of accuracy, the CPT theorem is experimentally
</p>
<p>confirmed. However, it is an open question, if there are violations below these limits
</p>
<p>which are predicted by some modern theories, e.g. quantum gravitation or string
</p>
<p>theories.
</p>
<p>U.7 Exercises and Solutions
</p>
<p>1. Determine all 2-dimensional matrices M with M2 = 1 (or E2).
Solution: We have</p>
<p/>
</div>
<div class="page"><p/>
<p>434 Appendix U: Elements of Relativistic Quantum Mechanics
</p>
<p>M =
(
</p>
<p>a b
</p>
<p>c d
</p>
<p>)
&rarr; M2 =
</p>
<p>(
a2 + bc b (a + d)
</p>
<p>c (a + d) d2 + bc
</p>
<p>)
!=
(
</p>
<p>1 0
</p>
<p>0 1
</p>
<p>)
. (U.115)
</p>
<p>This leads to the equations
</p>
<p>a2 + bc = 1 ; d2 + bc = 1
b (a + d) = 0 ; c (a + d) = 0. (U.116)
</p>
<p>Case (1) a + d = 0. This gives d = &minus;a and we have
</p>
<p>M1 =
(
</p>
<p>a b
</p>
<p>c &minus;a
</p>
<p>)
with bc = 1 &minus; a2. (U.117)
</p>
<p>Case (2) a + d 	= 0. It follows b = c = 0 and a2 = d2 = 1. Since a and d both
have the values &plusmn;1 and due to a + d 	= 0, we have a = d:
</p>
<p>M2 =
(
</p>
<p>a 0
</p>
<p>0 a
</p>
<p>)
with a2 = 1 &rarr; M2 = &plusmn;E2. (U.118)
</p>
<p>As is seen, we have not only the two solutions &plusmn;1 (or &plusmn;E2), but with M1 an
additional infinity of solutions.
</p>
<p>2. Show that the equations (U.21) may be written in the form (U.23).
</p>
<p>Solution: In a first step we write c2 ( &middot; p) ( &middot; p) = c2p2 as
</p>
<p>3&sum;
</p>
<p>j=1
 j p j
</p>
<p>3&sum;
</p>
<p>k=1
k pk =
</p>
<p>3&sum;
</p>
<p>j,k=1
 jk p j pk =
</p>
<p>3&sum;
</p>
<p>j=1
p2j . (U.119)
</p>
<p>Since the i are matrices, we have to take into account their order, i.e.  jk 	=
 jk . To arrive at a compact formulation, we use therefore a small trick and add
</p>
<p>the left side with reversed indices:
</p>
<p>3&sum;
</p>
<p>j,k=1
 jk p j pk +
</p>
<p>3&sum;
</p>
<p>j,k=1
k j pk p j = 2
</p>
<p>3&sum;
</p>
<p>j=1
p2j . (U.120)
</p>
<p>Since p j pk = pk p j , this may be written as
</p>
<p>3&sum;
</p>
<p>j,k=1
</p>
<p>(
 jk + k j
</p>
<p>)
p j pk = 2
</p>
<p>3&sum;
</p>
<p>j=1
p2j . (U.121)
</p>
<p>For j = k on the left hand side, we have  j j +  j j = 2, whereas for j 	= k
holds  jk +k j = 0. Combining these results, we can write  jk + jk =
2 jk .
</p>
<p>The second equation, namely ( &middot; p) +  ( &middot; p) = 0, can be written as</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix U: Elements of Relativistic Quantum Mechanics 435
</p>
<p>3&sum;
</p>
<p>j=1
 j p j + 
</p>
<p>3&sum;
</p>
<p>j=1
 j p j = 0. (U.122)
</p>
<p>Since p j is a scalar, it commutes with the matrices  j and , and we can write
</p>
<p>3&sum;
</p>
<p>j=1
</p>
<p>(
 j p j +  j p j
</p>
<p>)
=
</p>
<p>3&sum;
</p>
<p>j=1
</p>
<p>(
 j +  j
</p>
<p>)
p j . (U.123)
</p>
<p>Since the three components of the momentum are independent, the bracketed
</p>
<p>term must vanish, i.e.,  j +  j = 0.
3. Show that in the space of 2 &times; 2 matrices there are not four linearly independent
</p>
<p>anticommuting matrices.
</p>
<p>Solution: The space of 2&times;2 matrices is spanned, for instance, by the unit matrix
E2 and the three Pauli matrices i , i = 1, 2, 3. (In this exercise, we note the
unit matrix not by 1, but explicitly by E2.) Each other matrix A in this space is
</p>
<p>a linear combination of these four linear independent matrices (of the form)
</p>
<p>A = a0 E2 +
3&sum;
</p>
<p>k=1
akk . (U.124)
</p>
<p>The three Pauli matrices anticommute pairwise
</p>
<p>i j +  ji = 2i j E2 (U.125)
</p>
<p>with scalar coefficients ai . We have to look for a fourth matrix A (i.e., to determine
</p>
<p>the coefficients a j ) which anticommutes with all Pauli matrices, i.e. which fulfills
</p>
<p>Al + l A = 0 for l = 1, 2, 3. (U.126)
</p>
<p>Inserting (U.124) into (U.126) gives
</p>
<p>a0 (E2l + l E2)+
3&sum;
</p>
<p>k=1
ak (kl + lk) = 0 for l = 1, 2, 3. (U.127)
</p>
<p>With (U.125) and the fact that the Pauli matrices commute with the unit matrix,
</p>
<p>E2l = l E2 = l , we get
</p>
<p>2a0l +
3&sum;
</p>
<p>k=1
ak2kl E2 = 0 for l = 1, 2, 3 (U.128)</p>
<p/>
</div>
<div class="page"><p/>
<p>436 Appendix U: Elements of Relativistic Quantum Mechanics
</p>
<p>which may be written as
</p>
<p>a0l + al E2 = 0 for l = 1, 2, 3. (U.129)
</p>
<p>But since the four matrices E2 and l are linearly independent, this equation can
</p>
<p>only be satisfied by a0 = a1 = a2 = a3 = 0.
4. Write down explicitly the matrices  and  &middot;&nabla;.
</p>
<p>Solution:
</p>
<p>1 =
(
</p>
<p>0 1
1 0
</p>
<p>)
=
</p>
<p>

</p>
<p>0 0 0 1
</p>
<p>0 0 1 0
</p>
<p>0 1 0 0
</p>
<p>1 0 0 0
</p>
<p>

</p>
<p>2 =
(
</p>
<p>0 2
2 0
</p>
<p>)
=
</p>
<p>

</p>
<p>0 0 0 &minus;i
0 0 i 0
</p>
<p>0 &minus;i 0 0
i 0 0 0
</p>
<p>

</p>
<p>3 =
(
</p>
<p>0 3
3 0
</p>
<p>)
=
</p>
<p>

</p>
<p>0 0 1 0
</p>
<p>0 0 0 &minus;1
1 0 0 0
</p>
<p>0 &minus;1 0 0
</p>
<p>

</p>
<p>(U.130)
</p>
<p>and
</p>
<p>&middot; &nabla; = 1&part;x + 2&part;y + 3&part;z =
</p>
<p>

</p>
<p>0 0 &part;z &part;x &minus; i&part;y
0 0 &part;x + i&part;y &minus;&part;z
&part;z &part;x &minus; i&part;y 0 0
</p>
<p>&part;x + i&part;y &minus;&part;z 0 0
</p>
<p>
 .
</p>
<p>(U.131)
</p>
<p>5. Show that the matrices i are unitary.
</p>
<p>Solution: We have to show that i
&dagger;
i = &dagger;i i = 1 We have (remind i = &dagger;i )
</p>
<p>i
&dagger;
i =
</p>
<p>(
0 i
i 0
</p>
<p>)(
0 i
i 0
</p>
<p>)
=
</p>
<p>(
2i 0
</p>
<p>0 2i
</p>
<p>)
=
</p>
<p>(
1 0
</p>
<p>0 1
</p>
<p>)
= 1 (U.132)
</p>
<p>6. Write the Lorentz matrix 	 and the matrices 	i in form of block matrices.
</p>
<p>Solution:
</p>
<p>	 =
(
 I2 &minus; 1 0
</p>
<p>0 I2
</p>
<p>)
;	i =
</p>
<p>(
0 i &minus; 1i
i 0
</p>
<p>)
. (U.133)
</p>
<p>7. Show that
(
c &middot; p+ mc2 &minus; E
</p>
<p>) (
c &middot; p+ mc2 + E
</p>
<p>)
= 0. Remind that c &middot;
</p>
<p>p+mc2&plusmn;E is a short-hand notation for 4&times;4 matrices, so
(
c &middot; p+ mc2 &minus; E
</p>
<p>)
(
c &middot; p+ mc2 + E
</p>
<p>)
= 0 does not necessarily imply that at least one of the
</p>
<p>two factors (brackets) vanishes.
</p>
<p>Solution: Performing the multiplication leads to</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix U: Elements of Relativistic Quantum Mechanics 437
</p>
<p>(c &middot; p)
(
</p>
<p>c &middot; p+ mc2 + E
)
+ mc2
</p>
<p>(
c &middot; p+ mc2 + E
</p>
<p>)
&minus; E
</p>
<p>(
c &middot; p+ mc2 + E
</p>
<p>)
=
</p>
<p>= c2 ( &middot; p) ( &middot; p)+ mc3 ( &middot; p) + c ( &middot; p) E+
+mc3 ( &middot; p)+ m2c42 + Emc2 &minus; Ec ( &middot; p)&minus;Emc2 &minus; E2 =
= c2 ( &middot; p) ( &middot; p)+ mc3 ( &middot; p) + mc3 ( &middot; p)+ m2c42 &minus; E2.
</p>
<p>(U.134)
</p>
<p>Due to ( &middot; p) ( &middot; p) = p2, 2 = 1 and  +  = 0 (see (U.23)) follows
(
c &middot; p+ mc2 &minus; E
</p>
<p>) (
c &middot; p+ mc2 + E
</p>
<p>)
= c2p2 + m2c4 &minus; E2 = 0.
</p>
<p>(U.135)
8. Given the spinor with the components, = 1, . . . , 4. Write down explicitly
</p>
<p>.
Solution:
</p>
<p> =
</p>
<p>

</p>
<p>1
2
3
4
</p>
<p>
 ;  =
</p>
<p>(
&lowast;1 
</p>
<p>&lowast;
2 
</p>
<p>&lowast;
3 
</p>
<p>&lowast;
4
</p>
<p>)


</p>
<p>1 0 0 0
</p>
<p>0 1 0 0
</p>
<p>0 0 &minus;1 0
0 0 0 &minus;1
</p>
<p>
 =
</p>
<p>(
&lowast;1 
</p>
<p>&lowast;
2 &minus;&lowast;3 &minus;&lowast;4
</p>
<p>)
. (U.136)
</p>
<p>9. Given a solution of the free Klein&ndash;Gordon equation with positive energy only,
</p>
<p>i.e.,  (x) =
&int;
</p>
<p>d3k N (k) a (k) e&minus;ikx . Determine the normalization constant
N (k) under the assumptions
</p>
<p>&int;
d3x  (x) = 1 and
</p>
<p>&int;
d3k |a (k)|2 = 1.
</p>
<p>Solution: The probability density  is given by
</p>
<p> (x) = i
(
&lowast; &minus; &lowast;
</p>
<p>)
. (U.137)
</p>
<p>With
</p>
<p> (x) =
&int;
</p>
<p>d3k N (k) a (k) e&minus;ikx ;  (x) = &minus;i
&int;
</p>
<p>d3k ck0 N (k) a (k) e
&minus;ikx
</p>
<p>(U.138)
follows
</p>
<p> (x) =
&int;
</p>
<p>d3k ck0 N (k) a (k) e
&minus;ikx &middot;
</p>
<p>&int;
d3k&prime; N&lowast;
</p>
<p>(
k&prime;
)
</p>
<p>a&lowast;
(
k&prime;
)
</p>
<p>eik
&prime;x+
</p>
<p>+
&int;
</p>
<p>d3k&prime; ck&prime;0 N
&lowast; (k&prime;
</p>
<p>)
a&lowast;
</p>
<p>(
k&prime;
)
</p>
<p>eik
&prime;x &middot;
</p>
<p>&int;
d3k A (k) e&minus;ikx
</p>
<p>=
</p>
<p>=
&int;
</p>
<p>d3k
&int;
</p>
<p>d3k&prime;
[
ck0 N (k) a (k) N
</p>
<p>&lowast; (k&prime;
)
</p>
<p>a&lowast;
(
k&prime;
)
+ ck&prime;0 N&lowast;
</p>
<p>(
k&prime;
)
</p>
<p>a&lowast;
(
k&prime;
)
</p>
<p>N (k) a (k)
]
</p>
<p>e&minus;ikx eik
&prime;x =
</p>
<p>=
&int;
</p>
<p>d3k
&int;
</p>
<p>d3k&prime; c
(
k0 + k&prime;0
</p>
<p>)
N (k) a (k) N&lowast;
</p>
<p>(
k&prime;
)
</p>
<p>a&lowast;
(
k&prime;
)
</p>
<p>e&minus;ikx eik
&prime;x .
</p>
<p>(U.139)
Integration brings
</p>
<p>&int;
d3x  (x) =
</p>
<p>&int;
d3k
</p>
<p>&int;
d3k&prime; c
</p>
<p>(
k0 + k&prime;0
</p>
<p>)
N (k) a (k) N&lowast;
</p>
<p>(
k&prime;
)
</p>
<p>a&lowast;
(
k&prime;
) &int;
</p>
<p>d3x e&minus;ikx eik
&prime;x =
</p>
<p>=
&int;
</p>
<p>d3k
&int;
</p>
<p>d3k&prime; c
(
k0 + k&prime;0
</p>
<p>)
N (k) a (k) N&lowast;
</p>
<p>(
k&prime;
)
</p>
<p>a&lowast;
(
k&prime;
)

(
k &minus; k&prime;
</p>
<p>)
(2)3 =
</p>
<p>= (2)3
&int;
</p>
<p>d3k 2ck0 N (k) a (k) N
&lowast; (k) a&lowast; (k) = (2)3
</p>
<p>&int;
d3k 2ck0 |N (k) a (k)|2 .
</p>
<p>(U.140)
</p>
<p>Choosing
</p>
<p>N (k) = 1
(2)3/2
</p>
<p>1&radic;
2ck0
</p>
<p>= 1
(2)3/2
</p>
<p>1&radic;
2k
</p>
<p>(U.141)</p>
<p/>
</div>
<div class="page"><p/>
<p>438 Appendix U: Elements of Relativistic Quantum Mechanics
</p>
<p>brings
</p>
<p>&int;
d3x  (x) = (2)3
</p>
<p>&int;
d3k 2k
</p>
<p>1
</p>
<p>(2)3
1
</p>
<p>2k
|a (k)|2 =
</p>
<p>&int;
d3k |a (k)|2 = 1.
</p>
<p>(U.142)
</p>
<p>10. Show (U.49).
</p>
<p>Solution: From (U.48), we take
(
c &middot; p+ mc2 &minus; E
</p>
<p>)
us = 0 and write it in the
</p>
<p>form
</p>
<p>(
0 cp
</p>
<p>cp 0
</p>
<p>)(
us,u
us,l
</p>
<p>)
+
</p>
<p>(
mc2 0
</p>
<p>0 &minus;mc2
)(
</p>
<p>us,u
us,l
</p>
<p>)
&minus;
</p>
<p>&minus;
(
</p>
<p>E 0
</p>
<p>0 E
</p>
<p>)(
us,u
us,l
</p>
<p>)
= 0
</p>
<p>; us =
(
</p>
<p>us,u
us,l
</p>
<p>)
. (U.143)
</p>
<p>This gives the equations
</p>
<p>cpus,l +mc2us,u &minus; Eus,u = 0 and cpus,u &minus;mc2us,l &minus; Eus,l = 0. (U.144)
</p>
<p>Solving the second equation for us,l yields
</p>
<p>us,l =
cp
</p>
<p>E + mc2 us,u (U.145)
</p>
<p>and hence
</p>
<p>us =
(
</p>
<p>us,u
cp
</p>
<p>Ep+mc2 us,u
</p>
<p>)
. (U.146)
</p>
<p>Note that us is a 4-spinor. Hence, its norm is determined by usus = |us |2 =us,u
2 &minus;
</p>
<p>us,l
2 (note the sign on the r.h.s; see Appendix T, Vol. 1). Thus, to
</p>
<p>normalize us in (U.146) means to determine
us,u
</p>
<p>2 by the equation
</p>
<p>us,u
2 &minus;
</p>
<p>
cp
</p>
<p>Ep + mc2
us,u
</p>
<p>
2
</p>
<p>= 1. (U.147)
</p>
<p>This leads to157
</p>
<p>1 =
(
</p>
<p>1 &minus;
 cpEp+mc2
</p>
<p>
2
) us,u
</p>
<p>2 = (Ep+mc
2)
</p>
<p>2&minus;|cp|2
</p>
<p>(Ep+mc2)2
us,u
</p>
<p>2 =
</p>
<p>= E
2
p+2Epmc2+m2c4&minus;c2|p|2
</p>
<p>(Ep+mc2)2
us,u
</p>
<p>2 = 2mc2
Ep+mc2
</p>
<p>us,u
2
</p>
<p>(U.148)
</p>
<p>or
us,u
</p>
<p> =
&radic;
</p>
<p>Ep + mc2
2mc2
</p>
<p>. (U.149)
</p>
<p>157We use ( &middot; a) ( &middot; b) = a &middot; b+ i &middot; (a &times; b).</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix U: Elements of Relativistic Quantum Mechanics 439
</p>
<p>Thus, we can write us,u in the form
us,u
</p>
<p> &middot; e where e is an arbitrary unit 2-vector.
Choosing e = 1 and e = 2 brings the desired result.
The calculation of us,l runs analogously.
</p>
<p>11. Prove (U.50).
</p>
<p>Solution: From (U.49) we have
</p>
<p>us (p) =
&radic;
</p>
<p>Ep + mc2
2mc2
</p>
<p>(
s
</p>
<p>cp
</p>
<p>Ep+mc2
s
</p>
<p>)
; vs (p) =
</p>
<p>&radic;
Ep + mc2
</p>
<p>2mc2
</p>
<p>( cp
Ep+mc2
</p>
<p>s
</p>
<p>s
</p>
<p>)
; s = 1, 2.
</p>
<p>(U.150)
The adjoints are given by
</p>
<p>us (p) =
&radic;
</p>
<p>Ep + mc2
2mc2
</p>
<p>(

</p>
<p>&dagger;
s &minus;&dagger;s cpEp+mc2
</p>
<p>)
; vs (p) =
</p>
<p>&radic;
Ep + mc2
</p>
<p>2mc2
</p>
<p>(

</p>
<p>&dagger;
s
</p>
<p>cp
</p>
<p>Ep+mc2 &minus;
&dagger;
s
</p>
<p>)
; s = 1, 2.
</p>
<p>(U.151)
</p>
<p>Exemplarily, we consider vr (p) vs (p). We have
</p>
<p>vr (p) vs (p) =
&radic;
</p>
<p>Ep+mc2
2mc2
</p>
<p>(

</p>
<p>&dagger;
r
</p>
<p>cp
</p>
<p>Ep+mc2 &minus;
&dagger;
r
</p>
<p>)&radic;
Ep+mc2
</p>
<p>2mc2
</p>
<p>(
cp
</p>
<p>Ep+mc2 s
s
</p>
<p>)
=
</p>
<p>= Ep+mc
2
</p>
<p>2mc2
</p>
<p>[

</p>
<p>&dagger;
r
</p>
<p>cp
</p>
<p>Ep+mc2
cp
</p>
<p>Ep+mc2 s &minus; 
&dagger;
rs
</p>
<p>]
= Ep+mc
</p>
<p>2
</p>
<p>2mc2
</p>
<p>[(
cp
</p>
<p>Ep+mc2
)2
</p>
<p>&minus; 1
]
rs =
</p>
<p>= Ep+mc
2
</p>
<p>2mc2
</p>
<p>[(
cp
</p>
<p>Ep+mc2
)2
</p>
<p>&minus; E
2
p+2Epmc2+m2c4
</p>
<p>(Ep+mc2)2
</p>
<p>]
rs =
</p>
<p>= Ep+mc
2
</p>
<p>2mc2
</p>
<p>[(
cp
</p>
<p>Ep+mc2
)2
</p>
<p>&minus; m
2c4+c2p2+2Epmc2+m2c4
</p>
<p>(Ep+mc2)2
</p>
<p>]
rs =
</p>
<p>= Ep+mc
2
</p>
<p>2mc2
</p>
<p>[
&minus; 2m
</p>
<p>2c4+2Epmc2
</p>
<p>(Ep+mc2)2
</p>
<p>]
rs = &minus; 12mc2
</p>
<p>[
2m2c4+2Epmc2
</p>
<p>Ep+mc2
]
rs = &minus;
</p>
<p>[
mc2+Ep
Ep+mc2
</p>
<p>]
rs = &minus;rs .
</p>
<p>(U.152)
</p>
<p>The other relations analogously.
</p>
<p>12. Show that 0 is hermitian, k is anti-hermitian.
</p>
<p>Solution: We have
</p>
<p>0&dagger; =
(
</p>
<p>1 0
</p>
<p>0 &minus;1
</p>
<p>)&dagger;
=
</p>
<p>(
1 0
</p>
<p>0 &minus;1
</p>
<p>)
= 0 (U.153)
</p>
<p>and
</p>
<p>k&dagger; =
(
</p>
<p>0 k
&minus;k 0
</p>
<p>)&dagger;
=
</p>
<p>(
0 &minus;&dagger;k

</p>
<p>&dagger;
k 0
</p>
<p>)
=
</p>
<p>(
0 &minus;k
k 0
</p>
<p>)
= &minus;k . (U.154)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V
</p>
<p>Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>V.1 Exercises, Chap. 1
</p>
<p>1. Consider the relativistic energy-momentum relation
</p>
<p>E2 = m20c4 + p2c2. (V.1)
</p>
<p>Show that in the nonrelativistic limit v  c, it gives approximately (up to an
additive positive constant)
</p>
<p>E = p
2
</p>
<p>2m0
. (V.2)
</p>
<p>Solution: In the nonrelativistic limiting case, we find that
</p>
<p>E =
&radic;
</p>
<p>m20c
4 + p2c2 = m0c2
</p>
<p>&radic;
1 + p
</p>
<p>2
</p>
<p>m20c
2
&asymp; m0c2
</p>
<p>(
1 + p
</p>
<p>2
</p>
<p>2m20c
2
</p>
<p>)
(V.3)
</p>
<p>where we have used
&radic;
</p>
<p>1 +  &asymp; 1 + /2 (see Appendix D, Vol. 1; Taylor expan-
sion). It follows that
</p>
<p>E &asymp; m0c2 +
p2
</p>
<p>2m0
, (V.4)
</p>
<p>where m0c
2 is the above-mentioned positive constant. Since one can choose the
</p>
<p>zero point of classical energies arbitrarily, we choose it in such a way that this
</p>
<p>term vanishes. Incidentally, one usually writes simply m instead of m0 because
</p>
<p>the velocity dependence of the mass is negligible for v  c.
2. Show that the relation E = p &middot; c (c is the speed of light) holds only for objects
</p>
<p>with zero rest mass.
</p>
<p>Solution: The result follows directly from E2 = m20c4 + p2c2.
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>441</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>442 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>3. A (relativistic) object has zero rest mass. Show that in this case the dispersion
</p>
<p>relation reads 2 = c2k2.
Solution: The result follows directly from exercise 2 and the de Broglie relations.
</p>
<p>4. Let k &lt; 0,  &gt; 0. Is ei(kx&minus;t) a right- or left-moving plane wave?
Solution: If we set the exponent equal to zero, we find from kx &minus; t = 0 the
inequality:
</p>
<p>x
</p>
<p>t
= v = 
</p>
<p>k
&lt; 0. (V.5)
</p>
<p>Because of v &lt; 0, the wave runs from the right to the left.
</p>
<p>5. Solve the three-dimensional wave equation
</p>
<p>&part;2
 (r, t)
</p>
<p>&part;t2
= c2&nabla;2
 (r, t) (V.6)
</p>
<p>explicitly by using the separation of variables.
</p>
<p>6. Given the three-dimensional wave equation for a vector field A (r, t),
</p>
<p>&part;2A (r, t)
</p>
<p>&part;t2
= c2&nabla;2A (r, t) . (V.7)
</p>
<p>(a) What is a solution in the form of a plane wave?
</p>
<p>Solution:
</p>
<p>A (r, t) = A0ei(kr&minus;t); 2 = c2k2. (V.8)
</p>
<p>(b) Which condition must A0 satisfy if A is (a) a longitudinal, (b) a transverse
</p>
<p>wave?
</p>
<p>Solution: For a longitudinal wave, the amplitude vector is parallel to the
</p>
<p>propagation direction, while for a transverse wave it is perpendicular to it.
</p>
<p>For a longitudinal wave, therefore,A0 &sim; k; for a transverse wave,A0 &middot;k = 0.
7. Given the SEq
</p>
<p>i
&part;
</p>
<p>&part;t

 (r, t) = &minus; 
</p>
<p>2
</p>
<p>2m
&nabla;2
 (r, t)+ V (r, t)
 (r, t) (V.9)
</p>
<p>and two solutions 1 (r, t) and 2 (r, t). Show explicitly that any linear combi-
</p>
<p>nation of these solutions is again a solution.
</p>
<p>Solution: Since 1 and 2 are solutions of the SEq, we have
</p>
<p>i
&part;
</p>
<p>&part;t
i (r, t) = &minus;
</p>
<p>2
</p>
<p>2m
&nabla;2i (r, t)+ V (r, t)i (r, t) ; i = 1, 2. (V.10)
</p>
<p>We have to show that for a linear combination (r, t) = a1 (r, t)+ b2 (r, t)
with a, b &isin; C, we obtain</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 443
</p>
<p>i
&part;
</p>
<p>&part;t
(r, t) = &minus; 
</p>
<p>2
</p>
<p>2m
&nabla;2(r, t)+ V (r, t) (r, t) . (V.11)
</p>
<p>This holds true because of (for brevity, we omit the arguments r, t):
</p>
<p>i
&part;
</p>
<p>&part;t
 = i &part;
</p>
<p>&part;t
(a1 + b2) = ai
</p>
<p>&part;
</p>
<p>&part;t
1 + bi
</p>
<p>&part;
</p>
<p>&part;t
2
</p>
<p>= a
[
&minus; 
</p>
<p>2
</p>
<p>2m
&nabla;21 + V1
</p>
<p>]
+ b
</p>
<p>[
&minus; 
</p>
<p>2
</p>
<p>2m
&nabla;22 + V2
</p>
<p>]
</p>
<p>= &minus; 2
2m
</p>
<p>&nabla;2 (a1 + b2)+ V (a1 + b2) = &minus; 
2
</p>
<p>2m
&nabla;2+ V.
</p>
<p>(V.12)
</p>
<p>8. The wavefunction of a quantum object of mass m is given by
</p>
<p> (x, t) = 0 exp
(
&minus; x
</p>
<p>2
</p>
<p>2b2
&minus; i 
</p>
<p>2mb2
t
</p>
<p>)
, (V.13)
</p>
<p>b is a fixed length. Determine the potential energy V (x) of the quantum object.
</p>
<p>Solution: We determine V (x) by inserting  (x, t) into the time-dependent SEq.
</p>
<p>With
</p>
<p>&part;
</p>
<p>&part;t
 (x, t) = &minus;i 
</p>
<p>2mb2
0 exp
</p>
<p>(
&minus; x
</p>
<p>2
</p>
<p>2b2
&minus; i 
</p>
<p>2mb2
t
</p>
<p>)
</p>
<p>&part;
</p>
<p>&part;x
 (x, t) = &minus; x
</p>
<p>b2
0 exp
</p>
<p>(
&minus; x
</p>
<p>2
</p>
<p>2b2
&minus; i 
</p>
<p>2mb2
t
</p>
<p>)
</p>
<p>&part;2
</p>
<p>&part;x2
 (x, t) = &minus; 1
</p>
<p>b2
0 exp
</p>
<p>(
&minus; x2
</p>
<p>2b2
&minus; i 
</p>
<p>2mb2
t
)
+ x2
</p>
<p>b4
0 exp
</p>
<p>(
&minus; x2
</p>
<p>2b2
&minus; i 
</p>
<p>2mb2
t
)
</p>
<p>(V.14)
</p>
<p>it follows that
</p>
<p>i
</p>
<p>(
&minus;i 
</p>
<p>2mb2
</p>
<p>)
= &minus; 
</p>
<p>2
</p>
<p>2m
</p>
<p>(
&minus; 1
</p>
<p>b2
+ x
</p>
<p>2
</p>
<p>b4
</p>
<p>)
+ V (V.15)
</p>
<p>or
</p>
<p>V = 
2
</p>
<p>2m
</p>
<p>x2
</p>
<p>b4
(V.16)
</p>
<p>i.e. a harmonic-oscillator potential.
</p>
<p>9. Given the plane waves
</p>
<p>1 (x, t) = 01e&plusmn;i(kx&minus;t); 2 (x, t) = 02e&plusmn;i(kx+t); k, &gt; 0; 0i &isin; R.
(V.17)
</p>
<p>Explain in a visual way that 1 (x, t) is a right- and 2 (x, t) a left-moving plane
</p>
<p>wave.
</p>
<p>Solution: For heuristic reasoning we have to consider the real and imaginary
</p>
<p>parts of the functions (otherwise we would have to operate in a four-dimensional</p>
<p/>
</div>
<div class="page"><p/>
<p>444 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>Fig. V.1 Plane wave
</p>
<p>cos (kx &minus; t) with k &gt; 0,
 &gt; 0, travelling to the right.
</p>
<p>Blue for t = 0, red for t &gt; 0
</p>
<p>x
</p>
<p> cos(kx- t)
</p>
<p>&larr; t&gt;0 t=0 &rarr;
</p>
<p>space, which would not be intuitively accessible). We restrict ourselves to 1;
</p>
<p>the argument is analogous for 2 . We have
</p>
<p>1 (x, t) = 01 cos (kx &minus; t)&plusmn; i01 sin (kx &minus; t) . (V.18)
</p>
<p>We now consider the real part 01 cos (kx &minus; t). At the time t = 0, we have
01 cos (kx); one of the maxima of the function is where the argument of the
</p>
<p>cosine disappears, i.e. at x = 0. After a short period of time  , the function reads
01 cos (kx &minus;  ); the maximum is now at the point kx &minus; = 0, i.e. at x = k ,
see Fig. V.1. In other words, the maximum, and thus the entire curve moves to the
</p>
<p>right. The same result is obtained by considering 01 sin (kx &minus; t). Hence, we
can regard 1 (x, t) in sum as a plane wave, travelling to the right.
</p>
<p>V.2 Exercises, Chap. 2
</p>
<p>1. Given an electromagnetic wave E (r, t) = E0ei(kr&minus;t) in a charge-free region
of space (we consider only the electric field); show that the wave is transverse,
</p>
<p>i.e. that k &middot; E0 = 0 holds (Hint: cf. the Maxwell equation &nabla; E = 0). Specialize
to k = (0, 0, k).
Solution:
</p>
<p>0 = &nabla;E = &nabla;E0ei(kr&minus;t) = &part;x E0x ei(kr&minus;t) + &part;y . . .
</p>
<p>= E0x&part;x ei(kr&minus;t) + &part;y &middot; &middot; &middot; = E0x
&part;i (kr &minus; t)
</p>
<p>&part;x
ei(kr&minus;t) + &part;y . . .
</p>
<p>(V.19)
</p>
<p>Because of</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 445
</p>
<p>&part; (kr &minus; t)
&part;x
</p>
<p>= &part;kr
&part;x
</p>
<p>= &part;
(
kx x + ky y + kzz
</p>
<p>)
</p>
<p>&part;x
= kx , (V.20)
</p>
<p>it follows that
</p>
<p>0 = i E0x kx ei(kr&minus;t) + i E0ykyei(kr&minus;t) + i E0zkzei(kr&minus;t)
= iE0 &middot; kei(kr&minus;t)
</p>
<p>(V.21)
</p>
<p>and hence the assertion is demonstrated directly. For k = (0, 0, k), we have
k &middot; E0 = k E0z = 0, which leads to E0z = 0 due to k 	= 0.
</p>
<p>2. Linear combinations
</p>
<p>(a) Express |r as a linear combination of |h and |v. Do the same for |l.
(b) Express |h as a linear combination of |r and |l. Do the same for |v.
</p>
<p>3. A phase shift of 90 is described by ei/2 = i . What follows for a phase shift of
180?
Solution
</p>
<p>180 = ei = &minus;1. (V.22)
</p>
<p>4. Elliptical polarization: Given the state |z =  |h +  |v with ||2 + ||2 = 1;
express |z as superposition of |r and |l.
</p>
<p>V.3 Exercises, Chap. 3
</p>
<p>1. Show explicitly that the solutions of the Schr&ouml;dinger equation (3.1) span a vector
</p>
<p>space.
</p>
<p>2. Calculate
[
x, &part;
</p>
<p>2
</p>
<p>&part;x2
</p>
<p>]
.
</p>
<p>Solution:
(
</p>
<p>x
&part;2
</p>
<p>&part;x2
&minus; &part;
</p>
<p>2
</p>
<p>&part;x2
x
</p>
<p>)
f = x &part;
</p>
<p>2 f
</p>
<p>&part;x2
&minus; &part;
</p>
<p>&part;x
</p>
<p>(
x
&part; f
</p>
<p>&part;x
+ f (x)
</p>
<p>)
</p>
<p>= x &part;
2 f
</p>
<p>&part;x2
&minus;
</p>
<p>(
x
&part;2 f
</p>
<p>&part;x2
+ &part; f
</p>
<p>&part;x
+ &part; f
</p>
<p>&part;x
</p>
<p>)
= &minus;2&part; f
</p>
<p>&part;x
</p>
<p>(V.23)
</p>
<p>i.e. more compactly:
</p>
<p>[
x,
</p>
<p>&part;2
</p>
<p>&part;x2
</p>
<p>]
=
</p>
<p>(
x
&part;2
</p>
<p>&part;x2
&minus; &part;
</p>
<p>2
</p>
<p>&part;x2
x
</p>
<p>)
= &minus;2 &part;
</p>
<p>&part;x
. (V.24)
</p>
<p>3. Given the relativistic energy-momentum relation E2 = m20c4 + c2 p2; from this
dispersion relation, deduce a differential equation.
</p>
<p>Solution: With E &harr; i &part;
&part;t
</p>
<p>and p &harr; 
i
&nabla;, it follows that:</p>
<p/>
</div>
<div class="page"><p/>
<p>446 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>(
i
</p>
<p>&part;
</p>
<p>&part;t
</p>
<p>)2
= m20c4 + c2
</p>
<p>(

</p>
<p>i
&nabla;
</p>
<p>)2
or
</p>
<p>&part;2
</p>
<p>&part;t2
= c2&nabla;2 &minus; m
</p>
<p>2
0c
</p>
<p>4
</p>
<p>2
. (V.25)
</p>
<p>4. Separation: Deduce the time-independent Schr&ouml;dinger equation from the time-
</p>
<p>dependent Schr&ouml;dinger equation by means of the separation of variables.
</p>
<p>5. Given the eigenvalue problem
</p>
<p>&part;
</p>
<p>&part;x
f (x) =  f (x) ;  &isin; C (V.26)
</p>
<p>with f (x) satisfying the boundary conditions f (0) = 1 and f (1) = 2, calculate
eigenfunction and eigenvalue.
</p>
<p>Solution: The general solution of the differential equation reads f (x) = f0ex .
The boundary conditions lead to
</p>
<p>f (0) = f0 = 1 and f (1) = e = 2. (V.27)
</p>
<p>Hence,  = ln 2 is the only eigenvalue.
6. Given the eigenvalue problem
</p>
<p>&part;2
</p>
<p>&part;x2
f = 2 f ;  &isin; C (V.28)
</p>
<p>with f (x) satisfying the boundary conditions f (0) = f (L) = 0; L 	= 0,  	= 0,
calculate eigenfunctions and eigenvalues.
</p>
<p>Solution: The general solution of the differential equation reads f (x) =
f+ex+ f&minus;e&minus;x , with the integration constants f+ and f&minus;. Inserting the boundary
conditions leads to
</p>
<p>f (0) = f+ + f&minus; = 0
f (L) = f+eL + f&minus;e&minus;L = 0.
</p>
<p>(V.29)
</p>
<p>From this, it follows that:
</p>
<p>f&minus; = &minus; f+ (V.30)
</p>
<p>and thus
</p>
<p>f+e
L &minus; f+e&minus;L = 0. (V.31)
</p>
<p>This equation has nontrivial solutions only if
</p>
<p>eL &minus; e&minus;L = 0 or e2L = 1. (V.32)
</p>
<p>It follows then
</p>
<p> = im
L
</p>
<p>; m = 0,&plusmn;1,&plusmn;2, . . . (V.33)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 447
</p>
<p>The quantity  must therefore be imaginary, in order that the eigenvalue problem
</p>
<p>has a solution. The spectrum is discrete, whereby the eigenvalues 2 are always
</p>
<p>negative numbers, 2 = &minus;
(
</p>
<p>m
L
</p>
<p>)2
.
</p>
<p>The eigenfunctions are given by
</p>
<p>f (x) = f+ex &minus; f+e&minus;x = f+
(
</p>
<p>e
im
</p>
<p>L
x &minus; e&minus; imL x
</p>
<p>)
= 2i f+ sin
</p>
<p>m
</p>
<p>L
x . (V.34)
</p>
<p>The constant f+ remains undetermined, since the differential equation
&part;2
</p>
<p>&part;x2
f =
</p>
<p>2 f is linear (hence, for each solution, a multiple is again a solution); to determine
</p>
<p>it, an additional condition is required; see Chap. 5.
</p>
<p>7. Given the nonlinear differential equation
</p>
<p>y&prime;(x) = dy(x)
dx
</p>
<p>= y2(x). (V.35)
</p>
<p>y1 (x) and y2 (x) are two different nontrivial solutions of (V.35), i.e. y1 	= const &middot;
y2 and y1 y2 	= 0.
(a) Show that a multiple of a solution, i.e. f (x) = cy1 (x) with c 	= 0, c 	= 1,
</p>
<p>is not a solution of (V.35).
</p>
<p>Solution: If f (x) is a solution of (V.35), then f &prime; = f 2 must be fulfilled.
Because of
</p>
<p>f &prime; = cy&prime;1 = cy21
f 2 = c2 y21
</p>
<p>, (V.36)
</p>
<p>we obtain immediately c2 = c with the solutions c = 0 and c = 1, which
contradicts the preconditions.
</p>
<p>(b) Show that a linear combination of two solutions, i.e. g(x) = ay1 (x)+by2 (x)
with ab 	= 0, but otherwise arbitrary, is not a solution of (V.35).
Solution: If g(x) is a solution of (V.35), then g&prime; = g2 must hold. Because of
</p>
<p>g&prime; = ay&prime;1 + by&prime;2 = ay21 + by22
g2 = a2 y21 + 2aby1 y2 + b2 y22
</p>
<p>, (V.37)
</p>
<p>we obtain (
a2 &minus; a
</p>
<p>)
y21 + 2aby1 y2 +
</p>
<p>(
b2 &minus; b
</p>
<p>)
y22 = 0. (V.38)
</p>
<p>This equation may be solved e.g. for y1, and the result has the form
</p>
<p>y1 = const &middot; y2 or y1 = 0, which contradicts the preconditions.</p>
<p/>
</div>
<div class="page"><p/>
<p>448 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>Explicitly, we have
</p>
<p>y1 =
&minus;ab &plusmn;&radic;ab (a + b &minus; 1)
</p>
<p>a (a &minus; 1) y2 for a 	= 1; y1 =
1 &minus; b
</p>
<p>2
y2 for a = 1.
</p>
<p>(V.39)
</p>
<p>(c) Find the general solution of (V.35).
</p>
<p>Solution: Invoking the separation of variables, we can write (V.35) as
</p>
<p>dy
</p>
<p>y2
= dx; y 	= 0. (V.40)
</p>
<p>Integrating both sides yields
</p>
<p>&minus; 1
y
= x &minus; C, (V.41)
</p>
<p>where C is an arbitrary integration constant. Solving for y leads to
</p>
<p>y = 1
C &minus; x . (V.42)
</p>
<p>8. Radial momentum
</p>
<p>(a) Show that for the classical momentum p obeys
</p>
<p>p2 =
(
pr
</p>
<p>)2 +
(
p&times; r
</p>
<p>)2
. (V.43)
</p>
<p>Solution:
</p>
<p>(
pr
</p>
<p>)2 = p2 &middot; r2 &middot; cos2 ;
(
p&times; r
</p>
<p>)2 = p2 &middot; r2 &middot; sin2 . (V.44)
</p>
<p>The proposition follows because of r2 = 1.
(b) Deduce the quantum-mechanical expression pr for the classical radial
</p>
<p>momentum rp
(
= pr
</p>
<p>)
.
</p>
<p>Solution: For the translation into quantum mechanics, we have:
</p>
<p>pr f = 
i
&nabla;r f = 
</p>
<p>i
</p>
<p>(
&nabla;r
</p>
<p>)
f + 
</p>
<p>i
r&nabla; f. (V.45)
</p>
<p>With
</p>
<p>&nabla;r = &nabla; r
r
= 1
</p>
<p>r
&nabla;r + r&nabla; 1
</p>
<p>r
= 3
</p>
<p>r
&minus; r
</p>
<p>2
</p>
<p>r3
= 2
</p>
<p>r
(V.46)
</p>
<p>and r&nabla; f = &part;
&part;r
</p>
<p>f , it follows that:</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 449
</p>
<p>pr f = 
i
&nabla;r f = 
</p>
<p>i
</p>
<p>2
</p>
<p>r
f + 
</p>
<p>i
</p>
<p>&part;
</p>
<p>&part;r
f. (V.47)
</p>
<p>On the other hand, we can write
</p>
<p>rp f =
i
r&nabla; f = 
</p>
<p>i
</p>
<p>&part;
</p>
<p>&part;r
f, (V.48)
</p>
<p>obtaining
</p>
<p>(
rp
</p>
<p>)
qm
</p>
<p>f = pr f =
pr f + rp f
</p>
<p>2
= 
</p>
<p>i
</p>
<p>(
1
</p>
<p>r
f + &part;
</p>
<p>&part;r
f
</p>
<p>)
= 
</p>
<p>i
</p>
<p>1
</p>
<p>r
</p>
<p>&part;
</p>
<p>&part;r
r f
</p>
<p>(V.49)
</p>
<p>or, written as an operator,
</p>
<p>pr =

</p>
<p>i
</p>
<p>1
</p>
<p>r
</p>
<p>&part;
</p>
<p>&part;r
r. (V.50)
</p>
<p>For the square, we have
</p>
<p>p2r = &minus;2
1
</p>
<p>r
</p>
<p>&part;2
</p>
<p>&part;r2
r = &minus;2 1
</p>
<p>r2
</p>
<p>&part;
</p>
<p>&part;r
r2
</p>
<p>&part;
</p>
<p>&part;r
= &minus;2
</p>
<p>(
&part;2
</p>
<p>&part;r2
+ 2
</p>
<p>r
</p>
<p>&part;
</p>
<p>&part;r
</p>
<p>)
. (V.51)
</p>
<p>Compare this expression with the representation of the Laplacian in spherical
</p>
<p>coordinates.
</p>
<p>9. Show explicitly that the classical expression l = r&times;p needs not be symmetrized
for the translation into quantum mechanics.
</p>
<p>Solution: Here we have a product of operators, and we need to check whether
</p>
<p>the translation into quantum mechanics of the classical expressions r &times; p and
&minus;p&times; r, which are the same in the classical view, yields the same result. If not,
we have to symmetrize. We first consider only the x components. We have:
</p>
<p>(r &times; p)x = ypz &minus; zpy; &minus; (p&times; r)x = &minus;pyz + pz y. (V.52)
</p>
<p>Since y commutes with pz and z with py (analogously to the components ly and
</p>
<p>lz), then clearly r &times; p = &minus;p&times; r holds true also in quantum mechanics; hence,
we need not symmetrize.
</p>
<p>10. Given the operators A = x d
dx
</p>
<p>, B = d
dx
</p>
<p>x and C = d
dx
</p>
<p>:
</p>
<p>(a) Calculate A fi (x) for the functions f1(x) = x2, f2(x) = eikx and
f3(x) = ln x .
Solution:</p>
<p/>
</div>
<div class="page"><p/>
<p>450 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>A f1 = x
d
</p>
<p>dx
x2 = 2x2
</p>
<p>A f2 = x
d
</p>
<p>dx
eikx = ikxeikx
</p>
<p>A f3 = x
d
</p>
<p>dx
ln x = 1.
</p>
<p>(V.53)
</p>
<p>(b) Calculate A2 f (x) for arbitrary f (x).
</p>
<p>Solution:
</p>
<p>A2 f (x) =x d
dx
</p>
<p>x
d
</p>
<p>dx
f (x) = x d
</p>
<p>dx
x f &prime; = x
</p>
<p>(
x f &prime;&prime; + f &prime;
</p>
<p>)
= x2 f &prime;&prime; + x f &prime;.
</p>
<p>(V.54)
</p>
<p>(c) Calculate the commutators [A, B] and [B,C].
Solution:
</p>
<p>[A, B] f = x d
dx
</p>
<p>d
</p>
<p>dx
x f &minus; d
</p>
<p>dx
xx
</p>
<p>d
</p>
<p>dx
f = x d
</p>
<p>dx
</p>
<p>(
x f &prime; + f
</p>
<p>)
&minus; d
</p>
<p>dx
x2 f &prime;
</p>
<p>= x
(
x f &prime;&prime; + 2 f &prime;
</p>
<p>)
&minus;
</p>
<p>(
x2 f &prime;&prime; + 2x f &prime;
</p>
<p>)
= 0,
</p>
<p>(V.55)
</p>
<p>or, in compact form,
</p>
<p>[A, B] = 0. (V.56)
</p>
<p>For the second commutator, we have
</p>
<p>[B,C] f = d
dx
</p>
<p>x
d
</p>
<p>dx
f &minus; d
</p>
<p>dx
</p>
<p>d
</p>
<p>dx
x f = d
</p>
<p>dx
x f &prime; &minus; d
</p>
<p>dx
</p>
<p>(
x f &prime; + f
</p>
<p>)
= &minus; d
</p>
<p>dx
f,
</p>
<p>(V.57)
</p>
<p>or, in compact form,
</p>
<p>[B,C] = &minus; d
dx
</p>
<p>. (V.58)
</p>
<p>(d) Calculate eiC x2 &minus; (x + i)2. Prove the equation eiC eikx = e&minus;keikx .
Solution: For eiC we use the power series expansion of the e-function:
</p>
<p>eiC = ei ddx =
&infin;&sum;
</p>
<p>n=0
</p>
<p>in
</p>
<p>n!
dn
</p>
<p>dxn
. (V.59)
</p>
<p>It follows that
</p>
<p>eiC x2 =
&infin;&sum;
</p>
<p>n=0
</p>
<p>in
</p>
<p>n!
dn
</p>
<p>dxn
x2 =
</p>
<p>(
1 + i
</p>
<p>1!
d
</p>
<p>dx
+ i
</p>
<p>2
</p>
<p>2!
d2
</p>
<p>dx2
</p>
<p>)
x2
</p>
<p>= x2 + 2i x &minus; 1 = (x + i)2 (V.60)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 451
</p>
<p>or
</p>
<p>eiC x2&minus;(x + i)2 = 0. (V.61)
</p>
<p>For eiC eikx , we find
</p>
<p>eiC eikx =
&infin;&sum;
</p>
<p>n=0
</p>
<p>in
</p>
<p>n!
dn
</p>
<p>dxn
eikx =
</p>
<p>(
1 + i
</p>
<p>1!
d
</p>
<p>dx
+ i
</p>
<p>2
</p>
<p>2!
d2
</p>
<p>dx2
+ &middot; &middot; &middot; + i
</p>
<p>n
</p>
<p>n!
dn
</p>
<p>dxn
+ &middot; &middot; &middot;
</p>
<p>)
</p>
<p>eikx =
(
</p>
<p>1 + i
1! (ik)+
</p>
<p>i2
</p>
<p>2! (ik)
2 + &middot; &middot; &middot; + i
</p>
<p>n
</p>
<p>n! (ik)
n + &middot; &middot; &middot;
</p>
<p>)
eikx
</p>
<p>=
(
</p>
<p>1 &minus; k + k
2
</p>
<p>2! + &middot; &middot; &middot; +
(&minus;1)2n
</p>
<p>n! k
n + &middot; &middot; &middot;
</p>
<p>)
eikx = e&minus;keikx . (V.62)
</p>
<p>V.4 Exercises, Chap. 4
</p>
<p>1. Find examples for state spaces which
</p>
<p>(a) have the structure of a vector space,
</p>
<p>Solution: States of light waves, acoustical waves, water waves (insofar as
</p>
<p>they can be considered as linear phenomena), continuous functions on an
</p>
<p>interval, n &times; n matrices, Rn , polynomials of degree n &le; 8 etc.
(b) do not have the structure of a vector space.
</p>
<p>Solution: States of a coin (heads or tails), of dice (1, 2, 3, 4, 5, 6), of a ball
</p>
<p>in a roulette wheel, cruising altitudes of an airplane, number of fish in an
</p>
<p>aquarium, blood pressure or temperature of a patient etc.
</p>
<p>2. Polarization: Determine the length of the vector 1&radic;
2
</p>
<p>(
1
</p>
<p>i
</p>
<p>)
.
</p>
<p>3. Given y| = i
(
</p>
<p>1 &minus;2
)
</p>
<p>and z| =
(
</p>
<p>2 i
)
, determine y| z.
</p>
<p>4. The Pauli matrices are
</p>
<p>x =
(
</p>
<p>0 1
</p>
<p>1 0
</p>
<p>)
: y =
</p>
<p>(
0 &minus;i
i 0
</p>
<p>)
; z =
</p>
<p>(
1 0
</p>
<p>0 &minus;1
</p>
<p>)
. (V.63)
</p>
<p>In addition to x ,y,z , the notation 1,2,3 is also common.
</p>
<p>(a) Show that 2i = 1, i = x, y, z.
Solution:
</p>
<p>2x =
(
</p>
<p>0 1
</p>
<p>1 0
</p>
<p>)(
0 1
</p>
<p>1 0
</p>
<p>)
=
</p>
<p>(
1 0
</p>
<p>0 1
</p>
<p>)
(V.64)
</p>
<p>and analogously for 2y and 
2
z with the same result.
</p>
<p>(b) Determine the commutators
[
i , j
</p>
<p>]
= i j &minus;  ji and the anticommuta-
</p>
<p>tors
{
i , j
</p>
<p>}
= i j +  j (i 	= j).
</p>
<p>Solution: For the commutator
[
x ,y
</p>
<p>]
, it holds that:</p>
<p/>
</div>
<div class="page"><p/>
<p>452 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>[
x ,y
</p>
<p>]
=
</p>
<p>(
0 1
</p>
<p>1 0
</p>
<p>)(
0 &minus;i
i 0
</p>
<p>)
&minus;
</p>
<p>(
0 &minus;i
i 0
</p>
<p>)(
0 1
</p>
<p>1 0
</p>
<p>)
= 2iz, (V.65)
</p>
<p>and similarly for the other indices.
</p>
<p>For the anticommutator
{
x ,y
</p>
<p>}
, we have
</p>
<p>{
x ,y
</p>
<p>}
=
</p>
<p>(
0 1
</p>
<p>1 0
</p>
<p>)(
0 &minus;i
i 0
</p>
<p>)
+
</p>
<p>(
0 &minus;i
i 0
</p>
<p>)(
0 1
</p>
<p>1 0
</p>
<p>)
= 0 (V.66)
</p>
<p>and similarly for the other indices.
</p>
<p>(c) Calculate the eigenvalues and eigenvectors for each Pauli matrix.
</p>
<p>Solution: The eigenvalue equations read ivi = ivi . Because of 2i = 1,
we find
</p>
<p>2i vi =
{
</p>
<p>vi
iivi = 2i vi
</p>
<p>&rarr; i = &plusmn;1. (V.67)
</p>
<p>Hence, all three Pauli matrices have the eigenvalues &plusmn;1. The (normalized)
eigenvectors are given by
</p>
<p>vx,&plusmn;1 =
1&radic;
2
</p>
<p>(
1
</p>
<p>&plusmn;1
</p>
<p>)
; vy,&plusmn;1 =
</p>
<p>1&radic;
2
</p>
<p>(
1
</p>
<p>&plusmn;i
</p>
<p>)
</p>
<p>vz,+1 =
(
</p>
<p>1
</p>
<p>0
</p>
<p>)
; vz,&minus;1 =
</p>
<p>(
0
</p>
<p>1
</p>
<p>)
.
</p>
<p>(V.68)
</p>
<p>5. Determine the eigenvalues and eigenvectors of the matrix
</p>
<p>M =
(
</p>
<p>1 4
</p>
<p>2 &minus;1
</p>
<p>)
. (V.69)
</p>
<p>Normalize the eigenvectors. Are they orthogonal?
</p>
<p>6. Given the CONS {|a1 , |a2}, determine the eigenvalues and eigenvectors of the
operator
</p>
<p>M = |a1 a1| &minus; |a2 a2| . (V.70)
</p>
<p>Solution: We have
</p>
<p>M |a1 = (|a1 a1| &minus; |a2 a2|) |a1 = |a1
M |a2 = (|a1 a1| &minus; |a2 a2|) |a2 = &minus; |a2 . (V.71)
</p>
<p>Thus, the eigenvalues are 1 and&minus;1; the associated eigenvectors are |a1 and |a2.
7. Given a CONS {|n} and a state | =
</p>
<p>&sum;
n
</p>
<p>cn |n, cn &isin; C, calculate the
coefficients cn .</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 453
</p>
<p>Solution:
</p>
<p>i |  = i |
&sum;
</p>
<p>n
</p>
<p>cn |n =
&sum;
</p>
<p>n
</p>
<p>cn i | n =
&sum;
</p>
<p>n
</p>
<p>cnin = ci , (V.72)
</p>
<p>or, in compact form,
</p>
<p>cn = n|  . (V.73)
</p>
<p>8. Show in bra-ket notation: The system {|r , |l} is a CONS. Use the fact that
{|h , |v} is a CONS.
Solution: We start from
</p>
<p>|r = 1&radic;
2
|h + i&radic;
</p>
<p>2
|v ; |l = 1&radic;
</p>
<p>2
|h &minus; i&radic;
</p>
<p>2
|v . (V.74)
</p>
<p>It follows that
</p>
<p>r | r =
[
</p>
<p>1&radic;
2
h| &minus; i&radic;
</p>
<p>2
v|
</p>
<p>] [
1&radic;
2
|h + i&radic;
</p>
<p>2
|v
</p>
<p>]
= 1
</p>
<p>2
&minus; i
</p>
<p>2
</p>
<p>2
= 1, (V.75)
</p>
<p>and analogously for l| l. Furthermore, we have:
</p>
<p>r | l =
[
</p>
<p>1&radic;
2
h| &minus; i&radic;
</p>
<p>2
v|
</p>
<p>] [
1&radic;
2
|h &minus; i&radic;
</p>
<p>2
|v
</p>
<p>]
= 1
</p>
<p>2
+ i
</p>
<p>2
</p>
<p>2
= 0. (V.76)
</p>
<p>Hence, the orthonormality is proved. Completeness follows from
</p>
<p>|r r | + |l l|
=
</p>
<p>[
1&radic;
2
|h+ i&radic;
</p>
<p>2
|v
</p>
<p>] [
1&radic;
2
h| &minus; i&radic;
</p>
<p>2
v|
</p>
<p>]
+
[
</p>
<p>1&radic;
2
|h &minus; i&radic;
</p>
<p>2
|v
</p>
<p>] [
1&radic;
2
h| + i&radic;
</p>
<p>2
v|
</p>
<p>]
</p>
<p>= 1
2
|h h| &minus; i
</p>
<p>2
|h v| + i
</p>
<p>2
|v h| &minus; i
</p>
<p>2
</p>
<p>2
|v v| + c.c
</p>
<p>= |h h| + |v v| = 1
</p>
<p>(V.77)
</p>
<p>where c.c. means the complex conjugate of the preceding expression.
</p>
<p>If we use alternatively the representation
</p>
<p>|r &sim;= 1&radic;
2
</p>
<p>(
1
</p>
<p>i
</p>
<p>)
; |l &sim;= 1&radic;
</p>
<p>2
</p>
<p>(
1
</p>
<p>&minus;i
</p>
<p>)
, (V.78)
</p>
<p>the same result follows:
l| r = 0 &harr; |r &perp; |l
l| l = r | r = 1
|r r | + |l l| = 1.
</p>
<p>(V.79)</p>
<p/>
</div>
<div class="page"><p/>
<p>454 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>So {|r , |l} is also a CONS. Accordingly, {|h , |v} as well as {|r , |l} each
form a basis in V; every vector |z &isin; V can be written as |z = c1 |h + c2 |v or
|z = d1 |r + d2 |l with ci , di &isin; C.
</p>
<p>9. Given the operator |h r |:
(a) Is it a projection operator?
</p>
<p>Solution: No, because of
</p>
<p>|h r |h r | = 1&radic;
2
|h r | 	= |h r | due to r |h = 1&radic;
</p>
<p>2
. (V.80)
</p>
<p>(b) How does the operator appear in the representation (4.1)?
</p>
<p>Solution:
</p>
<p>|h r | &sim;= 1&radic;
2
</p>
<p>(
1
</p>
<p>0
</p>
<p>) (
1 &minus;i
</p>
<p>)
= 1&radic;
</p>
<p>2
</p>
<p>(
1 &minus;i
0 0
</p>
<p>)
. (V.81)
</p>
<p>(c) Given the state |z with the representation |z &sim;=
(
</p>
<p>z1
z2
</p>
<p>)
, apply the operator
</p>
<p>|h r | to this state (calculation making use of the representation).
Solution:
</p>
<p>|h r | z &sim;= 1&radic;
2
</p>
<p>(
1 &minus;i
0 0
</p>
<p>)(
z1
z2
</p>
<p>)
= 1&radic;
</p>
<p>2
</p>
<p>(
z1 &minus; i z2
</p>
<p>0
</p>
<p>)
. (V.82)
</p>
<p>(d) Use the concrete representation to prove the equality
</p>
<p>(|h r | z)&dagger; = z| r h| . (V.83)
</p>
<p>Solution:
</p>
<p>(1) (|h r | z)&dagger; &sim;=
(
</p>
<p>1&radic;
2
</p>
<p>(
z1 &minus; i z2
</p>
<p>0
</p>
<p>))&dagger;
= 1&radic;
</p>
<p>2
</p>
<p>(
z&lowast;1 + i z&lowast;2 0
</p>
<p>)
</p>
<p>(2) z| r h| &sim;= 1&radic;
2
</p>
<p>(
z&lowast;1 z
</p>
<p>&lowast;
2
</p>
<p>) ( 1
i
</p>
<p>) (
1 0
</p>
<p>)
</p>
<p>= z&lowast;1+i z&lowast;2&radic;
2
</p>
<p>(
1 0
</p>
<p>)
= 1&radic;
</p>
<p>2
</p>
<p>(
z&lowast;1 + i z&lowast;2 0
</p>
<p>)
.
</p>
<p>(V.84)
</p>
<p>10. We choose the following representation for the states |h and |v:
</p>
<p>|h &sim;= 1&radic;
2
</p>
<p>(
i
</p>
<p>1
</p>
<p>)
; |v &sim;= a&radic;
</p>
<p>2 |a|
</p>
<p>(
1
</p>
<p>i
</p>
<p>)
. (V.85)
</p>
<p>(a) Show that the representing vectors form a CONS.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 455
</p>
<p>Solution:
</p>
<p>N: h |h = 1
2
</p>
<p>(
&minus;i 1
</p>
<p>) ( i
1
</p>
<p>)
= 1
</p>
<p>2
</p>
<p>(
&minus;i2 + 1
</p>
<p>)
= 1; for |v analogously;
</p>
<p>O: v |h = a+
2|a|
</p>
<p>(
1 &minus;i
</p>
<p>) ( i
1
</p>
<p>)
= a+
</p>
<p>2|a| (i &minus; i) = 0;
</p>
<p>C: |h h| + |v v| &sim;= 12
(
</p>
<p>i
</p>
<p>1
</p>
<p>) (
&minus;i 1
</p>
<p>)
+ 1
</p>
<p>2
</p>
<p>(
1
</p>
<p>i
</p>
<p>) (
1 &minus;i
</p>
<p>)
</p>
<p>= 1
2
</p>
<p>(
&minus;i2 i
&minus;i 1
</p>
<p>)
+ 1
</p>
<p>2
</p>
<p>(
1 &minus;i
i &minus;i2
</p>
<p>)
=
</p>
<p>(
1 0
</p>
<p>0 1
</p>
<p>)
.
</p>
<p>(V.86)
</p>
<p>(b) Determine |r and |l in this representation. Specialize to the cases of
a = 1,&minus;1, i,&minus;i .
Solution: With
</p>
<p>|r = |h + i |v&radic;
2
</p>
<p>; |r = |h &minus; i |v&radic;
2
</p>
<p>, (V.87)
</p>
<p>it follows that:
</p>
<p>|h &plusmn; i |v&radic;
2
</p>
<p>&sim;= 1
2
</p>
<p>(
i
</p>
<p>1
</p>
<p>)
&plusmn; ia
</p>
<p>2 |a|
</p>
<p>(
1
</p>
<p>i
</p>
<p>)
= 1
</p>
<p>2
</p>
<p>
 i
</p>
<p>(
1 &plusmn; a|a|
</p>
<p>)
</p>
<p>1  a|a|
</p>
<p>
 . (V.88)
</p>
<p>11. Show that the three vectors
</p>
<p>a = 1&radic;
2
</p>
<p>

</p>
<p>1
</p>
<p>i
</p>
<p>0
</p>
<p>
 ; b =
</p>
<p>

</p>
<p>0
</p>
<p>0
</p>
<p>1
</p>
<p>
 ; c = &minus; 1&radic;
</p>
<p>2
</p>
<p>

</p>
<p>1
</p>
<p>&minus;i
0
</p>
<p>
 (V.89)
</p>
<p>form a CONS. Do the same for
</p>
<p>a = 1&radic;
2
</p>
<p>

</p>
<p>1
</p>
<p>0
</p>
<p>&minus;1
</p>
<p>
 ; b = 1
</p>
<p>2
</p>
<p>

</p>
<p>1&radic;
2
</p>
<p>1
</p>
<p>
 ; c = 1
</p>
<p>2
</p>
<p>

</p>
<p>1
</p>
<p>&minus;
&radic;
</p>
<p>2
</p>
<p>1
</p>
<p>
 . (V.90)
</p>
<p>12. A three-dimensional problem: Given the CONS {|u , |v , |w} and the operator
</p>
<p>L = |v u| + (|u + |w) v| + |v w| . (V.91)
</p>
<p>(a) Determine the eigenvalues and eigenvectors of L .
</p>
<p>Solution: The eigenvalue problem reads
</p>
<p>L | = l | . (V.92)</p>
<p/>
</div>
<div class="page"><p/>
<p>456 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>Since {|u , |v , |w} are a CONS, we can represent | as
</p>
<p>| = a |u + b |v + c |w . (V.93)
</p>
<p>We insert (V.91) and (V.93) into (V.92) and initially obtain
</p>
<p>[|vu| + (|u + |w)v| + |vw|][a|u + b|v + c|w]
= l[a|u + b|v + c|w]. (V.94)
</p>
<p>The multiplication of the left side, due to the orthonormality of the states
</p>
<p>|u , |v , |w, yields:
</p>
<p>a |v + b (|u + |w)+ c |v = l [a |u + b |v + c |w] (V.95)
</p>
<p>and from this follow the equations
</p>
<p>|u : b = la
|v : a + c = lb;
|w : b = lc
</p>
<p>with b = lc follows lc = la
a + c = l2c. (V.96)
</p>
<p>We can now have either l = 0, from which follows b = 0, c = &minus;a; or l 	= 0.
In the latter case, c = a and 2a = l2a, i.e. l = &plusmn;
</p>
<p>&radic;
2.
</p>
<p>To summarize: The three eigenvalues are = 0 and l = &plusmn;
&radic;
</p>
<p>2. The corre-
</p>
<p>sponding eigenvectors are given initially by
</p>
<p>l = 0 : |0 = a |u &minus; a |w
l = &plusmn;
</p>
<p>&radic;
2 : |&plusmn;&radic;2 = a |u &plusmn;
</p>
<p>&radic;
2a |v + a |w . (V.97)
</p>
<p>Normalizing theses states yields the final result:
</p>
<p>l = 0 : |0 = |u&minus;|w&radic;2
l = &plusmn;
</p>
<p>&radic;
2 : |&plusmn;&radic;2 = |u&plusmn;
</p>
<p>&radic;
2|v+|w
2
</p>
<p>.
(V.98)
</p>
<p>(b) Show that the three eigenvectors (V.98) form a CONS.
</p>
<p>V.5 Exercises, Chap. 5
</p>
<p>1. Given the free stationary SEq
</p>
<p>E(x) = &minus; 
2
</p>
<p>2m

</p>
<p>&prime;&prime;
(x) , (V.99)
</p>
<p>formulate the corresponding equation for the Fourier transform of .
</p>
<p>Solution: (x) is connected with its Fourier transform (k) by</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 457
</p>
<p>(x) = 1&radic;
2
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>(k) eikx dk; (k) = 1&radic;
2
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>(x) e&minus;ikx dx . (V.100)
</p>
<p>By inserting into the free SEq, we obtain
</p>
<p>E 1&radic;
2
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>(k) eikx dk = &minus; 2
2m
</p>
<p>1&radic;
2
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>(
&minus;k2
</p>
<p>)
(k) eikx dk
</p>
<p>or E(k) = 2k2
2m
</p>
<p>(k) .
</p>
<p>(V.101)
</p>
<p>2. Given the stationary SEq
</p>
<p>E(x) = &minus; 
2
</p>
<p>2m

</p>
<p>&prime;&prime;
(x)+ V (x) (x) , (V.102)
</p>
<p>formulate the corresponding equation for the Fourier transform of .
</p>
<p>Solution: Inserting into the free SEq initially gives
</p>
<p>E
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>(k) eikx dk = &minus; 
2
</p>
<p>2m
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>(
&minus;k2
</p>
<p>)
(k) eikx dk + V (x)
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>(k) eikx dk.
</p>
<p>(V.103)
</p>
<p>To eliminate the variable x , we consider the Fourier transform W (k) of V (x):
</p>
<p>V (x) = 1&radic;
2
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>W (k) eikx dk; W (k) = 1&radic;
2
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>V (x) e&minus;ikx dx . (V.104)
</p>
<p>With this, we have
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>[
E &minus; 
</p>
<p>2k2
</p>
<p>2m
</p>
<p>]
(k) eikx dk = 1&radic;
</p>
<p>2
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>W (k1) e
ik1x dk1
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>(k2) e
ik2x dk2.
</p>
<p>(V.105)
</p>
<p>We multiply by e&minus;i K x and integrate with respect to x :
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>[
E &minus; 2k2
</p>
<p>2m
</p>
<p>]
(k)
</p>
<p>

</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>ei(k&minus;K )x dx
</p>
<p>
 dk
</p>
<p>= 1&radic;
2
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>W (k1)
</p>
<p>

</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>ei(k1+k2&minus;K )x dx
</p>
<p>
 dk1
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>(k2) dk2.
</p>
<p>(V.106)</p>
<p/>
</div>
<div class="page"><p/>
<p>458 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>The square brackets are essentially the delta function,158 and thus it follows that
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>[
E &minus; 
</p>
<p>2k2
</p>
<p>2m
</p>
<p>]
(k)  (k &minus; K ) dk
</p>
<p>= 1&radic;
2
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>W (k1)  (k1 + k2 &minus; K ) dk1
&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>(k2) dk2 (V.107)
</p>
<p>or [
E &minus; 
</p>
<p>2 K 2
</p>
<p>2m
</p>
<p>]
(K ) = 1&radic;
</p>
<p>2
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>W (k1) (K &minus; k1) dk1. (V.108)
</p>
<p>The final result reads
</p>
<p>[
E &minus; 
</p>
<p>2k2
</p>
<p>2m
</p>
<p>]
(k) = 1&radic;
</p>
<p>2
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>W
(
k &prime;
)

(
k &minus; k &prime;
</p>
<p>)
dk &prime;. (V.109)
</p>
<p>This integral equation for (k) replaces the SEq (V.102) in momentum space,
</p>
<p>as originally formulated in the position representation. Since the two equations
</p>
<p>yield the same information in the end, it is more a matter of taste which one will
</p>
<p>be applied. We use the &lsquo;usual&rsquo; SEq (V.102), since the corresponding concepts,
</p>
<p>methods of solution etc. are for most people more familiar than those of integral
</p>
<p>equations.
</p>
<p>3. The Hamiltonian has discrete nondegenerate eigenvalues En , n = 1, 2, . . .. What
is the general solution of the time-dependent SEq?
</p>
<p>4. Infinite potential well: Show that the eigenfunctions in the form n(x) =&radic;
2
a
</p>
<p>ein sin(kn x) constitute an orthonormal system of functions (
a&int;
</p>
<p>0
</p>
<p>&lowast;m(x)n(x) =
mn). Hint: The integrals can be calculated for example by means of sin x sin y =
cos(x&minus;y)&minus;cos(x+y)
</p>
<p>2
or the exponential representation of the sine functions.
</p>
<p>5. Infinite potential well: Formulate the general solution of the time-dependent SEq
</p>
<p>and verify that the specification of the initial condition determines the wavefunc-
</p>
<p>tion. Concretize the consideration to the special cases (C &isin; C is an arbitrary
complex constant):
</p>
<p>(a) 
(x, t = 0) = C(x &minus; a
2
);
</p>
<p>(b) 
(x, t = 0) = C ;
(c) 
(x, t = 0) = Cei K x .
</p>
<p>Solution: As stated in the text, the general solution is (for simplicity, we
</p>
<p>158It is 
(
k &minus; k&prime;
</p>
<p>)
= 1
</p>
<p>2
</p>
<p>&infin;&int;
&minus;&infin;
</p>
<p>dx ei x(k&minus;k
&prime;). Some remarks concerning the delta function and its prop-
</p>
<p>erties are found in Appendix H, Vol. 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 459
</p>
<p>have set the phases n equal to zero):
</p>
<p>
 (x, t) =
&radic;
</p>
<p>2
</p>
<p>a
</p>
<p>&infin;&sum;
</p>
<p>n=1
cn sin kn xe
</p>
<p>&minus;in t ; cn &isin; C; n =
En
</p>
<p>
= k
</p>
<p>2
n
</p>
<p>2m
. (V.110)
</p>
<p>It follows that
</p>
<p>
 (x, 0) =
&radic;
</p>
<p>2
a
</p>
<p>&infin;&sum;
</p>
<p>n=1
cn sin kn x or
</p>
<p>&radic;
2
a
</p>
<p>a&int;
</p>
<p>0
</p>
<p>sin km x
 (x, 0) dx = 2a
&infin;&sum;
</p>
<p>n=1
cn
</p>
<p>a&int;
</p>
<p>0
</p>
<p>sin km x sin kndx .
</p>
<p>(V.111)
</p>
<p>The last integral equals a
2
mn , and we obtain
</p>
<p>cm =
&radic;
</p>
<p>2
</p>
<p>a
</p>
<p>a&int;
</p>
<p>0
</p>
<p>sin km x
 (x, 0) dx . (V.112)
</p>
<p>(a) 
(x, t = 0) = C(x &minus; a
2
).
</p>
<p>Solution:
</p>
<p>cm =
&radic;
</p>
<p>2
a
</p>
<p>a&int;
</p>
<p>0
</p>
<p>sin km x &middot; C(x &minus; a2 )dx
</p>
<p>=
&radic;
</p>
<p>2
a
C sin km a
</p>
<p>2
=
</p>
<p>&radic;
2
a
C sin m
</p>
<p>2
= 1&minus;(&minus;1)m
</p>
<p>2
(&minus;1) m&minus;12 .
</p>
<p>(V.113)
</p>
<p>6. Given the three-dimensional SEq E(r) = &minus; 2
2m
</p>
<p>&nabla;2(r), which energy eigen-
values are allowed if one requires the following periodic boundary conditions:
</p>
<p>(x, y, z) = (x + L x , y, z) = (x, y + L y, z) = (x, y, z + L z)?
Note: with such periodic boundary conditions one can model, among other
</p>
<p>things, three-dimensional periodic structures, such as solid lattices. In two dimen-
</p>
<p>sions, one can also imagine that these conditions define a torus on whose surface
</p>
<p>the quantum system is located; see Fig. V.2.
</p>
<p>Fig. V.2 Torus
</p>
<p>(two-dimensional surface)</p>
<p/>
</div>
<div class="page"><p/>
<p>460 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>Solution: We use again the separation ansatz, (r) = f (x)g(y)h(z). It leads in
the usual manner to f (x) = Ax eikx x + Bx e&minus;ikx x and corresponding expressions
for g(y) and h(z). Insertion into the SEq yields:
</p>
<p>E = 
2k2
</p>
<p>2m
(V.114)
</p>
<p>with k =
(
kx , ky, kz
</p>
<p>)
. The periodic boundary condition, e.g. for x , leads to
</p>
<p>(x, y, z) = (x + L x , y, z) &rarr;
&rarr; Ax eikx x + Bx e&minus;ikx x = Ax eikx (x+L x ) + Bx e&minus;ikx (x+L x ).
</p>
<p>(V.115)
</p>
<p>Hence, it follows that e&plusmn;ikx L x = 1. For integers n, we have 1 = e2in and it
follows that
</p>
<p>kx L x = 2nx or kx =
2nx
</p>
<p>L x
, nx &isin; N (V.116)
</p>
<p>With analogous results for y, z we obtain the following energy levels:
</p>
<p>E = 2
22
</p>
<p>m
</p>
<p>[(
nx
</p>
<p>L x
</p>
<p>)2
+
</p>
<p>(
ny
</p>
<p>L y
</p>
<p>)2
+
</p>
<p>(
nz
</p>
<p>L z
</p>
<p>)2]
. (V.117)
</p>
<p>We see nicely that the degree of degeneracy increases with increasing symmetry
</p>
<p>(e.g. L x = L y or L x = L y = L z).
Remark: In this case, one can work from the outset with the ansatz (r) = Aeikr.
</p>
<p>7. An electron is located between the two walls of an infinite potential well, which
</p>
<p>are one light year apart. Calculate roughly the magnitude of the difference
</p>
<p>between two adjacent energy levels.
</p>
<p>Solution: 1ly &asymp; 9.5 &middot; 1015m &asymp; 1016 m;  &asymp; 10&minus;34 Js; me &asymp; 10&minus;30 kg; 1J &asymp;
6 &middot; 1018 eV. We assume kn &asymp; nL . It follows that
</p>
<p>En &asymp;
22
</p>
<p>2mL2
n2 &asymp; 10&minus;69 n
</p>
<p>2
</p>
<p>2
J &asymp; 3 &middot; 10&minus;51n2eV. (V.118)
</p>
<p>8. Find examples for functions which
</p>
<p>(a) are integrable, but not square-integrable;
</p>
<p>Solution: f (x) = 1&radic;
x
</p>
<p>is integrable in the interval [0, 1], but not square
</p>
<p>integrable:
</p>
<p>1&int;
</p>
<p>0
</p>
<p>1&radic;
x
dx = 2.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 461
</p>
<p>(b) square-integrable, but not integrable.
</p>
<p>Solution: f (x) = 1
x
</p>
<p>is not integrable in the interval [1,&infin;], but is square
</p>
<p>integrable:
</p>
<p>&infin;&int;
</p>
<p>1
</p>
<p>1
x2
</p>
<p>dx = 1.
</p>
<p>9. Given the stationary SEq
</p>
<p>E (x) = &minus; 
2
</p>
<p>2m
&prime;&prime;(x)+ V (x)(x), (V.119)
</p>
<p>rewrite this equation for a dimensionless independent variable.
</p>
<p>Solution: We choose
</p>
<p>z = K x;  (x) =  (K x) =  (z) (V.120)
</p>
<p>with the yet undetermined constant K (unit 1/m) and the dimensionless variable
</p>
<p>z. Insertion yields initially
</p>
<p>E (z) = &minus; 
2
</p>
<p>2m
</p>
<p>d2
</p>
<p>dx2
 (z)+ V (x) (z) . (V.121)
</p>
<p>Changing variables in the derivative yields
</p>
<p>E (z) = &minus;
2 K 2
</p>
<p>2m
</p>
<p>d2
</p>
<p>dz2
 (z)+ V
</p>
<p>( z
K
</p>
<p>)
 (z) . (V.122)
</p>
<p>One should now choose K in such a manner that the prefactors of the functions
</p>
<p>are as simple as possible. If we use for example K 2 = 2m E
2
</p>
<p>(which need not
</p>
<p>necessarily be the cleverest choice), we obtain
</p>
<p> (z) = &minus; d
2
</p>
<p>dz2
 (z)+ V (z) (z) (V.123)
</p>
<p>with the dimensionless potential V (z) = 1
E
</p>
<p>V
(
</p>
<p>z
K
</p>
<p>)
.
</p>
<p>10. A short outlook into string theory (compactified or rolled-up dimensions): String
</p>
<p>theory assumes that the elementary building blocks are not point objects, but
</p>
<p>rather one-dimensional objects (strings) with a certain energy&mdash;comparable to
</p>
<p>an object in a one-dimensional potential well. Strings have a spatial extension of
</p>
<p>order of the Planck length and live in higher-dimensional spaces (e.g. dim = 10
or dim = 26), where only four dimensions are not rolled up (compactified)&mdash;
quite similar to our following simple example.
</p>
<p>For the formal treatment, we take the two-dimensional SEq
</p>
<p>&minus; 
2
</p>
<p>2m
</p>
<p>(
&part;2
</p>
<p>&part;x2
+ &part;
</p>
<p>2
</p>
<p>&part;y2
</p>
<p>)
= E (V.124)</p>
<p/>
</div>
<div class="page"><p/>
<p>462 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>Fig. V.3 The &lsquo;cylinder
</p>
<p>world&rsquo; of our toy string
</p>
<p>R
</p>
<p>a
</p>
<p>as starting point. In x direction, we have an infinite potential well
</p>
<p>V =
{
</p>
<p>0 for 0 &lt; x &lt; a
</p>
<p>&infin; otherwise (V.125)
</p>
<p>and for the y coordinate we postulate
</p>
<p> (x, y) =  (x, y + 2R) . (V.126)
</p>
<p>So we have a combination of two different boundary conditions: In the x direc-
</p>
<p>tion,  (0, y) =  (a, y) = 0 applies, while in the y direction the periodic
boundary condition  (x, y) =  (x, y + 2R) is valid. In other words, the
quantum object &lsquo;lives&rsquo; on the surface of a cylinder of length a and of radius R
</p>
<p>(see Fig. V.3). The problem reads is now to calculate the possible energy levels.
</p>
<p>Discuss in particular the situation when R  a.
Solution: For the solution of the SEq, we use the separation ansatz:
</p>
<p> (x, y) = (x)
 (y) (V.127)
</p>
<p>and obtain
</p>
<p>&minus; 
2
</p>
<p>2m
</p>
<p>1
</p>
<p>(x)
</p>
<p>d2(x)
</p>
<p>dx2
&minus; 
</p>
<p>2
</p>
<p>2m
</p>
<p>1
</p>
<p>
 (y)
</p>
<p>d2
 (y)
</p>
<p>dy2
= E . (V.128)
</p>
<p>The terms which depend on x or y have to be constant:
</p>
<p>&minus; 
2
</p>
<p>2m
</p>
<p>1
</p>
<p>(x)
</p>
<p>d2(x)
</p>
<p>dx2
= Ex
</p>
<p>&minus; 
2
</p>
<p>2m
</p>
<p>1
</p>
<p>
 (y)
</p>
<p>d2
 (y)
</p>
<p>dy2
= E &minus; Ex
</p>
<p>or
</p>
<p>d2(x)
</p>
<p>dx2
= &minus;2m
</p>
<p>2
Ex(x) = &minus;k2x(x)
</p>
<p>d2
 (y)
</p>
<p>dy2
= &minus;2m
</p>
<p>2
(E &minus; Ex )
 (y) = &minus;k2y
 (y) .
</p>
<p>(V.129)
</p>
<p>As usual, we obtain as solutions (real form)
</p>
<p>(x) = A sin kx x + B cos kx x

 (y) = C sin ky y + D cos ky y. (V.130)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 463
</p>
<p>With the boundary condition (0) = (a) = 0, the first equation gives
</p>
<p>B = A and sin kx a = 0 &rarr; kx =
N
</p>
<p>a
, n = 1, 2, . . . (V.131)
</p>
<p>For 
 (y), we obtain with 
 (y) = 
 (y + 2R) :
</p>
<p>C sin ky y + D cos ky y = C sin ky (y + 2R)+ D cos ky (y + 2R) . (V.132)
</p>
<p>Since C and D are independent integration constants, their coefficients must be
</p>
<p>equal on both sides. This leads to
</p>
<p>ky2R = 2M or ky =
M
</p>
<p>R
, M = 0, 1, 2, . . . (V.133)
</p>
<p>The range of values of M also includes zero, since in this case the trivial solution
</p>
<p>does not occur, because we have 
 (y) = D for ky = 0. This fact is especially
important for the discussion in the case R  a, as we shall see shortly.
For the energy, it follows because of E = Ex + Ey = 
</p>
<p>2
</p>
<p>2m
</p>
<p>(
k2x + k2y
</p>
<p>)
that:
</p>
<p>EN ,M =
2
</p>
<p>2m
</p>
<p>[(
N
</p>
<p>a
</p>
<p>)2
+
</p>
<p>(
M
</p>
<p>R
</p>
<p>)2]
; N = 1, 2, . . . ; M = 0, 1, 2, . . .
</p>
<p>(V.134)
</p>
<p>where N and M assume values independently of each other.
</p>
<p>Due to the second dimension, the energy spectrum has changed significantly
</p>
<p>overall. In particular, it can now be degenerate; that is the case for R = a

</p>
<p>&radic;
p
</p>
<p>q
,
</p>
<p>where p and q are differences of squares of natural numbers.
</p>
<p>For M = 0, the energy levels EN ,0 are those of the one-dimensional infinite
potential well. Where is the lowest new energy level? We evidently find it for
</p>
<p>N = 1 (N = 0 is not allowed) and M = 1, i.e.
</p>
<p>E1,1 =
2
</p>
<p>2m
</p>
<p>[(
a
</p>
<p>)2
+
</p>
<p>(
1
</p>
<p>R
</p>
<p>)2]
. (V.135)
</p>
<p>If we now consider a very &lsquo;thin&rsquo; cylinder, i.e. R  a, it follows that
</p>
<p>E1,1 &asymp;
2
</p>
<p>2m
</p>
<p>(
1
</p>
<p>R
</p>
<p>)2
for R  a. (V.136)
</p>
<p>In comparison to the &lsquo;unperturbed&rsquo; energy levels EN ,0, this means that
</p>
<p>E1,1 &asymp; EK ,0 with K &asymp;
a
</p>
<p>R
. (V.137)</p>
<p/>
</div>
<div class="page"><p/>
<p>464 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>Due to R  a, K is a very large number, so that the first new energy level E1,1 is
far beyond the low-lying energy levels EN ,0. In other words, an extra dimension
</p>
<p>cannot be seen at lower energies in experiments, if it is rolled up tightly enough.
</p>
<p>These effects can be seen only at sufficiently high energies.
</p>
<p>11. Given the free one-dimensional SEq (5.36) and the function (x), show that
</p>
<p>
 (x, t) = A 1&radic;
t
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>e
im
2
</p>
<p>(x&minus;y)2
t (y) dy (V.138)
</p>
<p>is a solution (A is a normalizing factor).
</p>
<p>Solution: We compute the partial derivatives. We find
</p>
<p>&part;t
 (x, t) = &minus;A
1
</p>
<p>2t
&radic;
</p>
<p>t
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>e
im
2
</p>
<p>(x&minus;y)2
2t (y) dy
</p>
<p>+ A 1&radic;
t
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>im
</p>
<p>2
</p>
<p>(
&minus; (x &minus; y)
</p>
<p>2
</p>
<p>t2
</p>
<p>)
e
</p>
<p>im
2
</p>
<p>(x&minus;y)2
t (y) dy
</p>
<p>&part;x
 (x, t) = A
1&radic;
t
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>im
</p>
<p>
</p>
<p>(x &minus; y)
t
</p>
<p>e
im
2
</p>
<p>(x&minus;y)2
t (y) dy
</p>
<p>&part;2x
 (x, t)=A
1&radic;
t
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>[
im
</p>
<p>
</p>
<p>1
</p>
<p>t
e
</p>
<p>im
2
</p>
<p>(x&minus;y)2
t +
</p>
<p>(
im
</p>
<p>
</p>
<p>(x &minus; y)
t
</p>
<p>)2]
e
</p>
<p>im
2
</p>
<p>(x&minus;y)2
t (y) dy.
</p>
<p>(V.139)
</p>
<p>It follows that
</p>
<p>i&part;t
 (x, t) = A
&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>[
&minus;i 1
</p>
<p>2t
&radic;
</p>
<p>t
+ i 1&radic;
</p>
<p>t
</p>
<p>im
</p>
<p>2
</p>
<p>(
&minus; (x &minus; y)
</p>
<p>2
</p>
<p>t2
</p>
<p>)]
e
</p>
<p>im
2
</p>
<p>(x&minus;y)2
t (y) dy
</p>
<p>&minus; 
2
</p>
<p>2m
&part;2x
 (x, t)=A
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>[
&minus; 
</p>
<p>2
</p>
<p>2m
</p>
<p>im
</p>
<p>
</p>
<p>1
</p>
<p>t
&radic;
</p>
<p>t
&minus; 
</p>
<p>2
</p>
<p>2m
</p>
<p>1&radic;
t
</p>
<p>(
im
</p>
<p>
</p>
<p>(x &minus; y)
t
</p>
<p>)2]
e
</p>
<p>im
2
</p>
<p>(x&minus;y)2
t (y) dy.
</p>
<p>(V.140)
</p>
<p>Comparison of the right-hand sides immediately verifies the assertion.
</p>
<p>Remark: One can show that:
</p>
<p>lim
t&rarr;0
</p>
<p>
 (x, t) = (x) . (V.141)
</p>
<p>Hence, (V.138) is another representation of the free one-dimensional SEq with
</p>
<p>the given initial condition 
 (x, 0).</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 465
</p>
<p>V.6 Exercises, Chap. 6
</p>
<p>1. Show that for all |zi  in (6.5), |zi 2 = 1 holds.
2. Given a MZI with symmetrical beam splitters, calculate the final state with and
</p>
<p>without a blocker if the initial state is given by  |H +  |V .
3. Given an operator A with
</p>
<p>A |H = a |H ; A |V  = b |V  , (V.142)
</p>
<p>determine the explicit form of A.
</p>
<p>Solution:
</p>
<p>A = a |H H | + b |V  V | &sim;=
(
</p>
<p>a 0
</p>
<p>0 b
</p>
<p>)
. (V.143)
</p>
<p>For the example a = 1 and b = &minus;1, it follows that A = |H H | &minus; |V  V | &sim;=(
1 0
</p>
<p>0 &minus;1
</p>
<p>)
.
</p>
<p>4. Which eigenvalues can a unitary operator have?
</p>
<p>Solution: We start from
</p>
<p>U | =  | ; U &dagger; = U&minus;1;  | = 1. (V.144)
</p>
<p>It follows that
</p>
<p>|U &dagger; = &lowast; | or |U &dagger;U | = &lowast;  | . (V.145)
</p>
<p>Due to U &dagger;U = 1 and  | = 1, we obtain immediately
</p>
<p>||2 = 1. (V.146)
</p>
<p>Hence, the eigenvalues of unitary operators are on the unit circle and have the
</p>
<p>form = ei (and not just = &plusmn;1, as is often inferred incorrectly from ||2 = 1).
5. Circularly- and linearly-polarized states are connected by |r = 1&radic;
</p>
<p>2
|h + i&radic;
</p>
<p>2
|v
</p>
<p>and |l = 1&radic;
2
|h &minus; i&radic;
</p>
<p>2
|v. Show that this basis transformation is unitary (or that
</p>
<p>the transformation matrix is unitary).
</p>
<p>Solution: The transformation between linearly- and circularly-polarized light can
</p>
<p>be described by the matrix 1&radic;
2
</p>
<p>(
1 i
</p>
<p>1 &minus;i
</p>
<p>)
, which is unitary.
</p>
<p>6. Give the matrix representation of the operators T , S and S&prime; from (6.11), (6.12)
and (6.13) and their combinations T ST and T S&prime;T .
Solution: We take into account that we are in a two-dimensional space. Therefore
</p>
<p>we can represent the basis states |H and |V  e.g. by the vectors
(
</p>
<p>1
</p>
<p>0
</p>
<p>)
and
</p>
<p>(
0
</p>
<p>1
</p>
<p>)
;
</p>
<p>the product |V  H | is then</p>
<p/>
</div>
<div class="page"><p/>
<p>466 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>|V  H | &sim;=
(
</p>
<p>0
</p>
<p>1
</p>
<p>) (
1 0
</p>
<p>)
=
</p>
<p>(
0 0
</p>
<p>1 0
</p>
<p>)
. (V.147)
</p>
<p>We can read the action of the beam splitter from (6.11); it is
</p>
<p>T &sim;= 1 + i
2
</p>
<p>(
1 i
</p>
<p>i 1
</p>
<p>)
. (V.148)
</p>
<p>For the mirror, it follows analogously from (6.12):
</p>
<p>S &sim;=
(
</p>
<p>0 &minus;1
&minus;1 0
</p>
<p>)
(V.149)
</p>
<p>i.e. the well-known phase jump . Then it follows for the case without a blocker
</p>
<p>T ST &sim;=
(
</p>
<p>1 0
</p>
<p>0 1
</p>
<p>)
. (V.150)
</p>
<p>For the case with a blocker, we have to replace S by S&prime;:
</p>
<p>S&prime; &sim;=
(
</p>
<p>0 0
</p>
<p>&minus;1 0
</p>
<p>)
(V.151)
</p>
<p>and obtain
</p>
<p>T S&prime;T &sim;= 1
2
</p>
<p>(
1 i
</p>
<p>&minus;i 1
</p>
<p>)
. (V.152)
</p>
<p>7. Given the operator
</p>
<p>U = a |H H | + b |H V | + c |V  H | + d |V  V | &sim;=
(
</p>
<p>a b
</p>
<p>c d
</p>
<p>)
; (V.153)
</p>
<p>for which values of the coefficients is U is a unitary operator? In other words:
</p>
<p>How is the general two-dimensional unitary transformation formulated?
</p>
<p>Solution: The equations UU &dagger; = U &dagger;U = 1 have to be satisfied, which reads in
matrix representation:
</p>
<p>(
a b
</p>
<p>c d
</p>
<p>)(
a&lowast; c&lowast;
</p>
<p>b&lowast; d&lowast;
</p>
<p>)
=
</p>
<p>(
1 0
</p>
<p>0 1
</p>
<p>)
and
</p>
<p>(
a&lowast; c&lowast;
</p>
<p>b&lowast; d&lowast;
</p>
<p>)(
a b
</p>
<p>c d
</p>
<p>)
=
</p>
<p>(
1 0
</p>
<p>0 1
</p>
<p>)
. (V.154)
</p>
<p>This gives the equations
</p>
<p>|a|2 + |b|2 = 1; ac&lowast; + bd&lowast; = 0
ca&lowast; + db&lowast; = 0; |c|2 + |d|2 = 1
</p>
<p>(V.155)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 467
</p>
<p>and
|a|2 + |c|2 = 1; a&lowast;b + c&lowast;d = 0
b&lowast;a + d&lowast;c = 0; |b|2 + |d|2 = 1.
</p>
<p>(V.156)
</p>
<p>From the equations with the square values, it follows immediately that
</p>
<p>|b|2 = |c|2 and |a|2 = |d|2 (V.157)
</p>
<p>and we can use the ansatz
</p>
<p>a = Aei; b = Bei; c = Bei; d = Aei; A2 + B2 = 1. (V.158)
</p>
<p>Thus, the remaining two equations ac&lowast; + bd&lowast; = 0 and b&lowast;a + d&lowast;c = 0 give
</p>
<p>eie&minus;i + eie&minus;i = 0 and e&minus;iei + e&minus;iei = 0. (V.159)
</p>
<p>A closer look reveals that these two equations are identical; as the result, we have
</p>
<p>for example:
</p>
<p>ei = &minus;ei(&minus;+) or  =  &minus; +  + . (V.160)
</p>
<p>Thus, we have in matrix representation initially
</p>
<p>U &sim;=
(
</p>
<p>Aei Bei
</p>
<p>Bei &minus;Ae&minus;i(&minus;&minus;)
)
; A2 + B2 = 1. (V.161)
</p>
<p>This result may be written in a structurally simpler manner. To this end, we put
</p>
<p>ei(++)/2 outside the brackets:
</p>
<p>U &sim;= ei(++)/2

 Ae
</p>
<p>i
(
&minus; ++
</p>
<p>2
</p>
<p>)
</p>
<p>Bei
&minus;&minus;
</p>
<p>2
</p>
<p>&minus;Be&minus;i &minus;&minus;2 Ae&minus;i
(
&minus; ++
</p>
<p>2
</p>
<p>)
</p>
<p>
 ; A2 + B2 = 1, (V.162)
</p>
<p>or, more compactly (with p = Aei
(
&minus; ++
</p>
<p>2
</p>
<p>)
</p>
<p>etc.):
</p>
<p>U &sim;= ei
(
</p>
<p>p q
</p>
<p>&minus;q&lowast; p&lowast;
)
; |p|2 + |q|2 = 1; p, q &isin; C;  &isin; R (V.163)
</p>
<p>as a general form of a two-dimensional unitary transformation.159
</p>
<p>As an important special case, we obtain the real rotation
</p>
<p>Urotation &sim;=
(
</p>
<p>cos sin 
</p>
<p>&minus; sin  cos
</p>
<p>)
; p = cos; q = sin ;  = 0. (V.164)
</p>
<p>159On extension with ei(+)/2, we find the equivalent representation ei
(
</p>
<p>p q
</p>
<p>q&lowast; &minus;p&lowast;
)
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>468 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>The symmetrical beam splitter follows with 1+i
2
</p>
<p>= 1&radic;
2
ei
</p>
<p>
4 ,
</p>
<p>Ubeam splitter &sim;=
1 + i
</p>
<p>2
</p>
<p>(
1 i
</p>
<p>&minus;i 1
</p>
<p>)
; p = 1&radic;
</p>
<p>2
; q = i&radic;
</p>
<p>2
;  = 
</p>
<p>4
. (V.165)
</p>
<p>The Hadamard matrix is found as
</p>
<p>UHadamard &sim;=
1&radic;
2
</p>
<p>(
1 1
</p>
<p>1 &minus;1
</p>
<p>)
; p = 1
</p>
<p>i
&radic;
</p>
<p>2
; q = 1
</p>
<p>i
&radic;
</p>
<p>2
;  = 
</p>
<p>2
. (V.166)
</p>
<p>8. Given a MZI without a blocker and with asymmetrical beam splitters (transmit-
</p>
<p>tance 	= reflectance), determine the properties required of the beam splitters in
order that a beam entering horizontally activates only detector 1, while detector
</p>
<p>2 remains dark.
</p>
<p>Solution: We can represent an asymmetrical beam splitter as
</p>
<p>T &sim;= (+ i)
(
 i
</p>
<p>i 
</p>
<p>)
; , &isin; R,&gt; 0; 2 + 2 = 1 (V.167)
</p>
<p>where  is the amplitude transmission coefficient and  the reflection coefficient
</p>
<p>of the beam splitter. The factor i in front of  denotes the relative phase shift
</p>
<p>between transmitted and reflected beams. T is unitary; cf. (V.163). The action of
</p>
<p>the whole Mach&ndash;Zehnder interferometer can be described by
</p>
<p>T2ST1 = (2 + i2)
(
2 i2
i2 2
</p>
<p>)(
0 &minus;1
&minus;1 0
</p>
<p>)
(1 + i1)
</p>
<p>(
1 i1
i1 1
</p>
<p>)
. (V.168)
</p>
<p>This term can be evaluated to give
</p>
<p>T2ST1 = &minus; (2 + i2) (1 + i1)
(
</p>
<p>i12 + i21 12 &minus; 12
12 &minus; 12 i12 + i21
</p>
<p>)
.
</p>
<p>(V.169)
</p>
<p>Hence, if we want detector 1 to always respond and detector 2 never, we must set
</p>
<p>12 = 12. It follows that
</p>
<p>2 =
12
</p>
<p>1
; 2 =
</p>
<p>12
</p>
<p>1
. (V.170)
</p>
<p>With 2i + 2i = 1, this yields
</p>
<p>22 + 22 = 22 +
21
</p>
<p>2
2
</p>
<p>21
= 22
</p>
<p>21
</p>
<p>(
21 + 21
</p>
<p>)
= 22
</p>
<p>21
= 1
</p>
<p>22 + 22 =
21
</p>
<p>2
2
</p>
<p>21
+ 22 =
</p>
<p>22
21
</p>
<p>(
21 + 21
</p>
<p>)
= 22
</p>
<p>21
= 1,
</p>
<p>(V.171)
</p>
<p>or, due to i ,i &gt; 0, finally</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 469
</p>
<p>2 = 1; 2 = 1. (V.172)
</p>
<p>In other words, the transmission and reflection factors of the second beam splitter
</p>
<p>must be reversed relative to the first beam splitter:
</p>
<p>T1 &sim;= (1 + i1)
(
1 i1
i1 1
</p>
<p>)
; T2 &sim;= (1 + i1)
</p>
<p>(
1 i1
i1 1
</p>
<p>)
. (V.173)
</p>
<p>For the total action of the Mach&ndash;Zehnder interferometer, it then follows:
</p>
<p>T2ST1 &sim;= &minus;i (1 + i1) (1 + i1)
(
</p>
<p>1 0
</p>
<p>0 1
</p>
<p>)
</p>
<p>= (1 + i1) (1 &minus; i1)
(
</p>
<p>1 0
</p>
<p>0 1
</p>
<p>)
=
</p>
<p>(
1 0
</p>
<p>0 1
</p>
<p>)
,
</p>
<p>(V.174)
</p>
<p>as expected. See also J. Pade and L. Polley, &lsquo;Wechselwirkungsfreie Quantenmes-
</p>
<p>sung&rsquo; (Interaction-free quantum measurement, in German), Physik in der Schule
</p>
<p>38/5 (2000) 343.
</p>
<p>V.7 Exercises, Chap. 7
</p>
<p>1. Show for  = | (x, t)|2 that:
&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p> (x, t) dx = 1 &forall; t. (V.175)
</p>
<p>Here we assume that (i) the potential is real, and (ii) 
 &sim;
x&rarr;&infin;
</p>
<p>xa , with a &lt; &minus; 1
2
.
</p>
<p>2. Infinite potential well: Given the wavefunctions
</p>
<p>(a) 
 (x, t) = e&minus;in t
&radic;
</p>
<p>2
a
</p>
<p>sin n
a
</p>
<p>x and
</p>
<p>(b) 
 (x, t) = cne&minus;in t
&radic;
</p>
<p>2
a
</p>
<p>sin n
a
</p>
<p>x + cme&minus;im t
&radic;
</p>
<p>2
a
</p>
<p>sin m
a
</p>
<p>x ,
</p>
<p>calculate for both cases the probability of finding the quantum object in the
</p>
<p>interval (x1, x2)
</p>
<p>wquantum mechanicsx1,x2 =
x2&int;
</p>
<p>x1
</p>
<p>
&lowast; (x, t)
 (x, t) dx . (V.176)
</p>
<p>3. Given the SEq i = H with a real potential, derive from the continuity
equation constructively (i.e. not just proving by insertion) that j is given by</p>
<p/>
</div>
<div class="page"><p/>
<p>470 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>j = 
2mi
</p>
<p>(
&lowast;&nabla; &minus; &nabla;&lowast;
</p>
<p>)
. (V.177)
</p>
<p>Solution: Since the potential is real, we have H = H&lowast; and therefore
</p>
<p>i = H; &minus; i&lowast; = H&lowast;. (V.178)
</p>
<p>Using the continuity equation, we can write
</p>
<p>&nabla;j = &minus; = &minus;&part;t&lowast; = &minus;&lowast; &minus; &lowast;. (V.179)
</p>
<p>On the right-hand side, we insert (V.178):
</p>
<p>&nabla;j = &minus;
(
&minus; H
</p>
<p>&lowast;
</p>
<p>i
</p>
<p>)
 &minus; &lowast;
</p>
<p>(
H
</p>
<p>i
</p>
<p>)
= i
</p>
<p>
</p>
<p>(
&lowast;H &minus; H&lowast;
</p>
<p>)
. (V.180)
</p>
<p>Since the potential is real, the potential terms cancel and we have:
</p>
<p>&nabla;j = 
2mi
</p>
<p>(
&lowast;&nabla;2 &minus; &nabla;2&lowast;
</p>
<p>)
. (V.181)
</p>
<p>On the right-hand side, we insert &plusmn;&nabla;&lowast; &middot;&nabla; and obtain
</p>
<p>&nabla;j = 
2mi
</p>
<p>(
&lowast;&nabla;2 +&nabla;&lowast; &middot;&nabla; &minus; &nabla;2&lowast; &minus;&nabla; &middot;&nabla;&lowast;
</p>
<p>)
</p>
<p>= 
2mi
</p>
<p>(&nabla; (&lowast;&nabla;)&minus;&nabla; (&nabla;&lowast;)) = 
2mi
</p>
<p>&nabla; (&lowast;&nabla; &minus; &nabla;&lowast;) .
(V.182)
</p>
<p>Comparing the right and left sides yields
</p>
<p>&nabla;
</p>
<p>(
j&minus; 
</p>
<p>2mi
</p>
<p>(
&lowast;&nabla; &minus; &nabla;&lowast;
</p>
<p>))
= 0, (V.183)
</p>
<p>and this is the desired result. (Strictly, it follows from the last equation due to
</p>
<p>&nabla; (&nabla; &times; A) = 0, however, that j = 
2mi
</p>
<p>(&lowast;&nabla; &minus; &nabla;&lowast;) + &nabla; &times; A, where A is
an arbitrary field.)
</p>
<p>4. Calculate j (one-dimensional) for  = Aex and  = Aeix , with  &isin; R and
A &isin; C.
</p>
<p>5. Calculate j (r, t) for 
 (r, t) = Aei(kr&minus;t).
6. Given a modification of the infinite potential well, namely the potential
</p>
<p>V (x) =
{
</p>
<p>iW for 0 &lt; x &lt; a
</p>
<p>&infin; otherwise ; W &isin; R, (V.184)
</p>
<p>calculate the energy spectrum and show that the norm of the (time-dependent)
</p>
<p>total wavefunction is independent of time only for W = 0.
Solution: The stationary SEq including boundary conditions reads</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 471
</p>
<p>E = &minus; 
2
</p>
<p>2m
&prime;&prime; + iW;  (0) =  (a) = 0; (V.185)
</p>
<p>here, we have to formulate a complex energy, i.e.
</p>
<p>E = ER + i E I . (V.186)
</p>
<p>With
</p>
<p>2 = 2m
2
</p>
<p>(E &minus; iW ) , (V.187)
</p>
<p>we find as solution
</p>
<p> = Aeix + Be&minus;ix . (V.188)
</p>
<p>The boundary condition at x = 0 yields B = &minus;A, the one at x = a leads due to
A 	= 0 to 0 = eia &minus; e&minus;ia or e2ia = 1. With the ansatz  = R + iI , it follows
that
</p>
<p>e2i(R+iI )a = e2iRae&minus;2I a = 1. (V.189)
</p>
<p>This gives immediately sin 2Ra = 0, and therefore
</p>
<p>Ra = n and I = 0. (V.190)
</p>
<p>From (V.187), we can conclude that E I = W (due to I = 0). For the real part
of the energy, we obtain the well-known relation
</p>
<p>ER,n =
22R
2m
</p>
<p>= 
2
</p>
<p>2m
</p>
<p>(n
a
</p>
<p>)2
; n = 1, 2, . . . (V.191)
</p>
<p>so that the energies are given by
</p>
<p>En = ER,n + iW. (V.192)
</p>
<p>We insert this into the total wavefunction
</p>
<p> (x, t) =
&sum;
</p>
<p>n
</p>
<p>cnn (x) e
&minus; i t
</p>
<p>
En (V.193)
</p>
<p>and obtain
</p>
<p> (x, t) = e t W
&sum;
</p>
<p>n
</p>
<p>cnn (x) e
&minus; i t
</p>
<p>
ER,n . (V.194)
</p>
<p>Depending on the sign of W 	= 0,  (x, t) tends for t &rarr; &plusmn;&infin; to 0 or to &infin;.
Explicitly, it holds that:</p>
<p/>
</div>
<div class="page"><p/>
<p>472 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>&int;
| (x, t)|2 dx = e 2t W
</p>
<p>&int; &sum;
</p>
<p>n
</p>
<p>c&lowast;n
&lowast;
n (x) e
</p>
<p>i t

</p>
<p>ER,n
&sum;
</p>
<p>m
</p>
<p>cmm (x) e
&minus; i t
</p>
<p>
ER,m dx
</p>
<p>= e 2t W
&sum;
</p>
<p>n,m
</p>
<p>c&lowast;ncme
i t

</p>
<p>ER,n e&minus;
i t

</p>
<p>ER,m
&int;
&lowast;n (x)m (x) dx .
</p>
<p>(V.195)
</p>
<p>Due to the orthonormality of the eigenfunctions n (x), it follows that
</p>
<p>&int;
| (x, t)|2 dx = e 2t W
</p>
<p>&sum;
</p>
<p>n
</p>
<p>|cn|2 . (V.196)
</p>
<p>As expected, we cannot obtain
&int;
| (x, t)|2 dx = 1 &forall; t .
</p>
<p>V.8 Exercises, Chap. 8
</p>
<p>1. Given that |1 1| + |2 2| = 1. Show: |e e| +

</p>
<p>&rang; &lang;

 = 1.
</p>
<p>2. Show that the matrices
</p>
<p>

</p>
<p>c 0 se&minus;i
</p>
<p>0 1 0
</p>
<p>&minus;sei 0 c
</p>
<p>
 and
</p>
<p>

</p>
<p>1 0 0
</p>
<p>0 c s
</p>
<p>0 &minus;s c
</p>
<p>
 with  &isin; R are
</p>
<p>unitary. The abbreviations s and c stand for sin and cos.
</p>
<p>3. Show that the product of two unitary matrices is also unitary.
</p>
<p>4. Is the beam splitter operator T from Chap. 6,
</p>
<p>T = 1 + i
2
</p>
<p>[1 + i |H V | + i |V  H |] , (V.197)
</p>
<p>a Hermitian, a unitary or a projection operator? {|H , |V } is a CONS.
5. Given A =
</p>
<p>(
1 i
</p>
<p>&minus;i 1
</p>
<p>)
:
</p>
<p>(a) Show that A is Hermitian, but not unitary.
</p>
<p>Solution:
</p>
<p>A&dagger; =
(
</p>
<p>1 i
</p>
<p>&minus;i 1
</p>
<p>)
= A; A&dagger; A = A2 = 2A 	=
</p>
<p>(
1 0
</p>
<p>0 1
</p>
<p>)
. (V.198)
</p>
<p>(b) Calculate ecA.
</p>
<p>Solution: Due to A2 = 2A, it follows that An = 2n&minus;1 A and therefore,
</p>
<p>ecA = 1 +
&sum;
</p>
<p>n=1
</p>
<p>cn
</p>
<p>n! A
n = 1 +
</p>
<p>&sum;
</p>
<p>n=1
</p>
<p>cn
</p>
<p>n! 2
n&minus;1 A. (V.199)
</p>
<p>This means that</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 473
</p>
<p>ecA = 1 + 1
2
</p>
<p>&sum;
</p>
<p>n=1
</p>
<p>cn
</p>
<p>n! 2
n A = 1 +
</p>
<p>(
e2c
</p>
<p>2
&minus; 1
</p>
<p>)
A. (V.200)
</p>
<p>6. Given the operators
</p>
<p>L1 =
|v (u| + w|)+ (|u + |w) v|&radic;
</p>
<p>2
</p>
<p>L2 =
&minus; |v (u| &minus; w|)+ (|u &minus; |w) v|
</p>
<p>i
&radic;
</p>
<p>2
</p>
<p>L3 = |u u| &minus; |w w| .
</p>
<p>(V.201)
</p>
<p>(a) Are these Hermitian, unitary or projection operators?
</p>
<p>Solution: We can see directly that the operators are Hermitian, e.g.
</p>
<p>L
&dagger;
2 =
</p>
<p>&minus; (|u &minus; |w) v| + |v (u| &minus; w|)
&minus;i
</p>
<p>&radic;
2
</p>
<p>= L2. (V.202)
</p>
<p>But they are neither unitary nor projective; we have e.g. for L3:
</p>
<p>L
&dagger;
3L3 = L23 = |u u| + |w w| (V.203)
</p>
<p>and this term is neither L3 nor the unity operator.
</p>
<p>(b) Calculate [L1, L2].
</p>
<p>Solution: We calculate first the individual terms, i.e.
</p>
<p>2i L1L2 = &minus; (|u + |w) (u| &minus; w|)
2i L2 L1 = (|u &minus; |w) (u| + w|) . (V.204)
</p>
<p>It follows that
</p>
<p>[L1, L2] =
&minus; |u u| + |u w| &minus; |w u| + |w w|
</p>
<p>2i
</p>
<p>&minus;|u u| + |u w| &minus; |w u| &minus; |w w|
2i
</p>
<p>= &minus;2 |u u| + 2 |w w|
2i
</p>
<p>= i (|u u| &minus; |w w|) = i L3.
</p>
<p>(V.205)
</p>
<p>7. Show that the time evolution
</p>
<p>|(t) = &minus; sin  |1 e&minus;i1t + cos |2 e&minus;i2t (V.206)
</p>
<p>is unitary.
</p>
<p>8. Determine explicitly e |(t) in (8.8), and
&lang;
 |(t).
</p>
<p>Solution: With (8.1) or (8.2), it holds that</p>
<p/>
</div>
<div class="page"><p/>
<p>474 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>e |(t) = &minus; sin  e |1 e&minus;i1t + cos e |2 e&minus;i2t
</p>
<p>= &minus; sin  cose&minus;i1t + cos sin e&minus;i2t
(V.207)
</p>
<p>as well as
</p>
<p>&lang;
 |(t) = &minus; sin 
</p>
<p>&lang;
 |1 e&minus;i1t + cos
</p>
<p>&lang;
 |2 e&minus;i2t
</p>
<p>= sin2 e&minus;i1t + cos2 e&minus;i2t .
(V.208)
</p>
<p>9. Determine explicitly pe in (8.9), and p.
</p>
<p>Solution: For pe, we consider first
</p>
<p>e&minus;i1t &minus; e&minus;i2t = e&minus;i 1+22 t
[
e&minus;i
</p>
<p>1&minus;2
2
</p>
<p>t &minus; ei 1&minus;22 t
]
= 2ie&minus;i 1+22 t sin
</p>
<p>(

</p>
<p>2
t
</p>
<p>)
.
</p>
<p>(V.209)
</p>
<p>It follows that
</p>
<p>pe = |e |(t)|2 = sin2  cos2  &middot; 4 sin2
(

</p>
<p>2
t
</p>
<p>)
= sin2 2 &middot; sin2
</p>
<p>(

</p>
<p>2
t
</p>
<p>)
.
</p>
<p>(V.210)
</p>
<p>For p, we use pe + p = 1 and obtain
</p>
<p>p = 1 &minus; sin2 2 &middot; sin2
(

</p>
<p>2
t
</p>
<p>)
. (V.211)
</p>
<p>If we want to calculate p explicitly, we start from
</p>
<p>p =
&lang; |(t)
</p>
<p>2 =
sin2 e&minus;i1t + cos2 e&minus;i2t
</p>
<p>2 . (V.212)
</p>
<p>Due to
(
sin2 + cos2 
</p>
<p>)2 = 1, this gives
</p>
<p>p = sin4 + 2 sin2  cos2  cos (t)+ cos4 
= 1 + 2 sin2  cos2  [cos (t)&minus; 1] . (V.213)
</p>
<p>We transform the square brackets by means of cos 2x = cos2 x &minus; sin2 x =
1 &minus; 2 sin2 x and obtain
</p>
<p>p = 1 &minus; 4 sin2  cos2  sin2
(

</p>
<p>2
t
</p>
<p>)
= 1 &minus; sin2 2 &middot; sin2
</p>
<p>(

</p>
<p>2
t
</p>
<p>)
. (V.214)
</p>
<p>10. Prove (8.10); find an approximation forE in the case of very small rest masses.
</p>
<p>Solution:</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 475
</p>
<p> = E = E1 &minus; E2 =
&radic;
</p>
<p>p2c2 + m21c4 &minus;
&radic;
</p>
<p>p2c2 + m22c4
</p>
<p>= pc
[&radic;
</p>
<p>1 + m
2
1c
</p>
<p>2
</p>
<p>p2
&minus;
</p>
<p>&radic;
1 + m
</p>
<p>2
2c
</p>
<p>2
</p>
<p>p2
</p>
<p>]
&asymp; pc
</p>
<p>[
1 + m
</p>
<p>2
1c
</p>
<p>2
</p>
<p>2p2
&minus; 1 &minus; m
</p>
<p>2
2c
</p>
<p>2
</p>
<p>2p2
</p>
<p>]
</p>
<p>= c
4
</p>
<p>2pc
</p>
<p>(
m21 &minus; m22
</p>
<p>)
:= c
</p>
<p>4m2
</p>
<p>2pc
.
</p>
<p>(V.215)
</p>
<p>11. Given the state
| (t) =
</p>
<p>&sum;
</p>
<p>n
</p>
<p>cn |n e&minus;i En t/ (V.216)
</p>
<p>with the initial condition | (0). {|n} is a CONS. How are the constants cn
related to the initial conditions?
</p>
<p>Solution: Since {|n} is a CONS, we have
</p>
<p>m | (0) =
&sum;
</p>
<p>n
</p>
<p>cn m |n =
&sum;
</p>
<p>n
</p>
<p>cnmn = cm (V.217)
</p>
<p>and the state reads
</p>
<p>| (t) =
&sum;
</p>
<p>n
</p>
<p>n | (0) |n e&minus;i En t/ =
&sum;
</p>
<p>n
</p>
<p>|n n| e&minus;i En t/ | (0) .
</p>
<p>(V.218)
</p>
<p>12. Given two CONS {|i } and {|i }. A quantum system is in the superposition
|z = &sum;i di |i .
(a) Calculate the probability of measuring the quantum system in the state |k.
</p>
<p>Solution:
</p>
<p>pk = |k |z|2 =

&sum;
</p>
<p>i
</p>
<p>di k |i 

</p>
<p>2
</p>
<p>. (V.219)
</p>
<p>(b) Show that
&sum;
</p>
<p>k pk = 1.
Solution: &sum;
</p>
<p>k
</p>
<p>pk =
&sum;
</p>
<p>k
</p>
<p>z |k k |z = z |z = 1. (V.220)
</p>
<p>13. Given the model system
</p>
<p>i
d
</p>
<p>dt
| (t) = H | (t) with H = 1 + Ay; A &gt; 0, (V.221)
</p>
<p>where y is the y-Pauli matrix. (For the sake of simplicity, we do not distinguish
</p>
<p>between = and &sim;=.)
(a) Determine the eigenvalues and eigenvectors of H .
</p>
<p>Solution: The eigenvalue problem reads</p>
<p/>
</div>
<div class="page"><p/>
<p>476 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>H | = E | or
(
</p>
<p>1 &minus;i A
i A 1
</p>
<p>)(
a
</p>
<p>b
</p>
<p>)
= E
</p>
<p>(
a
</p>
<p>b
</p>
<p>)
, (V.222)
</p>
<p>or explicitly
</p>
<p>a &minus; i Ab = Ea
i Aa + b = Eb or
</p>
<p>a (1 &minus; E) = i Ab
b (1 &minus; E) = &minus;i Aa. (V.223)
</p>
<p>It follows that
</p>
<p>b = &minus;a E &minus; 1
i A
</p>
<p>. (V.224)
</p>
<p>By inserting, we obtain (a 	= 0)
</p>
<p>a
1 &minus; E
</p>
<p>i A
(1 &minus; E) = &minus;i Aa or (E &minus; 1)2 = A2. (V.225)
</p>
<p>Hence, the eigenvalues read
</p>
<p>E1 = 1 + A; E2 = 1 &minus; A. (V.226)
</p>
<p>with (V.224), we obtain first for the associated eigenvectors
</p>
<p>|1 =
(
</p>
<p>a
</p>
<p>ia
</p>
<p>)
; |2 =
</p>
<p>(
a
</p>
<p>&minus;ia
</p>
<p>)
, (V.227)
</p>
<p>where a is arbitrary (due to the fact that the eigenvalue problem (V.222) is
</p>
<p>linear and a multiple of a solution is also a solution). We can fix a by an
</p>
<p>additional requirement; usually this is the normalization. If we require that
</p>
<p>|1 be normalized, it follows that
</p>
<p>1 |1 =
(
</p>
<p>a&lowast; &minus;ia&lowast;
) ( a
</p>
<p>ia
</p>
<p>)
= 2aa&lowast; != 1. (V.228)
</p>
<p>The simplest choice for a is 1/
&radic;
</p>
<p>2. Analogous statements hold for |2, and
we obtain finally the normalized eigenvectors (which are clearly orthogonal,
</p>
<p>as required):
</p>
<p>|1 =
1&radic;
2
</p>
<p>(
1
</p>
<p>i
</p>
<p>)
; |2 =
</p>
<p>1&radic;
2
</p>
<p>(
1
</p>
<p>&minus;i
</p>
<p>)
. (V.229)
</p>
<p>(b) How does the general expression | (t) read for a time-dependent state?
Solution: (1) Long version: The general expression | (t) for a time-
dependent state follows as a solution of the system
</p>
<p>i
d
</p>
<p>dt
| = H | or i d
</p>
<p>dt
</p>
<p>(
f
</p>
<p>g
</p>
<p>)
=
</p>
<p>(
1 &minus;i A
</p>
<p>i A 1
</p>
<p>)(
f
</p>
<p>g
</p>
<p>)
. (V.230)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 477
</p>
<p>The exponential ansatz (it is a differential equation with constant coeffi-
</p>
<p>cients): (
f
</p>
<p>g
</p>
<p>)
=
</p>
<p>(
F
</p>
<p>G
</p>
<p>)
et ; F,G constant (V.231)
</p>
<p>leads to
</p>
<p>i
</p>
<p>(
F
</p>
<p>G
</p>
<p>)
=
</p>
<p>(
1 &minus;i A
</p>
<p>i A 1
</p>
<p>)(
F
</p>
<p>G
</p>
<p>)
, (V.232)
</p>
<p>and by comparison with the eigenvalue problem just treated, for  the two
</p>
<p>solutions follow immediately:
</p>
<p>1,2 =
E1,2
</p>
<p>i
.
</p>
<p>Hence, the general state is
</p>
<p>| (t) = c1 |1 e&minus;i E1t/ + c2 |2 e&minus;i E2t/ (V.233)
</p>
<p>where the ci are integration constants which are determined by the initial
</p>
<p>conditions. In explicit form, this reads
</p>
<p>| (t) = e
&minus;i t/
&radic;
</p>
<p>2
</p>
<p>[
c1
</p>
<p>(
1
</p>
<p>i
</p>
<p>)
e&minus;i At/ + c2
</p>
<p>(
1
</p>
<p>&minus;i
</p>
<p>)
ei At/
</p>
<p>]
. (V.234)
</p>
<p>(2) Short version: Since {|1 , |2} is a CONS, each state can be repre-
sented at time t = 0 as a linear combination:
</p>
<p>| (0) = c1 |1 + c2 |2 . (V.235)
</p>
<p>Since {|1 , |2} are states with sharp energies, the time evolution is given
by
</p>
<p>| (t) = c1 |1 e&minus;i E1t/ + c2 |2 e&minus;i E2t/. (V.236)
</p>
<p>(c) How is | (t) expressed for the initial state | (t = 0) =
(
</p>
<p>1
</p>
<p>0
</p>
<p>)
?
</p>
<p>Solution: With the given initial condition, we have
</p>
<p>(
1
</p>
<p>0
</p>
<p>)
= 1&radic;
</p>
<p>2
</p>
<p>[
c1
</p>
<p>(
1
</p>
<p>i
</p>
<p>)
+ c2
</p>
<p>(
1
</p>
<p>&minus;i
</p>
<p>)]
, (V.237)
</p>
<p>and this leads immediately to
</p>
<p>c2 = c1; c1 =
1&radic;
2
. (V.238)</p>
<p/>
</div>
<div class="page"><p/>
<p>478 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>Hence, the total state for this initial state reads
</p>
<p>| (t) = e&minus;i t/&radic;
2
</p>
<p>[
1&radic;
2
</p>
<p>(
1
</p>
<p>i
</p>
<p>)
e&minus;i At/ + 1&radic;
</p>
<p>2
</p>
<p>(
1
</p>
<p>&minus;i
</p>
<p>)
ei At/
</p>
<p>]
</p>
<p>= e&minus;i t/


</p>
<p>cos
At
</p>
<p>
</p>
<p>sin
At
</p>
<p>
</p>
<p>
 .
</p>
<p>(V.239)
</p>
<p>(d) Assume that we measure | (t) from part c. With which probability will
we find the state | =
</p>
<p>(
1
</p>
<p>0
</p>
<p>)
(i.e. the initial state)?
</p>
<p>Solution: The probability follows as | ||2, i.e.
</p>
<p>| ||2 =

(
</p>
<p>1 0
) ( cos At
</p>
<p>
</p>
<p>sin At

</p>
<p>)
2
</p>
<p>= cos2 At

. (V.240)
</p>
<p>After the measurement, the state is |.
</p>
<p>V.9 Exercises, Chap. 9
</p>
<p>1. Given a Hermitian operator A and the eigenvalue problem An = ann , n =
1, 2, . . ., show that:
</p>
<p>(a) The eigenvalues are real.
</p>
<p>(b) The eigenfunctions are pairwise orthogonal. Here, it is assumed that the
</p>
<p>eigenvalues are nondegenerate.
</p>
<p>2. Show that the expectation value of a Hermitian operator is real.
</p>
<p>Solution: Due to the Hermiticity,
&int;
&lowast;A =
</p>
<p>&int;
(A)&lowast;  holds:
</p>
<p>A&lowast; =
(&int;
</p>
<p>&lowast;A
</p>
<p>)&lowast;
=
</p>
<p>&int;
 (A)&lowast; =
</p>
<p>&int;
(A)&lowast;  =
</p>
<p>&int;
&lowast;A = A .
</p>
<p>(V.241)
</p>
<p>3. Show that &int;

&lowast;1 A
2dV =
</p>
<p>&int;
(A
1)
</p>
<p>&lowast; 
2dV (V.242)
</p>
<p>holds for the operators r, p, H . Restrict the discussion to the one-dimensional
</p>
<p>case. Which conditions must the wavefunctions satisfy?
</p>
<p>Solution: In one dimension, it holds for a Hermitian operator A that:
</p>
<p>&int;
f &lowast;(x)Ag(x)dx =
</p>
<p>&int;
(A f (x))&lowast; g(x)dx . (V.243)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 479
</p>
<p>Evidently, this equation is satisfied for A = x .
For A = p = 
</p>
<p>i
d
</p>
<p>dx
, it follows with partial integration:
</p>
<p>&int;
f &lowast;(x)
</p>
<p>
</p>
<p>i
</p>
<p>d
</p>
<p>dx
g(x)dx = 
</p>
<p>i
</p>
<p>&int;
f &lowast;g&prime;dx
</p>
<p>= 
i
</p>
<p>[
( f &lowast;g)&infin;&minus;&infin; &minus;
</p>
<p>&int;
f &lowast;
</p>
<p>&prime;
gdx
</p>
<p>]
=
</p>
<p>&int; (
i
</p>
<p>d
</p>
<p>dx
f
</p>
<p>)&lowast;
gdx
</p>
<p>(V.244)
</p>
<p>where ( f &lowast;g)&infin;&minus;&infin; = 0 must be fulfilled.
For A = H , we need to worry only about the space derivatives (assuming a real
potential). With partial integration, we find:
</p>
<p>&int;
f &lowast;(x)g&prime;&prime;(x)dx =
</p>
<p>(
f &lowast;g&prime;
</p>
<p>)&infin;
&minus;&infin; &minus;
</p>
<p>&int;
f &lowast;&prime;g&prime;dx
</p>
<p>=
(
</p>
<p>f &lowast;g&prime;
)&infin;
&minus;&infin; &minus;
</p>
<p>[(
f &lowast;&prime;g
</p>
<p>)&infin;
&minus;&infin; &minus;
</p>
<p>&int;
f &lowast;&prime;&prime;gdx
</p>
<p>]
=
</p>
<p>&int;
f &lowast;&prime;&prime;gdx
</p>
<p>(V.245)
</p>
<p>where
(
</p>
<p>f &lowast;g&prime;
)&infin;
&minus;&infin; &minus;
</p>
<p>(
f &lowast;&prime;g
</p>
<p>)&infin;
&minus;&infin; = 0 must be fulfilled.
</p>
<p>4. Show that for the infinite potential well (between 0 and a), x = a
2
.
</p>
<p>Solution: With the eigenfunctions n =
&radic;
</p>
<p>2
a
</p>
<p>sin
(
</p>
<p>n
a
</p>
<p>x
)
; n = 1, 2, ..., we have:
</p>
<p>x = 2
a
</p>
<p>a&int;
</p>
<p>0
</p>
<p>x sin2
(n
</p>
<p>a
x
)
</p>
<p>dx = 2
a
</p>
<p>a&int;
</p>
<p>0
</p>
<p>x
1 &minus; cos
</p>
<p>(
2n
</p>
<p>a
x
)
</p>
<p>2
dx . (V.246)
</p>
<p>Because of
</p>
<p>a&int;
0
</p>
<p>x cos
</p>
<p>(
2nx
</p>
<p>a
</p>
<p>)
dx = a
</p>
<p>2n
</p>
<p>&part;
</p>
<p>&part;n
</p>
<p>a&int;
0
</p>
<p>sin
</p>
<p>(
2nx
</p>
<p>a
</p>
<p>)
dx
</p>
<p>= &minus; a
2n
</p>
<p>&part;
</p>
<p>&part;n
</p>
<p>(
cos
</p>
<p>(
2na
</p>
<p>a
</p>
<p>)
&minus; 1
</p>
<p>)
= 0,
</p>
<p>(V.247)
</p>
<p>it follows that
</p>
<p>x = 2
a
</p>
<p>a&int;
</p>
<p>0
</p>
<p>x
</p>
<p>2
dx = 2
</p>
<p>a
</p>
<p>a2
</p>
<p>4
= a
</p>
<p>2
. (V.248)
</p>
<p>5. Given the infinite potential well with walls at x = 0 and x = a; we consider the
state
</p>
<p>
 (x, t) =
&radic;
</p>
<p>2
</p>
<p>a
sin
</p>
<p>(n
a
</p>
<p>x
)
</p>
<p>e&minus;in t . (V.249)
</p>
<p>(a) Determine the position uncertainty x .
</p>
<p>Solution: We have
</p>
<p>(x)2 =
&lang;
x2
&rang;
&minus; x2 (V.250)</p>
<p/>
</div>
<div class="page"><p/>
<p>480 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>and thus it follows that
</p>
<p>(x)2 = 2
a
</p>
<p>a&int;
</p>
<p>0
</p>
<p>sin2
(n
</p>
<p>a
x
)
</p>
<p>x2dx &minus;
</p>
<p>
2
</p>
<p>a
</p>
<p>a&int;
</p>
<p>0
</p>
<p>sin2
(n
</p>
<p>a
x
)
</p>
<p>xdx
</p>
<p>

</p>
<p>2
</p>
<p>. (V.251)
</p>
<p>With
</p>
<p>a&int;
</p>
<p>0
</p>
<p>sin2
(n
</p>
<p>a
x
)
</p>
<p>x2dx = a
3
</p>
<p>12
</p>
<p>2n22 &minus; 3
n22
</p>
<p>;
a&int;
</p>
<p>0
</p>
<p>sin2
(n
</p>
<p>a
x
)
</p>
<p>xdx = a
2
</p>
<p>4
</p>
<p>(V.252)
</p>
<p>we obtain
</p>
<p>(x)2 = a
2
</p>
<p>6
</p>
<p>2n22 &minus; 3
n22
</p>
<p>&minus; a
2
</p>
<p>4
= a
</p>
<p>2
</p>
<p>4
</p>
<p>[
n22 &minus; 6
</p>
<p>3n22
</p>
<p>]
(V.253)
</p>
<p>or
</p>
<p>x = a
2
</p>
<p>&radic;
n22 &minus; 6
</p>
<p>3n22
&rarr;
</p>
<p>n&rarr;&infin;
a
</p>
<p>2
&radic;
</p>
<p>3
&asymp; 0.289a. (V.254)
</p>
<p>(b) Determine the momentum uncertainty p.
</p>
<p>Solution: We have
</p>
<p>(p)2 =
&lang;
p2
&rang;
&minus; p2 . (V.255)
</p>
<p>We first calculate p:
</p>
<p>p = 2
ai
</p>
<p>a&int;
</p>
<p>0
</p>
<p>sin
(
</p>
<p>n
a
</p>
<p>x
)
</p>
<p>d
dx
</p>
<p>sin
(
</p>
<p>n
a
</p>
<p>x
)
</p>
<p>dx = 
ai
</p>
<p>a&int;
</p>
<p>0
</p>
<p>d
dx
</p>
<p>sin2
(
</p>
<p>n
a
</p>
<p>x
)
</p>
<p>dx
</p>
<p>= 
ai
</p>
<p>[
sin2
</p>
<p>(
n
a
</p>
<p>x
)]a
</p>
<p>0
= 0.
</p>
<p>(V.256)
</p>
<p>For
&lang;
p2
&rang;
, we have
</p>
<p>&lang;
p2
&rang;
= &minus;2
</p>
<p>2
</p>
<p>a
</p>
<p>a&int;
</p>
<p>0
</p>
<p>sin
(n
</p>
<p>a
x
) d2
</p>
<p>dx2
sin
</p>
<p>(n
a
</p>
<p>x
)
</p>
<p>dx = 2
2
</p>
<p>a
</p>
<p>(n
a
</p>
<p>)2 a&int;
</p>
<p>0
</p>
<p>sin2
(n
</p>
<p>a
x
)
</p>
<p>dx
</p>
<p>= 2
2
</p>
<p>a
</p>
<p>(n
a
</p>
<p>)2 a
2n
</p>
<p>[n
a
</p>
<p>a &minus; cos n
a
</p>
<p>a sin
n
</p>
<p>a
a
]
=
</p>
<p>(
n
</p>
<p>a
</p>
<p>)2
.
</p>
<p>(V.257)
</p>
<p>It follows that
</p>
<p>p = n
a
</p>
<p>. (V.258)
</p>
<p>This gives for the product of the uncertainties:</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 481
</p>
<p>x &middot;p = a
2
</p>
<p>&radic;
n22 &minus; 6
</p>
<p>3n22
&middot; n
</p>
<p>a
= 
</p>
<p>2
</p>
<p>&radic;
n22 &minus; 6
</p>
<p>3
&gt;
</p>
<p>
</p>
<p>2
. (V.259)
</p>
<p>The last inequality holds due to 2 &gt; 9 &rarr; 2&minus;6
3
</p>
<p>&gt; 1.
</p>
<p>Occasionally, one encounters the fallacy that in the infinite potential well,
</p>
<p>due to E = p2
2m
</p>
<p>, a sharp energy leads to a sharp momentum. But that is
</p>
<p>true only for the absolute value of the momentum; the momentum itself is
</p>
<p>not sharp. The physical reason is that the states are standing waves, which
</p>
<p>correspond to a back-and-forth movement with correspondingly different
</p>
<p>momenta (hence p = 0); in other words, for a given energy, p is not
uniquely defined by p = &plusmn;
</p>
<p>&radic;
2m E .
</p>
<p>6. In the infinite potential well, a normalized state is given by
</p>
<p>
 (x, t) = cnn(x)e&minus;in t + cmm(x)e&minus;im t ; cn, cm &isin; C; n 	= m. (V.260)
</p>
<p>Calculate x.
Solution: We note first that due to the normalization, we have
</p>
<p>a&int;
</p>
<p>0
</p>
<p>
&lowast;
dx = |cn|2 + |cm |2 = 1. (V.261)
</p>
<p>with
</p>
<p>x =
a&int;
</p>
<p>0
</p>
<p>
&lowast;x
dx, (V.262)
</p>
<p>it follows that
</p>
<p>x= |cn|2
a&int;
</p>
<p>0
</p>
<p>x2ndx+ |cm |2
a&int;
</p>
<p>0
</p>
<p>x2mdx +
</p>
<p>

c
</p>
<p>&lowast;
ncme
</p>
<p>i(n&minus;m)t
a&int;
</p>
<p>0
</p>
<p>n xmdx+c.c
</p>
<p>


</p>
<p>= 2
a
|cn|2
</p>
<p>a&int;
</p>
<p>0
</p>
<p>x sin2 kn xdx + 2a |cm |2
a&int;
</p>
<p>0
</p>
<p>x sin2 km xdx
</p>
<p>+
</p>
<p>


</p>
<p>2
</p>
<p>a
c&lowast;ncme
</p>
<p>i(n&minus;m )t
a&int;
</p>
<p>0
</p>
<p>x sin kn x &middot; sin km xdx + c.c
</p>
<p>

 .
</p>
<p>(V.263)
</p>
<p>Because of kn = na , this gives
</p>
<p>a&int;
</p>
<p>0
</p>
<p>x sin2 kn xdx =
a2
</p>
<p>4
(V.264)</p>
<p/>
</div>
<div class="page"><p/>
<p>482 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>and
</p>
<p>a&int;
</p>
<p>0
</p>
<p>x sin kn x &middot; sin km xdx =
2nma2
</p>
<p>2
(
n2 &minus; m2
</p>
<p>)2
[
(&minus;1)n+m &minus; 1
</p>
<p>]
; n 	= m. (V.265)
</p>
<p>This means
</p>
<p>x = 2
a
|cn|2 a
</p>
<p>2
</p>
<p>4
+ 2
</p>
<p>a
|cm |2 a
</p>
<p>2
</p>
<p>4
</p>
<p>+
{
</p>
<p>2
a
</p>
<p>c&lowast;ncme
i(n&minus;m )t 2nma2
</p>
<p>2(n2&minus;m2)2
[
(&minus;1)n+m &minus; 1
</p>
<p>]
+ c.c
</p>
<p>}
</p>
<p>= a
2
+ 4nma
</p>
<p>2(n2&minus;m2)2
[
(&minus;1)n+m &minus; 1
</p>
<p>] {
c&lowast;ncme
</p>
<p>i(n&minus;m )t + c.c
}
.
</p>
<p>(V.266)
</p>
<p>with cn = |cn| ein , it follows that
</p>
<p>c&lowast;ncme
i(n&minus;m )t = |cn| |cm | ei(n&minus;m )t+i(m&minus;n). (V.267)
</p>
<p>Hence, we can compensate the phases by choosing a new zero of time; thus, we
</p>
<p>can setm&minus;n = 0. In addition, we use the shorthand notationnm = n&minus;m .
It follows then that:
</p>
<p>x = a
2
+ 8nma |cn| |cm |
</p>
<p>2
(
n2 &minus; m2
</p>
<p>)2
[
(&minus;1)n+m &minus; 1
</p>
<p>]
cosnm t (V.268)
</p>
<p>or
</p>
<p>x = a
2
&middot;
{
</p>
<p>1
</p>
<p>1 &minus; 32nm|cn ||cm |
2(n2&minus;m2)2
</p>
<p>cosnm t
for n + m =
</p>
<p>{
even
</p>
<p>odd.
(V.269)
</p>
<p>Calculation exercise: Show that
</p>
<p>32nm |cn| |cm |
2
</p>
<p>(
n2 &minus; m2
</p>
<p>)2 &lt; 1. (V.270)
</p>
<p>Solution: Because of |cn|2 + |cm |2 = 1 and (|cn| &minus; |cm |)2 &ge; 0, we have
|cn| |cm | &le; 12 , and therefore
</p>
<p>32nm |cn| |cm |
2
</p>
<p>(
n2 &minus; m2
</p>
<p>)2 &le;
16nm
</p>
<p>2
(
n2 &minus; m2
</p>
<p>)2 &le;
16n (n + 1)
2 (2n + 1)2
</p>
<p>&le; 16n
2
</p>
<p>24n2
= 4
</p>
<p>2
&lt; 1.
</p>
<p>(V.271)
</p>
<p>7. Consider an infinite square well with potential limits at x = 0 and x = a. The
initial value of the wavefunction is 
 (x, 0) =  &isin; R for b &minus;  &le; x &le; b + </p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 483
</p>
<p>and 
 (x, 0) = 0 otherwise (of course, 0 &le; b &minus;  and b +  &le; a). Remember
that the eigenfunctions n (x) =
</p>
<p>&radic;
2
a
</p>
<p>sin kn x with kn = na form a CONS.
(a) Normalize the initial state.
</p>
<p>Solution:
</p>
<p>a&int;
</p>
<p>0
</p>
<p>|
 (x, 0)|2 dx =
b+&int;
</p>
<p>b&minus;
</p>
<p>2dx = 2 &middot; 2 = 1 &rarr;  = 1&radic;
2
</p>
<p>. (V.272)
</p>
<p>(b) Calculate 
 (x, t).
</p>
<p>Solution: Start with
</p>
<p>
 (x, t) =
&sum;
</p>
<p>cnn (x) e
&minus;i En t
</p>
<p> with cn =
&int;
</p>
<p>&lowast;n (x)
 (x, 0) dx . (V.273)
</p>
<p>It follows then that
</p>
<p>cn =
&radic;
</p>
<p>2
</p>
<p>a
</p>
<p>b+&int;
</p>
<p>b&minus;
</p>
<p>sin kn x &middot;dx =
&radic;
</p>
<p>2
</p>
<p>a

</p>
<p>(
&minus;cos kn (b + )&minus; cos kn (b &minus; )
</p>
<p>kn
</p>
<p>)
,
</p>
<p>(V.274)
</p>
<p>and with this,
</p>
<p>cn = &minus;
&radic;
</p>
<p>2
</p>
<p>a

&minus;2 sin bkn &middot; sin kn
</p>
<p>kn
= 2&radic;
</p>
<p>a
sin bkn
</p>
<p>sin kn&radic;
kn
</p>
<p>. (V.275)
</p>
<p>For the total wavefunction, we find:
</p>
<p>
 (x, t) = 2&radic;
a
</p>
<p>&sum;
sin bkn
</p>
<p>sin kn&radic;
kn
</p>
<p>n (x) e
&minus;i En t
</p>
<p> . (V.276)
</p>
<p>(c) Find the probability of measuring the system in the state n.
</p>
<p>Solution: It is
</p>
<p>|cn|2 =
4
</p>
<p>a
sin2 bkn
</p>
<p>sin2 kn
</p>
<p>k2n
= 4a
</p>
<p>
</p>
<p>sin2
(
n b
</p>
<p>a
</p>
<p>)
sin2
</p>
<p>(
n 
</p>
<p>a
</p>
<p>)
</p>
<p>n22
. (V.277)
</p>
<p>8. Show that for the expectation value of a physical quantity A,
</p>
<p>i
d
</p>
<p>dt
A = [A, H ] + i
</p>
<p>&lang;
&part;
</p>
<p>&part;t
A
</p>
<p>&rang;
(V.278)
</p>
<p>holds. Show that for time-independent operators, the expectation value of the
</p>
<p>corresponding physical quantity is conserved, if A commutes with H .</p>
<p/>
</div>
<div class="page"><p/>
<p>484 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>9. Show that
d
</p>
<p>dt
r = 1
</p>
<p>m
p and d
</p>
<p>dt
p = &minus; &nabla;V  . (V.279)
</p>
<p>10. Under which conditions is the orbital angular momentum l = r&times;p a conserved
quantity?
</p>
<p>Solution: To check if the angular momentum l = r&times; 
i
&nabla; is a conserved quantity,
</p>
<p>due to the relation
</p>
<p>i
d
</p>
<p>dt
l = [l, H ] , (V.280)
</p>
<p>we need to calculate its commutator with H . Since the angular momentum is a
</p>
<p>vector, we have three equations; we limit ourselves to [lx , H ] and transfer the
</p>
<p>result to the two other components. We use
</p>
<p>lx = (r &times; p)x = ypz &minus; zpy =

</p>
<p>i
</p>
<p>(
y
&part;
</p>
<p>&part;z
&minus; z &part;
</p>
<p>&part;y
</p>
<p>)
(V.281)
</p>
<p>and
</p>
<p>H = &minus; 
2
</p>
<p>2m
&nabla;2 + V = H0 + V . (V.282)
</p>
<p>(a) First we show that [lx , H0] = 0 using H0 = p
2
</p>
<p>2m
and the relation [x, px ] = i
</p>
<p>plus the analogues for y, z. We split the expression
[
ypz &minus; zpy,p2
</p>
<p>]
and
</p>
<p>consider only
[
ypz,p
</p>
<p>2
]
; the other term follows by interchanging y and z.
</p>
<p>In this way (as always we assume that the order of the partial derivatives is
</p>
<p>irrelevant), we obtain:
</p>
<p>[
ypz,p
</p>
<p>2
]
=
</p>
<p>[
ypz, p
</p>
<p>2
x + p2y + p2z
</p>
<p>]
=
</p>
<p>[
ypz, p
</p>
<p>2
y
</p>
<p>]
(V.283)
</p>
<p>since p2x and p
2
z commute with ypz . The remaining term is rearranged:
</p>
<p>[
ypz, p
</p>
<p>2
y
</p>
<p>]
= ypz p2y &minus; p2y ypz = ypz p2y &minus; py
</p>
<p>(
ypy &minus; i
</p>
<p>)
pz
</p>
<p>=ypz p2y&minus;py ypy pz+pyipz=ypz p2y &minus;
(
ypy &minus; i
</p>
<p>)
py pz + pyipz
</p>
<p>= ypz p2y &minus; yp2y pz + 2ipy pz = 2ipy pz .
(V.284)
</p>
<p>It follows then that
</p>
<p>[
ypz &minus; zpy,p2
</p>
<p>]
= 2ipy pz &minus; 2ipz py = 0, (V.285)
</p>
<p>or [lx , H0] = 0, and analogously for ly , lz .
(b) It remains to calculate
</p>
<p>[lx , H ] = [lx , V ] . (V.286)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 485
</p>
<p>This expression (operator equation!) is evaluated as
</p>
<p>[lx , V ] =

</p>
<p>i
</p>
<p>(
y
&part;
</p>
<p>&part;z
&minus; z &part;
</p>
<p>&part;y
</p>
<p>)
V &minus; V 
</p>
<p>i
</p>
<p>(
y
&part;
</p>
<p>&part;z
&minus; z &part;
</p>
<p>&part;y
</p>
<p>)
</p>
<p>= 
i
</p>
<p>(
y
&part;V
</p>
<p>&part;z
&minus; z &part;V
</p>
<p>&part;y
</p>
<p>)
+ V 
</p>
<p>i
</p>
<p>(
y
&part;
</p>
<p>&part;z
&minus; z &part;
</p>
<p>&part;y
</p>
<p>)
&minus; V 
</p>
<p>i
</p>
<p>(
y
&part;
</p>
<p>&part;z
&minus; z &part;
</p>
<p>&part;y
</p>
<p>)
</p>
<p>= 
i
</p>
<p>(
y
&part;V
</p>
<p>&part;z
&minus; z &part;V
</p>
<p>&part;y
</p>
<p>)
= 
</p>
<p>i
(r &times;&nabla;V )x ,
</p>
<p>(V.287)
</p>
<p>or, for all three components in compact form:
</p>
<p>[l, H ] = 
i
(r &times;&nabla;V ). (V.288)
</p>
<p>In sum, this means that
</p>
<p>d
</p>
<p>dt
l = &minus; r &times;&nabla;V  . (V.289)
</p>
<p>The right-hand side is zero for V = V (r) with r = |r|, for then we have
&nabla;V (r) = r
</p>
<p>r
</p>
<p>&part;V (r)
&part;r
</p>
<p>. Hence, in general, i.e. excepting the radially-symmetric
</p>
<p>case, the angular momentum l is a not conserved quantity in an external
</p>
<p>potential.
</p>
<p>11. Given the Hamiltonian H with a discrete and non-degenerate spectrum En and
</p>
<p>eigenstates n (r), show that the energy uncertainty H vanishes, iff the quan-
</p>
<p>tum object is in an eigenstate of the energy.
</p>
<p>Solution: Time-dependent and stationary SEq are:
</p>
<p>i
&part;
</p>
<p>&part;t

 (r, t) = H
 (r, t) ; Hn (r) = Enn (r) . (V.290)
</p>
<p>The general solution, given by ({n (x)}, is a CONS)
</p>
<p>
 (r, t) =
&sum;
</p>
<p>n
</p>
<p>cnn(r)e
&minus;in t ;
</p>
<p>&sum;
</p>
<p>n
</p>
<p>|cn|2 = 1. (V.291)
</p>
<p>We have to calculate
</p>
<p>H =
&int;
</p>
<p>
&lowast;H
dV and
&lang;
H 2
</p>
<p>&rang;
=
</p>
<p>&int;

&lowast;H 2
dV . (V.292)
</p>
<p>Inserting gives</p>
<p/>
</div>
<div class="page"><p/>
<p>486 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>H =
&sum;
</p>
<p>nm
</p>
<p>&int;
c&lowast;n&lowast;nein t cm Emme&minus;im t dV =
</p>
<p>&sum;
</p>
<p>nm
</p>
<p>c&lowast;nein t cm Eme&minus;im t
&int;
&lowast;nmdV
</p>
<p>&lang;
H2
</p>
<p>&rang;
=
</p>
<p>&sum;
</p>
<p>nm
</p>
<p>&int;
c&lowast;n&lowast;nein t cm E2mme&minus;im t dV =
</p>
<p>&sum;
</p>
<p>nm
</p>
<p>c&lowast;nein t cm E2me&minus;im t
&int;
&lowast;nmdV .
</p>
<p>(V.293)
</p>
<p>Because of the orthonormality of the eigenfunctions (
&int;
&lowast;nmdV = nm), it
</p>
<p>follows that
</p>
<p>H =
&sum;
</p>
<p>n
</p>
<p>|cn|2 En;
&lang;
H 2
</p>
<p>&rang;
=
</p>
<p>&sum;
</p>
<p>n
</p>
<p>|cn|2 E2n (V.294)
</p>
<p>and therefore
</p>
<p>(H)2 =
&lang;
H 2
</p>
<p>&rang;
&minus; H2 =
</p>
<p>&sum;
</p>
<p>n
</p>
<p>|cn|2 E2n &minus;
(&sum;
</p>
<p>n
</p>
<p>|cn|2 En
)2
</p>
<p>;
&sum;
</p>
<p>n
</p>
<p>|cn|2 = 1.
</p>
<p>(V.295)
</p>
<p>We rewrite this as
</p>
<p>(H)2 =
&sum;
</p>
<p>n
</p>
<p>|cn|2 E2n &middot;
&sum;
</p>
<p>m
</p>
<p>|cm |2 &minus;
&sum;
</p>
<p>n
</p>
<p>|cn|2 En &middot;
&sum;
</p>
<p>m
</p>
<p>|cm |2 Em
</p>
<p>=
&sum;
</p>
<p>nm
</p>
<p>|cn|2 |cm |2 E2n &minus;
&sum;
</p>
<p>nm
</p>
<p>|cn|2 |cm |2 En Em =
&sum;
</p>
<p>nm
</p>
<p>|cn|2 |cm |2 En (En &minus; Em) .
</p>
<p>(V.296)
</p>
<p>The last double sum of course yields the same result if we interchange n and m.
</p>
<p>Using this fact, we write
</p>
<p>(H)2 = 1
2
</p>
<p>&sum;
</p>
<p>nm
</p>
<p>|cn|2 |cm |2 En (En &minus; Em)+ 12
&sum;
</p>
<p>nm
</p>
<p>|cm |2 |cn|2 Em (Em &minus; En)
</p>
<p>= 1
2
</p>
<p>&sum;
</p>
<p>nm
</p>
<p>|cn|2 |cm |2 (En &minus; Em)2 .
</p>
<p>(V.297)
</p>
<p>We see that all terms in the sum are non-negative. Hence, H is zero iff each
</p>
<p>term |cn|2 |cm |2 (En &minus; Em)2 vanishes. Since the terms for n = m or En = Em are
zero anyway, each of the terms |cn|2 |cm |2 with n 	= m has to vanish separately in
order to arrive at H = 0. This is the case iff (a) all cn are zero (trivial solution,
physically uninteresting); or (b) all cn are zero except one, say cN . But in this
</p>
<p>case, the state 
 is an eigenstate of H with the energy EN :
</p>
<p>
 (x, t) = cNN (x)e&minus;iN t . (V.298)
</p>
<p>This property of the variance exists for all Hermitian operators; it disappears iff
</p>
<p>the state is an eigenstate of the operator for which the mean value is evaluated
</p>
<p>(see Appendix O, Vol. 1).</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 487
</p>
<p>V.10 Exercises, Chap. 11
</p>
<p>1. Show that the equation &sum;
</p>
<p>i
</p>
<p>ci A j i = ac j (V.299)
</p>
<p>may be written in the matrix representation as
</p>
<p>Ac = ac (V.300)
</p>
<p>with the matrix
{
</p>
<p>A j i
}
&equiv; A and the column vector c. Is the equation also valid for
</p>
<p>non-square matrices?
</p>
<p>2. Do the functions of one variable which are continuous in the interval [0, 1] form
</p>
<p>a Hilbert space?
</p>
<p>Solution: No, since sequences of continuous functions may lead to discontinuous
</p>
<p>functions; example lim
n&rarr;&infin;
</p>
<p>xn in the interval [0, 1]. Thus, the criterion of complete-
</p>
<p>ness is not satisfied.
</p>
<p>3. The space l(2) consists of all vectors | with infinitely many components (coor-
dinates) c1, c2, . . ., such that
</p>
<p>|2 =
&sum;
</p>
<p>n
</p>
<p>|cn|2 &lt; &infin;. (V.301)
</p>
<p>Show that also the linear combination of two vectors | and | belongs to this
space, and that the scalar product  | is defined.
Solution: | has the coordinates d1, d2, . . .. Then we have
</p>
<p> | +  |2 =
&sum;
</p>
<p>n
</p>
<p>|cn + dn|2 &le; 2
&sum;
</p>
<p>n
</p>
<p>(
||2 |cn|2 + ||2 |dn|2
</p>
<p>)
&lt; &infin;.
</p>
<p>(V.302)
</p>
<p>The inequality is satisfied due to
</p>
<p>|a + b|2 = 2
(
|a|2 + |b|2
</p>
<p>)
&minus; |a &minus; b|2 . (V.303)
</p>
<p>For the scalar product, we obtain using the Schwarz inequality | || &le; | &middot;
| (see Appendix G, Vol. 1):
</p>
<p>| || =

&sum;
</p>
<p>n
</p>
<p>c&lowast;ndn
</p>
<p> &le;
&radic;&sum;
</p>
<p>n
</p>
<p>|cn|2 &middot;
&radic;&sum;
</p>
<p>n
</p>
<p>|dn|2 &lt; &infin;. (V.304)
</p>
<p>4. Given the operator A and the equation
</p>
<p>i
d
</p>
<p>dt
| = A | , (V.305)</p>
<p/>
</div>
<div class="page"><p/>
<p>488 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>which condition must A fulfill, so that the norm of | is conserved?
Solution: With d
</p>
<p>dt
| =
</p>
<p>
&rang;
</p>
<p>and i
&lang;

 = &minus;| A&dagger;, it holds that
</p>
<p>i
d
</p>
<p>dt
 | = i
</p>
<p>&lang;
 | + i 
</p>
<p>
&rang;
= &minus;| A&dagger; | + | A | . (V.306)
</p>
<p>Since this equation must hold for all allowed |, the conservation of the norm
implies &minus;A&dagger; + A = 0; hence, the operator A has to be Hermitian. In other
words: a linear differential equation of the first order must have the structure
</p>
<p>i d
dt
| = A | with A&dagger; = A in order for the norm to be conserved.
</p>
<p>5. Given the operator A. Derive the equation
</p>
<p>i
d
</p>
<p>dt
A = [A, H ] + i
</p>
<p>&lang;
A
&rang;
</p>
<p>(V.307)
</p>
<p>in the bra-ket formalism.
</p>
<p>Solution: Start from
</p>
<p>i
d
</p>
<p>dt
A = i d
</p>
<p>dt
| A | = i
</p>
<p>&lang;

 A | + i | A | + i | A
</p>
<p>
&rang;
.
</p>
<p>(V.308)
</p>
<p>This leads with i

</p>
<p>&rang;
= H | to
</p>
<p>i
d
</p>
<p>dt
A = &minus; | H &dagger; A | + | AH | + i | A | , (V.309)
</p>
<p>and, with H = H &dagger;, it follows finally
</p>
<p>i
d
</p>
<p>dt
A = | AH &minus; H A | + i | A | = [A, H ] + i
</p>
<p>&lang;
A
&rang;
. (V.310)
</p>
<p>6. Given the Hamiltonian H with discrete and non-degenerate spectrum, (a) in the
</p>
<p>formulation with space variables and (b) as abstract operator; what is in each case
</p>
<p>the matrix representation of the time-dependent SEq?
</p>
<p>(a) Solution: The eigenvalue equation or stationary SEq reads
</p>
<p>Hn (x) = Enn (x) , (V.311)
</p>
<p>and the time-dependent SEq is
</p>
<p>i&part;t (x, t) = H (x, t) . (V.312)
</p>
<p>Since {n (x)} is a CONS, we can write  (x, t) as
</p>
<p> (x, t) =
&sum;
</p>
<p>n
</p>
<p>cn (t)n (x) . (V.313)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 489
</p>
<p>We insert this expression in the SEq and obtain
</p>
<p>i
&sum;
</p>
<p>n
</p>
<p>&part;t cn (t)n (x) =
&sum;
</p>
<p>n
</p>
<p>cn (t) Hn (x) . (V.314)
</p>
<p>Multiplication by &lowast;m (x) and integration leads, due to the orthonormality of
{n (x)}, to
</p>
<p>i
&sum;
n
</p>
<p>&part;t cn (t)
&int;
&lowast;m (x)n (x) dx =
</p>
<p>&sum;
n
</p>
<p>cn (t)
&int;
&lowast;m (x) Hn (x) dx
</p>
<p>i&part;t cm (t) =
&sum;
n
</p>
<p>cn (t)
&int;
&lowast;m (x) Hn (x) dx .
</p>
<p>(V.315)
</p>
<p>Above, we have seen that
&int;
&lowast;m (x) Hn (x) dx = Emnm . With this, we
</p>
<p>obtain for the time-dependent SEq:
</p>
<p>i&part;t cm (t) = Emcm (t) , (V.316)
</p>
<p>or, in matrix form
</p>
<p>i&part;tc (t) = HMatrixc (t) (V.317)
</p>
<p>with the column vector and the matrix
</p>
<p>c =
</p>
<p>

</p>
<p>c1
c2
...
</p>
<p>
 ; HMatrix =
</p>
<p>

</p>
<p>E1 0 . . .
</p>
<p>0 E2 . . .
...
</p>
<p>...
. . .
</p>
<p>
 . (V.318)
</p>
<p>The solution of the ordinary differential equation (V.316) reads
</p>
<p>cm (t) = cm (0) e&minus;i Em t/, (V.319)
</p>
<p>and the solution  (x, t) obtains its familiar form:
</p>
<p> (x, t) =
&sum;
</p>
<p>n
</p>
<p>cm (0)n (x) e
&minus;i Em t/. (V.320)
</p>
<p>(b) Solution: For a change, we calculate by a slightly different route for the
</p>
<p>abstract case. We start from
</p>
<p>i&part;t | = H | (V.321)
</p>
<p>where the stationary SEq is given by
</p>
<p>H |n = En |n . (V.322)</p>
<p/>
</div>
<div class="page"><p/>
<p>490 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>Due to &part;t |n = 0, it follows that
</p>
<p>i&part;t n | = n| H | =
&sum;
</p>
<p>m
</p>
<p>n| H |m m | , (V.323)
</p>
<p>and with
</p>
<p>cn = n | and Hnm = n| H |m = Emnm (V.324)
</p>
<p>we find
</p>
<p>i&part;t cn =
&sum;
</p>
<p>m
</p>
<p>Hnmcm, (V.325)
</p>
<p>or, written compactly using the column vector c and the matrix HMatrix:
</p>
<p>i&part;tc = HMatrixc. (V.326)
</p>
<p>V.11 Exercises, Chap. 12
</p>
<p>1. Given an eigenstate |k of the momentum operator; how is this state described
in the position representation?
</p>
<p>2. Show by using x | k = 1&radic;
2
</p>
<p>eikx that the improper vectors |k form a CONS.
3. Given an improper vector |, what is the associated eigendifferential
</p>
<p>,
&rang;
?
</p>
<p>4. Given the state |k with the sharply-defined momentum k; we have x | k =
1&radic;
2
</p>
<p>eikx .
</p>
<p>(a) What is the (abstract) eigendifferential?
</p>
<p>Solution: With kn = nk (fixed screening), it follows that
</p>
<p>|kn,k =
1&radic;
k
</p>
<p>kn+k&int;
</p>
<p>kn
</p>
<p>k &prime;
&rang;
dk &prime;. (V.327)
</p>
<p>(b) How is the eigendifferential expressed in the position representation?
</p>
<p>Solution: Multiplication by x | gives
</p>
<p>x | kn,k =
1&radic;
2
</p>
<p>1&radic;
k
</p>
<p>kn+k&int;
</p>
<p>kn
</p>
<p>eik
&prime;x dk &prime;
</p>
<p>= eikn x e
ik&middot;x &minus; 1&radic;
2ki x
</p>
<p>= ei(kn+k/2)x 2 sin (k &middot; x/2)&radic;
2k &middot; x
</p>
<p>.
</p>
<p>(V.328)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 491
</p>
<p>(c) Show that the eigendifferentials of (b) are orthonormal.
</p>
<p>Solution: For the scalar product, we have
</p>
<p>km,k| kn,k
</p>
<p>=
&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>e&minus;ikm x&minus;ik&middot;x/2
2 sin (k &middot; x/2)&radic;
</p>
<p>2kx
&middot; eikn x+ik&middot;x/2 2 sin (k &middot; x/2)&radic;
</p>
<p>2kx
dx
</p>
<p>= 2
k
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>sin2 (k &middot; x/2)
x2
</p>
<p>&middot; ei(kn&minus;km )x dx .
</p>
<p>(V.329)
</p>
<p>Insertion and substitution of y = xk leads to
</p>
<p>km,k| kn,k =
2
</p>
<p>
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>sin2 (y/2)
</p>
<p>y2
&middot; ei(n&minus;m)ydy = kn ,km = n,m .
</p>
<p>(V.330)
</p>
<p>On the last integral: The term with sin (n &minus; m) y vanishes due to the point
symmetry of the sine function. Regarding the cosine, a formula tabulation
</p>
<p>or your own calculation gives:
</p>
<p>&infin;&int;
</p>
<p>0
</p>
<p>sin2 az
</p>
<p>z2
cos bz dz =
</p>
<p>{

2
</p>
<p>(
a &minus; b
</p>
<p>2
</p>
<p>)
</p>
<p>0
for
</p>
<p>b &lt; 2a
</p>
<p>b &ge; 2a. (V.331)
</p>
<p>5. Given the SEq in the abstract formulation
</p>
<p>i
d
</p>
<p>dt
| = H | , (V.332)
</p>
<p>(a) Formulate the equation in the position representation and in the momentum
</p>
<p>representation.
</p>
<p>Solution: We have
</p>
<p>i d
dt
x | =
</p>
<p>&int;
x | H
</p>
<p>x &prime;
&rang; &lang;
</p>
<p>x &prime; | dx &prime;
</p>
<p>i d
dt
k | =
</p>
<p>&int;
k| H
</p>
<p>k &prime;
&rang; &lang;
</p>
<p>k &prime; | dk &prime;.
(V.333)
</p>
<p>Since H is diagonal in the position representation, we obtain i d
dt
x | =
</p>
<p>x | H |x
&lang;
x &prime; | or i d
</p>
<p>dt
 (x) = H(x) (x).
</p>
<p>(b) How can one calculate the matrix element k| H
k &prime;
</p>
<p>&rang;
, if H is known in the
</p>
<p>position representation?
</p>
<p>Solution: We insert the one (identity operator) two times:
</p>
<p>k| H
k &prime;
</p>
<p>&rang;
=
</p>
<p>&int;
k| x x | H
</p>
<p>x &prime;
&rang; &lang;
</p>
<p>x &prime;
k &prime;
</p>
<p>&rang;
dx dx &prime;. (V.334)</p>
<p/>
</div>
<div class="page"><p/>
<p>492 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>Since H is diagonal in the position representation, we obtain with x |k =
1&radic;
2
</p>
<p>eikx :
</p>
<p>k| H
k &prime;
</p>
<p>&rang;
= 1
</p>
<p>2
</p>
<p>&int;
e&minus;ikx H (x) eik
</p>
<p>&prime;x dx . (V.335)
</p>
<p>6. Given a CONS {|n}; formulate the projection operator
</p>
<p>P1 = |1 1| (V.336)
</p>
<p>in the position representation.
</p>
<p>Solution: For a state |
, in the bra-ket notation we have:
</p>
<p>P1 |
 = |1 1 |
 = c |1 ; c = 1 |
 . (V.337)
</p>
<p>In the position representation, it follows that
</p>
<p>(P1
) (r) = c1 (r) = 1 (r) &middot;
&int;
</p>
<p>d3r &prime; &lowast;1
(
r &prime;
)


</p>
<p>(
r &prime;
)
. (V.338)
</p>
<p>We see explicitly that the operator is not diagonal in the position representation.
</p>
<p>In detail:
</p>
<p>P1|
 = |11|
 &harr; r |P1|
 = r |1 &middot; 1|1|

</p>
<p>= r |1 &middot;
&int;
</p>
<p>d3r &prime; 1|r &prime;r &prime;|
 (V.339)
</p>
<p>or
</p>
<p>&int;
d3r &prime; r |P1|r &prime;r &prime;|
 = 1(r) &middot;
</p>
<p>&int;
d3r &prime; &lowast;1(r
</p>
<p>&prime;)
(r &prime;). (V.340)
</p>
<p>7. A and B are self-adjoint operators with [A, B] = i, and |a is an eigenvector
of A for the eigenvalue a. Then we have
</p>
<p>a |[A, B]| a = a |AB &minus; B A| a = (a &minus; a) a |B| a = 0. (V.341)
</p>
<p>On the other hand, we also have:
</p>
<p>a |[A, B]| a = a |i| a = i 	= 0. (V.342)
</p>
<p>Question: where is the flaw in this argument?
</p>
<p>Solution: One can show (see Appendix I, Vol. 1) that at least one of the two oper-
</p>
<p>ators must be unbounded if we have [A, B] = i (which is not apparent at first
glance, and therefore&mdash;and because this term is introduced only in Chap. 13&mdash;
</p>
<p>the exercise is a bit unfair). This means that the eigenvectors are not normalizable
</p>
<p>and the corresponding scalar products do not exist. This is an example of how</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 493
</p>
<p>one has to be somewhat more cautious when dealing with unbounded operators
</p>
<p>and continuous spectra.
</p>
<p>V.12 Exercises, Chap. 13
</p>
<p>1. Let A be a linear and B an anti-linear operator; | is a state. Compute or simplify
A (i |) and B (i |).
</p>
<p>2. Show that the complex conjugation K is an anti-linear operator.
</p>
<p>Solution:
</p>
<p>Ki | = &minus;iK | or Ki = &minus;iK. (V.343)
</p>
<p>3. Show that the commutator C = [A, B] of two Hermitian operators A and B is
anti-Hermitian.
</p>
<p>Solution: We have
</p>
<p>C&dagger; = (AB &minus; B A)&dagger; = B A &minus; AB = &minus;C. (V.344)
</p>
<p>4. The Hermitian operators A and B fulfill [A, B] 	= 0. Consider the operator
Q = c [A, B]. For which c is Q a Hermitian operator?
</p>
<p>5. Consider the operator Q = AB, where A and B are Hermitian matrices. Under
what conditions is Q a Hermitian operator?
</p>
<p>6. Show in the bra-ket representation that:
</p>
<p>(a) Hermitian operators have real eigenvalues.
</p>
<p>(b) The eigenfunctions of Hermitian operators are pairwise orthogonal (assum-
</p>
<p>ing the spectrum is not degenerate).
</p>
<p>7. Show that the mean value of a Hermitian operator A is real, and the mean value
</p>
<p>of an anti-Hermitian operator B is imaginary.
</p>
<p>Solution:
</p>
<p>A&dagger; = | A |&dagger; = | A&dagger; | = | A | = A &rarr; A &isin; R
B&dagger; = | B |&dagger; = | B&dagger; | = &minus; | B | = &minus; B &rarr; B &isin; I.
</p>
<p>(V.345)
</p>
<p>8. What is the quantum-mechanical operator for the classical term p&times; l?
Solution: We have here a product of operators and have to check first whether
</p>
<p>the translation of the classically identical terms p&times; l and &minus;l&times; p into quantum
mechanics yields the same result. If not, we have to symmetrize, as shown in
</p>
<p>Chap. 3. We consider the x components. We have
</p>
<p>(p&times; l)x = pylz &minus; pzly = py
(
xpy &minus; ypx
</p>
<p>)
&minus; pz (zpx &minus; xpz)
</p>
<p>&minus; (l&times; p)x = &minus;ly pz + lz py = &minus; (zpx &minus; xpz) pz +
(
xpy &minus; ypx
</p>
<p>)
py .
</p>
<p>(V.346)</p>
<p/>
</div>
<div class="page"><p/>
<p>494 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>With [x, px ] = i and
[
x, py
</p>
<p>]
=
</p>
<p>[
x, pz
</p>
<p>]
= 0 etc. in mind, evaluation of the
</p>
<p>brackets leads to
</p>
<p>(p&times; l)x = py xpy &minus; py ypx&minus;pz zpx+pz xpz = xp2y + xp2z&minus;py ypx &minus; pz zpx
= x
</p>
<p>(
p2y + p2z
</p>
<p>)
+
</p>
<p>(
i&minus; ypy
</p>
<p>)
px+ (i&minus; zpz) px
</p>
<p>&minus; (l&times; p)x = &minus;zpx pz + xpz pz + xpy py &minus; ypx py = x
(
</p>
<p>p2y + p2z
)
&minus; px zpz &minus; px ypy .
</p>
<p>(V.347)
</p>
<p>We see that p &times; l 	= (p&times; l)&dagger; = &minus;l &times; p holds true; consequently, we have to
symmetrize and obtain for the quantum-mechanical operator
</p>
<p>(p&times; l)classical,x &rarr; (p&times; l)quantum,x = (p&times;l)x&minus;(l&times;p)x2
= x
</p>
<p>(
p2y + p2z
</p>
<p>)
+
</p>
<p>(
i&minus; zpz &minus; ypy
</p>
<p>)
px
</p>
<p>(V.348)
</p>
<p>plus cyclic permutations for the two other components. This operator is obviously
</p>
<p>Hermitian.
</p>
<p>One can transform the result into a more pleasing expression, see Appendix G,
</p>
<p>Vol. 2, &lsquo;Lenz vector&rsquo;. Moreover, we see that
</p>
<p>p&times; l+ l&times; p = 2ip (V.349)
</p>
<p>so that we can write
</p>
<p>(p&times; l)classical &rarr;
p&times; l+ p&times; l&minus; 2ip
</p>
<p>2
= p&times; l&minus; ip. (V.350)
</p>
<p>9. Calculate the mean value of z for the normalized state
</p>
<p>(
a
</p>
<p>b
</p>
<p>)
.
</p>
<p>Solution: It is
</p>
<p>z =
(
</p>
<p>a&lowast; b&lowast;
) ( 1 0
</p>
<p>0 &minus;1
</p>
<p>)(
a
</p>
<p>b
</p>
<p>)
= |a|2 &minus; |b|2 = 2 |a|2 &minus; 1. (V.351)
</p>
<p>10. Given the time-independent Hamiltonian H ; what is the associated time evolu-
</p>
<p>tion operator U (t)?
</p>
<p>11. Let U be the operator U = ei A, where A is a Hermitian operator. Show that U
is unitary.
</p>
<p>12. What are the eigenvalues that a unitary operator can have?
</p>
<p>13. Show that the time evolution operator e&minus;i
Ht
 is unitary.
</p>
<p>14. Show that scalar products, matrix elements, eigenvalues and expectation values
</p>
<p>are invariant under unitary transformations.
</p>
<p>Solution:
</p>
<p>Scalar products and matrix elements:</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 495
</p>
<p>&lang;

 &prime;
</p>
<p>&prime;
&rang;
= 
|U &dagger;U | = 
 |
</p>
<p>&lang;

 &prime;
</p>
<p> A&prime;
&prime;
</p>
<p>&rang;
= 
|U &dagger;U AU &dagger;U | = 
| A | .
</p>
<p>(V.352)
</p>
<p>Expectation value (see matrix element):
</p>
<p>&lang;
A&prime;
&rang;
=
</p>
<p>&lang;

 &prime;
</p>
<p> A&prime;

 &prime;
</p>
<p>&rang;
= 
|U &dagger;U AU &dagger;U |
 = 
| A |
 = A . (V.353)
</p>
<p>Eigenvalue: With
A |an = an |an (V.354)
</p>
<p>it follows that:
</p>
<p>U A |an =
{
</p>
<p>U AU &dagger;U |an = A&prime;
A&prime;n
</p>
<p>&rang;
</p>
<p>Uan |an = an
A&prime;n
</p>
<p>&rang;
,
</p>
<p>(V.355)
</p>
<p>or, compactly:
</p>
<p>A&prime;
A&prime;n
</p>
<p>&rang;
= an
</p>
<p>A&prime;n
&rang;
. (V.356)
</p>
<p>15. P1 and P2 are projection operators. Under which conditions are P = P1 + P2
and P = P1 P2 projection operators?
</p>
<p>16. Formulate the matrix representation of the operator P = |e1 e1| in R3.
Solution:
</p>
<p>P = |e1 e1| &sim;=
</p>
<p>

</p>
<p>1
</p>
<p>0
</p>
<p>0
</p>
<p>
(1 0 0
</p>
<p>)
=
</p>
<p>

</p>
<p>1 0 0
</p>
<p>0 0 0
</p>
<p>0 0 0
</p>
<p>
 . (V.357)
</p>
<p>17. What is the general definition of a projection operator?
</p>
<p>18. Given the CONS {|n} ; for which cn is the operator A =
&sum;
</p>
<p>cn |n n| a
projection operator?
</p>
<p>19. Which eigenvalues can a projection operator have?
</p>
<p>20. Given the CONS {|n} in a Hilbert space of dimension N . Consider the operator
</p>
<p>P =
&sum;
</p>
<p>n&le;N &prime;
|n n| (V.358)
</p>
<p>with N &prime; &le; N . Show that P is a projection operator.
21 Given the Operator A with a degenerate spectrum:
</p>
<p>A
n,r
</p>
<p>&rang;
= an
</p>
<p>n,r
&rang;
; r = 1, . . . gn. (V.359)
</p>
<p>(a) Formulate the projection operator onto the states with subscript n?
</p>
<p>Solution: It is
</p>
<p>Pn =
gn&sum;
</p>
<p>r=1
</p>
<p>n,r
&rang; &lang;
n,r
</p>
<p> ;
&sum;
</p>
<p>n
</p>
<p>Pn = 1. (V.360)</p>
<p/>
</div>
<div class="page"><p/>
<p>496 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>(b) Formulate the spectral representation of A.
</p>
<p>Solution: It is
</p>
<p>A = A &middot; 1 = A
&sum;
</p>
<p>n
</p>
<p>Pn = A
&sum;
</p>
<p>n
</p>
<p>&sum;
</p>
<p>r
</p>
<p>n,r
&rang; &lang;
n,r
</p>
<p> =
&sum;
</p>
<p>n,r
</p>
<p>n,r
&rang;
an
</p>
<p>&lang;
n,r
</p>
<p> .
</p>
<p>(V.361)
</p>
<p>22. Given the operators A = | | and B = | |. Let |  =  &isin; C,  	= 0.
For which  is the operator C = AB a projection operator?
Solution: C must be idempotent, i.e. C2 = C . This means
</p>
<p>AB AB = AB or |  |  |  | | = |  | |
&rarr; |&lowast; | = | | (V.362)
</p>
<p>or in short form,
</p>
<p>&lowast; = 1 &rarr;  = ei ,  &isin; R. (V.363)
</p>
<p>The Hermiticity of C means that
</p>
<p>C = AB = C&dagger; = B&dagger; A&dagger; = B A (V.364)
</p>
<p>and this leads to
</p>
<p>|  | | = |  | | or | | = |&lowast; | . (V.365)
</p>
<p>Multiplication from the left by | yields
</p>
<p>|&lowast; = |&lowast;  | (V.366)
</p>
<p>or
</p>
<p>| = e
i
</p>
<p> | | . (V.367)
</p>
<p>Hence, | and | must be collinear, and  |  | = 1.
23. Given the operator Q = B&dagger; B, where B is unitary. How can Q be more simply
</p>
<p>written?
</p>
<p>24. Given the operator Q = B&dagger; B, where B is not unitary. Show that the eigenvalues
of Q are real and that they are not negative.
</p>
<p>25. Given the operator A =  | |. Let |  =  	= 0;  and  are complex
constants. The states | and | are normalized. Which conditions must |,
|,  and  fulfill to ensure that A is a Hermitian, a unitary, or a projection
operator?
</p>
<p>Solution:
</p>
<p>(a) If A is a Hermitian operator, it must hold that</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 497
</p>
<p>A =  | | = A&dagger; = &lowast; | | . (V.368)
</p>
<p>We multiply from the right by | and obtain
</p>
<p> | |  = &lowast; | |  &rArr;  | = &lowast; | &rArr; | = 
&lowast;
</p>
<p>| .
(V.369)
</p>
<p>Since the states are normalized, it follows that || = 1. | and | differ
from each other only by a phase factor.
</p>
<p>(b) If A is a unitary operator, it must hold that A&dagger; A = 1, i.e.
</p>
<p>A&dagger; A = &lowast; | | | | = ||2 | | = 1. (V.370)
</p>
<p>This is satisfied for || = 1 and | | = 1. The requirement AA&dagger; = 1
leads analogously to | | = 1. This means that || = 1; also here, |
and | must agree up to a phase factor and | | = 1.
</p>
<p>(c) If A is a projector, it must hold that A2 = A, i.e.
</p>
<p>A2 = 2 | | = A =  | | . (V.371)
</p>
<p>This is satisfied for  = 1 and | = 
&lowast; |. Hence, with the result of part
</p>
<p>(a), it follows that || = 1 and || = 1.
26. Given a CONS {|n} and an operator
</p>
<p>A =
&sum;
</p>
<p>n,m
</p>
<p>cnm |n m | ; cnm &isin; C. (V.372)
</p>
<p>How must the coefficients cnm be chosen in order that A be a Hermitian, a unitary,
</p>
<p>or a projection operator?
</p>
<p>Solution:
</p>
<p>(a) If A is Hermitian, it must apply that
</p>
<p>A =
&sum;
</p>
<p>n,m
</p>
<p>cnm |n m | = A&dagger; =
&sum;
</p>
<p>n,m
</p>
<p>c&lowast;nm |m n| =
&sum;
</p>
<p>n,m
</p>
<p>c&lowast;mn |n m | .
</p>
<p>(V.373)
</p>
<p>Here, we interchanged the summation indices in the last step. The compar-
</p>
<p>ison shows immediately that
</p>
<p>cnm = c&lowast;mn. (V.374)
</p>
<p>If we represent (cnm) as a matrix C , the last equation means none other
</p>
<p>than the familiar adjoint: commutation of columns and rows plus complex
</p>
<p>conjugation.
</p>
<p>(b) If A is unitary, it must apply that:</p>
<p/>
</div>
<div class="page"><p/>
<p>498 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>AA&dagger; = &sum;
n,m
</p>
<p>cnm |n m |
&sum;
</p>
<p>n&prime;,m &prime;
c&lowast;n&prime;m &prime; |m &prime; n&prime; |
</p>
<p>= &sum;
n,m,n&prime;,m &prime;
</p>
<p>cnmc
&lowast;
n&prime;m &prime; |n mm &prime; n&prime; | = 1.
</p>
<p>(V.375)
</p>
<p>It follows that
</p>
<p>1 =
&sum;
</p>
<p>n,m,n&prime;
</p>
<p>cnmc
&lowast;
n&prime;m |n n&prime; | =
</p>
<p>&sum;
</p>
<p>n,n&prime;
</p>
<p>|n n&prime; |
&sum;
</p>
<p>m
</p>
<p>cnmc
&lowast;
n&prime;m . (V.376)
</p>
<p>Since {|n} is a CONS, we have 1 =
&sum;
</p>
<p>n |n n|. Hence, we must have
&sum;
</p>
<p>m
</p>
<p>c&lowast;n&prime;mcnm = nn&prime; . (V.377)
</p>
<p>Thus, the different rows of the matrix C = (cnm) have to be normalized and
pairwise orthogonal.
</p>
<p>(c) If A is a projector, it must apply that
</p>
<p>A2 =
&sum;
</p>
<p>n,m
</p>
<p>cnm |n m |
&sum;
</p>
<p>n&prime;,m &prime;
</p>
<p>cn&prime;m &prime; |n&prime; m &prime; | = A =
&sum;
</p>
<p>n,m
</p>
<p>cnm |n m | ,
</p>
<p>(V.378)
</p>
<p>and from part (a), cnm = c&lowast;mn . It follows that
&sum;
n,m
</p>
<p>cnm |n m | =
&sum;
</p>
<p>n,m,n&prime;,m &prime;
cnmcn&prime;m &prime; |n mn&prime; m &prime; |
</p>
<p>= &sum;
n,m,m &prime;
</p>
<p>cnmcmm &prime; |n m &prime; | (V.379)
</p>
<p>or &sum;
</p>
<p>n,m
</p>
<p>cnm |n m | =
&sum;
</p>
<p>n,m
</p>
<p>|n m |
&sum;
</p>
<p>l
</p>
<p>cnlclm . (V.380)
</p>
<p>Hence, we must have &sum;
</p>
<p>l
</p>
<p>cnlclm = cnm . (V.381)
</p>
<p>For the matrix C = (cnm), this means C2 = C .
27. A CONS {|n , n = 1, 2, . . . , N } spans a vector space V .
</p>
<p>(a) Show that each operator A acting in V can be represented as
</p>
<p>A =
&sum;
</p>
<p>n,m
</p>
<p>cnm |n m | . (V.382)
</p>
<p>Solution: If we let act A on a state of the CONS, the result has to be rep-
</p>
<p>resentable as a superposition of the |n (due to the completeness of the</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 499
</p>
<p>system). Thus we have
</p>
<p>A |m =
&sum;
</p>
<p>n
</p>
<p>cnm |n . (V.383)
</p>
<p>Multiplication from the right by m | and summation over m gives (again
due to completeness) the desired result.
</p>
<p>(b) Consider the special case (N = 3):
</p>
<p>A |1 = &minus; |2 ; A |2 = &minus; |3 ; A |3 = &minus; |1 + |2 . (V.384)
</p>
<p>What is the operator A? (Determine the coefficients cnm , i.e. formulate A as
</p>
<p>a linear combination of products |i 
&lang;
 j
</p>
<p>).
Solution: It follows that
</p>
<p>A = &minus; |2 1| &minus; |3 2| &minus; (|1 &minus; |2) 3| . (V.385)
</p>
<p>28. How is the generalized Heisenberg uncertainty relation formulated for each of
</p>
<p>the pairs (x, lx ) ,
(
x, ly
</p>
<p>)
, (x, lz)?
</p>
<p>Solution: We have
</p>
<p>xlx &minus; lx x =

</p>
<p>i
</p>
<p>[
x
(
y&part;z &minus; z&part;y
</p>
<p>)
&minus;
</p>
<p>(
y&part;z &minus; z&part;y
</p>
<p>)
x
]
= 0 (V.386)
</p>
<p>as well as
</p>
<p>xly &minus; ly x = i
[
x (z&part;x &minus; x&part;z)&minus; (z&part;x &minus; x&part;z) x
</p>
<p>]
= 
</p>
<p>i
[xz&part;x &minus; z&part;x x]
</p>
<p>= 
i
</p>
<p>[xz&part;x &minus; zx&part;x &minus; z] = iz, (V.387)
</p>
<p>and analogously
</p>
<p>xlz &minus; lz x = iy. (V.388)
</p>
<p>Hence, it follows due to
</p>
<p>A &middot;B &ge; 1
2
|[A, B]| (V.389)
</p>
<p>immediately that
</p>
<p>x &middot;ly = 0
x &middot;ly &ge; 12
</p>
<p>&lang;[x, ly
]&rang; = 
</p>
<p>2
|z|
</p>
<p>x &middot;lz &ge;
1
</p>
<p>2
</p>
<p>&lang;[x, lz
]&rang; = 
</p>
<p>2
|y| .
</p>
<p>(V.390)
</p>
<p>29. For the Pauli matrices, the following uncertainty relation holds:
</p>
<p>xy &ge; |z| . (V.391)</p>
<p/>
</div>
<div class="page"><p/>
<p>500 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>For which normalized states  =
(
</p>
<p>a
</p>
<p>b
</p>
<p>)
is the right-hand side a minimum/
</p>
<p>maximum?
</p>
<p>Solution: We have (see also (V.351))
</p>
<p>z =
(
</p>
<p>a&lowast; b&lowast;
) ( 1 0
</p>
<p>0 &minus;1
</p>
<p>)(
a
</p>
<p>b
</p>
<p>)
= |a|2 &minus; |b|2 = 2 |a|2 &minus; 1 (V.392)
</p>
<p>(because of the normalization |a|2 + |b|2 = 1). |z| is maximal (|z| = 1)
for |a| = 0, 1; and minimal (z = 0) for |a| = &plusmn; 1&radic;
</p>
<p>2
.
</p>
<p>30. What is the generalized uncertainty relation for H and p?
</p>
<p>Solution: It is
</p>
<p>H &middot;pi &ge;
1
</p>
<p>2
|[H, pi ]| . (V.393)
</p>
<p>With
</p>
<p>H pi &minus; pi H = V pi &minus; pi V = &minus; (pi V ) (V.394)
</p>
<p>it follows:
</p>
<p>H &middot;pi &ge;

</p>
<p>2
</p>
<p>
&part;V
</p>
<p>&part;xi
</p>
<p> . (V.395)
</p>
<p>31. The position operator in the Heisenberg picture, xH , is given by
</p>
<p>xH = ei
t H
 xe&minus;i
</p>
<p>t H
 . (V.396)
</p>
<p>How does this operator depend explicitly on time? The potential is assumed to
</p>
<p>be constant, dV
dx
</p>
<p>= 0. Hint: Use the equation
</p>
<p>ei A Be&minus;i A = B+i [A, B]+ i
2
</p>
<p>2! [A, [A, B]]+
i3
</p>
<p>3! [A, [A, [A, B]]]+&middot; &middot; &middot; (V.397)
</p>
<p>or
</p>
<p>i
d
</p>
<p>dt
xH = [xH , H ] (V.398)
</p>
<p>(or both for practice).
</p>
<p>32. A Hamiltonian H depends on a parameter q, H = H (q). In addition, E (q) is
a nondegenerate eigenvalue and | (q) the corresponding eigenvector:
</p>
<p>H (q) | (q) = E (q) | (q) . (V.399)
</p>
<p>Show that
&part;E (q)
</p>
<p>&part;q
=  (q)| &part;H (q)
</p>
<p>&part;q
| (q) . (V.400)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 501
</p>
<p>(This equation is also called the Feynman&ndash;Hellmann theorem.)
</p>
<p>Solution: It holds that
</p>
<p> (q)| H (q) | (q) = E (q) . (V.401)
</p>
<p>We differentiate both sides with respect to q and obtain
</p>
<p>&part;E (q)
</p>
<p>&part;q
=
</p>
<p>&lang;
&part;
</p>
<p>&part;q
 (q)
</p>
<p> H (q) | (q) +  (q)|
&part;H (q)
</p>
<p>&part;q
| (q) +  (q)| H (q)
</p>
<p>
&part;
</p>
<p>&part;q
 (q)
</p>
<p>&rang;
</p>
<p>=  (q)| &part;H (q)
&part;q
</p>
<p>| (q) + E (q)
&lang;
&part;
</p>
<p>&part;q
 (q) | (q) + E (q)  (q)
</p>
<p>
&part;
</p>
<p>&part;q
 (q)
</p>
<p>&rang;
</p>
<p>=  (q)| &part;H (q)
&part;q
</p>
<p>| (q) + E (q) &part;
&part;q
</p>
<p> (q) | (q) =  (q)| &part;H (q)
&part;q
</p>
<p>| (q) ,
(V.402)
</p>
<p>due to
&part;
</p>
<p>&part;q
 (q) | (q) = &part;
</p>
<p>&part;q
1 = 0. (V.403)
</p>
<p>33. {|n} is a CONS. Every solution of the SEq may be written as
</p>
<p>| =
&sum;
</p>
<p>l
</p>
<p>al |l (V.404)
</p>
<p>and every operator A as
</p>
<p>A =
&sum;
</p>
<p>mn
</p>
<p>cmn |n m| . (V.405)
</p>
<p>Can the non-Hermitian operator A (i.e. cmn 	= c&lowast;nm for at least one pair n,m)
have a real expectation value (for arbitrary states |) under these conditions?
Solution: Let
</p>
<p>A&dagger; =
&sum;
</p>
<p>mn
</p>
<p>c&lowast;mn |m n| =
&sum;
</p>
<p>mn
</p>
<p>c&lowast;nm |n m| 	= A (V.406)
</p>
<p>or
</p>
<p>cmn 	= c&lowast;nm . (V.407)
</p>
<p>For the expectation value, it holds that
</p>
<p>A =
&sum;
</p>
<p>mnll &prime;
</p>
<p>a&lowast;l l| cmn |n m| al &prime;
l &prime;
&rang;
=
</p>
<p>&sum;
</p>
<p>mnll &prime;
</p>
<p>a&lowast;l cmnnlml &prime;al &prime; =
&sum;
</p>
<p>mn
</p>
<p>a&lowast;n cmnam .
</p>
<p>(V.408)
</p>
<p>This expectation value must be real, i.e.
</p>
<p>A =
&sum;
</p>
<p>mn
</p>
<p>a&lowast;n cmnam = A&lowast; =
&sum;
</p>
<p>mn
</p>
<p>anc
&lowast;
mna
</p>
<p>&lowast;
m =
</p>
<p>&sum;
</p>
<p>mn
</p>
<p>amc
&lowast;
nma
</p>
<p>&lowast;
n . (V.409)</p>
<p/>
</div>
<div class="page"><p/>
<p>502 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>If the last equation has to hold for all possible values of {an} (i.e. for all solutions
of the SEq), we must have
</p>
<p>cmn = c&lowast;nm . (V.410)
</p>
<p>Thus, we have a contradiction.
</p>
<p>34. We consider the Hamiltonian H = 1 + ay , already introduced in the exercises
for Chap. 8.
</p>
<p>(a) What is the expected result of the measurement of the x-component of the
</p>
<p>spin in the state |t  with |0 =
(
</p>
<p>1
</p>
<p>0
</p>
<p>)
?
</p>
<p>Solution: The x-component of the spin is represented by the operator sx =

</p>
<p>2
x . Since |t  is normalized, the expectation value is given by sx  =
</p>
<p>t |sx |t  with |t  = e&minus;i t/
(
</p>
<p>cos at

</p>
<p>sin at

</p>
<p>)
(see Chap. 8), i.e.
</p>
<p>sx  = 2
(
</p>
<p>cos at

</p>
<p>sin at

</p>
<p>) (0 1
1 0
</p>
<p>)(
cos at
</p>
<p>
</p>
<p>sin at

</p>
<p>)
</p>
<p>= 
2
</p>
<p>2 cos at

</p>
<p>sin at

= 
</p>
<p>2
sin 2at
</p>
<p>
.
</p>
<p>(V.411)
</p>
<p>(b) What is the uncertainty sx in this state?
</p>
<p>Solution: It is 2sx =
&lang;
s2x
&rang;
&minus; sx 2 . Because of s2x = 
</p>
<p>2
</p>
<p>4
</p>
<p>(
1 0
</p>
<p>0 1
</p>
<p>)
, we have
</p>
<p>&lang;
s2x
&rang;
= 2
</p>
<p>4
and it follows that
</p>
<p>sx =
&radic;&lang;
</p>
<p>s2x
&rang;
&minus; sx 2 =
</p>
<p>
</p>
<p>2
</p>
<p>&radic;
1 &minus; sin2 2at
</p>
<p>
= 
</p>
<p>2
</p>
<p>cos
2at
</p>
<p>
</p>
<p> . (V.412)
</p>
<p>(c) Calculate the commutator
[
sx , sy
</p>
<p>]
and formulate the uncertainty relation for
</p>
<p>the observables sx and sy for arbitrary times t .
</p>
<p>Solution: start with
[
sx , sy
</p>
<p>]
= isz . With this, the generalized uncertainty
</p>
<p>relation reads
</p>
<p>sxsy &ge;

</p>
<p>2
|sz| . (V.413)
</p>
<p>We calculate the two sides separately. First the left side: We know sx . For
</p>
<p>sy , we calculate first
&lang;
sy
&rang;
. It is
</p>
<p>&lang;
sy
&rang;
= 
</p>
<p>2
</p>
<p>(
cos at
</p>
<p>
sin at
</p>
<p>
</p>
<p>) (0 &minus;i
i 0
</p>
<p>)(
cos at
</p>
<p>
</p>
<p>sin at

</p>
<p>)
= 0 (V.414)
</p>
<p>and therefore 2sy =
&lang;
s2y
&rang;
=
</p>
<p>&lang;

</p>
<p>2
</p>
<p>4
</p>
<p>(
1 0
</p>
<p>0 1
</p>
<p>)&rang;
= 2
</p>
<p>4
. All together, it follows that</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 503
</p>
<p>sxsy =
2
</p>
<p>4
</p>
<p>cos
2at
</p>
<p>
</p>
<p> . (V.415)
</p>
<p>Now the right side: we have:
</p>
<p>
</p>
<p>2
|sz| =
</p>
<p>
</p>
<p>2
</p>
<p>
</p>
<p>(
cos
</p>
<p>at
</p>
<p>
sin
</p>
<p>at
</p>
<p>
</p>
<p>)

</p>
<p>2
</p>
<p>(
1 0
</p>
<p>0 &minus;1
</p>
<p>)
 cos
</p>
<p>at
</p>
<p>
</p>
<p>sin
at
</p>
<p>
</p>
<p>


</p>
<p>= 
2
</p>
<p>4
</p>
<p>cos2
at
</p>
<p>
&minus; sin2 at
</p>
<p>
</p>
<p> =
2
</p>
<p>4
</p>
<p>cos
2at
</p>
<p>
</p>
<p> .
</p>
<p>(V.416)
</p>
<p>Hence, both sides are equal (so to speak the closest realization of the uncer-
</p>
<p>tainty relation).
</p>
<p>35. Given an eigenvalue problem A |am = am |am ({|am} is a CONS); we can
define a function of the operator by
</p>
<p>F (A) |am := F (am) |am . (V.417)
</p>
<p>(a) Show that:
</p>
<p>F (A) =
&sum;
</p>
<p>m
</p>
<p>F (am) Pm (V.418)
</p>
<p>with Pm = |am am |.
Solution: We have
</p>
<p>F (A) |am = F (am) |am &rarr; F (A) |am am | = F (am) |am am | ,
(V.419)
</p>
<p>and due to the completeness of the eigenvectors, it follows that
</p>
<p>F (A) =
&sum;
</p>
<p>m
</p>
<p>F (am) |am am | =
&sum;
</p>
<p>m
</p>
<p>F (am) Pm . (V.420)
</p>
<p>(b) Show that if F (a) is real for all eigenvalues am , then F (A) is self-adjoint.
</p>
<p>Solution: Let F&lowast; (am) = F (am). Then it follows that
</p>
<p>[F (A)]&dagger; =
&sum;
</p>
<p>m
</p>
<p>F&lowast; (am) Pm =
&sum;
</p>
<p>m
</p>
<p>F (am) Pm = F (A) . (V.421)
</p>
<p>36. What are the conditions which the elements of a two-dimensional normal matrix
</p>
<p>have to fulfill?
</p>
<p>Solution: With
</p>
<p>A =
(
</p>
<p>a b
</p>
<p>c d
</p>
<p>)
; AA&dagger; = A&dagger; A, (V.422)</p>
<p/>
</div>
<div class="page"><p/>
<p>504 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>it follows that (
a b
</p>
<p>c d
</p>
<p>)(
a&lowast; c&lowast;
</p>
<p>b&lowast; d&lowast;
</p>
<p>)
=
</p>
<p>(
a&lowast; c&lowast;
</p>
<p>b&lowast; d&lowast;
</p>
<p>)(
a b
</p>
<p>c d
</p>
<p>)
, (V.423)
</p>
<p>and thus
</p>
<p>(
aa&lowast; + bb&lowast; ac&lowast; + bd&lowast;
ca&lowast; + db&lowast; cc&lowast; + dd&lowast;
</p>
<p>)
=
</p>
<p>(
a&lowast;a + c&lowast;c a&lowast;b + c&lowast;d
b&lowast;a + d&lowast;c b&lowast;b + d&lowast;d
</p>
<p>)
. (V.424)
</p>
<p>This gives the two equations
</p>
<p>bb&lowast; = c&lowast;c; ac&lowast; + bd&lowast; = a&lowast;b + c&lowast;d (V.425)
</p>
<p>or
</p>
<p>bb&lowast; = c&lowast;c; (a &minus; d) c&lowast; =
(
a&lowast; &minus; d&lowast;
</p>
<p>)
b. (V.426)
</p>
<p>These two equations have the particular solution a = d and bb&lowast; = c&lowast;c. For
a 	= d, the solution reads
</p>
<p>A =
(
</p>
<p>a b
a&minus;d
</p>
<p>a&lowast;&minus;d&lowast; b
&lowast; d
</p>
<p>)
. (V.427)
</p>
<p>37. Given the matrix
</p>
<p>A =
(
</p>
<p>0 2
</p>
<p>1 0
</p>
<p>)
;  	= 0. (V.428)
</p>
<p>(a) Is A normal?
</p>
<p>Solution: We have
</p>
<p>A&dagger; =
(
</p>
<p>0 1
</p>
<p>&lowast;2 0
</p>
<p>)
(V.429)
</p>
<p>and therefore
</p>
<p>AA&dagger; =
(
</p>
<p>0 2
</p>
<p>1 0
</p>
<p>)(
0 1
</p>
<p>&lowast;2 0
</p>
<p>)
=
</p>
<p>(
||4 0
</p>
<p>0 1
</p>
<p>)
(V.430)
</p>
<p>and
</p>
<p>A&dagger; A =
(
</p>
<p>0 1
</p>
<p>&lowast;2 0
</p>
<p>)(
0 2
</p>
<p>1 0
</p>
<p>)
=
</p>
<p>(
1 0
</p>
<p>0 ||4
)
. (V.431)
</p>
<p>Hence, the matrix is not normal for || 	= 1.
(b) Show that A is diagonalizable for almost all , but not by a unitary transfor-
</p>
<p>mation.
</p>
<p>Solution: A has the eigenvalues &plusmn;. A is diagonalizable, if there is a matrix
B =
</p>
<p>(
a b
</p>
<p>c d
</p>
<p>)
such that</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 505
</p>
<p>(
a b
</p>
<p>c d
</p>
<p>)(
0 2
</p>
<p>1 0
</p>
<p>)
=
</p>
<p>(
 0
</p>
<p>0 &minus;
</p>
<p>)(
a b
</p>
<p>c d
</p>
<p>)
with det B = ad &minus; bc 	= 0
</p>
<p>(V.432)
</p>
<p>holds. From the last equation, it follows that
</p>
<p>(
b a2
</p>
<p>d c2
</p>
<p>)
=
</p>
<p>(
a b
</p>
<p>&minus;c &minus;d
</p>
<p>)
. (V.433)
</p>
<p>This gives the two equations b = a and d = &minus;c. From them, it follows
that
</p>
<p>B =
(
</p>
<p>a a
</p>
<p>c &minus;c
</p>
<p>)
with det B = &minus;2ac 	= 0. (V.434)
</p>
<p>Hence, neither a nor c must vanish.
</p>
<p>Now we have to determine whether B is unitary. We have
</p>
<p>B B&dagger; =
(
</p>
<p>a a
</p>
<p>c &minus;c
</p>
<p>)(
a&lowast; c&lowast;
</p>
<p>&lowast;a&lowast; &minus;&lowast;c&lowast;
)
</p>
<p>=
( |a|2
</p>
<p>(
1 + ||2
</p>
<p>)
ac&lowast;
</p>
<p>(
1 &minus; ||2
</p>
<p>)
</p>
<p>a&lowast;c
(
1 &minus; ||2
</p>
<p>)
|c|2
</p>
<p>(
1 + ||2
</p>
<p>)
)
; ac 	= 0.
</p>
<p>(V.435)
</p>
<p>One sees directly that B is unitary only for || = 1.
38. In the derivation of the uncertainty relation, the functions must be in the domains
</p>
<p>of definition of the operators and of the operator products involved. If they are
</p>
<p>not, we do not obtain meaningful statements. As an example we consider the
</p>
<p>function:
</p>
<p>f (x) = sin x
2
</p>
<p>x
. (V.436)
</p>
<p>(a) Is f (x) square-integrable?
</p>
<p>Solution: We have
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>f 2(x)dx =
&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>sin2 x2
</p>
<p>x2
dx = 2&radic;. (V.437)
</p>
<p>Hence, the function is square-integrable.
</p>
<p>(b) Is f (x) in the domain of definition of the operator x?
</p>
<p>Solution: No, x f (x) = sin x2 is not square-integrable. In other words: f (x)
is not in the domain of definition of x .
</p>
<p>(c) Can a meaningful uncertainty relation be derived for f (x)?
</p>
<p>Solution: For x , we have</p>
<p/>
</div>
<div class="page"><p/>
<p>506 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>(x)2 =
&lang;
x2
&rang;
&minus; x2 =
</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>sin2 x2dx &minus;
</p>
<p>

</p>
<p>&infin;&int;
</p>
<p>&minus;&infin;
</p>
<p>sin2 x2
</p>
<p>x
dx
</p>
<p>

</p>
<p>2
</p>
<p>(V.438)
</p>
<p>and a similar formulation holds for p. Even if we accepted the value &infin; for&int;
sin2 x2dx , the statementxp &ge; 
</p>
<p>2
is trivially satisfied, i.e. meaningless,
</p>
<p>similar to &infin; &ge; 
2
</p>
<p>.
</p>
<p>(d) Can similar statements be made for the function g(x) = sin x
x
</p>
<p>?
</p>
<p>39. Given two operators A and B which commute with their commutator, [A, [A, B]]
</p>
<p>= [B, [A, B]] = 0. Show that:
[
B, An
</p>
<p>]
= n [B, A] An&minus;1. (V.439)
</p>
<p>Solution: We use mathematical induction. The equation clearly is valid for n = 1.
If it applies for n, it follows for n + 1:
</p>
<p>[
B, An+1
</p>
<p>]
=B An+1&minus;An+1 B=B An+1 &minus; AB An + AB An&minus;An+1 B
</p>
<p>= [B, A] An+A [B, An]= [B, A] An + n A [B, A] An&minus;1 = (n + 1) [B, A] An.
(V.440)
</p>
<p>40. Show that the momentum operator is given in the coordinate representation by
</p>
<p>p = 
i
</p>
<p>d
dx
</p>
<p>. Make use only of the commutator [x, p] = i and derive, making use
of the previous exercise, the result:
</p>
<p>[p, f (x)] = 
i
</p>
<p>d f (x)
</p>
<p>dx
. (V.441)
</p>
<p>Solution: We expand the function f (x) in a Taylor series e.g. around zero:
</p>
<p>f (x) =
&sum;
</p>
<p>n
</p>
<p>f (n)(0) x
n
</p>
<p>n! . Then it holds that
</p>
<p>[p, f (x)] =
&sum;
</p>
<p>n
</p>
<p>f (n)(0)
1
</p>
<p>n!
[
</p>
<p>p, xn
]
. (V.442)
</p>
<p>We transform the commutator on the right side by means of (V.439) and obtain
</p>
<p>with [p, xn] = n [p, x] xn&minus;1:
</p>
<p>[p, f (x)] =
&sum;
</p>
<p>n
</p>
<p>f (n)(0)
1
</p>
<p>n!n [p, x] x
n&minus;1 = 
</p>
<p>i
</p>
<p>&sum;
</p>
<p>n
</p>
<p>f (n)(0)
xn&minus;1
</p>
<p>(n &minus; 1)! =

</p>
<p>i
</p>
<p>d f (x)
</p>
<p>dx
</p>
<p>(V.443)</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 507
</p>
<p>i.e. (V.441). The form of this equation suggests the ansatz p =  d
dx
</p>
<p>; we insert
</p>
<p>and obtain with the auxiliary function g(x), because of
</p>
<p>[p, f (x)] g(x) = 
i
</p>
<p>d f (x)
</p>
<p>dx
&middot; g(x) (V.444)
</p>
<p>initially:
</p>
<p>[p, f (x)] g(x) =  d
dx
</p>
<p>f (x)g(x)&minus;  f (x) d
dx
</p>
<p>g(x) = g(x) d
dx
</p>
<p>f (x). (V.445)
</p>
<p>Finally, the constant  is determined by the comparison with (V.444), giving 
i
,
</p>
<p>and we obtain the desired result.
</p>
<p>41. Given two operators A and B which commute with their commutator, [A, [A, B]]
</p>
<p>= [B, [A, B]] = 0. Show that:
</p>
<p>eA+B = eAeBe&minus; 12 [A,B]. (V.446)
</p>
<p>This is a special case of the Baker-Campbell-Hausdorff formula (relation, the-
</p>
<p>orem). The general case considers eA+B for two operators, which do not have
to commute with their commutator (this is used e.g. in (V.397)). By the way,
</p>
<p>these authors published their work in 1900, well before the birth of quantum
</p>
<p>mechanics.
</p>
<p>(a) First, prove the equation
</p>
<p>[
B, ex A
</p>
<p>]
= ex A [B, A] x . (V.447)
</p>
<p>Solution: With the power series expansion of the e-function, we have
</p>
<p>[
B, ex A
</p>
<p>]
= &sum; 1
</p>
<p>n! x
n [B, An]
</p>
<p>ex A [B, A] x = &sum; 1
n! x
</p>
<p>n+1 An [B, A] .
</p>
<p>(V.448)
</p>
<p>We compare same powers of x :
</p>
<p>1
</p>
<p>(n + 1)! x
n+1 [B, An+1
</p>
<p>]
= 1
</p>
<p>n! x
n+1 An [B, A] . (V.449)
</p>
<p>This equation is already proved, cf. (V.439).
</p>
<p>(b) Define
</p>
<p>G (x) = ex Aex B (V.450)
</p>
<p>and show the following equation holds:</p>
<p/>
</div>
<div class="page"><p/>
<p>508 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>dG
</p>
<p>dx
= (A + B + [A, B] x)G. (V.451)
</p>
<p>Integrate this equation.
</p>
<p>Solution: Taking the derivative of G gives
</p>
<p>dG
</p>
<p>dx
= d
</p>
<p>dx
ex Aex B = Aex Aex B + ex Aex B B. (V.452)
</p>
<p>The second term on the right side is transformed by means of the result of
</p>
<p>part (a):
</p>
<p>ex Aex B B = ex A Bex B =
(
Bex A + ex A [A, B] x
</p>
<p>)
ex B
</p>
<p>= (B + [A, B] x) ex Aex B&minus; (V.453)
</p>
<p>It follows that
dG
</p>
<p>dx
= (A + B + [A, B] x)G. (V.454)
</p>
<p>We can integrate this equation directly, since the operator (A+B) commutes
with [A, B]:
</p>
<p>G(x) = G0e(A+B)x+
1
2
</p>
<p>[A,B]x2 . (V.455)
</p>
<p>Due to G(x = 0) = 1, the integration constant G0 is 1, and the final result
follows for x = 1:
</p>
<p>eAeB = eA+B+ 12 [A,B]. (V.456)
</p>
<p>V.13 Exercises, Chap. 14
</p>
<p>1. Given an observable A and a state |. Show by means of Postulates (2.1) and
(2.2) that the expected result of a measurement of A is given by A = | A |.
To simplify the discussion, we consider an observable A whose eigenvalues are
</p>
<p>discrete and non-degenerate and whose eigenvectors form a CONS, A |n =
an |n.
Solution: Let | have the form | = &sum;n cn |n, where at least one coefficient
is not zero. Then we know from Postulate 2.1 that the probability of finding |
in the state |n (i.e. of measuring an) is given by
</p>
<p>pn = |n ||2 = | Pn | (V.457)
</p>
<p>where Pn = |n n| is the projection operator onto the subspace n. As always,
we assume that | is normalized.
As we have already seen in Chap. 9, this is where the expectation value comes</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 509
</p>
<p>into play. If one has measured the quantity an on a single system, one cannot
</p>
<p>draw any definite conclusions about the state prior to the measurement, since
</p>
<p>we are dealing with probabilities. To get more information, one can&mdash;at least in
</p>
<p>principle&mdash;proceed by preparing an ensemble, i.e. by preparing many individual
</p>
<p>systems so that they are all in the same state |. Now one determines which
of the states |n is occupied by each member of the ensemble. If the number
of measurements N is very large, we have an experimental statement about the
</p>
<p>expectation value of A in the state |, namely
</p>
<p>A = lim
N&rarr;&infin;
</p>
<p>1
</p>
<p>N
</p>
<p>N&sum;
</p>
<p>m=1
Am (V.458)
</p>
<p>where Am is the result of the mth measurement. These results vary from one
</p>
<p>measurement to the next, but always show one of the values an . The theoretical
</p>
<p>value (expectation value) is (we assume no degeneracy):
</p>
<p>A =
&sum;
</p>
<p>n
</p>
<p>pnan =
&sum;
</p>
<p>n
</p>
<p> |n an n | = | A | (V.459)
</p>
<p>where we have used the spectral decomposition A = &sum;n |n an n| (see
Chap. 13).
</p>
<p>2. Show that the operator sx + sz is Hermitian, but does not represent a measurable
physical quantity if understood literally, i.e., as the instruction to measure the
</p>
<p>x-component plus (and) the z-component of the spin. The spin matrices si are
</p>
<p>related to the Pauli matrices i by si = 2i .
Solution: The operator sx + sz = 2
</p>
<p>(
1 1
</p>
<p>1 &minus;1
</p>
<p>)
is obviously Hermitian. Its eigen-
</p>
<p>values are &plusmn; &radic;
2
</p>
<p>(check yourself); and, according to our postulates, one obtains
</p>
<p>one of theses values as the result of a measurement. If, on the other hand, we
</p>
<p>measure sx and sz separately and then add the results, the measurement gives for
</p>
<p>each of the two operators &plusmn;
2
</p>
<p>, and thus in the sum one of the three results , 0
</p>
<p>or &minus;; i.e. values which clearly do not match up with &plusmn; &radic;
2
.
</p>
<p>Of course, the core of the problem is that the two operators sx and sz do not
</p>
<p>commute.
</p>
<p>3. (An example concerning projections, probabilities and expectation values.) The
</p>
<p>angular momentum operator L for angular momentum 1 can be represented in
</p>
<p>the vector space C3 by the following matrices (see Chap. 16, Vol. 2):
</p>
<p>L x =
&radic;
2
</p>
<p>

</p>
<p>0 1 0
</p>
<p>1 0 1
</p>
<p>0 1 0
</p>
<p>
 ; L y =
</p>
<p>&radic;
2
</p>
<p>

</p>
<p>0 &minus;i 0
i 0 &minus;i
0 i 0
</p>
<p>
 ; L z = 
</p>
<p>

</p>
<p>1 0 0
</p>
<p>0 0 0
</p>
<p>0 0 &minus;1
</p>
<p>
 .
</p>
<p>(V.460)</p>
<p/>
</div>
<div class="page"><p/>
<p>510 Appendix V: Exercises and Solutions to Chaps. 1&ndash;14
</p>
<p>(a) Which measured results are possible in a measurement of L i (i = x, y, z)?
Solution: The measured result must be one of the eigenvalues of L i . For L z ,
</p>
<p>one sees directly that the eigenvalues are+, 0,&minus; (diagonal elements). The
calculation shows (check yourself) that L x and L y have these eigenvalues,
</p>
<p>also.
</p>
<p>(b) What are the corresponding eigenvectors for L z?
</p>
<p>Solution: The calculation shows (check yourself) that the eigenvectors asso-
</p>
<p>ciated with the eigenvalues +, 0,&minus; are
</p>
<p>|+ =
</p>
<p>

</p>
<p>1
</p>
<p>0
</p>
<p>0
</p>
<p>
 ; |0 =
</p>
<p>

</p>
<p>0
</p>
<p>1
</p>
<p>0
</p>
<p>
 ; |&minus; =
</p>
<p>

</p>
<p>0
</p>
<p>0
</p>
<p>1
</p>
<p>
 . (V.461)
</p>
<p>(c) What are the probabilities of measuring the results +, 0,&minus; on the state
</p>
<p>| =
</p>
<p>

</p>
<p>1
</p>
<p>i
</p>
<p>&minus;2
</p>
<p>
 ? (V.462)
</p>
<p>Solution: First, we have to normalize the state:
</p>
<p>|norm =
1&radic;
6
</p>
<p>

</p>
<p>1
</p>
<p>i
</p>
<p>&minus;2
</p>
<p>
 . (V.463)
</p>
<p>We see directly that the probabilities for obtaining the measured result +
or 0 or &minus; are given by 1
</p>
<p>6
or 1
</p>
<p>6
or 2
</p>
<p>3
. The probabilities sum up to 1, as indeed
</p>
<p>they must.
</p>
<p>4. Given the state
</p>
<p>|v =
|x1 e&minus;it + |x2 e&minus;2it&radic;
</p>
<p>2
(V.464)
</p>
<p>with normalized and mutually orthogonal states |xi : We measure the x1 com-
ponent of |v . After the measurement, we have
</p>
<p>|n = |x1 e&minus;it . (V.465)
</p>
<p>Illustrate this state reduction by considering the change in the real or imaginary
</p>
<p>part of |.
Solution: By measuring at time T , we cause the state |v to collapse into |x1
(apart from a possible phase). This is the initial value of the time evolution
</p>
<p>after the measurement. Since the energy is sharp, the time behavior is given
</p>
<p>by |x1 e&minus;it . The Hilbert space is two-dimensional (spanned by |x1 and |x2).
Because of the complex prefactors, we thus have a four-dimensional space. To</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix V: Exercises and Solutions to Chaps. 1&ndash;14 511
</p>
<p>-1 -0.5 0 0.5 1
</p>
<p>-0.8
</p>
<p>-0.4
</p>
<p>0
</p>
<p>after measurement
</p>
<p>before measurement
</p>
<p>Fig. V.4 Visualization of the state reduction by a measurement
</p>
<p>enable a visual presentation, we restrict ourselves to considering the real part.
</p>
<p>Before the measurement, we have
</p>
<p>Re |v =
|x1 cost + |x2 cos 2t&radic;
</p>
<p>2
&sim;= 1&radic;
</p>
<p>2
</p>
<p>(
cost
</p>
<p>cos 2t
</p>
<p>)
; (V.466)
</p>
<p>and afterwards,
</p>
<p>Re |n = |x1 cost &sim;=
(
</p>
<p>cost
</p>
<p>0
</p>
<p>)
. (V.467)
</p>
<p>For the visualization we use cos 2t = cos2 t&minus;sin2 t = 2 cos2 t&minus;1. Before
the measurement, Re |v moves on the parabola x2 = 2x21 &minus;1; afterwards, back
and forth on the x1 axis, as indicated in Fig. V.4.</p>
<p/>
</div>
<div class="page"><p/>
<p>Further Reading
</p>
<p>1. J. Audretsch (ed.), Verschr&auml;nkte Welt-Faszination der Quanten (Entangled World-Fascination
</p>
<p>of Quantum, in German) (Wiley-VCH, Weinheim, 2002)
</p>
<p>2. J. Audretsch, Entangled Systems (Wiley-VCH, Weinheim, 2007)
</p>
<p>3. J.-L. Basdevant, J. Dalibard, Quantum Mechanics (Springer, Berlin, 2002)
</p>
<p>4. D.R. Bes, Quantum Mechanics (Springer, Berlin, 2004)
</p>
<p>5. B.H. Bransden, C.J. Joachain, Quantum Mechanics (Pearson Education Limited, Harlow, 2000)
</p>
<p>6. C. Cohen-Tannoudji, B. Diu, F. Lalo&euml;, Quantum Mechanics, vols. 1 &amp; 2 (Hermann Paris/Wiley,
</p>
<p>New York, 1977)
</p>
<p>7. F. Embacher, Homepage with much material about quantum theory (also for school), University
</p>
<p>Vienna (2012), http://homepage.univie.ac.at/franz.embacher/
</p>
<p>8. R.P. Feynman, R.B. Leighton, M. Sand, Quantum Mechanics. The Feynman Lectures on
</p>
<p>Physics, vol. 3 (Addison-Wesley Reading, Massachusetts, 1965)
</p>
<p>9. T. Flie&szlig;bach, Quantenmechanik (Quantum Mechanics, in German), 3rd edn. (Spektrum
</p>
<p>Akademischer Verlag, Heidelberg, 2000)
</p>
<p>10. K. Gottfried, T.-M. Yan, Quantum Mechanics: Fundamentals (Springer, New York, 2006)
</p>
<p>11. K.T. Hecht, Quantum Mechanics (Springer, New York, 2000)
</p>
<p>12. C.J. Isham, Quantum Theory-Mathematical and Structural Foundations (Imperial College
</p>
<p>Press, London, 2008)
</p>
<p>13. T. Lancaster, S.J. Blundell, Quantum Field Theory for the Gifted Amateur (Oxford University
</p>
<p>Press, Oxford, 2014)
</p>
<p>14. R.D. Klauber, Student Friendly Quantum Field Theory, 2nd edn. (Sandrove Press, Fairfield,
</p>
<p>2015)
</p>
<p>15. M. Le Bellac, Quantum Physics (Cambridge University Press, Cambridge, 2006)
</p>
<p>16. H. L&uuml;th, Quantenphysik in der Nanowelt (Quantum Physics in the Nanoworld, in German)
</p>
<p>(Springer, Berlin, 2009)
</p>
<p>17. E. Merzbacher, Quantum Mechanics, 3rd edn. (Wiley, New York, 1998)
</p>
<p>18. A. Messiah, Quantum Mechanics, vols. 1 &amp; 2 (North-Holland Publishing Company, Amster-
</p>
<p>dam, 1964)
</p>
<p>19. M&uuml;nchener Internetprojekt zur Lehrerfortbildung in Quantenmechanik (Munich internet
</p>
<p>project for teacher training in quantum mechanics, in German) (2012), http://homepages.
</p>
<p>physik.uni-muenchen.de/milq/
</p>
<p>20. G. M&uuml;nster, Quantentheorie (Quantum Theory, in German) (Walter de Gruyter, Berlin, 2006)
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>513</p>
<p/>
<div class="annotation"><a href="http://homepage.univie.ac.at/franz.embacher/">http://homepage.univie.ac.at/franz.embacher/</a></div>
<div class="annotation"><a href="http://homepages.physik.uni-muenchen.de/milq/">http://homepages.physik.uni-muenchen.de/milq/</a></div>
<div class="annotation"><a href="http://homepages.physik.uni-muenchen.de/milq/">http://homepages.physik.uni-muenchen.de/milq/</a></div>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>514 Further Reading
</p>
<p>21. W. Nolting, Grundkurs Theoretische Physik 5, Quantenmechanik, Teil 1: Grundlagen und
</p>
<p>Quantenmechanik Teil 2: Methoden und Anwendungen (Quantum Mechanics, Part 1: Basics
</p>
<p>and Part 2: Methods and Applications, in German) (Verlag Zimmermann-Neufang, Ulmen,
</p>
<p>1992)
</p>
<p>22. A. Peres, Quantum Theory-Concepts and Methods (Kluwer Academic Publishers, Doordrecht,
</p>
<p>1995)
</p>
<p>23. A.I.M. Rae, Quantum Mechanics, 5th edn. (Taylor and Francis, New York, 2008)
</p>
<p>24. H. Rollnik, Quantentheorie 1 &amp; 2 (Quantum Theory 1 &amp; 2, in German), 2nd edn. (Springer,
</p>
<p>Berlin, 2003)
</p>
<p>25. H. Schulz, Physik mit Bleistift (Physics with a Pencil, in German), 4th edn. (Verlag Harri
</p>
<p>Deutsch, Frankfurt am Main, 2001)
</p>
<p>26. F. Schwabl, Quantum Mechanics, 3rd edn. (Springer, Berlin, 2002)
</p>
<p>27. N. Zettili, Quantum Mechanics, Concepts and Applications, 2nd edn. (Wiley, New York, 2009)</p>
<p/>
</div>
<div class="page"><p/>
<p>Index of Volume 1
</p>
<p>A
</p>
<p>Adjoint, 43, 251
</p>
<p>Anticommutator, 37
</p>
<p>Antiparticle, 429
</p>
<p>B
</p>
<p>Baker-Campbell-Hausdorff formula, 186
</p>
<p>BB84 protocol, 133
</p>
<p>Beam splitter, 73
</p>
<p>phase shift, 317
</p>
<p>Born&rsquo;s rule, 190
</p>
<p>Bra, 43
</p>
<p>Bra-Ket notation, 43
</p>
<p>C
</p>
<p>Commutator, 37
</p>
<p>Completeness
</p>
<p>of a vector system, 45
</p>
<p>of the Hilbert space, 142, 144
</p>
<p>Complete orthonormal system, 46
</p>
<p>Conserved quantity, 120
</p>
<p>Constant of the motion, 120
</p>
<p>Continuity equation, 95, 333
</p>
<p>Correspondence principle, 36
</p>
<p>Covariance, 419
</p>
<p>CPT theorem, 433
</p>
<p>D
</p>
<p>De Broglie relations, 5
</p>
<p>Degeneracy, 33
</p>
<p>Degree of degeneracy, 173
</p>
<p>Delayed-choice experiments, 83, 327
</p>
<p>Delta function, 274
</p>
<p>Dirac adjoint, 414
</p>
<p>Dirac equation, 408
</p>
<p>Dirac matrices, 420
</p>
<p>Dirac sea, 431
</p>
<p>Dispersion relation, 5
</p>
<p>Dyadic product, 46, 249
</p>
<p>Dyson series, 348
</p>
<p>E
</p>
<p>Eigendifferential, 154
</p>
<p>Eigenfunction, 33
</p>
<p>Eigenvalue, 33
</p>
<p>Eigenvalue problem, 33, 51
</p>
<p>Eigenvector, 33
</p>
<p>Eigenvector-eigenvalue rule, 192
</p>
<p>Einstein convention, 382
</p>
<p>Energy, negative, 429
</p>
<p>Ensemble, 26
</p>
<p>Euler&ndash;Lagrange equation, 388, 391
</p>
<p>Expansion theorem, 142, 144
</p>
<p>Expectation value, 91, 109
</p>
<p>time behavior, 119
</p>
<p>F
</p>
<p>Fapp, 130
</p>
<p>Feynman-St&uuml;ckelberg interpretation, 432
</p>
<p>Field tensor, electromagnetic, 400
</p>
<p>Four-gradient, 384
</p>
<p>Fourier series, 281
</p>
<p>Fourier transform
</p>
<p>discrete, 282
</p>
<p>quantum, 282
</p>
<p>Fourier transformation, 273
</p>
<p>Four-momentum, 383
</p>
<p>Four-potential, 384
</p>
<p>Four-vector, 376
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>515</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>516 Index of Volume 1
</p>
<p>contravariant, 378
</p>
<p>covariant, 378
</p>
<p>inner product, 379
</p>
<p>G
</p>
<p>Gaussian distribution, 65
</p>
<p>G-factor, 427
</p>
<p>Green&rsquo;s function, 281
</p>
<p>H
</p>
<p>Hadamard
</p>
<p>matrix, 282
</p>
<p>transformation, 83
</p>
<p>Hamilton equations, 388
</p>
<p>Hamiltonian density, 391
</p>
<p>Hamlet effect, 319
</p>
<p>Hardy&rsquo;s experiment, 84
</p>
<p>Heisenberg cut, 197
</p>
<p>Hidden variable, 27
</p>
<p>Hilbert space, 142, 144, 268
</p>
<p>extended, 155
</p>
<p>Hole theory, 430
</p>
<p>I
</p>
<p>Inner product, 44
</p>
<p>Interaction-free quantum measurement, 76,
</p>
<p>322
</p>
<p>Interaction picture, 346
</p>
<p>Interpretation
</p>
<p>ensemble, 27
</p>
<p>minimal, 194
</p>
<p>standard, 194
</p>
<p>K
</p>
<p>Ket, 43
</p>
<p>Klein&ndash;Gordon equation, 36, 406
</p>
<p>Kronecker symbol, 45
</p>
<p>generalized, 156
</p>
<p>L
</p>
<p>Lagrange function, 388
</p>
<p>Lagrangian density, 390
</p>
<p>Lenz vector, 38
</p>
<p>Levi-Civita symbol, 246
</p>
<p>Linearity of the Schr&ouml;dinger equation, 29
</p>
<p>Lorentz boost, 376
</p>
<p>M
</p>
<p>Mach&ndash;Zehnder interferometer, 73
</p>
<p>Malus, law of, 22
</p>
<p>Matrix element, 147, 149
</p>
<p>Matrix mechanics, 143, 145
</p>
<p>Maxwell equations, 397
</p>
<p>Mean value, 91, 109
</p>
<p>Measurement, 48, 128, 180, 195
</p>
<p>continuous, 371
</p>
<p>ideal, 191, 369
</p>
<p>indirect, 371
</p>
<p>selective, 371
</p>
<p>Measurement problem, 50
</p>
<p>Minimal coupling, 425
</p>
<p>Momentum,conjugated, 388
</p>
<p>Momentum representation, 159
</p>
<p>N
</p>
<p>Norm, 43, 266
</p>
<p>conserved, 193
</p>
<p>O
</p>
<p>Objective chance, 50
</p>
<p>Observable, 166, 304
</p>
<p>One-time pad, 131
</p>
<p>Operator
</p>
<p>angular momentum, 36
</p>
<p>anti-Hermitian, 166
</p>
<p>antilinear, 165
</p>
<p>bounded, 165, 292
</p>
<p>Hamiltonian, 32, 193
</p>
<p>Hermitian, 105, 117, 166, 293
</p>
<p>linear, 165
</p>
<p>local, 160
</p>
<p>momentum, 35
</p>
<p>normal, 181
</p>
<p>pedigree, 181
</p>
<p>polarization, 51
</p>
<p>position, 35
</p>
<p>positive, 167
</p>
<p>projection, 47, 177
</p>
<p>self-adjoint, 105, 147, 149, 166, 293
</p>
<p>time evolution, 175, 193
</p>
<p>unit&auml;rer, 174
</p>
<p>unitary, 81, 297
</p>
<p>Orthogonality, 44
</p>
<p>Orthonormality, 44
</p>
<p>P
</p>
<p>Parity transformation, 424
</p>
<p>Pauli equation, 427
</p>
<p>Pauli matrices, 53
</p>
<p>Picture</p>
<p/>
</div>
<div class="page"><p/>
<p>Index of Volume 2 517
</p>
<p>Heisenberg, 345
</p>
<p>interaction, 345
</p>
<p>Schr&ouml;dinger, 345
</p>
<p>Poisson bracket, 389, 392
</p>
<p>Polarization
</p>
<p>circular, 18
</p>
<p>elliptical, 18
</p>
<p>linear, 17
</p>
<p>Position representation, 158
</p>
<p>Postulates of quantum mechanics, 351
</p>
<p>Potential
</p>
<p>real, 94
</p>
<p>time-independent, 31
</p>
<p>Potential well
</p>
<p>infinite, 55
</p>
<p>Preparation, 180, 370
</p>
<p>Probability, 195
</p>
<p>Probability amplitude, 190
</p>
<p>Probability current density, 95
</p>
<p>Probability density, 87
</p>
<p>Projection, 47
</p>
<p>Propagator, 175, 193
</p>
<p>Property, 179, 192
</p>
<p>Q
</p>
<p>Quantization, canonical , 393
</p>
<p>Quantum cryptography, 130, 339
</p>
<p>Quantum eraser, 330
</p>
<p>Quantum hopping, 311
</p>
<p>Quantum Zeno effect, 82, 319
</p>
<p>anti-, 319
</p>
<p>R
</p>
<p>Relativistic quantum mechanics, 405
</p>
<p>Rotation, active and passive, 22
</p>
<p>S
</p>
<p>Scalar product, 44, 141, 143, 265
</p>
<p>Schr&ouml;dinger equation, 192
</p>
<p>properties, 29
</p>
<p>stationary, 32
</p>
<p>time-dependent, 10
</p>
<p>time-independent, 31
</p>
<p>Self interference, 77
</p>
<p>Separation of variables, 6
</p>
<p>Special relativity, 375
</p>
<p>Spectral representation, 178
</p>
<p>Spectrum, 33
</p>
<p>degenerate, 173
</p>
<p>Spinor, 414, 424
</p>
<p>Square-integrable, 61
</p>
<p>Standard deviation, 115, 336
</p>
<p>State, 126
</p>
<p>flavor, 100
</p>
<p>improper, 153
</p>
<p>mass, 100
</p>
<p>proper, 153
</p>
<p>unstable, 320
</p>
<p>State function, 11
</p>
<p>State reduction, 92, 191
</p>
<p>State space, 20, 41, 188
</p>
<p>String theory, 70
</p>
<p>Superposition principle, 6, 29, 313
</p>
<p>System
</p>
<p>closed, 367
</p>
<p>isolated, 192, 367
</p>
<p>open, 200, 367
</p>
<p>Systems of units, 205
</p>
<p>T
</p>
<p>Tensor, metric, 381
</p>
<p>Theorem
</p>
<p>of Feynman-Hellmann, 184
</p>
<p>Stone&rsquo;s, 298
</p>
<p>Time evolution, 197
</p>
<p>irreversible, 191
</p>
<p>reversible, 193
</p>
<p>U
</p>
<p>Uncertainty
</p>
<p>s. Standardabweichung, 115
</p>
<p>Uncertainty principle, 300
</p>
<p>Uncertainty relations, 170
</p>
<p>Unitary space, 44, 142, 144
</p>
<p>V
</p>
<p>Variance, 115, 336
</p>
<p>Vector space, 41, 264
</p>
<p>W
</p>
<p>Wave
</p>
<p>plane, 8
</p>
<p>Wave equation, 6
</p>
<p>Wave function, 11
</p>
<p>collapse, 92, 128, 191, 196
</p>
<p>Wave-particle duality, 77
</p>
<p>Which-way information, 77, 327
</p>
<p>Z
</p>
<p>Zero vector, 44, 268</p>
<p/>
</div>
<div class="page"><p/>
<p>Index of Volume 2
</p>
<p>A
</p>
<p>Amplitude amplification, 198
</p>
<p>Angular momentum
</p>
<p>addition, 37
</p>
<p>generalized, 30
</p>
<p>spectrum, 30
</p>
<p>B
</p>
<p>Balmer formula, 49
</p>
<p>Bell&rsquo;s inequality, 91, 309
</p>
<p>Bessel functions, 175
</p>
<p>Bhabha scattering, 463, 473
</p>
<p>Bohr radius, 48
</p>
<p>Born approximation, 177, 179
</p>
<p>Born&rsquo;s series, 179
</p>
<p>Bosons, 33, 137
</p>
<p>C
</p>
<p>Canonical quantization, 397, 398
</p>
<p>Central potential, 44
</p>
<p>Classically allowed region, 5
</p>
<p>Classically forbidden region, 5
</p>
<p>Clebsch&ndash;Gordan coefficients, 38
</p>
<p>Commutation relation, 105
</p>
<p>Completeness
</p>
<p>of quantum mechanics, 89, 203
</p>
<p>Complete system of commuting observ-
</p>
<p>ables, 52
</p>
<p>Conservation of parity, 109
</p>
<p>Conserved quantity, 102
</p>
<p>Constant of the motion, 102
</p>
<p>Contextuality, 52
</p>
<p>Contraction, 447
</p>
<p>Coordinate
</p>
<p>center-of-mass, 43, 263
</p>
<p>relative, 43, 263
</p>
<p>Cross section
</p>
<p>differential, 170
</p>
<p>total, 171
</p>
<p>D
</p>
<p>Darwin term, 71
</p>
<p>Decoherence, 152
</p>
<p>Decomposition
</p>
<p>biorthonormal, 157
</p>
<p>triorthonormal, 158
</p>
<p>Degeneracy
</p>
<p>accidental, 47, 50
</p>
<p>essential, 47, 50
</p>
<p>Density matrix, 119
</p>
<p>Deutsch algorithm, 194
</p>
<p>Dirac equation, 38, 70
</p>
<p>E
</p>
<p>Entangled photons, production, 297
</p>
<p>Entanglement, 80
</p>
<p>EPR, 88
</p>
<p>F
</p>
<p>Fermions, 33, 137
</p>
<p>Feynman
</p>
<p>amplitude, 466
</p>
<p>diagram, 444
</p>
<p>rules, 476
</p>
<p>Fine structure constant, 50
</p>
<p>Four lepton scattering, 462
</p>
<p>G
</p>
<p>Galilean transformations, 100
</p>
<p>&copy; Springer Nature Switzerland AG 2018
</p>
<p>J. Pade, Quantum Mechanics for Pedestrians 1, Undergraduate Lecture
</p>
<p>Notes in Physics, https://doi.org/10.1007/978-3-030-00464-4
</p>
<p>519</p>
<p/>
<div class="annotation"><a href="https://doi.org/10.1007/978-3-030-00464-4">https://doi.org/10.1007/978-3-030-00464-4</a></div>
</div>
<div class="page"><p/>
<p>520 Index of Volume 2
</p>
<p>Gate
</p>
<p>CNOT, 192, 340
</p>
<p>Hadamard, 191
</p>
<p>kickback, 193
</p>
<p>phase shift, 191
</p>
<p>quantum, 190
</p>
<p>G-factor, 478
</p>
<p>Gleason&rsquo;s theorem, 205
</p>
<p>Green&rsquo;s function, 177
</p>
<p>Ground state, 59
</p>
<p>Group velocity, 24, 254
</p>
<p>Grover&rsquo;s algorithm, 196
</p>
<p>Grover, search algorithm, 345
</p>
<p>H
</p>
<p>Hadamard
</p>
<p>matrix, 334
</p>
<p>transformation, 333
</p>
<p>Hardy, experiment, 301
</p>
<p>Harmonic oscillator, 55
</p>
<p>Heisenberg cut, 159, 221
</p>
<p>Helicity, 108
</p>
<p>Helium
</p>
<p>ortho, 140
</p>
<p>para, 140
</p>
<p>Helium atom, 138
</p>
<p>Coulomb energy, 142, 325
</p>
<p>exchange energy, 142, 325
</p>
<p>spectrum, 140
</p>
<p>Hermite polynomials, 62, 243
</p>
<p>Hidden variables, 89, 203
</p>
<p>Hydrogen atom
</p>
<p>algebraic approach, 279
</p>
<p>analytic treatment, 267
</p>
<p>analytical discussion, 47
</p>
<p>eigenfunctions, 241
</p>
<p>fine structure, 70, 295
</p>
<p>hyperfine structure, 74
</p>
<p>perturbation theory, 70
</p>
<p>perturbative calculation, 293
</p>
<p>relativistic energy levels, 73
</p>
<p>spectrum, 49, 72
</p>
<p>I
</p>
<p>Infinitesimal generator, 102
</p>
<p>Interaction
</p>
<p>Hamiltonian, 433
</p>
<p>Lagrangian, 432
</p>
<p>Interpretation
</p>
<p>Bohm, 228, 375
</p>
<p>collapse, 230, 384
</p>
<p>consistent histories, 230, 380
</p>
<p>Copenhagen, 225
</p>
<p>ensemble, 227
</p>
<p>many-worlds, 228, 378
</p>
<p>minimal, 226
</p>
<p>standard, 227
</p>
<p>L
</p>
<p>Laguerre polynomials, 50, 242
</p>
<p>Lamb shift, 75
</p>
<p>Legendre functions, 36
</p>
<p>Lenz vector, 279
</p>
<p>Levi-Civita symbol, 30
</p>
<p>Line (21 cm), 75
</p>
<p>Lippmann-Schwinger equation, 178
</p>
<p>Locality, 84, 89
</p>
<p>Local realism, 96
</p>
<p>M
</p>
<p>Mach&ndash;Zehnder interferometer, 301, 333,
</p>
<p>339
</p>
<p>Mass shell condition, 437
</p>
<p>Mean value, 118
</p>
<p>Mixture, statistical, 120
</p>
<p>M&oslash;ller scattering, 470, 472
</p>
<p>Multipole expansion, 37
</p>
<p>N
</p>
<p>Non-contextuality, 205
</p>
<p>Non-locality, 84
</p>
<p>Normal order, 423
</p>
<p>Nuclear magnetic resonance spectroscopy,
</p>
<p>75
</p>
<p>Number states, 393
</p>
<p>O
</p>
<p>Objective chance, 203
</p>
<p>Occupation number representation, 393
</p>
<p>One-body problem, equivalent, 44, 266
</p>
<p>Operator
</p>
<p>angular momentum, 30
</p>
<p>annihilation, 56
</p>
<p>antisymmetrization, 136
</p>
<p>antiunitary, 101
</p>
<p>creation, 57
</p>
<p>density, 122
</p>
<p>for mixed states, 121
</p>
<p>for pure states, 117
</p>
<p>reduced, 123, 155
</p>
<p>Hamiltonian</p>
<p/>
</div>
<div class="page"><p/>
<p>Index of Volume 2 521
</p>
<p>for central potential, 45
</p>
<p>for the harmonic oscillator, 56
</p>
<p>relativistic corrections, 70
</p>
<p>ladder, 31, 57
</p>
<p>Laplacian, spherical coordinates, 44
</p>
<p>lowering, 31, 56
</p>
<p>occupation number, 58
</p>
<p>orbital angular momentum, 29
</p>
<p>parity, 109
</p>
<p>polarization, 211
</p>
<p>raising, 31, 57
</p>
<p>scalar, 107
</p>
<p>statistical, 117
</p>
<p>symmetrization, 136
</p>
<p>time reversal, 113
</p>
<p>unitary, 100
</p>
<p>vector, 107, 281
</p>
<p>Orbital angular momentum, 29
</p>
<p>Oscillator length, 61
</p>
<p>P
</p>
<p>Partial-wave method, 173
</p>
<p>Particle
</p>
<p>distinguishable, 132
</p>
<p>identical, 133
</p>
<p>Particle, external and internal, 460
</p>
<p>Pauli exclusion principle, 137
</p>
<p>Pauli principle, 145, 409
</p>
<p>PCT-invariance, 114
</p>
<p>Permutation, 134
</p>
<p>Perturbation theory
</p>
<p>degenerate, 69
</p>
<p>nondengenerate, 66
</p>
<p>Phase velocity, 24, 254
</p>
<p>Photon, 392
</p>
<p>Pointer variable, 157
</p>
<p>Potential
</p>
<p>delta, 25
</p>
<p>effective, 46, 174
</p>
<p>Yukawa, 181, 564
</p>
<p>Potential barrier, 17
</p>
<p>Potential step, 6
</p>
<p>Potential well
</p>
<p>finite, 11, 20
</p>
<p>infinite, 20
</p>
<p>Principal quantum number, 49
</p>
<p>Product space, 79, 247
</p>
<p>Propagator, 449
</p>
<p>Q
</p>
<p>Quantum computer, 188, 190, 339
</p>
<p>Quantum copier, 183
</p>
<p>Quantum electrodynamics, 431
</p>
<p>Quantum field theory, 387
</p>
<p>Quantum logic, 231
</p>
<p>Quantum register, 189
</p>
<p>Quantum teleportation, 185
</p>
<p>Qubit, 188
</p>
<p>R
</p>
<p>Radial equation, 45
</p>
<p>Realism
</p>
<p>local, 89
</p>
<p>Reality, physical, 89
</p>
<p>Reduced mass, 44, 263
</p>
<p>Reflection coefficient, 10
</p>
<p>Renormalization, 480
</p>
<p>Ritz method, 143
</p>
<p>Rotation, active and passive, 99
</p>
<p>Rydberg constant, 49
</p>
<p>S
</p>
<p>Scattering
</p>
<p>elastic, 169
</p>
<p>identical particles, 329
</p>
<p>Scattering amplitude, 172
</p>
<p>Scattering cross section
</p>
<p>Rutherford, 181, 565
</p>
<p>Scattering problem
</p>
<p>stationary, 172
</p>
<p>Schr&ouml;dinger&rsquo;s cat, 85
</p>
<p>Shor&rsquo;s algorithm, 198, 351
</p>
<p>Slater determinant, 137
</p>
<p>S-operator, S-matrix, 435
</p>
<p>Spherical Bessel functions, 240
</p>
<p>Spherical function, 36
</p>
<p>Spherical harmonics, 237
</p>
<p>Spin-orbit coupling, 71
</p>
<p>State
</p>
<p>bell, 85
</p>
<p>entangled, 80
</p>
<p>GHZ, 210
</p>
<p>ground, 14
</p>
<p>mixed, 120
</p>
<p>product, 81
</p>
<p>pure, 117
</p>
<p>total angular momentum, 37
</p>
<p>total antisymmetric, 135
</p>
<p>total symmetric, 135
</p>
<p>Symmetry, 99
</p>
<p>for special Galilean transformation, 311
</p>
<p>spatial reflection, 109</p>
<p/>
</div>
<div class="page"><p/>
<p>522 Index of Volume 2
</p>
<p>spatial rotation, 106
</p>
<p>spatial translation, 104
</p>
<p>special Galilean transformation, 109
</p>
<p>time reversal, 111
</p>
<p>time translation, 103
</p>
<p>Symmetry transformations
</p>
<p>continuous, 101
</p>
<p>discrete, 109
</p>
<p>System
</p>
<p>open, 150
</p>
<p>T
</p>
<p>Tensor product, 79, 247
</p>
<p>Theorem
</p>
<p>Gleason, 367
</p>
<p>Kochen&ndash;Specker, 204
</p>
<p>Kramers, 323
</p>
<p>no-cloning, 183
</p>
<p>optical, 176
</p>
<p>spin-statistics, 33
</p>
<p>Stone&rsquo;s, 102
</p>
<p>Wigner&rsquo;s, 101
</p>
<p>Time order, 428
</p>
<p>Trace, 118
</p>
<p>partial, 124
</p>
<p>Transmission coefficient, 10
</p>
<p>Transposition, 134
</p>
<p>Tunnel effect, 17
</p>
<p>Turning point, classical, 5, 176
</p>
<p>V
</p>
<p>Vacuum state, 59
</p>
<p>Value-definiteness, 205
</p>
<p>Vector
</p>
<p>axial, 111
</p>
<p>polar, 111
</p>
<p>Virtual particles, 437
</p>
<p>Von Neumann equation, 123
</p>
<p>W
</p>
<p>Wave
</p>
<p>plane, 244
</p>
<p>spherical, 172, 245
</p>
<p>Wavefunction
</p>
<p>radial, 46
</p>
<p>Wave packet, 22, 253
</p>
<p>Wick&rsquo;s theorem, 453
</p>
<p>Z
</p>
<p>Zero-point energy, 61</p>
<p/>
</div>
<ul>	<li>Preface to the Second Edition, Volume 1</li>
	<li>Preface to the First Edition, Volume 1</li>
	<li>Contents</li>
	<li>Contents of Volume 2</li>
	<li>Introduction</li>
	<li>Overview of Volume 1</li>
	<li>Part I Fundamentals</li>
	<li>1 Towards the Schr&ouml;dinger Equation</li>
<ul>	<li>1.1 How to Find a New Theory</li>
	<li>1.2 The Classical Wave Equation and the Schr&ouml;dinger Equation</li>
<ul>	<li>1.2.1 From the Wave Equation to the Dispersion Relation</li>
	<li>1.2.2 From the Dispersion Relation to the Schr&ouml;dinger Equation</li>
</ul>
	<li>1.3 Exercises</li>
</ul>
	<li>2 Polarization</li>
<ul>	<li>2.1 Light as Waves</li>
<ul>	<li>2.1.1 The Typical Shape of an Electromagnetic Wave</li>
	<li>2.1.2 Linear and Circular Polarization</li>
	<li>2.1.3 From Polarization to the Space of States</li>
</ul>
	<li>2.2 Light as Photons</li>
<ul>	<li>2.2.1 Single Photons and Polarization</li>
	<li>2.2.2 Measuring the Polarization of Single Photons</li>
</ul>
	<li>2.3 Exercises</li>
</ul>
	<li>3 More on the Schr&ouml;dinger Equation</li>
<ul>	<li>3.1 Properties of the Schr&ouml;dinger Equation</li>
	<li>3.2 The Time-Independent Schr&ouml;dinger Equation</li>
	<li>3.3 Operators</li>
<ul>	<li>3.3.1 Classical Numbers and Quantum-Mechanical Operators</li>
	<li>3.3.2 Commutation of Operators; Commutators</li>
</ul>
	<li>3.4 Exercises</li>
</ul>
	<li>4 Complex Vector Spaces and Quantum Mechanics</li>
<ul>	<li>4.1 Norm, Bra-Ket Notation</li>
	<li>4.2 Orthogonality, Orthonormality</li>
	<li>4.3 Completeness</li>
	<li>4.4 Projection Operators, Measurement</li>
<ul>	<li>4.4.1 Projection Operators</li>
	<li>4.4.2 Measurement and Eigenvalues</li>
	<li>4.4.3 Summary</li>
</ul>
	<li>4.5 Exercises</li>
</ul>
	<li>5 Two Simple Solutions of the Schr&ouml;dinger Equation</li>
<ul>	<li>5.1 The Infinite Potential Well</li>
<ul>	<li>5.1.1 Solution of the Schr&ouml;dinger Equation, Energy Quantization</li>
	<li>5.1.2 Solution of the Time-Dependent Schr&ouml;dinger Equation</li>
	<li>5.1.3 Properties of the Eigenfunctions and Their Consequences</li>
	<li>5.1.4 Determination of the Coefficients cn</li>
</ul>
	<li>5.2 Free Motion</li>
<ul>	<li>5.2.1 General Solution</li>
	<li>5.2.2 Example: Gaussian Distribution</li>
</ul>
	<li>5.3 General Potentials</li>
	<li>5.4 Exercises</li>
</ul>
	<li>6 Interaction-Free Measurement</li>
<ul>	<li>6.1 Experimental Results</li>
<ul>	<li>6.1.1 Classical Light Rays and Particles in the Mach&ndash;Zehnder Interferometer</li>
	<li>6.1.2 Photons in the Mach&ndash;Zehnder Interferometer</li>
</ul>
	<li>6.2 Formal Description, Unitary Operators</li>
<ul>	<li>6.2.1 First Approach</li>
	<li>6.2.2 Second Approach (Operators)</li>
</ul>
	<li>6.3 Concluding Remarks</li>
<ul>	<li>6.3.1 Extensions</li>
	<li>6.3.2 Quantum Zeno Effect</li>
	<li>6.3.3 Delayed-Choice Experiments</li>
	<li>6.3.4 The Hadamard Transformation</li>
	<li>6.3.5 From the MZI to the Quantum Computer</li>
	<li>6.3.6 Hardy's Experiment</li>
	<li>6.3.7 How Interaction-Free is the `Interaction-Free' Quantum Measurement?</li>
</ul>
	<li>6.4 Exercises</li>
</ul>
	<li>7 Position Probability</li>
<ul>	<li>7.1 Position Probability and Measurements</li>
<ul>	<li>7.1.1 Example: Infinite Potential Wall</li>
	<li>7.1.2 Bound Systems</li>
	<li>7.1.3 Free Systems</li>
</ul>
	<li>7.2 Real Potentials</li>
	<li>7.3 Probability Current Density</li>
	<li>7.4 Exercises</li>
</ul>
	<li>8 Neutrino Oscillations</li>
<ul>	<li>8.1 The Neutrino Problem</li>
	<li>8.2 Modelling the Neutrino Oscillations</li>
<ul>	<li>8.2.1 States</li>
	<li>8.2.2 Time Evolution</li>
	<li>8.2.3 Numerical Data</li>
	<li>8.2.4 Three-Dimensional Neutrino Oscillations</li>
</ul>
	<li>8.3 Generalizations</li>
<ul>	<li>8.3.1 Hermitian Operators</li>
	<li>8.3.2 Time Evolution and Measurement</li>
</ul>
	<li>8.4 Exercises</li>
</ul>
	<li>9 Expectation Values, Mean Values,  and Measured Values</li>
<ul>	<li>9.1 Mean Values and Expectation Values</li>
<ul>	<li>9.1.1 Mean Values of Classical Measurements</li>
	<li>9.1.2 Expectation Value of the Position in Quantum Mechanics</li>
	<li>9.1.3 Expectation Value of the Momentum in Quantum Mechanics</li>
	<li>9.1.4 General Definition of the Expectation Value</li>
	<li>9.1.5 Variance, Standard Deviation</li>
</ul>
	<li>9.2 Hermitian Operators</li>
<ul>	<li>9.2.1 Hermitian Operators Have Real Eigenvalues</li>
	<li>9.2.2 Eigenfunctions of Different Eigenvalues Are Orthogonal</li>
</ul>
	<li>9.3 Time Behavior, Conserved Quantities</li>
<ul>	<li>9.3.1 Time Behavior of Expectation Values</li>
	<li>9.3.2 Conserved Quantities</li>
	<li>9.3.3 Ehrenfest's Theorem</li>
</ul>
	<li>9.4 Exercises</li>
</ul>
	<li>10 Stopover; Then on to Quantum Cryptography</li>
<ul>	<li>10.1 Outline</li>
	<li>10.2 Summary and Open Questions</li>
<ul>	<li>10.2.1 Summary</li>
	<li>10.2.2 Open Questions</li>
</ul>
	<li>10.3 Quantum Cryptography</li>
<ul>	<li>10.3.1 Introduction</li>
	<li>10.3.2 One-Time Pad</li>
	<li>10.3.3 BB84 Protocol Without Eve</li>
	<li>10.3.4 BB84 Protocol with Eve</li>
</ul>
</ul>
	<li>11 Abstract Notation</li>
<ul>	<li>11.1 Hilbert Space</li>
<ul>	<li>11.1.1 Wavefunctions and Coordinate Vectors</li>
	<li>11.1.2 The Scalar Product</li>
	<li>11.1.3 Hilbert Space</li>
</ul>
	<li>11.2 Matrix Mechanics</li>
	<li>11.3 Abstract Formulation</li>
	<li>11.4 Concrete: Abstract</li>
	<li>11.5 Exercises</li>
</ul>
	<li>12 Continuous Spectra</li>
<ul>	<li>12.1 Improper Vectors</li>
	<li>12.2 Position Representation and Momentum Representation</li>
	<li>12.3 Conclusions</li>
	<li>12.4 Exercises</li>
</ul>
	<li>13 Operators</li>
<ul>	<li>13.1 Hermitian Operators, Observables</li>
<ul>	<li>13.1.1 Three Important Properties of Hermitian Operators</li>
	<li>13.1.2 Uncertainty Relations</li>
	<li>13.1.3 Degenerate Spectra</li>
</ul>
	<li>13.2 Unitary Operators</li>
<ul>	<li>13.2.1 Unitary Transformations</li>
	<li>13.2.2 Functions of Operators, the Time-Evolution Operator</li>
</ul>
	<li>13.3 Projection Operators</li>
<ul>	<li>13.3.1 Spectral Representation</li>
	<li>13.3.2 Projection and Properties</li>
	<li>13.3.3 Measurements</li>
</ul>
	<li>13.4 Systematics of the Operators</li>
	<li>13.5 Exercises</li>
</ul>
	<li>14 Postulates of Quantum Mechanics</li>
<ul>	<li>14.1 Postulates</li>
<ul>	<li>14.1.1 States, State Space (Question 1)</li>
	<li>14.1.2 Probability Amplitudes, Probability (Question 2)</li>
	<li>14.1.3 Physical Quantities and Hermitian Operators (Question 2)</li>
	<li>14.1.4 Measurement and State Reduction (Question 2)</li>
	<li>14.1.5 Time Evolution (Question 3)</li>
</ul>
	<li>14.2 Some Open Problems</li>
	<li>14.3 Concluding Remarks</li>
<ul>	<li>14.3.1 Postulates of Quantum Mechanics as a Framework</li>
	<li>14.3.2 Outlook</li>
</ul>
	<li>14.4 Exercises</li>
</ul>
	<li>A Abbreviations and Notations</li>
	<li>B Units and Constants</li>
<ul>	<li>B.1  Systems of Units</li>
<ul>	<li>B.1.1  Planck Units</li>
	<li>B.1.2  Theoretical Units (Units of High-Energy Physics)</li>
	<li>B.1.3 Atomic Units</li>
	<li>B.1.4 Units of Energy</li>
</ul>
	<li>B.2  Some Constants</li>
	<li>B.3  Dimensional Analysis</li>
	<li>B.4  Powers of 10 and Abbreviations</li>
	<li>B.5  The Greek Alphabet</li>
</ul>
	<li>C Complex Numbers</li>
<ul>	<li>C.1 Calculating with Complex Numbers</li>
	<li>C.2 Are Complex Numbers Less Intuitive than Real Numbers?</li>
	<li>C.3 Exercises</li>
</ul>
	<li>D Calculus I</li>
<ul>	<li>D.1 One Real Independent Variable</li>
<ul>	<li>D.1.1 The Taylor Expansion</li>
	<li>D.1.2 L'H&ocirc;pital's Rule</li>
	<li>D.1.3 Mean Value Theorem for Integration</li>
</ul>
	<li>D.2 Several Independent Variables</li>
<ul>	<li>D.2.1 Differentiation</li>
	<li>D.2.2 Taylor Series</li>
	<li>D.2.3 Vector Algebra</li>
</ul>
	<li>D.3 Coordinate Systems</li>
<ul>	<li>D.3.1 Polar Coordinates</li>
	<li>D.3.2 Cylindrical Coordinates</li>
	<li>D.3.3 Spherical Coordinates</li>
</ul>
	<li>D.4 Exercises</li>
</ul>
	<li>E Calculus II</li>
<ul>	<li>E.1 Differential Equations: Some General Remarks</li>
	<li>E.2 Ordinary Differential Equations</li>
	<li>E.3 Partial Differential Equations</li>
	<li>E.4 Exercises</li>
</ul>
	<li>F Linear Algebra I</li>
<ul>	<li>F.1 Vectors (Real, Three Dimensional)</li>
<ul>	<li>F.1.1 Basis, Linear Independence</li>
	<li>F.1.2 Scalar Product, Vector Product</li>
	<li>F.1.3 Polar and Axial Vectors</li>
</ul>
	<li>F.2 Matrix Calculus</li>
<ul>	<li>F.2.1 Special Matrices</li>
	<li>F.2.2 The Eigenvalue Problem</li>
	<li>F.2.3 A Remark on Hermitian Matrices</li>
</ul>
	<li>F.3 Exercises</li>
</ul>
	<li>G Linear Algebra II</li>
<ul>	<li>G.1 Groups</li>
	<li>G.2 Vector Spaces</li>
	<li>G.3 Scalar Product </li>
	<li>G.4 Norm</li>
	<li>G.5 Metric</li>
	<li>G.6 Schwarz's Inequality</li>
	<li>G.7 Orthogonality</li>
	<li>G.8 Hilbert Space </li>
	<li>G.9 Cast Algebra</li>
	<li>G.10 Exercises</li>
</ul>
	<li>H Fourier Transforms and the Delta Function</li>
<ul>	<li>H.1 Fourier Transforms</li>
	<li>H.2 The Delta Function</li>
<ul>	<li>H.2.1 Formal Derivation</li>
	<li>H.2.2 Heuristic Derivation of the Delta Function</li>
	<li>H.2.3 Examples, Properties, Applications</li>
	<li>H.2.4 The Delta Function and the Laplace Operator</li>
</ul>
	<li>H.3 Fourier Series</li>
	<li>H.4 Discrete and Quantum Fourier Transforms</li>
	<li>H.5 Exercises</li>
</ul>
	<li>I Operators</li>
<ul>	<li>I.1 Norm, Domain of Definition</li>
<ul>	<li>I.1.1 The Norm</li>
	<li>I.1.2 Bounded Operators</li>
	<li>I.1.3 Domain of Definition</li>
</ul>
	<li>I.2 Hermitian, Self-adjoint</li>
<ul>	<li>I.2.1 Definitions and Differences</li>
	<li>I.2.2 Two Examples</li>
	<li>I.2.3 A Note on Terminology</li>
</ul>
	<li>I.3 Unitary Operators; Stone's Theorem</li>
<ul>	<li>I.3.1 Stone's Theorem</li>
	<li>I.3.2 Unitary or Hermitian?</li>
</ul>
	<li>I.4 The Uncertainty Principle</li>
<ul>	<li>I.4.1 Derivation 1</li>
	<li>I.4.2 Derivation 2</li>
	<li>I.4.3 Remarks on the Uncertainty Principle</li>
</ul>
	<li>I.5 Hermitian Operators, Observables</li>
	<li>I.6 Exercises</li>
</ul>
	<li>J From Quantum Hopping to the Schr&ouml;dinger Equation</li>
	<li>K The Phase Shift at a Beam Splitter</li>
	<li>L The Quantum Zeno Effect</li>
<ul>	<li>L.1 Unstable Systems</li>
	<li>L.2 Simple Model Calculation</li>
	<li>L.3 Interaction-Free Quantum Measurement</li>
</ul>
	<li>M Delayed Choice and the Quantum Eraser</li>
<ul>	<li>M.1 Delayed Choice Experiments</li>
<ul>	<li>M.1.1 Setup 1</li>
	<li>M.1.2 Setup 2</li>
</ul>
	<li>M.2 The Quantum Eraser</li>
</ul>
	<li>N The Equation of Continuity</li>
	<li>O Variance, Expectation Values</li>
<ul>	<li>O.1 Variance, Moments</li>
	<li>O.2 Expectation Value, Mean Value</li>
	<li>O.3 Discrete and Continuous</li>
	<li>O.4 Standard Deviation in Quantum Mechanics</li>
<ul>	<li>O.4.1 Example: Two-State System</li>
	<li>O.4.2 General Case</li>
</ul>
</ul>
	<li>P On Quantum Cryptography</li>
<ul>	<li>P.1 Verification of the Key</li>
	<li>P.2 An Example</li>
</ul>
	<li>Q Schr&ouml;dinger Picture, Heisenberg Picture, Interaction Picture</li>
<ul>	<li>Q.1 Schr&ouml;dinger and Heisenberg Picture</li>
	<li>Q.2 Interaction Picture</li>
<ul>	<li>Q.2.1 Exercises and Solutions</li>
</ul>
</ul>
	<li>R The Postulates of Quantum Mechanics</li>
<ul>	<li>R.1 Postulate, Axiom, Rule?</li>
	<li>R.2 Formulations of Some Authors</li>
<ul>	<li>R.2.1 J. Audretsch</li>
	<li>R.2.2 J.-L. Basdevant and J. Dalibard</li>
	<li>R.2.3 D.R. Bes</li>
	<li>R.2.4 B.H. Bransden and C.J. Joachain</li>
	<li>R.2.5 C. Cohen-Tannoudji, B. Diu, and F. Lalo&euml;</li>
	<li>R.2.6 K. Gottfried and T.-M. Yan</li>
	<li>R.2.7 C.J. Isham</li>
	<li>R.2.8 M. LeBellac</li>
	<li>R.2.9 G. M&uuml;nster</li>
	<li>R.2.10 W. Nolting</li>
	<li>R.2.11 A.I.M. Rae</li>
	<li>R.2.12 H. Rollnik</li>
	<li>R.2.13 H. Schulz</li>
	<li>R.2.14 F. Schwabl</li>
	<li>R.2.15 N. Zettili</li>
</ul>
</ul>
	<li>S System and Measurement: Some Concepts</li>
<ul>	<li>S.1 System: Isolated, Closed, Open</li>
	<li>S.2 Measurement</li>
</ul>
	<li>T Recaps and Outlines</li>
<ul>	<li>T.1 Discrete - Continuous</li>
<ul>	<li>T.1.1 Exercises and Solutions</li>
</ul>
	<li>T.2 Special Relativity</li>
<ul>	<li>T.2.1 Lorentz Boost and Four-Vectors</li>
	<li>T.2.2 Four-Vector Inner Product, Metric Tensor, Einstein Convention</li>
	<li>T.2.3 Exercises and Solutions</li>
</ul>
	<li>T.3 Classical Field Theory</li>
<ul>	<li>T.3.1 Particles</li>
	<li>T.3.2 Fields</li>
	<li>T.3.3 Canonical Quantization </li>
	<li>T.3.4 Some Lagrangian Densities</li>
	<li>T.3.5 Exercises and Solutions</li>
</ul>
	<li>T.4 Electrodynamics</li>
<ul>	<li>T.4.1 Maxwell Equations, Potentials, Gauge </li>
	<li>T.4.2 Free Solutions</li>
	<li>T.4.3 Electromagnetic Field Tensor </li>
	<li>T.4.4 Lagrangian mathcalL</li>
	<li>T.4.5 Some Lagrangian Densities</li>
	<li>T.4.6 Exercises and Solutions</li>
</ul>
</ul>
	<li>U Elements of Relativistic Quantum Mechanics</li>
<ul>	<li>U.1 Introduction</li>
	<li>U.2 Constructing Relativistic Equations</li>
<ul>	<li>U.2.1 Klein&ndash;Gordon Equation</li>
	<li>U.2.2 Dirac Equation </li>
</ul>
	<li>U.3 Plane Wave Solutions</li>
<ul>	<li>U.3.1 Klein&ndash;Gordon Equation</li>
	<li>U.3.2 Dirac Equation</li>
</ul>
	<li>U.4 Covariant Formulation of the Dirac Equation</li>
<ul>	<li>U.4.1 Introducing  Matrices</li>
	<li>U.4.2 How to Show the Covariance of the Dirac Equation - A Short Outline</li>
	<li>U.4.3 Coupling to the Electromagnetic Field</li>
	<li>U.4.4 Nonrelativistic Limit of the Dirac Equation</li>
</ul>
	<li>U.5 Dirac Equation and the Hydrogen Atom</li>
	<li>U.6 Discussion of the Dirac Equation</li>
<ul>	<li>U.6.1 Pros and Cons of the Dirac Equation</li>
	<li>U.6.2 Antiparticles </li>
	<li>U.6.3 Negative Energies </li>
	<li>U.6.4 Feynman&ndash;St&uuml;ckelberg Interpretation </li>
</ul>
	<li>U.7 Exercises and Solutions</li>
</ul>
	<li>V Exercises and Solutions to Chaps. [chap:1]1&ndash;[chap:14]14</li>
<ul>	<li>V.1 Exercises, Chap.[chap:1]1</li>
	<li>V.2 Exercises, Chap.[chap:2]2</li>
	<li>V.3 Exercises, Chap.[chap:3]3</li>
	<li>V.4 Exercises, Chap.[chap:4]4</li>
	<li>V.5 Exercises, Chap.[chap:5]5</li>
	<li>V.6  Exercises, Chap.[chap:6]6</li>
	<li>V.7  Exercises, Chap.[chap:7]7</li>
	<li>V.8  Exercises, Chap.[chap:8]8</li>
	<li>V.9  Exercises, Chap.[chap:9]9</li>
	<li>V.10 Exercises, Chap.[chap:11]11</li>
	<li>V.11 Exercises, Chap.[chap:12]12</li>
	<li>V.12 Exercises, Chap.[chap:13]13</li>
	<li>V.13 Exercises, Chap.[chap:14]14</li>
</ul>
	<li> Further Reading</li>
<ul>	<li/>
</ul>
	<li> Index of Volume 1</li>
	<li> Index of Volume 2</li>
</ul>
</body></html>